Cross-linguistically, there are two devices for grammar to process the focus mark [+FocusJ: the fronting of focused constituents and the insertion of a Focus Mark such as the English 'be' before focused constituents. In this mode of formulation, a comparative study of Focus and Focus-Marking in Chinese and Malay has been conducted. These two languages are similar and different. They are similar in opting for the use of Focus Mark instead of focused constituents movement. They are different in the nature of Focus Mark itself Focus mark is the copular verb SHI A in Chinese, but, as the language simply does not have a copular verb, in Malay two complementary particles KAH/LAK are chosen, and all other contrasts in Focus-Marking between the two languages are demonstrated to follow from the difference in the nature of Focus Mark and some other independently motivated conditions in a modularized theory of grammar. 
In this paper, I present an explicit analysis of association with focus in Japanese. The proposed formal analysis, basically couched in Rooth's (1985) alternative semantics supported by the syntactic devices of HPSG, captures the behaviors of focus particles in Japanese in a fairly straightforward manner. Section 1 is devoted to a survey of data and definitions of key terms and concepts in this paper. In section 2, I present my analysis and explain in detail how it accounts for the observed phenomena. 
This paper presents an analysis of indirect passives in Japanese in terms of event structure and qualia structure proposed in the framework of the generative lexicon (Pustejovsky 1995). On the assumption that the event structure of the indirect passive construction is based on the causative structure, the present analysis accounts for the adversative interpretation of indirect passive sentences, the selection restriction on verbs, and the obligatory presence of the adjunct phrase. 1. Introduction Linguists normally assume that the active sentence in (1 a) and the passive sentence in (lb) designate the same situation, rejecting the idea that the active and passive sentences denote distinct events. (1) a. The assassin killed the senator. b. The senator was killed by the assassin. 
The contemporary Japanese verbal honorific constructions can be motivated in view of the RRG clausal organization and its operator hierarchy. I will present an analysis of the subject honorific construction based on a functional analysis of existential and copulative verbs, and relate it to adjectival verbs. I will also sketch a possible origin of the non-subject honorific construction. 
This paper studies the derivation of multiple nominative constructions (MNC) in Japanese. First, discussing the MNC-sentences in which there is a relation of inalienable possession between nominative noun phrases, I will argue that the set of local economy principles that choose among potentially possible steps at a single stage of a derivation contains a principle that minimizes the size of moved elements. Second, considering the derivation of the MNC-sentences in which there is no relation of inalienable possession between nominative noun phrases, I will show a new piece of evidence for the Merge-over-Move principle.  
We present a method for the automatic learning of stemming rules for the Indonesian language. The learning process uses an unlabelled corpus. In the first phase the candidate (word, stem) pairs are automatically extracted from a set of online documents. This phase uses a dictionary but is nevertheless not trivial because of morphing. In the second phase the rules are induced from the thus obtained list of pairs of words with their respective stems. We evaluate the effectiveness of our method for different sizes of the training set with different settings on the thresholds of the support and confidence of each rule. We discuss how these variables affect the quantity and quality of the rules produced. 
Latent Semantic Indexing (LSI) approach provides a promising solution to overcome the language barrier between queries and documents, but unfortunately the high dimensions of the training matrix is computationally prohibitive for its key step of Singular Value Decomposition (SVD). Based on the semantic parallelism of the multi-linguistic training corpus we prove in this paper that, theoretically if the training term-by-document matrix can appear in either of two symmetry forms, strong or weak, the dimension of the matrix under decomposition can be reduced to the size of a monolingual matrix. The retrieval accuracy will not deteriorate in such a simplification. And we also discuss what these two forms of symmetry mean in the context of multi-linguistic information retrieval. Although in real world data the term-by-document matrices are not naturally in either symmetry form, we suggest a way to make them appear more symmetric in the strong form by means of word clustering and term weighting. A real data experiment is also given to support our method of simplification. 
This paper presents an integrated approach for Chinese word segmentation, which can perform disambiguation and unknown word identification simultaneously on the input. In this work, a hybrid model is used to score known word candidates and unknown word candidates equally by incorporating the modified word-formation models (viz. word-juncture models and wordformation patterns) into word bigram models, with which different types of features are statistically computed and combined for this integrated segmentation, including internal wordformation power of components in a word, affinity relations between these components and the external contextual information. To enhance the precision and avoid the problem of combination explosion in word candidate construction, a filter algorithm is also given to block ineligible unknown word candidates. In this way, ambiguity and unknown word can be resolved effectively. The results of our experiment on Peking University corpus show that the integrated approach outperforms the other two-stage methods under discussion.  
 Tracy Holloway KING Palo Alto Research Center 3333 Coyote Hill Rd. Palo Alto, CA 94304 USA thking@parc.com  We report on a preliminary investigation of the difficulty of converting a grammar of one language into a grammar of a typologically similar language. In this investigation, we started with the ParGram grammar of Japanese and used that as the basis for a grammar of Korean. The results are encouraging for the use of grammar porting to bootstrap new grammar development.  
06  Extract 1 Homo 56 57 58 59 60 61  A:...(2.1) wo juede wo you shihou\ I.Mr4f,M1141R\ 
Mandarin shape nouns, such as afanglxing2 'square' and sanljiao3xing2 'triangle', share a set of very interesting lexical semantic features. These nouns can refer to either the contour (i.e. the outside edge) or enclosed area of the shape. In this paper, we will try to explain this lexical semantic fact both in terms of the cognitive theory of grounding and the visualization. Our study will be focused on quanlzi5 'circle', which has the typical semantic behaviors of the shape nouns but also allow two additional interesting meaning extensions. 1. Background: the meaning of shape nouns Shape is an instance of a visual configuration while people perceive or recognize an object (Zusne 1970). We name these visual forms in term of their stimulous properties that we identified, such as a triangle is composed by three co-terminated lines that form three angles, and a circle is a continuous curved line without angles. By studying the semantics of shape nouns, we can gain substantial knowledge about what prominent properties people perceive while identifying shapes, and what cognitive concept that underlying the meaning of shape nouns. Mandarin shape nouns, such as a fanglxing2 'square' and sanifiao3xing2 'triangle', share a set of very interesting lexical semantic features. These nouns can refer to either the contour (i.e. the outside edge) or enclosed area of the shape. In this paper, we will try to explain this lexical semantic fact both in terms of the cognitive theory of grounding and the visualization. Our study will be focused on quanlzi5 'circle', which has the typical semantic behaviors of the shape nouns but also allow two additional interesting meaning extensions. In the following, Section 2 presents the data of quanizi5. Section 3 describes the sense distinction of the meaning of quanlzi5. Section 4 explains the lexical semantics of shapes in term of the figure/ground theory and the visualization. Section 5 is a conclusion.  115  2. The Meaning of Quanlzi5 'Circle': the data Like all shape nouns, quanlzi5 has the basic meanings of a circular contour (1) or the area enclosed by the circular contour (2). In addition, its area meaning can be extended metaphorically to refer to an area defined by human activity (3), an abstract confinement (4), and a set of related people often defined by their social strata (5). And its linear contour meaning can be extended to refer to a path where the end point coincides with the start point (6).  (1) tamen weicheng yige quanzi tiaowu they surround-be a circle dance `They made a circle to dance.'  (2) 't  gg  ni zhi neng zhan zai quanzi nei bu keyi paochulai  you only allow stand  circle inside NEG allow get out  `You can only stand inside the circle. Don't get out.'  (3)  ta de shenghuo quanzi jiushi taida he gongguan  she DE living  circle just NTU and GongGuan  `Her usual circle of activity just covers NTU (campus) and GongGuan.'  (4) .-3ze.  . 4s)c  -T- 7 -ft  A  jiaoyou  shi kuoda shenghuo quanzi de yizhong fangshi  make-friends is expand  life circle DE  a  way  `Making (new) friends is a way to expand (your) circle of life.'  (5) ta zai  meiguo  Rig *A de huaren  -141 quanzi hen youming  she BE America DE Chinese-people circle very famous  `She is famous in the Chinese societies in America.'  VO,  women zai shanshang milu  le yizhi zai rao quanzi  we  at mountain lost-way LE always ASP circle circle  `We got lost in the mountain and were going around in circles.'  116  quanlzi5 in (1) and (2) displays an original and physical meaning respectively, and has more abstract meaning in (3)—(6). Like all lexical polysemy, ambiguity between physical and metaphorical meanings of quanlzi5 is attested, such as in (7). Quanlzi5 has three possible readings: a professional group (7a), the area that dancing circle enclosed (7b), and the dancing circle itself (7c).  (7) NS  T  iAt  rA  iflffl  yinwei tiao lei le to jueding likai zhe ge  quanzi  because dance tired LE he decide leave this CLASS circle  (7a) He decided to leave from the dancing circle (formed by dancers) because he is tired.  (7b) He decided to leave the dancing area circle because of being tired.  (7c) He decided to leave the dancing circle because he is tired of all the dancing.  Based on the above examples, quanlzi5 seems to have four meanings. In next section, we will display the semantic representation of quanlzi5 in terms of a semantic model and give a clear explanation.  3. Accounting for the Meaning of Quanlzi5 Quanlzi5 is an example of polysemy, which is a situation where a single word has a set of related meanings. According to Ahrens et al (1998), the lexical meaning of a polysemy can be distinguished into two levels: senses and meaning facets'. They stated that a lexical sense entails the following properties:  (A) different sense cannot appear in the same context (unless the complexity is triggered deliberately); (B) a sense is not an instance of metonymic or meronymic extension, but may be an instance of metaphorical extension (Lin and Ahrens, 2000); (C) the link between two senses cannot be inherited by a class of nouns.  On the other hand, a meaning facet has the properties as follows: 
To what degree do different languages share similar conceptual metaphors? Charteris-Black and Ennis (2001) examined this question by running a comparative, corpus-based study of metaphors in Spanish and English financial reporting. They argue that both languages show considerable similarity in the choice of conceptual metaphors with the same linguistic expressions but that there is a differing degree of frequency. They attribute this similarity to the common cultural identity of the two languages such as the similar economic system and their Latinate origins. In this paper, we use a corpus-based approach in comparing the choice of conceptual metaphors in Mandarin Chinese with those of Spanish and English. We focus our discussion on the conceptual metaphor STOCK MARKET IS OCEAN WATER in Mandarin Chinese and compare it with the Spanish and English data. We carry out our analysis within the framework of the Conceptual Mapping (CM) Model (Ahrens 2002). With this model, we are able to demonstrate that although different languages share similar conceptual metaphors, they differ in what is mapped linguistically. These differences can be shown in their specific Mapping Principles. 
Part-of-speech tagging for a large corpus is a labour intensive and time-consuming task. In order to achieve fast and high quality tagging, algorithms should be high precision and in particular, its tagging results should require less manual proofreading. In this paper, we proposed a context-rule model to achieve both the above goals for pos tagging. We compared the tagging precisions between Markov bi-gram model and context-rule classifier. According to the experiments, context-rule classifier performs better than those two other algorithms. Also, it covers the data sparseness problem by utilizing more context features, and reduces the amount of corpus that is need to be manual proofread by introducing the confidence measure.  
Chinese is written without word delimiters so word segmentation is generally considered a key step in processing Chinese texts. This paper presents a new statistical approach to segment Chinese sequences into words based on contextual entropy on both sides of a bigram. It is used to capture the dependency with the left and right contexts in which a bigram occurs. Our approach tries to segment by finding the word boundaries instead of the words. Experimental results show that it is effective for Chinese word segmentation.  
Topic segmentation, which aims to fmd the boundaries between topic blocks in a text, is an important task for semantic analysis of texts. Although different solutions have been proposed for the task, many limitations and difficulties exist in the approaches. In particular most of the methods do not work well for such case as short texts, internet news and student's writings. In this paper, we focus on the short texts and present a method for topic segmentation. It can overcome the limitations in previous works. In preliminary experiments, the method show the accuracy of topic segmentation is increased effectively. 1. Introduction Topic segmentation, which aims to fmd the boundaries between topic blocks in a text, is an important task for semantic analysis of texts. In general, they rely on such knowledge as word recurrence, collocations, thesaurus, linguistics cues, or the combination of those, to measure the similarity between sentences, and estimate whether topic shift occurs (Ferret(2002)). The studies are also classified into two schemes: one is based on locally comparing adjacent blocks of sentences, while the other method is based on considering topic block boundary decision as a global optimization (Bigi et.a1(1998)). Although the approaches for topic segmentation have been worked well for long texts, the assumptions it requires limit most of useful applications. First, before topic segmentation, the thresholds, coefficients, or parameters of formulas in some methods must be estimated beforehand depend on the characters of text sets or the experience of users. It not only limits applications to be fully automatic but also causes difficulty in many domains. These methods perform especially poor when they are applied to such short texts as internet news and student's writings (Ponte and Croft(1997)). Since keywords from sentences are quite few and not reliable for short texts, the errors in measure naturally occur more frequently. It results in rapid decrease of the accuracy of topic segmentation. The accuracy of these methods not only relies on the characteristics of the short texts, but also depends on the languages. For Chinese, the usage of punctuation marks such as comma mark is often ambiguous (Chen(1993)). For instance, the sentence ending with comma mark is not always considered a complete sentence on syntax or semantics. For incomplete sentences, the step of extracting reliable keywords is far more difficult. This causes, due to the characteristic of the language, further decrease of the accuracy of topic segmentation. Apparently, short texts are troublesome and quite difficult for existing methods in topic segmentation. However, given a theme, a huge collection of reference texts for short texts can easily be obtained. This large corpus or thesaurus allows us to overcome the above mentioned limitations and define similarity between sentences. This paper will present a new method to segment topics for short texts. Section 2 reviews previous studies for text segmentation. Section 3 discusses the proposed technique in detail. Section 4 shows the performance of the method on some experiments. Section 5  59  discusses the conclusion. 2. Previous Work There are two features in short texts processed in this paper. First, the content of the texts is complete but its length is short. That is, topic blocks in texts are sometimes composed of few sentences or one sentence. Secondly, the sentences in topic blocks may be incomplete on syntax or semantics. Though many of previous methods have been worked well, there are somewhat restrictions in the approaches applied to segment short texts. We will discuss detail below. In earlier research, the topic blocks must be composed of several punctual paragraphs. Hearst(1997) introduces the TextTiling algorithm that measures the similarities between paragraphs with word sets of neighbor paragraphs, and then uses the similarities and known paragraph boundaries to place the position of topic shifts. It is useful to the lengthy texts in which article structure is punctual. Similarly, Salton et. al(1996) measures the similarity between paragraphs with term weight methods such as tf-idf weight, and links two paragraphs together when the similarity between them exceeds a threshold. Then, all links among paragraphs form a paragraph relation network, called text relationship map, and the sets of the paragraphs linked serially imply the same topic. The major difficulty of the method is that the principle for stating threshold is inexact. Moreover, the method applied to sentences does not work well because there are not many terms in the sentences. Nevertheless, it points out an interesting observation: that topic relationship not only exists in neighboring sentences but also in sentences in a topic block. In our studies, this idea will be used to develop a scheme to tolerate measuring errors. In recent years, some of the approaches focus on detecting the occurrence of the topic shifts in sentence level. Beeftnan et. al(1999) develops a feature-based method with an exponential model to detect the places of topic shift in sentence. It uses cue-words feature, one of two features in the method, to detect occurrences of specific words that frequently occurs in the topic blocks boundaries. In our domain, such approach is often not applicable since cue-words may not appear in the few sentences in a topic block. Alternative approaches such focus on topic detection and tracking (TDT), which aims to process huge amounts of information as newswire. Hidden Markov model (HMM) (e.g. Blei and Moreno(2001), Yamron et. *1998)) is a major technique for topic segmentation in TDT group. Ferret(2002) also presents and implements a method, called TOPICOLL, which combines word repetition and the lexical cohesion in texts for segmentation and link detection. The system processes texts linearly. However, it involves a delay state before deciding whether or not the current processing segment really ends. Nevertheless, the delay state is only effective for the tolerance of the similarity errors among several serial words. Ponte and Croft(1997) presents a three-step method. It first uses lexical context analysis (LCA) to extract semantically related words in a sentence, and compute the number of related words in common between sentences. Secondly, it scores each possible topic block with both internal-external scoring model and Gaussian length model. Finally, it determines the position of the topic shifts by dynamic programming with the score of each possible topic blocks. In contrast to the previous methods mentioned earlier, it is a global optimization method. The method claims that it works well in spite of small segments with few common words. However, it must properly estimate the maximum segment size before segmentation by dynamic programming, and obtain the parameters of length model from a training set. When the variance of the size of topic blocks is large, the estimation of parameters is usually not reliable for the method. Furthermore, it is also unreliable to rely only on counting the numbers of related words in common between sentences to measure similarity. Based on the observations, we develop a new approach which is both efficient for detecting topic shift in short texts and effective above for fault tolerance of the similarities between sentences. Moreover, it does not need any thresholds or parameters used by other methods. 3. Methodology The method in this paper is divided into four steps: retrieving and expanding keywords, measuring similarity between sentences, scoring candidate topic blocks, and ranking segment sequences. In 160  retrieving and expanding keywords and similarity measure between sentences stages, the previous methods are usually classified into domain-specific methods, domain-independent methods, or hybrid methods according to the properties of their resource and application. Based on the properties of short texts as mention earlier, the method in this paper is designed for a domain-specific method in retrieving and expanding keywords stage. Moreover, the method determines the positions of topic shifts with global optimization. We discuss the detail below. Previous studies often use a set of nouns or noun terms to represent both a sentence and implied bearings of a topic. This assumption is not effective or sufficient for short texts. Since a topic is usually changed with just a few sentences and the nouns in the sentence are either replaced with pronouns or even disappear. To overcome this difficulty, Ponte and Croft(1 997), and Xu and Croft(1996) propose to expand the set of nouns extracted from a sentence to include other nouns for representation of a sentence. The expansion method is based on technique of query expansion in information extraction. However, all these efforts and improvements are still not sufficient for many such other applications as student writings and internet pages. In our method, we define the original keywords of a sentence as the set of noun, verb, adjective words and phrases in the sentence based on the characteristics of the short texts. Furthermore, the original keywords will be extended the following method to include other terms and become a large set of keywords called expansion keywords.  3.1 Retrieving and expanding keywords  In this subsection, the step for expanding keywords is based on the concepts of co-occurrence and  distance of two keywords in a passage which is composed of a fixed number of serial sentences. The  concept of co-occurrence (Baeza-Yates and Ribeiro-Neto(1999), Ponte and Croft(1997), Xu and  Croft(1996)) of two keywords in the same passage has been used to imply the possible existence of the  same topic. We note that the frequency of the co-occurrence is an important cue of determining the  degree of correlation between two keywords. Furthermore, the degree of correlation of two keywords  also depends on their distance in a passage (Baeza-Yates and Ribeiro-Neto(1999)). We will define the  distance as the number of the sentences between two keywords.  Fig. 1 shows an example for the concepts of co-occurrence and distance of two keywords. The distance between keyword k2 and k3 is 1, the distance between keyword k3 and k4 is 3. Because the length of the passage in Fig. 1 is 3, keyword k2, k3, and k4 are co-occurrence words in the passage, but  keyword kl and k4 are not.  ki  k2 k3  k4  si s2 s3 • The sentences of the set of reference texts 0 The keywords of the sentences Fig. 1 An example for computing the relationship between the keywords  Based on the concepts of both co-occurrence and distance, our method will compute a matrix  which describes the correlations among keywords. First, all keywords are retrieved from the set of reference texts. If the number of the keywords is n, then the correlation matrix R is a n x n matrix. The element r1-i of matrix R represents the correlation between keyword i and j, and is computed as  following:  ri,i =  occ(i, j)  (1)  teT pet  where t is the text in the set T of reference texts, p is the passage in the text t, and occ(i, j) is computed  161  as following:  
This paper presents the model that can be used to filter the texts which the user is interested in from a large scale of source texts in Chinese or in English. Each text which the user is interested in can be represented as a vector in the vector space of classifiable sememes. The text to be sifted is represented as a vector too. The relevance of the text to the user can be measured by using the cosine angle between the text and its k nearest neighbor in the vector space. Experiments have been done and their results show that this scheme yields good results . 
This paper attempts to explore the semantics of onomatopoeic speech act verbs. Language abounds with small bits of utterances to show speaker's emotions, to maintain the flow of speech and to do some daily exchange routines. These tiny vocalizations have been regarded as vocal gestures and largely studied under the framework of 'interjection'. In this paper, the emphasis is placed on the perlocutionary force the vocal tokens contain. We describe their conventionalized lexical meaning and term them as onomatopoeic speech act verb. An onomatopoeic speech act verb refers to a syntactically independent monomorphemic utterance which performs illocutionary or perlocutionary forces. It is normally directed at the listener, which making the recipient to do something or to solicit recipient's response or reaction. They are onomatopoeic because most of them are imitation of the sounds produced by doing some actions.  
 source, pathways, and endpoint functionality.  3.0. The Database 3.1. Comparison of Liu's and Hsieh's database  Table 1. Comparison of Liu's and Hsieh's database.  Texts  Liu's 5  Hsieh's 15  Durations  255 mins  135 mins  The number of occurrences  of JTU  291  6672  The number of JIU per minutes  1.14 4.94  From the table above, we can see that the number of occurrences of jiu is much higher in  the present data, about five times as many as those in Liu's data. In average, Mandarin speakers utters five tokens of jiu per minute. And I follow Liu's study and categorize jiu  according to the function and summarize the distribution in Table 2. Before that, I give a brief description of the categorization of jiu.  3.2. The categorization of jiu 3.2.1. As a linking element  According to Liu (1993), there are main functions of jiu: as a linking element and as a limiting element. As a linking element, jiu relates two propositions in an  antecedent-consequent relation and always occurs with the consequent clause, as in A, jiu B  (A and B are two different propositions). Without making further sub-categorization here, Liu  recognizes the dependency of A-C can be of any semantic type, but that the jiu-marked  consequent proposition clauses are directly resultative of the antecedent.  As the following examples reveal, however, not all jiu-marked propositions in such  relations are a consequence of the event indicated in the antecedent clauses. Based on the data  I have, I therefore sub-categorize such an A-C linking usage into three types:  LCONDITIONAL. In this usage, jiu usually cooccurs with the following phrases:  zhiyau...jiu "R5- ...a"; yaushi ...jiu "5  ruguo ...jiu "prm...-a"; na(...)jiu.  iirs(• • •)-a", (na/ruguo)dehua. fiu "(NIABV)115-1.. .g" ; buran. fiu "T ...a". For  example:  (1) Basketball 103 B:..zhiyau you henduo ren  R-gz44(faA 104 jiu keyi cangdau  "As long as we have lots of persons, we can get (the basketball field)."  2.CONSEQUENTIAL: In this usage, jiu marks the consequent clause as a consequence of  2 hi my data, I did not take into account the use of jiu in the phrase offiushishuo, which is routinized as a discourse marker. 183  the event indexed by the antecedent clauses, and usually co-occurs with the following phrases: yinwei...jiu "M...a";fieguo...jiu "ei...a"; (suoyi/na)...jiu (/uede/xing) (shuo) " ( pfitv#13) ...a ( *WE ) ( ) "; suoyi...jiu "ffij.),...-sr. For example: (2) Basketball 410 .. suoyi wo ye jiu yizhi da pfiwRtst–ittIT 411 .. ranhou pingchangjiu yizhi tiau mfA').41-t–a.ot 412 .. yizhi tiau ----nt -- 413 ..jiu bian gau le\ atwx-r\ "(At the beginning of this semester, everyone was playing the basketball;) therefore, I also started playing the basketball. And in usual time with nothing to do, I just jumped and jumped; as a result, I became taller." 3.SEQENTIAL: In this usage, jiu marks the temporal sequence of two propositions, as in A, then jiu B. It usually co-occurs with such phrases as houlai...jiu "fA*....-,51"; ranhou...jiu".M...a"; na...jiu IN ...a". For example: (3) Basketball 200 B: ..ranhou MfA 201 ..zhau ren yihou 1-1AUtA ---> 202 ..jiu you ji qingchu shi shei -4tii-EM3VAS "Then, after you've found someone, you have to remember who you did find." 3.2.2. As a temporal linking element When jiu is used as a temporal linking element, it relates an event/state to a temporal frame, be it a temporal phrase or a temporal clause. The essential function of jiu is to assert a direct, or immediate relation between an event/state and a temporal reference point, by signaling an extremely short interval. It implies a time span between a temporal reference point and the event/state, and excludes all but the beginning of the span. jiu as a temporal linking function to mark "immediate future" (Biq 1988: 83) the event is described relative to the speech act time or to some assumed nominative reference point. For example: (4) Match 408: na-mo zau jiu kai-shi zuo-mei NI 7. -T-Miii/FR "(You) started making matches at that early age." 3.2.3. As a limiting element (flu can also be used as a sentential particle to limit, usually in an emphatic manner, the referential or predicational scope of a subsequent constituent. As a limiting element, jiu does not indicate relations across two propositional entities; instead, it is relevant only to one following constituent. The limiting jiu is usually considered to be 'emphatic' (Biq 1988. Li & Thompson 1981), and Liu (1993) attributes such emphatic reading to its scope limiting character: highlight a selected member out of a general set. For example: 184  (5) MCON3 124: wo fiu xi-huan yun-dong MIA-WEE  "I like nothing but sports."  (6) Match  116: ta jiu jiau ke-yi tai yi-dian  ft1:-  "What he can do is just lifting his leg a little bit."  3.2.4 As a concessive conditional marker  In this usage,fiu always combines with either shi 'be' or suan 'count' as in --ax I itA.  Such a usage is rare in our data; I found only two examples: (7) Homo 134 A:.. na suoyi ni fiusuan zhenzheng you #13PRWIMXAEg 135 ..ni dagai ye buhui tai ganjue dau fiFtVitT tStY11 "Therefore, even if you do have (that kind of propensity), you probably wouldn't be aware of it." (8) MCON1 445 ..na zue zhuyau shi yinwei zheli feng da AIWA -K; 446 ..ranhou women zheli shi di de ATARIMIliflrfg ▪ 447 ..fiushi women gang sau-guo grAafrININA 448 ..keshi TirA 449 ..ta yi cue SP—rIX 450 ..ye cue dau  "The most important is that the wind here is rather high and then (the ground) here is low; even if we just finish sweeping, the wind will still blow the dirt (and the trash) here."  3.2.5. hiu in prepositional phrases  Another use ofjiu is the occurrence ofjiu in a prepositional phrase, as in the phrases of  jiu...reyian / jiu...cengmian  El", in which jiu introduces the topical frame  for the following proposition. For example:  (9) Assig  318 .. ta ciji you dingwei EPZR. 319 ..ranhou zai xuexiau  • 320 ..fiu zemo ativ 321 ..zemo  • 322 ..zemo gen tongxue hudong de cengmian  323 „flu dau nali glYr_Wel "She has located herself in a clear position. With regard to the relationship between classmates in school, she would never exceed the assumed relation."  185  3.2.6. The identificational use In her thesis, Liu also discusses the identificational use of:flu, as infiushi 111",jiushuo  or the reduced form jiu, which she views as a convergence of its linking and limiting  functions. In this usageflushi or jiu functions as an equating or identificational marker that  attaches a preceding NP with a structurally parallel NP or relative type of clause. Though she  gives examples to illustrate the usage, she does not indicate the number of occurrences in her  distribution table (Liu 1993:81&138).  Following are Liu's examples (p.138):  (10) ta shoo ta jihu dou meiyou shenmo tiezhe le AfatAtiVAT*614Efitilt WT jiushi pin-xue pin de hen yanzhong.  glEkrairnaf4fitEW "She said she barely has any iron (in her blood); in other words, she has serious anaemia." (11) tong-sheng-nan, ta hai keyi la,  Tong Sheng Nan fthaTictkik  man Jiushuo ta men-zhe tou zuo la, ren man quianxu de.  , 1,1114Milk  t,  "Tong Sheng Na is alright; that is, he is hardworking and modest."  From the examples, we can clearly see that the two tokens of jiu here are more like  paraphrasing than identificational usage. Furthermore, the use of jiushuo as an identificational  (or paraphrasing) marker does not occur in my data.  4.0 Results and Findings  Table 2 summarizes a comparison of the distribution of jiu in Liu's and Hsieh's results.  Table 2. The distribution of jiu in Liu's and Hsieh's database  Liu's  Hsieh's  Linking  45  A-C linking  114  324  103  176  Temporal linking  47  23  Limiting  122  158  Concessive  6  2  Prepositional  0  
The problem of verb-noun categorial ambiguity is critical and relatively unique for non-inflectional languages, especially Chinese. We consider the verb-noun categorial fluidity a continuum and any categorial shift a transitional process. A synchronous corpus-based study was conducted to compare the phenomenon with respect to news texts collected from Hong Kong, Beijing, and Taiwan. It was found that about 15% of the verbs in the Hong Kong and Taiwan texts were undergoing the verb-noun categorial shift; whereas Beijing texts had more than 18% of the verbs undergoing this shift. The results also have important implications on various natural language applications, including lexicography, part-of-speech tagging of Chinese, as well as other natural language processing tasks. 
The main aim of this paper is to provide a new analysis of licensers of negative polarity items (NPIs). The problems with Fauconnier-Ladusaw's downward entailment analysis have been argued since Linebarger (1980). I will show that there exists a class of weak NPI licensers characterized by non-monotonicity and exclusivity. Weak negation, which is monotone decreasing, has been known to license weak NPIs such as any and ever (Zwarts 1993). However, non-monotonic items also trigger these wideners. Exclusivity or uniqueness characterizes non-monotonic operators, such as only, exactly n, superlatives, ordinal numerals, the determiner the, generic NPs, and also if and only if clauses, hope, happy, glad and others. Many of them function as generalized quantifiers which prohibit either downward or upward entailment. As Jespersen (1917) traces the origin of NPIs back to the strengthening of negation, non-monotonic contexts also favor strengthening by these words. We begin by considering the limited distribution of polarity items. The following section presents shortcomings of previous analyses, and then, non-monotonic expressions and their exclusivity are discussed. 
We propose and study a new variant of the SVM — the SVM with uneven margins, tailored for document categorisation problems (i.e. problems where classes are highly unbalanced). Our experiments showed that the new algorithm significantly outperformed the SVM with respect to the document categorisation for small categories. Furthermore, we report the results of the SVM as well as our new algorithm on the Reuters Chinese corpus for document categorisation, which we believe is the first result on this new Chinese corpus. 
 The Semantic Knowledge-base of Contemporary Chinese (SKCC) is a large scale Chinese semantic resource developed by the Institute of Computational Linguistics of Peking University. It provides a large amount of semantic information such as semantic hierarchy and collocation features for 66,539 Chinese words and their English counterparts. Its POS and semantic classification represent the latest progress in Chinese linguistics and language engineering. The descriptions of semantic attributes are fairly thorough, comprehensive and authoritative. The main work in this paper is to introduce the outline of SKCC, and establish a multi-level WSD model based on it. The results indicate that the SCK is effective for word sense disambiguation in Chinese and are likely to be important for general NLP.  Key words: semantic knowledge-base, word sense disambiguation (WSD), lexical semantics, Chinese language processing.  
In this paper an approach is proposed to combine different order N-grams based on the discriminative estimation criterion, on which the parameters of n-gram can be optimized. To raise the power of modeling language information, we propose several schemes to combine conventional different order n-gram language model. We employ Newton Gradient method to estimate the assumption probabilities and then test the optimally selected language model. We conduct experiments on the platform of conversion from Chinese pinyin to Chinese character. The experimental results show that the memory capacity of language model can be remarkably lowered with hide loss of accuracy. 1. Introduction In Chinese natural language processing domain, the parameters of n-gram can be estimated by calculating the frequency of word pair in text corpus and then normalizing the frequency. This conventional language model cannot satisfy our requirements since it is dependent of discriminative capability. We propose discriminative estimation approach, which can directly relate the estimation of n-gram parameters to its discriminative capability. We optimize the parameters of n-gram on the criterion of discriminative estimation by using Newton Gradient method. When we establish N-gram we artificially introduce an assumption over the relationship among adjacent words. Uni-gram is based upon the assumption that all words appear in the corpus independently. Bi-gram assumes that only contiguous words correlate with each other and tri-gram puts a constraint on the language information that one word can be predicted only by its two predecessor words. Some words are free of context and some depend on short or long history information under some circumstances. In this sense single N-gram could model the language phenomena with some compromise. This paper addresses the impact of the different assumption from different order n-gram on the performance of language model and proposed the combination and optimal selection of different n-gram to battle with artificial assumption and possible data sparsity problem. In the following sections, we first bring up with the discriminative estimation criterion. Next we describe the assumption from N-gram. In the following section we introduce the scheme for combination of different order N-gram. In the next section, an approach to optimal selection of different order language model is proposed. At last we report the experimental results on the platform of conversion from Chinese pinyin to Chinese character. 251  2 Discriminative Estimation In natural language processing, statistical language model has been proved to be a successful method and great efforts have been taken in building n-gram language model. (R. Isotani, 1994) reports the result of the research on stochastic language model with local and global language information. N-gram can be looked as Markov chain model, so maximum likelihood estimation can be used in ngram similar to the estimation of HMM parameters (L. Bahl, 1983). At the same time, the estimation of n-gram parameters is not surely relative to discriminative capability. Discriminative capability means the power that n-gram gets rise to correct results with higher score or probability contrast to wrong results. In speech recognition, discriminative training of HMM has been proposed from the viewpoint of pattern recognition (P. Chang, 1993; W. Chou, 1995). In this training approach, high recognition rate for training set is the motivation. The complicated objective function results in the complex formula and insupportable computational cost for parameter estimation. To simplify the above estimation criterion, we introduce the following objective function.  where 6 and 6 denote the correct word string and all possible word string, respectively. We can estimate the parameters of n-gram using Newton gradient method. For the parameters in the numerator,  
Multigram language model has become important in Speech Recognition, Natural Language Processing and Information Retrieval. An essential task in multigram language model is to establish a set of significant multigram compounds. In Yamamotto and Church (2001), an 0(NlogN) time complexity method based on Generalised Suffix Array (GSA) has been found, which computes the (term frequency) and df (document frequency) over 0(N) classes of substrings. The ff'and df form the essential statistics on which the metrics, such as MI (Mutual Information) and RIDF (Residual Inverse Document Frequency)', are based for multigram compound discovery. In this paper, it is shown that two related data structures to GSA, Generalised Suffix Tree (GST) and Generalised Directed Acyclic Word Graph (GDAWG) can afford even more efficient methods of multigram compound discovery than GSA. Namely, 0(N) algorithms for computing ff-and df have been found in GST and GDAWG. These data structures also exhibit a series of related, and desirable properties, including an 0(N) time complexity algorithm to classify 0(N2) substrings into 0(N) classes. An experiment based on 6 million bytes of text demonstrates that our theoretical analysis is consistent with the empirical results that can be observed.  
This paper addresses a novel translation method based on Hidden Markov Model using template rules after learning them from the bilingual corpus. The method can enhance the translation accuracy and ensure a low complexity in comparing with the pervious template learning translation method and draws a new perspective for applying statistical machine learning on example based translations. domain. Keywords: Machine translation, EBMT, Template learning translation, HMM. 
In our information era, keywords are very useful to information retrieval, text clustering and so on. News is always a domain attracting a large amount of attention. Aiming at news documents' characteristics and the resources available, this paper proposes to use Maximum Entropy (ME) model to conduct automatic keyword indexing. The focus of ME-based keyword indexing is how to obtain all the candidate items and select useful features for ME model. First, we make use of some relatively mature linguistic techniques and tools to obtain all the possible candidate items. Then, a feature set of ME model will be introduced. At last we test the model, and experimental results are given.  
Automatic Multi-word Units Extraction is an important issue in Natural Language Processing. This paper has proposed a new statistical method based on a large-scale balanced corpus to extract multi-word units. We have used two improved traditional parameters: mutual information and log-likelihood ratio, and have increased the precision for the top 10,000 words extracted through the method to 80.13%. The results of the research indicate that this method is more efficient and robust than previous multi-word units extraction methods. 
This paper addresses a novel sentence reduction algorithm base on decision tree model where semantic information is used to enhance the accuracy of sentence reduction. The proposed algorithm is able to deal with the changeable order problem in sentence reduction. Experimental show a better result when comparing with the original methods. 
We report a Japanese parsing system with a linguistically fine-grained grammar based on the Lexical-Functional Grammar (LFG) formalism. The system is the first Japanese LFG parser with over 97% coverage for real-world text. We evaluated the accuracy of the system comparing it with standard Japanese dependency parsers. The LFG parser shows roughly equivalent performance on the dependency accuracy to the standard parsers. It also provides reasonably accurate results of case detection.  
This paper describes a statistical approach for modeling Chinese-to-English back-transliteration. Unlike previous approaches, the model does not involve the use of either a pronunciation dictionary for converting source words into phonetic symbols or manually assigned phonetic similarity scores between source and target words. The parameters of the proposed model are automatically learned from a bilingual proper name list. The experimental results for back-transliteration indicate that the proposed method provides significant improvement over previous work. 
Of the 57 basic sentence categories (SC) defined in the HNC theory, action-effect SC is an important one with distinctive features. In the light of the HNC conceptual network, action-effect sentences in the Chinese language arise directly from causative verbs and compelling verbs, and indirectly from general acting verbs, i.e. via the use of the " de(4)" construction. In Chinese-English machine translation, action-effect sentences follow different SC and SF (sentence format) transfer rules. Therefore, different transfer frames should be adopted so as to ensure the generation of TL sentences with proper syntactico-semantic structures. Experiments show that the rules underlying the SC-SF transfer of action-effect sentences from Chinese to English can cover 90.3% of all the sentences in question. 
Adams B. BODOMO Department of Linguistics University of Hong Kong Pokfulam Road, Hong Kong abbodomo@hku.hk  1. Introduction) Verb order is an important issue in complex multi-verbal predicate constructions, for example, serial verb constructions (SVCs). With more than one verb in the construction, how are the verbs sequenced? What constraints are at play to govern their order? In this paper, we attempt to investigate several constraints that are related to the issue. We also propose a different ranking of these constraints for Cantonese, a Yue dialect spoken in the southeastern parts of China, and Dagaare, a Gur language spoken in the northwestern areas of Ghana, to account for the different verb orderings found in these two languages for SVCs.  We adopt an optimality-theoretic approach in our analysis. One of the advantages of doing so is that languages can be compared quite easily and effectively. This is because in Optimality Theory (OT), constraints are ranked. Each language has a different ranking of constraints and it is the rankings of the same set of constraints that are compared. This is the focus of the paper. A number of constraints are ranked and the rankings are used to account for and to compare the verb order phenomena of Cantonese and Dagaare SVCs.  The outline of our paper is as follows. We first undertake a brief survey of the phenomenon of SVCs in section 2. We then investigate each constraint that will be employed in our analysis of verb order in SVCs in section 3. In section 4 we attempt to model the verb order of Cantonese SVCs. Section 5 provides an account of the verb order of Dagaare SVCs. Section 6 concludes the paper.  2. The Phenomenon — Serial Verb Constructions (SVCs) Cantonese and Dagaare both allow verb serialization. A serial verb construction is, very broadly speaking, a construction that has two or more lexical verbs within a single clause. None of the verbs can be considered as contributing 'less' to the semantics of the whole construction, unlike the constructions which involve 'light' verbs in the Romance languages. Also unlike these constructions, the choice of verbs in SVCs is rather flexible. Verb serialization is not confined to any particular verbs. Consider the Cantonese SVC example in (1):  (1)  iffi  ngo5 zaal_gan2 cel lai4 zips  leis  aa32  1.SG drive.PROG car come pick-up 2.SG PART'.  `I am on my way to pick you up.  
In this paper we investigate the structure of spatial expressions in Saisiyat based in part on corpus data. Our corpus material includes both narratives and conversations that together run for approximately 100 minutes. First, the corpus was searched for syntactic patterns of spatial expressions. The structure of dynamic motion expressions is then examined with regard to types and tokens of motion verbs, expression of ground elements, directionality in path verbs, and serial verb constructions. We conclude that Saisiyat, like most of the Austronesian languages in Taiwan, is a verb-framed language. 
The current paper argues that the phenomenon in Japanese that case phrases occur without their head verb before the finite complementizer would falsify the HPSG valence/content analysis, for example, in Sag 1997, Pollard and Sag 1994, if no phantom relation corresponding to a verb is used in the syntax or semantics. The HPSG valence/content analysis is that the content of a case phrase structure-shares with a part of the content of its immediately larger constituent only through the valence of its head verb. In the framework of Koga 2000, which does not assume this, a syntax & semantics phrasal rule is proposed to specify the inherent meaning of a case phrase plus the finite complementizer, and not more than that inherent meaning. The semantics of every case form is specified independently of its head verb in Koga 2000. Koga's 2000 constraint-based grammar of case was implemented on unicorn3 parser developed at University of Illinois at Urbana-Champaign. 
Negative factors and their floating are the semantic marks of objective identification of the meaning of huaiyi and sub-categorization of sentence patterns of the huaiyi sentences. This article discusses negative factors, the floating of negative factors and recognition of different patterns of huaiyi sentences. It has also bee argued that there is an operational mark of syntactic-semantic generalization which functions to identify the semantic patterns of sentences, and demonstrated the nature and use of this kind of mark. We argue that more reliable connection can be established between human beings and computers. Some related theoretical and methodological issues are also addressed in this article. 
The task of identifying proper names, unknown words and new terms, is an important step in text processing systems. This paper describes a method of using mutual information to collect possible segments as candidates of these three feature types in a document scope. Then the construction and context of each possible feature is examined to determine its type, canonical form and meaning. Adding very little domain-specific knowledge, this method adapts to various domains easily. 
Two kinds of paraphrases extracted from a bilingual parallel corpus were analyzed. One is from an adjectival predicate sentence to a non-adjectival one. The other is from a passive form to a non-passive form. The ability to extract paraphrases is strongly desired for paraphrasing studies. Although extracting paraphrases from multi-lingual parallel corpora is possible, the type of paraphrases extracted is unknown. We discovered what types of examples can be obtained, and what types of paraphrasing will be available for the two kinds of paraphrases.  
Word selection is an vital factor to improve the quality of machine translation. This paper introduces a new model for word selection based on lexical semantic knowledge, which could deal with the problem significantly better. Meanwhile, the construction of the English lexical semantic knowledge base required for the model in our Chinese-English machine translation system is also discussed in detail. 
This paper proposes to disambiguate word senses by corpus-based ontology learning. Our approach is a hybrid method. First, we apply the previously-secured dictionary information to select the correct senses of some ambiguous words with high precision, and then use the ontology to disambiguate the remaining ambiguous words. The mutual information between concepts in the ontology was calculated before using the ontology as knowledge for disambiguating word senses. If mutual information is regarded as a weight between ontology concepts, the ontology can be treated as a graph with weighted edges, and then we locate the least weighted path from one concept to the other concept. In our practical machine translation system, our word sense disambiguation method achieved a 9% improvement over methods which do not use ontology for Korean translation. 1. Introduction An ontology is a knowledge base with information about concepts existing in the world, their properties, and how they relate to each other. An ontology is different from a thesaurus in that it contains only language independent information and many other semantic relations, as well as taxonomic relations. In this paper, we propose to use the ontology to disambiguate word senses. All approaches to word sense disambiguation (WSD) make use of words in a sentence to mutually disambiguate each other. The distinctions between various approaches lie in the source and type of knowledge made by the lexical units in a sentence. Thus, all these approaches can be classified into Al-based, knowledge-based, or corpus-based approaches, according to their sources and types of knowledge (Ide, 1998). AI-based WSD methods (Dahlgren, 1988) use a semantic network, or frames containing information about word functions and the relation to other words in individual sentences; or preference semantics, which specifies selectional restrictions for combinations of lexical items in a sentence. The difficulty with handcrafting the knowledge sources is the major disadvantage of AI-based systems. Knowledge-based methods (Resnik, 1995a; Yarowsky, 1992) have utilized machine-readable dictionaries (MRD), thesauri, and computational lexicons, such as WordNet. Since most MRDs and thesauri were created for human use and display inconsistencies, these methods have clear limitations. Corpus-based methods (Dagan, 1994; Gale, 1992) extract statistical information from corpora which is monolingual or bilingual, and raw or sense-tagged. The problem of data sparseness commonly occurs in the corpus-based approach, and is especially severe when processing in WSD. A smoothing and concept-based method is used to address this problem. Our WSD approach is a hybrid method, which combines the advantages of corpus-based and knowledge-based methods. We use our semi-automatically constructed ontology as an external knowledge source and secured dictionary information as context information. First, we apply the previously-secured dictionary information to select the correct senses of some ambiguous words with high precision, and then use the ontology to disambiguate the remaining ambiguous words. The remainder of this paper is organized as follows. In the next section, we describe the semi-automatic ontology construction methodology briefly. The ontology learning is explained in 399  L,  Root (dummy node)  I  
To make real Web information more machine processable, this paper presents a new approach to intra-page and inter-page semantic analysis of Web pages. Our approach consists of Web pages structure analysis and semantic clustering for intra-page semantic analysis, and machine learning based link semantic analysis for inter-page analysis. Based on the automatic repetitive patterns discovery in structure level and clustering in semantic level, we explore the intra-page semantic structure of Web pages and extend the processing unit from the whole page to a finer granularity, i.e., semantic information blocks within pages. After observing the various hyperlinks, we synthesize the Web inter-page semantic and define an information organizing oriented hyperlink semantic category. Considering the presentation of the hyperlink carrier and intra-page semantic structure, we propose corresponding feature selection and quantification methods, and then exploit the C4.5 decision-tree method to classify hyperlink semantic type and analyze the inter-page semantic structure. In our experiments, the results suggest that our approch is feasible for machine processing.  I  Introduction 
Today, corpus plays an important role in development and evaluation language and speech technologies, such as part of speech tagging, parsing, word sense disambiguation, text categorization, named entity classification, information extraction, question answering, structure discovery (clustering), speech recognition and machine translation systems, etc. One can exploit valuable statistical parameters taken from corpus to train and evaluate those systems. Developing such a corpus has been a challenging work in context that a huge data needed to be processed and annotated. In this paper we first represent our developing method for a multiobjective Vietnamese language corpus, namely VnCorpus, together with the description of various kinds of sources from which we have used to build up this database. It then goes on to describe some first experiences in using this corpus for the segmentation of sentences into Vietnamese words and for the recognition of Vietnamese continuous speech. Upon completion the corpus will constitute a valuable resource for research in the fields of computational linguistics, language and speech technologies.  
In Chinese, anaphors are frequently omitted, termed zero anaphor (ZA), from text due to their prominence. Thus the information carried by ZAs in text can not be used to contribute the calculation of text categorization. In this paper, we employ a ZA resolution method to recover the omissions of anaphors in text. Then the resulting text is used as the input of a text categorization system. The experiment result shows that ZA resolution method enhances the accuracy of text categorization from 79% to 84%.  
This paper aims to account for dependency of long-distance anaphors within the derivational approach. Dependency between the antecedent and the anaphor is determined by the universal operations, Merge, Move and Agree. Following Hornstein (2001) and Zwart (2002), anaphors in Korean, Chinese, and Japanese are argued to merge with antecedents to obtain anaphoricity. How such a merged complex participates in derivation is demonstrated using both local and long-distance binding examples. Logophoricity and discourse effects are obtained after computation within CHL when the antecedent and the anaphor are not merged at the outset.  
 Investigating existing and non-occurring onset clusters in Thai led to the postulation of a voicing constraint. Native speakers were asked to give well-formedness judgments to novel words with and without violations of the constraint. The findings support the argument for the existence of the constraint in the speaker's mind. Furthermore, it was found that within all groups of novel words, categorized by whether or not they obey the constraint and whether or not they contain the existing clusters, there were segmental neighborhood effects. The novel words in dense segmental neighborhoods were rated significantly higher than those in sparse segmental neighborhoods. Finally, the present study puts forward the proposal and evidence that the degree of tonal neighborhood density also influences the speaker's perception of novel words.  1. Clusters in Thai  Putting true Thai words in minimal pairs reveals that the language has 11 possible consonant clusters, which show up exclusively in the onset position (Naksakul 1998). The clusters consist of /pr p hr pl phl tr kr khr kl kh1 kw/ and /khw/. That is, the second consonant of a legal cluster is restricted to those in the set {r, 1, w} . Regarding the first consonant, they are drawn from the set of consonants belonging to the plosive class shown in (1).  (1) Plosives in Thai  Voiceless  Unaspirated  p  t  k  Aspirated  Ph  th  kh  Voiced  Unaspirated  b  d  Tumtavitikul (1997: 312) mentions (2a) and (2b) as constraints on the occurrence of clusters in Thai:  (2) a. C2 = {1, r, w}, and if C2 = [IN], then C 1 = [k, kh] b. *[ocson] [awn] It is true that the existing clusters obey the constraints. However, the constraints do not rule out the following plosive and C2 combinations: (3) /bl br dl dr tl thl thr/ In the present work, I am interested in looking at the set of combinations seen in (3) in comparison with the set of legal clusters. The next section presents the issues concerning onset clusters in Thai that I will investigate.  441  2. Hypotheses  As already seen, no combination in (3) violates (2b). In more general terms, they do not violate the Sonority Sequencing Principle, which, for onsets, requires the sonority of the segments to increase toward the nucleus. Since the sequences of sounds in (3) do not surface in the language, we need additional constraints to account for their nonoccurrence. Looking in wider context shows that the sequence of a nasal and an approximant can neither be found. Therefore, I will say that in general the language does not tolerate any sequence of a voiced consonant followed by an approximant. The constraint in (4) is then postulated to account for the data observed.  (4) * iF.cons I +cons voice +son +cont  For convenience, I will call this *[+voice]. The constraint may be a subpart of the constraint banning the sequence of consecutive voiced consonants in general. This is tentatively formulated for the purpose of the current study. To be explored in future work are effects of similarity in voicing quality of consonants in languages. Putting *[+voice] in a relatively high rank results that the first four clusters in (3), i.e. /bl, br, dl/ and /dr/, become illegal clusters in Thai. As for AV, /thl/ and /thr/, they obey both the Sonority Sequencing Principle and *[+voice]. The reason why they do not constitute legal onset clusters in the language does not seem obvious. Since the non-coronal plosives do not have any problem preceding III and Int, one may try to explain this by resorting to articulatory reasons. For example, one may say that the coronal plosives cannot occur with /1/ or Int because the sounds share the same place of articulation. However, this type of explanation can be immediately dismissed due to the existence of /tr/ as a legal cluster in the language. Since at the current stage, nothing appears to rule out /t1J, /thl/ and /thr/ as illegal clusters but the clusters are still not included in the legal category, I will simply group them under what I will call "Gap". The label reflects their missing distribution. Consequently, we now have three groups of clusters, as seen in (5).  (5) a. Legal: no constraint violation - /pr phr pl ph1 tr kr khr kl khl kw Ow/  b. Illegal: violation of *[+voice] - /bl br dl dr/  c. Gap: no constraint violation, but not in Legal  - thl thr/  A number of studies have come out with the objective of investigating the relationship between neighborhood density, i.e. the number of neighbors a given word has after changing one segment of the word, and word processing and judgment of well-formedness. On one side, research has shown that lexical neighborhood influences word perception in English, e.g. Luce & Pisoni (1998), Newman, Sawusch & Luce (1997), and Vitevitch & Luce (1998). On the other side, Frisch and Zawaydeh (2001) demonstrated that in Arabic similarity to existing words does not have significant effects on speakers' well-formedness judgments of novel forms. Instead, their results support the psychological reality of the so-called OCP constraint. In this paper, I will investigate the relationship between degrees of wordlikeness and well-formedness judgments of nonwords, having in mind the following hypotheses:  Hypothesis 1: If speakers are aware of the constraint *[+voice] while performing well-formedness judgments, given a comparable amount of neighborhood density, the ratings for novel words with no violation of *j+voice] should be consistently higher than those incurring a violation of the constraint. That is, to argue for the psychological reality of *[+voice], novel words with illegal clusters are expected to be less acceptable than those containing legal clusters or 'Gap' clusters.  Hypothesis 2: If lexical statistics turn out to be a crucial factor influencing well-formedness judgments, the mean rating of novel words in high-density neighborhoods should be significantly  442  higher than the mean rating of novels words that are similar to a smaller amount of existing words in the lexicon. As will be seen in the next section, the stimuli are constructed in such a way that each of the three groups, i.e. Legal, Illegal, and Gap, contains both dense- and sparse-neighborhood novel words. If this hypothesis is valid, we should be able to observe neighborhood effects across the board.  Hypothesis 3: Since Thai is a tone language, it would be interesting to see whether or not tone is involved when speakers make judgments on the well-formedness of novel words. To investigate this, I will make a distinction between segmental and tonal neighborhood. If tonal neighborhood density does affect speakers' well-formedness judgments, keeping other things constant, novel words in dense tonal neighborhoods should be given significantly higher ratings than those in sparse tonal neighborhoods.  The next section presents the design of the experiment and how the experiment was carried out. It is expected that the results from this experiment will provide the answer to the question regarding the status of the constraint postulated, that is whether (4) exists in the speaker's mind. Moreover, another objective of the experiment is to obtain additional findings concerning segmental and tonal neighborhood effects on speakers' well-formedness judgments of novel words.  3. Method  3.1. Material  I constructed 96 novel words to represent the clusters in the Legal, Illegal, and Gap groups. Within each group, the words were equally divided into two subgroups. The first subgroup had high segmental neighborhood density while the other had low segmental neighborhood density. Table 1 provides a summary of all the subgroups regarding their segmental neighborhood density. Each subgroup consisted of 16 novel words.  LEGAL  DENSE  8-13  (M = 10.5625)  SPARSE 0  ILLEGAL 5-6 (M = 5.5625) 0  GAP 4-6 (M = 4.8125) 0  Table 1. Segmental neighborhood density  Neighborhood density is measured by counting the number of existing words that match when one segment in a novel word is substituted. The transcriptions of stimuli were compared to all other transcriptions in Thiengburanathum's (1998) Thai-English Dictionary. I restricted the scope of existing words which the novel words can match after single segment substitution to those words that were familiar or still in use. I did not count any ancient or poetic words or words from a particular dialect that speakers may not know or may be unfamiliar with. To maximize the neighborhood density of the dense groups, only one-syllable novel words were used in the experiment. Despite the effort to make the neighborhood density as high as possible, the numbers of neighbors the stimuli in the dense subgroups had still turned out to be not very high, especially for Illegal Dense and Gap Dense. I then tried to expand the difference between the dense and sparse subgroups by defining stimuli in sparse neighborhoods as any novel words that match to none of the existing words when a segment is substituted. Although /br/, /dr/ and /t hr/ do not constitute legal clusters in the language, the following words are listed in Thiengburanathum's (1998) dictionary: blDk 'block', blu: 'blue', bre:k 'brake', dra.f 'draft', thre:t 'trade', and thre:n `train(v.)' . When constructing novel words, these English loanwords and any forms that would match or remind the participants of English words were avoided.  443  In addition, I did not include novel words with the onset Al/ in the stimuli set because the Thai script representing this sequence of sounds may mislead the speakers when they perform wellformedness judgments. There exist a number of Thai words that begin with the Thai letter representing /t/ followed by the letter for However, they are not pronounced together as a cluster. The vowel /a/ is always inserted between /t/ and Ill in speaking. When seeing this sequence in writing, the participants may give a relatively high rating to a novel word containing the sequence because the writing appears familiar to them. Regarding the issue of tone, only the novel words in Legal Dense were set up in such a way that they had different degrees of tonal neighborhood density. As Thai does not possess a large number of one-syllable words beginning with clusters, I found it very difficult to construct novel words having high tonal neighborhood density. For some subgroups, it is impossible to do the task. In the current experiment, sparse tonal neighborhood stimuli refer to those novel words which do not match any existing words after tone replacement. As for those in dense tonal neighborhoods, the range of existing words sharing with them all the phonemes except for the tone is one to two (M = 1.5). Both dense and sparse tonal neighborhood density groups have a comparable amount of segmental neighborhood density.  DENSE SPARSE  1-2 (M = 1.5) 0  Segmental Neighborhood Density 8-12 (M = 10) 10 –13 (M = 11.125)  Table 2. Tonal neighborhood density of two subgroups in. Legal Dense, shown with their segmental neighborhood density  Finally, another 60 novel words were added to the stimulus set as fillers. They were onesyllable words without any consonant cluster. Together with those beginning with clusters, the number of novel words added up to 156 in total.  3.2. Participants  Ten native speakers of Thai living in the Washington DC metro area took part in the study on a volunteer basis. Their age ranged from 23 to 30 years old. All of them had a bachelor's degree from home. They came to the US to continue their studies. At the time of this study, the time of the participants' stay in the US ranged from 6 months to 4 years.  3.3. Procedure  The list of 156 novel words was presented both orthographically and auditorily to the participants in an acceptability rating task. The stimuli were written in Thai script. Each stimulus was associated with a 1-5 scale. For the audio, I digitally recorded myself saying each novel form. All the 156 digital files were then arranged into a playlist with 5-second silence added between each file. In the experiment, the playlist was presented to them over headphones. The participants were instructed to judge each novel word using a 1-5 scale. A rating of 1 was described as 'This is least likely to be a word of Thai'. The degree of acceptability increased gradually to a rating of 5, which was described as 'This is most likely to be a word of Thai'. While performing the task, the participants were allowed to press the Stop button any time they would like to take a break or if they needed more time to think about a particular item before rating it. If they were not sure about any item, they were told to rate it according to their first impression. It took about 20 minutes to complete the task.  444  4. Results and Discussion  In this section, I will discuss the results of the experiment in the order of the hypotheses given in Section 2. That is, I will start from the discussion on the psychological reality of the constraint *[+voice]. Next, the results concerning segmental neighborhood effects on the speakers' well-. formedness judgments of the novel words will be examined. Finally, it will be shown whether or not some items are rated better than the others if they have higher tonal neighborhood density.  4.1. Voicing constraint  The mean ratings for all the stimuli within the six subgroups of Legal Dense, Legal Sparse, Illegal Dense, Illegal Sparse, Gap Dense, and Gap Sparse are given in the following figure:  5  4.5 -  4-  c co 3.53  _  cc 2.5 S 2 2 1 .5 - 
The purpose of this paper is to discuss treatment of 'focus particles' in Japanese sentences to be incorporated in a Japanese parsing system based on the Lexical-Functional Grammar(LFG) formalism. Focus particle can follow nouns, quantifiers, verbs, other particles (postpositions) and auxiliary verbs. Thus, it is necessary for a large-scale grammar to treat focus particles properly. Furthermore, there are syntactic and semantic ambiguities caused by the particles. We propose phrase structure rules and lexical entry constraints which cover focus particles in various positions and account for the ambiguities. 
Different syntactic frameworks have different ways to deal with dislocation constructions. One major disagreement is whether or not empty categories should be assumed. Researchers have been working on this issue on the ground of "psychological reality" (of empty categories), yet have not come to an agreement. The first aim of this paper is to propose an experimental scheme to settle the issue of empty categories. Our second aim is to propose that the application of the experiment to L2 is beneficial to see the teaching effect in L2 acquisition. If native speakers' result supports no-empty-category analysis, yet, L2 learners exhibit the different result, L2 teaching of "wh-movement" created a category which is not in native speakers' mental grammar. 
A large part of wide coverage Tree Adjoining Grammars (TAG) is formed by trees that satisfy the restrictions imposed by Tree Insertion Grammars (TIG). This characteristic can be used to reduce the practical complexity of TAG parsing, applying the standard adjunction operation only in those cases in which the simpler cubic-time TIG adjunction cannot be applied. In this paper, we describe a parsing algorithm managing simultaneous adjunctions in TAG and TIG.
The paper describes an incremental parsing algorithm for natural languages that uses normalized interfaces of modules of proof-nets. This algorithm produces at each step the different possible partial syntactical analyses of the first words of a sentence. Thus, it can analyze texts on the fly leaving partially analyzed sentences.
This paper presents a technique for the representation and the implementation of interaction relations between different domains of linguistic analysis. This solution relies on the localization of the linguistic objects in the context. The relations are then implemented by means of interaction constraints, each domain information being expressed independently.
In this paper, we present a method which may speed up Earley parsers in practice. A first pass called a guiding parser builds an intermediate structure called a guide which is used by a second pass, an Earley parser, called a guided parser whose Predictor phase is slightly modified in such a way that it selects an initial item only if this item is in the guide. This approach is validated by practical experiments preformed on a large test set with an English context-free grammar.
We present a novel approach to supertagging w.r.t. some lexicalized grammar G. It differs from previous approaches in several ways:- These supertaggers rely only on structural information: they do not need any training phase;- These supertaggers do not compute the {``}best{``} supertag for each word, but rather a set of supertags. These sets of supertags do not exclude any supertag that will eventually be used in a valid complete derivation (i.e., we have a recall score of 100{\%});- These supertaggers are in fact true parsers which accept supersets of L(G) that can be more efficiently parsed than the sentences of L(G).
Integration of two stochastic context-free grammars can be useful in two pass approaches used, for example, in speech recognition and understanding. Based on an algorithm proposed by [Nederhof and Satta, 2002] for the non-probabilistic case, left-to-right strategies for the search for the best solution based on CKY and Earley parsers are discussed. The restriction that one of the two grammars must be non recursive does not represent a problem in the considered applications.
Visual language editors should provide a user-friendly environment where users are supported in an effective way in the construction of visual sentences. In this paper, we propose an approach for the construction of syntax-directed visual language editors by integrating incremental parsers into freehand editors. The approach combines the LR-based techniques for parsing visual languages with the more general incremental Generalized LR parsing techniques developed for string languages.
Within a grammar formalism that treats syntax analysis as a global optimization problem, methods are investigated to improve parsing performance by recombining the solutions of smaller and easier subproblems. The robust nature of the formalism allows the application of this technique with little change to the original grammar.
In this paper, we present a definition of unification of weighted feature structures designed to deal with constraint relaxation. The application of phrase structure rules in a unification-based Natural Language Processing system is adapted such that inconsistent values do not lead to failure, but are penalised. These penalties are based on the signature and the shape of the feature structures, and thus realise an elegant and general approach to relaxation.
We propose two statistical left-corner parsers and investigate their accuracy at varying speeds. The parser based on a generative probability model achieves state-of-the-art accuracy when sufficient time is available, but when high speed is required the parser based on a discriminative probability model performs better. Neural network probability estimation is used to handle conditioning on both the unbounded parse histories and the unbounded lookahead strings.
The paper introduces PACE {---} a parser comparison and evaluation system for the syntactic processing of natural languages. The analysis is based on context free grammar with contextual extensions (constraints). The system is able to manage very large and extremely ambiguous CF grammars. It is independent of the parsing algorithm used. The tool can solve the contextual constraints on the resulting CF structure, select the best parsing trees according to their probabilities, or combine them. We discuss the advantages and disadvantages of our modular design as well as how efficiently it processes the standard evaluation grammars.
In this paper, we propose a new probabilistic GLR parsing method that can solve the problems of conventional methods. Our proposed Conditional Action Model uses Surface Phrasal Types (SPTs) encoding the functional word sequences of the sub-trees for describing structural characteristics of the partial parse. And, the proposed GLR model outperforms the previous methods by about 6{\textasciitilde}8{\%}.
In this paper, we describe an approach to analysis for spoken language translation that combines phrase-level grammar-based parsing and automatic domain action classification. The job of the analyzer is to transform utterances into a shallow semantic task-oriented interlingua representation. The goal of our hybrid approach is to provide accurate real-time analyses and to improve robustness and portability to new domains and languages.
Parser does the part of speech (POS) identification in a sentence, which is required for Machine Translation (MT). An intelligent parser is a parser, which takes care of semantics along with the POS in a sentence. Use of such intelligent parser will reduce the complexity in semantics during MT apriori.
We show that a well-known algorithm to compute the intersection of a context-fre language and a regular language can be extended to apply to a probabilistic context-free grammar and a probabilistic finite automaton, provided the two probabilistic models are combined through multiplication. The result is a probabilistic context-free grammar that contains joint information about the original grammar and automaton.
This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85{\%} with a very simple grammar.
In this paper an efficient algorithm for dependency parsing is described in which ambiguous dependency structure of a sentence is represented in the form of a graph. The idea of the algorithm is shortly outlined and some issues as to its time complexity are discussed.
We investigate an aspect of the relationship between parsing and corpus-based methods in NLP that has received relatively little attention: coverage augmentation in rule-based parsers. In the specific task of determining grammatical relations (such as subjects and objects) in transcribed spoken language, we show that a combination of rule-based and corpus-based approaches, where a rule-based system is used as the teacher (or an automatic data annotator) to a corpus-based system, outperforms either system in isolation.
We present a new formalism, partially ordered multiset context-free grammars (poms-CFG), along with an Earley-style parsing algorithm. The formalism, which can be thought of as a generalization of context-free grammars with partially ordered right-hand sides, is of interest in its own right, and also as infrastructure for obtaining tighter complexity bounds for more expressive context-free formalisms intended to express free or multiple word-order, such as ID/LP grammars. We reduce ID/LP grammars to poms-grammars, thereby getting finer-grained bounds on the parsing complexity of ID/LP grammars. We argue that in practice, the width of attested ID/LP grammars is small, yielding effectively polynomial time complexity for ID/LP grammar parsing.
Given a probabilistic parsing model and an evaluation metric for scoring the match between parse-trees, e.g., PARSEVAL [Black et al., 1991], this paper addresses the problem of how to select the on average best scoring parse-tree for an input sentence. Common wisdom dictates that it is optimal to select the parse with the highest probability, regardless of the evaluation metric. In contrast, the Maximizing Metrics (MM) method [Goodman, 1998, Stolcke et al., 1997] proposes that an algorithm that optimizes the evaluation metric itself constitutes the optimal choice. We study the MM method within parsing. We observe that the MM does not always hold for tree-bank models, and that optimizing weak metrics is not interesting for semantic processing. Subsequently, we state an alternative proposition: the optimal algorithm must maximize the metric that scores parse-trees according to linguistically relevant features. We present new algorithms that optimize metrics that take into account increasingly more linguistic features, and exhibit experiments in support of our claim.
In this paper, we propose a method for analyzing word-word dependencies using deterministic bottom-up manner using Support Vector machines. We experimented with dependency trees converted from Penn treebank data, and achieved over 90{\%} accuracy of word-word dependency. Though the result is little worse than the most up-to-date phrase structure based parsers, it looks satisfactorily accurate considering that our parser uses no information from phrase structures.
One of the main goals of this paper is to describe a formal procedure linking inﬂectional and derivational processes in Czech and to indicate that they can be, if appropriate tools and resources are used, applied to other Slavonic languages. The tools developed at the NLP Laboratory FI MU, have been used, particularly the morphological analyser ajka and the program I par for processing and maintaining the morphological database. 
In this paper we describe the mapping of Zaliznjak’s (1977) morphological classes into the lexical representation language DATR (Evans and Gazdar 1996). On the basis of the resulting DATR theory a set of fully inflected forms together with their associated morphosyntax can automatically be generated from the electronic version of Zaliznjak’s dictionary (Ilola and Mustajoki 1989). From this data we plan to develop a wide-coverage morphosyntactic lemmatizer and tagger for Russian. 
Word-level morphosyntactic descriptions, such as “Ncmsn” designating a common masculine singular noun in the nominative, have been developed for all Slavic languages, yet there have been few attempts to arrive at a proposal that would be harmonised across the languages. Standardisation adds to the interchange potential of the resources, making it easier to develop multilingual applications or to evaluate language technology tools across several languages. The process of the harmonisation of morphosyntactic categories, esp. for morphologically rich Slavic languages is also interesting from a language-typological perspective. The EU MULTEXT-East project developed corpora, lexica and tools for seven languages, with the focus being on morphosyntactic data, including formal, EAGLES-based speciﬁcations for lexical morphosyntactic descriptions. The speciﬁcations were later extended, so that they currently cover nine languages, ﬁve from the Slavic family: Bulgarian, Croatian, Czech, Serbian and Slovene. The paper presents these morphosyntactic speciﬁcations, giving their background and structure, including the encoding of the tables as TEI feature structures. The ﬁve Slavic language speciﬁcations are discussed in more depth.  
The article notes certain weaknesses of current efforts aiming at the standardization of POS tagsets for morphologically rich languages and argues that, in order to achieve clear mappings between tagsets, it is necessary to have clear and formal rules of delimiting POSs and grammatical categories within any given tagset. An attempt at constructing such a tagset for Polish is presented. 
The paper presents the work being done so far on the building of the Croatian Morphological Lexicon (CML). It has been collected since 2002 in the Institute of Linguistics, Faculty of Philosophy, University of Zagreb. The CML is planned to have two sub-lexicons: derivative/compositional and inflectional, both produced by a generator. The result of generation is lexicon as two distinct lists of generated combinations of morphemes and complete word-forms both with additional data that can be used in further processing. The inflectional component is presented more in detail in the second part of the paper. At the end, the several possible applications of CML are discussed. Introduction Our aim was to make a model of Croatian morphological system in the form of lexicon stored in a database. The lexicon, named Croatian Morphological Lexicon (CML), would include all combinations of morphemes according to morphotactic rules and generated by two morphological generators. 
This paper presents an approach to the unsupervised learning of parts of speech which uses both morphological and syntactic information. While the model is more complex than those which have been employed for unsupervised learning of POS tags in English, which use only syntactic information, the variety of languages in the world requires that we consider morphology as well. In many languages, morphology provides better clues to a word’s category than word order. We present the computational model for POS learning, and present results for applying it to Bulgarian, a Slavic language with relatively free word order and rich morphology. 
The technology of ﬁnite-state transducers is implemented to recognize, lemmatize and tag composite tenses in Serbian in a way that connects the auxiliary and main verb. The suggested approach uses a morphological electronic dictionary of simple words and appropriate local grammars. 
We present a case study of a complex stochastic disambiguator of alternatives of morphosyntactic tags which allows for using incomplete disambiguation, shorthand tag notation, external tagset deﬁnition and external deﬁnition of multivalued context features. The tagger bases on Naive Bayes modeling and allows for using almost as general context features as in classical trigram taggers as well as more speciﬁc ones. Its preliminary results for Polish still do not meet our expectations. Possible sources of the tagger’s failures can be: inhomogeneity of the training corpus in preparation, lack of the automatic search of probability models, too general conditional independence assumptions in deﬁning the class of interpretable models. Automatization of high-quality morphosyntactic tagging for strongly inﬂective languages, such as Slavic languages, seems to be a much harder task than so called part-of-speech (POS) tagging for weaker inﬂective languages. An important factor increasing the complexity is the very design of the tagset. Usually, the tags assigned to wordlike segments in the former task are long lists of subsequent attribute values, e.g. POS, number, case, gender, person etc. (Hajicˇ and Hladká, 1998; Wolin´ski and Przepiórkowski, 2001), so they provide much more information than almost atomic labels used for POS tagging (Manning and  Schütze, 1999). To make the matter harder, many formal descriptions become easier when the tag attribute values are allowed to form RSRL-like type hierarchies (Przepiórkowski et al., 2002). Allowing the values to be partially ambiguous depending on a context raises questions what is the accurate level of disambiguation (Wolin´ski and Przepiórkowski, 2001) and how to model it probabilistically in terms of random variables taking disjunctive values (Brew, 1995). Working for a project aiming at building a large morphosyntactically tagged corpus of written Polish (information site http://dach.ipipan. waw.pl/CORPUS/), we have tried implementing a highly reconﬁgurable stochastic tagger addressing some of these problems. The main features of our software are as follows: • The tagger is a contextual disambiguator: It only prunes the lists of tags admissible for successive word-segments, given by a separate morphological analyzer. Superiority of this approach over simulating a stochastic morphological analyzer by a tagger has been discussed in Hajicˇ (2000). • The tags processed by the tagger have form of short human-readable lists of attribute values. Especially, non-applicable attributes are omitted and multiple atomic values can be given for the same attribute. • The tagger’s internal representation of disambiguation alternatives (tagger’s decisions) is different than the list of all admissible atomic tags. In this approach, some kind  of contextually-dependent incompleteness of disambiguation could be learned. • Special conﬁguration ﬁles inform the tagger which tag attributes are to be disambiguated and what multivalued context attribute are relevant for that. The tagger’s inference uses a series of Naive-Bayes-like assumptions founded on joint distributions of disambiguation decisions for one tag attribute and values of one context attribute. • The values of the context attributes are automatically instantiated and smoothed strings whose templates are given in a hand-made conﬁguration ﬁle. This approach allows to combine strengths of generality of context attributes as in n-gram models (Brants, 2000; Megyesi, 2001) with their speciﬁcity as for binary features in MaxEnt taggers (Ratnaparkhi, 1996; Hajicˇ and Hladká, 1998). Possibility of using alternative ﬁles deﬁning the templates of context attributes eases constructive critiques of the particular deﬁnition of them. • The tagger processes XML-formatted texts where input and output ﬁles have the same structure. Especially, it can be run on its own output to disambiguate some attributes in cascade rather than simultaneously. • The tagger can be used for any other language supplied with morphological analyzer, training data and tagset-dependent conﬁguration ﬁles. In the following bulk of paper, we shall present the features of our tagger in more detail, we shall discuss its preliminary results for our Polish tagging project, as well as, we shall share remarks on possible extensions/improvements of the software behavior. Our general feeling is that the tagger as we have it implemented and conﬁgured now for Polish is not a very practical program: It works very slow and it makes much more mistakes than state-ofthe-art taggers. In fact, the tagger gives its users so much freedom of manual conﬁguration and feedback information in the error reports that they get  lost. We hope that much accuracy can be earned when some automatic search for the optimal conﬁguration ﬁles is implemented. On the hand, we are still afraid if we have not underﬁtted with various conditional independence assumptions, which restrain the tagger from seeing sequences of very speciﬁc tags as something systematic: Especially, each tag attribute is disambiguated probabilistically independently and the program does not allow for treating tags as atomic entities in probability modeling. This cannot be overcome without a major change in the program and, worse, in the already complex structure of its conﬁguration ﬁles. Despite all these drawbacks, we present some study of how one can think creatively about tagging and how one cannot ﬁnd a quick break-even when trying to implement too many good guidelines in one piece. We report on lots of apparently technical details, hoping that their exposition can help see the tagger as something more than a black box and identify the sources of its errors. 
This paper demonstrates the modelling of morphological knowledge in Bulgarian and applications of the created data sets in an integrated framework for production and manipulation of language resources. The production scenario is exempliﬁed by the Bulgarian verb as the morphologically richest and most problematic part-of-speech category. The deﬁnition of the set of morphosyntactic speciﬁcations for verbs in the lexicon is described. The application of the tagset in the automatic morphological analysis of text corpora is accounted for. A Type Model of Bulgarian verbs handling the attachment of short pronominal elements to verbs, is presented. 
In this paper we discuss morpho-syntactic clues that can be used to facilitate terminological processing in Serbian. A method (called SRCE) for automatic extraction of multiword terms is presented. The approach incorporates a set of generic morpho-syntactic filters for recognition of term candidates, a method for conflation of morphological variants and a module for foreign word recognition. Morpho-syntactic filters describe general term formation patterns, and are implemented as generic regular expressions. The inner structure together with the agreements within term candidates are used as clues to discover the boundaries of nested terms. The results of the terminological processing of a textbook corpus in the domains of mathematics and computer science are presented. 
This paper deals with development and application of Russian morphology software and resources. The approach is particularly dependent on advanced morphological analysis. The paper presents the structure, formats and content of Russian dictionaries and corpora. Relevant aspects of the UML data models, XML format and related technologies are surveyed. We introducee the system based on Java and Oracle 9i DBMS. 
Although there is an increasing shift towards evaluating Natural Language Generation (NLG) systems, there are still many NLG-speciﬁc open issues that hinder effective comparative and quantitative evaluation in this ﬁeld. The paper starts off by describing a task-based, i.e., black-box evaluation of a hypertext NLG system. Then we examine the problem of glass-box, i.e., module speciﬁc, evaluation in language generation, with focus on evaluating machine learning methods for text planning. 
This paper presents a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system: PEACE (French acronym for Paradigme d’Evaluation Automatique de la Compre´hension hors et En-contexte). This paradigm will be the basis of the French Technolangue MEDIA project, in which dialog systems from various academic and industrial sites will be tested in an evaluation campaign coordinated by ELRA/ELDA (over the next two years). Despite previous efforts such as EAGLES, DISC, AUPELF ARCB2 or the ongoing American DARPA COMMUNICATOR project, the spoken dialog community still lacks common reference tasks and widely agreed upon methods for comparing and diagnosing systems and techniques. Automatic solutions are nowadays being sought both to make possible the comparison of different approaches by means of reliable indicators with generic evaluation methodologies and also to reduce system development costs. However achieving independence from both the dialog system and the task performed seems to be more and more a utopia. Most of the evaluations have up to now either tackled the system as a whole, or based the measurements on dialog-context-free information. The  PEACE proposal aims at bypassing some of these shortcomings by extracting, from real dialog corpora, test sets that synthesize contextual information. 
We present new statistical methods for evaluating information extraction systems. The methods were developed to evaluate a system used by political scientists to extract event information from news leads about international politics. The nature of this data presents two problems for evaluators: 1) the frequency distribution of event types in international event data is strongly skewed, so a random sample of newsleads will typically fail to contain any low frequency events. 2) Manual information extraction necessary to create evaluation sets is costly, and most effort is wasted coding high frequency categories . We present an evaluation scheme that overcomes these problems with considerably less manual effort than traditional methods, and also allows us to interpret an information extraction system as an estimator (in the statistical sense) and to estimate its bias. 
Accurate evaluation of machine translation (MT) is an open problem. A brief survey of the current approach to tackle this problem is presented and a new proposal is introduced. This proposal attempts to measure the percentage of words, which should be modified at the output of an automatic translator in order to obtain a correct translation. To show the feasibility of the method we have assessed the most important SpanishCatalan translators in comparing the results obtained by the various methods. 
In this paper we attempt to apply the IBM algorithm, BLEU, to the output of four different summarizers in order to perform an intrinsic evaluation of their output. The objective of this experiment is to explore whether a metric, originally developed for the evaluation of machine translation output, could be used for assessing another type of output reliably. Changing the type of text to be evaluated by BLEU into automatically generated extracts and setting the conditions and parameters of the evaluation experiment according to the idiosyncrasies of the task, we put the feasibility of porting BLEU in different Natural Language Processing research areas under test. Furthermore, some important conclusions relevant to the resources needed for evaluating summaries have come up as a side-effect of running the whole experiment.  translation of the original document, in the target language. In Summarization, the generated text should be an informative, reduced version of the original document (single-document summary), or sets of documents (multi-document summary) in the form of an abstract, or an extract. Abstracts present an overview of the main points expressed in the original document, while extracts consist of a number of informative sentences taken directly from the source document. The fact that, by their very nature, automatically generated extracts carry the single sentence qualities of the source documents1, may lead one to the conclusion that evaluating this type of text is trivial, as compared to the evaluation of abstracts or even machine translation, since in the latter, one needs to be able to evaluate the content of the generated translation in terms of grammaticality, semantic equivalence to the source document and other quality characteristics (Hovy et al., 2002). Though the evaluation of generated extracts is not as demanding as the evaluation of Machine Translation, it does have two critical idiosyncratic aspects that render the evaluation task difﬁcult:  
A wide range of parser and/or grammar evaluation methods have been reported in the literature. However, in most cases these evaluations take the parsers independently (intrinsic evaluations), and only in a few cases has the effect of different parsers in real applications been measured (extrinsic evaluations). This paper compares two evaluations of the Link Grammar parser and the Conexor Functional Dependency Grammar parser. The parsing systems, despite both being dependency-based, return different types of dependencies, making a direct comparison impossible. In the intrinsic evaluation, the accuracy of the parsers is compared independently by converting the dependencies into grammatical relations and using the methodology of Carroll et al. (1998) for parser comparison. In the extrinsic evaluation, the parsers’ impact in a practical application is compared within the context of answer extraction. The differences in the results are signiﬁcant. 
An external lexicon quality measure called the L-measure is derived from the F-measure (Rijsbergen, 1979; Larsen and Aone, 1999). The typically small sample sizes available for minority languages and the evaluation of Semitic language lexicons are two main factors considered. Large-scale evaluation results for the Maltilex Corpus are presented (Rosner et al., 1999). 
This paper attacks one part of the question "Are evaluation methods, metrics and resources reusable" by arguing that a set of ISO standards developed for the evaluation of software in general are as applicable to natural language processing software as to any other. Main features of the ISO proposals are presented, and a number of applications where they have been applied are mentioned, although not discussed in any detail. Acknowledgements The work recorded here is far from being all my own. I would like first to record my thanks to Nigel Bevan, technical editor of the ISO standards discussed for much interesting and enlightening discussion. Then many thanks must go to all my colleagues in the EAGLES and ISLE projects, especially Sandra Manzi and Andrei Popescu-Belis. Finally, I must thank all those whose work on applying the standards reported here provoked reflection and helped to convince me of the value of the approach: Marc Blasband, Maria Canelli, Dominique Estival, Daniele Grasso, Véronique Sauron, Marianne Starlander and Nancy Underwood. 
The next generation internet applications will feature not only the ability to understand spoken and written natural language text, (pen) gestures and body postures, they will also and importantly be able to engage with the user in a natural dialogue about the application. In this paper we will describe the design of a multimodal dialogue and action management module, part of the COMIC demonstrator, which is aimed at these next generation applications. The design uses well understood structures like stacks and augmented transition networks in a novel way to obtain the ﬂexibility needed for mixed-initiative dialogue. We also show how this is applied to the application of the COMIC demonstrator - bathroom design. 
This paper presents a corpus study of bridging deﬁnite descriptions in the french corpus PAROLE. It proposes a typology of bridging relations; describes a system for annotating NPs which allows for a user friendly collection of all relevant information on the bridging deﬁnite descriptions occurring in the corpus and discusses the results of the corpus study1. 
The Spoken Dutch Corpus project (19982003) is aimed at the development of a corpus of 1,000 hours of speech. The corpus is being annotated with various types of transcriptions and annotations and will be distributed together with the speech recordings. In order for users to access the data efficiently and with relative ease, exploitation software is being developed that can handle both sound files and other types of data files. After a brief introduction in which the goals of the project are outlined, the present paper first describes the Spoken Dutch Corpus as it is presently being constructed and then goes on to describe in some detail the exploitation software. The exploitation environment makes it possible to view the data contained in the Spoken Dutch Corpus as one item in a network of other similarly structured corpora. Our outlook on such a corpus universe can be found in the ‘Future Work’ section. 
Keywords: graphics, understanding, discourse, plan-based models Information graphics that appear in newspapers and magazines generally have a message that the viewer is intended to recognize. This paper argues that understanding such information graphics is a discourse-level problem. In particular, it requires assimilating information from multiple knowledge sources to recognize the intended message of the graphic, just as recognizing intention in text does. Moreover, when an article is composed of text and graphics, the intended message of the information graphic (its discourse intention) must be integrated into the discourse structure of the surrounding text and contributes to the overall discourse intention of the article. This paper describes how we extend plan-based techniques that have been used for understanding traditional discourse to the understanding of information graphics. This work is part of a project to develop an interactive natural language system that provides sight-impaired users with access to information graphics. 
In this paper we present a detailed scheme for annotating expressions of opinions, beliefs, emotions, sentiment and speculation (private states) in the news and other discourse. We explore inter-annotator agreement for individual private state expressions, and show that these low-level annotations are useful for producing higher-level subjective sentence annotations. 
This paper describes the results of corpus and experimental investigation into the factors that affect the way clariﬁcation questions in dialogue are interpreted, and the way they are responded to. We present some results from an investigation using the BNC which show some general correlations between clariﬁcation request type, likelihood of answering, answer type and distance between question and answer. We then describe a new experimental technique for integrating manipulations into text-based synchronous dialogue, and give more speciﬁc results concerning the effect of word category and level of grounding on interpretation and response type. 
We describe an information-theoretic argument-interpretation mechanism embedded in an interactive system. Our mechanism receives as input an argument entered through a web interface. It generates candidate interpretations in terms of its underlying knowledge representation – a Bayesian network, and applies the Minimum Message Length principle to select the best candidate. The results of our preliminary evaluations are encouraging, with the system generally producing plausible interpretations of users’ arguments. Keywords: Minimum message length, discourse interpretation, Bayesian networks. 
This paper proposes a general theory of conversational inferences which distinguishes two kinds of inferences: the hard way and the easy way. The theory accounts for a wider range of non-literal utterance meanings than Gricean and relevance theories and is motivated by the types of utterances in which the hearer fails to infer nonliteral meanings. 
We present an overview of a comprehensive formal theory of the interpretation of sentential fragments, which has as components an empirically validated taxonomy, an analysis of the syntax and compositional semantics of fragments, and a formalisation of their contextual interpretation. We also brieﬂy describe an implementation of this theory, and quantify the potential practical use of handling fragments in dialogue systems. 
We realize a telephone-based collaborative natural language dialogue system. Since natural language involves very various expressions, a large number of VoiceXML scripts need to be prepared to handle all possible input patterns. We realize ﬂexible dialogue management for various user utterances by generating VoiceXML scripts dynamically. Moreover, we address appropriate user modeling in order to generate cooperative responses to each user. Speciﬁcally, we set up three dimensions of user models: skill level to the system, knowledge level on the target domain and the degree of hastiness. The models are automatically derived by decision tree learning using real dialogue data collected by the system. Experimental evaluation shows that the cooperative responses adapted to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users. Keywords: spoken dialogue system, user model, VoiceXML, cooperative responses, dialogue strategy 
Chat system has gained popularity as a tool for real-time conversation. However, standard chat systems have problems due to lack of timing information. To tackle this problem, we have built a system which has the following functions: 1) function of making typing state visible; 2) floor holding function at the start of typing. The evaluation results show that the sys-tem with each new function significantly increases the number of turns, which indicates the effectiveness of the new functions for smooth communication. The survey results showed that the system with the function of making typing state visible significantly different from that without them concerning 1) easiness of adjusting the timing of utterances and smoothness of conversations, and 2) easiness of using the system.  to share timing information, which is thought to be necessary for smooth communication. This often makes chat conversations confusing such as ones with a lot of repetitions and corrections. To tackle the problem of lack of timing information, we have implemented a system which has the following functions: 1) function of making typing state visible; 2) floor holding function at the start of typing. To evaluate the effectiveness of the system, the length of utterance and the number of utterances are used as quantitative index for smooth communication. We also conducted questionnaire surveys of users' evaluation of the system from effective-ness of the new functions to easiness of using the system. The rest of the paper is organized as follows. Section two explains the problems of standard chat systems and related studies to tackle them. Section three describes our implemented new system with explanatory examples. Section four shows the effectiveness of our new system by quantitative evaluation results. Section five concludes with some final remarks and our further attempt to improve the system.  
Recently the technology for speech recognition and language processing for spoken dialogue systems has been improved, and speech recognition systems and dialogue systems have been developed to the extent of practical usage. In order to become more practical, not only those fundamental techniques but also the techniques of portability and expansibility should be developed. In our previous research, we demonstrated the portability of the speech recognition module to a developed portal spoken dialogue system. And we constructed a dialogue strategy design tool of dialogue script for controlling the dialogue strategy. In this paper, we report a highly portable interpreter using a commercial electronic dictionary. We apply this to three domains/tasks and conﬁrm the validity of the interpreter for each domain/task. Keywords: spoken dialogue system, robust interpreter, portability, dialogue script 
 2 Using Virtual Advisers in FOCAL  We present the spoken dialogue system designed and implemented for Virtual Advisers in the FOCAL environment. Its architecture is based on: Dialogue Agents using propositional attitudes, a Natural Language Understanding component using typed unification grammar, and a commercial speaker-independent speech recognition system. The current application aims to facilitate the multimedia presentation of military planning information in a semi-immersive environment. 
This paper describes a method for “bootstrapping” a Reinforcement Learningbased dialog manager using a Wizard-ofOz trial. The state space and action set are discovered through the annotation, and an initial policy is generated using a Supervised Learning algorithm. The method is tested and shown to create an initial policy which performs significantly better and with less effort than a handcrafted policy, and can be generated using a small number of dialogs.  When the MDP framework is applied to dialog management, the state space is usually constructed from vector components including information state, dialog history, recognition confidence, database status, etc. In most of the work to date both the state space and action set are hand selected, in part to ensure a limited state space, and to ensure training can proceed using a tractable number of dialogs. However, hand selection becomes impractical as system size increases, and automatic generation/selection of these elements is currently an open problem, closely related to the problem of exponential state space size. 3 A method for bootstrapping RL-based systems  
This paper proposes a new framework for a spoken dialogue system based on dialogue examples between human subjects and the Wizard of OZ (WOZ) system. Using this framework and a model of information retrieval dialogue, a spoken dialogue system for retrieving shop information while driving in a car has been designed. The system refers to the dialogue examples to ﬁnd an example that is suitable for generating a query or a reply. The authors have also constructed a large-scale dialogue database using a WOZ system, which enables efﬁcient collection of dialogue examples.  system action. A system using this technique cannot run effectively, however, without a large volume of example data. Traditionally, though, collecting human-to-human dialogue data and manually providing such supplementary information for each instance of input speech has required considerable labor. In this paper, we address this problem and propose a new technique for constructing an examplebased dialogue system using, as example data, the dialogue performed between a human subject and a pseudo-spoken-dialogue system based on the Wizard of OZ (WOZ) scheme. We also describe a speciﬁc spoken dialogue system for information retrieval that we constructed using this technique. 2 Dialogue Processing Based on Examples  
In this paper we present implications for development of dialogue systems, based on an evaluation of the system BIRDQUEST which combine dialogue interaction with information extraction. A number of issues detected during the evaluation concerning primarily dialogue management, and domain knowledge representation and use are presented and discussed. 
We present evidence for the importance of low-level phenomena in dialogue interaction and use this to motivate a multi-layered approach to dialogue processing. We describe an architecture that separates content-level communicative processes from interaction-level phenomena (such as feedback, grounding, turn-management), and provide details of speciﬁc implementations of a number of such phenomena. 
In this paper we present a contextual extension to ONTOSCORE, a system for scoring sets of concepts on the basis of an ontology. We apply the contextually enhanced system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence. We conducted several annotation experiments and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses (both with and without discourse context). We also showed, that annotators can reliably identify the overall best hypothesis from a given n-best list. While the original ONTOSCORE system correctly assigns the highest score to 84.06% of the corpus, the inclusion of the conceptual context increases the number of correct classiﬁcations to yield 86.76%, given a baseline of 63.91% in both cases. 
This paper reports a tool which assists the user in annotating a video corpus and enables the user to search for a semantic or pragmatic structure in a GDA tagged corpus. An XQL format is allowed for search patterns as well as a plain phrase. This tool is capable of generating a GDA timestamped corpus from a video file manually. It will be publicly available for academic purposes.  GDA1 (Global Document Annotation), which is an XML tag set, adds information on syntax, semantics, and pragmatics to texts (Hashida 1998). The texts with GDA organically corresponding to voice and a video will contribute to the basic research into these technologies and promote the application development. 2 GDA tagged corpus This chapter explains the GDA tag set and a method which relates tagged data with the video image.  
We present a light-weight tool for the annotation of linguistic data on multiple levels. It is based on the simpliﬁcation of annotations to sets of markables having attributes and standing in certain relations to each other. We describe the main features of the tool, emphasizing its simplicity, customizability and versatility. 
We describe a coding scheme for machine translation of spoken taskoriented dialogue. The coding scheme covers two levels of speaker intention − domain independent speech acts and domain dependent domain actions. Our database contains over 14,000 tagged sentences in English, Italian, and German. We argue that domain actions, and not speech acts, are the relevant discourse unit for improving translation quality. We also show that, although domain actions are domain specific, the approach scales up to large domains without an explosion of domain actions and can be coded with high inter-coder reliability across research sites. Furthermore, although the number of domain actions is on the order of ten times the number of speech acts, sparseness is not a problem for the training of classifiers for identifying the domain action. We describe our work on developing high accuracy speech act and domain action classifiers, which is the core of the source language analysis module of our NESPOLE machine translation system. 
 2 Requirements of annotation tools  Annotation of discourse phenomena is a notoriously difﬁcult task which cannot be carried out without the help of annotation tools. In this paper we present a Perspicuous and Adjustable Links Annotator (PALinkA), a tool successfully used in several of our projects. We also brieﬂy describe three types of discourse annotations applied using the tool. 
This paper describes a dialogue management system in which an attempt is made to factor out a declarative theory of context updates in dialogue from a procedural theory of generating and interpreting utterances in dialogue. 
This paper describes a method of multimodal language processing that reﬂects experiences shared by people and robots. Through incremental online optimization in the process of interaction, the user and the robot form mutual beliefs represented by a stochastic model. Based on these mutual beliefs, the robot can interpret even fragmental and ambiguous utterances, and can act and generate utterances appropriate for a given situation. 
The DIPPER architecture is a collection of software agents for prototyping spoken dialogue systems. Implemented on top of the Open Agent Architecture (OAA), it comprises agents for speech input and output, dialogue management, and further supporting agents. We deﬁne a formal syntax and semantics for the DIPPER information state update language. The language is independent of particular programming languages, and incorporates procedural attachments for access to external resources using OAA. 
A key challenge for users and designers of spoken language systems is determining the form of the commands that the system can recognize. Using more than 60 hours of interactions, we quantitatively analyze the acquisition of system vocabulary by novice users. We contrast the longitudinal performance of long-term novice users with both expert system developers and guest users. We ﬁnd that novice users successfully learn the form of system requests, achieving a signiﬁcant decrease in ill-formed utterances. However, the working vocabulary on which novice users converge is signiﬁcantly smaller than that of expert users, and their rate of speech recognition errors remains higher. Finally, we observe that only 50% of each user’s small vocabulary is shared with any other, indicating the importance of the ﬂexibility of a conversational interface that allows users to converge to their own preferred vocabulary. Keywords Spoken Language System; NoviceExpert; Lexical Entrainment 
We present a demonstration of a prototype system aimed at providing support with procedural tasks for astronauts on board the International Space Station. Current functionality includes navigation within the procedure, previewing steps, requesting a list of images or a particular image, recording voice notes and spoken alarms, setting parameters such as audio volume. Dialogue capabilities include handling spoken corrections for an entire dialogue move, reestablishing context in response to a user request, responding to user barge-in, and help on demand. The current system has been partially reimplemented for better efficiency and in response to feedback from astronauts and astronaut training personnel. Added features include visual and spoken step previewing, and spoken correction of dialogue moves. The intention is to introduce the system into astronaut training as a prelude to flight on board the International Space Station. 
This paper presents results of using belief functions to rank the list of candidate information provided in a noisy dialogue input. The information under consideration is the intended task to be performed and the information provided for the completion of the task. As an example, we use the task of information access in a multi-domain dialogue system. Currently, the system contains knowledge of ten different domains. Callers calling in are greeted with an open-ended “How may I help you?” prompt (Thomson and Wisowaty, 1999; Chu-Carroll and Carpenter, 1999; Gorin et al., 1997). After receiving a reply from the caller, we extract word evidences from the recognized utterances. By using transferable belief model (TBM), we in turn determine the task that the caller intends to perform as well as any information provided. 
 2 Effects of emotion in dialogue  Communication behaviour is affected by emotion. Here we discuss how dialogue is affected by participants’ emotion and how expressions of emotion are manifested in its content. Keywords: Dialogue, Emotions, Annotation 
 ertheless, natural dialogue corpora can be used for dialogue systems development, a distilling method can be implemented to simplify the real dialogues (Jönsson, Dahlbäck 2000).  The paper gives an overview of a typology of dialogue acts used for annotating Estonian spoken dialogues. Several problems of the classification and determining of dialogue acts are considered. Our further aim is to develop a dialogue system which can interact with the user in natural language following the norms and rules of human-human communication. 
Combining the principle of Differential Latent Semantic Index (DLSI) (Chen et al., 2001) and the Template Matching Technique (Tokuda and Chen, 2001), we propose a new user queries-based patent document retrieval system by NLP technology. The DLSI method ﬁrst narrows down the search space of a sought-after patent document by content search and the template matching technique then pins down the documents by exploiting the words-based template matching scheme by syntactic search. Compared with the synonymous search scheme by thesaurus dictionaries, the new method results in an improved overall retrieval efﬁciency of patent documents. 
The main area of this paper concerns the neural methods for mapping scientific and technical information (articles, patents) and for assisting a user in carrying out the complex process of analysing large quantities of such information. In the procedure of information analysis, like in the domain of patent analysis, the complexity of the studied topics and the accuracy of the question to be answered may often lead the analyst to partition his reasoning into viewpoints. Most of the classical information analysis tools can only manage an analysis of the studied domain in a global way. The information analysis tool that will be considered in our study is the MultiSOM tool whose core model represents a significant extension of the classical Kohonen SOM neural model. The MultiSOM neural-based tool introduces the concepts of viewpoints and dynamics into the information analysis with its multi-maps displays and its intermap communication process. The dynamic information exchange between maps can be exploited by an analyst in order to perform cooperative deduction between several different analyzes that have been performed on the same data. The paper demonstrates the efficiency of a viewpoint-oriented-analysis as compared to a global analysis in the domain of patents. Both objective and subjective quality criteria are taken into account for quality evaluation. The experimental context of the paper is constituted by a patent database of 1000 patents related to oil engineering. The patents structure and  the patents field semantics are firstly exploited in order to generate different viewpoints corresponding to different areas of interest for the analysts. In the experiment the selected viewpoints correspond to uses, advantages, patentees, and titles subfields of the patents. The indexing vocabulary of each viewpoint is automatically extracted of its related textual contents in the patents through a full text analysis. The resulting vocabulary is then used to rebuild patents descriptions regarding each viewpoint. These descriptions are finally classified through the unsupervised MultiSOM algorithm resulting in as much different maps as viewpoints. A fifth “global viewpoint” which represent the combination of all the specific ones is also considered in order to perform our comparison between a global classification mechanism and a pure viewpoint-oriented classification mechanism. 1. Introduction The digital maps are not only tools of visualization. They also represent an analysis tool. Appropriate display of class points can give the analyst an insight that it is impossible to get from reading tables of output or simple summary statistics. For some tasks, appropriate visualization is the only tool needed to solve a problem or confirm a hypothesis, even though we do not usually think of maps as a kind of analysis, as for patent analysis. There is many ways to create digital maps. The one we consider here is based on Artificial Neural Networks (ANNs). ANNs are a useful class of models consisting of layers of nodes. The power of ANNs is derived from their learning capability defined as a change in the weight matrix  (W), which represents the strength of the links among nodes. Moreover, both their relationships with multivariate data analysis and their non-linear capabilities represent added-values for classing and mapping. The Kohonen self-organizing map (SOM) model is a specific kind of ANN which implements in only one step the tasks of classing and mapping a data set. In the SOM case, the learning is competitive and unsupervised and the approach gives central attention to spatial order in the classing of data. The purpose is to compress information by forming reduced representations of the most relevant features, without loss of information about their interrelationships. The main advantages of the SOM model are its robustness and its very good illustrative power. Conversely, the fact that original model he his only able to deal with one classification of the data at a time might be considered as a serious bottleneck for exploiting it for fine mining tasks. In this article we shall be dealing with an innovation that was firstly introduced for the information retrieval purposes [13]. It has also been successfully tested for multimedia mining and browsing tasks, exploiting both the multi-map concept and the synergy between images and text on the same maps [14]. It is the multi-map extension of the Kohonen SOM algorithm. This will be from now signified by the name of MultiSOM. As we shall notice, the MultiSOM introduces the concepts of viewpoints and dynamics into the information analysis concept with its multi-map displays and its inter-map communication process. The dynamic information exchange between maps can be exploited by an analyst in order to perform cooperative deduction between several different analyzes that have been performed on the same data. The principal intent of this article is to propose the MultiSOM model as an ANN implementation of the information analysis concept. We will mainly focuses on the study of the contribution of the viewpoint's oriented data analysis proposed by the MultiSOM model as compared to the global analysis proposed by the other models. An attempt will be made to define a protocol and to design a platform for this comparison. As soon as the MultiSOM model can be used either in a global way or in a viewpoint-  oriented way, it will be used as the reference model for our comparison. The section 2 of the article presents the Kohonen self-organizing maps (SOM) and their main applications in mapping of science and technology. Sections 3 deals with MultiSOM, the multi-map innovation of the SOM algorithm. The context of the experiment on the oil engineering patents and the preprocessing of these latter will be described in the section 4. The Section 5 describes the protocol of comparison which has been set up along with its results. The conclusions are finally exposed. 2. The self-organizing map (SOM) The basic principle of the SOM is that our knowledge organization at higher levels is created during learning by algorithms that promote selforganization in an spatial order (see [5],[6],[7],[8],[9],[10],[11],[12],[28]). Thus, the architecture form of the SOM network is based on the understanding that the representation of data features might assume the form of a self-organizing feature map that is geometrically organized as a grid or lattice. In the pure form, the SOM defines an "elastic net" of points (parameter, reference, or codebook vectors) that are fitted to the input data space to approximate its density function in an ordered way. The algorithm takes thus a set of Ndimensional objects as input and maps them onto nodes of a two-dimensional grid, resulting in an orderly feature map [9]. A layer of two-dimensional array of competitive output nodes is used to form the feature map. The lattice type of array can be defined to be square, rectangular, hexagonal, or even irregular. Every input is connected to every output node via a variable connection weight. It is the self-organizing property. The SOM belongs to the category of the unsupervised competitive learning networks [4],[11],[13]. It is called competitive learning because there is a set of nodes that compete with one another to become active. To this category belongs also the adaptive resonance theory (ART) model of Grossberg and Carpenter, as well as the self-organizing maps discussed in this paper. In the SOM, the competitive learning means also that a number of nodes is comparing the same input data with their internal parameters, and the node with the best match (say, "winner") is then  tuning itself to that input, in addition the best matching node activates its topographical neighbors in the network to take part in tuning to the same input. More a node is distant from the winning node the learning is weaker. It is also called unsupervised learning because no information concerning the correct classes is provided to the network during its training. Like any unsupervised clustering method, the SOM can be used to find classes in the input data, and to identify an unknown data vector with one of the classes. Moreover, the SOM represents the results of its classing process in an ordered twodimensional space (R2). A mapping from a highdimensional data space Rn onto a two dimensional lattice of nodes is thus defined. Such a mapping can effectively be used to visualize metric ordering relations of input data. As Kohonen [9] says: "The main applications of the SOM are in the visualization of complex data in a two dimensional display, and creation of abstractions like in many classing techniques." The SOM algorithm is presented in details in ([2],[9],[12],[13],[19]). It consists of two basic procedures: (1) selecting a winning node and (2) updating weights of the winning node and its neighboring nodes. This preliminary learning phase is not straightforward process [9]. It necessitates several different learning steps, single map evaluations, and comparisons between a lot of generated maps in order to find at least a reliable map, at most an optimal one [13],[32]. Let x(t) = {x1(t), x2(t),…, xN(t)} be the input vector selected at time t, and Wk(t) = {Wk1(t), Wk2(t),…, WkN(t)} the weights for node k at time t. The smallest of the Euclidean distances ||x(t) – Wk(t)|| can be made to define the winning node s: ||x(t) – Ws(t)|| = min ||x(t) – Wk(t)|| After the winning node s thus selected, the weights of s and the weights of the nodes in a defined neighborhood (for example all nodes within a square or a cycle around the winning node) are adjusted so that similar input patterns are more likely to select this node again. This is achieved through the following computation: Wki(t+1) = Wki(t) + α(t) × h(t) × [Xi(t) – Wki (t)], for 1 ≤ i ≤ N  where α(t) is a gain term (0 ≤ α(t) ≤ 1) that decreases in time and converges to 0, and h(t) is the neighborhood function. Once the SOM algorithm is achieved, the data can be set to the nodes of the map. For each input data vector, the winning node is selected according to the algorithm first step presented above, and the data are affected to this selected node. In the quantitative studies of science, the Kohonen self-organizing maps have been successfully used for mapping scientific journal networks [2], and also author co-citation data [33]. Maps have been also successfully used for several other applications in the general area of data analysis like for classifying meeting output [30], for classing socio-economic data [32] and for documentary database contents mapping and browsing [13],14]. Kaski et al. have implemented a specific adaptation of SOM, named WEBSOM, for the analysis of important document collections [6]. WEBSOM main characteristic is to include strategies for reducing the dimension of the entry data descriptions by using random projection techniques applied on word histograms extracted from the document contents. WEBSOM method has been tested for patents abstract analysis [7]. Nevertheless, as this method only manages such an analysis in a global way, it can only provide the analyst with general overview of the topics covered by the patents along with their interactions. A more exhaustive description of all the SOM applications might be found in [32]. After the map building, the main characteristics of the classes resulting from the topographical classification process have to be highlighted to the analyst in order to provide him an overview (i.e. a global summary) of the analysis results. This task is difficult because the profiles of the obtained classes are mostly complex weighted combination of indexes extracted from the data. We have previously observed that single extraction strategy like the one proposed by [17] could cause shortcomings or mistakes in the interpretation of the database contents. The first set of solutions we proposed for solving this problem, like class labeling and zoning strategies or generalization mechanisms, are presented in [14]. Figure 3 of  section 4 presents a map resulting from these processes. In all the following sections, we will consider that the classification process deals with electronic documents associated with their description in the form of index vectors. Classes will be represented by node vectors or class profile; each component of the vectors being the coordinate of a document index element (keyword). The list of the input data, which are the documents affected to the node, will represent the “class members” profile. The conceptual mean of the classes will be below called a topic. This semantic information is supplied by the classified keywords and documents. 3. The MultiSOM model The communication between self-organizing maps that has been first introduced in the context of an information retrieval model [10], represents a major amelioration of the basic Kohonen SOM model. From a practical point of view, the multimap display introduces in the information analysis the use of viewpoints. Each different viewpoint is achieved in the form of map. Each map is a spatial order in which the information is represented into nodes (classes) and spatial areas (group of classes). The multi-map enables a user to highlight semantic relationships between different topics belonging to different viewpoints. Each map represents a particular viewpoint. Figure 4 of section 4 illustrates it. 3.1 The viewpoint paradigm The viewpoint building principle consists in separating the description space of the documents into different subspaces corresponding to different keyword subsets. The set of V all possible viewpoints issued from the description space D of a document set can be defined as: n U V = {v1, v2, …, vn}, vi ∈ P(D), with vi = D i=1 where each vi represents a viewpoint and P(D) represents the set of the parts of the description space of the documents D; the union of the different viewpoints constitutes the description space of the documents.  The viewpoint subsets issued from V may be overlapping ones. Moreover, they may also fit into the structure of the document when they correspond to different vocabulary subsets associated to different documents subfields, if any. Other viewpoints may be also manually extracted from an overall document description space. At last, the viewpoint model is flexible enough to tolerate document descriptions belonging to different media, as soon as these descriptions can be implemented by description vectors (for ex. an image can be simultaneously described both by a keyword vector and by color histogram vector). The inter-map communication mechanism, which is described hereafter, takes directly benefit of the above described viewpoint model in order to overcome the low quality problem inherent to a global classification approach while conserving a overall view on the interaction between the data. 3.2 Inter-map communication mechanism In MultiSOM, this inter-map communication is based on the use of the data that have been projected onto the maps as intermediary nodes or activity transmitters between maps. The intercommunication process between maps operates in three successive steps. Figure 1 shows graphically the three steps of this intercommunication mechanism. At the step 1, the original activity is directly set up by the user on the node or on the logical areas of a source map through decisions represented by different scalable modalities (full acceptance, moderated acceptance, moderated rejection, full rejection) directly associated to nodes activity levels. This procedure can be interpreted as the user’s choices to highlight (positively or negatively) different topics representing his centers of interest relatively to the viewpoint associated to the source map. The original activity could also be indirectly set up by the projection of a user’s query on the nodes of a source map. The effect of this process will then be to highlight the topics that are more or less related to that query. The activity transmission protocol, which corresponds to the steps 2 and 3 of the inter-map communication mechanism, is extensively described in [24].  To perform in the best conditions, the inter-map communication process obviously necessitates that a significant part of the data should play that roles between the maps. This last condition could be  easily verified if each vector used for the map generation indexes a significant part of the bibliographic database.  Source signal: direct user activation or query matching activation [1]  Source Map  [3]  [2]  Figure 1: Inter-map communication mechanism. This figure represents the main steps of the inter-map communication mechanism. [1] The activity is set up directly by the user or by a query formulation on one or several nodes of one or several source map. [2] The activity is transmitted to the data nodes associated to the activated class nodes of the source map. [3] The activity is transmitted through the data nodes to other maps to which these data are associated. Positive as well as negative activity could be managed in the same process. Note that the data are in this case indexed document.  4. Application In the two preceding sections we have introduced MultiSOM after having previously presented the SOM algorithm. In this section, we shall then use a real example, to make some of the notions more concrete. We argue that visualization into form of a set of maps represents an important added-value for analysis in the technology watching tasks, as well as in science watch, and in knowledge discovery in databases. Our example is a set of 1000 patents about oil engineering technology recorded during the year 1999. 4.1 The analysis phase The role of the MultiSOM application has been firstly planed by the domain expert in order to get answers to such various kinds of questions on the patents that: 1: “Which are the relationships between the patentees?” 2: “Which are the advantages of the different oils?”,  3: “Does a patentee works on a specific engineering technology, for which advantage and for which use?”, 4: “Which is the technology that is used by a given patentee without being used by another one?”, 5: “Which are the main advantages of a specific oil component and do this advantages have been mentioned in all the patents using this component?”. An analysis carried out on all the possible types of question led the expert to define different viewpoints on the patents that could be associated to different closed semantic domains appearing in these questions. One of the main aim of the expert was to be able to use each viewpoints separately in order to get answers to domain closed questions (like questions 1,2) while maintaining the possibility of a multi-viewpoint communication in order to get answers to multi-domain questions (like questions 3,4,5) that might also contain negation (like question 4). The specific viewpoints which have  been highlighted by the expert from the set of possible questions are: 1: Patentees, 2: Title (often contains information on the specific components used in the patent), 3: Use, 4: Advantages. A fifth “global viewpoint” which represent the combination of all the specific ones is also considered in order to perform our comparison between a global classification mechanism, of the WEBSOM type, and a pure viewpoint-oriented classification mechanism, of the MultiSOM type. 4.2 The technical realization The role of this phase consists in mapping the four specific viewpoints highlighted by the domain expert in the preceding phase in four different maps. A preliminary task consists in obtaining the index set (i.e. the vocabulary set) associated to each viewpoint from the full text of the patents. This task has been itself divided into three elementary steps. At the step 1, the structure of the patent abstracts is parsed in order to extract the subfields corresponding to the Use and to the Advantages viewpoints1. At the step 2, the rough index set of each subfield is constructed by the use of a basic computer-based indexing tool [4]. This tool extracts terms and noun phrases from the subfield content according to a normalized terminology and its syntactical variations. It eliminates as well usual language templates. At the step 3, the normalization of the rough index set associated to each viewpoint is performed by the domain expert in order to obtain the final index sets. The normalization of the Title, Use and Advantages subfields consists in choosing a single representative among the terms or noun phrases which represent the same concept (for ex., “oil fabrication” and “oil engineering” noun phrases will be both assimilated to the single “oil engineering” noun phrase). The normalization of the Patentees viewpoint is operated in the same way considering that the same firm can appear with different names in the set of published patents. 
We describe the overview of patent retrieval task at NTCIR-3. The main task was the technical survey task, where participants tried to retrieve relevant patents to news articles. In this paper, we introduce the task design, the patent collections, the characteristics of the submitted systems, and the results overview. We also arranged the freestyled task, where participants could try anything they want as far as the patent collections were used. We describe the brief summaries of the proposals submitted to the free-styled task. 
Pseudo relevance feedback is empirically known as a useful method for enhancing retrieval performance. For example, we can apply the Rocchio method, which is well-known relevance feedback method, to the results of an initial search by assuming that the top-ranked documents are relevant. In this paper, for searching the NTCIR-3 patent test collection through pseudo feedback, we employ two relevance feedback mechanism; (1) the Rocchio method, and (2) a new method that is based on Taylor formula of linear search functions. The test collection consists of near 700,000 records including full text of Japanese patent materials. Unfortunately, effectiveness of our pseudo feedback methods was not empirically observed at all in the experiment. 
In cross-database retrieval, the domain of queries di ers from that of the retrieval target in the distribution of term occurrences. This causes incorrect term weighting in the retrieval system which assigns to each term a retrieval weight based on the distribution of term occurrences. To resolve the problem, we propose \term distillation", a framework for query term selection in cross-database retrieval. The experiments using the NTCIR-3 patent retrieval test collection demonstrate that term distillation is e ective for cross-database retrieval. 
A corpus -based diachronic analysis of patent documents, based mainly on the morphologically productive use of certain terms can help in tracking the evolution of key developments in a rapidly e volving specialist field. The patent texts were o btained from the US Patent & Trade Marks Office’s on-line service and the terms were extracted automatically from the texts. The chosen specialist field was that of fast-switching devices and systems. The method presented draws from liter ature on biblio - and sciento -metrics, info rmation extraction, corpus linguistics, and on aspects of English morphology. This interdisciplinary fram ework shows that the evolution of word -formation closely shadows the devel opments in a field of technology. Introduction A patent document is written to pe rsuade a techno legal authority that the patentee should be allowed to manufacture, sell, or deal in an article to the exclusion of other persons. The article is typ ically based on an invention that the patentee(s) claim has been theirs. The term article is important in that it refers to a tangible object and its u sage is to emphasise that ideas, intangibles essentially, ca nnot be patented. Patent documents are the repos itory of how technology advances and, more importantly, show how language supports the change. The techno-legal authority requires the patent document to follow a template. This template is divided broadly into two parts: first, legal te m-  plates comprising pate ntee’s details, juri sdictional scope, and related item; second, technical templates divided into a summary of the patentee’s claims, relation of the article to previously patented articles – the so -called prior art – and the scientific/technical basis of t he claim. The scientific claim is written in a language that is similar to the language of journal p apers. One important task that is slowly emerging is the extent to which the analysis of a patent doc ument can be automated particularly to a ssess the overlap between the claims in the document about the article to be patented with that of related, rel evant and even counter -claims about the article. The related and rel evant claims and counter claims may be found in existing patent documents and may, more in directly, exist in journal papers. A patent document has to make references to all other relevant/related articles that have been patented prior to the invention of the art icle, which is yet to be patented and is the object of the patent document. The ref erences are made primarily by citing the name of the prior art patentees and the titles of their patent documents. A patent doc ument also has other linguistic descriptions of prior art; such d escriptions are reminiscent of citations of journal papers in a journal paper. The overlap of a new patent document with a set of existing patent documents may suggest the impact of extant knowledge in patent documents on emerging knowledge in the new patent document. Such an overlap has been studied by the impact of US semiconductor technology on the rest of the world (Appleyard and Kalsow: 1999): this overlap relies largely on the fr equency of citation of a US patent by the name of its author or the author’s place of work. In computational linguistic (CL) terms thi s exercise relies on proper noun extra ction. The patent document relates to an explicit and exclusive right over an intellectual property. A journal article relates to an implicit and i nclusive  right over an intellectual property. The overlap between the se two forms of claims is crucial not only in ascertaining the rights of the patentee, or the abuse of the rights of others by the pa tentee, but also for monitoring the effectiveness of r esearch based on a specialism as a whole or that of its component gro ups. The effect of one author or a group of authors working in an institution is indirectly mea sured by the so-called impact factor . This fa ctor relates to the frequency of citation of one or more journal papers written by an author or by a group. The calculation of the impact fa ctor relies mainly on computing the frequency of the authors’ name(s) within a corpus of journal articles. Such an impact factor type calculation is used typically in bibl iometrics (Garfield 1995). Again, as in intra -patent impact studies mentioned above, in CL terms this is an exercise in proper noun identification and extraction. The analysis of a patent document, together with the analysis of the related corpora of other patent documents and intellectual property doc uments, should be based on a framework which provides methods and tec hniques for analysing the contents of the doc ument and of the corpora. For us the source of a framework still lies in li nguistic and language studies. Here we are pa rticularly interested in word formation and terminology u sage in highly specialised disc iplines particularly those disciplines that deal with inta ngible articles coupling the word formation and terminology u sage with the citation patterns of proper nouns brings us closer to analysing the contents of a pa tent document and its siblings distributed over co rpora. Information scientists usually use the referen cing data of research documents to analyse know ledge evolution in scientific fields as well as to identify the key authors, institutes , and journals in specific domains, using tools such as publication counts, cit ation analysis, co -citation analysis, and co-term analysis to do so. In recent years, patent documents have gained considerable attention as a valuable resource that can be use d to analyse tec hnology advances using the same tools. Gupta and Pangannaya (2000) have applied bibliometric analysis to carbon nanotube patents to measure the growth of activity of carbon nan otube industries and their links with sc ience. They have also used patents data to study the country -wise  distribution of patenting activity for the USA, J apan, and other countries. Sector -wise performances of industry, academia and government, and the active players of carbon nanotubes were also stu died. They d escribe the nature of inventions taking place in this particular field of technology, and the authors claim to have identified the emerging r esearch directions, and the active companies and research groups involved. Meyer (2001) has used citation anal ysis and co-word analysis of patent documents and sc ientific literature to explore the interrel ationship between nano -science and nano -technology. Meyer investigated patent citation relations at the orga nizational levels along with geographical locations and affiliations of inventors and a uthors. The term co-occurrence is used by Meyer to find the rel ationship between the patent documents and the two scientific liter ature databases SCI and INSPEC. He has noticed that ‘…the terms that occur frequently in the document titles of all databases are related to […] instrume ntalities and/or are located in fields that are generally associated with substantial indu strial research activity’ (2001:177). Meyer has a rgued that ‘Our data suggests that nano -technology and nano-science are essentially separate and he terogeneous, yet interrelated cumulative stru ctures’ (2001:164). The study of word formation through n eologisms within the special language of science and techno logy has led some authors to argue that it is the scientists as technologists who attempt to rationalise our experience of the world around us in written language by using new words or forms or by relexicalising the existing stock (see Ahmad 2000 for relevant references). Some lexicogr aphers (see for example Quirk et al. 1985) have su ggested that neologisms can be formed by two processes: First, the addition or combination of elements such as compounding: Resonant Tunne ling Diodes and Scanning tu nneling microscopy are examples for this type of neologism (compoundin g as a neologism formation is used extensively in science and technology literature); Second, the r eduction of elements into abbreviated forms. The abbreviations FET (Field E ffect Transistor) and MOSFET (Metallic Oxide Semiconductor FET) are examples of this type. Neologisms appear to signal the eme rgence of new concepts or artefacts and the frequency of this new word might indicate the scientific comm u-  nity’s acceptance of this new concept or artefact. Effenberger (1995) has argued that ‘… the faster a subject field is developing, the more novelties are constructed, discovered or created. And these no velties are talked and written about. In o rder to make this technical communication as efficient as possible, provision should be made for avoiding misunderstanding. One crucial point in this process is the vocabulary that is being used’ (1995:131, emphasis added). In this paper we discuss the idiosyncratic la nguage used in patent documents. The language is replete with terms and there are instances within a patent document that suggest that the authors not only use the specialist terms but use a local syntax as well. We look specifically at the structure of the US Patents and suggest how with existing tec hniques used in information extraction and NLP, including term extraction and proper noun identif ication, one can perform fairly complex tasks in patent analysis – some of which are performed by patent experts by hand currently (Section 2). This examination suggests to us a model of develo pment in computer and semi -conductor technology: an incremental model where each subsequent pa tent helps in the development of ever -complex artifacts – starting from devices onto circuits and onto systems. We will look at one of the key i nventions in the field of semiconducto rs physics – the electron tunneling device . These devices co mbine technical elegance, experimental complexity and manufacturing challenge. Due to its strategic i mportance, a number of patents have been o btained by the US government and also by a nu mber of US and Japanese companies (Section 3). Section 4 concludes this paper. The Structure of US PTO Doc uments and a Local Grammar for the Doc uments The USPTO database is a representative sa mple of patent documents. The USPTO has documents r elated to most bra nches of science and technology. It includes information about all US patent doc uments since the first pa tent issued in 1970 to the most recent. The USPTO database a llows the user to search the full text of the patent documents for a certain word or a co mbination of words. It also  provides a field search for specific information such as inventor or assignee. The search can also be conducted for a sp ecific year or range of years. The US Patents are written partly as a legal text and partly as a scientific d ocument. Over the last 50 years or so, it appears that US Patent doc uments have been structured in terms of layout and have a superficial r esemblance to Marvin Minsky’s frame-like knowledge represe ntation schema. The patent document can be divided into three main parts for the present discussion: The first part comprises the biographical details of the inventors (and their employers) together with the title of the invention and a brief free -text abstract, dates when the patent was applied for and when the patent was granted and so on. The free text is essentially a summary of the claims of the pa tentee; The second part contains external refe rences of three sorts: the first sort is the specialist domain of the invention – the subject class indica ting the super-ordinate class and instances; the se cond sort are other cited patents organised as a 4 -tuple: (i) patent number, (ii) date of approval, (iii) first i nventor and (iv) classification number; and, the third sort is a bibliographic reference to public ations that may have contributed to the pa tent; The third part of a current US Patent document co mprises ‘claims’ related to the patent and the d escription of the ‘invention’ (there are diagrams of the inve ntion attached to the document and the diagrams d escribed in the text). Table 1 on the next page shows the template of the current (c. 1980 and a fter) USPTO’s. The ‘claims’ of the patentees are clearly itemised and initialised by the number of the claim; the first claim is the basis of the patent abstract generally. The ‘background to the invention’ is written in an idi osyncratic fashion as well – the invention is first contextualised in a broader group of other inventions to date and then the specific nature of the invention is e xemplified. The broader an d the specific are usually marked by phrases like ‘The (present) invention relates to’ and the specificity is phrased as ‘(more) specif ically.’ or ‘(more) pa rticularly’. These phrases are followed by one or more noun phrases connected with, for example, c onjunctions or qualifiers. The first noun phrase names the article i nvented, for instance, a name of a new device, circuit or a fabr icating or testing pro cess.  FIELD  VALUE  United States Patent Number  NUMBER  First Inventor  PROPER NOUN ET AL.  Date Patent Approved  DATE  Title:  FREE TEXT  Abstract:  FREE TEXT  Inventors:  PROPER NOUNS  Assignee:  PROPER NOUNS  Application No.:  NUMBER  Filed:  DATE  Patent Classification Data:  NUMBER  References Cited [Refe renced By]:  [PATENT NUMBER, DATE, FIRST INVENTOR,  CLASS NO.]  Parent Case Text:  FREE TEXT  CROSS REFERENCE TO RELATED APPLICATION  Claims:  ‘What is claimed is: ‘  CLAIM 1:  FORMULAIC FREE TEXT  CLAIM 2:  FORMULAIC FREE TEXT  Description  BACKGROUND OF THE INVENTION  1. Field of the Inve ntion:  FORMULAIC FREE TEXT  2. Related Background Art:  FORMULAIC FREE TEXT  SUMMARY OF THE INVENTION:  SEMI FORMULAIC FREE TEXT  BRIEF DESCRIPTION OF THE DRAWINGS:  FREE TEXT  DETAILED DESCRIPTION OF THE PREFERRED FREE TEXT.  EMBODIMENTS:  Table 1: A slot -filler template of the US PTO a pproved patent documents.  The NP comprises d eterminers and modal verbs together with (compound) nouns. The first NP is optionally followed by a qualification that restricts or extends the scope of the disco very – the enlargement or restriction is named and another NP is used for the naming and so on. This simple grammar can be verified by exa mining a corpus of patent documents. To illu strate this point we have looked at a recent randomly selected patent on memory devices – a patent filed by Kabushiki Ka isha Toshiba of Japan (or Toshiba for short), and approved by USPTO on 20 th May 2003, on a sem iconductor memory device which uses the emergent notion of memory cells (a memory cell is a tiny area within the memory array tha t actually stores the bit in the form of an electrical charge 1). An analysis of the title and that of the ‘Background of the Invention: Field of I nvention’ fields shows the use of this restricted syntax (Table 2). In much the  same as the ‘claims’ and ‘th e ‘background’, the ‘summary of the invention’ is also phrased in a formulaic manner (see Table 1 for the structure of the patent document). The analysis of the other slots governed by a simpler grammar yields interesting results and suggests that the name s of assignees and the ma nner in which patents are being cited can be easily inter-related (Table 3). Toshiba’s USPTO 6567330 refers to 8 other patents. The details of the referenced patents are in a 4 -tuple, which can be unambiguously interpreted. Each of the refe renced patents refers to about 10 patents in turn. An examination of 82 such patents may help to initiate, perhaps, a discussion of the ‘invention life cycle’ or ‘licen sing potential of a patent’ (Mogee 1997), or even a discussion of ‘micro fo undations of innovation systems’ (Ande rsen 2000).  
Patent corpus processing should be centered around patent claim processing because claims are the most important part in patent speciﬁcations. It is common that claims written in Japanese are described in one sentence with peculiar style and wording and are difﬁcult to understand for ordinary people. The peculiarity is caused by structural complexity of the sentences and many difﬁcult terms used in the description. We have already proposed a framework to represent the structure of patent claims and a method to automatically analyze it. We are currently investigating a method to clarify terms in patent claims and to ﬁnd the explanatory portions from the detailed description part of the patent speciﬁcations. Through both approaches, we believe we can improve readability of patent claims. 
We propose a NLP methodology for analyzing patent claims that combines symbolic grammar formalisms with dataintensive methods while enhancing analysis robustness. The output of our analyzer is a shallow interlingual representation that captures both the structure and content of a claim text. The methodology can be used in any patent-related application, such as machine translation, improving readability of patent claims, information retrieval, extraction, summarization, generation, etc. The methodology should be universal in the sense that it could be applied to any language, other parts of patent documentation and text as such. 
This paper describes the outline of a linguistic annotation framework under development by ISO TC37 SC WG1-1. This international standard provides an architecture for the creation, annotation, and manipulation of linguistic resources and processing software. The goal is to provide maximum flexibility for encoders and annotators, while at the same time enabling interchange and re-use of annotated linguistic resources. We describe here the outline of the standard for the purposes of enabling annotators to begin to explore how their schemes may map into the framework. 
This work presents the data model we adopted for annotating coreference. Our data model includes different levels of annotation, such as part-of-speech, syntax and discourse. We compare our encoding schemes to the abstract XML encoding being proposed as standard. We also present our tool for coreference resolution that handles our data model. 
We propose an ontology-based framework for linguistic annotation of written texts. We argue that linguistic annotation can be actually considered a special case of semantic annotation with regard to an ontology such as pursued within the context of the Semantic Web. Furthermore, we present CREAM, a semantic annotation framework, as well as its concrete implementation OntoMat and show how they can be used for the purpose of linguistic annotation. We demonstrate the value of our framework by applying it to the annotation of anaphoric relations in written texts. 
In this paper we describe the overall model for MILE lexical entries and provide an instantiation of the model in RDF/OWL. This work has been done with an eye toward the goal of creating a web-based registry of lexical data categories and enabling the description of lexical information by establishing relations among them, and/or using predefined objects that may reside at various locations on the web. It is also assumed that using OWL specifications to enhance specifications of the ontology of lexical objects will eventually enable the exploitation of inferencing engines to retrieve and possibly create lexical information on the fly, as suited to particular contexts. As such, the model and RDF instantiation provided here are in line with the goals of ISO TC37 SC4, and should be fully mappable to the proposed pivot. 
Term extraction systems are now an integral part of the compiling of specialized dictionaries and updating of term banks. In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French. Conceptual relationships are deduced from speciﬁc types of term variations, morphological and syntagmatic, and are expressed through lexical functions. The linguistic precision of the conceptual structuring through morphological variations is of 95 %. 
The translation of compound nouns is a major issue in machine translation due to their frequency of occurrence and high productivity. Various shallow methods have been proposed to translate compound nouns, notable amongst which are memory-based machine translation and word-to-word compositional machine translation. This paper describes the results of a feasibility study on the ability of these methods to translate Japanese and English noun-noun compounds. 
This paper describes an implementation to compute positional ngram statistics (i.e. Frequency and Mutual Expectation) based on masks, suffix array-based data structures and multidimensional arrays. Positional ngrams are ordered sequences of words that represent continuous or discontinuous substrings of a corpus. In particular, the positional ngram model has shown successful results for the extraction of discontinuous collocations from large corpora. However, its computation is heavy. For instance, 4.299.742 positional ngrams (n=1..7) can be generated from a 100.000-word size corpus in a seven-word size window context. In comparison, only 700.000 ngrams would be computed for the classical ngram model. It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space. Our solution shows O(h(F) N log N) time complexity where N is the corpus size and h(F) a function of the window context.  cies is the n-multigram model designed by Deligne and Bimbot (1995). All these models have in common the fact that they need to compute continuous string frequencies. This task can be colossal when gigabytes of data need to be processed. Indeed, Yamamoto and Church (2000) show that there exist N(N+1)/2 substrings in a N-size corpus. That is the reason why low order ngrams have been commonly used in Natural Language Processing applications. In the specific field of multiword unit extraction, Dias (2002) has introduced the positional ngram model that has evidenced successful results for the extraction of discontinuous collocations from large corpora. Unlikely previous models, positional ngrams are ordered sequences of tokens that represent continuous or discontinuous substrings of a corpus computed in a (2.F+1)word size window context (F represents the context in terms of words on the right and on the left of any word in the corpus). As a consequence, the number of generated substrings rapidly explodes and reaches astronomic figures. Dias (2002) shows that ∆ (Equation 1) positional ngrams can be computed for an N-size corpus in a (2.F+1)-size window context.  
We present a new approach to extracting keyphrases based on statistical language models. Our approach is to use pointwise KL-divergence between multiple language models for scoring both phraseness and informativeness, which can be uniﬁed into a single score to rank extracted phrases. 
This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora. While classical hybrid systems manually define local part-ofspeech patterns that lead to the identification of well-known multiword units (mainly compound nouns), our solution automatically identifies relevant syntactical patterns from the corpus. Word statistics are then combined with the endogenously acquired linguistic information in order to extract the most relevant sequences of words. As a result, (1) human intervention is avoided providing total flexibility of use of the system and (2) different multiword units like phrasal verbs, adverbial locutions and prepositional locutions may be identified. The system has been tested on the Brown Corpus leading to encouraging results. 
Automatic extraction of multiword expressions (MWE) presents a tough challenge for the NLP community and corpus linguistics. Although various statistically driven or knowledge-based approaches have been proposed and tested, efficient MWE extraction still remains an unsolved issue. In this paper, we present our research work in which we tested approaching the MWE issue using a semantic field annotator. We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts. The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach. In our evaluation, this approach extracted a total of 4,195 MWE candidates, of which, after manual checking, 3,792 were accepted as valid MWEs, producing a precision of 90.39% and an estimated recall of 39.38%. Of the accepted MWEs, 68.22% or 2,587 are low frequency terms, occurring only once or twice in the corpus. These results show that our approach provides a practical solution to MWE extraction.  
In this paper we investigate the phenomenon of verb-particle constructions, discussing their characteristics and their availability for use with NLP systems. We concentrate in particular on the coverage provided by some electronic resources. Given the constantly growing number of verb-particle combinations, possible ways of extending the coverage of the available resources are investigated, taking into account regular patterns found in some productive combinations of verbs and particles. We discuss, in particular, the use of Levin’s (1993) classes of verbs as a means to obtain productive verb-particle constructions, and discuss the issues involved in adopting such an approach. 
This paper describes a distributional approach to the semantics of verb-particle constructions (e.g. put up, make off ). We report ﬁrst on a framework for implementing and evaluating such models. We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions. 
The purpose of this study is to construct a semantic analysis method for disambiguating Japanese compound verbs. Japanese speakers produce a rich variety of compound verbs, making it difficult to process them by computer. We construct a method employing 110 disambiguation rules based on the semantic features of the first verb of a compound and syntactic patterns consisting of co-occurrence between verbs and nouns. The disambiguation rules are evaluated by applying them to compound verbs in the dictionary. The obtained accuracy is 87.19% for our rules. This result shows the advantage of our method. 
This paper presents a constructioninspeciﬁc model of multiword expression decomposability based on latent semantic analysis. We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability. We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet. Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet. 
In this paper, we will investigate a cross-linguistic phenomenon referred to as complex prepositions (CPs), which is a frequent type of multiword expressions (MWEs) in many languages. Based on empirical data, we will point out the problems of the traditional treatment of CPs as complex lexical categories, and, thus, propose an analysis using the formal paradigm of the HPSG in the tradition of (Pollard and Sag, 1994). Our objective is to provide an approach to CPs which (1) convincingly explains empirical data, (2) is consistent with the underlying formal framework and does not require any extensions or modiﬁcations of the existing description apparatus, (3) is computationally tractable. 
This paper proposes an unsupervised training approach to resolving overlapping ambiguities in Chinese word segmentation. We present an ensemble of adapted Naïve Bayesian classifiers that can be trained using an unlabelled Chinese text corpus. These classifiers differ in that they use context words within windows of different sizes as features. The performance of our approach is evaluated on a manually annotated test set. Experimental results show that the proposed approach achieves an accuracy of 94.3%, rivaling the rule-based and supervised training methods.  
We present an unsupervised learning strategy for word sense disambiguation (WSD) that exploits multiple linguistic resources including a parallel corpus, a bilingual machine readable dictionary, and a thesaurus. The approach is based on Class Based Sense Definition Model (CBSDM) that generates the glosses and translations for a class of word senses. The model can be applied to resolve sense ambiguity for words in a parallel corpus. That sense tagging procedure, in effect, produces a semantic bilingual concordance, which can be used to train WSD systems for the two languages involved. Experimental results show that CBSDM trained on Longman Dictionary of Contemporary English, English-Chinese Edition (LDOCE E-C) and Longman Lexicon of Contemporary English (LLOCE) is very effectively in turning a Chinese-English parallel corpus into sense tagged data for development of WSD systems. 1. Introduction Word sense disambiguation has been an important research area for over 50 years. WSD is crucial for many applications, including machine translation, information retrieval, part of speech tagging, etc. Ide and Veronis (1998) pointed out the two major problems of WSD: sense tagging and data sparseness. On one hand, tagged data are very difficult to come by, since sense tagging is considerably more difficult than other forms of linguistic annotation. On the other hand, although the data sparseness is a common problem, it is especially severe for WSD. The problems were attacked in various ways. Yarowsky (1992) showed a class-based approach  under which a very large untagged corpus and thesaurus can be used effectively for unsupervised training for noun homograph disambiguation. However, the method does not offer a method that explicitly produces sense tagged data for any given sense inventory. Li and Huang (1999) described a similar unsupervised approach for Chinese text based on a Chinese thesaurus. As noted in Merialdo (1994), even minimal hand tagging improved on the results of unsupervised methods. Yarowsky (1995) showed that the learning strategy of bootstrapping from small tagged data led to results rivaling supervised training methods. Li and Li (2002) extended the approach by using corpora in two languages to bootstrap the learning process. They showed bilingual bootstrapping is even more effective. The bootstrapping approach is limited by lack of a systematic procedure of preparing seed data for any word in a given sense inventory. The approach also suffers from errors propagating from one iteration into the next. Li and Huang Another alternative involves using a parallel corpus as a surrogate for tagged data. Gale, Church and Yarowsky (1992) exploited the so-called one sense per translation constraint for WSD. They reported high precision rates of a WSD system for two-way disambiguation of six English nouns based on their translations in an English-French Parallel corpus. However, when working with a particular sense inventory, there is no obvious way to know whether the one sense per translation constraint holds or how to determine the relevant translations automatically. Diab and Resnik (2002) extended the translation-based learning strategy with a weakened constraint that many instances of a word in a parallel corpus often correspond to lexically varied but semantically consistent translations. They proposed to group those translations into a target set, which can be automatically tagged with correct senses  based on the hypernym hierarchy of WordNet.  Diab and Resnik’s work represents a departure  from previous unsupervised approaches in that no  seed data is needed and explicit tagged data are  produced for a given sense inventory (WordNet in  their case). The system trained on the tagged data  was shown to be on a par with the best “supervised  training” systems in SENSEVAL-2 competition.  However, Diab and Resnik’s method is only appli-  cable to nominal WordNet senses. Moreover, the  method is seriously hampered by noise and seman-  tic inconsistency in a target set. Worse still, it is  not always possible to rely on the hypernym hier-  archy for tagging a target set. For instance, the  relevant senses of the target set of {serve, tee off}  for the Chinese counterpart  [faqiu] do not  have a common hypernym:  Sense 15 serve – (put the ball into play; as in games like tennis) move – (have a turn; make one’s move in a game) Sense 1 Tee off – (strike a golf ball from a tee at the start of a game) play – (participating in game or sports) compete – (compete for something)  This paper describes a new WSD approach to simultaneously attack the problems of tagging and data sparseness. The approach assumes the availability of a parallel corpus of text written in E (the first language, L1+) and C (the second language, L2), an L1 to L2 bilingual machine readable dictionary M, and a L1 thesaurus T. A so-called Mutually Assured Resolution of Sense Algorithm (MARS) and Class Based Sense Definition Model (CBSDM) are proposed to identify the word senses in I for each word in a semantic class of words L in T. Unlike Diab and Resnik, we do not apply the MARS algorithm directly to target sets to avoid the noisy words therein. The derived classes senses and their relevant glosses in L1 and L2 make it possible to build Class Based Sense Definition and Translation Models (CBSDM and CBSTM), which subsequently can be applied to assign sense tags to words in a parallel corpus. The main idea is to exploit the defining L1 and L2 words in the glosses to resolve the sense ambi-  + This has nothing to do with the direction of translation and is not to be confused with the native and second language distinction made in the literature of Teaching English As a Second Language (TESL) and Computer Assisted Language Learning.  guity. For instance, for the class containing “serve”  and “tee off,” the approach exploits common defin-  ing words, including “ball” and “game” in two  relevant serve-15 and tee off-1 to assign the cor-  rect senses to “serve” and “tee off.” The character  bigram  [faqiu] in an English-Chinese  MRD:  serve v 10 [I∅; T1] to begin play by striking (the  ball) to the opponent  (LDOCE E-C p.  1300),  would make it possible to align and sense tag “serve” or “tee off” in a parallel corpus such as the bilingual citations in Example 1:  (1C) (1E) drink a capful before teeing off at each hole. (Source: Sinorama, 1999, Nov. Issue, p.15, Who Played the First Stroke?). That effectively attaches semantic information to bilingual citations and turns a parallel corpus into a Bilingual Semantic Concordance (BSC). The BSC enables us to simultaneously attack two critical WSD problems of sense tagging difficulties and data sparseness, thus provides an effective approach to WSD. BSC also embodies a projection of the sense inventory from L1 onto L2, thus creates a new sense inventory and semantic concordance for L2. If I is based on WordNet for English, it is then possible to obtain an L2 WordNet. There are many additional applications of BSC, including bilingual lexicography, cross language information retrieval, and computer assisted language learning. The remainder of the paper is organized as follows: Sections 2 and 3 lay out the approach and describe the MARS and SWAT algorithms. Section 4 describes experiments and evaluation. Section 5 contains discussion and we conclude in Section 6. 2. Class Based Sense Definition Model We will first illustrate our approach with an example. A formal treatment of the approach will follow in Section 2.2. 2.1 An example  To make full use of existing machine readable dictionaries and thesauri, some kind of linkage and integration is necessary (Knight and Luk, 1994). Therefore, we are interested in linking thesaurus classes and MRD senses: Given a thesaurus class S, it is important that the relevant senses for each word w in S is determined in a MRD-based sense inventory I. We will show such linkage is useful for WSD and is feasible, based solely on the words of the glosses in I. For instance, given the following set of word (N060) in Longman Lexicon of Contemporary English (McArthur 1992): L = {difficult, hard, stiff, tough, arduous, awkward}. Although those words are highly ambiguous, the juxtaposition immediately brings to mind the relevant senses. Specifically for the sense inventory of LDOCE E-C, the relevant senses for L are as follows:  D(S) = “easy hard do make understand difficult do understand difficult do difficult do easy demanding effort needing much effort difficult well made use difficult use causing difficulty” If we have the relevant senses, it is a simple matter of counting to estimate P(d). Conversely, with P(d) available to us, we can pick the relevant sense of S in I which is most likely generated by P(d). The problem of learning the model P(d) lend itself nicely to an iterative relaxation method such as the Expectation and Maximization Algorithm (Dempster, Laird, Rubin, 1977). Initially, we assume all senses of S word in I is equally likely and use all the defining words therein to estimate P(d) regardless of whether they are relevant. For LDOCE senses, initial estimate of the relevant glosses is as follows:  D(S) = “easy hard do make understand people unfriendly quarrelling pleased … firm stiff broken pressed bent difficult do understand forceful needing using force body mind …bent painful moving moved … strong weakened suffer uncomfortable conditions cut worn broken …needing effort difficult lacking skill moving body parts body CLUMSY made use difficult use causing difficulty”  Therefore, we have the intended senses, S S = {difficult-1, hard-2, stiff-6, tough-4, arduous-1, awkward-2}. It is reasonable to assume each sense in I is accompanied by a sense definition written in the same language (L1). We use D(S) to denote the glosses of S. Therefore we have D(S) = “not easy; hard to do, make, understand, etc.; difficult to do or understand; difficult to do; difficult to do; not easy; demanding effort; needing much effort; difficult; not well made for use; difficult to use; causing difficulty;” The intuition of bringing out the intended senses of semantically related words can be formalized by Class Based Sense Definition Model (CBSDM), which is a micro language model generating D(S), the glosses of S in I. For simplicity, we assume an unigram language model P(d) that generates the content words d in the glosses of S. Therefore, we have  Table 1. The initial CBSDM for n-word list {difficult,  hard, stiff, tough, arduous, awkward} based on the rele-  vant and irrelevant LDOCE senses, n = 6.  Defining word d Count, k  P(d) = k/n  Difficult  5  0.83  Effort  3  0.50  Understand  2  0.33  Bad  2  0.33  Bent  2  0.33  Body  2  0.33  Broken  2  0.33  Difficulty  2  0.33  Easy  2  0.33  Firm  2  0.33  Hard  2  0.33  Moving  2  0.33  Needing  2  0.33  Water  2  0.33  As evident from Table 1, the initial estimates of P(d) are quite close to the true probability distribution (based on the relevant senses only). The three top ranking defining words “difficult,” “effort,” and “understand” appear in glosses of relevant senses,  and not in irrelevant senses. Admittedly, there are still some noisy, irrelevant words such as “bent” and “broken.” But they do not figure prominently in the model from the start and will fade out graduately with successive iterations of re-estimation. We estimate the probability of a particular sense s being in S by P(D(s)), the probability of its gloss under P(d). For intance, we have  Table 3. Classes Based Sense Translation Model for {difficult-1, hard-2, stiff-6, tough-4, arduous-1, awkward-2} in LDOCE*.  P(hard-1) = P(D(hard-1)) = P(“firm and stiff; which …”), P(hard-2) = P(D(hard-2)) = P(“difficult to do or understand”).  On the other hand, we re-estimate the probability P(d) of a defining word d under CBSDM by how often d appears in a sense s and P(s). P(d) is positively prepositional to the frequency of d in D(s) and to the value of P(s). Under that reestimation scheme, the defining words in relevant senses will figure more prominently in CBSDM, leading to more accurate estimation for probability of s being in S. For instance, in the first round, “difficult” in the gloss of hard-2 will weigh twice more than “firm” in the gloss of irrelevant hard-1, leading to relatively higher unigram probability for “difficult.” That in turn makes hard-2 even more probable than hard-1. See Table 2.  Table 2. First round estimates for P(s), the probability of  sense s in S.  Sense*  Definition  P(s)  hard-1 firm and stiff; which can- 0.2857  not easily be broken  hard-2 difficult to do or under-  0.7143  stand  stiff-1 not easily bent  0.2857  stiff-6 difficult to do  0.7143  * in LDOCE.  ** Assuming P(s) ≈ max P(d ) d∈D ( s )  Often the senses in I are accompanied with glosses written in a second language (L2); exclusively (as in a simple bilingual word list) or additionally (as in LDOCE E-C). Either way, the words in L2 glosses can be incorporated into D(s) and P(d). For instance, the character unigrams and/or overlapping bigrams in the Mandarin glosses of S in LDOCE E-C and their appearance counts and probability are shown in Table 3.  We call the part of CBSDM that are involved with words written in L2, Class Based Sense Translation Model. CBSTM trained on a thesaurus and a bilingual MRD can be exploited to align words and translation counter part as well as to assign word sense in a parallel corpus. For instance, given a pair of aligned sentences in a parallel corpus: (2E) A scholar close to Needham analyses the reasons that he was able to achieve this huge work as being due to a combination of factors that would be hard to find in any other person. (Source: 1990, Dec Issue Page 24, Giving Justice Back to China --Dr. Joseph Needham and the History of Science and Civilisation in China) It is possible to apply CBSTM to obtain the following pair of translation equivalent, ( [nan], “hard”) and, at the same time, determine the intended sense. For instance, we can label the citation with hard-2LDOCE, leading to the following quadruple: (3) (hard, [nan], hard-2 LDOCE , (2C, 2E)) After we have done this for all pairs of word and translation counterpart, we would in effect establish a Bilingual Semantic Concordance (BSC).  2.2 The Model We assume that there is a Class Based Sense Definition Model, which can be viewed as a language model that generates the glosses for a class of senses S. Assume that we are given L, the words of S but not explicitly the intended senses S. In addition, we are given a sense inventory I in the form of an MRD with the regular glosses, which are written in L1 and/or L2. We are concerned with two problems: (1) Unsupervised training of M, CBSDM for S; (2) Determining S by identifying a relevant sense in I, if existing, for each word in L. Those two problems can be solved based on Maximum Likelihood Principle: Finding M and S such that M generates the glosses of S with maximum probability. For that, we utilize the Expectation and Maximization Algorithm to derive M and S through Mutually Assured Resolution of Sense Algorithm (MARS) given below:  Mutual Assured Resolution of Sense Algorithm  Determine the intended sense for each of a set of seman-  tic related words.  Input: (1) Class of words L = {w1 w2 …wn}; (2) Sense inventory I.  Output: (1) Senses S from I for words in L;  (2) CBSTM M from L1 to L2.  1. Initially, we assume that each of the senses wi,j, j =  1, mi in I is equally probable to be in S with prob-  ability  P(wi, j  | i,L)  =  
 This paper proposes a new approach to  segmentation of utterances into sentences  using a new linguistic model based upon  Maximum-entropy-weighted  Bi-  directional N-grams. The usual N-gram  algorithm searches for sentence bounda-  ries in a text from left to right only. Thus  a candidate sentence boundary in the text  is evaluated mainly with respect to its left  context, without fully considering its right  context. Using this approach, utterances  are often divided into incomplete sen-  tences or fragments. In order to make use  of both the right and left contexts of can-  didate sentence boundaries, we propose a  new linguistic modeling approach based  on Maximum-entropy-weighted Bi-  directional N-grams. Experimental results  indicate that the new approach signifi-  cantly outperforms the usual N-gram al-  gorithm for segmenting both Chinese and  English utterances.  
Word extraction is one of the important tasks in text information processing. There are mainly two kinds of statisticbased measures for word extraction: the internal measure and the contextual measure. This paper discusses these two kinds of measures for Chinese word extraction. First, nine widely adopted internal measures are tested and compared on individual basis. Then various schemes of combining these measures are tried so as to improve the performance. Finally, the left/right entropy is integrated to see the effect of contextual measures. Genetic algorithm is explored to automatically adjust the weights of combination and thresholds. Experiments focusing on two-character Chinese word extraction show a promising result: the F-measure of mutual information, the most powerful internal measure, is 57.82%, whereas the best combination scheme of internal measures achieves the F-measure of 59.87%. With the integration of the contextual measure, the word extraction achieves the F-measure of 68.48% at last. 
Statistical methods for extracting Chinese unknown words usually suffer a problem that superfluous character strings with strong statistical associations are extracted as well. To solve this problem, this paper proposes to use a set of general morphological rules to broaden the coverage and on the other hand, the rules are appended with different linguistic and statistical constraints to increase the precision of the representation. To disambiguate rule applications and reduce the complexity of the rule matching, a bottom-up merging algorithm for extraction is proposed, which merges possible morphemes recursively by consulting above the general rules and dynamically decides which rule should be applied first according to the priorities of the rules. Effects of different priority strategies are compared in our experiment, and experimental results show that the performance of proposed method is very promising. 
The length of a constituent (number of syllables in a word or number of words in a phrase), or rhythm, plays an important role in Chinese syntax. This paper systematically surveys the distribution of rhythm in constructions in Chinese from the statistical data acquired from a shallow tree bank. Based on our survey, we then used the rhythm feature in a practical shallow parsing task by using rhythm as a statistical feature to augment a PCFG model. Our results show that using the probabilistic rhythm feature significantly improves the performance of our shallow parser. 
In this paper, we describe an approach to annotate the propositions in the Penn Chinese Treebank. We describe how diathesis alternation patterns can be used to make coarse sense distinctions for Chinese verbs as a necessary step in annotating the predicate-structure of Chinese verbs. We then discuss the representation scheme we use to label the semantic arguments and adjuncts of the predicates. We discuss several complications for this type of annotation and describe our solutions. We then discuss how a lexical database with predicate-argument structure information can be used to ensure consistent annotation. Finally, we discuss possible applications for this resource. 
In the investigation for Chinese named entity (NE) recognition, we are confronted with two principal challenges. One is how to ensure the quality of word segmentation and Part-of-Speech (POS) tagging, because its consequence has an adverse impact on the performance of NE recognition. Another is how to flexibly, reliably and accurately recognize NEs. In order to cope with the challenges, we propose a system architecture which is divided into two phases. In the first phase, we should reduce word segmentation and POS tagging errors leading to the second phase as much as possible. For this purpose, we utilize machine learning techniques to repair such errors. In the second phase, we design Finite State Cascades (FSC) which can be automatically constructed depending on the recognition rule sets as a shallow parser for the recognition of NEs. The advantages of that are reliable, accurate and easy to do maintenance for FSC. Additionally, to recognize special NEs, we work out the corresponding strategies to enhance the correctness of the recognition. The experimental evaluation of the system has shown that the total average recall and precision for six types of NEs are 83% and 85% re-  spectively. Therefore, the system architecture is reasonable and effective.  
Ngram modeling is simple in language modeling and has been widely used in many applications. However, it can only capture the short distance context dependency within an N-word window where the largest practical N for natural language is three. In the meantime, much of context dependency in natural language occurs beyond a three-word window. In order to incorporate this kind of long distance context dependency, this paper proposes a new MI-Ngram modeling approach. The MI-Ngram model consists of two components: an ngram model and an MI model. The ngram model captures the short distance context dependency within an N-word window while the MI model captures the long distance context dependency between the word pairs beyond the N-word window by using the concept of mutual information. It is found that MI-Ngram modeling has much better performance than ngram modeling. Evaluation on the XINHUA new corpus of 29 million words shows that inclusion of the best 1,600,000 word pairs decreases the perplexity of the MI-Trigram model by 20 percent compared with the trigram model. In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model. 
This paper introduces an efficient analyser for the Chinese language, which efficiently and effectively integrates word segmentation, part-of-speech tagging, partial parsing and full parsing. The Chinese efficient analyser is based on a Hidden Markov Model (HMM) and an HMM-based tagger. That is, all the components are based on the same HMM-based tagging engine. One advantage of using the same single engine is that it largely decreases the code size and makes the maintenance easy. Another advantage is that it is easy to optimise the code and thus improve the speed while speed plays a critical important role in many applications. Finally, the performances of all the components can benefit from the optimisation of existing algorithms and/or adoption of better algorithms to a single engine. Experiments show that all the components can achieve state-of-art performances with high efficiency for the Chinese language. 
At present most of corpora are annotated mainly with syntactic knowledge. In this paper, we attempt to build a large corpus and annotate semantic knowledge with dependency grammar. We believe that words are the basic units of semantics, and the structure and meaning of a sentence consist mainly of a series of semantic dependencies between individual words. A 1,000,000-wordscale corpus annotated with semantic dependency has been built. Compared with syntactic knowledge, semantic knowledge is more difficult to annotate, for ambiguity problem is more serious. In the paper, the strategy to improve consistency is addressed, and congruence is defined to measure the consistency of tagged corpus.. Finally, we will compare our corpus with other well-known corpora. 
In our information era, keywords are very useful to information retrieval, text clustering and so on. News is always a domain attracting a large amount of attention. However, the majority of news articles come without keywords, and indexing them manually costs highly. Aiming at news articles’ characteristics and the resources available, this paper introduces a simple procedure to index keywords based on the scoring system. In the process of indexing, we make use of some relatively mature linguistic techniques and tools to filter those meaningless candidate items. Furthermore, according to the hierarchical relations of content words, keywords are not restricted to extracting from text. These methods have improved our system a lot. At last experimental results are given and analyzed, showing that the quality of extracted keywords are satisfying. 
Natural language parsing has to be accurate and quick. Explanation-based Learning (EBL) is a technique to speed-up parsing. The accuracy however often declines with EBL. The paper shows that this accuracy loss is not due to the EBL framework as such, but to deductive parsing. Abductive EBL allows extending the deductive closure of the parser. We present a Chinese parser based on abduction. Experiments show improvements in accuracy and efﬁciency.1 
The Semantic Knowledge-base of Contemporary Chinese (SKCC) is a large scale Chinese semantic resource developed by the Institute of Computational Linguistics of Peking University. It provides a large amount of semantic information such as semantic hierarchy and collocation features for 66,539 Chinese words and their English counterparts. Its POS and semantic classification represent the latest progress in Chinese linguistics and language engineering. The descriptions of semantic attributes are fairly thorough, comprehensive and authoritative. The paper introduces the outline of SKCC, and indicates that it is effective for word sense disambiguation in MT applications and is likely to be important for general Chinese language processing. Key words: Semantic knowledge-base, lexical semantic, computational lexicography, word sense disambiguation （WSD）, Chinese language processing  Peking University has been engaged in research and development of the Semantic Knowledge-base of Contemporary Chinese (SKCC) in the last eight years. This lexicon-building project was a collaboration with the Institute of Computing Technology, Chinese Academy of Sciences during 1994-1998, and resulted in a machine-readable bilingual lexicon suitable for use with Machine Translation applications, which contained a fairly complete characterization of the semantic classification, valence specifications and collocation properties for 49 thousands Chinese words and their English counterparts (Wang Hui, 1998). Since 2001, the further development of SKCC has been co-conducted by ICL and Chinese Department of Peking University. At present, SKCC has made great progress. Not only is the scale extended to 66,539 entries, but also the quality has been immensely improved. The semantic classification in the updated edition of SKCC is the embodiment of the very latest progress in Chinese linguistics and language engineering, while the semantic descriptions are comprehensive and thorough. It can provide rich lexical semantic information for various NLP applications. 2 Outline of SKCC  
The verb-noun sequence in Chinese often creates ambiguities in parsing. These ambiguities can usually be resolved if we know in advance whether the verb and the noun tend to be in the verb-object relation or the modifier-head relation. In this paper, we describe a learning procedure whereby such knowledge can be automatically acquired. Using an existing (imperfect) parser with a chart filter and a tree filter, a large corpus, and the log-likelihood-ratio (LLR) algorithm, we were able to acquire verb-noun pairs which typically occur either in verbobject relations or modifier-head relations. The learned pairs are then used in the parsing process for disambiguation. Evaluation shows that the accuracy of the original parser improves significantly with the use of the automatically acquired knowledge.  (2) 办理 手续 的 费用 banli shouxu de feiyong handle procedure DE expense “the expense of going through the procedure” In (1), the verb-noun sequence “登记 手续” is an example of the modifier-head relation while “办理 手续” in (2) is an example of the verb-object relation. The correct analyses of these two phrases are given in Figure 1 and Figure 2, where “RELCL” stands for “relative clause”: Figure 1. Correct analysis of (1) Figure 2. Correct analysis of (2)  
Single character named entity (SCNE) is a name entity (NE) composed of one Chinese ¡ ¢ £ character, such as “ ” (zhong1, China) and “ ” e2,Russia . SCNE is very common in written Chinese text. However, due to the lack of in-depth research, SCNE is a major source of errors in named entity recognition (NER). This paper formulates the SCNE recognition within the sourcechannel model framework. Our experiments show very encouraging results: an Fscore of 81.01% for single character location name recognition, and an F-score of 68.02% for single character person name recognition. An alternative view of the SCNE recognition problem is to formulate it as a classification task. We construct two classifiers based on maximum entropy model (ME) and vector space model (VSM), respectively. We compare all proposed approaches, showing that the sourcechannel model performs the best in most cases. 
This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this ﬁrst international contest, analyze these results, and make some recommendations for the future. 
Our proposed method is to use a Hidden Markov Model-based word segmenter and a Support Vector Machine-based chunker for Chinese word segmentation. Firstly, input sentences are analyzed by the Hidden Markov Model-based word segmenter. The word segmenter produces n-best word candidates together with some class information and conﬁdence measures. Secondly, the extracted words are broken into character units and each character is annotated with the possible word class and the position in the word, which are then used as the features for the chunker. Finally, the Support Vector Machine-based chunker brings character units together into words so as to determine the word boundaries. 
This paper presents a primarily data-driven Chinese word segmentation system and its performances on the closed track using two corpora at the ﬁrst international Chinese word segmentation bakeoff. The system consists of a new words recognizer, a base segmentation algorithm, and procedures for combining single characters, sufﬁxes, and checking segmentation consistencies.  
Word segmentation is the first step in Chinese information processing, and the performance of the segmenter, therefore, has a direct and great influence on the processing steps that follow. Different segmenters will give different results when handling issues like word boundary. And we will present in this paper that there is no need for an absolute definition of word boundary for all segmenters, and that different results of segmentation shall be acceptable if they can help to reach a correct syntactic analysis in the end. Keyword: automatic Chinese word segmentation, word segmentation evaluation, corpus, natural language processing 1. Introduction On behalf of the Institute of Computational Linguistics, Peking University, we would like to thank ACL-SIGHAN for sponsoring the First International Chinese Word Segmentation Bakeoff, which provides us an opportunity to present our achievement of the past decade. 
In this paper we present a two-stage statistical word segmentation system for Chinese based on word bigram and wordformation models. This system was evaluated on Peking University corpora at the First International Chinese Word Segmentation Bakeoff. We also give results and discussions on this evaluation. 
This paper presents our recent work for participation in the First International Chinese Word Segmentation Bakeoff (ICWSB-1). It is based on a generalpurpose ngram model for word segmentation and a case-based learning approach to disambiguation. This system excels in identifying in-vocabulary (IV) words, achieving a recall of around 96-98%. Here we present our strategies for language model training and disambiguation rule learning, analyze the system’s performance, and discuss areas for further improvement, e.g., out-of-vocabulary (OOV) word discovery. 
This paper presents a Unicode based Chinese word segmentor. It can handle Chinese text in Simplified, Traditional, or mixed mode. The system uses the strategy of divide-and-conquer to handle the recognition of personal names, numbers, time and numerical values, etc in the preprocessing stage. The segmentor further uses tagging information to work on disambiguation. Adopting a modular design approach, different functional parts are separately implemented using different modules and each module tackles one problem at a time providing more flexibility and extensibility. Results show that with added pre-processing modules and accessorial modules, the accuracy of the segmentor is increased and the system is easily adaptive to different applications. 
In this paper, we roughly described the procedures of our segmentation system, including the methods for resolving segmentation ambiguities and identifying unknown words. The CKIP group of Academia Sinica participated in testing on open and closed tracks of Beijing University (PK) and Hong Kong Cityu (HK). The evaluation results show our system performs very well in either HK open track or HK closed track and just acceptable in PK tracks. Some explanations and analysis are presented in this paper. 
Word segmentation in MSR-NLP is an integral part of a sentence analyzer which includes basic segmentation, derivational morphology, named entity recognition, new word identification, word lattice pruning and parsing. The final segmentation is produced from the leaves of parse trees. The output can be customized to meet different segmentation standards through the value combinations of a set of parameters. The system participated in four tracks of the segmentation bakeoff -PK-open, PK-close, CTB-open and CTBclosed – and ranked #1, #2, #2 and #3 respectively in those tracks. Analysis of the results shows that each component of the system contributed to the scores. 
 In this paper we present Chinese word  segmentation algorithms based on the so-  called LMR tagging. Our LMR taggers  are implemented with the Maximum En-  tropy Markov Model and we then use  Transformation-Based Learning to com-  bine the results of the two LMR taggers  that scan the input in opposite direc¢t¡¤io£¥§n¦ s.  Our ©s¨y£¥s§t¦em achieves F-scores of  and  on the Academia Sinica corpus  and the Hong Kong City University corpus  respectively.  
SYSTRAN’s Chinese word segmentation is one important component of its Chinese-English machine translation system. The Chinese word segmentation module uses a rule-based approach, based on a large dictionary and fine-grained linguistic rules. It works on generalpurpose texts from different Chinesespeaking regions, with comparable performance. SYSTRAN participated in the four open tracks in the First International Chinese Word Segmentation Bakeoff. This paper gives a general description of the segmentation module, as well as the results and analysis of its performance in the Bakeoff.  system began. Although the project only lasted for three months, some important changes were made in the segmentation convention, regarding the distinction between words and phrases 2 . Along with new developments of the SYSTRAN MT engine, the segmentation engine has recently been re-implemented. The dictionary and the general approach remain unchanged, but dictionary lookup and rule matching were re-implemented using finite-state technology, and linguistic rules for the segmentation module are now expressed using a context-free-based formalism, improving maintainability. The re-implementation generates multiple segmentation results with associated probabilities. This will allow for disambiguation at a later stage of the MT process, and will widen the possibility of word segmentation for other applications.  
This paper introduces a Chinese word tokenization system through HMM-based chunking. Experiments show that such a system can well deal with the unknown word problem in Chinese word tokenization. 
Paraphrases, which stem from the variety of lexical and grammatical means of expressing meaning available in a language, pose challenges for a sentence generation system. In this paper, we discuss the generation of paraphrases from predicate argument structure using a simple, uniform generation methodology. Central to our approach are lexico-grammatical resources which pair elementary semantic structures with their syntactic realization and a simple but powerful mechanism for combining resources. 
This paper describes our ongoing research project on text simpliﬁcation for congenitally deaf people. Text simpliﬁcation we are aiming at is the task of offering a deaf reader a syntactic and lexical paraphrase of a given text for assisting her/him to understand what it means. In this paper, we discuss the issues we should address to realize text simpliﬁcation and report on the present results in three different aspects of this task: readability assessment, paraphrase representation and post-transfer error detection. 
This paper proposes a new method of ranking near-synonyms ordered by their suitability of nuances in a particular context. Our method distincts near-synonyms by semantic features extracted from their deﬁnition statements in an ordinary dictionary, and ranks them by the types of features and a particular context. Our method is an initial step to achieve a semantic paraphrase system for authoring support. 
We present a Question Answering system for technical domains which makes an intelligent use of paraphrases to increase the likelihood of ﬁnding the answer to the user’s question. The system implements a simple and eﬃcient logic representation of questions and answers that maps paraphrases to the same underlying semantic representation. Further, paraphrases of technical terminology are dealt with by a separate process that detects surface variants. 
We describe a set of paraphrase patterns for questions which we derived from a corpus of questions, and report the result of using them in the automatic recognition of question paraphrases. The aim of our paraphrase patterns is to factor out different syntactic variations of interrogative words, since the interrogative part of a question adds a syntactic superstructure on the sentence part (i.e., the rest of the question), thereby making it difﬁcult for an automatic system to analyze the question. The patterns we derived are rules which map surface syntactic structures to semantic case frames, which serve as the canonical representation of questions. We also describe the process in which we acquired question paraphrases, which we used as the test data. The results obtained by using the patterns in paraphrase recognition were quite promising. 
We describe an ongoing work in information extraction which is seen as a text normalization task. The normalized representation can be used to detect paraphrases in texts. Normalization and paraphrase detection tasks are built on top of a robust analyzer for English and are exclusively achieved using symbolic methods. Both grammar development rules and information extraction rules are expressed within the same formalism and are developed in an integrated way. The experiment we describe in the paper is evaluated and presents encouraging results. 
Our general research aim is to extract the actual intentions of persons when they respond to open-ended questionnaires. These intentions include the desire to make requests, complaints, expressions of resignation and so forth, but here we focus on extracting the intention to make a request. To do so, we first have to judge whether their responses contain the intent to make a request. Therefore, as a first step, we have developed a criterion for judging the existence of request intentions in responses. This criterion, which is based on paraphrasing, is described in detail in this paper. Our assumption is that a response with request intentions can be paraphrased into a typical request expression, e.g., “I would like to ...”, while responses without request are not paraphrasable. The criterion is evaluated in terms of objectivity, reproducibility and effectiveness. Objectivity is demonstrated by showing that machine learning methods can learn the criterion from a set of intention-tagged data, while reproducibility, that the judgments of three annotators are reasonably consistent, and effectiveness, that judgments based not on the criterion but on intuition do not agree. This means the criterion is necessary to achieve reproducibility. These experiments indicate that the criterion can be used to judge the existence of request intentions in responses reliably.  
We present an approach for automatically learning paraphrases from aligned monolingual corpora. Our algorithm works by generalizing the syntactic paths between corresponding anchors in aligned sentence pairs. Compared to previous work, structural paraphrases generated by our algorithm tend to be much longer on average, and are capable of capturing long-distance dependencies. In addition to a standalone evaluation of our paraphrases, we also describe a question answering application currently under development that could immensely beneﬁt from automatically-learned structural paraphrases. 
We are trying to ﬁnd paraphrases from Japanese news articles which can be used for Information Extraction. We focused on the fact that a single event can be reported in more than one article in different ways. However, certain kinds of noun phrases such as names, dates and numbers behave as “anchors” which are unlikely to change across articles. Our key idea is to identify these anchors among comparable articles and extract portions of expressions which share the anchors. This way we can extract expressions which convey the same information. Obtained paraphrases are generalized as templates and stored for future use. In this paper, ﬁrst we describe our basic idea of paraphrase acquisition. Our method is divided into roughly four steps, each of which is explained in turn. Then we illustrate several issues which we encounter in real texts. To solve these problems, we introduce two techniques: coreference resolution and structural restriction of possible portions of expressions. Finally we discuss the experimental results and conclusions. 
Automatically acquiring synonymous words (synonyms) from corpora is a challenging task. For this task, methods that use only one kind of resources are inadequate because of low precision or low recall. To improve the performance of synonym extraction, we propose a method to extract synonyms with multiple resources including a monolingual dictionary, a bilingual corpus, and a large monolingual corpus. This approach uses an ensemble to combine the synonyms extracted by individual extractors which use the three resources. Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms. 
This paper proposes a novel method to extract paraphrases of Japanese noun phrases from a set of documents. The proposed method consists of three steps: (1) retrieving passages using character-based index terms given a noun phrase as an input query, (2) ﬁltering the retrieved passages with syntactic and semantic constraints, and (3) ranking the passages and reformatting them into grammatical forms. Experiments were conducted to evaluate the method by using 53 noun phrases and three years worth of newspaper articles. The accuracy of the method needs to be further improved for fully automatic paraphrasing but the proposed method can extract novel paraphrases which past approaches could not. 
Automatic evaluation of translation quality has proved to be useful when the target language is English. In this paper the evaluation of translation into Japanese is studied. An existing method based on n-gram similarity between translations and reference sentences is difﬁcult to apply to the evaluation of Japanese because of the agglutinativeness and variation of semantically similar expressions in Japanese. The proposed method applies a set of paraphrasing rules to the reference sentences in order to increase the similarity score for the expressions that differ only in their writing styles. Experimental results show the paraphrasing rules improved the correlation between automatic evaluation and human evaluation from 0.80 to 0.93. 
We investigate lexical paraphrasing in the context of two distinct applications: document retrieval and node identiﬁcation. Document retrieval – the ﬁrst step in question answering – retrieves documents that contain answers to user queries. Node identiﬁcation – performed in the context of a Bayesian argumentation system – matches users’ Natural Language sentences to nodes in a Bayesian network. Lexical paraphrases are generated using syntactic, semantic and corpus-based information. Our evaluation shows that lexical paraphrasing improves retrieval performance for both applications. 
 This paper investigates three multilingual  named entity corpora, including named  people, named locations and named  organizations.  Frequency-based  approaches with and without dictionary  are proposed to extract formulation rules  of named entities for individual languages,  and transformation rules for mapping  among languages. We consider the issues  of abbreviation and compound keyword at  a distance. Keywords specify not only the  types of named entities, but also tell out  which parts of a named entity should be  meaning-translated and which part should  be phoneme-transliterated.  An  application of the results on cross  language information retrieval is also  shown.  
We are aiming to acquire named entity (NE) translation knowledge from nonparallel, content-aligned corpora, by utilizing NE extraction techniques. For this research, we are constructing a JapaneseEnglish broadcast news corpus with NE tags. The tags represent not only NE class information but also coreference information within the same monolingual document and between corresponding Japanese-English document pairs. Analysis of about 1,100 annotated article pairs has shown that if NE occurrence information, such as classes, number of occurrence and occurrence order, is given for each language, it may provide a good clue for corresponding NEs across languages. 
This work studies Named Entity Classiﬁcation (NEC) for Catalan without making use of large annotated resources of this language. Two views are explored and compared, namely exploiting solely the Catalan resources, and a direct training of bilingual classiﬁcation models (Spanish and Catalan), given that a large collection of annotated examples is available for Spanish. The empirical results obtained on real data point out that multilingual models clearly outperform monolingual ones, and that the resulting Catalan NEC models are easier to improve by bootstrapping on unlabelled data. 
We introduce a multi-language named-entity recognition system based on HMM. Japanese, Chinese, Korean and English versions have already been implemented. In principle, it can analyze any other language if we have training data of the target language. This system has a common analytical engine and it can handle any language simply by changing the lexical analysis rules and statistical language model. In this paper, we describe the architecture and accuracy of the named-entity system, and report preliminary experiments on automatic bilingual named-entity dictionary construction using the Japanese and English named-entity recognizer. 1. Introduction There is increasing demand for cross-language information retrieval. Due to the development of the World Wide Web, we can access information written in not only our mother language but also foreign languages. One report has English as the dominant language of web pages (76.6 %), followed by Japanese (2.77 %), German (2.28 %), Chinese (1.69 %), French (1.09 %), Spanish (0.81 %), and Korean (0.65 %) [1]. Internet users who are not fluent in English finds this situation far from satisfactory; the many useful information sources in English are not open to them. To implement a multi-language information retrieval system, it is indispensable to develop multi-language text analysis techniques such as morphological analysis and named-entity recognition. They are needed in many natural language processing applications such as machine  translation, information retrieval, and information extraction. We developed a multi-language named-entity recognition system based on HMM. This system is mainly for Japanese, Chinese, Korean and English, but it can handle any other language if we have training data of the target language. This system has a common analytical engine and only the lexical analysis rules and statistical language model need be changed to handle any other language. Previous works on multi-language named-entity recognition are mainly for European languages [2]. Our system is the first one that can handle Asian languages, as far as we know. In the following sections, we first describe the system architecture and language model of our named-entity recognition system. We then describe the evaluation results of our system. Finally, we report preliminary experiments on the automatic construction of a bilingual named-entity dictionary. 2. System Architecture Our goal is to build a practical multi-language named-entity recognition system for multi-language information retrieval. To accomplish our aim, there are several conditions that should be fulfilled. First is to solve the differences between the features of languages. Second is to have a good adaptability to a variety of genres because there are an endless variety of texts on the WWW. Third is to combine high accuracy and processing speed because the users of information retrieval are sensitive to processing speed. To fulfill the first condition, we divided our system architecture into language dependent parts and language independent parts. For the second and third conditions, we used a combination of statistical language model and optimal word sequence search. Details of the language model and word sequence search are discussed in more depth later; we start with an explanation of the system's architecture.  Figure 1 overviews the multi-language named-entity recognition system. We have implemented Japanese (JP), Chinese (CN), Korean (KR) and English (EN) versions, but it can, in principle, treat any other language. There are two language dependent aspects. One involves the character encoding system, and the other involves the language features themselves such as orthography, the kinds of character types, and word segmentation. We adopted a character code converter for the former and a lexical analyzer for the latter. In order to handle language independent aspects, we adopted N-best word sequence search and a statistical language model in the analytical engine. The following sections describe the character code converter, lexical analyzer, and analytical engine. 2.1. Character Code Conversion If computers are to handle multilingual text, it is essential to decide the character set and its encoding. The character set is a collection of characters and encoding is a mapping between numbers and characters. One character set could have several encoding schemes. Hundreds of character sets and attendant encoding schemes are used on a regional basis. Most of them are standards from the countries where the language is spoken, and differ from country to country. Examples include JIS from Japan, GB from China and KSC from Korea; EUC-JP, EUC-CN and EUC-KR are the corresponding encoding schemes [3]. We call these encoding schemes ‘local codes’ in this paper. It is impossible for local code to handle two different character sets at the same time, so Unicode was invented to bring together all the languages of the world [4]. In Unicode, character type is defined as Unicode property through the assignment of a range of code points such as alphanumerics, symbols, kanji (Chinese character), hiragana (Japanese syllabary character), hangul (Korean character) and so on. The proposed lexical analyzer allows us to define arbitrary properties other than those defined by the Unicode standard. The character code converter changes the input text encoding from local code to  Language X Plain text (local code)  Character Code Converter (local code to Unicode)  Lexical Analyzer  Lexical Analysis Rule  Word Candidates  Analytical Engine  N-best Word Sequence Search  Morph Analyzer NE Recognizer  Statistical Language Model (Dictionaries) JP CN KR EN  Character Code Converter (Unicode to local code) NE recognized text (local code) Figure 1. System Overview Unicode and the output from Unicode to local code. That is, the internal code of our system is Unicode (UCS-4). Our system can accept EUC-JP, EUC-CN, EUC-KR and UTF-8 as input-output encoding schemes. In principle, we can use any encoding scheme if the encoding has round-trip conversion mapping between Unicode. We assume that the input encoding is either specified by the user, or automatically detected by using conventional techniques such as [5]. 2.2. Lexical Analyzer The lexical analyzer recognizes words in the input sentence. It also plays an important role in solving the language differences, that is, it generates adequate word candidates for every language. The lexical analyzer uses regular expressions and is controlled by lexical analysis rules that reflect the differences in language features. We assume the following three language features; 1. character type and word length 2. orthography and spacing 3. word candidate generation The features can be set as parameters in the lexical analyzer. We explain these three features in the following sections.  2.2.1 Character Type and Word Length Table 1 shows the varieties of character types in each language. Character types influence the average word length. For example, in Japanese, kanji (Chinese character) words have about 2 characters and katakana (phonetic character used primarily to represent loanwords) words are about 5 characters long such as ‘パスワード (password)’. In Chinese, most kanji words have 2 characters but proper nouns for native Chinese are usually 3 characters, and those representing loanwords are about 4 characters long such as ‘贝克汉姆 (Beckham)’. In Korean, one hangul corresponds to one kanji and one hangul consists of one consonant - one vowel - one consonant, so loanwords written in hangul are about 3 characters long such as ‘인터넷 (internet)’. Character type and word length are related to word candidate generation in section 2.2.3. Table 1. Character Types EN alphabet symbol number JP alphabet symbol number kanji hiragana katakana CN alphabet symbol number kanji KR alphabet symbol number kanji hangul 2.2.2 Orthography and Spacing There is an obvious difference in orthography between each language, that is, European languages put a space between words while Japanese and Chinese do not. In Korean, spaces are used to delimit phrases (called as eojeol in Korean) not words, and space usage depends greatly on the individual. Therefore, another important role of the lexical analyzer is to handle spaces. In Japanese and Chinese, spaces should usually be recognized as tokens, but in English and Korean, spaces must be ignored because it indicates words or phrases. For example, the following analysis results are preferred; I have a pen Æ ‘I/pronoun’ ‘have/verb’ ‘a/article’ ‘pen/noun’ and never must be analyzed as follows; ‘I/pronoun’ ‘ /space’ ‘have/verb’ ‘ /space’  ‘a/article’ ‘ /space’ ‘pen/noun’ There are, however, many compound nouns that include spaces such as ‘New York’, ‘United States’ and so on. In this case, spaces must be recognized as a character in a compound word. In Korean, it is necessary not only to segment one phrase separated by a space like Japanese, but also to recognize compound words including spaces like English. These differences in handling spaces are related to the problem of whether spaces must be included in the statistical language model or not. In Japanese and Chinese, it is rare for spaces to appear in a sentence, so the appearance of a space is an important clue in improving analysis accuracy. In English and Korean, however, they are used so often that they don’t have any important meaning in the contextual sense. The lexical analyzer can treat spaces appropriately. The rules for Japanese and Chinese, always recognize a space as a token, while for those for English and Korean consider spaces only a part of compound words such as ‘New York’. 2.2.3 Word Candidate Generation In our system, the analytical engine can list all dictionary word candidates from the input string by dictionary lookup. However, it is also necessary to generate word candidates for other than dictionary words, i.e. unknown words candidates. We use the lexical analyzer to generate word candidates that are not in the dictionary. It is more difficult to generate word candidates for Asian languages than for European languages, because Asian languages don’t put a space between words as mentioned above. The first step in word candidate generation is to make word candidates from the input string. The simplest way is to list all substrings as word candidates at every point in the sentence. This technique can be used for any language but its disadvantage is that there are so many linguistically meaningless candidates that it takes too long to calculate the probabilities of all combinations of the  candidates in the following analytical process.  A much more effective approach is to limit  word candidates to only those substrings  that are likely to be words.  The character types are often helpful in  word candidate generation. For example, a  cross-linguistic characteristic is that  numbers and symbols are often used for  serial numbers, phone numbers, block  numbers, and so on, and some distinctive  character strings of alphabets and symbols  such  as  ‘http://www…’  and  ‘name@abc.mail.address’  are  URLs,  Email-addresses and so on. This is not  foolproof since the writing styles often differ  from language to language. Furthermore, it  is better to generate such kinds of word  candidates based on the longest match  method because substrings of these  candidates do not usually constitute a word.  In Japanese, a change between character  types often indicates a word boundary. For  example, katakana words are loanwords and  so must be generated based on the longest  match method. In Chinese and Korean,  sentences mainly consist of one character  type, such as kanji or hangul, so the  character types are not as effective for word  recognition as they are in Japanese. However,  changes from kanji or hangul to  alphanumerics and symbols often indicate  word changes.  And word length is also useful to put a  limit on the length of word candidates. It is a  waste of time to make long kanji words  (length is 5 or more characters) in Japanese  unless the substring matched with the  dictionary, because its average length is  about 2 characters. In Korean, although  hanguls (syllabaries) are converted into a  sequence of hangul Jamo (consonant or  vowel) internally in order to facilitate the  morphological analysis, the length of hangul  words are defined in hangul syllabaries.  We designed the lexical analyzer so that it  can correctly treat spaces and word  candidate generation depending on the  character types for each language. Table 2  shows sample lexical analysis rules for  Japanese (JP) and English (EN). For  example, in Japanese, if character type is kanji or hiragana, the lexical analyzer attempts to output word candidates with lengths of 1 to 3. If character type is katakana, alphabet, or number, it generates one candidate based on the longest match method until character type changes. If the input is ‘1500km’, word candidates are ‘1500’ and ‘km’. Subset character strings such as ‘1’, ‘15’, ‘500’, ‘k’ and ‘m’ are never output as candidates. It is possible for a candidate to consist of several character types. Japanese has many words that consist of kanji and hiragana such as ‘離れて(away from)‘. In any language there are many words that consist of numbers and alphabetic characters such as ‘2nd’, or alphabetic characters and symbols such as ‘U.N.’. Furthermore, if we want to treat positional notation and decimal numbers, we may need to change the Unicode properties, that is, we add ‘.’ and ‘,’ to number-property. The character type ‘compound’ in English rule indicates compound words. The lexical analyzer generates a compound word (up to 2 words long) with recognition of the space between them. In Japanese, a space is always recognized as one word, a symbol. Table 3 shows the word candidates output by the lexical analyzer following the rules of Table 2. The Japanese and English inputs are parallel sentences. It is apparent that the efficiency of word candidate generation improves dramatically compared to the case of generating all character strings as  Table 2. Lexical Analysis Rule  Character Type Word Length  kanji  1-3  hiragana  1-3  katakana  until type changes  JP alphabet number  until type changes until type changes  symbol  
Progress in human language technology requires increasing amounts of data and annotation in a growing variety of languages. Research in Named Entity extraction is no exception. Linguistic Data Consortium is creating annotated corpora to support information extraction in English, Chinese, Arabic, and other languages for a variety of US Governmentsponsored programs. This paper covers the scope of annotation and research tasks within these programs, describes some of the challenges of multilingual corpus development for entity extraction, and concludes with a description of the corpora developed to support this research. 
We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications. We demonstrate the application of statistical machine translation techniques to “translate” the phonemic representation of an English name, obtained by using an automatic text-to-speech system, to a sequence of initials and ﬁnals, commonly used subword units of pronunciation for Chinese. We then use another statistical translation model to map the initial/ﬁnal sequence to Chinese characters. We also present an evaluation of this module in retrieval of Mandarin spoken documents from the TDT corpus using English text queries. 
Named Entity Recognition is one of the key techniques in the fields of natural language processing, information retrieval, question answering and so on. Unfortunately, Chinese Named Entity Recognition (NER) is more difficult for the lack of capitalization information and the uncertainty in word segmentation. In this paper, we present a hybrid algorithm which can combine a class-based statistical model with various types of human knowledge very well. In order to avoid data sparseness problem, we employ a back-off model and /TONG YI CI CI LIN , a Chinese thesaurus, to smooth the parameters in the model. The F-measure of person names, location names, and organization names on the newswire test data for the 1999 IEER evaluation in Mandarin is 86.84%, 84.40% and 76.22% respectively. 
The question of whether metonymy carries across languages has always been interesting for language representation and processing. Until now attempts to answer this question have always been based on small-scale analyses. With the advent of EuroWordNet (Vossen 1998), a multilingual thesaurus covering eight languages and organized along the same lines as WordNet (http://www.cogsci.princeton.edu/~wn/) we have a unique opportunity to research this question on a large scale. In this paper we systematically explore sets of concepts comprising possible metonymic relations that have been identified in WordNet. The sets of concepts are evaluated, and a contrastive analysis of their lexicalization patterns in English, Dutch and Spanish is performed. Our investigation gives insight into the cross-linguistic nature of metonymic polysemy and defines a methodology for dynamic extensions of semantic resources.  1. Introduction Viewed traditionally, metonymy is a nonliteral figure of speech in which the name of one thing is substituted for that of another related to it. It has been described as a cognitive process in which one conceptual entity, the vehicle, provides mental access to another conceptual entity (Radden 1999). In its basic form, it establishes a semantic relation between two concepts that are associated with word forms. The semantic shift expressed by the relation may or may not be accompanied by a shift in form. The semantic relation that is captured by metonymy is one of semantic contiguity, in the sense that in many cases there are systematic relations between metonymically related concepts that can be regarded as slots in conceptual frames (cf. Fillmore 1977). For example, in the sentence ‘The colonies revolted against the crown.’ crown is used as a symbol for the monarchy as well as denoting the traditional head ornament worn  by the monarch. As the example above shows, polysemy is a common way in which metonymically related concepts manifest themselves in language. It is to be expected that any systematic semantic relations between concepts expressed by these sense distinctions are lexicalized, i.e. they are explicitly listed in dictionaries and independent of a pragmatic situation. For example, university is on the one hand an institution and on the other a building. The semantic relation between the two senses is ‘is housed in’. Regular polysemy is a subset of metonymy that covers the systematicity of the semantic relations involved. It can be defined as a subset of metonymically related senses of the same word displaying a conventional as opposed to novel type of semantic contiguity relation. This relation holds for related senses of two or more words (Apresjan, 1973), i.e. is a lexicalized pattern, not a nonce formation (a pragmatically defined novel metonymy), and can therefore be  called regular. It is this subtype of metonymy that we concentrate on in this paper. 2. Regular polysemy across languages The question whether regular polysemy is a cross-linguistic phenomenon has until now only been approached by small scale analyses. For instance, Kamei and Wakao (Kamei, 1992) approached the question from the perspective of machine translation and conducted a comparative survey of the acceptability of metonymic expressions in English, Chinese and Japanese consisting of 25 test sentences. The results they report show that in some cases English and Japanese share metonymic patters to the exclusion of Chinese, but that in others English and Chinese team up. (Seto1996) performed a study into the lexicalization of the container-content schema in various languages (Japanese, Korean, Mongolian, Javanese, Turkish, Italian, Germanic and English). This pattern is lexicalized in English by ‘kettle’: 1. A metal pot for stewing or boiling; usually with a lid 2. The quantity a kettle will hold His observation was that the pattern is observable in all languages, and can be considered cross-linguistic. This small study seems to indicate that the regular polysemic pattern extends over language family boundaries to such an extent that it almost seems universal. This could suggest that the pattern is rooted in general human conceptualisation, and reflects an important non-arbitrary semantic relation between concepts or objects in the world. Indeed, if we describe the relation between container and content in terms of Aristotle’s qualia structure (Pustejovsky 1995), we see that it is the function of a container to hold an object or substance (telic role) and that a 4. Methodology  container is normally brought into existence for this purpose. More small-scale studies like the ones described above have been performed, mostly relying on introspection and smallscale dictionary analysis. A limited number of patterns that are valid in more than one language have been identified such as container/content and producer/product (Peters 2000). With the availability of WordNet and EuroWordNet it has become possible to investigate the cross-linguistic nature of metonymy on a large scale. 3. EuroWordNet EuroWordNet (EWN) (Vossen 1997; Peters 1998) is a multilingual thesaurus incorporating wordnets from eight languages: English, Italian, Dutch, German, Spanish, French, Czech, Estonian. The wordnets have been built in various ways. Some of them have been created on the basis of language specific resources and matched onto the original Princeton WordNet (Fellbaum 1998) when the interlingual relations were created. They therefore reflect the language specific lexicalization patterns and semantic organization. Others have been built from the start on the basis of a match between WordNet and bilingual dictionaries. In this case the conceptual structure is less language specific but can be regarded as the conceptual overlap between the structure of the English WordNet and the ontological structure associated with that particular language. EuroWordNet gives us for the first time the opportunity to examine the question of the language independence of regular polysemy in a more systematic and automatic way. The following methodology has been followed:  First, the hierarchy of WordNet1.6 was analysed in order to obtain English candidates for regular polysemic patterns (section 4.1). Then a process we call lexical triangulation was applied to these data within EuroWordNet (section 4.2). The results were then manually evaluated. 4.1 Automatic candidate selection A technique was developed (Peters 2000) for identifying sense combinations in WordNet where the senses involved potentially display a regular polysemic fabric (something made by weaving or felting or knitting or crocheting natural or synthetic fibers)  relation, i.e. where the senses involved are candidates for systematic relatedness. In order to obtain these candidate patterns WordNet (WN) has been automatically analysed by exploiting its hierarchical structure. Wherever there are two or more words with senses in one part of the hierarchy, which also have senses in another part of the hierarchy, then we have a candidate pattern of regular polysemy. The patterns are candidates because there seems to be an observed regularity for two or more words. This follows the definition of (Apresjan 1973) mentioned in the introduction. An example can be found in Figure 1 below.  covering  hypernym combination  (a natural object that covers or envelops)  fleece words whose senses occur under both hypernyms hair tapa wool  Figure 1: words in WordNet covered by the pattern fabric/covering  We have restricted our experiments to cases where the related meanings are of the same syntactic class (nouns). The procedure does not discover all regular polysemy rela tions, because the outcome is heavily dependent on the consistency of the encoding of these regularities in WordNet. 4.2 Lexical triangulation In order to determine whether regular polysemy is indeed a cross-linguistic phenomenon, one needs to compare languages, preferably from different language families. Data will depend heavily on vocabulary coverage in various languages, and until the advent of EuroWordNet no serious lexical data sets were available for analysis. The EuroWordNet database is the most comprehensive multilingual thesaurus to  date. This resource not only provides us with an appropriate amount of lexical information in terms of vocabulary coverage, but also has the additional advantages that its taxonomic building blocks are identical for all languages involved and the language specific concepts are all linked to an interlingua which is based on the full set of the original Princeton WordNet (version 1.5), and is referred to as the interlingual index (ILI). We started with a comparative analysis of Germanic and Romance languages. The main reason for this is that the size of the corresponding wordnets is large enough to yield significant results. For our analysis we used three languages: English, Dutch and Spanish, hence the term for this process: lexical triangulation. Singling out areas where three languagespecific lexicalization patterns converge  enabled us to identify metonymic patterns that supported the hypothesis that certain metonymic relationships have a higher degree of universality. We extracted the sense combinations of Spanish and Dutch words that participate in any of the potential regular polysemic patterns from the initial large set described in section 4.1. In other words, we concentrate here on lexicalization patterns in three different languages: sense  combinations that are lexicalized by one language-specific word in English, Spanish and Dutch. The first step in this process was the reduction of the search space for regular polysemic patterns in EuroWordNet. First we determined the conceptual overlap for nouns between the English, Dutch and Spanish wordnets. Table 1 below shows the number of nouns in the three wordnets involved.  language Number of noun synsets  English Dutch Spanish  66025 28352 24073  Number of corresponding ILI concepts 66025 26779 24087  Table 1: conceptual coverage of English, Dutch and Spanish wordnets  The conceptual overlap between these wordnets is computed simply by determining the intersection of ILI noun concepts covered by each of the wordnets. The total overlap is 17007 ILI concepts. There are 920 English polysemous nouns with two senses or more within synsets linked to this set of ILI concepts. Their senses have identical language specific lexicalizations in Spanish and Dutch. For example, the English word church has one sense that is a building and another that is an institution. The same sense distinctions apply to the Spanish iglesia and the Dutch kerk . The senses in the different wordnets are linked through the ILI concepts by means of equivalence synonymy or nearsynonymy relations (Vossen 1997). The second step was to map these noun senses onto the results from the wordnet analysis described in section 4.1, and then to evaluate the cross-linguistic validity of the regular polysemic patterns that have been projected from the English monolingual wordnet onto the Dutch and Spanish wordnets.  5. Evaluation The cross-linguistic filter yields a subset of the monolingual analysis data described in section 4.1. It covers 404 distinct English nouns out of a total of 8062 (5%). This original filter considered nouns satisfying the criteria of Apresjan (cf. section 1), i.e. they are one of at least 2 words with sense distinctions that exhibit a particular relationship. The percentage covered by the crosslinguistic data compared to the original analysis gradually varies from a 100% for the very small potential classes of regular polysemy (2-3 words) to 1-2% for middle sized (30-50 words) and large classes (100+ words). In order to create a set for manual evaluation, the set of 404 English nouns was reduced by strengthening the Apresjan criterion and requiring that a word be considered only if it was one of at least a three word set illustrating the regular polysemy (RP). We will refer to this as a three-word RP class. The rationale behind this was that two word candidate RP classes introduce noise because of the increased  probability of a fortuitous coincidence of senses belonging to a set of just two words. This step reduced the number of participating words to 394. At this point, 177 words were randomly chosen from this set for manual evaluation. The evaluation consisted of examining the hypernym pairs that reflect a candidate regular polysemic relation.1 The criteria used in this step are semantic homogeneity (the semantic relation that defines the candidate RP class should apply to the majority of the participating words) and specificity of the pattern (the lower the position of the hypernymic pair in the hierarchy, the more specific the semantic relation). 109 of these words displayed valid regular polysemic patterns (62%), 68 did not (38%). This means that by means of this automatic filtering method we have a 62% success rate for identifying valid regular polysemic patterns. Below are a few examples of crosslinguistic RP classes that have satisfied the criteria of the evaluation. Hypernymic Pair: Control (the activity of managing or exerting control over something) – Trait (a distinguishing feature of one's personal nature) English RP class (7 total): abstinence, sobriety, inhibition, restraint, self-control, self-denial, self-discipline Dutch RP class (2 total): zelfcontrole, onthouding Spanish RP class (3 total): autodiscipline. abstinencia, abnegación, inhibición Coverage of the intersection between all three languages: 36% of set derived from WordNet Hypernymic Pair: Fabric (something made by weaving or felting or knitting or 
In this paper we address the issue of the encoding of information on metaphors in a WordNet-like database, i.e. the Italian wordnet in EuroWordNet (ItalWordNet). When analysing corpus data we find a huge number of metaphoric expressions which can be hardly dealt with by using as reference database ItalWordNet. In particular, we have compared information contained both in dictionaries of Italian and in ItalWordNet with actual uses of words found in a corpus. We thus put forward proposals to enrich a resource like ItalWordNet with relevant information. 
This paper addresses the question whether metaphors can be represented in WordNets. For this purpose, domain-centered data is collected from the Hamburg Metaphor Database, an online source created for the study of possible metaphor representations in WordNets. Based on the results of the analyses of French and German corpus data and EuroWordNet, the implementation problem is discussed. It can be shown that a much more complete representation of synsets and relations between synsets in the source domain as well as a clearer indication of the level of ﬁgurativity for individual synsets are needed before global conceptual metaphors can be dealt with in WordNets. 
Aptness is an umbrella term that covers a multitude of issues in the interpretation and generation of creative metaphor. In this paper we concentrate on one of these issues — the notion of lexical systematicity — and explore its role in ascertaining the coherence of creative metaphor relative to the structure of the target concept being described. We argue that all else being equal, the most apt metaphors are those that resonate most with the way the target concept is literally and metaphorically organized. As such, the lexicon plays a key role in enforcing and recognizing aptness, insofar as this existing organization will already have been lexicalized. We perform our exploration in the context of WordNet, and describe how relational structures can be automatically extracted from this lexical taxonomy to facilitate the interpretation of creative metaphors. 
The goal of this paper is to integrate the Conceptual Mapping Model with an ontology-based knowledge representation (i.e. Suggested Upper Merged Ontology (SUMO)) in order to demonstrate that conceptual metaphor analysis can be restricted and eventually, automated. In particular, we will propose a corporabased operational definition for Mapping Principles, which are explanations of why a conventional conceptual metaphor has a particular source-target domain pairing. This paper will examine 2000 random examples of ‘economy’ (jingji) in Mandarin Chinese and postulate Mapping Principles based frequency and delimited with SUMO. 
It has recently been claimed that aspect is compositional in idioms, just as it is in literal language (McGinnis 2002). We show that, although this is true for many idioms, there appear to be a number of interesting exceptions. We present examples of idiomatic expressions where aspect is not derived compositionally – or at least, not in McGinnis’s sense. Many (but not all) of these exceptions fall into a class described by Jackendoff (1997a) as ‘fake object resultatives’. We draw some tentative conclusions about the nature and classification of those idioms which show apparent noncompositionality of aspect. Furthermore, we suggest that such idioms might be regarded as aspectually compositional, if aspectual composition is taken to include as part of its input Krifka’s (1992) ‘thematic relations’. 
This paper demonstrates that the polysemy of the verb grow is a result of natural extension of individual meanings connoted by its basic literal meaning and that the polysemy of grow, as such, can be disambiguated by applying simple rules of elimination to the argument structures, which are the contexts that make particular senses viable. 
The objective of this paper is to present a new dimension of Game Theoretic Semantics (GTS) using the idea of the coordination problem game to explain the semantics of metaphor. A metaphorical expression1 such as ‘man is a wolf’ is a contradictory statement that insists all objects in the set of ‘man’ also falls under ‘wolf’. The study of metaphorical expression should be on intentions of such contradictory language use, their intended effect, and the conditions to lead to the effects. This is the tradition of Rhetoric since Aristotle. It may be natural to characterize this approach as pragmatic in the tradition of the late 20th century paradigm of linguistics, which is the trichotomy of syntax, semantics and pragmatics. However the pragmatic approach cannot explain what Richards (1936) called ‘tension’ between two thoughts in a metaphorical expression. GTS has been devel- 
Machine-learning based entity extraction requires a large corpus of annotated training to achieve acceptable results. However, the cost of expert annotation of relevant data, coupled with issues of inter-annotator variability, makes it expensive and time-consuming to create the necessary corpora. We report here on a simple method for the automatic creation of large quantities of imperfect training data for a biological entity (gene or protein) extraction system. We used resources available in the FlyBase model organism database; these resources include a curated lists of genes and the articles from which the entries were drawn, together a synonym lexicon. We applied simple pattern matching to identify gene names in the associated abstracts and filtered these entities using the list of curated entries for the article. This process created a data set that could be used to train a simple Hidden Markov Model (HMM) entity tagger. The results from the HMM tagger were comparable to those reported by other groups (F-measure of 0.75). This method has the advantage of being rapidly transferable to new domains that have similar existing resources. 
This paper describes techniques for unsupervised word sense disambiguation of English and German medical documents using UMLS. We present both monolingual techniques which rely only on the structure of UMLS, and bilingual techniques which also rely on the availability of parallel corpora. The best results are obtained using relations between terms given by UMLS, a method which achieves 74% precision, 66% coverage for English and 79% precision, 73% coverage for German on evaluation corpora and over 83% coverage over the whole corpus. The success of this technique for German shows that a lexical resource giving relations between concepts used to index an English document collection can be used for high quality disambiguation in another language. 
In this paper we present an approach to term classification based on verb complementation patterns. The complementation patterns have been automatically learnt by combining information found in a corpus and an ontology, both belonging to the biomedical domain. The learning process is unsupervised and has been implemented as an iterative reasoning procedure based on a partial order relation induced by the domain-specific ontology. First, term recognition was performed by both looking up the dictionary of terms listed in the ontology and applying the C/NC-value method. Subsequently, domain-specific verbs were automatically identified in the corpus. Finally, the classes of terms typically selected as arguments for the considered verbs were induced from the corpus and the ontology. This information was used to classify newly recognised terms. The precision of the classification method reached 64%. 
Named entity recognition is a fundamental task in biological relationship mining. This paper employs protein collocates extracted from a biological corpus to enhance the performance of protein name recognizers. Yapex and KeX are taken as examples. The precision of Yapex is increased from 70.90% to 81.94% at the low expense of recall rate (i.e., only decrease 2.39%) when collocates are incorporated. We also integrate the results proposed by Yapex and KeX, and employs collocates to filter the merged results. Because the candidates suggested by these two systems may be inconsistent, i.e., overlap in partial, one of them is considered as a basis. The experiments show that Yapex-based integration is better than KeX-based integration. 
Using SVMs for named entity recognition, we are often confronted with the multi-class problem. Larger as the number of classes is, more severe the multiclass problem is. Especially, one-vs-rest method is apt to drop the performance by generating severe unbalanced class distribution. In this study, to tackle the problem, we take a two-phase named entity recognition method based on SVMs and dictionary; at the ﬁrst phase, we try to identify each entity by a SVM classiﬁer and post-process the identiﬁed entities by a simple dictionary look-up; at the second phase, we try to classify the semantic class of the identiﬁed entity by SVMs. By dividing the task into two subtasks, i.e. the entity identiﬁcation and the semantic classiﬁcation, the unbalanced class distribution problem can be alleviated. Furthermore, we can select the features relevant to each task and take an alternative classiﬁcation method according to the task. The experimental results on the GENIA corpus show that the proposed method is effective not only in the reduction of training cost but also in performance improvement: the identiﬁcation performance is about 79.9(Fβ = 1), the semantic classiﬁcation accuracy is about 66.5(Fβ = 1).  
Dictionary-based protein name recognition is the ﬁrst step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to ﬁlter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GENIA corpus show that the ﬁltering using a naive Bayes classiﬁer greatly improves precision with slight loss of recall, resulting in a much better F-score. 
In this paper, we explore how to adapt a general Hidden Markov Model-based named entity recognizer effectively to biomedical domain. We integrate various features, including simple deterministic features, morphological features, POS features and semantic trigger features, to capture various evidences especially for biomedical named entity and evaluate their contributions. We also present a simple algorithm to solve the abbreviation problem and a rule-based method to deal with the cascaded phenomena in biomedical domain. Our experiments on GENIA V3.0 and GENIA V1.1 achieve the 66.1 and 62.5 F-measure respectively, which outperform the previous best published results by 8.1 F-measure when using the same training and testing data. 
Support Vector Machines have achieved state of the art performance in several classiﬁcation tasks. In this article we apply them to the identiﬁcation and semantic annotation of scientiﬁc and technical terminology in the domain of molecular biology. This illustrates the extensibility of the traditional named entity task to special domains with extensive terminologies such as those in medicine and related disciplines. We illustrate SVM’s capabilities using a sample of 100 journal abstracts texts taken from the {human, blood cell, transcription factor} domain of MEDLINE. Approximately 3400 terms are annotated and the model performs at about 74% F-score on cross-validation tests. A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance. 
We explore the use of morphological analysis as preprocessing for protein name tagging. Our method ﬁnds protein names by chunking based on a morpheme, the smallest unit determined by the morphological analysis. This helps to recognize the exact boundaries of protein names. Moreover, our morphological analyzer can deal with compounds. This offers a simple way to adapt name descriptions from biomedical resources for language processing. Using GENIA corpus 3.01, our method attains f-score of 70 points for protein molecule names, and 75 points for protein names including molecules, families and domains. 
We describe our work in progress on natural language analysis in medical questionanswering in the context of a broader medical text-retrieval project. We analyze the limitations in the medical domain of the technologies that have been developed for general question-answering systems, and describe an alternative approach whose organizing principle is the identiﬁcation of semantic roles in both question and answer texts that correspond to the ﬁelds of PICO format. 
Natural language processing (NLP) is critical for improvement of the healthcare process because it has the potential to encode the vast amount of clinical data in textual patient reports. Many clinical applications require coded data to function appropriately, such as decision support and quality assurance applications. However, in order to be applicable in the clinical domain, performance of the NLP systems must be adequate. A valuable clinical application is the detection of infectious diseases, such as surveillance of healthcare-associated pneumonia in newborns (e.g. neonates) because it produces significant rates of morbidity and mortality, and manual surveillance of respiratory infection in these patients is a challenge. Studies have already demonstrated that automated surveillance using NLP tools is a useful adjunct to manual clinical management, and is an effective tool for infection control practitioners. This paper presents a study aimed at evaluating the feasibility of an NLP-based electronic clinical monitoring system to identify healthcare-associated pneumonia in neo-  nates. We estimated sensitivity, specificity, and positive predictive value by comparing the detection with clinicians’ judgments and our results demonstrated that the automated method was indeed feasible. Sensitivity (recall) was 87.5%, and specificity (true negative rates) was 94.1%. 
This paper addresses a very specific problem that happens to be common in health science research. We present a machine learning based method for identifying patients diagnosed with congestive heart failure and other related conditions by automatically classifying clinical notes. This method relies on a Perceptron neural network classifier trained on comparable amounts of positive and negative samples of clinical notes previously categorized by human experts. The documents are represented as feature vectors where features are a mix of single words and concept mappings to MeSH and HICDA ontologies. The method is designed and implemented to support a particular epidemiological study but has broader implications for clinical research. In this paper, we describe the method and present experimental classification results based on classification accuracy and positive predictive value. 
It is well known that standardising the annotation of language resources signiﬁcantly raises their potential, as it enables re-use and spurs the development of common technologies. Despite the fact that increasingly complex linguistic information is being added to biomedical texts, no standard solutions have so far been proposed for their encoding. This paper describes a standardised XML tagset (DTD) for annotated biomedical corpora and other resources, which is based on the Text Encoding Initiative Guidelines P4, a general and parameterisable standard for encoding language resources. We ground the discussion in the encoding of the GENIA corpus, which currently contains 2,000 abstracts taken from the MEDLINE database, and has almost 100,000 hand-annotated terms marked for semantic class from the accompanying ontology. The paper introduces GENIA and TEI and implements a TEI parametrisation and conversion for the GENIA corpus. A number of aspects of biomedical language are discussed, such as complex tokenisation, prevalence of contractions and complex terms, and the linkage and encoding of ontologies.  
Objectives: To explore the phenomenon of adjectival modification in biomedical discourse across two genres: the biomedical literature and patient records. Methods: Adjectival modifiers are removed from phrases extracted from two corpora (three million noun phrases extracted from MEDLINE, on the one hand, and clinical notes from the Mayo Clinic, on the other). The original phrases, the adjectives extracted, and the resulting demodified phrases are compared across the two corpora after normalization. Quantitative comparisons (frequency of occurrence) are performed on the whole domain. Qualitative comparisons are performed on the two subdomains (disorders and procedures). Results: Although the average number of adjectives per phrase is equivalent in the two corpora (1.4), there are more adjective types in MAYO than in MEDLINE for disorders and procedures. For disorder phrases, the 38% of adjective types common to the two corpora account for 85% of the occurrences. The predominance of adjectives in one corpus is analyzed. Discussion: Potential applications of this approach are discussed, namely terminology acquisition, information retrieval, and genre characterization.  
The classiﬁcation task is an integral part of named entity extraction. This task has not received much attention in the biomedical setting, partly due to the fact that protein name recognition has been the focus of the majority of the work in this ﬁeld. We study this problem and focus on different sources of information that can be utilized for the classiﬁcation task and investigate the extent of their contributions for classiﬁcation in this domain. However, while developing a speciﬁc algorithm for the classiﬁcation of the names is not our main focus, we make use of some simple techniques to investigate different sources of information and verify our intuitions about their usefulness. 
In this paper we discuss the performance of a text-based classification approach by comparing different types of features. We consider the automatic classification of gene names from the molecular biology literature, by using a support-vector machine method. Classification features range from words, lemmas and stems, to automatically extracted terms. Also, simple co-occurrences of genes within documents are considered. The preliminary experiments performed on a set of 3,000 S. cerevisiae gene names and 53,000 Medline abstracts have shown that using domain-specific terms can improve the performance compared to the standard bag-of-words approach, in particular for genes classified with higher confidence, and for under-represented classes. 
Many researchers have used lexical networks and ontologies to mitigate synonymy and polysemy problems in Question Answering (QA), systems coupled with taggers, query classiﬁers, and answer extractors in complex and ad-hoc ways. We seek to make QA systems reproducible with shared and modest human effort, carefully separating knowledge from algorithms. To this end, we propose an aesthetically “clean” Bayesian inference scheme for exploiting lexical relations for passage-scoring for QA . The factors which contribute to the efﬁcacy of Bayesian Inferencing on lexical relations are soft word sense disambiguation, parameter smoothing which ameliorates the data sparsity problem and estimation of joint probability over words which overcomes the deﬁciency of naive-bayes-like approaches. Our system is superior to vector-space ranking techniques from IR, and its accuracy approaches that of the top contenders at the TREC QA tasks in recent years. 
We explore the problem of single sentence summarisation. In the news domain, such a summary might resemble a headline. The headline generation system we present uses Singular Value Decomposition (SVD) to guide the generation of a headline towards the theme that best represents the document to be summarised. In doing so, the intuition is that the generated summary will more accurately reflect the content of the source document. This paper presents SVD as an alternative method to determine if a word is a suitable candidate for inclusion in the headline. The results of a recall based evaluation comparing three different strategies to word selection, indicate that thematic information does help improve recall. 
 Dept. of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong csgngai@polyu.edu.hk  Human Language Technology Center, Dept. of Electrical & Electronic Engineering, University of Science & Technology (HKUST) Clear Water Bay, Hong Kong  eepercy@ee.ust.hk  the top N sentences as salient sentences by 46.3%.  We propose Hidden Markov models with unsupervised training for extractive summarization. Extractive summarization selects salient sentences from documents to be included in a summary. Unsupervised clustering combined with heuristics is a popular approach because no annotated data is required. However, conventional clustering methods such as K-means do not take text cohesion into consideration. Probabilistic methods are more rigorous and robust, but they usually require supervised training with annotated data. Our method incorporates unsupervised training with clustering, into a probabilistic framework. Clustering is done by modified K-means (MKM)--a method that yields more optimal clusters than the conventional K-means method. Text cohesion is modeled by the transition probabilities of an HMM, and term distribution is modeled by the emission probabilities. The final decoding process tags sentences in a text with theme class labels. Parameter training is carried out by the segmental K-means (SKM) algorithm. The output of our system can be used to extract salient sentences for summaries, or used for topic detection. Content-based evaluation shows that our method outperforms an existing extractive summarizer by 22.8% in terms of relative similarity, and outperforms a baseline summarizer that selects  
We report evaluation results for our summarization system and analyze the resulting summarization data for three different types of corpora. To develop a robust summarization system, we have created a system based on sentence extraction and applied it to summarize Japanese and English newspaper articles, obtained some of the top results at two evaluation workshops. We have also created sentence extraction data from Japanese lectures and evaluated our system with these data. In addition to the evaluation results, we analyze the relationships between key sentences and the features used in sentence extraction. We ﬁnd that discrete combinations of features match distributions of key sentences better than sequential combinations. 
Automatic text extraction techniques have proved robust, but very often their summaries are not coherent. In this paper, we propose a new extraction method which uses local coherence as a means to improve the overall quality of automatic summaries. Two algorithms for sentence selection are proposed and evaluated on scientiﬁc documents. Evaluation showed that the method ameliorates the quality of summaries, noticeable improvements being obtained for longer summaries produced by an algorithm which selects sentences using an evolutionary algorithm. 
HITIQA is an interactive question answering technology designed to allow intelligence analysts and other users of information systems to pose questions in natural language and obtain relevant answers, or the assistance they require in order to perform their tasks. Our objective in HITIQA is to allow the user to submit exploratory, analytical, non-factual questions, such as “What has been Russia’s reaction to U.S. bombing of Kosovo?” The distinguishing property of such questions is that one cannot generally anticipate what might constitute the answer. While certain types of things may be expected (e.g., diplomatic statements), the answer is heavily conditioned by what information is in fact available on the topic. From a practical viewpoint, analytical questions are often underspecified, thus casting a broad net on a space of possible answers. Therefore, clarification dialogue is often needed to negotiate with the user the exact scope and intent of the question. 
The discovery of semantic relations from text becomes increasingly important for applications such as Question Answering, Information Extraction, Summarization, Text Understanding and others. This paper presents a method for the automatic discovery of manner relations using a Naive Bayes learning algorithm. The method was tested on the UPenn Treebank2 corpus, and the targeted manner relations were detected with a precision of 64.44% and a recall of 68.67%. 
This paper proposes a machine learning based question classiﬁcation method using a kernel function, Hierarchical Directed Acyclic Graph (HDAG) Kernel. The HDAG Kernel directly accepts structured natural language data, such as several levels of chunks and their relations, and computes the value of the kernel function at a practical cost and time while reﬂecting all of these structures. We examine the proposed method in a question classiﬁcation experiment using 5011 Japanese questions that are labeled by 150 question types. The results demonstrate that our proposed method improves the performance of question classiﬁcation over that by conventional methods such as bag-of-words and their combinations. 
In this paper, we show that we can obtain a good baseline performance for Question Answering (QA) by using only 4 simple features. Using these features, we contrast two approaches used for a Maximum Entropy based QA system. We view the QA problem as a classification problem and as a reranking problem. Our results indicate that the QA system viewed as a reranker clearly outperforms the QA system used as a classifier. Both systems are trained using the same data.  phrase. Evaluation of a QA system is judged on the basis on the final output answer and the corresponding evidence provided by the segment. This paper focuses on the answer pinpointing module. Typical QA systems perform re-ranking of candidate answers as an important step in pinpointing. The goal is to rank the most likely answer first by using either symbolic or statistical methods. Some QA systems make use of statistical answer pinpointing (Xu et. al, 2002; Ittycheriah, 2001; Ittycheriah and Salim, 2002) by treating it as a classification problem. In this paper, we cast the pinpointing problem in a statistical framework and compare two approaches, classification and re-ranking. 2 Statistical Answer Pinpointing  
Causation relations are a pervasive feature of human language. Despite this, the automatic acquisition of causal information in text has proved to be a difﬁcult task in NLP. This paper provides a method for the automatic detection and extraction of causal relations. We also present an inductive learning approach to the automatic discovery of lexical and semantic constraints necessary in the disambiguation of causal relations that are then used in question answering. We devised a classiﬁcation of causal questions and tested the procedure on a QA system.  automatic discovery of lexical and semantic constraints necessary in the disambiguation of verbal causal relations. After a brief review of the previous work in Computational Linguistics on causation in section 2, we present in section 3 a classiﬁcation of lexico-syntactic patterns that are used to express causation in English texts and show the difﬁculties involved in the automatic detection and extraction of these patterns. A method for automatic detection of causation patterns and validation of ambiguous verbal lexico-syntactic patterns referring to causation is proposed in section 4. Results are discussed in section 5, and in section 6 the application of causal relations in Question Answering is demonstrated. 2 Previous Work in Computational Linguistics  
Most question answering (QA) systems rely on both keyword index and Named Entity (NE) tagging. The corpus from which the QA systems attempt to retrieve answers is usually mixed case text. However, there are numerous corpora that consist of case insensitive documents, e.g. speech recognition results. This paper presents a successful approach to QA on a case insensitive corpus, whereby a preprocessing module is designed to restore the case-sensitive form. The document pool with the restored case then feeds the QA system, which remains unchanged. The case restoration preprocessing is implemented as a Hidden Markov Model trained on a large raw corpus of case sensitive documents. It is demonstrated that this approach leads to very limited degradation in QA benchmarking (2.8%), mainly due to the limited degradation in the underlying information extraction support. 
In this paper we study the effectiveness of applying sentence compression on an extraction based multi-document summarization system. Our results show that pure syntactic-based compression does not improve system performance. Topic signature-based reranking of compressed sentences does not help much either. However reranking using an oracle showed a significant improvement remains possible. Keywords: Text Summarization, Sentence Extraction, Sentence Compression, Evaluation. 
In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text. The idea of our approach is to exploit both the local and global properties of paragraphs. The local property can be considered as clusters of signiﬁcant words within each paragraph, while the global property can be though of as relations of all paragraphs in a document. These two properties are combined for ranking and extracting summaries. Experimental results on real-world data sets are encouraging. 
 al., 1991). However, content-based filtering has some limitations:  In this work, we apply a clustering technique to integrate the contents of items into the item-based collaborative filtering framework. The group rating information that is obtained from the clustering result provides a way to introduce content information into collaborative recommendation and solves the cold start problem. Extensive experiments have been conducted on MovieLens data to analyze the characteristics of our technique. The results show that our approach contributes to the improvement of prediction quality of the item-based collaborative filtering, especially for the cold start problem. 
We have developed an effective probabilistic classiﬁer for document classiﬁcation by introducing the concept of the differential document vectors and DLSI (differential latent semantics index) spaces. A simple posteriori calculation using the intra- and extra-document statistics demonstrates the advantage of the DLSI space-based probabilistic classiﬁer over the popularly used LSI space-based classiﬁer in classiﬁcation performance. 
In this paper, we investigate the use of multivariate Poisson model and feature weighting to learn naive Bayes text classiﬁer. Our new naive Bayes text classiﬁcation model assumes that a document is generated by a multivariate Poisson model while the previous works consider a document as a vector of binary term features based on the presence or absence of each term. We also explore the use of feature weighting for the naive Bayes text classiﬁcation rather than feature selection, which is a quite costly process when a small number of the new training documents are continuously provided. Experimental results on the two test collections indicate that our new model with the proposed parameter estimation and the feature weighting technique leads to substantial improvements compared to the unigram language model classiﬁers that are known to outperform the original pure naive Bayes text classiﬁers. 
We present a simple approach for Asian language text classiﬁcation without word segmentation, based on statistical § -gram language modeling. In particular, we examine Chinese and Japanese text classiﬁcation. With character § -gram models, our approach avoids word segmentation. However, unlike traditional ad hoc § -gram models, the statistical language modeling based approach has strong information theoretic basis and avoids explicit feature selection procedure which potentially loses signiﬁcantly amount of useful information. We systematically study the key factors in language modeling and their inﬂuence on classiﬁcation. Experiments on Chinese TREC and Japanese NTCIR topic detection show that the simple approach can achieve better performance compared to traditional approaches while avoiding word segmentation, which demonstrates its superiority in Asian language text classiﬁcation. 
Text categorization, as an essential component of applications for user navigation on the World Wide Web using QuestionAnswering in Japanese, requires more effective features for the categorization of documents and the efﬁcient acquisition of knowledge. In the questions addressed by such navigation, we focus on those questions for procedures and intend to clarify speciﬁcation of the answers. 
Recent years saw an increased interest in the use and the construction of large corpora. With this increased interest and awareness has come an expansion in the application to knowledge acquisition and bilingual terminology extraction. The present paper will seek to present an approach to bilingual lexicon extraction from non-aligned comparable corpora, combination to linguisticsbased pruning and evaluations on CrossLanguage Information Retrieval. We propose and explore a two-stages translation model for the acquisition of bilingual terminology from comparable corpora, disambiguation and selection of best translation alternatives on the basis of their morphological knowledge. Evaluations using a large-scale test collection on JapaneseEnglish and different weighting schemes of SMART retrieval system conﬁrmed the effectiveness of the proposed combination of two-stages comparable corpora and linguistics-based pruning on CrossLanguage Information Retrieval. Keywords: Cross-Language Information Retrieval, Comparable corpora, Translation, Disambiguation, Part-of-Speech.  
This paper describes two new features of the BRIDJE system for cross-language information access. The ﬁrst feature is the partial disambiguation function of the Bi-directional Retriever, which can be used for search request translation in cross-language IR. Its advantage over a “black-box” machine translation approach is consistent across ﬁve test collections and across two language permutations: English-Japanese and Japanese-English. The second new feature is the Information Distiller, which performs interactive summarisation of retrieved documents based on Semantic Role Analysis. Our examples illustrate the usefulness of this feature, and our evaluation results show that the precision of Semantic Role Analysis is very high.  
Query expansion by pseudo-relevance feedback is a well-established technique in both mono- and cross- lingual information retrieval, enriching and disambiguating the typically terse queries provided by searchers. Comparable document-side expansion is a relatively more recent development motivated by error-prone transcription and translation processes in spoken document and cross-language retrieval. In the cross-language case, one can perform expansion before translation, after translation, and at both points. We investigate the relative impact of pre- and post- translation document expansion for cross-language spoken document retrieval in Mandarin Chinese. We ﬁnd that posttranslation expansion yields a highly signiﬁcant improvement in retrieval effectiveness, while improvements due to pretranslation expansion alone or in combination do not reach signiﬁcance. We identify two key factors of segmentation and translation in Chinese orthography that limit the effectiveness of pre-translation expansion in the Chinese-English case, while post-translation expansion yields its full beneﬁt. 
In this paper, we focus on performing LSI on very low SVD dimensions. The results show that there is a nearly linear surface in the local query region. Using low-dimensional LSI on local query region we can capture such a linear surface, obtain much better performance than VSM and come comparably to global LSI. The surprisingly small requirements of the SVD dimension resolve the computation restrictions. Moreover, on the condition that several relevant sample documents are available, application of low-dimensional LSI to these documents yielded comparable IR performance to local RF but in a different manner. 
The accuracy of IR result continues to grow on importance as exponential growth of WWW, and it is therefore increasingly important that appropriate retrieval technologies be developed for the web. We explore a new type of IR, “answer set based IR”, and its operational experience. Our proposed approach attempts to provide high quality answer documents to user by maintaining a knowledge base with expected queries and corresponding answer document. We will elaborate on our architecture and the experimental results. Keywords: answer set driven IR, attribute- based classification, automatic knowledge base construction,. 1. Introduction The goal of Information Retrieval (IR) is finding answer suited to user question from massive document collections with satisfied response time. With the exponential growth of information on the Web, user is expecting to find answer more fast with less effort. Current IR systems especially focus on improving precision the result rather than recall. A notable trend in IR is to provide more accurate, immediately usable information as in Question Answering systems(Q/A) [1] or in some systems using pre-constructed question/answer document pairs [2, 3], known “answer set driven” system. While traditional search engine uses term indexing, i.e. tf*idf, answer approaches use syntactic, semantic and pragmatic knowledge provided expert, i.e. WordNet[4]. Another difference comes from the fact that answer approach returns “answer set” distilled  information need of user as retrieval result, not just document appeared query terms. The TREC Q/A track [1, 5, 6] which has motivated much of the recent work in the field focuses on fact-based, short-answer question type, e.g. “Who is Barbara Jordan?” or “What is Mardi Gras?. The Q/A runs find an actual answer in TREC collection, rather than a ranked list of documents, in response to a question. On the other hand, user queries in answer set driven system, like AskJeeves[2], are more implicit and conceptual. These system was developed targeting the Web [7, 8], is larger than the TREC Q/A document collection. Whereas the user gives incomplete query to system, they need not only answers but related information. Sometimes the user even has uncertainty what exactly they need. For example, the user query just “Paris” is answered by gathering information including Paris city guide, photographs of Paris, and so on. To catch information need of user, these system have pre-defined query pattern and prepared correct answers belonging to each question. Since it is still considered difficult, if not impossible, to capture semantics and pragmatics of sentences in user queries and documents, such systems require knowledge bases built manually so that a certain level of quality can be guaranteed. Needless to say, this knowledge base construction process is laborintensive, typically requiring significant and continuous human efforts [9]. This paper rests on the both directions: a new type of IR and its operational experience. Our system, named “AnyQ”1, attempts to provide high quality answer documents to user queries by maintaining a knowledge base consisting of expected queries and corresponding answer document. We defined the semantic category of the answer as attributes and the 
Though dynamic programming matching can carry out approximate string matching when there may be deletions or insertions in a document, its effectiveness and efﬁciency are usually too poor to use it for large-scale information retrieval. In this paper, we propose a method of dynamic programming matching for information retrieval. This method is as effective as a conventional information retrieval system, even though it is capable of approximate matching. It is also as efﬁcient as a conventional system. Keywords: Dynamic programming, Corpus-based, Japanese. 
In this paper, we examine how to improve the precision and recall of document clustering by utilizing meta-data. We use meta-data through NewsML tags to assist clustering and show that this approach is effective through experiments on sample news data. Experimental result shows that clustering using NewsML could improve average recall and precision over the same without using NewsML by about 10%. Our algorithm facilitates effective e-business for the news media and publishing industry to empower e-business. 
In business, the retrieval of up-to-date, or fresh, information is very important. It is difficult for conventional search engines based on a centralized architecture to retrieve fresh information, because they take a long time to collect documents via Web robots. In contrast to a centralized architecture, a search engine based on a distributed architecture does not need to collect documents, because each site makes an index independently. As a result, distributed search engines can be used to retrieve fresh information. However, fast indexing alone is not enough to retrieve fresh information, as support for temporal information based retrieval is also required. In this paper, we describe temporal information retrieval in distributed search engines. In particular, we propose an implementation of temporal ranking. 1. Introduction 
In this work, we propose a new method for extracting user preferences from a few documents that might interest users. For this end, we first extract candidate terms and choose a number of terms called initial representative keywords (IRKs) from them through fuzzy inference. Then, by expanding IRKs and reweighting them using term co-occurrence similarity, the final representative keywords are extracted. Performance of our approach is heavily influenced by effectiveness of selection method for IRKs so we choose fuzzy inference because it is more effective in handling the uncertainty inherent in selecting representative keywords of documents. The problem addressed in this paper can be viewed as the one of finding a representative vector of documents in the linear text classification literature. So, to show the usefulness of our approach, we compare it with two famous methods Rocchio and Widrow-Hoff - on the Reuters-21578 collection. The results show that our approach outperforms the other approaches. 
In this paper, we describe ontology-based text categorization in which the domain ontologies are automatically acquired through morphological rules and statistical methods. The ontology-based approach is a promising way for general information retrieval applications such as knowledge management or knowledge discovery. As a way to evaluate the quality of domain ontologies, we test our method through several experiments. Automatically acquired domain ontologies, with or without manual editing, have been used for text categorization. The results are quite satisfactory. Furthermore, we have developed an automatic method to evaluate the quality of our domain ontology. 1. Introduction Domain ontology, consisting of important concepts and relationships of the concepts in the domain, is useful in a variety of applications (Gruber, 1993). However, evaluating the quality of domain ontologies is not straightforward. Reusing an ontology for several applications can be a practical method for evaluating domain ontology. Since text categorization is a general tool for information retrieval, knowledge management and knowledge discovery, we test the ability of domain ontology to categorize news clips in this paper.  Traditional IR methods use keyword distribution form a training corpus to assign testing document. However, using only keywords in a training set cannot guarantee satisfactory results since authors may use different keywords. We believe that, news clip events are categorized by concepts, not just keywords. Previous works shows that the latent semantic index (LSI) method and the n-gram method give good results for Chinese news categorization (Wu et al., 1998). However, the indices of LSI and n-grams are less meaningful semantically. The implicit rules acquired by these methods can be understood by computers, not humans. Thus, manual editing for exceptions and personalization are not possible and it is difficult to further reuse these indices for knowledge management. With good domain ontology we can identify the concept structure of sentences in a document. Our idea is to compile the concepts within documents in a training set and use these concepts to understand documents in a testing set. However, building rigorous domain ontology is laborious and time-consuming. Previous works suggest that ontology acquisition is an iterative process, which includes keyword collection and structure reorganization. The ontology is revised, refined, and accumulated by a human editor at each iteration (Noy and McGuinness, 2001). For example, in order to find a hyponym of a keyword, the human editor must observe sentences containing this keyword and its related hyponyms (Hearst, 1992). The editor then deduces rules for finding more hyponyms of this keyword. At each iteration the editor refines the rules to obtain better quality pairs of keyword-hyponyms. To speed up  the above labor-intensive approach, semiautomatic approaches have been designed in which a human editor only has to verify the results of the acquisition (Maedche and Staab, 2000). A knowledge representation framework, Information Map (InfoMap) in our previous work (Hsu et al., 2001), has been designed to integrate various linguistic, common-sense and domain knowledge. InfoMap is designed to perform natural language understanding, and applied to many application domains, such as question answering (QA), knowledge management and organization memory (Wu et al., 2002), and shows good results. An important characteristic of InfoMap is that it extracts events from a sentence by capturing the topic words, usually subject-verb pairs or hypernym-hyponym pairs, which are defined in the domain ontology.  We shall review the InfoMap ontology framework in Section 2. The ontology acquisition process and extraction rules will be introduced in Section 3. We describe ontology-based text categorization in Section 4. Experimental results are reported in Section 5. We conclude our work in Section 6. 2. Information Map InfoMap can serve as domain ontology as well as an inference engine. InfoMap is designed for NLP applications; its basic function is to identify the event structure of a sentence. We shall briefly describe InfoMap in this section. Figure 1 gives example ontology of the Central News Agency (CNA), the target in our experiment. 2.1 InfoMap Structure Format As a domain ontology, InfoMap consists of domain concepts and their related sub-concepts such as categories, attributes, activities. The relationships of a concept and its associated subconcepts form a tree-like taxonomy. InfoMap also defines references to connect nodes from different branches which serves to integrate these hierarchical concepts into a network. InfoMap not only classifies concepts, but also connects the concepts by defining the relationships among them.  Concept A  Legend Function node Concept node  Category Attribute  Concept A' (Sub-concept of concept A) Concept B (relavant but not belong to concept A)  Figure 1. Ontology Structure for CNA News  Action  Concept C (An activity of concept A)  Figure 2. Skeleton of the Ontology Structure of InfoMap  In InfoMap, concept nodes represent concepts and function nodes represent the relationships between concepts. The root node of a domain is the name of the domain. Following the root node, important topics are stored in a hierarchical order. These topics have sub-categories that list related sub-topics in a recursive fashion. Figure 1 is a partial view of the domain ontology of the CNA. Under each domain there are several topics and each topic might have sub-concepts and associated attributes. In this example, note that, the domain ontology is automatically acquired from a domain corpus, hence the quality is poor. Figure 2 shows the skeleton order of a concept using InfoMap. 2.2 Event Structure Since concepts that are semantically related are often clustered together, one can use InfoMap to discern the main event structure in a natural language sentence. The process of identifying the event structure, we call a firing mechanism, which matches words in a sentence to both concepts and relationships in InfoMap. Suppose keywords of concept A and its subconcept B (or its hyponyms) appear in a sentence. It is likely that the author is describing an event “B of A”. For example, when the words “tire” and “car” appear in a sentence, normally this sentence would be about the tire of a car (not tire in the sense of fatigue). Therefore, a word-pair with a semantic relationship can give more concrete information than two words without a semantic relationship. Of course, certain syntactic constraints also need to be satisfied. This can be extended to a noun-verb pair or a combination of noun, verb and adjective. We call such words in a sentence an event structure. This mechanism seems to be especially effective for Chinese sentences. 2.3 Domain Speculation With the help of domain ontologies, one can categorize a piece of text into a specific domain by categorizing each individual sentence within the text. There are many different ways to use domain ontology to categorize text. It can be used as a dictionary, as a keyword lists and as a structure to identify NL events. Take a single sentence for example. We first use InfoMap as a dictionary to do word segmentation (necessary for Chinese  sentences) in which the ambiguity can be resolved by checking the domain topic in the ontology. After words are segmented, we can examine the distribution of these words in the ontology and effectively identify the densest cluster. Thus, we can use InfoMap to identify the domains of the sentences and their associated keywords. Section 4.1 will further elaborate on this. 3. Automatic Ontology Acquisition The automatically domain ontology acquisition from a domain corpus has three steps: 1. Identify the domain keywords. 2. Find the relative concepts. 3. Merge the correlated activities. 3.1 Domain Keyword Identification The first step of automatic domain ontology acquisition is to identify domain keywords. Identifying Chinese unknown words is difficult since the word boundary is not marked in Chinese corpus. According to an inspection of a 5 million word Chinese corpus (Chen et al., 1996), 3.51% of words are not listed in the CKIP lexicon (a Chinese lexicon with more than 80,000 entries). We use reoccurrence frequency and fan-out numbers to characterize words and their boundaries according to PAT-tree (Chien, 1999). We then adopt the TF/IDF classifier to choose domain keywords. The domain keywords serve as the seed topics in the ontology. We then apply SOAT to automatically obtain related concepts. 3.2 SOAT To build the domain ontology for a new domain, we need to collect domain keywords and concepts by finding relationships among keywords. We adopt a semi-automatic domain ontology acquisition tool (SOAT, Wu et al., 2002), to construct a new ontology from a domain corpus. With a given domain corpus, SOAT can build a prototype of the domain ontology. InfoMap uses two major relationships among concepts: taxonomic relationships (category and synonym) and non-taxonomic relationships (attribute and action). SOAT defines rules, which consist of patterns of keywords and variables, to capture these relationships. The extraction rules in  SOAT are morphological rules constructed from part-of-speech (POS) tagged phrase structure. Here we briefly introduce the SOAT process: Input: domain corpus with the POS tag Output: domain ontology prototype Steps: 
This paper present a method based on the behavior of nonnative speaker for reduction sentence in foreign language. We demonstrate an algorithm using semantic information in order to produce two reduced sentences in two difference languages and ensure both grammatical and sentence meaning of the original sentence in reduced sentences. In addition, the orders of reduced sentences are able to be different from original sentences. 
Since the Web consists of documents in various domains or genres, the method for Cross-Language Information Retrieval (CLIR) of Web documents should be independent of a particular domain. In this paper, we propose a CLIR method which employs a Web directory provided in multiple language versions (such as Yahoo!). In the proposed method, feature terms are ﬁrst extracted from Web documents for each category in the source and the target languages. Then, one or more corresponding categories in another language are determined beforehand by comparing similarities between categories across languages. Using these category pairs, we intend to resolve ambiguities of simple dictionary translation by narrowing the categories to be retrieved in the target language. 
Named entity recognition is important in sophisticated information service system such as Question Answering and Text Mining since most of the answer type and text mining unit depend on the named entity type. Therefore we focus on named entity recognition model in Korean. Korean named entity recognition is difficult since each word of named entity has not specific features such as the capitalizing feature of English. It has high dependence on the large amounts of hand-labeled data and the named entity dictionary, even though these are tedious and expensive to create. In this paper, we devise HMM based named entity recognizer to consider various context models. Furthermore, we consider weakly supervised learning technique, CoTraining, to combine labeled data and unlabeled data. Keywords : Korean Named Entity, HMM, Co-Training  hand-labeled data and the NE dictionary, even though these are tedious and expensive to create. In the case of NE classification, NE can be classified with the clues such as inner word and context word. Although these clue words present the feature of NE type, it can be used in detecting the NE since the contained word and context word can be used in determining the boundary of NE. However, the clue words can provoke ambiguity to determine the NEtype sincevarious NEs can share the same clue word . Therefore, we devise the statistical model based NE recognizer which can unify the detection and classification. Furthermore, we consider unlabeled data based statis tical learning to extend the initial seed data. The weakly supervised learning technique is Co-Training method. In this paper, we describe the HMM based Korean NE recognition and Co-Training method for HMM based boosting.  1. Introduction Named entity(NE) recognition is important for recent sophisticated information service such as question answering and text mining since it recognizes the words to present the core information in text. In particular, the NE recognizer is important module in the well-known question answering systems such as FALCON, IBM[3][10]. NE recognizer is well suited for the recognition of answer type which can be equal to the NE type or not. Although an answer is not exactly matched to the NE, these two types can be mapping to each otherby using WordNet[3]. NE recognition can be explained with two steps, NE detection and NE classification. Whereas NE detection is to catch the named entities in the text , NE classification is to classify NE into person, organization or location. In Korean, NE detection is difficult since each word of name entity has not specific features such as the capitalizing feature of English. It has high dependence on the large amounts of  2. The Problem NE dictionary is not enough to cover all of the NEs since there area fewtypes of NEs besides single word .We classify Korean NE into three types. The first is single word type, the second is compound noun type, and the third is noun phrase type. The single word type is usually single noun. The second type, compound noun, is composed of a few words and affix. The third type can have grammatical morphemes besides nouns. The example is described in Figure 1. It describes NEs with PERSON(PER), LOCATION (LOC), and ORGANIZATION(ORG). In each type, second sentence is the result of morphological analysis. The example shows the diversity of the Korean NE type. In the single word type, if the dictionary has the word, then the NE could be detected easily. However, compound noun type and noun phrase type require a tremendous number of entries in the dictionary. Moreover, in Korean these kinds of NE types are used differently in each people . Therefore, it  should usethe clue word to recognize NE which is shown as the compound noun or noun phrase type. This clue word, which is context or inner word of NE, can be used in NE detection and classification, but we found that the clue word can provoke another problem which is the ambiguity; thus different types of NEs can share the same clue word. Which means that the clue word dictionary cannot be the unique solution. Figure 1. Named entity example1 Consequently, we suggest three approaches for NE recognition. The first is feature dictionary based approach which classify the clue words and generate feature types of the context or inner word of NE. The second is statistical approach which needs named entity tagged corpus and context model to recognize NE. The third is unlabeled data based boosting approach since statistical approach, which needs hand-labeled NE tagged corpus, cannot avoid data sparseness. 3. Related Works Statistical approach in NE recognition can be classified into supervised learning and weakly supervised learning. Supervised learning is based on labeled data. On the other hand, weakly supervised method is the learning approach to combine labeled data and unlabeled data. From the supervised learning point of view, the most representative research is HMM based NE recognition. It builds various  bigram models of NEs and predicts next NE type with the previous history, lexical item and NE type. Using simple word feature Bikel shows F-measure 90% in English [4]. Zhou’s HMM-based chunk tagger approach adopt more detailed feature than Bikel’s, and show F-measure 94.3%[5]. In this paper, when we designed feature model, we followed the HMM-based chunk tagger approach considering the property of Korean NE. Recently there have been many researches in weakly supervised learning technique to combine labeled data and unlabeled data. Co-Training method Blum is most famous approach to boost the initial learning data with unlabeled data[2]. Blum showed that using a large unlabeled data to boost performance of a learning algorithm could be used to classify web pages when only a small set of labeled examples is available [2]. Nigam demonstrated that when learning from labeled and unlabeled data, algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not[7][8]. Collins and Singer showed that the use of unlabeled data can reduce the requirements for supervision to just 7 simple “seed” rules [9]. In addition to a heuristic based on decision list learning, they also presented a boosting-like framework that builds on ideas from Blum[2]. 
This paper proposes an algorithm for causality inference based on a set of lexical knowledge bases that contain information about such items as event role, is-a hierarchy, relevant relation, antonymy, and other features. These lexical knowledge bases have mainly made use of lexical features and symbols in HowNet. Several types of questions are experimented to test the effectiveness of the algorithm here proposed. Particularly in this paper, the question form of “why” is dealt with to show how causality inference works. 
 In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrasebased models. The units of translation are blocks – pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying high-precision word alignment. The system performance is signiﬁcantly increased by applying a novel block extension algorithm using an additional highrecall word alignment. The blocks are further ﬁltered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an ArabicEnglish translation task. 
We deﬁne, implement and evaluate a novel model for statistical machine translation, which is based on shallow syntactic analysis (part-of-speech tagging and phrase chunking) in both the source and target languages. It is able to model long-distance constituent motion and other syntactic phenomena without requiring a full parse in either language. We also examine aspects of lexical transfer, suggesting and exploring a concept of translation coercion across parts of speech, as well as a transfer model based on lemma-to-lemma translation probabilities, which holds promise for improving machine translation of low-density languages. Experiments are performed in both Arabic-to-English and French-to-English translation demonstrating the efﬁcacy of the proposed techniques. Performance is automatically evaluated via the Bleu score metric. 
We propose new methods to take advantage of text in resource-rich languages to sharpen statistical language models in resource-deﬁcient languages. We achieve this through an extension of the method of lexical triggers to the cross-language problem, and by developing a likelihoodbased adaptation scheme¡ for combining a trigger model with an -gram model. We describe the application of such language models for automatic speech recognition. By exploiting a side-corpus of contemporaneous English news articles for adapting a static Chinese language model to transcribe Mandarin news stories, we demonstrate signiﬁcant reductions in both perplexity and recognition errors. We also compare our cross-lingual adaptation scheme to monolingual language model adaptation, and to an alternate method for exploiting cross-lingual cues, via crosslingual information retrieval and machine translation, proposed elsewhere. 
We address the problem of sentence alignment for monolingual corpora, a phenomenon distinct from alignment in parallel corpora. Aligning large comparable corpora automatically would provide a valuable resource for learning of text-totext rewriting rules. We incorporate context into the search for an optimal alignment in two complementary ways: learning rules for matching paragraphs using topic structure and further reﬁning the matching through local alignment to ﬁnd good sentence pairs. Evaluation shows that our alignment method outperforms state-of-the-art systems developed for the same task. 
This paper explores the problem of ﬁnding non-local dependencies. First, we isolate a set of features useful for this task. Second, we develop both a two-step approach which combines a trace tagger with a state-of-the-art lexicalized parser and a one-step approach which ﬁnds nonlocal dependencies while parsing. We ﬁnd that the former outperforms the latter because it makes better use of the features we isolate. 
 load  We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features. We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features. 
The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using previous tag information to find the highest probability tag sequence for a given sentence. Further we examine the use of sentence level syntactic pattern features to increase performance. We analyze our strategy on both human annotated and automatically identified frame elements, and compare performance to previous work on identical test data. Experiments indicate a statistically significant improvement (p<0.01) of over 6%. 
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar. This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.  
In this paper we explore the variation of sentences as a function of the sentence number. We demonstrate that while the entropy of the sentence increases with the sentence number, it decreases at the paragraph boundaries in accordance with the Entropy Rate Constancy principle (introduced in related work). We also demonstrate that the principle holds for different genres and languages and explore the role of genre informativeness. We investigate potential causes of entropy variation by looking at the tree depth, the branching factor, the size of constituents, and the occurrence of gapping. 
This paper compares a range of methods for classifying words based on linguistic diagnostics, focusing on the task of learning countabilities for English nouns. We propose two basic approaches to feature representation: distribution-based representation, which simply looks at the distribution of features in the corpus data, and agreement-based representation which analyses the level of tokenwise agreement between multiple preprocessor systems. We additionally compare a single multiclass classiﬁer architecture with a suite of binary classiﬁers, and combine analyses from multiple preprocessors. Finally, we present and evaluate a feature selection method.  with differences in meaning: I submitted two papers “documents” (countable) vs. Please use white paper “substance to be written on” (uncountable). This research complements that described in Baldwin and Bond (2003), where we present the linguistic foundations and features drawn upon in the countability classiﬁcation task, and motivate the claim that countability preferences can be learned from corpus evidence. In this paper, we focus on the methods used to tackle the task of countability classiﬁcation based on this ﬁxed feature set. The remainder of this paper is structured as follows. Section 2 outlines the countability classes, resources and pre-processors. Section 3 presents two methods of representing the feature space. Section 4 details the different classiﬁer designs and the dataset, which are then evaluated in Section 5. Finally, we conclude the paper with a discussion in Section 6.  
We propose the use of Lexicalized Tree Adjoining Grammar (LTAG) as a source of features that are useful for reranking the output of a statistical parser. In this paper, we extend the notion of a tree kernel over arbitrary sub-trees of the parse to the derivation trees and derived trees provided by the LTAG formalism, and in addition, we extend the original deﬁnition of the tree kernel, making it more lexicalized and more compact. We use LTAG based features for the parse reranking task and obtain labeled recall and precision of 89.7%/90.0% on WSJ section 23 of Penn Treebank for sentences of length ≤ 100 words. Our results show that the use of LTAG based tree kernel gives rise to a 17% relative difference in f -score improvement over the use of a linear kernel without LTAG based features. 
This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG). Log-linear models can easily encode the long-range dependencies inherent in coordination and extraction phenomena, which CCG was designed to handle. Log-linear models have previously been applied to statistical parsing, under the assumption that all possible parses for a sentence can be enumerated. Enumerating all parses is infeasible for large grammars; however, dynamic programming over a packed chart can be used to efﬁciently estimate the model parameters. We describe a parellelised implementation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation. 
This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. High-precision classiﬁers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. The learned patterns are then used to identify more subjective sentences. The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision. 
Successful application of multi-view cotraining algorithms relies on the ability to factor the available features into views that are compatible and uncorrelated. This can potentially preclude their use on problems such as coreference resolution that lack an obvious feature split. To bootstrap coreference classiﬁers, we propose and evaluate a single-view weakly supervised algorithm that relies on two different learning algorithms in lieu of the two different views required by co-training. In addition, we investigate a method for ranking unlabeled instances to be fed back into the bootstrapping loop as labeled data, aiming to alleviate the problem of performance deterioration that is commonly observed in the course of bootstrapping. 
A Natural Language Generation system produces text using as input semantic data. One of its very ﬁrst tasks is to decide which pieces of information to convey in the output. This task, called Content Selection, is quite domain dependent, requiring considerable re-engineering to transport the system from one scenario to another. In this paper, we present a method to acquire content selection rules automatically from a corpus of text and associated semantics. Our proposed technique was evaluated by comparing its output with information selected by human authors in unseen texts, where we were able to ﬁlter half the input data set without loss of recall. 
Opinion question answering is a challenging task for natural language processing. In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level. We present a Bayesian classiﬁer for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the signiﬁcantly harder task of detecting opinions at the sentence level. We also present a ﬁrst model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion. Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classiﬁcation (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy). 
A maximum entropy (ME) model is usually estimated so that it conforms to equality constraints on feature expectations. However, the equality constraint is inappropriate for sparse and therefore unreliable features. This study explores an ME model with box-type inequality constraints, where the equality can be violated to reﬂect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension.  
Discriminative models have been of interest in the NLP community in recent years. Previous research has shown that they are advantageous over generative models. In this paper, we investigate how different objective functions and optimization methods affect the performance of the classiﬁers in the discriminative learning framework. We focus on the sequence labelling problem, particularly POS tagging and NER tasks. Our experiments show that changing the objective function is not as effective as changing the features included in the model. 
This paper describes a fast algorithm that selects features for conditional maximum entropy modeling. Berger et al. (1996) presents an incremental feature selection (IFS) algorithm, which computes the approximate gains for all candidate features at each selection stage, and is very time-consuming for any problems with large feature spaces. In this new algorithm, instead, we only compute the approximate gains for the top-ranked features based on the models obtained from previous stages. Experiments on WSJ data in Penn Treebank are conducted to show that the new algorithm greatly speeds up the feature selection process while maintaining the same quality of selected features. One variant of this new algorithm with look-ahead functionality is also tested to further confirm the good quality of the selected features. The new algorithm is easy to implement, and given a feature space of size F, it only uses O(F) more space than the original IFS algorithm. 
We investigate the performance of the Structured Language Model (SLM) in terms of perplexity (PPL) when its components are modeled by connectionist models. The connectionist models use a distributed representation of the items in the history and make much better use of contexts than currently used interpolated or back-off models, not only because of the inherent capability of the connectionist model in ﬁghting the data sparseness problem, but also because of the sublinear growth in the model size when the context length is increased. The connectionist models can be further trained by an EM procedure, similar to the previously used procedure for training the SLM. Our experiments show that the connectionist models can signiﬁcantly improve the PPL over the interpolated and back-off models on the UPENN Treebank corpora, after interpolating with a baseline trigram language model. The EM training procedure can improve the connectionist models further, by using hidden events obtained by the SLM parser. 
We present a new framework for classifying common nouns that extends namedentity classiﬁcation. We used a ﬁxed set of 26 semantic labels, which we called supersenses. These are the labels used by lexicographers developing WordNet. This framework has a number of practical advantages. We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. We also deﬁne a more realistic evaluation procedure than cross-validation. 
We present a machine learning framework for resolving other-anaphora. Besides morpho-syntactic, recency, and semantic features based on existing lexical knowledge resources, our algorithm obtains additional semantic knowledge from the Web. We search the Web via lexico-syntactic patterns that are speciﬁc to other-anaphors. Incorporating this innovative feature leads to an 11.4 percentage point improvement in the classiﬁer’s -measure (25% improvement relative to results without this feature). 
Anaphora resolution is one of the most important research topics in Natural Language Processing. In English, overt pronouns such as she and deﬁnite noun phrases such as the company are anaphors that refer to preceding entities (antecedents). In Japanese, anaphors are often omitted, and these omissions are called zero pronouns. There are two major approaches to zero pronoun resolution: the heuristic approach and the machine learning approach. Since we have to take various factors into consideration, it is difﬁcult to ﬁnd a good combination of heuristic rules. Therefore, the machine learning approach is attractive, but it requires a large amount of training data. In this paper, we propose a method that combines ranking rules and machine learning. The ranking rules are simple and effective, while machine learning can take more factors into account. From the results of our experiments, this combination gives better performance than either of the two previous approaches. 
 The paper presents a maximum entropy  Chinese character-based parser trained on  the Chinese Treebank (“CTB” hence-  forth). Word-based parse trees in  CTB are ﬁrst converted into character-  based trees, where word-level part-of-  speech (POS) tags become constituent  labels and character-level tags are de-  rived from word-level POS tags. A  maximum entropy parser is then trained  on the character-based corpus. The  parser does word-segmentation, POS-  tagging and parsing in work. An average label and word-segmentation  a uniﬁed F-measure F-measure  f¢r¡¤am£¦¥¨§e© £§  are achieved by the parser. Our re-  sults show that word-level POS tags can  improve signiﬁcantly word-segmentation,  but higher-level syntactic strutures are of  little use to word segmentation in the max-  imum entropy parser. A word-dictionary  helps to improve both word-segmentation  and parsing accuracy.  
When building a Chinese named entity recognition system, one must deal with certain language-speciﬁc issues such as whether the model should be based on characters or words. While there is no unique answer to this question, we discuss in detail advantages and disadvantages of each model, identify problems in segmentation and suggest possible solutions, presenting our observations, analysis, and experimental results. The second topic of this paper is classiﬁer combination. We present and describe four classiﬁers for Chinese named entity recognition and describe various methods for combining their outputs. The results demonstrate that classiﬁer combination is an effective technique of improving system performance: experiments over a large annotated corpus of ﬁne-grained entity types exhibit a 10% relative reduction in F-measure error. 
We explore how virtual examples (artiﬁcially created examples) improve performance of text classiﬁcation with Support Vector Machines (SVMs). We propose techniques to create virtual examples for text classiﬁcation based on the assumption that the category of a document is unchanged even if a small number of words are added or deleted. We evaluate the proposed methods by Reuters-21758 test set collection. Experimental results show virtual examples improve the performance of text classiﬁcation with SVMs, especially for small training sets. 
In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and ngrams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the POS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.  The aim of keyword assignment is to ﬁnd a small set of terms that describes a speciﬁc document, independently of the domain it belongs to. However, the latter may very well beneﬁt from the results of the former, as appropriate keywords often are of a terminological character. In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach ﬁrst proposed by Turney (2000). Two important issues are how to deﬁne the potential terms, and what features of these terms are considered discriminative, i.e., how to represent the data, and consequently what is given as input to the learning algorithm. In this paper, experiments with three term selection approaches are presented: n-grams; noun phrase (NP) chunks; and terms matching any of a set of part-of-speech (POS) tag sequences. Four different features are used: term frequency, collection frequency, relative position of the ﬁrst occurrence, and the POS tag(s) assigned to the term.  
Our goal is to be able to answer questions about text that go beyond facts explicitly stated in the text, a task which inherently requires extracting a “deep” level of meaning from that text. Our approach treats meaning processing fundamentally as a modeling activity, in which a knowledge base of common-sense expectations guides interpretation of text, and text suggests which parts of the knowledge base might be relevant. In this paper, we describe our ongoing investigations to develop this approach into a usable method for meaning processing. 
We have been developing techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction, focusing initially on the Brown corpus. We apply interpretive rules to clausal patterns and patterns of modiﬁcation, and concurrently abstract general “possibilistic” propositions from the resulting formulas. Two examples are “A person may believe a proposition”, and “Children may live with relatives”. Our methods currently yield over 117,000 such propositions (of variable quality) for the Brown corpus (more than 2 per sentence). We report here on our efforts to evaluate these results with a judging scheme aimed at determining how many of these propositions pass muster as “reasonable general claims” about the world in the opinion of human judges. We ﬁnd that nearly 60% of the extracted propositions are favorably judged according to our scheme by any given judge. The percentage unanimously judged to be reasonable claims by multiple judges is lower, but still sufﬁciently high to suggest that our techniques may be of some use in tackling the long-standing “knowledge acquisition bottleneck” in AI. 
The approach to knowledge representation taken in a multi-modal multi-domain dialogue system - SMARTKOM - is presented. We focus on the ontological and representational issues and choices helping to construct an ontology, which is shared by multiple components of the system, can be re-used in different projects and applied to various tasks. Finally, examples highlighting the usefulness of our approach are given. 
In this paper, we briefly and informally illustrate, using a few annotated examples, the static and dynamic knowledge resources of ontological semantics. We then present the main motivations and desiderata of our approach and then discuss issues related to making ontological-semantic applications feasible through the judicious stepwise enhancement of static and dynamic knowledge sources while at all times maintaining a working system. 1. Introduction This paper discusses selected issues in ontological semantics (OS), an implemented computational-semantic theory that deals with the extraction, representation and use of meaning in natural language texts. Unlike practically all other work in computational semantics, OS makes itself responsible for all the necessary components and stages in automatic text meaning analysis: it addresses lexical and compositional meaning as well as pragmatics and discourse issues. Its processing heuristics are derived from syntax, morphology and other “preprocessing,” non-semantic analysis stages that are still incorporated in the system, as well as from detailed underlying world models that include specifications not only of basic events, objects and properties but also of complex events, or scripts. The goal of OS is the extraction, representation and manipulation of meaning in natural language texts with a view toward supporting applications such as MT or question answering. Text meaning is represented in text meaning representations (TMRs) that are derived compositionally, primarily from meanings of words and phrases in the text. Word and phrase meaning is encoded in the ontological-semantic lexicon. The underlying ontology is the main metalanguage of lexical meaning specification.1 As a result, TMRs largely consist of 
This paper describes the creation of a script in the framework of ontological semantics as the formal representation of the complex event BANKRUPTCY. This script for BANKRUPTCY serves as the exemplary basis for a discussion of the general motivations for including scripts in NLP, as well as the discovery process for, and format of, scripts for the purposes of processing coreference and inferencing which are required, for example, in high-end Q&A and IE applications. 
We argue that the detection of entailment and contradiction relations between texts is a minimal metric for the evaluation of text understanding systems. Intensionality, which is widespread in natural language, raises a number of detection issues that cannot be brushed aside. We describe a contexted clausal representation, derived from approaches in formal semantics, that permits an extended range of intensional entailments and contradictions to be tractably detected. 
We present an implemented model of story understanding and apply it to the understanding of a children’s story. We argue that understanding a story consists of building multirepresentation models of the story and that story models are efﬁciently constructed using a satisﬁability solver. We present a computer program that contains multiple representations of commonsense knowledge, takes a narrative as input, transforms the narrative and representations of commonsense knowledge into a satisﬁability problem, runs a satisﬁability solver, and produces models of the story as output. The narrative, models, and representations are expressed in the language of Shanahan’s event calculus. 
The paper1 presents a lightweight knowledgebased reasoning framework for the JAVELIN open-domain Question Answering (QA) system. We propose a constrained representation of text meaning, along with a ﬂexible uniﬁcation strategy that matches questions with retrieved passages based on semantic similarities and weighted relations between words.  framework based on semantic similarities and weighted relations between words. We obtain a lightweight roboust mechanism to match questions with answer candidates. The organization of the paper is as follows: Section 2 brieﬂy presents system components; Section 3 discusses syntactic processing strategies; Sections 4 and 5 describe our preliminary semantic representation and the uniﬁcation framework which assigns conﬁdence values to answer candidates. The ﬁnal section contains a summary and future plans.  
Using a speciﬁc example of a newspaper commentary, the paper explores the relationship between ’surface-oriented’ and ’deep’ analysis for purposes such as text summarization. The discussion is followed by a description of our ongoing work on automatic commentary understanding and the current state of the implementation.  This goal does not seem reachable with methods of shallow analysis alone. But why exactly is it not, and what methods are needed in addition? In the following, we work through a sample commentary and analyse the steps and the knowledge necessary to arrive at the desired result, i.e., a concise summary. Thereafter, we sketch the state of our implementation work, which follows the goal of fusing surface-based methods with knowledge-based analysis. 2 Sample commentary  
We present the architecture and data model for TEXTRACT, a document analysis framework for text analysis components. The framework and components have been deployed in research and industrial environments for text analysis and text mining tasks. 
The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field analyses from shallow components. This integration increases robustness, directs the search space and hence reduces processing time of the deep parser. In this paper, we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture. The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications. 
This paper reports work aimed at developing an open, distributed learning environment, OLLIE, where researchers can experiment with different Machine Learning (ML) methods for Information Extraction. Once the required level of performance is reached, the ML algorithms can be used to speed up the manual annotation process. OLLIE uses a browser client while data storage and ML training is performed on servers. The different ML algorithms use a uniﬁed programming interface; the integration of new ones is straightforward. 
This paper describes the outline of a linguistic annotation framework under development by ISO TC37 SC WG1-1. This international standard will provide an architecture for the creation, annotation, and manipulation of linguistic resources and processing software. The outline described here results from a meeting of approximately 20 experts in the field, who determined the principles and fundamental structure of the framework. The goal is to provide maximum flexibility for encoders and annotators, while at the same time enabling interchange and re-use of annotated linguistic resources. 
Natural Language Processing (NLP) system developers face a number of new challenges. Interest is increasing for real-world systems that use NLP tools and techniques. The quantity of text now available for training and processing is increasing dramatically. Also, the range of languages and tasks being researched continues to grow rapidly. Thus it is an ideal time to consider the development of new experimental frameworks. We describe the requirements, initial design and exploratory implementation of a high performance NLP infrastructure. 
In Natural Language Processing (NLP), research results from software engineering and software technology have often been neglected. This paper describes some factors that add complexity to the task of engineering reusable NLP systems (beyond conventional software systems). Current work in the area of design patterns and composition languages is described and claimed relevant for natural language processing. The beneﬁts of NLP componentware and barriers to reuse are outlined, and the dichotomies “system versus experiment” and “toolkit versus framework” are discussed. It is argued that in order to live up to its name language engineering must not neglect component quality and architectural evaluation when reporting new NLP research.  Figure 1: Dimensions of Language Engineering Complexity.  
Information extraction (IE) systems assist analysts to assimilate information from electronic documents. This paper focuses on IE tasks designed to support information discovery applications. Since information discovery implies examining large volumes of documents drawn from various sources for situations that cannot be anticipated a priori, they require IE systems to have breadth as well as depth. This implies the need for a domain-independent IE system that can easily be customized for specific domains: end users must be given tools to customize the system on their own. It also implies the need for defining new intermediate level IE tasks that are richer than the subject-verb-object (SVO) triples produced by shallow systems, yet not as complex as the domain-specific scenarios defined by the Message Understanding Conference (MUC). This paper describes a robust, scalable IE engine designed for such purposes. It describes new IE tasks such as entity profiles, and concept-based general events which represent realistic goals in terms of what can be accomplished in the near-term as well as providing useful, actionable information. These new tasks also facilitate the correlation of output from an IE engine with existing structured data. Benchmarking results for the core engine and applications utilizing the engine are presented. 
The paper presents a system architecture for the automatic generation of interface speciﬁcations from ontologies.1 The ensuing interfaces (XML schema deﬁnitions) preserve a signiﬁcant amount of the knowledge originally encoded in the ontology. The approach is relevant for the engineering of large-scale language technology systems. It has been successfully deployed in a complex multi-modal dialogue system SMARTKOM. 
IBM Research has over 200 people working on Unstructured Information Management (UIM) technologies with a strong focus on HLT. Spread out over the globe they are engaged in activities ranging from natural language dialog to machine translation to bioinformatics to open-domain question answering. An analysis of these activities strongly suggested that improving the organization’s ability to quickly discover each other's results and rapidly combine different technologies and approaches would accelerate scientific advance. Furthermore, the ability to reuse and combine results through a common architecture and a robust software framework would accelerate the transfer of research results in HLT into IBM’s product platforms. Market analyses indicating a growing need to process unstructured information, specifically multi-lingual, natural language text, coupled with IBM Research’s investment in HLT, led to the development of middleware architecture for processing unstructured information dubbed UIMA. At the heart of UIMA are powerful search capabilities and a data-driven framework for the development, composition and distributed deployment of analysis engines. In this paper we give a general introduction to UIMA focusing on the design points of its analysis engine architecture and we discuss how UIMA is helping to accelerate research and technology transfer.  
Modern dialog and information systems are increasingly based on distributed component architectures to cope with all kinds of heterogeneity and to enable ﬂexible re-use of existing software components. This contribution presents the MULTIPLATFORM testbed as a powerful framework for the development of integrated multimodal dialog systems. The paper provides a general overview of our approach and explicates its foundations. It describes advanced sample applications that have been realized using the integration platform and compares our approach to related works. 
We present the system description language SDL that offers a declarative way of specifying new complex NLP systems from already existing modules with the help of three operators: sequence, parallelism, and unrestricted iteration. Given a system description and modules that implement a minimal interface, the SDL compiler returns a running Java program which realizes exactly the desired behavior of the original speci£cation. The execution semantics of SDL is complemented by a precise formal semantics, de£ned in terms of concepts of function theory. The SDL compiler is part of the SProUT shallow language platform, a system for the development and processing of multilingual resources. 
In a multimodal conversation, user referring patterns could be complex, involving multiple referring expressions from speech utterances and multiple gestures. To resolve those references, multimodal integration based on semantic constraints is insufficient. In this paper, we describe a graph-based probabilistic approach that simultaneously combines both semantic and temporal constraints to achieve a high performance.  
Research on dialog systems has so far concentrated on interactions between a single user and a machine. In this paper we identify novel research directions arising from multi-party human computer interaction, i.e. scenarios where several human participants interact with a dialog system. 
We review existing types of dialogue managers (DMs), and propose that the Information State (IS) approach may allow both complexity of dialogue and ease of portability. We discuss implementational drawbacks of the only existing IS DM, and describe our work underway to develop a new DM resolving those drawbacks. 
Lockheed Martin’s Advanced Technology Laboratories has been designing, developing, testing, and evaluating spoken language understanding systems in several unique operational environments over the past ﬁve years. Through these experiences we have encountered numerous challenges in making each system become an integral part of a user’s operations. In this paper, we discuss these challenges and report how we overcame them with respect to a number of domains.  
This paper describes our vision for a future time when end users of mixed-initiative spoken dialogue systems will be able to dynamically conﬁgure the system to suit their personalized goals. We argue that spoken dialogue systems will only become a common utility in society once they can be reconﬁgured, essentially instantaneously, to support a new working vocabulary within a new domain or subdomain. For example, if a user is interested in restaurants in Seattle, the system would go off-line to gather information from resources such as the Web, and would infer from that knowledge an appropriate working vocabulary, language models, and dialogue control mechanism for a subsequent spoken conversation on this topic. In addition to painting this vision, the paper also discusses our recent research efforts directed towards the technology development necessary to realize this larger goal. 
We introduce a method for using images for word sense disambiguation, either alone, or in conjunction with traditional text based methods. The approach is based in recent work on a method for predicting words for images which can be learned from image datasets with associated text. When word prediction is constrained to a narrow set of choices such as possible senses, it can be quite reliable, and we use these predictions either by themselves or to reinforce standard methods. We provide preliminary results on a subset of the Corel image database which has three to five keywords per image. The subset was automatically selected to have a greater portion of keywords with sense ambiguity and the word senses were hand labeled to provide ground truth for testing. Results on this data strongly suggest that images can help with word sense disambiguation. 
We discuss the properties of a collection of news photos and captions, collected from the Associated Press and Reuters. Captions have a vocabulary dominated by proper names. We have implemented various text clustering algorithms to organize these items by topic, as well as an iconic matcher that identiﬁes articles that share a picture. We have found that the special structure of captions allows us to extract some names of people actually portrayed in the image quite reliably, using a simple syntactic analysis. We have been able to build a directory of face images of individuals from this collection. 
 We propose a computational model of visually-grounded spatial language understanding, based on a study of how people verbally describe objects in visual scenes. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to a broad range of referring expressions for a large percentage of test cases. In an analysis of the system’s successes and failures we reveal how visual context inﬂuences the semantics of utterances and propose future extensions to the model that take such context into account. 
We present on-going work on the topic of learning translation models between image data and text (English) captions. Most approaches to this problem assume a one-to-one or a ﬂat, oneto-many mapping between a segmented image region and a word. However, this assumption is very restrictive from the computer vision standpoint, and fails to account for two important properties of image segmentation: 1) objects often consist of multiple parts, each captured by an individual region; and 2) individual regions are often over-segmented into multiple subregions. Moreover, this assumption also fails to capture the structural relations among words, e.g., part/whole relations. We outline a general framework that accommodates a many-to-many mapping between image regions and words, allowing for structured descriptions on both sides. In this paper, we describe our extensions to the probabilistic translation model of Brown et al. (1993) (as in Duygulu et al. (2002)) that enable the creation of structured models of image objects. We demonstrate our work in progress, in which a set of annotated images is used to derive a set of labeled, structured descriptions in the presence of oversegmentation. 
Learning of new words is assisted by contextual information. This context can come in several forms, including observations in nonlinguistic semantic domains, as well as the linguistic context in which the new word was presented. We outline a general architecture for word learning, in which structural alignment coordinates this contextual information in order to restrict the possible interpretations of unknown words. We identify spatial relations as an applicable semantic domain, and describe a system-in-progress for implementing the general architecture using video sequences as our non-linguistic input. For example, when the complete system is presented with “The bird dove to the rock,” with a video sequence of a bird ﬂying from a tree to a rock, and with the meanings for all the words except the preposition “to,” the system will register the unknown “to” with the corresponding aspect of the bird’s trajectory.  words and non-linguistic semantic domain fragments. In particular, we are interested in using the context of the word’s introduction to limit possible interpretations. Assuming linguistic inputs are syntactic multiword utterances (e.g. phrases and sentences), this context includes the semantic and syntactic relationship of the new word to other words in the linguistic input. In a multimodal learning environment, the context also includes input observed in the non-linguistic domain. Our system is designed to coordinate all of these contextual clues in order to restrict the set of possible interpretations of the new word. By leveraging previously learned words to enable the learning of new words, we create a bootstrapping system for word learning. In Section 2, we outline a general architecture based on symbolic structural alignment (Gentner and Markman, 1997) for solving the stated problem. In this outline, we identify necessary subsystems and requirements the system must satisfy. In Section 3, we identify the visual domain of spatial relations as a potential semantic domain, and in Sections 4–6 we describe a system-inprogress which instantiates the general architecture for this semantic domain.  
The objective of this research is to develop a system for miniature language learning based on a minimum of prewired language-specific functionality, that is compatible with observations of perceptual and language capabilities in human development. In the proposed system, meaning is extracted from video images based on detection of physical contact and its parameters. Mapping of sentence form to meaning is performed by learning grammatical constructions that are retrieved from a construction inventory based on the constellation of closed class items uniquely identifying the target sentence structure. The resulting system displays robust acquisition behavior that reproduces certain observations from developmental studies, with very modest “innate” language specificity. 1. Introduction Feldman et al. (1990) posed the problem of "miniature" language acquisition based on <sentence, image> pairs as a "touchstone" for cognitive science. In this task, an artificial system is confronted with a reduced version of the problem of language acquisition faced by the child, that involves both the extraction of meaning from the image, and the mapping of the paired sentence onto this meaning. Extraction of Meaning In this developmental context, Mandler (1999) suggested that the infant begins to construct meaning from the scene based on the extraction of perceptual primitives. From simple representations such as contact, support, attachment (Talmy 1988) the infant could construct progressively more elaborate representations of visuospatial meaning. Thus, the physical event "collision" is a form of the perceptual primitive “contact”. Kotovsky & Baillargeon (1998) observed that at 6 months, infants demonstrate sensitivity to the parameters of objects involved in a collision, and the resulting effect on the  collision, suggesting indeed that infants can represent contact as an event predicate with agent and patient arguments. Siskind (2001) has demonstrated that force dynamic primitives of contact, support, attachment can be extracted from video event sequences and used to recognize events including pick-up, put-down, and stack based on their characterization in an event logic. The use of these intermediate representations renders the system robust to variability in motion and view parameters. Most importantly, Siskind demonstrated that the lexical semantics for a number of verbs could be established by automatic image processing. Sentence to meaning mapping: Once meaning is extracted from the scene, the significant problem of mapping sentences to meanings remains. The nativist perspective on this problem holds that the <sentence, meaning> data to which the child is exposed is highly indeterminate, and underspecifies the mapping to be learned. This “poverty of the stimulus” is a central argument for the existence of a genetically specified universal grammar, such that language acquisition consists of configuring the UG for the appropriate target language (Chomsky 1995). In this framework, once a given parameter is set, its use should apply to new constructions in a generalized, generative manner. An alternative functionalist perspective holds that learning plays a much more central role in language acquisition. The infant develops an inventory of grammatical constructions as mappings from form to meaning (Goldberg 1995). These constructions are initially rather fixed and specific, and later become generalized into a more abstract compositional form employed by the adult (Tomasello 1999). In this context, construction of the relation between perceptual and  cognitive representations and grammatical form plays a central role in learning language (e.g. Feldman et al. 1990, 1996; Langacker 1991; Mandler 1999; Talmy 1998). These issues of learnability and innateness have provided a rich motivation for simulation studies that have taken a number of different forms. Elman (1990) demonstrated that recurrent networks are sensitive to predictable structure in grammatical sequences. Subsequent studies of grammar induction demonstrate how syntactic structure can be recovered from sentences (e.g. Stolcke & Omohundro 1994). From the “grounding of language in meaning” perspective (e.g. Feldman et al. 1990, 1996; Langacker 1991; Goldberg 1995) Chang & Maia (2001) exploited the relations between action representation and simple verb frames in a construction grammar approach. In effort to consider more complex grammatical forms, Miikkulainen (1996) demonstrated a system that learned the mapping between relative phrase constructions and multiple event representations, based on the use of a stack for maintaining state information during the processing of the next embedded clause in a recursive manner. In a more generalized approach, Dominey (2000) exploited the regularity that sentence to meaning mapping is encoded in all languages by word order and grammatical marking (bound or free) (Bates et al. 1982). That model was based on the functional neurophysiology of cognitive sequence and language processing and an associated neural network model that has been demonstrated to simulate interesting aspects of infant (Dominey & Ramus 2000) and adult language processing (Dominey et al. 2003). Objectives The goals of the current study are fourfold: First to test the hypothesis that meaning can be extracted from visual scenes based on the detection of contact and its parameters in an approach similar to but significantly simplified from Siskind (2001); Second to determine whether the model of Dominey (2000) can be extended to handle embedded relative clauses; Third to demonstrate that these two systems can be combined to perform miniature language acquisition; and finally to demonstrate that the combined system can provide insight into the developmental progression in human language acquisition without the necessity of a pre-wired parameterized grammar system (Chomsky 1995). The Training Data The human experimenter enacts and simultaneously narrates visual scenes made up of events that occur between a red cylinder, a green block and a blue  semicircle or “moon” on a black matte table surface. A video camera above the surface provides a video image that is processed by a color-based recognition and tracking system (Smart – Panlab, Barcelona Spain) that generates a time ordered sequence of the contacts that occur between objects that is subsequently processed for event analysis (below). The simultaneous narration of the ongoing events is processed by a commercial speech-totext system (IBM ViaVoiceTM). Speech and vision data were acquired and then processed off-line yielding a data set of matched sentence – scene pairs that were provided as input to the structure mapping model. A total of ~300 <sentence, scene> pairs were tested in the following experiments. 2. Visual Scenes and analysis For a given video sequence the visual scene analysis generates the corresponding event description in the format event(agent, object, recipient). Single Event Labeling Events are defined in terms of contacts between elements. A contact is defined in terms of the time at which it occurred, the agent, object, and duration of the contact. The agent is determined as the element that had a larger relative velocity towards the other element involved in the contact. Based on these parameters of contact, scene events are recognized as follows: Touch(agent, object): A single contact, in which (a) the duration of the contact is inferior to touch_duration (1.5 seconds), and (b) the object is not displaced during the duration of the contact. Push(agent, object): A single contact in which (a) the duration of the contact is superior or equal to touch_duration and inferior to take_duration (5 sec), (b) the object is displaced during the duration of the contact, and (c) the agent and object are not in contact at the end of the event. Take(agent, object): A single contact in which (a) the duration of contact is superior or equal to take_duration, (b) the object is displaced during the contact, and (c) the agent and object remain in contact. Take(agent, object, source): Multiple contacts, as the agent takes the object from the source. For the first contact between the agent and the object (a) the duration of contact is superior or equal to take_duration, (b) the object is displaced during the contact, and (c) the agent and object remain in contact. For the optional second contact between the agent and the source (a) the duration of the contact is inferior to take_duration, and (b) the agent and source do not remain in contact. Finally, contact between the object and source is broken during the event.  Give(agent, object, recipient): In this multiple contact event, the agent first takes the object, and then gives the object to the recipient. For the first contact between the agent and the object (a) the duration of contact is inferior to take_duration, (b) the object is displaced during the contact, and (c) the agent and object do not remain in contact. For the second contact between the object and the recipient (a) the duration of the contact is superior to take_duration, and (b) the object and recipient remain in contact. For the third (optional) contact between the agent and the recipient (a) the duration of the contact is inferior to take_duration and thus the elements do not remain in contact. These event labeling templates form the basis for a template matching algorithm that labels events based on the contact list, similar to the spanning interval and event logic of Siskind (2001). Complex “Hierarchical” Events The events described above are simple in the sense that there have no hierarchical structure. This imposes serious limitations on the syntactic complexity of the corresponding sentences (Feldman et al. 1996, Miikkulainen 1996). The sentence “The block that pushed the moon was touched by the triangle” illustrates a complex event that exemplifies this issue. The corresponding compound event will be recognized and represented as a pair of temporally successive simple event descriptions, in this case: push(block, moon), and touch(triangle, block). The “block” serves as the link that connects these two simple events in order to form a complex hierarchical event. 3. Structure mapping for language learning The mapping of sentence form onto meaning (Goldberg 1995) takes place at two distinct levels: Words are associated with individual components of event descriptions, and grammatical structure is associated with functional roles within scene events. The first level has been addressed by Siskind (1996), Roy & Pentland (2000) and Steels (2001) and we treat it here in a relatively simple but effective manner. Our principle interest lies more in the second level of mapping between scene and sentence structure. Word Meaning In the initial learning phases there is no influence of syntactic knowledge and the word-referent associations are stored in the WordToReferent matrix (Eqn 1) by associating every word with every referent in the current scene (α = 0), exploiting the cross-situational regularity (Siskind 1996) that a given word will have a higher  coincidence with referent to which it refers than with other referents. This initial word learning contributes to learning the mapping between sentence and scene structure (Eqn. 4, 5 & 6 below). Then, knowledge of the syntactic structure, encoded in FormToMeaning can be used to identify the appropriate referent (in the SEA) for a given word (in the OCA), corresponding to a non-zero value of α in Eqn. 1. In this “syntactic bootstrapping” for the new word “gugle,” for example, syntactic knowledge of Agent-Event-Object structure of the sentence “John pushed the gugle” can be used to assign “gugle” to the object of push.  WordToReferent(i,j) = WordToReferent(i,j) +  OCA(k,i) * SEA(m,j) *  αFormToMeaning(m,k)  (1)  FormToMeaning Construction Inventory  WordToReferent  Predicted Referents Array (PRA) Action Agent Object Recipient Scene Event Array (SEA)  Open Class Array (OCA)  Construction Index Closed class words  Visual Scene Analysis  Speech Input Processing  Figure 1. Structure-Mapping Architecture. Open class words in OCA are translated to Predicted Referents in the PRA via the WorldToReferent mapping. PRA elements are mapped onto their roles in the SEA by the FormToMeaning mapping, specific to each sentence type. This mapping is retrieved from Construction Inventory, via the ConstructionIndex that encodes the closed class words that characterize each sentence type. Open vs Closed Class Word Categories Our approach is based on the cross-linguistic observation that open class words (e.g. nouns, verbs, adjectives and adverbs) are assigned to their thematic roles based on word order and/or grammatical function words or morphemes (Bates et al. 1982). Newborn infants are sensitive to the perceptual properties that distinguish these two categories (Shi et al. 1999), and in adults, these categories are processed by dissociable neurophysiological systems (Brown et al. 1999).  Similarly, artificial neural networks can also learn to make this function/content distinction (Morgan et al. 1996). Thus, for the speech input that is provided to the learning model open and closed class words are directed to separate processing streams that preserve their order and identity, as indicated in Figure 1. Note that by making this dissociation between open and closed class elements, the grammar learning problem is substantially simplified. Again, it is thus of interest that newborn infants can perform this lexical categorization (Shi et al. 1999), and we have recently demonstrated that a recurrent network of leaky integrator neurons can categorize open and closed class words based on the structure of the F0 component of the speech signal in French and English (Blanc, Dodane & Dominey 2003). Mapping Sentence to Meaning In terms of the architecture in Figure 2, this mapping can be characterized in the following successive steps. First, words in the Open Class Array are decoded into their corresponding scene referents (via the WordToReferent mapping) to yield the Predicted Referents Array that contains the translated words while preserving their original order from the OCA (Eqn 2).  n ∑ PRA(k,j) = OCA(k,i) * WordToReferent(i,j) (2) i=1 Next, each sentence type will correspond to a specific form to meaning mapping between the PRA and the SEA. encoded in the FormToMeaning array. The problem will be to retrieve for each sentence type, the appropriate corresponding FormToMeaning mapping. To solve this problem, we recall that each sentence type will have a unique constellation of closed class words and/or bound morphemes (Bates et al. 1982) that can be coded in a ConstructionIndex (Eqn.3) that forms a unique identifier for each sentence type. Thus, the appropriate FormToMeaning mapping for each sentence type can be indexed in ConstructionInventory by its corresponding ConstructionIndex.  ConstructionIndex = fcircularShift(ConstructionIndex,  FunctionWord)  (3)  The link between the ConstructionIndex and the corresponding FormToMeaning mapping is established as follows. As each new sentence is processed, we first reconstruct the specific FormToMeaning mapping for that sentence (Eqn 4), by mapping words to referents (in PRA) and referents to scene elements (in SEA). The resulting, FormToMeaningCurrent encodes the correspondence between word order (that is preserved in the PRA Eqn 2) and thematic roles in the SEA. Note that the quality of FormToMeaningCurrent will depend on the quality of  acquired word meanings in WordToReferent. Thus, syntactic learning requires a minimum baseline of semantic knowledge.  FormToMeaningCurrent(m,k) =  n ∑ PRA(k,i)*SEA(m,i)  (4)  i=1  Given the FormToMeaningCurrent mapping for the current sentence, we can now associate it in the ConstructionInventory with the corresponding function word configuration or ConstructionIndex for that sentence, expressed in (Eqn 5).  ConstructionInventory(i,j) = ConstructionInventory(i,j)  + ConstructionIndex(i)  * FormToMeaning-Current(j)  (5)  Finally, once this learning has occurred, for new sentences we can now extract the FormToMeaning mapping from the learned ConstructionInventory by using the ConstructionIndex as an index into this associative memory, illustrated in Eqn. 6.  FormToMeaning(i) =  n ∑  ConstructionInventory(i,j)  *  ConstructinIndex(j)  (6)  i=1  To accommodate the dual scenes for complex events Eqns. 4-7 are instantiated twice each, to represent the two components of the dual scene. In the case of simple scenes, the second component of the dual scene representation is null. We evaluate performance by using the WordToReferent and FormToMeaning knowledge to construct for a given input sentence the “predicted scene”. That is, the model will construct an internal representation of the scene that should correspond to the input sentence. This is achieved by first converting the Open-Class-Array into its corresponding scene items in the PredictedReferents-Array as specified in Eqn. 2. The referents are then re-ordered into the proper scene representation via application of the FormToMeaning transformation as described in Eqn. 7.  PSA(m,i) = PRA(k,i) * FormToMeaning(m,k) (7)  When learning has proceeded correctly, the predicted scene array (PSA) contents should match those of the scene event array (SEA) that is directly derived from input to the model. We then quantify performance error in terms of the number of mismatches between PSA and  SEA. 4. Experimental results Hirsh-Pasek & Golinkof (1996) indicate that children can use knowledge of word meaning to acquire a fixed SVO template around 18 months, and then expand this to non-canonical sentence forms around 24+ months. Tomasello (1999) similarly indicates that fixed grammatical constructions will be used initially, and that these will then provide the basis for the development of more generalized constructions (Goldberg 1995). The following experiments attempt to follow this type of developmental progression. A. Learning of Active Forms for Simple Events 1. Active: The block pushed the triangle. 2. Dative: The block gave the triangle to the moon. For this experiment, 17 scene/sentence pairs were generated that employed the 5 different events, and narrations in the active voice, corresponding to the grammatical forms 1 and 2. The model was trained for 32 passes through the 17 scene/sentence pairs for a total of 544 scene/sentence pairs. During the first 200 scene/sentence pair trials, α in Eqn. 1 was 0 (i.e. no syntactic bootstrapping before syntax is adquired), and thereafter it was 1. This was necessary in order to avoid the random effect of syntactic knowledge on semantic learning in the initial learning stages. The trained system displayed error free performance for all 17 sentences, and generalization to new sentences that had not previously been tested. B. Passive forms This experiment examined learning active and passive grammatical forms, employing grammatical forms 1-4. Word meanings were used from Experiment A, so only the structural FormToMeaning mappings were learned. 3. Passive: The triangle was pushed by the block. 4. Dative Passive: The moon was given to the triangle by the block. Seventeen new scene/sentence pairs were generated with active and passive grammatical forms for the narration. Within 3 training passes through the 17 sentences (51 scene/sentence pairs), error free performance was achieved, with confirmation of error free generalization to new untrained sentences of these types. The rapid learning indicates the importance of lexicon in establishing the form to meaning mapping for the grammatical constructions.  C. Relative forms for Complex Events Here we consider complex scenes narrated by sentences with relative clauses. Eleven complex scene/sentence pairs were generated with narration corresponding to the grammatical forms indicated in 5 – 10: 5. The block that pushed the triangle touched the moon. 6. The block pushed the triangle that touched the moon. 7. The block that pushed the triangle was touched by the moon. 8. The block pushed the triangle that was touched the moon. 9. The block that was pushed by the triangle touched the moon. 10. The block was pushed by the triangle that touched the moon. After presentation of 88 scene/sentence pairs, the model performed without error for these 6 grammatical forms, and displayed error-free generalization to new sentences that had not been used during the training for all six grammatical forms. D. Combined Test with and Without Lexicon A total of 27 scene/sentence pairs, used in Experiments B and C, were employed that exercised the ensemble of grammatical forms 1 – 10 using the learned WordToReferent mappings. After exposure to 162 scene/sentence pairs the model performed and generalized without error. When this combined test was performed without the pre-learned lexical mappings in WordToReferent, the system failed to converge, illustrating the advantage of following the developmental progression from lexicon to simple to complex grammatical structure. This also illustrates the importance of interaction between syntactic and semantic knowledge that is treated in more detail in Dominey (2000). E. Some Scaling Issues A small lexicon and construction inventory are used to illustrate the system behavior. Based on the independant representation formats, the architecture should scale well. The has now been tested with a larger lexicon, and has learned over 35 grammatical constructions. The system should extend to all languages in which sentence to meaning mapping is encoded by word order and/or grammatical marking (Bates et al. 1982). In the current study, deliberate human event production yielded essentially perfect recognition, though the learning model is relatively robust (Dominey 2000) to elevated scene error rates.  F. Representing Hierarchical Structure The knowledge of the system is expressed in the WorldToReferent and FormToMeaning matrices. In order to deal with complex sentences with embedded clauses, it is necessary to use this same knowledge at different levels of the hierarchy. For this, a "branching mechanism" is necessary, that ordinates the input and output vectors corresponding to meaning and word events. An effective solution to that problem is to learn the branching for each construction as we have done. However, a real account of the human faculty of recursion should be both general (i.e. it should apply to any reasonably complex structure) and plausible (i.e. the branching mechanism should be connectionist). In order to provide this level of generality, neural models need to include a logical "stack" (cf Miikkulainen 1996), in order to process the context of embedded sentences. Complex structures themselves may be represented in a connectionist way, using the Recursive Auto-Associative Memory (Pollack, 1990). In (Voegtlin and Dominey 2003), we proposed a representation system for complex events, that is both generative (it can handle any structure) and systematic (it can generalize, and it does so in a compositional way). This system could be used here, as its representation readily provides a caserole system. The advantages are twofold. First, the branching mechanism is implemented in a neurally realistic way. Second, the recursion capability of the system will allow it to apply its knowledge to any sentence form, whether known or new. Future research will address this issue.  construction inventory is being built up. This forms the basis for the infant’s subsequent ability to de- and recompose these constructions in a truly compositional manner, a topic of future research. Acknowledgments Supported by the EuroCores OMLL project, the French ACI Integrative and Computational Neuroscience Project, and the HFSP MCILA Project. Appendix: Sentence and scene descriptions The <sentence, meaning> pairs for training and testing are constructed from the following templates. The lexicon consists of 5 nouns (cylinder, moon, block, cat, dog), 5 verbs (touch, push, take, give, say), and 8 function words (to, by, from, was, that, it, itself, and)1. A.1 Single event scenes 1. Agent verb object. (Active) Verb(agent, object) 2. Object was verbed by agent. (Passive) Verb(agent, object). 3. Agent verbed object to recipient. (Dative) Verb(agent, object, recipient) 4. Object was verbed to recipient by agent. (Dative passive) Action1(agent1, object2, recipient3). 5. Agent1 action1 recipient3 object2. Verb(agent, object, recipient).  Conclusion The current study demonstrates (1) that the perceptual primitive of contact (available to infants at 5 months), can be used to perform event description in a manner that is similar to but significantly simpler than Siskind (2001), (2) that a novel implementation of principles from construction grammar can be used to map sentence form to these meanings together in an integrated system, (3) that relative clauses can be processed in a manner that is similar to, but requires less specific machinery (e.g. no stack) than that in Miikkalanian (1996), and finally (4) that the resulting system displays robust acquisition behavior that reproduces certain observations from developmental studies with very modest “innate” language specificity. Note that one could have taken the same approach by integrating Siskind’s (2001) full event system, and Miikkulainen’s (1996) embedded case-role system. Each of these however required significant architectural complexity to accomplish the full job. The current goal was to identify minimal event recognition and form-tomeaning mapping capabilities that could be integrated into a coherent system that performs at the level of a human infant in the first years of development when the  A.2 Double event relatives 6. Agent1 that verb1ed object2 verb2ed object3. (Relative agent). Action1(agent1,object2), Action2(agent1,object3) 7. Object3 was action2ed by agent1 that action1ed object2. (Relative object). Action1(agent1,object2), Action2(agent1,object3) 8. Agent1 that action21ed object2 was action22ed by agent3 Action1(agent1,object2), Action2(agent3,object1) 9. Agent3 action2ed object1 that action1ed object2 Action1(agent1,object2), Action2(agent3,object1) 10. Obj2 that was action1ed by agent1 action2ed obj3 Action1(agent1,object2), Action2(agent2,object3) 11. Obj3 was act2d by agent2 that was act1d by agent1 Action1(agent1,object2), Action2(agent2,object3) 12. Obj2 that was action1ed by agent1 was action2ed by ag3 Action1(agent1,object2), Action2(agent3,object2) 13. ag3 act22ed obj2 that was act21ed by ag1 Action21(agent1,object2), Action22(agent3,object2) 
This paper introduces an open computational framework for visual perception and grounded language acquisition called Experience-Based Language Acquisition (EBLA). EBLA can “watch” a series of short videos and acquire a simple language of nouns and verbs corresponding to the objects and object-object relations in those videos. Upon acquiring this protolanguage, EBLA can perform basic scene analysis to generate descriptions of novel videos. The performance of EBLA has been evaluated based on accuracy and speed of protolanguage acquisition as well as on accuracy of generated scene descriptions. For a test set of simple animations, EBLA had average acquisition success rates as high as 100% and average description success rates as high as 96.7%. For a larger set of real videos, EBLA had average acquisition success rates as high as 95.8% and average description success rates as high as 65.3%. The lower description success rate for the videos is attributed to the wide variance in the appearance of objects across the test set. While there have been several systems capable of learning object or event labels for videos, EBLA is the first known system to  acquire both nouns and verbs using a grounded computer vision system. 
 We study the problem of learning to recognise objects in the context of autonomous agents. We cast object recognition as the process of attaching meaningful concepts to speciﬁc regions of an image. In other words, given a set of images and their captions, the goal is to segment the image, in either an intelligent or naive fashion, then to ﬁnd the proper mapping between words and regions. In this paper, we demonstrate that a model that learns spatial relationships between individual words not only provides accurate annotations, but also allows one to perform recognition that respects the real-time constraints of an autonomous, mobile robot. 
We consider the problem of how the meanings of words can be grounded in sensor data. A probabilistic representation for the meanings of words is deﬁned, a method for recovering meanings from observational information about word use in the face of referential uncertainty is described, and empirical results with real utterances and robot sensor data are presented.  
How can we build robots that engage in ﬂuid spoken conversations with people, moving beyond canned responses to words and towards actually understanding? As a step towards addressing this question, we introduce a robotic architecture that provides a basis for grounding word meanings. The architecture provides perceptual, procedural, and affordance representations for grounding words. A perceptuallycoupled on-line simulator enables sensorymotor representations that can shift points of view. Held together, we show that this architecture provides a rich set of data structures and procedures that provide the foundations for grounding the meaning of certain classes of words. 
We present an empirical corpus study of the meaning and usage of time phrases in weather forecasts; this is based on a novel corpus analysis technique where we align phrases from the forecast text with data extracted from a numerical weather simulation. Previous papers have summarised this analysis and discussed the substantial variations we discovered among individual writers, which was perhaps our most surprising ﬁnding. In this paper we describe our analysis procedure and results in considerably more detail, and also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words.  example, by evening apparently meant 1800 to some people, but 0000 to others. Although the possibility of such variation in individual idiolects has been acknowledged in the past (for example, (Nunberg, 1978; Parikh, 1994)), it seems to be ignored by most recent work on lexical semantics. We have published other papers that have summarised our key ﬁndings, notably variation between individuals (Reiter and Sripada, 2002a; Reiter and Sripada, 2002b); and also described the corpus itself (Sripada et al., 2003b). The purpose of this paper is to describe our analysis procedure (including alignment) and results in detail, and to also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words. 2 Previous Research  
Measuring differences between nearsynonyms constitutes a major challenge in the development of electronic dictionaries and natural language processing systems. This paper presents a pilot study on how Population Test Method (PTM) may be used as an effective, empirical tool to define near-synonyms in a quantifiable manner. Use of PTM presumes that all knowledge about lexical meaning in a language resides collectively in the mind(s) of its native speakers, and that this intersubjective understanding may be extracted via targeted surveys that encourage creative, thinking responses. In this paper we show (1) examples of such tests performed on a group of high school students in Finland, (2) resulting data from the tests that is surprisingly quantifiable, and (3) a web-based visualization program we are developing to analyze and present the collected data. 
The audio bitstream in music encodes a high amount of statistical, acoustic, emotional and cultural information. But music also has an important linguistic accessory; most musical artists are described in great detail in record reviews, fan sites and news items. We highlight current and ongoing research into extracting relevant features from audio and simultaneously learning language features linked to the music. We show results in a “query-bydescription” task in which we learn the perceptual meaning of automatically-discovered single-term descriptive components, as well as a method of automatically uncovering ‘semantically attached’ terms (terms that have perceptual grounding.) We then show recent work in ‘semantic basis functions’ – parameter spaces of description (such as fast ... slow or male ... female) that encode the highest descriptive variance in a semantic space. 
This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches. 
The production of accurate and complete multiple-document summaries is challenged by the complexity of judging the usefulness of information to the user. Our aim is to determine whether identifying sub-events in a news topic could help us capture essential information to produce better summaries. In our first experiment, we asked human judges to determine the relative utility of sentences as they related to the subevents of a larger topic. We used this data to create summaries by three different methods, and we then compared these summaries with three automatically created summaries. In our second experiment, we show how the results of our first experiment can be applied to a cluster-based automatic summarization system. Through both experiments, we examine the use of inter-judge agreement and a relative utility metric that accounts for the complexity of determining sentence quality in relation to a topic. 1. Introduction Multiple articles on a particular topic tend to contain redundant information as well as information that is unique to each article. For instance, different news sources covering the same topic may take different angles, or new information may become available in a later report. So, while all the articles are related to the larger topic, each article may be associated with any of several sub-events. We wanted to find a way to capture the unique sub-event information that is characteristic in multiple-document coverage of a single topic. We predicted that breaking documents down to their sub-events and capturing those sentences in each sub-event with the highest utility would produce an accurate, thorough, and diverse multidocument summary.  In our first experiment, we compared six methods of summarization to see which produces the best summaries. The methods included three automatic and three manual methods of producing summaries. We used relative utility to capture and measure subtleties in determining sentence relevance. We created multiple document summaries using both a sub-event based approach and a topic-based approach. Generally, we expected to find that the manual summaries performed better than the automatic summaries. In our second experiment, we designed a multi-document summarizer which relied on a clustering method, and we tested the three policies we devised for creating summaries from the manual summarization technique developed in our first experiment. 2. Related Work Much work has preceded and informed this paper. Allan et al.’s (1998) work on summarizing novelty recognizes that news topics consist of a series of events – what we call “sub-events,” to distinguish the difference between a news topic and its sub-events. However, their method differs in its approach, which uses an algorithm to identify “novel” sentences, rather than the use of human judges. In other related work, sentences are either judged “on-topic” or “off-topic” (Allan et al., 2001a) (Allan et al., 2001b). Carbonell and Goldstein use Maximal Marginal Relevance (MMR) to identify “novel” information to improve query answering results, and they also apply this method to multiple-document summarization (Carbonell and Goldstein, 1997 and Goldstein, 1999). Success in the use of inter-judge agreement has led us to pursue the use of the current evaluation methods. However, this experiment differs from prior work in that we use judges to determine the relevance of sentences to sub-events rather than to evaluate summaries (Radev et al., 2000). Finally, McKeown et al. (1999), Hatzivassiloglou et al. (2001) and Boros et al. (2001) have shown the challenges and potential payoffs of using sentence clustering in extractive summarization.  3. Article Corpus Our study involves two experiments carried out on one corpus of news articles. The article corpus was selected from a cluster of eleven articles describing the 2000 crash of Gulf Air flight 072. From these articles we chose a corpus of five articles, containing a total of 159 sentences. All the articles cover a single news event, the plane crash and its aftermath. The articles were gathered on the web from sources reporting on the event as it unfolded, and come from various news agencies, such as ABC News, Fox News, and the BBC. All of the articles give some discussion of the events leading up to and following the crash, with particular articles focusing on areas of special interest, such as the toll on Egypt, from where many of the passengers had come. The article titles in Table 1, below, illustrate the range of sub-events that are covered under the crash topic.  Article ID Source Date Headline  30  BBC  Aug. 25 Bodies recovered from  Gulf Air crash  41  Fox News Aug. 25 Egyptians Suffer Second  Air Tragedy in a Year  81  USA Today Aug. 25 One American among 143  dead in crash  87  ABC News Aug. 26 Prayers for victims of  Bahrain crash  97  Fox News Aug. 26 Did Pilot Error Cause Air  Crash  Table 1. Corpus article characteristics.  4. Experiment 1: Sub-Event Analysis Our first experiment involved having human judges analyze the sentences in our corpus for degree of saliency to a series of sub-events comprising the topic.  4.1 Description of Sub-Event User Study The goal of this experiment was to study the effectiveness of breaking a news topic down into sub-events, in order to capture not simply salience, but also diversity (Goldstein, 1998). The sub-events were chosen to cover all of the material in the reports and to represent the most significant aspects of the news topic. For the Gulf Air crash, we determined that the sub-events were: 1. The plane takes off 2. Something goes wrong 3. The plane crashes 4. Rescue and recovery effort 5. Gulf Air releases information 6. Government agencies react  7. Friends, relatives and nations mourn 8. Black box(es) are searched for 9. Black box(es) are recovered 10. Black box(es) are sent for analysis  We instructed judges to rank the degree of sentence relevance to each sub-event. Judges were instructed to use a scale, such that a score of ten indicated that the sentence was critical to the sub-event, and a score of 0 indicated that the sentence was irrelevant. Thus, the judges processed the 159 sentences from 5 documents ten times, once pertaining to each sub-event. This experiment produced for each judge 1590 data points which were analyzed according to the methods described in the next section. We used the data on the relevance of the sentences to the sub-events to calculate inter-judge agreement. In this manner, we determined which sentences had the overall highest relevance to each subevent. We used this ranking to produce summaries at different levels of compression.  5. Methods for Producing Summaries To gather data about the effectiveness of dividing news topics into their sub-events for creating summaries, we utilized data from human judges, upon which we manually performed three algorithms. These algorithms and their application are described in detail below. We were interested to determine if the Round Robin method (described below,) which has been used by McKeown et al. (1999), Boros et al. (2001) and by Hatzivassiloglou et al. (2001), was the most effective.  5.1 Sub-Event-Based Algorithms After collecting judges’ scores of relevance for each sentence for each subtopic, we then ranked the sentences according to three different algorithms to create multipledocument summaries. From this data, we created summary extracts using three algorithms, as follows:  •  Algorithm 1) Highest Score Anywhere - pick the  sentence which is most relevant to any subevent, no matter  the subevent; pick the next sentence which is most relevant  to any subevent, etc.  •  Algorithm 2) Sum of All Scores - for each  sentence, sum its relevance score for each cluster, pick  the sentence with the highest sum; then pick the  sentence with the second highest sum, etc.  •  Algorithm 3) Round Robin - pick the sentence  which has the most relevance for subevent 1, pick the  sentence with the most relevance for subevent 2, etc. After  picking 1 sentence from each subevent, pick the sentence  with the 2nd best relevance to subevent 1, etc.  Sub-Event 1  Sub-Event 2  Sub-Event 3  Judge 1 Judge 2 Judge 3 Judge 1 Judge 2 Judge 3 Judge 1 Judge 2 Judge 3 Article 30,  Sentence 1  
This study examines the usefulness of common off the shelf compression software such as gzip in enhancing already existing summaries and producing summaries from scratch. Since the gzip algorithm works by removing repetitive data from a ﬁle in order to compress it, we should be able to determine which sentences in a summary contain the least repetitive data by judging the gzipped size of the summary with the sentence compared to the gzipped size of the summary without the sentence. By picking the sentence that increased the size of the summary the most, we hypothesized that the summary will gain the sentence with the most new information. This hypothesis was found to be true in many cases and to varying degrees in this study. 
We investigate the problem of summarizing text documents that contain errors as a result of optical character recognition. Each stage in the process is tested, the error effects analyzed, and possible solutions suggested. Our experimental results show that current approaches, which are developed to deal with clean text, suffer significant degradation even with slight increases in the noise level of a document. We conclude by proposing possible ways of improving the performance of noisy document summarization. 
We report on the SUM project which applies automatic summarisation techniques to the legal domain. We pursue a methodology based on Teufel and Moens (2002) where sentences are classiﬁed according to their argumentative role. We describe some experiments with judgments of the House of Lords where we have performed automatic linguistic annotation of a small sample set in order to explore correlations between linguistic features and argumentative roles. We use state-of-the-art NLP techniques to perform the linguistic annotation using XML-based tools and a combination of rulebased and statistical methods. We focus here on the predictive capacity of tense and aspect features for a classiﬁer. 
For one document, current summarization systems produce a uniform version of summary for all users. Personalized summarizations are necessary in order to represent users’ preferences and interests. Annotation is getting important for document sharing and collaborative filtering, which in fact record users’ dynamic behaviors compared to traditional steady profiles. In this paper we introduce a new summarization system based on users’ annotations. Annotations and their contexts are extracted to represent features of sentences, which are given different weights for representation of the document. Our system produces two versions of summaries for each document: generic summary without considering annotations and annotation-based summary. Since annotation is a kind of personal data, annotation-based summary is tailored to user’s interests to some extent. We show by experiments that annotations can help a lot in improving summarization performance compared to no annotation consideration. At the same time, we make an extensive study on users’ annotating behaviors and annotations distribution, and propose a variety of techniques to evaluate the relationships between annotations and summaries, such as how the number of annotations affects the summarizing performance. A study about collaborative filtering is also made to evaluate the summarization based on annotations of similar users. *This work was done when the first author visited Microsoft Research Asia.  
We describe the outline of Text Summarization Challenge 2 (TSC2 hereafter), a sequel text summarization evaluation conducted as one of the tasks at the NTCIR Workshop 3. First, we describe briefly the previous evaluation, Text Summarization Challenge (TSC1) as introduction to TSC2. Then we explain TSC2 including the participants, the two tasks in TSC2, data used, evaluation methods for each task, and brief report on the results. Keywords: automatic text summarization, summarization evaluation 
We present a new approach to summary evaluation which combines two novel aspects, namely (a) content comparison between gold standard summary and system summary via factoids, a pseudo-semantic representation based on atomic information units which can be robustly marked in text, and (b) use of a gold standard consensus summary, in our case based on 50 individual summaries of one text. Even though future work on more than one source text is imperative, our experiments indicate that (1) ranking with regard to a single gold standard summary is insuﬃcient as rankings based on any two randomly chosen summaries are very dissimilar (correlations average ρ = 0.20), (2) a stable consensus summary can only be expected if a larger number of summaries are collected (in the range of at least 30-40 summaries), and (3) similarity measurement using unigrams shows a similarly low ranking correlation when compared with factoid-based ranking. 
Automatic Multi-Document summarization is still hard to realize. Under such circumstances, we believe, it is important to observe how humans are doing the same task, and look around for different strategies. We prepared 100 document sets similar to the ones used in the DUC multi-document summarization task. For each document set, several people prepared the following data and we conducted a survey. A) Free style summarization B) Sentence Extraction type summarization C) Axis (type of main topic) D) Table style summary In particular, we will describe the last two in detail, as these could lead to a new direction for multisummarization research. 
In this paper we present an empirical study of the potential and limitation of sentence extraction in text summarization. Our results show that the single document generic summarization task as defined in DUC 2001 needs to be carefully refocused as reflected in the low inter-human agreement at 100-word 1 (0.40 score) and high upper bound at full text 2 (0.88) summaries. For 100-word summaries, the performance upper bound, 0.65, achieved oracle extracts3. Such oracle extracts show the promise of sentence extraction algorithms; however, we first need to raise inter-human agreement to be able to achieve this performance level. We show that compression is a promising direction and that the compression ratio of summaries affects average human and system performance. 
This paper presents a new approach to syntactic disambiguation based on lexicalized grammars. While existing disambiguation models decompose the probability of parsing results into that of primitive dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 
This paper introduces a novel Support Vector Machines (SVMs) based voting algorithm for reranking, which provides a way to solve the sequential models indirectly. We have presented a risk formulation under the PAC framework for this voting algorithm. We have applied this algorithm to the parse reranking problem, and achieved labeled recall and precision of 89.4%/89.8% on WSJ section 23 of Penn Treebank. 
We describe new features and algorithms for HPSG parse selection models and address the task of creating annotated material to train them. We evaluate the ability of several sample selection methods to reduce the number of annotated sentences necessary to achieve a given level of performance. Our best method achieves a 60% reduction in the amount of training material without any loss in accuracy. 
We explore the idea of creating a subjectivity classiﬁer that uses lists of subjective nouns learned by bootstrapping algorithms. The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences. First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns. Then we train a Naive Bayes classiﬁer using the subjective nouns, discourse features, and subjectivity clues identiﬁed in prior research. The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classiﬁer performed well, achieving 77% recall with 81% precision. 
This paper presents a set of algorithms for distinguishing personal names with multiple real referents in text, based on little or no supervision. The approach utilizes an unsupervised clustering technique over a rich feature space of biographic facts, which are automatically extracted via a language-independent bootstrapping process. The induced clustering of named entities are then partitioned and linked to their real referents via the automatically extracted biographic data. Performance is evaluated based on both a test set of handlabeled multi-referent personal names and via automatically generated pseudonames. 
In this paper, we improve an unsupervised learning method using the ExpectationMaximization (EM) algorithm proposed by Nigam et al. for text classiﬁcation problems in order to apply it to word sense disambiguation (WSD) problems. The improved method stops the EM algorithm at the optimum iteration number. To estimate that number, we propose two methods. In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2. The score of our method is a match for the best public score of this task. Furthermore, our methods were conﬁrmed to be effective also for verb WSD problems. 
This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other’s output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can signiﬁcantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we ﬁnd that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 
Statistical machine learning algorithms have been successfully applied to many natural language processing (NLP) problems. Compared to manually constructed systems, statistical NLP systems are often easier to develop and maintain since only annotated training text is required. From annotated data, the underlying statistical algorithm can build a model so that annotations for future data can be predicted. However, the performance of a statistical system can also depend heavily on the characteristics of the training data. If we apply such a system to text with characteristics different from that of the training data, then performance degradation will occur. In this paper, we examine this issue empirically using the sentence boundary detection problem. We propose and compare several methods that can be used to update a statistical NLP system when moving to a different domain. 
Previous work has argued that memory-based learning is better than abstraction-based learning for a set of language learning tasks. In this paper, we first attempt to generalize these results to a new set of language learning tasks from the area of spoken dialog systems and to a different abstraction-based learner. We then examine the utility of various exceptionality measures for predicting where one learner is better than the other. Our results show that generalization of previous results to our tasks is not so obvious and some of the exceptionality measures may be used to characterize the performance of our learners. 
We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs. The feature set was previously shown to work well in a supervised learning setting, using known English verb classes. In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task. We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties. We ﬁnd that the unsupervised method we tried cannot be consistently applied to our data. However, the semisupervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well. 
This paper reports on experiments in classifying the semantic role annotations assigned to prepositional phrases in both the PENN TREEBANK and FRAMENET. In both cases, experiments are done to see how the prepositions can be classiﬁed given the dataset’s role inventory, using standard word-sense disambiguation features. In addition to using traditional word collocations, the experiments incorporate class-based collocations in the form of WordNet hypernyms. For Treebank, the word collocations achieve slightly better performance: 78.5% versus 77.4% when separate classiﬁers are used per preposition. When using a single classiﬁer for all of the prepositions together, the combined approach yields a signiﬁcant gain at 85.8% accuracy versus 81.3% for wordonly collocations. For FrameNet, the combined use of both collocation types achieves better performance for the individual classiﬁers: 70.3% versus 68.5%. However, classiﬁcation using a single classiﬁer is not effective due to confusion among the ﬁne-grained roles. 
This paper introduces PhraseNet, a contextsensitive lexical semantic knowledge base system. Based on the supposition that semantic proximity is not simply a relation between two words in isolation, but rather a relation between them in their context, English nouns and verbs, along with contexts they appear in, are organized in PhraseNet into Consets; Consets capture the underlying lexical concept, and are connected with several semantic relations that respect contextually sensitive lexical information. PhraseNet makes use of WordNet as an important knowledge source. It enhances a WordNet synset with its contextual information and reﬁnes its relational structure by maintaining only those relations that respect contextual constraints. The contextual information allows for supporting more functionalities compared with those of WordNet. Natural language researchers as well as linguists and language learners can gain from accessing PhraseNet with a word token and its context, to retrieve relevant semantic information. We describe the design and construction of PhraseNet and give preliminary experimental evidence to its usefulness for NLP researches. 
The purpose of this work is to investigate the use of machine learning approaches for conﬁdence estimation within a statistical machine translation application. Speciﬁcally, we attempt to learn probabilities of correctness for various model predictions, based on the native probabilites (i.e. the probabilites given by the original model) and on features of the current context. Our experiments were conducted using three original translation models and two types of neural nets (single-layer and multilayer perceptrons) for the conﬁdence estimation task. 
We describe an approach to tagging a monolingual dictionary with linguistic features. In particular, we annotate the dictionary entries with parts of speech, number, and tense information. The algorithm uses a bilingual corpus as well as a statistical lexicon to ﬁnd candidate training examples for speciﬁc feature values (e.g. plural). Then a similarity measure in the space deﬁned by the training data serves to deﬁne a classiﬁer for unseen data. We report evaluation results for a French dictionary, while the approach is general enough to be applied to any language pair. In a further step, we show that the proposed framework can be used to assign linguistic roles to extracted morphemes, e.g. noun plural markers. While the morphemes can be extracted using any algorithm, we present a simple algorithm for doing so. The emphasis hereby is not on the algorithm itself, but on the power of the framework to assign roles, which are ultimately indispensable for tasks such as Machine Translation. 
In this paper we demonstrate methods of improving both the recall and the precision of automatic methods for extraction of hyponymy (IS A) relations from free text. By applying latent semantic analysis (LSA) to ﬁlter extracted hyponymy relations we reduce the rate of error of our initial pattern-based hyponymy extraction by 30%, achieving precision of 58%. Applying a graph-based model of noun-noun similarity learned automatically from coordination patterns to previously extracted correct hyponymy relations, we achieve roughly a ﬁvefold increase in the number of correct hyponymy relations extracted. 
This paper deﬁnes a general form for classbased probabilistic language models and proposes an efﬁcient algorithm for clustering based on this. Our evaluation experiments revealed that our method decreased computation time drastically, while retaining accuracy. 
Combining a naive Bayes classiﬁer with the EM algorithm is one of the promising approaches for making use of unlabeled data for disambiguation tasks when using local context features including word sense disambiguation and spelling correction. However, the use of unlabeled data via the basic EM algorithm often causes disastrous performance degradation instead of improving classiﬁcation performance, resulting in poor classiﬁcation performance on average. In this study, we introduce a class distribution constraint into the iteration process of the EM algorithm. This constraint keeps the class distribution of unlabeled data consistent with the class distribution estimated from labeled data, preventing the EM algorithm from converging into an undesirable state. Experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classiﬁcation performance when the amount of labeled data is small. 
As part of our work on automatically building knowledge structures from text, we apply machine learning to determine which clauses from multiple narratives describing similar situations should be grouped together as descriptions of the same type of occurrence. Our approach to the problem uses textual similarity and context from other clauses. Besides training data, our system uses only a partial parser as outside knowledge. We present results evaluating the cohesiveness of the aggregated clauses and a brief overview of how this work ﬁts into our overall system. 
In this paper, we describe a system that applies maximum entropy (ME) models to the task of named entity recognition (NER). Starting with an annotated corpus and a set of features which are easily obtainable for almost any language, we ﬁrst build a baseline NE recognizer which is then used to extract the named entities and their context information from additional nonannotated data. In turn, these lists are incorporated into the ﬁnal recognizer to further improve the recognition accuracy.  
Named Entity Recognition (NER) systems need to integrate a wide variety of information for optimal performance. This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. 
This paper presents a classiﬁer-combination experimental framework for named entity recognition in which four diverse classiﬁers (robust linear classiﬁer, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions. When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data. 
In this approach to named entity recognition, a recurrent neural network, known as Long Short-Term Memory, is applied. The network is trained to perform 2 passes on each sentence, outputting its decisions on the second pass. The ﬁrst pass is used to acquire information for disambiguation during the second pass. SARDNET, a self-organising map for sequences is used to generate representations for the lexical items presented to the LSTM network, whilst orthogonal representations are used to represent the part of speech and chunk tags. 
¤ We discuss two named-entity recognition mod- els which use characters and character -grams either exclusively or as an important part of their data representation. The ﬁrst model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model ¥ with substantially richer context features. Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data). This number represents a 25% error reduction over the same model without word-internal (substring) features. 
This paper presents a named entity classiﬁcation system that utilises both orthographic and contextual information. The random subspace method was employed to generate and reﬁne attribute models. Supervised and unsupervised learning techniques used in the recombination of models to produce the ﬁnal results. 
We present a named entity recognition and classiﬁcation system that uses only probabilistic character-level features. Classiﬁcations by multiple orthographic tries are combined in a hidden Markov model framework to incorporate both internal and contextual evidence. As part of the system, we perform a preprocessing stage in which capitalisation is restored to sentence-initial and all-caps words with high accuracy. We report f-values of 86.65 and 79.78 for English, and 50.62 and 54.43 for the German datasets. 
This paper investigates stacking and voting methods for combining strong classiﬁers like boosting, SVM, and TBL, on the named-entity recognition task. We demonstrate several effective approaches, culminating in a model that achieves error rate reductions on the development and test sets of 63.6% and 55.0% (English) and 47.0% and 51.7% (German) over the CoNLL-2003 standard baseline respectively, and 19.7% over a strong AdaBoost baseline model from CoNLL-2002. 
We used the memory-based learner Timbl (Daelemans et al., 2002) to ﬁnd names in English and German newspaper text. A ﬁrst system used only the training data, and a number of gazetteers. The results show that gazetteers are not beneﬁcial in the English case, while they are for the German data. Type-token generalization was applied, but also reduced performance. The second system used gazetteers derived from the unannotated corpus, as well as the ratio of capitalized versus uncapitalized use of each word. These strategies gave an increase in performance.  on the basis of its k nearest neighbours in the training set. Before classiﬁcation, the learner assigns weights to each of the features, marking their importance for the learning task. Features with higher weights are treated as more important in classiﬁcation as those with lower weights. Timbl has some parameters which can be adjusted in order to improve learning. For the NER system described in this paper, we varied the parameters k and m. k is the number of nearest neighbours Timbl looks at. m determines the feature metrics, i.e. the importance weights given to each feature, and the way similarity between values of the same feature is computed. This parameter can be adjusted separately for each feature. The two metrics used were weighted overlap and modiﬁed value difference.  
ProAlign combines several different approaches in order to produce high quality word word alignments. Like competitive linking, ProAlign uses a constrained search to ﬁnd high scoring alignments. Like EM-based methods, a probability model is used to rank possible alignments. The goal of this paper is to give a bird’s eye view of the ProAlign system to encourage discussion and comparison. 
This paper presents the experimental results of our attemps to reduce the size of the parameter space in word alignment algorithm. We use IBM Model 4 as a baseline. In order to reduce the parameter space, we pre-processed the training corpus using a word lemmatizer and a bilingual term extraction algorithm. Using these additional components, we obtained an improvement in the alignment error rate. 
Simple baselines provide insights into the value of scoring functions and give starting points for measuring the performance improvements of technological advances. This paper presents baseline unsupervised techniques for performing word alignment based on geometric and word edit distances as well as supervised fusion of the results of these techniques using the nearest neighbor rule. 
We provide a rather informal presentation of a prototype system for word alignment based on our previous translation equivalence approach, discuss the problems encountered in the shared-task on word-aligning of a parallel Romanian-English text, present the preliminary evaluation results and suggest further ways of improving the alignment accuracy. 
We present two methods for the automatic creation of parallel corpora. Whereas previous work into the automatic construction of parallel corpora has focused on harvesting them from the web, we examine the use of existing parallel corpora to bootstrap data for new language pairs. First, we extend existing parallel corpora using co-training, wherein machine translations are selectively added to training corpora with multiple source texts. Retraining translation models yields modest improvements. Second, we simulate the creation of training data for a language pair for which a parallel corpus is not available. Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. This suggests the method may be useful in the creation of parallel corpora for languages with scarce resources. 
Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness. It retrieves example sentences similar to the input and adjusts their translations to obtain the output. However, it has problems in that the performance degrades when input sentences are long and when the style of inputs and that of the example corpus are different. This paper proposes a method for retrieving “meaning-equivalent sentences” to overcome these two problems. A meaning-equivalent sentence shares the main meaning with an input despite lacking some unimportant information. The translations of meaning-equivalent sentences correspond to “rough translations.” The retrieval is based on content words, modality, and tense. 
We propose a method of constructing an example-based machine translation (EBMT) system that exploits a content-aligned bilingual corpus. First, the sentences and phrases in the corpus are aligned across the two languages, and the pairs with high translation conﬁdence are selected and stored in the translation memory. Then, for a given input sentences, the system searches for ﬁtting examples based on both the monolingual similarity and the translation conﬁdence of the pair, and the obtained results are then combined to generate the translation. Our experiments on translation selection showed the accuracy of 85% demonstrating the basic feasibility of our approach.  Figure 1: Translation Example (TE).  
The term translation spotting (TS) refers to the task of identifying the target-language (TL) words that correspond to a given set of sourcelanguage (SL) words in a pair of text segments known to be mutual translations. This article examines this task within the context of a sub-sentential translation-memory system, i.e. a translation support tool capable of proposing translations for portions of a SL sentence, extracted from an archive of existing translations. Different methods are proposed, based on a statistical translation model. These methods take advantage of certain characteristics of the application, to produce TL segments submitted to constraints of contiguity and compositionality. Experiments show that imposing these constraints allows important gains in accuracy, with regard to the most probable alignments predicted by the model. 
We present an unsupervised extraction of sequence-to-sequence correspondences from parallel corpora by sequential pattern mining. The main characteristics of our method are two-fold. First, we propose a systematic way to enumerate all possible translation pair candidates of rigid and gapped sequences without falling into combinatorial explosion. Second, our method uses an efﬁcient data structure and algorithm for calculating frequencies in a contingency table for each translation pair candidate. Our method is empirically evaluated using English-Japanese parallel corpora of 6 million words. Results indicate that it works well for multi-word translations, giving 56-84% accuracy at 19% token coverage and 11% type coverage. 
This paper presents a study on optimizing sentence pair alignment scores of a bilingual sentence alignment module. Five candidate scores based on perplexity and sentence length are introduced and tested. Then a linear regression model based on those candidates is proposed and trained to predict sentence pairs’ alignment quality scores solicited from human subjects. Experiments are carried out on data automatically collected from Internet. The correlation between the scores generated by the linear regression model and the scores from human subjects is in the range of the inter-subject agreement score correlations. Pearson's correlation ranges from 0.53 up to 0.72 in our experiments. 
Corpus-based Natural Language Processing (NLP) tasks for such popular languages as English, French, etc. have been well studied with satisfactory achievements. In contrast, corpus-based NLP tasks for unpopular languages (e.g. Vietnamese) are at a deadlock due to absence of annotated training data for these languages. Furthermore, hand-annotation of even reasonably well-determined features such as part-ofspeech (POS) tags has proved to be labor intensive and costly. In this paper, we suggest a solution to partially overcome the annotated resource shortage in Vietnamese by building a POS-tagger for an automatically word-aligned English-Vietnamese parallel Corpus (named EVC). This POS-tagger made use of the Transformation-Based Learning (or TBL) method to bootstrap the POS-annotation results of the English POS-tagger by exploiting the POS-information of the corresponding Vietnamese words via their wordalignments in EVC. Then, we directly project POSannotations from English side to Vietnamese via available word alignments. This POS-annotated Vietnamese corpus will be manually corrected to become an annotated training data for Vietnamese NLP tasks such as POS-tagger, Phrase-Chunker, Parser, Word-Sense Disambiguator, etc. 
This paper presents a framework for extracting English and Chinese transliterated word pairs from parallel texts. The approach is based on the statistical machine transliteration model to exploit the phonetic similarities between English words and corresponding Chinese transliterations. For a given proper noun in English, the proposed method extracts the corresponding transliterated word from the aligned text in Chinese. Under the proposed approach, the parameters of the model are automatically learned from a bilingual proper name list. Experimental results show that the average rates of word and character precision are 86.0% and 94.4%, respectively. The rates can be further improved with the addition of simple linguistic processing. 
We propose a method to split and translate input sentences for speech translation in order to overcome the long sentence problem. This approach is based on three criteria used to judge the goodness of translation results. The criteria utilize the output of an MT system only and assumes neither a particular language nor a particular MT approach. In an experiment with an EBMT system, in which prior methods cannot work or work badly, the proposed split-and-translate method achieves much better results in translation quality. 
This paper presents the results of applying the Latent Semantic Analysis (LSA) methodology to a small collection of parallel texts in French and English. The goal of the analysis was to determine what the methodology might reveal regarding the difficulty level of either the machinetranslation (MT) task or the text-alignment (TA) task. In a perfectly parallel corpus where the texts are exactly aligned, it is expected that the word distributions between the two languages be perfectly symmetrical. Where they are symmetrical, the difficulty level of the machine-translation or the textalignment task should be low. The results of this analysis show that even in a perfectly aligned corpus, the word distributions between the two languages deviate and because they do, LSA may contribute much to our understanding of the difficulty of the MT and TA tasks. 1. Credits This paper discusses an implementation of the Latent Semantic Analysis (LSA) methodology against a small collection of perfectly parallel texts in French and English1. The texts were made available by the HLT-NAACL and are taken from daily House journals of the Canadian Parliament. They were edited by Ulrich Germann. The LSA procedures were implemented in R, a system for statistical computation and graphics, and were written by John C. Paolillo at Indiana University, Bloomington. 2. Introduction LSA is an analytical methodology that uses mathematical procedures and vector space modeling 
A parallel corpus of texts in English and in Inuktitut, an Inuit language, is presented. These texts are from the Nunavut Hansards. The parallel texts are processed in two phases, the sentence alignment phase and the word correspondence phase. Our sentence alignment technique achieves a precision of 91.4% and a recall of 92.3%. Our word correspondence technique is aimed at providing the broadest coverage collection of reliable pairs of Inuktitut and English morphemes for dictionary expansion. For an agglutinative language like Inuktitut, this entails considering substrings, not simply whole words. We employ a Pointwise Mutual Information method (PMI) and attain a coverage of 72.3% of English words and a precision of 87%. 
This paper describes classification of typed student utterances within AutoTutor, an intelligent tutoring system. Utterances are classified to one of 18 categories, including 16 question categories. The classifier presented uses part of speech tagging, cascaded finite state transducers, and simple disambiguation rules. Shallow NLP is well suited to the task: session log file analysis reveals significant classification of eleven question categories, frozen expressions, and assertions. 
Automatic classiﬁcation of short textual answers by students to questions about topics in physics, computing, etc., is an attractive approach to diagnostic assessment of learning. We present a language for expressing rules that can classify text based on the presence and relative positions of words, lists of synonyms and other abstractions of a single word. We also describe a system, based on Mitchell’s version spaces algorithm, that learns rules in this language. These rules can be used to categorize student responses to short-answer questions. The system is trained on written responses captured by an online assessment system that poses multiple choice questions and asks the student to justify their answers with textual explanations of their reasoning. Several experiments are described that examine the effects of the use of negative data and tagging students explanations with their answer to the original multiple choice question. 
This paper describes a novel computer-aided procedure for generating multiple-choice tests from electronic instructional documents. In addition to employing various NLP techniques including term extraction and shallow parsing, the program makes use of language resources such as a corpus and WordNet. The system generates test questions and distractors, offering the user the option to post-edit the test items. 1. Introduction Multiple-choice tests have proved to be an efficient tool for measuring students' achievement.1 The manual construction of such tests, however, is a timeconsuming and labour-intensive task. In this paper we seek to provide an alternative to the lengthy and demanding activity of developing multiplechoice tests and propose a new, NLP-based approach for generating tests from narrative texts (textbooks, encyclopaedias). The approach uses a simple set of transformational rules, a shallow parser, automatic term extraction, word sense disambiguation, a corpus and WordNet. While in the current experiment we have used an electronic textbook in linguistics to automatically generate test items in this area, we should note that the methodology is general and can be extended to practically any other area. To the best of our knowledge, no related work has been reported addressing such a type of application.2 
PLASER is a multimedia tool with instant feedback designed to teach English pronunciation for high-school students of Hong Kong whose mother tongue is Cantonese Chinese. The objective is to teach correct pronunciation and not to assess a student’s overall pronunciation quality. Major challenges related to speech recognition technology include: allowance for non-native accent, reliable and corrective feedbacks, and visualization of errors. PLASER employs hidden Markov models to represent position-dependent English phonemes. They are discriminatively trained using the standard American English TIMIT corpus together with a set of TIMIT utterances collected from “good” local English speakers. There are two kinds of speaking exercises: minimal-pair exercises and word exercises. In the word exercises, PLASER computes a conﬁdence-based score for each phoneme of the given word, and paints each vowel or consonant segment in the word using a novel 3-color scheme to indicate their pronunciation accuracy. PLASER was used by 900 students of grade 7 and 8 over a period of 2–3 months. About 80% of the students said that they preferred using PLASER over traditional English classes to learn pronunciation. A pronunciation test was also conducted before and after they used PLASER. The result from 210 students shows that the students’ pronunciation skill was improved. (The statistics is signiﬁcant at the 99% conﬁdence level.) ∗ Mr. Tam is now a graduate student at the Department of Computer Science at Carnegie Mellon University. † Mr. Chan is now working at SpeechWorks Inc.  
This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring. Most current tutorial dialogue systems are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). However, prior studies have shown considerable beneﬁts of tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994; Hausmann and Chi, 2002). Thus, we are currently developing a speech based dialogue system that uses a text based system for tutoring conceptual physics (VanLehn et al., 2002) as its “back-end”. In order to explore the relative effectiveness between these two input modalities in our task domain, we have started by collecting parallel human-human tutoring corpora both for text based and speech based tutoring. In both cases, students interact with the tutor through a web interface. We present here a comparison between the two on a number of features of dialogue that have been demonstrated to correlate reliably with learning gains with students interacting with the tutor using the text based interface (Rose´ et al., submitted). 
This paper focuses on the transformation of grammar checking technology into a learning environment for second language writing. Our starting point is a grammar checker for Swedish, called Granska. Two studies have been conducted aimed at exploring the use of computer support for writing in the context of second language learning. In the ﬁrst study, we developed a methodology to study naturalistic writing, and the impact of the grammar checker on the writer’s text. In the second study, we were interested in how the methodology developed earlier would work in an educational setting. The problems with false alarms and limited recall are deﬁnitely a sensitive issue in the context of second language learners and educational settings. Both learners and teachers are concerned about the false alarms, and without perfectly working text analyzers, new strategies for dealing with these problems have to be further explored and developed together with learners and teachers. 
This paper argues that computational cognitive psychology and computational linguistics have much to offer the science of language by adopting the research strategy that Donald Stokes called Pasteur’s quadrant--starting and testing success with important real world problems--and that education offers an ideal venue. Some putative examples from applications of Latent Semantic Analysis (LSA) are presented, as well as some detail on how LSA works, what it is and is not, and what it does and doesn’t do. For example, LSA is used successfully in automatic essay grading with content coverage feedback, computing optimal sequences of study materials, and partially automating metadata tagging, but is insufficient for scoring mathematical and short textual answers, for revealing reasons. It is explained that LSA is not construable as measuring co-occurrence, but rather measure the similarity of words in their effect on passage meaning, 
Latent semantic analysis (LSA) has been used in several intelligent tutoring systems(ITS’s) for assessing students’ learning by evaluating their answers to questions in the tutoring domain. It is based on word-document cooccurrence statistics in the training corpus and a dimensionality reduction technique. However, it doesn’t consider the word-order or syntactic information, which can improve the knowledge representation and therefore lead to better performance of an ITS. We present here an approach called Syntactically Enhanced LSA (SELSA) which generalizes LSA by considering a word along with its syntactic neighborhood given by the part-of-speech tag of its preceding word, as a unit of knowledge representation. The experimental results on AutoTutor task to evaluate students’ answers to basic computer science questions by SELSA and its comparison with LSA are presented in terms of several cognitive measures. SELSA is able to correctly evaluate a few more answers than LSA but is having less correlation with human evaluators than LSA has. It also provides better discrimination of syntactic-semantic knowledge representation than LSA. 
To date, traditional NLP parsers have not been widely successful in TESOLoriented applications, particularly in scoring written compositions. Re-engineering such applications to provide the necessary robustness for handling ungrammatical English has proven a formidable obstacle. We discuss the use of a nontraditional parser for rating compositions that attenuates some of these difﬁculties. Its dependency-based shallow parsing approach provides signiﬁcant robustness in the face of language learners’ ungrammatical compositions. This paper discusses how a corpus of L2 essays for English was rated using the parser, and how the automatic evaulations compared to those obtained by manual methods. The types of modiﬁcations that were made to the system are discussed. Limitations to the current system are described, future plans for developing the system are sketched, and further applications beyond English essay rating are mentioned. 
We present CarmelTC, a novel hybrid text classiﬁcation approach for analyzing essay answers to qualitative physics questions, which builds upon work presented in (Rose´ et al., 2002a). CarmelTC learns to classify units of text based on features extracted from a syntactic analysis of that text as well as on a Naive Bayes classiﬁcation of that text. We explore the tradeoffs between symbolic and “bag of words” approaches. Our goal has been to combine the strengths of both of these approaches while avoiding some of the weaknesses. Our evaluation demonstrates that the hybrid CarmelTC approach outperforms two “bag of words” approaches, namely LSA and a Naive Bayes, as well as a purely symbolic approach. 
Here we present work on using spatial knowledge in conjunction with information extraction (IE). Considerable volume of location data was imported in a knowledge base (KB) with entities of general importance used for semantic annotation, indexing, and retrieval of text. The Semantic Web knowledge representation standards are used, namely RDF(S). An extensive upper-level ontology with more than two hundred classes is designed. With respect to the locations, the goal was to include the most important categories considering public and tasks not specially related to geography or related areas. The locations data is derived from number of publicly available resources and combined to assure best performance for domainindependent named-entity recognition in text. An evaluation and comparison to high performance IE application is given. 
 2 Outline of GeoLogica  Issues in the description of places are discussed in the context of a logical geospatial theory. This theory lies at the core of the system GeoLogica, which deduces answers to geographical questions based on knowledge provided by multiple agents. 
In this paper we present an approach to the acquisition of geographical gazetteers. Instead of creating these resources manually, we propose to extract gazetteers from the World Wide Web, using Data Mining techniques. The bootstrapping approach, investigated in our study, allows us to create new gazetteers using only a small seed dataset (1260 words). In addition to gazetteers, the system produces classiﬁers. They can be used online to determine a class (CITY, ISLAND, RIVER, MOUNTAIN, REGION, COUNTRY) of any geographical name. Our classiﬁers perform with the average accuracy of 86.5%. 
To be unambiguous about a Chinese geographic name represented in English text as Pinyin, one needs to recover the name in Chinese characters. We present our approach to this back-transliteration problem based on processes such as bilingual geographic name lookup, name suggestion using place name character and pair frequencies, and confirmation via a collection of monolingual names or the WWW. Evaluation shows that about 48% to 72% of the correct names can be recovered as the top candidate, and 82% to 86% within top ten, depending on the processes employed. 
The task of named entity annotation of unseen text has recently been successfully automated with near-human performance. But the full task involves more than annotation, i.e. identifying the scope of each (continuous) text span and its class (such as place name). It also involves grounding the named entity (i.e. establishing its denotation with respect to the world or a model). The latter aspect has so far been neglected. In this paper, we show how geo-spatial named entities can be grounded using geographic coordinates, and how the results can be visualized using off-the-shelf software. We use this to compare a “textual surrogate” of a newspaper story, with a “visual surrogate” based on geographic coordinates. 
Ambiguity is very high for location names. For example, there are 23 cities named ‘Buffalo’ in the U.S. Based on our previous work, this paper presents a refined hybrid approach to geographic references using our information extraction engine InfoXtract. The InfoXtract location normalization module consists of local pattern matching and discourse co-occurrence analysis as well as default senses. Multiple knowledge sources are used in a number of ways: (i) pattern matching driven by local context, (ii) maximum spanning tree search for discourse analysis, and (iii) applying default sense heuristics and extracting default senses from the web. The results are benchmarked with 96% accuracy on our test collections that consist of both news articles and tourist guides. The performance contribution for each component of the module is also benchmarked and discussed. 
We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems. We train data-driven place name classiﬁers using toponyms already disambiguated in the training text — by such existing cues as “Nashville, Tenn.” or “Springﬁeld, MA” — and test the system on texts where these cues have been stripped out and on hand-tagged historical texts. We experiment on three English-language corpora of varying provenance and complexity: newsfeed from the 1990s, personal narratives from the 19th century American west, and memoirs and records of the U.S. Civil War. Disambiguation accuracy ranges from 87% for news to 69% for some historical collections. 
We describe a purely conﬁdence-based geographic term disambiguation system that crucially relies on the notion of “positive” and “negative” context and methods for combining conﬁdence-based disambiguation with measures of relevance to a user’s query.  
The work presented in this paper concerns Information Retrieval from geographical documents, i.e. documents with a major geographic component. The ﬁnal aim, in response to an informational query of the user, is to return a ranked list of relevant passages in selected documents, allowing text browsing within them. We consider in this paper the spatial component of the texts and the queries. The idea is to perform an off-line linguistic analysis of the document, extracting spatial expressions (i.e. expressions denoting geographical localisations). The point is that such expressions are (in general) much more complex than simple place names. We present a linguistic analyser which recognises them, performing a semantic analysis and computing symbolic representations of their "content". These representations, stored in the text thanks to XML annotation, will act as indexes of passages with which queries are compared. The matching of queries with text expressions is a complex process, needing several kinds of numeric and symbolic computations. A prospective outline of it is described. 
 We deﬁne a data model for storing geographic information from multiple sources that enables the efﬁcient production of customizable gazetteers. The GazDB separates names from features while storing the relationships between them. Geographic names are stored in a variety of resolutions to allow for i18n and for multiplicity of naming. Geographic features are categorized along several axes to facilitate selection and ﬁltering.  
Reliably recognizing, disambiguating, normalizing, storing, and displaying geographic names poses many challenges. However, associating each name with a geographical point location cannot be the final stage. We also need to understand each name’s role within the document, and its association with adjacent text. The paper develops these points through a discussion of two different types of historical texts, both rich in geographic names: descriptive gazetteer entries and travellers’ narratives. It concludes by discussing the limitations of existing mark-up systems in this area. 
We describe a basic Geo-coding service encompassing a geo-parsing tool and integrated digital gazetteer service. The development of a geo-parser comes from the need to explicitly georeference large resource collections such as the Statistical Accounts of Scotland which currently only contain implicit georeferences in the form of placennames thus making such collections inherently geographically searchable.  Figure 1: The geo-coding process  
This paper describes an incremental chart parser that generates look-ahead categories on the fly for a controlled natural language. These predictive hints tell the author what kind of syntactic (or semantic) structure can follow the current input string and thereby aim at helping the author to reduce the cognitive burden to learn and remember the rules of the controlled language. The parser can handle modifications (insertion, deletion, and replacement) to the input string without the need to reparse the entire string. These modifications are a function of the size of the tokens changed rather than the size of the entire input. 
Conventional approaches to the generation of referring expressions place the task within a pipelined architecture, typically somewhere between text planning and linguistic realisation. In this paper, we look at the issues that arise in generating one-anaphoric referring expressions; examination of this task causes us to reﬂect on the current predominant architectural models for natural language generation, and leads us to suggest an alternative architecture where decisions that inﬂuence forms of reference happen much earlier in the process of natural language generation. 
Currently, the most common technique for Natural Language parsing is done by using pattern matching through references to a database with the aid of grammatical structures models. But the huge variety of linguistical syntax and semantics means that accurate real time analysis is very difficult. We investigate several optimisation approaches to reduce the search space for finding an accurate parse of a sentence where individual words can have multiple possible syntactic categories, and categories and phrases can combine together in different ways. The algorithms we consider include mechanisms for ordering that reduce the search cost without loss of completeness or accuracy as well as mechanisms that prune the space and may result in eliminating valid parses or returning a suboptimal as the best parse. We discuss the development and benchmarking of the existing and proposed algorithms in terms of accuracy, search space and parse time. Speed up of an order of magnitude was achieved without loss of completeness, whilst decrease of over two orders of magnitude was achieved in the search space. A further order of magnitude reduction of both time and search space was achieved at the expense of some loss of accuracy in finding the most probable parse.  
In Korean, in order to generate a coherent text, a redundantly prominent noun should be replaced by a non-zero pronoun or zero pronoun. Otherwise, the text becomes unnatural. Specifically, a redundant noun in Korean is frequently omitted while a redundant noun in English is replaced by a pronoun. This paper proposes a generation algorithm of the zero pronoun, using a Cost-based Centering Model which considers the inference cost. For an objective evaluation of our algorithm, we collected 87 texts from three genres, and manually recovered the omitted elements. Using the collected texts, we verify that our algorithm is well defined to explain the phenomenon of the zero pronoun in Korean. We also show that the proposed approach resolves both the overgeneration of the zero pronoun in Continue and its under-generation in other transitions in terms of Centering.  linguistic representation of information. To generate a coherent text, we must pay attention to each stage of generation, such as content determination, text structuring, aggregation, and generation of anaphoric expression (pronominalization). Among these stages, we are especially interested in the generation of anaphoric expression focusing on zero pronouns, because generating the appropriate zero pronoun is directly connected with text coherence. Consider the following short text. (1) Na-nun (I, topic) cinyel-toyn (on display) os (the dresses) cwung (one of) maum-ye tu-nun kes-i issess-ta (attracted me). (One of the dresses on display attracted me.) (2) [Na-nun (I, topic), ø]1 [os-oul (dress, object), ø] ip-e po-ass-ta (putted on). (I (ø) putted it (ø) on.) (3) Haciman (however), [na-nun (I, topic), ø] [os-i (dress, subject), ø] nemu (too) khesu (big) [os-ul (it, object), ø] sal-swu (buy) eps-ess-ta.(can not). (However, the dress (ø) was too big so that I (ø) cannot buy it (ø)) In the above text, ‘na (I)’ appears repeatedly as a topic2 in sentence (1), (2), and (3), and ‘os (dress)’  
In dependency parsing of long sentences with fewer subjects than predicates, it is difficult to recognize which predicate governs which subject. To handle such syntactic ambiguity between subjects and predicates, this paper proposes an “Sclause” segmentation method, where an S(ubject)clause is defined as a group of words containing several predicates and their common subject. We propose an automatic S-clause segmentation method using decision trees. The S-clause information was shown to be very effective in analyzing long sentences, with an improved performance of 5 percent. 
If a sentence is ambiguous, it often happens that the correct reading is the one which can most easily be incorporated into the discourse context. In this paper we present a simple method for implementing this intuition using the mechanism of presupposition resolution. The basic idea is that we can choose between the alternative readings of an ambiguous sentence by picking the reading which has the greatest number of satistiﬁed presuppositions. We present two uses of the disambiguation algorithm in our bilingual human-machine dialogue system. 
This paper presents a range of methods for classifying Dutch noun countability based on either Dutch or English data. The classiﬁcation is founded on translational equivalences and the corpus analysis of linguistic features which correlate with particular countability classes. We show that crosslingual classiﬁcation on the basis of word-to-word or featureto-feature mappings between English and Dutch performs at least as well as in-language classiﬁcation based on gold-standard Dutch countability data. 
Interlinear text is a common presentational format for linguistic information, and its creation and management have been greatly facilitated by the development of specialised software. In earlier work we developed a four-level model and corresponding formal speciﬁcation for interlinear text. Here we describe a suitable XML representation for the model and show how it can be rendered into a variety of convenient presentational formats. We conclude by discussing architectural extensions, an application programming interface for interlinear text, and prospects for embedding the interlinear model into existing applications.  
This paper describes a novel model for term frequency distributions that is derived from queuing-theory. It is compared with Poisson distributions, in terms of how well the models describe the observed distributions of terms, and it is demonstrated that a model for term frequency distributions based on queue utilisation generally gives a better ﬁt. It is further demonstrated that the ratio of the ﬁt/error between the Poisson and queue utilisation distributions may be used as a good indication of how interesting a word is in relation to the topic of the discourse. A number of possible reasons for this are discussed.  
It is impossible to perform root-based searching, concordancing, and grammar checking in Arabic without a method to match words with roots and vice versa. A comprehensive word list is essential for incremental searching, predictive SMS messaging, and spell checking, but due to the derivational and inﬂectional nature of Arabic, a comprehensive word list is taxing on storage space and access speed. This paper describes a method for compactly storing and eﬃciently accessing an extensive dictionary of Arabic words by their morphological properties and roots. Compression of the dictionary is based on T-Code encoding, which follows the Huﬀman encoding model. The special characteristics inherent in the recursive augmentation method with which codes are created allow compact storage on disk and in memory. They also facilitate the eﬃcient use of bandwidth, for Arabic text transmission, over intranets and the Internet. 
This paper presents the area under the Receiver Operating Characteristics (ROC) curve as an alternative metric for evaluating word sense disambiguation performance. The current metrics – accuracy, precision and recall – while suitable for two-way classiﬁcation, are shown to be inadequate when disambiguating between three or more senses. Speciﬁcally, these measures do not facilitate comparison with baseline performance nor are they sensitive to non-uniform misclassiﬁcation costs. Both of these issues can be addressed using ROC analysis. 
This paper1 presents our approach to the problem of single sentence summarisation. We investigate the use of Singular Value Decomposition (SVD) to guide the generation of a summary towards the theme that is the focus of the document to be summarised. In doing so, the intuition is that the generated summary will more accurately reflect the content of the source document. Currently, we operate in the news domain and at present, our summaries are modelled on headlines. This paper presents SVD as an alternative method to determine if a word is a suitable candidate for inclusion in the headline. The results of a recall based evaluation comparing three different strategies to word selection, indicate that thematic information does help improve recall. 
From the view point of the linguistic typology, Korean and Japanese have many grammatical similarities which enable it to easily construct a sense-tagged Korean corpus through an existing high-quality Japanese-to-Korean machine translation system. The sense-tagged corpus may serve as a knowledge source to extract useful clues for word sense disambiguation (WSD). This paper addresses a disambiguation model for Korean nouns, whose execution is based on the concept codes extracted from the sense-tagged corpus and the semantic similarity values over a thesaurus hierarchy. By the help of the automatically constructed sensetagged corpus, we overcome the knowledge acquisition bottleneck. Also, we show that the performance of word sense disambiguation can be improved by combining several base classifiers. In an experimental evaluation, the proposed model using a majority voting achieved an average precision of 77.75% with an improvement over the baseline by 15.00%, which is very promising for real world MT systems.  
In this paper we present an evaluation of overlap-based measures of similarity for sentences in the same language. The measures include syntactic and semantic information, and to that end they incorporate grammatical relations and ﬂat logical forms. A full parser is required to build the above information. Separate extrinsic evaluations within the context of question answering have been made with two different parsers to test the impact of the parser and the overlap measures. 
In this paper, we present an applicationoriented evaluation of three Part-ofSpeech (PoS) taggers in a word sense disambiguation (WSD) system. Following the intuition that high quality input is likely to inﬂuence the ﬁnal results of a complex system, we test whether the more accurate taggers also produce better results when integrated into the WSD system. For this purpose, a stand-alone evaluation of the PoS taggers is used to assess which tagger is the most accurate. The results of the WSD task, computed on the training section of the Dutch Senseval-2 data, including the PoS information from all three taggers show that the most accurate PoS tags do indeed lead to the best results, thereby verifying our hypothesis. A surprising result, however, is the fact that the performance of the complex WSD system with the different PoS tags included does not necessarily reﬂect the stand-alone accuracy of the PoS taggers. 
Natural language database interfaces require translation knowledge to convert user questions into formal database queries. Previously, translation knowledge acquisition heavily depends on human specialties such as NLP, DBMS and domain engineering, consequently undermining domain portability. This paper attempts to semi-automatically construct translation knowledge by introducing a physically-derived conceptual database schema, and by simplifying translation knowledge into two structures – classreferring documents and classconstraining selection restrictions. Based on these two structures, this paper proposes a noun translation method that employs an information retrieval framework. 
We present new results for the DSTO project on document classification of military messages. We report more specifically on the improvements to the Part-Of-Speech (POS) tagging, a probabilistic process that assigns a tag to a token, and discuss the training for Date Time Groups POS tags. A new implementation of the rule-based classifier is described. The results obtained on two databases of real military messages are encouraging and the document classification module has now been integrated with a query user interface.  log operational events and Lotus e-mail for actions and administrative functions. Around 200 messages per day are entered into these Lotus Notes log databases. Many DJFHQ staff members have expressed difficulty in finding particular information in their information reservoirs and our goal is to develop a more effective query interface between DJFHQ staff and their information reservoirs. This work already resulted in the development of the Query Building Interface (QBI), which was designed to create a better search interface to multiple log databases and to the users e-mail database. The rule-based Document Classifier we describe here has been trained and evaluated on Lotus operational log databases (OPS logs) from DJFHQ. It can now provide a categorisation for each document from the OPS logs and is integrated with QBI, as described in Section 7.2.  
In this paper, we present a method for the semantic tagging of word chunks extracted from a written transcription of conversations. This work is part of an ongoing project for an information extraction system in the ﬁeld of maritime Search And Rescue (SAR). Our purpose is to automatically annotate parts of texts with concepts from a SAR ontology. Our approach combines two knowledge sources a SAR ontology and the Wordsmyth dictionarythesaurus, and it uses a similarity measure for the classiﬁcation. Evaluation is carried out by comparing the output of the system with key answers of predeﬁned extraction templates.  process to extract or reject a word according to the semantic tag and the context. The rationale behind our approach, is that the relevance of a word depends strongly on how close it is to the SAR domain and its context of use. We believe that reasoning on semantic tags instead of the word is a way of getting around some of the problems of small-scale corpora. In this paper, we focus on semantic tagging based on a domain-speciﬁc ontology, a dictionarythesaurus and the overlapping coefﬁcient similarity measure (Manning and Schutze, 2001) to semantically annotate words. We ﬁrst describe the corpus (section 2), then the overall IE system (section 3). Next we explain the different components of the semantic tagger (section 4) and we present the preliminary results of our experiments (section 5). Finally we give some directions for future work (section 6).  
We investigate Global Index Grammars (GIGs), a grammar formalism that uses a stack of indices associated with productions and has restricted context-sensitive power. We discuss some of the structural descriptions that GIGs can generate compared with those generated by LIGs. We show also how GIGs can represent structural descriptions corresponding to HPSGs (Pollard and Sag, 1994) schemas. 
This paper investigates the correlation between acoustic conﬁdence scores as returned by speech recognizers with recognition quality. We report the results of two machine learning experiments that predict the word error rate of recognition hypotheses and the conﬁdence error rate for individual words within them. 
Multimodal dialogue systems allow users to input information in multiple modalities. These systems can handle simultaneous or sequential composite multimodal input. Different coordination schemes require such systems to capture, collect and integrate user input in different modalities, and then respond to a joint interpretation. We performed a study to understand the variability of input in multimodal dialogue systems and to evaluate methods to perform the collection of input information. An enhancement in the form of incorporation of a dynamic time window to a multimodal input fusion module was proposed in the study. We found that the enhanced module provides superior temporal characteristics and robustness when compared to previous methods. 
We describe an algorithm for recovering non-local dependencies in syntactic dependency structures. The patternmatching approach proposed by Johnson (2002) for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classiﬁer that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in (Johnson, 2002). 
This paper suggests the efﬁcient indexing method based on a concept vector space that is capable of representing the semantic content of a document. The two information measure, namely the information quantity and the information ratio, are deﬁned to represent the degree of the semantic importance within a document. The proposed method is expected to compensate the limitations of term frequency based methods by exploiting related lexical items. Furthermore, with information ratio, this approach is independent of document length. 
This paper investigates an application of the ranked region algebra to information retrieval from large scale but unannotated documents. We automatically annotated documents with document structure and semantic tags by using taggers, and retrieve information by specifying structure represented by tags and words using ranked region algebra. We report in detail what kind of data can be retrieved in the experiments by this approach. 
Discourse chunking is a simple way to segment dialogues according to how dialogue participants raise topics and negotiate them. This paper explains a method for arranging dialogues into chunks, and also shows how discourse chunking can be used to improve performance for a dialogue act tagger that uses a case-based reasoning approach. 
In this paper, we elucidate how Korean temporal markers, OE and DONGAN contribute to specifying the event time and formalize it in terms of typed lambda calculus. We also present a computational method for constructing temporal representation of Korean sentences on the basis of G grammar proposed by [Renaud, 1992;1996]. 
 Coreference resolution systems usually attempt to ﬁnd a suitable antecedent for (almost) every noun phrase. Recent studies, however, show that many deﬁnite NPs are not anaphoric. The same claim, obviously, holds for the indeﬁnites as well.  In this study we try ¢to¡¤l£¦e¥¨a§r©na¤u¥tomat ically  t¢wo c£"l!a#ss iﬁcations,  and  , relevant for this problem. We  use a small training corpus (MUC-7), but  also acquire some data from the Internet.  Combining our classiﬁers sequentially, we  achieve 88.9% precision and 84.6% recall  for discourse new entities.  We expect our classiﬁers to provide a good preﬁltering for coreference resolution systems, improving both their speed and performance.  
This paper describes our preliminary attempt to automatically recognize zero adnominals, a subgroup of zero pronouns, in Japanese discourse. Based on the corpus study, we define and classify what we call “argument-taking nouns (ATNs),” i.e., nouns that can appear with zero adnominals. We propose an ATN recognition algorithm that consists of lexicon-based heuristics, drawn from the observations of our analysis. We finally present the result of the algorithm evaluation and discuss future directions. 
The development of multi-channel digital broadcasting has generated a demand not only for new services but also for smart and highly functional capabilities in all broadcast-related devices. This is especially true of the television receivers on the viewer's side. With the aim of achieving a friendly interface that anybody can use with ease, we built a prototype interface system that operates a television through voice interactions using natural language. At the current stage of our research, we are using this system to investigate the usefulness and problem areas of the spoken dialogue interface for television operations.  Recently we conducted a usability test targeting data broadcasts in BS digital broadcasting. The results of the test revealed that many subjects had trouble accessing hierarchically arranged data. This finding revealed the need for an easy means of accessing desired programs. One such means is a spoken natural language dialogue (hereafter spoken dialogue) interface for TV operations. If spoken dialogue could be used to select and search for programs, to operate peripheral devices, and to give information in reply to system queries, we can envisage such an interface as being extremely valuable in a multi-channel and multiservice function viewing environment. With this in mind, we have set out to build an interface system that could operate a television via spoken dialogue in place of manual operations. 2 Collecting dialogue data for TV operations  
This position paper argues for an interactive approach to text understanding. The proposed model extends an existing semantics-based text authoring system by using the input text as a source of information to assist the user in re-authoring its content. The approach permits a reliable deep semantic analysis by combining automatic information extraction with a minimal amount of human intervention. 
 2 Sign language phenomena  We demonstrate a text to sign language translation system for investigating sign language (SL) structure and assisting in production of sign narratives and informative presentations1. The system is demonstrable on a conventional PC laptop computer. 
This paper presents a novel information system integrating advanced information extraction technology and automatic hyper-linking. Extracted entities are mapped into a domain ontology that relates concepts to a selection of hyperlinks. For information extraction, we use SProUT, a generic platform for the development and use of multilingual text processing components. By combining finite-state and unification-based formalisms, the grammar formalism used in SProUT offers both processing efficiency and a high degree of decalrativeness. The ExtraLink demo system showcases the extraction of relevant concepts from German texts in the tourism domain, offering the direct connection to associated web documents on demand. 
This paper proposes a method of collecting a dozen terms that are closely related to a given seed term. The proposed method consists of three steps. The ﬁrst step, compiling corpus step, collects texts that contain the given seed term by using search engines. The second step, automatic term recognition, extracts important terms from the corpus by using Nakagawa’s method. These extracted terms become the candidates for the ﬁnal step. The ﬁnal step, ﬁltering step, removes inappropriate terms from the candidates based on search engine hits. An evaluation result shows that the precision of the method is 85%. 
We describe iNeATS – an interactive multi-document summarization system that integrates a state-of-the-art summarization engine with an advanced user interface. Three main goals of the system are: (1) provide a user with control over the summarization process, (2) support exploration of the document set with the summary as the staring point, and (3) combine text summaries with alternative presentations such as a map-based visualization of documents. 
We argue that verbal patient diagnosis is a promising application for limited-domain speech translation, and describe an architecture designed for this type of task which represents a compromise between principled linguistics-based processing on the one hand and efﬁcient phrasal translation on the other. We propose to demonstrate a prototype system instantiating this architecture, which has been built on top of the Open Source REGULUS 2 platform. The prototype translates spoken yes-no questions about headache symptoms from English to Japanese, using a vocabulary of about 200 words. 
The present paper will seek to present an approach to bilingual lexicon extraction from non-aligned comparable corpora, phrasal translation as well as evaluations on Cross-Language Information Retrieval. A two-stages translation model is proposed for the acquisition of bilingual terminology from comparable corpora, disambiguation and selection of best translation alternatives according to their linguistics-based knowledge. Different rescoring techniques are proposed and evaluated in order to select best phrasal translation alternatives. Results demonstrate that the proposed translation model yields better translations and retrieval effectiveness could be achieved across JapaneseEnglish language pair. 
This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data. In this paper, we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus, which contains information on learners’ errors. 
This paper describes a spoken dialog QA system as a substitution for call centers. The system is capable of making dialogs for both ﬁxing speech recognition errors and for clarifying vague questions, based on only large text knowledge base. We introduce two measures to make dialogs for ﬁxing recognition errors. An experimental evaluation shows the advantages of these measures. 
We have been investigating an interactive approach for Open-domain QA (ODQA) and have constructed a spoken interactive ODQA system, SPIQA. The system derives disambiguating queries (DQs) that draw out additional information. To test the efﬁciency of additional information requested by the DQs, the system reconstructs the user’s initial question by combining the addition information with question. The combination is then used for answer extraction. Experimental results revealed the potential of the generated DQs. 
In this paper, we proposed a new supervised word sense disambiguation (WSD) method based on a pairwise alignment technique, which is used generally to measure a similarity between DNA sequences. The new method obtained 2.8%-14.2% improvements of the accuracy in our experiment for WSD.  The words such as “musket”, “loaded” and “bird shot” would seem useful in deciding the sense of “ﬁre”, and serve as clue to leading the sense to “go off or discharge”. It seems that there is no clue to another sense. For this case, an approach based on association is useful for WSD. However, an approach based on selectional restriction would not be appropriate, because these clues do not have the direct syntactic dependencies on “ﬁre”. On the other hand, consider the sentence in EDR Corpus:  
The FrameNet project has developed a lexical knowledge base providing a unique level of detail as to the the possible syntactic realizations of the speciﬁc semantic roles evoked by each predicator, for roughly 7,000 lexical units, on the basis of annotating more than 100,000 example sentences extracted from corpora. An interim version of the FrameNet data was released in October, 2002 and is being widely used. A new, more portable version of the FrameNet software is also being made available to researchers elsewhere, including the Spanish FrameNet project. This demo and poster will brieﬂy explain the principles of Frame Semantics and demonstrate the new uniﬁed tools for lexicon building and annotation and also FrameSQL, a search tool for ﬁnding patterns in annotated sentences. We will discuss the content and format of the data releases and how the software and data can be used by other NLP researchers. 
In this paper, we present a method that automatically constructs a Named Entity (NE) tagged corpus from the web to be used for learning of Named Entity Recognition systems. We use an NE list and an web search engine to collect web documents which contain the NE instances. The documents are reﬁned through sentence separation and text reﬁnement procedures and NE instances are ﬁnally tagged with the appropriate NE categories. Our experiments demonstrates that the suggested method can acquire enough NE tagged corpus equally useful to the manually tagged one without any human intervention. 
In the Japanese language, as a predicate is placed at the end of a sentence, the content of a sentence cannot be inferred until reaching the end. However, when the content is complicated and the sentence is long, people want to know at an earlier stage in the sentence whether the content is negative, affirmative, or interrogative. In Japanese, the grammatical form called the KO-OU relation exists. The KO-OU relation is a kind of concord. If a KO element appears, then an OU element appears in the latter part of a sentence. It is being pointed out that the KO-OU relation gives advance notice to the element that appears in the latter part of a sentence. In this paper, we present the method of extracting automatically the KO-OU expression data from large-scale electronic corpus and verify the usefulness of the KO-OU expression data. 
We have developed willex, a tool that helps grammar developers to work efﬁciently by using annotated corpora and recording parsing errors. Willex has two major new functions. First, it decreases ambiguity of the parsing results by comparing them to an annotated corpus and removing wrong partial results both automatically and manually. Second, willex accumulates parsing errors as data for the developers to clarify the defects of the grammar statistically. We applied willex to a large-scale HPSG-style grammar as an example. 
Speech interfaces to question-answering systems offer signiﬁcant potential for ﬁnding information with phones and mobile networked devices. We describe a demonstration of spoken question answering using a commercial dictation engine whose language models we have customized to questions, a Web-based textprediction interface allowing quick correction of errors, and an open-domain question-answering system, AnswerBus, which is freely available on the Web. We describe a small evaluation of the effect of recognition errors on the precision of the answers returned and make some concrete recommendations for modifying a question-answering system for improving robustness to spoken input. 
This paper proposes a principled approach for analysis of semantic relations between constituents in compound nouns based on lexical semantic structure. One of the difﬁculties of compound noun analysis is that the mechanisms governing the decision system of semantic relations and the representation method of semantic relations associated with lexical and contextual meaning are not obvious. The aim of our research is to clarify how lexical semantics contribute to the relations in compound nouns since such nouns are very productive and are supposed to be governed by systematic mechanisms. The results of applying our approach to the analysis of noun-deverbal compounds in Japanese and English show that lexical conceptual structure contributes to the restrictional rules in compounds. 
An empirical comparison of CFG ﬁltering techniques for LTAG and HPSG is presented. We demonstrate that an approximation of HPSG produces a more effective CFG ﬁlter than that of LTAG. We also investigate the reason for that difference. 
This paper proposes an automatic method of detecting grammar elements that decrease readability in a Japanese sentence. The method consists of two components: (1) the check list of the grammar elements that should be detected; and (2) the detector, which is a search program of the grammar elements from a sentence. By deﬁning a readability level for every grammar element, we can ﬁnd which part of the sentence is difﬁcult to read. 
We will demonstrate the latest version of an ongoing project to create an intelligent procedure assistant for use by astronauts on the International Space Station (ISS). The system functionality includes spoken dialogue control of navigation, coordinated display of the procedure text, display of related pictures, alarms, and recording and playback of voice notes. The demo also exempliﬁes several interesting component technologies. Speech recognition and language understanding have been developed using the Open Source REGULUS 2 toolkit. This implements an approach to portable grammar-based language modelling in which all models are derived from a single linguistically motivated uniﬁcation grammar. Domain-speciﬁc CFG language models are produced by ﬁrst specialising the grammar using an automatic corpus-based method, and then compiling the resulting specialised grammars into CFG form. Translation between language centered and domain centered semantic representations is carried out by ALTERF, another Open Source toolkit, which combines rule-based and corpusbased processing in a transparent way.  
Since written Chinese has no space to delimit words, segmenting Chinese texts becomes an essential task. During this task, the problem of unknown word occurs. It is impossible to register all words in a dictionary as new words can always be created by combining characters. We propose a uniﬁed solution to detect unknown words in Chinese texts. First, a morphological analysis is done to obtain initial segmentation and POS tags and then a chunker is used to detect unknown words. 
This paper describes a Web-based English-Chinese concordance system, TotalRecall, developed to promote translation reuse and encourage authentic and idiomatic use in second language writing. We exploited and structured existing highquality translations from the bilingual Sinorama Magazine to build the concordance of authentic text and translation. Novel approaches were taken to provide high-precision bilingual alignment on the sentence, phrase and word levels. A browser-based user interface (UI) is also developed for ease of access over the Internet. Users can search for word, phrase or expression in English or Chinese. The Web-based user interface facilitates the recording of the user actions to provide data for further research. 
Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings. Unlike previous statistical formalisms (limited to isomorphic trees), synchronous TSG allows local distortion of the tree topology. We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.  
Recent work in Question Answering has focused on web-based systems that extract answers using simple lexicosyntactic patterns. We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions. We evaluate our strategy on a challenging subset of questions, i.e. “Who is …” questions, against a state of the art web-based Question Answering system. Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system. 
In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results. 
We introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an end-to-end QA system. Our noisy-channel system outperforms a stateof-the-art rule-based QA system that uses similar resources. We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing. 
Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP). In NLP, although feature combinations are crucial to improving performance, they are heuristically selected. Kernel methods change this situation. The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs. Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis. In this paper, we extend a Basket Mining algorithm to convert a kernel-based classiﬁer into a simple and fast linear classiﬁer. Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classiﬁers are about 30 to 300 times faster than the standard kernel-based classiﬁers. 
This paper proposes the “Hierarchical Directed Acyclic Graph (HDAG) Kernel” for structured natural language data. The HDAG Kernel directly accepts several levels of both chunks and their relations, and then efﬁciently computes the weighed sum of the number of common attribute sequences of the HDAGs. We applied the proposed method to question classiﬁcation and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function. The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods. 
Recent text and speech processing applications such as speech mining raise new and more general problems related to the construction of language models. We present and describe in detail several new and efﬁcient algorithms to address these more general problems and report experimental results demonstrating their usefulness. We give an algorithm for computing efﬁciently the expected counts of any sequence in a word lattice output by a speech recognizer or any arbitrary weighted automaton; ¢ describe a new technique for creating exact representa- tions of -gram language models by weighted automata whose size is practical for ofﬂine use even for a vocab- ¢ ulary size of about 500,000 words and an -gram order ¢¤£¦¥ ; and present a simple and more general technique for constructing class-based language models that allows each class to represent an arbitrary weighted automaton. An efﬁcient implementation of our algorithms and techniques has been incorporated in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities. 
We present a supervised machine learning algorithm for metonymy resolution, which exploits the similarity between examples of conventional metonymy. We show that syntactic head-modiﬁer relations are a high precision feature for metonymy recognition but suffer from data sparseness. We partially overcome this problem by integrating a thesaurus and introducing simpler grammatical features, thereby preserving precision and increasing recall. Our algorithm generalises over two levels of contextual similarity. Resulting inferences exceed the complexity of inferences undertaken in word sense disambiguation. We also compare automatic and manual methods for syntactic feature extraction. 
Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data. We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods. In contrast to previous work, we particularly focus on clustering polysemic verbs. A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data. 
We have aligned Japanese and English news articles and sentences to make a large parallel corpus. We ﬁrst used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles. However, the results included many incorrect alignments. To remove these, we propose two measures (scores) that evaluate the validity of alignments. The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR. They enhance each other to improve the accuracy of alignment. Using these measures, we have successfully constructed a largescale article and sentence alignment corpus available to the public. 
We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms. 
Word alignment plays a crucial role in statistical machine translation. Word-aligned corpora have been found to be an excellent source of translation-related knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-speciﬁc features. Our experiments show that this model can be an effective tool for improving an existing word alignment. 
We present a probabilistic parsing model for German trained on the Negra treebank. We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German. Learning curves show that this effect is not due to lack of training data. We propose an alternative model that uses sister-head dependencies instead of head-head dependencies. This model outperforms the baseline, achieving a labeled precision and recall of up to 74%. This indicates that sister-head dependencies are more appropriate for treebanks with very ﬂat structures such as Negra. 
We present a novel, data-driven method for integrated shallow and deep parsing. Mediated by an XML-based multi-layer annotation architecture, we interleave a robust, but accurate stochastic topological ﬁeld parser of German with a constraintbased HPSG parser. Our annotation-based method for dovetailing shallow and deep phrasal constraints is highly ﬂexible, allowing targeted and ﬁne-grained guidance of constraint-based parsing. We conduct systematic experiments that demonstrate substantial performance gains.1 
The paper describes two parsing schemes: a shallow approach based on machine learning and a cascaded ﬁnite-state parser with a hand-crafted grammar. It discusses several ways to combine them and presents evaluation results for the two individual approaches and their combination. An underspeciﬁcation scheme for the output of the ﬁnite-state parser is introduced and shown to improve performance. 
Automatically acquiring synonymous collocation pairs such as <turn on, OBJ, light> and <switch on, OBJ, light> from corpora is a challenging task. For this task, we can, in general, have a large monolingual corpus and/or a very limited bilingual corpus. Methods that use monolingual corpora alone or use bilingual corpora alone are apparently inadequate because of low precision or low coverage. In this paper, we propose a method that uses both these resources to get an optimal compromise of precision and coverage. This method first gets candidates of synonymous collocation pairs based on a monolingual corpus and a word thesaurus, and then selects the appropriate pairs from the candidates using their translations in a second language. The translations of the candidates are obtained with a statistical translation model which is trained with a small bilingual corpus and a large monolingual corpus. The translation information is proved as effective to select synonymous collocation pairs. Experimental results indicate that the average precision and recall of our approach are 74% and 64% respectively, which outperform those methods that only use monolingual corpora and those that only use bilingual corpora. 
Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 
Standard IR systems can process queries such as “web NOT internet”, enabling users who are interested in arachnids to avoid documents about computing. The documents retrieved for such a query should be irrelevant to the negated query term. Most systems implement this by reprocessing results after retrieval to remove documents containing the unwanted string of letters. This paper describes and evaluates a theoretically motivated method for removing unwanted meanings directly from the original query in vector models, with the same vector negation operator as used in quantum logic. Irrelevance in vector spaces is modelled using orthogonality, so query vectors are made orthogonal to the negated term or terms. As well as removing unwanted terms, this form of vector negation reduces the occurrence of synonyms and neighbours of the negated terms by as much as 76% compared with standard Boolean methods. By altering the query vector itself, vector negation removes not only unwanted strings but unwanted meanings. 
In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary wordreorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm. In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints. This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. We show a connection between the ITG constraints and the since 1870 known Schro¨der numbers. We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task. The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints. Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses. The experiments will show that the baseline ITG constraints are not sufﬁcient on the Canadian Hansards task. Therefore, we present an extension to the ITG constraints. These extended ITG constraints increase the alignment coverage from about 87% to 96%.  
Truecasing is the process of restoring case information to badly-cased or noncased text. This paper explores truecasing issues and proposes a statistical, language modeling based truecaser which achieves an accuracy of ∼98% on news articles. Task based evaluation shows a 26% F-measure improvement in named entity recognition when using truecasing. In the context of automatic content extraction, mention detection on automatic speech recognition text is also improved by a factor of 8. Truecasing also enhances machine translation output legibility and yields a BLEU score improvement of 80.2%. This paper argues for the use of truecasing as a valuable component in text processing applications. 
Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the ﬁnal translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efﬁcient training an unsmoothed error count. We show that signiﬁcantly better results can often be obtained if the ﬁnal evaluation criterion is taken directly into account as part of the training procedure. 
We apply a decision tree based approach to pronoun resolution in spoken dialogue. Our system deals with pronouns with NPand non-NP-antecedents. We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features. We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron’s (2002) manually tuned system. 
In this paper we propose a competition learning approach to coreference resolution. Traditionally, supervised machine learning approaches adopt the singlecandidate model. Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. By contrast, our approach adopts a twin-candidate learning model. Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the singlecandidate model. 
We introduce a MetaGrammar, which allows us to automatically generate, from a single and compact MetaGrammar hierarchy, parallel Lexical Functional Grammars (LFG) and Tree-Adjoining Grammars (TAG) for French and for English: the grammar writer speciﬁes in compact manner syntactic properties that are potentially framework-, and to some extent language-independent (such as subcategorization, valency alternations and realization of syntactic functions), from which grammars for several frameworks and languages are automatically generated ofﬂine.1 
This paper proposes the application of ﬁnite-state approximation techniques on a uniﬁcation-based grammar of word formation for a language like German. A reﬁnement of an RTN-based approximation algorithm is proposed, which extends the state space of the automaton by selectively adding distinctions based on the parsing history at the point of entering a context-free rule. The selection of history items exploits the speciﬁc linguistic nature of word formation. As experiments show, this algorithm avoids an explosion of the size of the automaton in the approximation construction. 
This paper presents a new bottom-up chart parsing algorithm for Prolog along with a compilation procedure that reduces the amount of copying at run-time to a constant number (2) per edge. It has applications to uniﬁcation-based grammars with very large partially ordered categories, in which copying is expensive, and can facilitate the use of more sophisticated indexing strategies for retrieving such categories that may otherwise be overwhelmed by the cost of such copying. It also provides a new perspective on “quick-checking” and related heuristics, which seems to conﬁrm that forcing an early failure (as opposed to seeking an early guarantee of success) is in fact the best approach to use. A preliminary empirical evaluation of its performance is also provided. 
Many applications of natural language processing technologies involve analyzing texts that concern the psychological states and processes of people, including their beliefs, goals, predictions, explanations, and plans. In this paper, we describe our efforts to create a robust, large-scale lexical-semantic resource for the recognition and classification of expressions of commonsense psychology in English Text. We achieve high levels of precision and recall by hand-authoring sets of local grammars for commonsense psychology concepts, and show that this approach can achieve classification performance greater than that obtained by using machine learning techniques. We demonstrate the utility of this resource for large-scale corpus analysis by identifying references to adversarial and competitive goals in political speeches throughout U.S. history. 
In this paper, we present a learning approach to the scenario template task of information extraction, where information ﬁlling one template could come from multiple sentences. When tested on the MUC4 task, our learning approach achieves accuracy competitive to the best of the MUC-4 systems, which were all built with manually engineered rules. Our analysis reveals that our use of full parsing and state-of-the-art learning algorithms have contributed to the good performance. To our knowledge, this is the ﬁrst research to have demonstrated that a learning approach to the full-scale information extraction task could achieve performance rivaling that of the knowledgeengineering approach. 
Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction. Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain. The effect of these alternative models has not been previously studied. In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns. 
Link detection has been regarded as a core technology for the Topic Detection and Tracking tasks of new event detection. In this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of precision and recall on both systems. Motivated by these arguments, we introduce a number of new performance enhancing techniques including part of speech tagging, new similarity measures and expanded stop lists. Experimental results validate our hypothesis. 
This paper concerns the discourse understanding process in spoken dialogue systems. This process enables the system to understand user utterances based on the context of a dialogue. Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding, it is not appropriate to decide on a single understanding result after each user utterance. By holding multiple candidates for understanding results and resolving the ambiguity as the dialogue progresses, the discourse understanding accuracy can be improved. This paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpora. Unlike conventional methods that use hand-crafted rules, the proposed method enables easy design of the discourse understanding process. Experiment results have shown that a system that exploits the proposed method performs sufﬁciently and that holding multiple candidates for understanding results is effective. †Currently with the School of Media Science, Tokyo University of Technology, 1404-1 Katakuracho, Hachioji, Tokyo 192-0982, Japan.  
 This paper discusses the challenges and proposes a solution to performing information retrieval on the Web using Chinese natural language speech query. The main contribution of this research is in devising a divide-and-conquer strategy to alleviate the speech recognition errors. It uses the query model to facilitate the extraction of main core semantic string (CSS) from the Chinese natural language speech query. It then breaks the CSS into basic components corresponding to phrases, and uses a multi-tier strategy to map the basic components to known phrases in order to further eliminate the errors. The resulting system has been found to be effective. 
We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems. Unlike previous studies that focus on user’s knowledge or typical kinds of users, the user model we propose is more comprehensive. Speciﬁcally, we set up three dimensions of user models: skill level to the system, knowledge level on the target domain and the degree of hastiness. Moreover, the models are automatically derived by decision tree learning using real dialogue data collected by the system. We obtained reasonable classiﬁcation accuracy for all dimensions. Dialogue strategies based on the user modeling are implemented in Kyoto city bus information system that has been developed at our laboratory. Experimental evaluation shows that the cooperative responses adaptive to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users. 
Pipelined Natural Language Generation (NLG) systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions, lexical choice, and revision. This has given rise to discussions about the relative placement of these new modules in the overall architecture. Recent work on another aspect of multi-paragraph text, discourse markers, indicates it is time to consider where a discourse marker insertion algorithm ﬁts in. We present examples which suggest that in a pipelined NLG architecture, the best approach is to strongly tie it to a revision component. Finally, we evaluate the approach in a working multi-page system. 
This paper presents a Chinese word segmentation system that uses improved sourcechannel models of Chinese sentence generation. Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities. Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition. The performance of the system is evaluated on a manually annotated test set, and is also compared with several state-ofthe-art systems, taking into account the fact that the definition of Chinese words often varies from system to system. 
We present a language-independent and unsupervised algorithm for the segmentation of words into morphs. The algorithm is based on a new generative probabilistic model, which makes use of relevant prior information on the length and frequency distributions of morphs in a language. Our algorithm is shown to outperform two competing algorithms, when evaluated on data from a language with agglutinative morphology (Finnish), and to perform well also on English data. 
It is well known that occurrence counts of words in documents are often modeled poorly by standard distributions like the binomial or Poisson. Observed counts vary more than simple models predict, prompting the use of overdispersed models like Gamma-Poisson or Beta-binomial mixtures as robust alternatives. Another deﬁciency of standard models is due to the fact that most words never occur in a given document, resulting in large amounts of zero counts. We propose using zeroinﬂated models for dealing with this, and evaluate competing models on a Naive Bayes text classiﬁcation task. Simple zero-inﬂated models can account for practically relevant variation, and can be easier to work with than overdispersed models. 
This paper presents a method to develop a class of variable memory Markov models that have higher memory capacity than traditional (uniform memory) Markov models. The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm. A series of comparative experiments show the resulting models outperform uniform memory Markov models in a part-of-speech tagging task. 
This paper describes an alternative translation model based on a text chunk under the framework of statistical machine translation. The translation model suggested here ﬁrst performs chunking. Then, each word in a chunk is translated. Finally, translated chunks are reordered. Under this scenario of translation modeling, we have experimented on a broadcoverage Japanese-English traveling corpus and achieved improved performance. 
We deﬁne noun phrase translation as a subtask of machine translation. This enables us to build a dedicated noun phrase translation subsystem that improves over the currently best general statistical machine translation methods by incorporating special modeling and special features. We achieved 65.5% translation accuracy in a German-English translation task vs. 53.2% with IBM Model 4.  We go on to tackle the task of noun phrase translation in a maximum entropy reranking framework. Treating translation as a reranking problem instead of as a search problem enables us to use features over the full translation pair. We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on deﬁning subtasks within statistical machine translation has been performed on, e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002).  
Phrase level translation models are effective in improving translation quality by addressing the problem of local re-ordering across language boundaries. Methods that attempt to fundamentally modify the traditional IBM translation model to incorporate phrases typically do so at a prohibitive computational cost. We present a technique that begins with improved IBM models to create phrase level knowledge sources that effectively represent local as well as global phrasal context. Our method is robust to noisy alignments at both the sentence and corpus level, delivering high quality phrase level translation pairs that contribute to significant improvements in translation quality (as measured by the BLEU metric) over word based lexica as well as a competing alignment based method.  
This paper proposes the use of uncertainty reduction in machine learning methods such as co-training and bilingual bootstrapping, which are referred to, in a general term, as ‘collaborative bootstrapping’. The paper indicates that uncertainty reduction is an important factor for enhancing the performance of collaborative bootstrapping. It proposes a new measure for representing the degree of uncertainty correlation of the two classifiers in collaborative bootstrapping and uses the measure in analysis of collaborative bootstrapping. Furthermore, it proposes a new algorithm of collaborative bootstrapping on the basis of uncertainty reduction. Experimental results have verified the correctness of the analysis and have demonstrated the significance of the new algorithm. 
This paper presents a new bootstrapping approach to named entity (NE) classification. This approach only requires a few common noun/pronoun seeds that correspond to the concept for the target NE type, e.g. he/she/man/woman for PERSON NE. The entire bootstrapping procedure is implemented as training two successive learners: (i) a decision list is used to learn the parsing-based high precision NE rules; (ii) a Hidden Markov Model is then trained to learn string sequence-based NE patterns. The second learner uses the training corpus automatically tagged by the first learner. The resulting NE system approaches supervised NE performance for some NE types. The system also demonstrates intuitive support for tagging user-defined NE types. The differences of this approach from the co-training-based NE bootstrapping are also discussed. 
This paper presents a method for unsupervised discovery of semantic patterns. Semantic patterns are useful for a variety of text understanding tasks, in particular for locating events in text for information extraction. The method builds upon previously described approaches to iterative unsupervised pattern acquisition. One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision. Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously. This provides natural stopping criteria for the unsupervised learners, while maintaining good precision levels at termination. We discuss the results of experiments with several scenarios, and examine different aspects of the new procedure. 
This paper is concerned with learning categorial grammars in Gold’s model. In contrast to k-valued classical categorial grammars, k-valued Lambek grammars are not learnable from strings. This result was shown for several variants but the question was left open for the weakest one, the non-associative variant NL. We show that the class of rigid and kvalued NL grammars is unlearnable from strings, for each k; this result is obtained by a speciﬁc construction of a limit point in the considered class, that does not use product operator. Another interest of our construction is that it provides limit points for the whole hierarchy of Lambek grammars, including the recent pregroup grammars. Such a result aims at clarifying the possible directions for future learning algorithms: it expresses the difﬁculty of learning categorial grammars from strings and the need for an adequate structure on examples. 
The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al. (2002), and deﬁnes a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. 
Minimal Recursion Semantics (MRS) is the standard formalism used in large-scale HPSG grammars to model underspeciﬁed semantics. We present the ﬁrst provably efﬁcient algorithm to enumerate the readings of MRS structures, by translating them into normal dominance constraints. 
We present a large-scale meta evaluation of eight evaluation measures for both single-document and multi-document summarizers. To this end we built a corpus consisting of (a) 100 Million automatic summaries using six summarizers and baselines at ten summary lengths in both English and Chinese, (b) more than 10,000 manual abstracts and extracts, and (c) 200 Million automatic document and summary retrievals using 20 queries. We present both qualitative and quantitative results showing the strengths and drawbacks of all evaluation methods and how they rank the different summarizers.  agree which sentences should be in a summary) or too expensive (an approach using manual judgements can scale up to a few hundred summaries but not to tens or hundreds of thousands). In this paper, we present a comparison of six summarizers as well as a meta-evaluation including eight measures: Precision/Recall, Percent Agreement, Kappa, Relative Utility, Relevance Correlation, and three types of Content-Based measures (cosine, longest common subsequence, and word overlap). We found that while all measures tend to rank summarizers in different orders, measures like Kappa, Relative Utility, Relevance Correlation and Content-Based each offer signiﬁcant advantages over the more simplistic methods. 2 Data, Annotation, and Experimental Design  
This paper presents an unsupervised learning approach to building a non-English (Arabic) stemmer. The stemming model is based on statistical machine translation and it uses an English stemmer and a small (10K sentences) parallel corpus as its sole training resources. No parallel text is needed after the training phase. Monolingual, unannotated text can be used to further improve the stemmer by allowing it to adapt to a desired domain or genre. Examples and results will be given for Arabic , but the approach is applicable to any language that needs afﬁx removal. Our resource-frugal approach results in 87.5% agreement with a state of the art, proprietary Arabic stemmer built using rules, afﬁx lists, and human annotated text, in addition to an unsupervised component. Task-based evaluation using Arabic information retrieval indicates an improvement of 22-38% in average precision over unstemmed text, and 96% of the performance of the proprietary stemmer above. 
We approximate Arabic’s rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest.  
An investment of effort over the last two years has begun to produce a wealth of data concerning computational psycholinguistic models of syntax acquisition. The data is generated by running simulations on a recently completed database of word order patterns from over 3,000 abstract languages. This article presents the design of the database which contains sentence patterns, grammars and derivations that can be used to test acquisition models from widely divergent paradigms. The domain is generated from grammars that are linguistically motivated by current syntactic theory and the sentence patterns have been validated as psychologically/developmentally plausible by checking their frequency of occurrence in corpora of child-directed speech. A small case-study simulation is also presented. 
We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-theart. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize. In the early 1990s, as probabilistic methods swept NLP, parsing work revived the investigation of probabilistic context-free grammars (PCFGs) (Booth and Thomson, 1973; Baker, 1979). However, early results on the utility of PCFGs for parse disambiguation and language modeling were somewhat disappointing. A conviction arose that lexicalized PCFGs (where head words annotate phrasal nodes) were the key tool for high performance PCFG parsing. This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (Ford et al., 1982; Hindle and Rooth, 1993). In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001). However, several results have brought into question how large a role lexicalization plays in such parsers. Johnson (1998) showed that the perfor-  mance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category. The Penn treebank covering PCFG is a poor tool for parsing because the context-freedom assumptions it embodies are far too strong, and weakening them in this way makes the model much better. More recently, Gildea (2001) discusses how taking the bilexical probabilities out of a good current lexicalized PCFG parser hurts performance hardly at all: by at most 0.5% for test text from the same domain as the training data, and not at all for test text from a different domain.1 But it is precisely these bilexical dependencies that backed the intuition that lexicalized PCFGs should be very successful, for example in Hindle and Rooth’s demonstration from PP attachment. We take this as a reﬂection of the fundamental sparseness of the lexical dependency information available in the Penn Treebank. As a speech person would say, one million words of training data just isn’t enough. Even for topics central to the treebank’s Wall Street Journal text, such as stocks, many very plausible dependencies occur only once, for example stocks stabilized, while many others occur not at all, for example stocks skyrocketed.2 The best-performing lexicalized PCFGs have increasingly made use of subcategorization3 of the 1There are minor differences, but all the current best-known lexicalized PCFGs employ both monolexical statistics, which describe the phrasal categories of arguments and adjuncts that appear around a head lexical item, and bilexical statistics, or dependencies, which describe the likelihood of a head word taking as a dependent a phrase headed by a certain other word. 2This observation motivates various class- or similaritybased approaches to combating sparseness, and this remains a promising avenue of work, but success in this area has proven somewhat elusive, and, at any rate, current lexicalized PCFGs do simply use exact word matches if available, and interpolate with syntactic category-based estimates when they are not. 3In this paper we use the term subcategorization in the original general sense of Chomsky (1965), for where a syntactic cat-  categories appearing in the Penn treebank. Charniak (2000) shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating “base NPs” from noun phrases with phrasal modiﬁers, and distinguishing sentences with empty subjects from those where there is an overt subject NP. While he gives incomplete experimental results as to their efﬁcacy, we can assume that these features were incorporated because of beneﬁcial effects on parsing that were complementary to lexicalization. In this paper, we show that the parsing performance that can be achieved by an unlexicalized PCFG is far higher than has previously been demonstrated, and is, indeed, much higher than community wisdom has thought possible. We describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla PCFG and state-of-the-art lexicalized models. Speciﬁcally, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of Magerman (1995) and Collins (1996) (though not more recent models, such as Charniak (1997) or Collins (1999)). One beneﬁt of this result is a much-strengthened lower bound on the capacity of an unlexicalized PCFG. To the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneﬁcial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both needed to make the right decision and available in the training data. Secondly, this result afﬁrms the value of linguistic analysis for feature discovery. The result has other uses and advantages: an unlexicalized PCFG is easier to interpret, reason about, and improve than the more complex lexicalized models. The grammar representation is much more compact, no longer requiring large structures that store lexicalized probabilities. The parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories, for example dividing verb phrases into ﬁnite and non-ﬁnite verb phrases, rather than in the modern restricted usage where the term refers only to the syntactic argument frames of predicators. 4 O(n3) vs. O(n5) for a naive implementation, or vs. O(n4) if using the clever approach of Eisner and Satta (1999).  constants. An unlexicalized PCFG parser is much simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (Caraballo and Charniak, 1998; Charniak et al., 1998). It is not our goal to argue against the use of lexicalized probabilities in high-performance probabilistic parsing. It has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities, and a parser should make use of such information where possible. We focus here on using unlexicalized, structural context because we feel that this information has been underexploited and underappreciated. We see this investigation as only one part of the foundation for state-of-the-art parsing which employs both lexical and structural conditioning. 
We present a novel approach for ﬁnding discontinuities that outperforms previously published results on this task. Rather than using a deeper grammar formalism, our system combines a simple unlexicalized PCFG parser with a shallow pre-processor. This pre-processor, which we call a trace tagger, does surprisingly well on detecting where discontinuities can occur without using phase structure information. 
When rules of transfer-based machine translation (MT) are automatically acquired from bilingual corpora, incorrect/redundant rules are generated due to acquisition errors or translation variety in the corpora. As a new countermeasure to this problem, we propose a feedback cleaning method using automatic evaluation of MT quality, which removes incorrect/redundant rules as a way to increase the evaluation score. BLEU is utilized for the automatic evaluation. The hillclimbing algorithm, which involves features of this task, is applied to searching for the optimal combination of rules. Our experiments show that the MT quality improves by 10% in test sentences according to a subjective evaluation. This is considerable improvement over previous methods. 
A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning. In this paper, we evaluate an approach to automatically acquire sensetagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task. Our investigation reveals that this method of acquiring sense-tagged data is promising. On a subset of the most difficult SENSEVAL-2 nouns, the accuracy difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage. Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs. 
This paper describes a method for learning the countability preferences of English nouns from raw text corpora. The method maps the corpus-attested lexico-syntactic properties of each noun onto a feature vector, and uses a suite of memory-based classiﬁers to predict membership in 4 countability classes. We were able to assign countability to English nouns with a precision of 94.6%. 
Noun extraction is very important for many NLP applications such as information retrieval, automatic text classiﬁcation, and information extraction. Most of the previous Korean noun extraction systems use a morphological analyzer or a Partof-Speech (POS) tagger. Therefore, they require much of the linguistic knowledge such as morpheme dictionaries and rules (e.g. morphosyntactic rules and morphological rules). This paper proposes a new noun extraction method that uses the syllable based word recognition model. It ﬁnds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries. Furthermore, it does not require any labor for constructing and maintaining linguistic knowledge. We have performed various experiments with a wide range of variables inﬂuencing the performance. The experimental results show that without morphological analysis or POS tagging, the proposed method achieves comparable performance with the previous methods. 
This paper describes two methods for detecting word segments and their morphological information in a Japanese spontaneous speech corpus, and describes how to tag a large spontaneous speech corpus accurately by using the two methods. The ﬁrst method is used to detect any type of word segments. The second method is used when there are several deﬁnitions for word segments and their POS categories, and when one type of word segments includes another type of word segments. In this paper, we show that by using semiautomatic analysis we achieve a precision of better than 99% for detecting and tagging short words and 97% for long words; the two types of words that comprise the corpus. We also show that better accuracy is achieved by using both methods than by using only the ﬁrst. 
This paper proposes a hybrid of handcrafted rules and a machine learning method for chunking Korean. In the partially free word-order languages such as Korean and Japanese, a small number of rules dominate the performance due to their well-developed postpositions and endings. Thus, the proposed method is primarily based on the rules, and then the residual errors are corrected by adopting a memory-based machine learning method. Since the memory-based learning is an efﬁcient method to handle exceptions in natural language processing, it is good at checking whether the estimates are exceptional cases of the rules and revising them. An evaluation of the method yields the improvement in F-score over the rules or various machine learning methods alone. 
Supertagging is the tagging process of assigning the correct elementary tree of LTAG, or the correct supertag, to each word of an input sentence1. In this paper we propose to use supertags to expose syntactic dependencies which are unavailable with POS tags. We ﬁrst propose a novel method of applying Sparse Network of Winnow (SNoW) to sequential models. Then we use it to construct a supertagger that uses long distance syntactical dependencies, and the supertagger achieves an accuracy of ¢¤£¦¥¨§© . We apply the supertagger to NP chunking. The use of supertags in NP chunking gives rise to almost © absolute increase (from ¢¤£¦¥ to ¢¤£¦¥¢¤ ) in F-score under Transformation Based Learning(TBL) frame. The surpertagger described here provides an effective and efﬁcient way to exploit syntactic information. 
Phrasal Verbs are an important feature of the English language. Properly identifying them provides the basis for an English parser to decode the related structures. Phrasal verbs have been a challenge to Natural Language Processing (NLP) because they sit at the borderline between lexicon and syntax. Traditional NLP frameworks that separate the lexicon module from the parser make it difficult to handle this problem properly. This paper presents a finite state approach that integrates a phrasal verb expert lexicon between shallow parsing and deep parsing to handle morpho-syntactic interaction. With precision/recall combined performance benchmarked consistently at 95.8%-97.5%, the Phrasal Verb identification problem has basically been solved with the presented method. 
This paper presents a dependency language model (DLM) that captures linguistic constraints via a dependency structure, i.e., a set of probabilistic dependencies that express the relations between headwords of each phrase in a sentence by an acyclic, planar, undirected graph. Our contributions are three-fold. First, we incorporate the dependency structure into an n-gram language model to capture long distance word dependency. Second, we present an unsupervised learning method that discovers the dependency structure of a sentence using a bootstrapping procedure. Finally, we evaluate the proposed models on a realistic application (Japanese Kana-Kanji conversion). Experiments show that the best DLM achieves an 11.3% error rate reduction over the word trigram model. 
We describe the ongoing construction of a large, semantically annotated corpus resource as reliable basis for the largescale acquisition of word-semantic information, e.g. the construction of domainindependent lexica. The backbone of the annotation are semantic roles in the frame semantics paradigm. We report experiences and evaluate the annotated data from the ﬁrst project stage. On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation. 
Ordering information is a critical task for natural language generation applications. In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation. We describe a model that learns constraints on sentence order from a corpus of domainspeciﬁc texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model’s task. We also assess the appropriateness of such a model for multidocument summarization. 
We investigate the verbal and nonverbal means for grounding, and propose a design for embodied conversational agents that relies on both kinds of signals to establish common ground in human-computer interaction. We analyzed eye gaze, head nods and attentional focus in the context of a direction-giving task. The distribution of nonverbal behaviors differed depending on the type of dialogue move being grounded, and the overall pattern reflected a monitoring of lack of negative feedback. Based on these results, we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state. 
We present a domain-independent topic segmentation algorithm for multi-party speech. Our feature-based algorithm combines knowledge about content using a text-based algorithm as a feature and about form using linguistic and acoustic cues about topic shifts extracted from speech. This segmentation algorithm uses automatically induced decision rules to combine the different features. The embedded text-based algorithm builds on lexical cohesion and has performance comparable to state-of-the-art algorithms based on lexical information. A signiﬁcant error reduction is obtained by combining the two knowledge sources. 
This paper presents a method of Chinese named entity (NE) identification using a class-based language model (LM). Our NE identification concentrates on three types of NEs, namely, personal names (PERs), location names (LOCs) and organization names (ORGs). Each type of NE is defined as a class. Our language model consists of two sub-models: (1) a set of entity models, each of which estimates the generative probability of a Chinese character string given an NE class; and (2) a contextual model, which estimates the generative probability of a class sequence. The class-based LM thus provides a statistical framework for incorporating Chinese word segmentation and NE identification in a unified way. This paper also describes methods for identifying nested NEs and NE abbreviations. Evaluation based on a test data with broad coverage shows that the proposed model achieves the performance of state-of-the-art Chinese NE identification systems. Keywords: Named entity identification, class-based language model, contextual model, entity model 1. Introduction Named Entity (NE) identification is the problem of detecting entity names in documents and then classifying them into corresponding categories. This is an important step in many natural language processing applications, such as information extraction (IE), question answering (QA), and machine translation (MT). A lot of researches have been carried out on English NE identification. As a result, some systems have been widely applied in practice. On the other hand, Chinese NE identification is a different task because in Chinese, there is no space to mark the boundaries of words and no clear definition of words. In addition, Chinese NE 
This paper presents a stochastic model to tackle the problem of Chinese named entity recognition. In this research, we unify component tokens of named entity and their contexts into a generalized role set, which is like part-of-speech (POS). The probabilities of role emission and transition are acquired after machine learning on a role-labeled data set, which is transformed from a hand-corrected corpus after word segmentation and POS tagging are performed. Given an original string, role Viterbi tagging is employed on tokens segmented in the initial process. Then named entities are identified and classified through maximum matching on the best role sequence. In addition, named entity recognition using role model is incorporated along with the unified class-based bigram model for word segmentation. Thus, named entity candidates can be further selected in the final process of Chinese lexical analysis. Various evaluations conducted using one  
Semantic lexicons are indispensable to research in lexical semantics and word sense disambiguation (WSD). For the study of WSD for English text, researchers have been using different kinds of lexicographic resources, including machine readable dictionaries (MRDs), machine readable thesauri, and bilingual corpora. In recent years, WordNet has become the most widely used resource for the study of WSD and lexical semantics in general. This paper describes the Class-Based Translation Model and its application in assigning translations to nominal senses in WordNet in order to build a prototype Chinese WordNet. Experiments and evaluations show that the proposed approach can potentially be adopted to speed up the construction of WordNet for Chinese and other languages. 1. Introduction WordNet has received widespread interest since its introduction in 1990 [Miller 1990]. As a large-scale semantic lexical database, WordNet covers a large vocabulary, similar to a typical college dictionary, but its information is organized differently. The synonymous word senses are grouped into so-called synsets. Noun senses are further organized into a deep IS-A hierarchy. The database also contains many semantic relations, including hypernyms, hyponyms, holonyms, meronyms, etc. WordNet has been applied in a wide range of studies on  * Department of Computer Science, National Tsing Hua University  101, Sec. 2, Kuang Fu Road, Hsinchu, Taiwan, ROC  E-mail: jschang@cs.nthu.edu.tw  + Department of Communication Engineering, National Chiao Tung University  1001, University Road, Hsinchu, Taiwan, ROC  E-mail: tracylin@mail.nctu.edu.tw  ** Department of Information Manangement, National Taichung Institute of Technology  San Ming Road, Taichung, Taiwan, ROC ++ Dept of Computer Science, Van Nung Institute of Technology  E-mail: gny@mail.ntit.edu.tw  
Automatic extraction of bilingual Multi-Word Units is an important subject of research in the automatic bilingual corpus alignment field. There are many cases of single source words corresponding to target multi-word units. This paper presents an algorithm for the automatic alignment of single source words and target multi-word units from a sentence-aligned parallel spoken language corpus. On the other hand, the output can be also used to extract bilingual multi-word units. The problem with previous approaches is that the retrieval results mainly depend on the identification of suitable Bi-grams to initiate the iterative process. To extract multi-word units, this algorithm utilizes the normalized association score difference of multi target words corresponding to the same single source word, and then utilizes the average association score to align the single source words and target multi-word units. The algorithm is based on the Local Bests algorithm supplemented by two heuristic strategies: excluding words in a stop-list and preferring longer multi-word units. Key words: bilingual alignment; multiword unit; translation lexicon; average association score; normalized association score difference; 1. Introduction 1.1 The Background of Automatic Extraction of Bilingual Multi-Word Units In the natural language processing field, which includes machine translation, machine assistant translation, bilingual lexicon compilation, terminology, information retrieval, natural language generation, second language teaching etc., the automatic extraction of bilingual multi-word units (steady collocations, multi-word phrases, multi-word terms etc.) is an * Center for Speech Interaction Technology Research, Institute of Acoustics, Chinese Academy of Sciences Address: 17 Zhongguancun Rd. Beijing 100080, China E-mail: {chenbx , dulm}@iis.ac.cn  Boxing Chen and Limin Du  important aspect of the automatic alignment of bilingual corpus technology. Since the 1980’s, the technique of automatic alignment of a bilingual corpus has undergone great improvement; and during the mid- and late-1990’s, many researchers began to research the automatic construction of a bilingual translation lexicon [Fung 1995; Wu et al. 1995; Hiemstra 1996; Melamed 1996 etc.] Their works have focused on the alignment of single words. At the same time, the extraction of multi-word units in singular languages has been also studied. Church utilized mutual information to evaluate the degree of association between two words [Church 1990]; hence, mutual information has played an important role in multi-word unit extraction research, and it is used most often with this technology by means of a statistical method. Many researchers [Smadja 1993; Nagao et al. 1994; Kita et al. 1994; Zhou et al. 1995; Shimohata et al. 1997; Yamamoto et al. 1998] have utilized mutual information (or the transformation of mutual information) as an important parameter to extract multi-word units. The shortcoming of these methods is that low frequency multi-word units are easy to eliminate, and the output of extraction mainly depends on the verification of suitable Bi-grams when the iterative algorithm initiates. Automatic extraction of bilingual multi-word units is based on the automatic extraction of bilingual word and multi-word units in singular languages. Research in this field has also proceeded [Smadja et al. 1996; Haruno et al. 1996; Melamed 1997 etc], but the problem with this approach is that it relies on statistical methods more than the characteristics of the language per se and is mainly limited to the extraction of noun phrases. Because of the above problems and the fact that Chinese-English corpuses are commonly small, we provide an algorithm that uses the average association score and normalized association score difference. We also apply the Local Bests algorithm, stopword filtration and longer unit preference methods to extract Chinese or English multi-word units.  1.2 The Object of Our Research  In research on the results produced by single-English-word to single-Chinese-word alignment, we have found an interesting phenomenon: During the phase of Chinese word segmentation, if the translation of an English word (“A”) comprises of several Chinese words (“BCD”), the mutual information and the t-score for each “B-A, C-A, D-A” mapping are both very high and close to each other. Thus, we can use the average association score and the normalized association score difference to extract the translation equivalent pairs of single-English-word to multiple-Chinese-word mappings.  For example, when names and professional terms are translated, “Patterson” is translated  as “  ,” which includes three entries in a Chinese dictionary (“ ,” “ ,” and “ ”);  “Internet” is translated as “  ,” which includes three entries in a Chinese dictionary  Preparatory Work on Automatic Extraction of Bilingual Multi-Word Units from Parallel Corpora  (“ ,” “ ,” and “ ”). Furthermore, the same situation occurs with some non-professional  terms. For example, “my” is translated as “ .” Also, the same rule applies to  Chinese-English translation. For example, “  ” is translated as “get funny,” and “  ” as “get fresh.”  Therefore, the research presented in this paper is focused on single-source-word to multi-target-word-unit alignment. The alignment of bilingual multi-word units will be the focus of our future research.  2. Algorithm  The method we use to align single source words with target multi-word units from a parallel corpus can be divided into the following steps (we use the mutual information and t-score as the association score): (1) Word segmentation: We do word segmentation first because Chinese has no word delimiters. (2) Calculating the co-occurrence frequency: If a word pair appears once in an aligned bilingual sentence pair, one co-occurrence is counted. (3) Computing the association score of single word pairs: We calculate the mutual information and t-score of the source words and their co-occurrence target words. (4) Calculating the average association score and normalized association score: We calculate the average mutual information and normalized mutual information difference, and the average t-score and normalized t-score difference of every source word and its co-occurrence target words’ N-gram (N: 2-7, since most phrases have of 2-6 words). (5) The Local Bests algorithm: We utilize the Local Bests algorithm to eliminate non-local best target multi-word units. (6) Stop-word list filtration: Some words cannot be used as the first or the last word of a multi-word unit, so we use the stop-word list to filter these multi-word units. (7) Bigger association score preference: After the above filtration, from among the remaining multi-word units, we choose N items with the maximal average mutual information and average t-score as the  Boxing Chen and Limin Du  candidate target translation.  (8) Longer unit preference:  We extract multi-word units but not words, so if the longer word string C1 entirely contains another shorter word string C2, then string C1 is taken as the translation of the source word.  (9) Lexicon classification:  According to the above four parameters, we classify the lexicons into four levels of translation lexicons.  We will use “Glasgow:  ,” which appears in the corpus as shown in Figure 1, as  an example to explain the whole process.  Figure 1. Sentence Example. The reasons why we choose “Glasgow” are: (1) the occurrence frequency of “Glasgow” is quite low, only two times, which is easily ignored by the previous algorithm; (2) the Chinese translation of “Glasgow” is unique, so the correct extraction of this lemma can prove the accuracy of our algorithm; (3) “Glasgow” contains four single-character words, and it will be found later that our algorithm is more effective with multi-word units made up of two words, so here we use “Glasgow” to prove that our algorithm is also effective with multi-word units made up of more than two words. 2.1 Chinese Word Segmentation We used the “maximum probability word segmentation method” [Chen 1999] and The Grammatical Knowledge-base of Contemporary Chinese published by Peking University [Yu 1998]. The idea behind this method is: first find out all the possible words in the input Chinese string on a vocabulary basis and then find out all the possible segmentation paths, from which we can find the best path (with the maximal probability) as the output. We randomly sampled 1000 sentences to check: if we did not take “un-listed words that are divided” as an error, then the precision rate was 98.88%; but if it was being taken as an error, the precision rate was 88.74%. The unlisted words in DECC1.0 (Daily English-Chinese Corpus) were mainly the Chinese translations of foreign personal names and place names. The main focus of our research here was the aggregation of single Chinese characters that are produced through  Preparatory Work on Automatic Extraction of Bilingual Multi-Word Units from Parallel Corpora segmentation. The results of word segmentation are shown in Figure 2: Figure 2. Word Segmentation Results. 2.2 Calculate the Co-occurrence Frequency There were many translation sentence pairs in the corpus. For each possible word pair in these translation sentence pairs, the higher the probability of appearance it had, the higher the probability it had of being the correct translation word pair. We built a co-occurrence model to count the number of appearances: it was counted as a co-occurrence each time the word pair appears in a sentence pair. The reasons are as follows: First, the length of a sentence in spoken language is usually shorter than that in a written language; for example, in the corpus DECC1.0, the average length of English sentences is 7.07 words, and the average length of Chinese sentences is 6.87 words and expressions. Secondly, the corresponding sense units of English-Chinese sentence pairs in spoken language are not always aligned in terms of position, as shown in Figure 3.  Figure 3. Example of Word Alignment.  2.3 Calculate the Mutual Information and T-Score Having calculated the word pair’s co-occurrence frequency and the frequency of every word, we use formulas (1) and (2) to calculate the mutual information MI (S,T ) and t-score t(S,T ) of any source word and its single target word. As for the association verifying score [Fung 1995], the higher the t-score, the higher the degree of association between S and T:  MI (S , T ) = log Pr( S , T ) ,  
WordNet provides plenty of lexical meaning; therefore, it is very helpful in natural language processing research. Each lexical meaning in Princeton WordNet is presented in English. In this work, we attempt to use a bilingual dictionary as the backbone to automatically map English WordNet to a Chinese form. However, we encounter many barriers between the two different languages when we observe the preliminary result for the linkage between English WordNet and the bilingual dictionary. This mapping causes the Chinese translation of the English synonym  *  E-mail: ksj@cis.scu.edu.tw  collection (Synset) to correspond to unstructured Chinese compound words, phrases, and even long string sentence instead of independent Chinese lexical words. This phenomenon violates the aim of Chinese WordNet to take the lexical word as the basic component. Therefore, this research will perform further processing to study this phenomenon. The objectives of this paper are as follows: First, we will discover core lexical words and characteristic words from Chinese compound words. Next, those lexical words will be expressed by means of conceptual representations. For the core lexical words, we use grammar structure analysis to locate such words. For characteristic words, we use sememes in HowNet to represent their lexical meanings. Certainly, there exists a problem of ambiguity when Chinese lexical words are translated into their lexical meanings. To resolve this problem, we use lexical parts-of-speech and hypernyms of WordNet to reduce the lexical ambiguity. We experimented on nouns, and the experimental results show that sense disambiguation could achieve a 93.8% applicability rate and a 93.5% correct rate. 1.  GUM, CYC, ONTOS, MICROKOSMOS, EDR WordNet [Gomez, 1998]  (Lexical Relation)  (Conceptual Relation)  WordNet [Miller, 1990; Fellbaum,  1998]  [Farreres, Rigau and Rodriguez, 1998]  WordNet  [Gonzalo et al., 1998; Mandala, Tokunaga and Tanaka, 1998 ]  [Knight and Luk,  1994]  [Jing, 1998]  [Aslandogam et al., 1997]  WordNet  WordNet  EuroWordNet  [Atserias et al., 1997; Farreres,  Rigau, and Rodriguez, 1998]  WordNet  [Lee, Lee and Yun, 2000]  WordNet Princeton WordNet  WordNet  WordNet  WordNet  (Synset)  [  ]  WordNet  2.  [Gale, Church and Yarowsky, 1992; Yarowsky, 1992,  1995; Resnik, 1993; Dagan and Itai, 1994; Luk, 1995; Ng and Lee, 1996; Riloff and Jones,  1999]  [Guthrie et al., 1991; Slator, 1991; Li, Szpakowicz and Matwin,  1995; Chen and Chang, 1998, Yang and Ker, 2002]  [Chang, Ker and Chen, 1998; Chen and  Chang, 1998; Chen and Lin, 2000; Chen, Lin and Lin, 2000; Dorr et al., 2000; Carpuat et al.,  2002; Wang, 2002]  WordNet  [Chen and Lin, 2000]  WordNet HowNet  [Dorr et al., 2000; Carpuat  et al., 2002 ]  [Chang, Ker and Chen, 1998; Chen and Chang, 1998]  3.  3.1 WordNet  WordNet  
In this paper we report results of a supervised machine-learning approach to Chinese word segmentation. A maximum entropy tagger is trained on manually annotated data to automatically assign to Chinese characters, or hanzi, tags that indicate the position of a hanzi within a word. The tagged output is then converted into segmented text for evaluation. Preliminary results show that this approach is competitive against other supervised machine-learning segmenters reported in previous studies, achieving precision and recall rates of 95.01% and 94.94% respectively, trained on a 237K-word training set. Key Words: Chinese word segmentation, supervised machine-learning, maximum entropy, character tagging 1. Introduction It is generally agreed among researchers that word segmentation is a necessary first step in Chinese language processing. However, unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community [Fan and Tsai, 1988; Gan, 1995; Gan, Palmer, and Lua, 1996; Guo, 1997; Jin and Chen, 1998; Sproat and Shih, 1990; Sproat et al., 1996; Wu and Jiang, 1998; Wu, 2003]. It is easy to demonstrate that the lack of natural delimiters itself is not the heart of the problem. In a hypothetical language where all words are represented with a finite set of symbols, if one subset of the symbols always start a word and another subset, mutually exclusive from the previous subset, always end a word, identifying words would be a trivial * Institute for Research in Cognitive Science , Suite 400A, 3401 Walnut Street University of Pennsylvania, Philadelphia, PA 19104, USA E-mail: xueniwen@linc.cis.upenn.edu  30  Nianwen Xue  exercise. Nor can the problem be attributed to the lack of inflectional morphology. Although it is true in Indo-European languages inflectional affixes can generally be used to signal word boundaries, it is conceivable that a hypothetical language can use symbols other than inflectional morphemes to serve the same purpose. Therefore the issue is neither the lack of natural word delimiters nor the lack of inflectional morphemes in a language, rather it is whether the language has a way of unambiguously signaling the boundaries of a word.  The real difficulty in automatic Chinese word segmentation is the lack of such unambiguous word boundary indicators. In fact, most hanzi can occur in different positions within different words. The examples in Table 1 show how the Chinese character 产 (“produce”) can occur in four different positions. This state of affairs makes it impossible to simply list mutually exclusive subsets of hanzi that have distinct distributions, even though the number of hanzi in the Chinese writing system is in fact finite. As long as a hanzi can occur in different word-internal positions, it cannot be relied upon to determine word boundaries as they could be if their positions were more or less fixed.  Table 1. A hanzi can occur in multiple word-internal positions  Position Left Word by itself Middle Right  Example 产生 ’to come up with’ 产小麦 ’to grow wheat’ 生产线 ’assembly line’ 生产 ’to produce’  The fact that a hanzi can occur in multiple word-internal positions leads to ambiguities of various kinds, which are described in detail in [Gan, 1995]. For example, 文 can occur in both word-initial and word-final positions. It occurs in the word-final position in 日文 (“Japanese”) but in the word-initial position in 文章(“article”). In a sentence that has a string “日文章”, as in (1)1, an automatic segmenter would face the dilemma whether to insert a word boundary marker between 日 and 文, thus grouping 文章 as a word, or to mark 日文 as a word, to the exclusion of 章. The same scenario also applies to 章, since like 文, it can also occur in both word-initial and word-final positions.  1. (a) Segmentation I  日文 章魚 怎麼 說?  Japanese octopus how say “How to say octopus in Japanese?”  (b) Segmentation II  1Adapted from [Sproat et al.,1996]  Chinese Word Segmentation as Character Tagging  31  日 文章 魚 怎麼 說? Japan article fish how say Ambiguity also arises because some hanzi should be considered to be just word components in certain contexts and words by themselves in others. For example, 魚 can be considered to be just a word component in 章魚. It can also be a word by itself in other contexts. Presented with the string 章魚 in a Chinese sentence, a human or automatic segmenter would have to decide whether 魚 should be a word by itself or form another word with the previous hanzi. Given that 日, 文章, 章魚, 魚 are all possible words in Chinese, how does one decide that 日文 章魚 is the right segmentation for the sentence in (1) while 日 文章 魚 is not? Obviously it is not enough to know just what words are in the lexicon. In this specific case, a human segmenter can resort to world knowledge to resolve this ambiguity, knowing that 日 文章 魚 would not make any kind of real-world sense. In other cases a human segmenter can also rely on syntactic knowledge to properly segment a sentence. For instance, 枪 should be considered a word in (2a) and two words in (2b): 2. a 警察 枪-杀 了 那 个 逃犯 police gun-kill LE that CL escapee “Police killed the escapee with a gun.” b 警察 用 枪 杀 了 那 个 逃犯 Police with gun kill LE that CL escapee “Police killed the escapee with a gun” In (2b), 枪 is a word by itself and forms a phrasal constituent with the preceding 用. In order to get the segmentation right for the example in (2) one needs to know, for example, that 用 has to take a complement and in the case of (2b) the complement is 枪. Therefore it is impossible for 枪 to be part of the word 枪杀. The human segmenter has little difficulty resolving these ambiguities and coming up with the correct segmentation since they have linguistic and world knowledge at their disposal. However, the means available to the human segmenter cannot be made available to computers just as easily. As a result, an automatic word segmenter would have to bypass such limitations to resolve these ambiguities. In addition to the ambiguity problem, another problem that is often cited in the literature is the problem of so-called out-of-vocabulary or “unknown” words [Wu and Jiang, 1998]. The unknown word problem arises because machine-readable dictionaries cannot possibly list all  32  Nianwen Xue  the words encountered in NLP tasks exhaustively2. For one thing, although the number of hanzi generally remains constant, Chinese has several productive new word creation mechanisms. First of all, new words can be created through compounding, in which new words are formed through the combination of existing words, or through suoxie, in which components of existing words are extracted and combined to form new words. Second, new names are created by combining existing characters in a very unpredictable manner. Third, there are also transliterations of foreign names. These are just a few of the many ways new words can be introduced in Chinese. The key to accurate automatic word identification in Chinese lies in the successful resolution of these ambiguities and a proper way to handle out-of-vocabulary words. We have demonstrated that the ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions. Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. If the positions of all the hanzi in a sentence can be determined with the help of the context, the word segmentation problem would be solved. This is the line of thinking we are going to pursue in the present work. There are several reasons why we may expect this approach to work. First, Chinese words generally have fewer than four characters. As a result, the number of positions is small. Second, although each hanzi can in principle occur in all possible positions, not all hanzi behave this way. A substantial number of hanzi are distributed in a constrained manner. For example, 们, the plural marker, almost always occurs in the word-final position. Finally, although Chinese words cannot be exhaustively listed and new words are bound to occur in naturally occurring text, the same is not true for hanzi. The number of hanzi stays fairly constant and we do not generally expect to see new hanzi. In this paper, we model the Chinese word segmentation problem as a hanzi tagging problem and use a machine-learning algorithm to determine the word-internal positions of hanzi with the help of contextual information. The remainder of this paper is organized as follows. In Section 2, we briefly review the representative approaches in the previous studies on Chinese word segmentation. In Section 3, we describe how the word segmentation problem can be modeled as a tagging problem and how the maximum entropy model is used to solve this problem. We describe our experiments in Section 4. In Section 5, we report our experimental results, using the maximum matching algorithm as a baseline. We also evaluate these results against previous approaches and discuss the contributions of different feature sets and the effectiveness of different tag sets. We conclude this paper and discuss future work in Section 6.  2See [Guo, 1997] for a different point of view  Chinese Word Segmentation as Character Tagging  33  2. Previous Work Various methods have been proposed to address the word segmentation problem in previous studies. Noting that linguistic information, syntactic information in particular, can help identify words, [Gan, 1995] and [Wu and Jiang, 1998] treated word segmentation as inseparable from Chinese sentence understanding as a whole. As a result, the success of the word segmentation task is tied to the success of the sentence understanding task, which is just as difficult as the word segmentation problem, if not more difficult. Most of the word segmentation systems reported in previous studies are stand-alone systems and they fall into three main categories, depending on whether they use statistical information and electronic dictionaries. These are purely statistical approaches [Sproat and Shih, 1990; Sun, Shen, and Tsou, 1998; Ge, Pratt, and Smyth, 1999; Peng and Schuurmans, 2001], non-statistical dictionary-based approaches [Liang, 1993; Gu and Mao, 1994] and statistical and dictionary-based approaches [Sproat et al., 1996]. More recently work on Chinese word segmentation also includes supervised machine-learning approaches [Palmer, 1997; Hockenmaier and Brew, 1998; Xue, 2001]. Purely dictionary-based approaches generally addresses the ambiguity problem with some heuristics, and the most successful heuristics are variations of the maximum matching algorithm. A maximum matching algorithm is a greedy search routine that walks through a sentence trying to find the longest string of hanzi starting from a given point in the sentence that matches a word entry in a pre-compiled dictionary. For instance, assuming 关 (“close”), 心 (“heart”) and 关心 (“care about”) are all listed in the dictionary, given a string of hanzi 关-心, the maximum matching algorithm always favors 关心 as a word, over 关-心 as a string of two words. This is because 关心 is a longer string than 关 and both of them are in the dictionary. When the segmenter finds 关, it will continue to search and see if there is a possible extension. When it finds another word 关心 in the dictionary it will decide against inserting a word boundary between 关 and 心. When the algorithm can no longer extend the string of hanzi it stops searching and inserts a word boundary marker. The process is repeated from the next hanzi till it reaches the end of the sentence. The algorithm is successful because in a lot of cases, the longest string also happens to be correct segmentation. For example, for the example in (1), the algorithm will rightly decide that (1a) rather than (1b) is the correct segmentation for the sentence, assuming 日, 日文, 文章, 章鱼 and 鱼 are all listed in the dictionary. However, this algorithm will output the wrong segmentation for (2b), in which it will incorrectly group 枪杀 as a word. In addition, the maximum matching algorithm does not have a built-in mechanism to deal with out-of-vocabulary words. In general, the completeness of the dictionary to a large extent determines the degree of success for segmenters using this approach.  34  Nianwen Xue  As a representative of purely statistical approaches, [Sproat and Shih, 1990] relies on the mutual information of two adjacent characters to decide whether they form a two-character word. Given a string of characters c1...cn , the pair of adjacent characters with the largest mutual information greater than a pre-determined threshold is grouped as a word. This process is repeated until there are no more pairs of adjacent characters with a mutual information value greater than the threshold. This algorithm is extended by [Sun, Shen, and Tsou, 1998] so that association measures other than mutual information are also taken into consideration. More recently, [Ge, Pratt, and Smyth, 1999; Peng and Schuurmans, 2001] applied expectation maximization methods to Chinese word segmentation. For example, [Peng and Schuurmans, 2001] used an EM-based algorithm to estimate probabilities for words in a dictionary and use mutual information to weed out proposed words whose components are not strongly associated. Purely statistical approaches have the advantage of not needing a dictionary or training data, and since unsegmented data are easy to obtain, they can be easily trained on any data source. The drawback is that statistical approaches generally do not perform well in terms of the accuracy of the segmentation. Statistical dictionary-based approaches attempt to get the best of both worlds by combining the use of a dictionary and statistical information such as word frequency. [Sproat et al., 1996] represents a dictionary as a weighted finite-state transducer. Each dictionary entry is represented as a sequence of arcs labeled with a hanzi and its phonemic transcription, starting from an initial state 0 and terminated by a weighted arch labeled with an empty string ε and a part-of-speech tag. The weight represents the estimated cost of the word, which is its negative log probability. The probabilities of the dictionary words as well as morphologically derived words not in the dictionary are estimated from a large unlabeled corpus. Given a string of acceptable symbols (all the hanzi plus the empty string), there exists a function that takes this string of symbols as input and produces as output a transducer that maps all the symbols to themselves. The path that has the cheapest cost is selected as the best segmentation for this string of characters. Compared with purely statistical approaches, statistical dictionary-based approaches have the guidance of a dictionary and as a result they generally outperform purely statistical approaches in terms of segmentation accuracy. Recent work on Chinese word segmentation has also used the transformation-based error-driven algorithm [Brill, 1993] and achieved various degrees of success [Palmer, 1997; Hockenmaier and Brew, 1998; Xue, 2001]. The transformation-based error-driven algorithm is a supervised machine-learning routine first proposed by [Brill, 1993] and initially used in POS tagging as well as parsing. It has been applied to Chinese word segmentation by [Palmer, 1997; Hockenmaier and Brew, 1998; Xue, 2001]. Although the actual implementation of this algorithm may differ slightly, in general the transformation-based error-driven approaches try  Chinese Word Segmentation as Character Tagging  35  to learn a set of n -gram rules from a training corpus and apply them to segment new text. The input to the learning routine is a (manually or automatically) segmented corpus and its unsegmenteded (or undersegmented) counterpart. The learning algorithm compares the segmented corpus and the undersegmented dummy corpus at each iteration and finds the rule that achieves the maximum gain if applied. The rule with the maximum gain is the one that makes the dummy corpus most like the reference corpus. The maximum gain is calculated with an evaluation function which quantifies the gain and takes the largest value. The rules are instantiations of a set of pre-defined templates. After the rule with the maximum gain is found, it is applied to the dummy corpus, which will better resemble the reference corpus as a result. This process is repeated until the maximum gain drops below a pre-defined threshold, which indicates improvement achieved through further training will no longer be significant. The output of the training process would be a ranked set of rules instantiating the predefined set of templates. The rules will then be used to segment new text. Like statistical approaches, this approach provides a trainable method to learn the rules from a corpus and it is not labor-intensive. The drawback is that compared with statistical approaches, this algorithm is not very efficient. The present work represents another supervised machine-learning approach. Specifically, we applied the maximum entropy model, a statistical machine-learning algorithm to Chinese word segmentation. 3. A supervised machine-learning algorithm to Chinese word segmentation In this section, we first formalize the idea of tagging hanzi based on their word-internal positions and describe the tag set we used. We then briefly describe the maximum entropy model, which has been successfully applied to POS tagging as well as parsing [Ratnaparkhi, 1996; Ratnaparkhi, 1998]. 3.1 Reformulating word segmentation as a tagging problem Before we apply the machine-learning algorithm first we convert the manually segmented words in the corpus into a tagged sequence of Chinese characters. To do this, we tag each character with one of the four tags, LL, RR, MM and LR depending on its position within a word. It is tagged LL if it occurs on the left boundary of a word, and forms a word with the character(s) on its right. It is tagged RR if it occurs on the right boundary of a word, and forms a word with the character(s) on its left. It is tagged MM if it occurs in the middle of a word. It is tagged LR if it forms a word by itself. We call such tags position-of-character (POC) tags to differentiate them from the more familiar part-of-speech (POS) tags. For example, the manually segmented string in (3a) will be tagged as (3b):  36  Nianwen Xue  3. (a) 上海 计划 到 本 世纪 末 实现 人均 国内 生产 总值 五千 美元  (b) 上/LL 海/RR 计/LL 划/RR 到/LR 本/LR 世/LL 纪/RR 末/LR 实/LL 现/RR 人/LL 均/RR 国/LL 内/RR 生/LL 产/RR 总/LL 值/RR 五/LL 千/RR 美/LL 元/RR  (c) Shanghai plans to reach the goal of 5,000 dollars in per capita GDP by the end of the century.  Given a manually segmented corpus, a POC-tagged corpus can be derived trivially with perfect accuracy. The reason why we use such POC-tagged sequences of characters instead of applying n -gram rules to segmented corpus directly [Palmer, 1997; Hockenmaier and Brew, 1998; Xue, 2001] is that they are much easier to manipulate in the training process. In addition, the POC tags reflect our observation that the ambiguity problem is due to the fact that a hanzi can occur in different word-internal positions and it can be resolved in context. Naturally, while some characters have only one POC tag, most characters will receive multiple POC tags, in the same way that words can have multiple POS tags. Table 2 shows how all four of the POC tags can be assigned to the character 产 (“produce”):  Table2. A character can receive as many as four tags  Position Left Word by itself Middle Right  Tag  Example  LL 产生 ’to come up with’  LR 产 小麦 ’to grow wheat’  MM 生产线 ’assembly line’  RR 生产 ’to produce’  If there is ambiguity in segmenting a sentence or any string of hanzi, then there must be some hanzi in the sentence that can receive multiple tags. For example, each of the first four characters of the sentence in (1) would have two tags. The task of the word segmentation is to choose the correct tag for each of the hanzi in the sentence. The eight possible tag sequences for (1) are shown in (4a), and the correct tag sequence is (4b).  4. (a) 日/LL | LR 文/RR | LL 章/LL | RR 鱼/RR | LR 怎/LL 么/RR 说/LR ?  (b) 日/LL 文/RR 章/LL 鱼/RR 怎/LL 么/RR 说/LR ?  Also like POS tags, how a character is POC-tagged in naturally occurring text is affected by the context in which it occurs. For example, if the preceding character is tagged LR or RR, then the next character can only be tagged LL or LR. How a character is tagged is also affected by the surrounding characters. For example, 关 (``close'') should be tagged RR if the previous character is 开 (``open'') and neither of them forms a word with other characters, while it should be tagged LL if the next character is 心 (``heart'') and neither of them forms a word with other characters. This state of affairs closely mimics the familiar POS tagging  Chinese Word Segmentation as Character Tagging  37  problem and lends itself naturally to a solution similar to that of POS tagging. The task is one of ambiguity resolution in which the correct POC tag is determined among several possible POC tags in a specific context. Our next step is to train a maximum entropy model on the perfectly POC-tagged data derived from a manually segmented corpus to automatically POC-tag unseen text.  3.2 The maximum entropy tagger  The maximum entropy model used in POS-tagging is described in detail in [Ratnaparkhi,  1996] and the POC tagger here uses the same probability model. The probability model is defined over H ×T , where H is the set of possible contexts or "histories" and T is the set of possible tags. The model’s joint probability of a history h and a tag t is defined as  ∏ p(h,t) = πμ  k  αf  ( j  h  ,t  )  j  (1)  j =1  where π is a normalization constant, {μ, α1,..., αk } are the model parameters and { f1,..., fk } are known as features, where f j (h, t) ∈{0,1} . Each feature f j has a corresponding parameter α j , hat effectively serves as a "weight" of this feature. In the training process, given a sequence of characters {c1,..., cn} and their POC tags {t1,..., tn} as training data, the purpose is to determine the parameters {μ, α1,..., αk } that maximize the likelihood of the training data using p :  ∏ ∏ ∏ L(P) =  n P(hi ,ti ) =  πμ α n  k  f  ( j  hi  ,ti  )  j  (2)  i =1  i =1  j =1  The success of the model in tagging depends to a large extent on the selection of suitable features. Given (h, t) , a feature must encode information that helps to predict t . The features we used in this experiment are instantiations of the feature templates in (5). Feature templates (b) to (e) represent character features while (f) represents tag features. The character and tag features are also represented graphically in Figure 1, where C−3...C3 are characters and T−3...T3 are POC tags. Each arrow or arc represents one feature template. Feature template (a) represents the default feature.  5 Feature templates  (a) Default feature  (b) The current character ( C0 ) (c) The previous (next) two characters ( C−2 , C−1, C1, C2 ) (d) The previous (next) character and the current character (C-1 C0, C0 C1),  38  Nianwen Xue  the previous two characters (C-2 C-1), and the next two characters (C1 C2) (e) The previous and the next character (C-1 C1) (f) The tag of the previous character (T-1), and the tag of the character two before the current character (T-2)  … C-3  C-2  C-1  C0  C1  C2  C3 …  … T-3  T-2  T-1  T0  T1  T2  T3 …  Figure 1 Features used in the maximum entropy segmenter  In general, given (h, t) , these features are in the form of co-occurrence relations between t and some type of context h , or between t and some properties of the current character. For  example,  f (h ,t ) ={ i i i  
The present study attempts to measure and compare the morphological productivity of five Mandarin Chinese suffixes: the verbal suffix -hua, the plural suffix -men, and the nominal suffixes -r, -zi, and -tou. These suffixes are predicted to differ in their degree of productivity: -hua and -men appear to be productive, being able to systematically form a word with a variety of base words, whereas -zi and -tou (and perhaps also -r) may be limited in productivity. Baayen [1989, 1992] proposes the use of corpus data in measuring productivity in word formation. Based on word-token frequencies in a large corpus of texts, his token-based measure of productivity expresses productivity as the probability that a new word form of an affix will be encountered in a corpus. We first use the token-based measure to examine the productivity of the Mandarin suffixes. The present study, then, proposes a type-based measure of productivity that employs the deleted estimation method [Jelinek & Mercer, 1985] in defining unseen words of a corpus and expresses productivity by the ratio of unseen word types to all word types. The proposed type-based measure yields the productivity ranking “-men, -hua, -r, -zi, -tou,” where -men is the most productive and -tou is the least productive. The effects of corpus-data variability on a productivity measure are also examined. The proposed measure is found to obtain a consistent productivity ranking despite variability in corpus data. Keywords: Mandarin Chinese word formation, Mandarin Chinese suffixes, morphological productivity, corpus-based productivity measure. 1. Introduction 1.1 Morphological Productivity The focus of a study of morphological productivity is on derivational affixation that involves a base word and an affix [Aronoff, 1976], as seen in sharp + -ness → sharpness, electric + -ity * Ph.D. Program in Linguistics, The Graduate Center, The City University of New York, 365 Fifth Avenue, New York, NY 10016, U.S.A. e-mail: enishimoto@gc.cuny.edu  50  Eiji Nishimoto  → electricity, child + -ish → childish.1 Native speakers of a language have intuitions about what are and are not acceptable words of their language, and if presented with non-existent, potential words [Aronoff, 1983], they accept certain word formations more readily than others [Anshen & Aronoff, 1981; Aronoff & Schvaneveldt, 1978; Cutler, 1980]. Most intriguing in the issue of productivity is that the degree of productivity varies among affixes, and many studies in the literature have been devoted to accounting for this particular aspect of productivity [see Bauer, 2001, and Plag, 1999, for an overview]. How the degree of productivity varies among affixes is best illustrated by the English nominal suffixes -ness and -ity, which are often considered “rivals” as they sometimes share a base word (e.g., clear → clearness or clarity). In general, -ness is felt to be more productive than -ity.2 The word formation of -ity is limited, for example, by the Latinate Restriction [Aronoff, 1976: 51] that requires the base word to be of Latinate origin; hence, purity is acceptable but *cleanity is not. In contrast, -ness freely attaches to a variety of base words of both Latinate and Germanic (native) origin; thus, both pureness and cleanness are acceptable. There are also some affixes that could be regarded as unproductive; for example, Aronoff and Anshen [1998: 243] note that the English nominal suffix -th (as in long → length) has long been unsuccessful in forming a new word that survives, despite attempts at terms like coolth. Varying degrees of productivity are also observed in Mandarin Chinese word formation. As will be discussed shortly, some Mandarin suffixes appear to be more productive than others. 1.2 Measuring the Degree of Productivity Early studies on productivity mainly focused on restrictions on word formation and viewed the degree of productivity to be determined by such restrictions [Booij, 1977; Schultink, 1961; van Marle, 1985]. Booij [1977: 120], for example, considers the degree of productivity of a word formation rule to be inversely proportional to the amount of restrictions that the word formation rule is subject to. Although the view that productivity is affected by restrictions on word formation is certainly to the point, from a quantitative point of view, measuring productivity by the amount of restrictions on word formation is limited in that the restrictive weight of such restrictions is unknown [Baayen & Renouf, 1996: 87]. Baayen [1989, 1992] proposes a corpus-based approach to the quantitative study of productivity. His productivity measure uses word frequencies in a large corpus of texts to 
It is shown that for a large corpus, Zipf 's law for both words in English and characters in Chinese does not hold for all ranks. The frequency falls below the frequency predicted by Zipf's law for English words for rank greater than about 5,000 and for Chinese characters for rank greater than about 1,000. However, when single words or characters are combined together with n-gram words or characters in one list and put in order of frequency, the frequency of tokens in the combined list follows Zipf’s law approximately with the slope close to -1 on a loglog plot for all n-grams, down to the lowest frequencies in both languages. This behaviour is also found for English 2-byte and 3-byte word fragments. It only happens when all n-grams are used, including semantically incomplete n-grams. Previous theories do not predict this behaviour, possibly because conditional probabilities of tokens have not been properly represented. Keywords: Zipf 's law, Chinese character, Chinese compound word, n-grams, phrases. 1. Introduction  1.1 Zipf's law  The law discovered empirically by [Zipf 1949] for word tokens in a corpus states that if f is  the frequency of a word in the corpus and r is the rank, then:  f =k  (1)  r  where k is a constant for the corpus. When log(f) is drawn against log(r) in a graph (which is  * Computer Science School, Queen's University Belfast, Belfast BT7 1NN, Northern Ireland, UK. Email: {q.le, e.sicilia, j.ming, fj.smith}@qub.ac.uk  78  Le Quan Ha et al.  called a Zipf curve), a straight line is obtained with a slope of –1. An example with a small corpus of 250,000 tokens made up of paragraphs chosen at random from the Brown corpus of American English [Francis and Kucera 1964] is given in Figure 1; in this the tokens do not include punctuation marks and numbers. Typographical errors, if any, will appear in the hapax legomenon. 100000  10000  log frequency  1000  100 10  -0.2  
We present a new approach to aligning bilingual English and Chinese text at sub-sentential level by interleaving alphabetic texts and punctuations matches. With sub-sentential alignment, we expect to improve the effectiveness of alignment at word, chunk and phrase levels and provide finer grained and more reusable translation memory. 1. Introduction Recently, there are renewed interests in using bilingual corpus for building systems for statistical machine translation (Brown et al. 1988, 1991), including data-driven machine translation (Richardson et al. 2002), computer-assisted revision of translation (Jutras 2000) and cross-language information retrieval (Kwok 2001). It is therefore useful for the bilingual corpus to be aligned at the sentence level and even subsentence level with very high precision (Moore 2002; Chuang, You and Chang 2002, Kueng and Su 2002). Especially, for further analyses such as phrase alignment, word alignment (Ker and Chang 1997; Melamed 2000) and translation memory, high-precision alignment at sub-sentential levels would be very useful. Alignment at sub-sentential level has the potential of improving the effectiveness of alignment at word and phrase levels and providing finer grained and more reusable translation memory. Much work has been reported in the literature of computational linguistics on how to align sentences, while very little is touched on alignment just below the sentence level. The most effective approach for sentence alignment is the length-based approach proposed by Brown et al. (1991) and by Gale and Church (1991). Both methods use normal distribution to model the ratio of lengths between the counterpart sentences measured in number of words or characters. Length-based approach for aligning parallel corpora has commonly been used and produces surprisingly good results for the language pair of French and English at success rates well over 96%. However, it does not perform as well for alignment of text in two distant languages such as Chinese and English.  Yeh (2003) proposed a punctuation-based approach for sentence alignment which produces even high accuracy rates than the length based approach. It was pointed out that the ways different languages use punctuations are more or less similar and the correspondence of punctuations across different languages can be obtained using a small set of training data. By soft matching punctuations of the two languages in ordered comparison, the probabilities of mutual translation for a pair of bilingual sentences can be estimated more effectively than lengths. This is not surprising since the average sentence contains many punctuations which carry more information than lengths. Yeh also examined the results of punctuationbased sentence alignment and observed: “Although word alignment links do cross one and other a lot, they general seem not to cross the links between punctuations. It appears that we can obtain sub-sentential alignment at clause and phrase levels from the alignment of punctuation.” This observation indicates that in bilingual corpus pieces of text delimited by punctuations behave much the same way as sentences with non-crossing alignment links. Therefore, it is reasonable to align pieces of text ending with a couple of punctuations, much the same way as sentence alignment. Building on their work, we develop a new approach to sub-sentential alignment by interleaving the matches of alphabetic texts and punctuations. In the following, we first give an example for bilingual sub-sentential alignment in Section 2. Then we introduce our probability model in Section 3. Next, we describe experimental setup and results in Section 4. We conclude in Section 5 with discussion and future work. 2. Example Consider a pair of counterpart paragraphs in the official records of Hong Kong Legislative Council: “My goal is simply this - to safeguard Hong Kong's way of life. This way of life not only produces impressive material and cultural benefits; it also incorporates values that we all cherish. Our prosperity and stability underpin our way of life. But, equally, Hong Kong's way of life is the foundation on which we must build our future stability and prosperity.” 我的目標很簡單，就是要保障香港的生活方式。這個生活方式，不單在物質和文化方面為我們帶來了 重大的利益，而且更融合了大家都珍惜的價值觀。香港的安定繁榮是我們生活方式的支柱。同樣地， 我們未來的安定繁榮，亦必須以香港的生活方式為基礎。(Source: Oct. 7, 1992, Governor Christopher Francis Patten’s address to the HK LEGCO) By sub-sentential alignment, we mean identifying the shortest possible pair of counterpart texts ending with punctuations. From the example above, the following is the intended results of sub-sentential alignment: 2  z My goal is simply this – 我的目標很簡單， z to safeguard Hong Kong's way of life. 就是要保障香港的生活方式。 z This way of life not only produces impressive material and cultural benefits; 這個生活方式，不單在物質和文化方面為我們帶來了重大的利益， z it also incorporates values that we all cherish. 而且更融合了大家都珍惜的價值觀。 Notice that longer pairs such as the following translation equivalent pair of sentences My goal is simply this – to safeguard Hong Kong's way of life. 我的目標很簡單，就是要保障香港的生活方式。 does not fit the bill, since a finer grained subdivision into two 1-1 matches, (My goal is simply this –, 我的 目標很簡單，) and (to safeguard Hong Kong's way of life., 就是要保障香港的生活方式。) also preserve translation equivalence. Not unlike the situation in sentence alignment, there are many to one, one to many, and many to many matches. For instance, it is not possible to find a 1-1 match for “This way of life not only produces impressive material and cultural benefits;” since it only corresponds to “這個生活 方式，” in part. Therefore, we have to combine the subsequent clause “不單在物質和文化方面為我們 帶來了重大的利益，” for a 1-2 match. 3. Probability Model In this section we describe our probability model. To do so, we will first introduce some necessary notation. Let E be an English fragment e1, e2,…,em and C be a Chinese paragraph c1, c2,…,cn, which ei and cj is a text-fragment as described in Section 2. We define a link l(ei, cj) for ei and cj that are translation ( or part of a translation ) of one another. We define null link l(ei, c0) for ei which does not correspond to a translation. The null link l(e0, cj) is defined similarly. An alignment A for two paragraphs E and C is a set of links such that every text-fragment in E and C participates in at least one link, and a text-block linked to e0 or c0 participates in no other links. We define the alignment problem as finding the alignment A that maximizes P(A|E, C). An alignment A consists of t links {l1, l2,…, lt}, where each lk = (eik, cjk) for some ik and jk.We will refer to consecutive subsets of A as lij = {li ,li+1,...,l j } , Given this notation, P(A|E, C) can be decomposed as follows: 3  t ∏ P( A | E, F ) = P(l1t | E, F ) = P(lk | E,C,l1k−1) k =1  For each condition probability, given any pair ei and cj, the link probabilities can be determined directly from combining the probability of length-based model with punctuation-based model. From the paper of Gale and Church in 1993 for length-based model, we know the match probability is Prob( δ | match ) Prob(match) and Prob( δ | match ) can be estimated by  Prob( δ | match ) = 2 ( 1 – Prob( |δ| ) )  Where Prob( |δ| ) is the probability that random variable, z, with a standardized ( mean zero, variance one) normal distribution, has magnitude at least as large as |δ|. That is,  Where  ∫ Prob(δ ) =  
We present a new approach to aligning bilingual English and Chinese text at sub-sentential level by interleaving alphabetic texts and punctuations matches. With sub-sentential alignment, we expect to improve the effectiveness of alignment at word, chunk and phrase levels and provide finer grained and more reusable translation memory. 1. Introduction Recently, there are renewed interests in using bilingual corpus for building systems for statistical machine translation (Brown et al. 1988, 1991), including data-driven machine translation (2002), computerassisted revision of translation (Jutras 2000) and cross-language information retrieval (Kwok 2001). It is therefore useful for the bilingual corpus to be aligned at the sentence level and even sub-sentence level with very high precision (Moore 2002; Chuang, You and Chang 2002, Kueng and Su 2002). Especially, for further analyses such as phrase alignment, word alignment (Ker and Chang 1997; Melamed 2000) and translation memory, high precision and quality alignment at sentence or sub-sentential levels would be very useful. Furthermore, alignment at sub-sentential level has the potential of improving the effectiveness of alignment at word, chunk and phrase levels and providing finer grained and more reusable translation memory. Much work has been reported in the literature of computational linguistics studying how to align sentences. One of the most effective approaches is length-based approach proposed by Brown et al. and by Gale and Church. Length-based approach for aligning parallel corpora has commonly been used and produces surprisingly good results for the language pair of French and English at success rates well over 96%. However, it does not perform as well for alignment of two distant languages such as ChineseEnglish. Furthermore, for sub-sentential alignment, length-based approach gets less effectiveness than running it in sentence level since sub-sentence has less information in length.  Punctuations based approach (Yeh, Chuang and Chang 2003 ) for sentence alignment produces high accuracy rates as same as length based approach and was independent of languages. Although the ways different languages around the world use punctuations vary, symbols such as commas and full stops are used in most languages to demarcate writing, while question and exclamation marks are used to show emphasis. However, for sub-sentential alignment, punctuation-based approach has the same problem as length-based approach — no enough information in sub-sentence since sub-sentence might be very short and just include one or two punctuations within it. Yeh, Chuang and Chang (2003) examined the results of punctuation-based sentence alignment and observed: “Although word alignment links do cross one and other a lot, they general seem not to cross the links between punctuations. It appears that we can obtain sub-sentential alignment at clause and phrase levels from the alignment of punctuation.” Building on their work, we develop a new approach to sub-sentential alignment by interleaving the alignment of text and punctuations. In the following, we first give an example for bilingual sub-sentential alignment in Section 2. Then we introduce our probability model in Section 3. Next, we describe experimental setup and results in Section 4. We conclude in Section 5 with discussion and future work. 2. Example Consider a pair of aligned sentences in a parallel corpus as below: “My goal is simply this - to safeguard Hong Kong's way of life. This way of life not only produces impressive material and cultural benefits; it also incorporates values that we all cherish. Our prosperity and stability underpin our way of life. But, equally, Hong Kong's way of life is the foundation on which we must build our future stability and prosperity.” 我的目標很簡單，就是要保障香港的生活方式。這個生活方式，不單在物質和文化方面為我們帶來了 重大的利益，而且更融合了大家都珍惜的價值觀。香港的安定繁榮是我們生活方式的支柱。同樣地， 我們未來的安定繁榮，亦必須以香港的生活方式為基礎。 We can observe that although word alignment links might cross one and other a lot, there exist some text-blocks as follow that general seem not to cross the links between punctuations: “My goal is simply this –“ “我的目標很簡單，” “to safeguard Hong Kong's way of life.” “就是要保障香港的生活方式。” “This way of life not only produces impressive material and cultural benefits;” “這個生活方式，不單在物質和文化方面為我們帶來了重大的利益，” 2  “it also incorporates values that we all cherish.” “而且更融合了大家都珍惜的價值觀。” … That’s what we call sub-sentences here. From the examples above, we can define that a sub-sentence is a text-block that include at least one or more punctuations. That’s an unclear definition since a sentence and a paragraph also fit the definition too. However, what we want is to find out the shortest parallel textblock pairs that fit the definition. That’s why in the third pair of above examples, “這個生活方式，” is a Chinese text-block but we have to combine it with ”不單在物質和文化方面為我們帶來了重大的利 益，”, because we can’t find any English text-block correspond to “這個生活方式，”, we have to combine the two Chinese above first, than we can find the corresponding one : “This way of life not only produces impressive material and cultural benefits;”. 3. Probability Model In this section we describe our probability model. To do so, we will first introduce some necessary notation. Let E be an English paragraph e1, e2,…,em and C be a Chinese paragraph c1, c2,…,cn, which ei and cj is a text-blocks as described in Section 2. We define a link l(ei, cj) to exist if ei and cj are translation ( or part of a translation ) of one another. We define null link l(ei, c0) to exist if ei does not correspond to a translation of any cj. The null link l(e0, cj) is defined similarly. An alignment A for two paragraphs E and C is a set of links such that every text-block in E and C participates in at least one link, and a text-block linked to e0 or c0 participates in no other links. We define the alignment problem as finding the alignment A that maximizes P(A|E, C). An alignment A consists of t links {l1, l2,…, lt}, where each lk = l(eik, cjk) for some ik and jk.We will refer to consecutive subsets of A as lij = {li ,li+1,...,l j } , Given this notation, P(A|E, C) can be decomposed as follows: t ∏ P( A | E, F ) = P(l1t | E, F ) = P(lk | E,C,l1k−1) k =1 For each condition probability, given any pair ei and cj, the link probabilities can be determined directly from combining the probability of length-based model with punctuation-based model. From the paper of Gale and Church in 1993 for length-based model, we know the match probability is Prob( δ | match ) Prob(match) and Prob( δ | match ) can be estimated by Prob( δ | match ) = 2 ( 1 – Prob( |δ| ) ) 3  Where Prob( |δ| ) is the probability that random variable, z, with a standardized ( mean zero, variance one) normal distribution, has magnitude at least as large as |δ|. That is,  Where  ∫ Prob(δ ) =  
This paper describes a Web-based English-Chinese concordance system, TotalRecall, being developed in National Digital Learning Project – CANDLE, to promote translation reuse and encourage authentic and idiomatic use in second language learning. We exploited and structured existing high-quality translations from the bilingual Sinorama Magazine to build the concordance of authentic text and translation. Novel approaches were taken to provide high-precision bilingual alignment on the sentence, phrase and word levels. A browser-based user interface also developed for ease of access over the Internet. Users can search for word, phrase or expression in English or Chinese. The Web-based user interface facilitates the recording of the user actions to provide data for further research. 
Hsinchu, 300, Taiwan, ROC jschang@cs.nthu.edu.tw  Tracy Lin Department of Communication Engineering National Chiao Tung University 1001, Ta Hsueh Road, Hsinchu, 300, Taiwan, ROC tracylin@faculity.nctu.edu.tw  This prototype system demonstrates a novel method of word segmentation based on corpus statistics. Since the central technique we used is unsupervised training based on a large corpus, we refer to this approach as unsupervised word segmentation. The unsupervised approach is general in scope and can be applied to both Mandarin Chinese and Taiwanese. In this prototype, we illustrate its use in word segmentation of Taiwanese Bible written in Hanzi and Romanized characters. Basically, it involves: z Computing mutual information, MI, between Hanzi and Romanized characters A and B. If A and B have a relatively high MI, we lean toward treating AB as a word. z Using a greedy method to form words of 2 to 4 characters in the input sentences. z Building an N-gram model from the results of first-round word segmentation z Segmenting words based on the N-gram model z Iterating between the above two steps: building N-gram and word segmentation  Computing mutual information. Using mutual information is motivated by the observation of previous work by Hank and Church (1990) and Sproat and Shih (1990). If A and B have a relatively high MI that is over a certain threshold, we prefer to identify AB as a word over those having lower MI values. In the experiment with Taiwanese Bible, the system identified Hanzi and Romanized syllables. Out of those, we obtained pairs of consecutive single or double Hanzi characters and Romanized syllables. So those pairs are commonly known as character bigrams, trigrams, and fourgrams. We differed from the common N-gram calculation and treated those as pairs of character sequence in order to apply mutual information statistics. Table 1 shows some examples of the pairs and MI values. We have excluded pairs having MI 2.2 or lower.  Table 1. Example of consecutive pairs  (C1, C2) and MI values for the text of Taiwanese Bible  C1  C2  Mutual Information  婦  仁人  568.6012  婦  仁  281.1248  果  子  34.9152  婦仁 人  34.6398  園內 樹  23.6275  仁  人  16.4376  內  樹裡  10.7914  園  內  10.6569  通  食  8.9151  樹  裡  4.2192  仁人 對  3.2395  阮  通食  2.8967  Word Segmentation. With the potential words and MI values indicating their likelihood, we proceeded to segment the text of a large corpus into words. For the Taiwanese Bible, we had to take care of the problem of text being written down in more than one writing system: we had mixed Hanzi and Romanized syllables as input. Using a greedy method, we gradually formed words of 2 to 4 characters (or Romanized syllables) in the input sentences. A word with high-MI constituent characters took precedence in forming words.  Table 2. Example of consecutive pairs and MI values for the text of Taiwanese Bible  Left Syllable String, C1  Right Syllable String, C2 2-Syllable pairs  Mutual Information MI(C1, C2)  婦  仁  281.1248  果  子  34.9152  仁  人  16.4376  園  內  10.6569  通  食  8.9151  樹  裡  4.2192  仁人  對  3.2395  3-Syllable pairs  婦  仁人  568.6012  婦仁  人  34.6398  園內  樹  23.6275  內  樹裡  10.7914  阮  通食  2.8967  When successive words were formed, they could not contradict with the words determined previously. For instance, given the input “婦仁人對蛇講：「園內樹裡的果子阮通食,” we looked  up the table storing MI statistics and obtained the information shown in Table2. First, we formed words of two characters. Based on the information in Table 2, the system formed the words, 婦 仁, 果子, 通食, 園內, 樹裡. Notice that 仁人 is not selected because of confliction with previous decision about the word 婦仁. Subsequently, we tried to extend the two-syllable words chosen. A word is extended to three or four syllables if the MI is increased and in the corpus over τ % of instances the two-character words can be extended that way. Currently, we set τ = 60. Admittedly, there is limitation to what distributional regularity based on MI can be exploited for word segmentation and there were still many errors in the first-round word segmentation results. For instance, for the input, “我祈禱耶和華講：『主耶和華啊 … ,” the system produced the segmentation of “我 / 祈禱 / 耶和華 / 講 / ： / 『 / 主耶 / 和華 / 啊 /.” The first instance of 耶和華 was segmented correctly, while the second instance of 耶和華 was over-segmented because of the frequent character 主 before it. That problem can be partially alleviated by an Expectation Maximization Algorithm of learning an N-gram model of word segmentation.  Building an N-gram model. Currently, we used the unigram model where the probability of each word was estimated based on the Good-Turing smoothing technique. First we tally the total number of words N and count R of each word W. Let N r be the number of distinct words have count r. Also, let N 0 be the number of distinct syllable strings that never appear as a word. Godd-Turing smoothing stipulates that we calculate r’ as an adjustment for r as follows:  r0 = N1 / N 0  ri = (i+1) Ni+1 / N i  After the adjustment step, we obtained the probability for the unigram model as follows:  P( W ) = r’ / N where r’ is the smoothed count of W  For instance, we had the counts after the first-round MI-based segmentation as showed in Table 3.  Table 3. Good-Turing estimates for unigrams: Adjusted frequencies and probabilities  r  Nr  r*  PGT(.)  0 972,444  
deduction. Studying lexical semantics is no exception. Linguists also probe lexicons from bottom-up or top-down perspectives. 1.1 Bottom-up approach: from lexical items to semantic fields By following this approach, linguists may study from either a single lexical item (e.g. Fillmore and Atkins 1992), a pair or a set of near synonyms (e.g. Tsai et al 1996, 1998, Chief et al 2000, Liu et al 2000, Wu and Liu 2001, Liu 2000, 2002a, 2002b, 2003, etc.), or a class of lexical items (Chang et al 2000b, Lien 2001 & 2002, etc.) in order to capture the generalization of semantic components, constraints and rules for a semantic field, thereby constructing their theories. Generalizations may be derived from an observation of syntactic behavior and collocations of the items. The linguistic data may be collected from linguists’ own intuition, informants’ judgment, dictionaries, or from electronic thesauri e.g. WordNet (http://www.cogsci.princeton.edu/~wn/index.shtml/), and corpora such as British National Corpus (BNC) at http://www.hcu.ox.ac.uk/BNC/, and Sinica Corpus at http://www.sinica.edu.tw/ftms-bin/kiwi.sh/. 1.2 Top-down approach: from upper classes to lexical items Using this approach, linguists start from an upper class, probe their way through the subclasses, and then to specific lexical entries. In general, the aim of this method is to facilitate language processing by constructing a taxonomy or ontology of the human lexicon. Semantic hierarchy and inheritance relations are the two main research targets. HowNet (http://www.keenage.com/html/c_index.html/) and Suggested Upper Merged Ontology * This study is supported in part by NSC 91-2411-H-009-012-ME. I am indebted to my colleagues in the project and two anonymous reviewers for their insightful comments.  (SUMO at http://ontology.teknowledge.com/) are two of the online representatives. They contain a nearly complete hierarchy for Chinese and English words respectively. VerbNet (http://www.cis.upenn.edu/verbnet/) based on Levin (1993) and FrameNet I (http://www.icsi.berkeley.edu/~framenet/) are two of the other less exhausted cases. In Levin (1993), there are forty-eight verb classes grouped by a variety of syntactic alternations, but these classes are not structured by other upper classes. Though the concept of domains is obliterated in FrameNet II, FrameNet I contains fourteen domains with subordinate frames and lemmas, but these domains are not subsumed to other superior classes. The problem of the bottom-up approach is that the semantic properties of each lexical item may be extracted and the overt syntactic behavior may be accounted for, but the inheritance relationship, with its parent and ancestor classes, remains opaque. In contrast, the problem surrounding the top-down approach is that the inheritance relationship among the different levels may be clear enough to account for the covert syntactic behavior, but the detailed semantic attributes may be missed. To compensate for this drawback, SUMO combines its ontology with WordNet synsets. (Pease et al 2002), and researchers are now pursuing a multi-lingual semantic network (Huang et al 2002). A prototype of the Chinese-English bilingual interface of general and domain-specific ontologies, constructed by the Chinese Knowledge Information Processing Group (CKIP), is now also available at http://godel.iis.sinica.edu.tw/CKIP/ontology/. This study aims to provide a bidirectional approach, incorporating the above two methods in order to explore a detailed analysis of the finer semantic distinctions of conversation verbs. 2 Conversation verbs To extract Chinese conversation verbs, several resources were consulted. Firstly, Conversation is one of the fourteen frames of the Communication domain in FrameNet I, and there are both Chinese and English words, as well as definitions, in HowNet. By retrieving the corresponding Chinese words and definitions of the English lemmas subsumed to the Conversation frame in FrameNet, a set of possible Chinese candidates is obtained. Secondly, the resultant set of candidates was checked with the lexical items in CKIP’s Chinese-English bilingual ontologies. Any items that are used only in mainland China were temporarily ruled out. Thirdly, dictionaries, thesauri, and the intuition of native speakers were consulted. Finally, entries and their frequency in Sinica Corpus were taken into consideration. In this way, a set of target Chinese conversation verbs was obtained, e.g. 交談 jiao1 tan2, 談話 tan2 hua4, 會 談 hui4 tan2 ‘talk’, 閒聊 xian2 liao2 ‘gab’ and 聊天 liao2 tian1 ‘chat’, 交流 jiao1 liu2, and 溝通 gou1 tong1 ‘communicate’, 商量 shang1 liang2, 討論 tao3 lun4, and 商討 shang1 tao3 ‘discuss’, 吵架 chao3 jia4 ‘quarrel’ and 爭辯 zheng1 bian4 ‘debate’, etc. After setting the target items, their syntactic behavior and collocations were probed. In  addition, their upper class, the domain of communication was also investigated. In what follows, we will first illustrate how the near synonyms were analyzed from a bottom-up approach and then elaborate on a top-down method.  3. Analysis of near synonyms In this section, we will use three verbs of ‘talk/converse’ as an example to illustrate the bottom-up approach: jiao1 tan2, tan2 hua4 and hui4 tan2, literally meaning ‘talk to each other’, ‘talk words’ and ‘meet and talk’ respectively.  3.1 Grammatical function distribution As shown in table 1 below, sixty percent of the jiao1 tan2 tokens function as a predicate, the main verb of a clause. In contrast, the majority of the tokens of tan2 hua4 and hui4 tan2 are used as a head noun. The three lexical items have approximately the same functional percentage as a modifier.  Lemma 交談 jiao1 tan2 談話 tan2 hua4 會談 hui4 tan2 Function  Predicate  71 (60%)  57 (31%)  50 (33%)  Head Noun  30 (25%)  101 (54%)  82 (54%)  Modifier  17 (15%)  28 (15%)  20 (13%)  Total  118 (100%)  186 (100%)  152 (100%)  Table 1: Grammatical function distribution of jao1 tan2, tan2 hua4, and hui4 tan2  3.2 Collocation All three verbs can be modified by a duration, e.g. Ta1 men jiao1 tan2/hui4 tan2 le shi2 fen1 zhong1 and Ta1 men tan2 le shi2 fen1 zhong1 de hua4 ‘They have talked for ten minutes’. The three verbs can all collocate with the progressive (正 zheng4) 在 zai4 and the experiential 過 guo4, e.g. Ta1 men zheng4 zai4 jiao1 tan2/tan2 hua4/ hui4 tan2 ‘They are talking to each other,’ and Ta1 men jiao1 tan2 guo4 /tan2 guo4 hua4/hui4 tan2 guo4 ‘They have talked to each other.’ In addition, they can all be followed by the inchoative particle 了 le, e.g. Ta1 men (kai1 shi3) jiao1 tan2/tan2 hua4/hui4 tan2 le! ‘They start to talk!’ From the above facts, and by following the methodology used by Chang et al (2000a), we can induce the generalization that these verbs are bounded process verbs. However, these verbs contrast with ‘discuss’ verbs such as 商量 shang1 liang2 and 討論 tao3 lun4 in that they do not take a Topic directly, e.g. ‘*jiao1 tan2/*tan2 hua4/*hui4 tan2/shang1 liang2/tao3 lun4 shi4 qing2 ‘*converse/discuss about something’. Furthermore, they do not take a Message in the same manner as other saying verbs, e.g. ‘Ta1 men *jiao1 tan2/*tan2 hua4/*hui4 tan2/shuo1 ta1 men mei2 you3 qian2 ‘They *conversed/*talked/said they had no money.’  In addition, the subject agent, the Speaker, of the three verbs must be plural, e.g. Ta1  gen1 wo3 /wo3 men/ *wo jiao1 tan2 le ban4 xiao3 shi2/tan2 le ban4 xiao3 shi2 de hua4/hui4 tan2 le ban4 xiao3 shi2 ‘He and I /we/*I have talked for half an hour.’ This symbolizes the reciprocality of a conversation event, in which both the speaker and the listener do the speaking and listening. However, hui4 tan2 differs from the other two in that its speakers are mostly officials. When hui4 tan2 functions as a predicate, only 18% (9/50) of the Speakers are common people. Most Speakers (82%) are government officials, representatives of countries or parties, or school officials. In addition, among the nine instances of non-officials there are two doctor-and-patient pairs, and two businessmen pairs.  When the Speakers are realized as Interlocutor_1 and Interlocutor_2, being an argument in a matrix clause or in a subordinate clause as a pre-nominal modifier, they may be linked with or without an overt connective such as 與 yu3, 和 he2/han4, and 跟 gen1, e.g. 戈巴契夫 與葉爾辛/美國國務卿貝克和伊拉克總統海珊/我們跟所有相關的人士/辜汪 ge1 ba1 qi4 fu1 yu3 ye4 er3 xin1/mei3 guo2 guo2 wu4 qing1 bei4 ke4 he2/han4 yi1 la1 ke4 zong3 tong3 hai3 shan1/wo3 men gen1 suo3 you3 xiang1 guan1 de ren2 shi4/gu1 wang1 ‘Gorbacheve and Yeltsin/the American Secretary of State, James Baker, and the President of Iraq, Saddam Hussein/we and all the related people/Koo and Wang’. Among these three overt connectives and the covert linker, gen1 is the most colloquial and is often used in daily conversation, whereas yu3 and the covert linker usually appear in formal texts. There are seventy-one instances of Interlocutor_1 and Interlocuteor_2 using hui4 tan2 in Sinica Corpus. The distribution of the four linking devices is shown in table 2 below.  pattern count total  Interlocutor_1 conj. Interlocutor_2 Interlocutor_1 Interlocutor_2  與 yu3 和 he2/han4 跟 gen1  covert linker  46  9  
In this paper, we describe a new method for extracting monolingual collocations. The method is based on statistical methods extracts. VN collocations from large textual corpora. Being able to extract a large number of collocations is very critical to machine translation and many other application. The method has an element of snowballing in it. Initially, one identifies a pattern that will produce a large portion of VN collocations. We experimented with an implementation of the proposed method on a large corpus with satisfactory results. The patterns are further refined to improve on the precision ration. 
Much work has been reported in the literature of computational linguistics studying how to align English-French and English-Germany sentences. While the length-based approach (Church and Gale 1991; Brown et al. 1992) to sentence alignment produces surprisingly good results for the language pair of French and English at success rates well over 96% by sentence, it does not fair as well for alignment of English and Chinese sentences. Work on sentence alignment of English and Chinese texts (Wu 1994), indicates that the lengths of English and Chinese texts are not as highly correlated as in French-English task, leading to lower success rate (85-94%) for length based aligners. Simard, Foster, and Isabelle (1992) proposed 
The named-entity phrases in free text represent a formidable challenge to text analysis. Translating a named-entity is important for the task of Cross Language Information Retrieval and Question Answering. However, both tasks are not easy to handle because named-entities found in free text are often not listed in a monolingual or bilingual dictionary. Although it is possible to identify and translate named-entities on the fly without a list of proper names and transliterations, an extensive list certainly will ensure the high accuracy rate of text analysis. We use a list of proper names and transliterations to train a Machine Transliteration Model. With the model it is possible to extract proper names and their transliterations in a bilingual corpus with high average precision and recall rates.  1. Introduction Multilingual named entity identification and (back) transliteration has been increasingly recognized as an important research area for many applications, including machine translation (MT), cross language information retrieval (CLIR), and question answering (QA). These transliterated words are often domainspecific and many of them are not found in existing bilingual dictionaries. Thus, it is difficult to handle transliteration only via simple dictionary lookup. For CLIR, the accuracy of transliteration highly affects the performance of retrieval. Transliteration of proper names tends to be varied from translator to translator. Consensus on transliteration of celebrated place and person names emerges over a short period of inconsistency and stays  unique and unchanged thereafter. But for less known persons and unfamiliar places, the transliterations of names may vary a great deal. That is exacerbated by different systems used for Ramanizing Chinese or Japanese person and place names. For back transliteration task of converting many transliterations back to the unique original name, there is one and only solution. So back transliteration is considered more difficult than transliteration. Knight and Graehl (1998) pioneered the study of machine transliteration and proposed a statistical transliteration model from English to Japanese to experiment on back transliteration of Japanese named entities. Most previous approaches to machine transliteration (Al-Onaizan and Knight, 2002; Chen et al., 1998; Lin and Chen, 2002); English/Japanese (Knight and Graehl, 1998; Lee and Choi, 1997; Oh and Choi, 2002) focused on the tasks of transliteration and back-transliteration. Very little has been touched upon for the issue of aligning and acquiring words and transliterations in a parallel corpus. The alternative to on-the-fly (back) machine transliteration is simple lookup in an extensive list automatically acquired from parallel corpora. Most instances of (back) transliteration of proper names can often be found in a parallel corpus of substantial size and relevant to the task. For instance, fifty topics of the CLIR task in the NTCIR 3 evaluation conference contain many named entities (NEs) that require (back) transliteration. The CLIR task involves document retrieval from a collection of late 1990s news articles published in Taiwan. Most of those NEs and transliterations can be found in the articles from the Sinorama Corpus of parallel Chinese-English articles dated from 1990 to 2001, including “Bill Clinton,” “Chernobyl,” “Chiayi,” “Han dynasty,” “James Soong,” “Kosovo,” “Mount Ali,” “Nobel Prize,” “Oscar,” “Titanic,” and “Zhu Rong Ji.” Therefore it is important for CLIR research that we align and extract words and transliterations in a parallel corpus. In this paper, we propose a new machine transliteration method based on a statistical model trained automatically on a bilingual proper name list via unsupervised learning. We also describe how the parameters in the model can be estimated and smoothed for best results. Moreover, we show how the model can be applied to align and extract words and their transliterations in a parallel corpus. 2  The remainder of the paper is organized as follows: Section 2 lays out the model and describes how to apply the model to align word and transliteration. Section 3 describes how the model is trained on a set of proper names and transliterations. Section 4 describes experiments and evaluation. Section 5 contains discussion and we conclude in Section 6.  2. Machine Transliteration Model  We will first illustrate our approach with examples. A formal treatment of the approach will follow in Section 2.2. 2.1 Examples  Consider the case where one is to convert a word in English into another language, says Chinese, based on its phonemes rather than meaning. For instance, consider transliteration of the word “Stanford,” into  Chinese. The most common transliteration of “Stanford” is “  .” (Ramanization: [shi-dan-fo]). We  assume that transliteration is a piecemeal, statistical process, converting one to six letters at a time to a Chinese character. For instance, to transliterate “Stanford,” the word is broken into “s,” “tan,” “for,” and “d,” which are converted into zero to two Chinese characters independently. Those fragments of the word in question are called transliteration units (TUs). In this case, the TU “s” is converted to the Chinese character “ ,” “tan” to “ ,” “for” to “ ,” and “d” to the empty string λ. In other words, we model the  transliteration process based on independence of conversion of TUs. Therefore, we have the translitera-  tion probability of getting the transliteration “  ” given “Stanford,” P(  | Stanford),  P(  | Stanford) = P( | s) P( | tan) P( | for) P( λ | d)  There are several ways such a machine transliteration model (MTM) can be applied, including (1) transliteration of proper names (2) back transliteration to the original proper name (3) wordtransliteration alignment in a parallel corpus. We formulate those three problems based on the probabilistic function under MTM:  3  Transliteration problem (TP) Given a word w (usually a proper noun) in a language (L1), produce automatically the transliteration t in another language (L2). For instance, the transliterations in (2) are the results of solving the TP for four given words in (1).  (1) Berg, Stanford, Nobel,  (2) ,  ,  , Tsing Hua  Back transliteration Problem (BTP) Given a transliteration t in a language (L2), produce automatically the original word w in (L1) that gives rise to t. For instance, the words in (4) are the results of solving the BTP for two given transliterations in (3).  (3)  , Lin Ku-fang  (4) Michelangelo,  Word Transliteration Alignment Problem (WTAP)  Given a pair of sentence and translation counterpart, align the words and transliterations therein. For in-  stance, given (5a) and (5b), the alignment results are the three word-transliteration pairs in (6), while the  two pairs of word and back transliteration in (8) are the results of solving WTAP for (7a) and (7b)  (5a) Paul Berg, professor emeritus of biology at Stanford University and a Nobel laureate, …  (5b)  
In this paper, we describe an algorithm that employs syntactic and statistical analysis to extract bilingual collocations from a parallel corpus. The preferred syntactic patterns are obtained from idioms and collocations in a machine-readable dictionary. Phrases matching the patterns are extract from aligned sentences in a parallel corpus. Those phrases are subsequently matched up via cross-linguistic statistical association. Statistical association between the whole collocations as well as words in collocations is used jointly to link a collocation and its counterpart collocation in the other language. We experimented with an implementation of the proposed method on a very large Chinese-English parallel corpus with satisfactory results. 1. Introduction Collocations like terminology tend to be lexicalized and have a somewhat more restricted meaning than the surface form suggested (Justeson and Katz 1995). Collocations are recurrent combinations of words that co-occur more often than chance. The words in a collocation may appear next to each other (rigid collocations) or otherwise (flexible/elastic collocations). On the other hand, collocations can be classified into lexical and grammatical collocations (Benson, Benson, Ilson, 1986).  
In this paper we will present a system, called LiveTrans, which can generate translation suggestions for given user queries and provide an English-Chinese cross-language search service for the retrieval of both Web pages and images. The system effectively utilizes two kinds of Web resources: anchor texts and search results. The developed anchor-text-based and search-result-based methods are complementary in the precision and coverage rates and promising in extracting translations of unknown query terms that were not included in general-purpose translation dictionaries. Experimental results demonstrate the feasibility of the system.  1. Introduction  To deal with automatic construction of translation lexicons, conventional research on machine translation (MT)  [3] and cross-language information retrieval (CLIR) [1, 5, 7, 10, 13, 18] has generally used statistical  techniques to automatically extract word translations from domain-specific parallel/comparable bilingual texts,  such as bilingual newspapers [4, 11, 12, 20, 21]. However, only a certain set of their translations can be  extracted through corpora with limited domains. In our research, we are interested in extracting translations of  technical terms and proper names in diverse subjects, which are especially needed in performing CLIR  海珊 哈珊 侯賽因 嚴重急性呼吸道症候群 services for Web users, e.g., “Hussein” ( / /  ), “SARS” (  ). Existing  CLIR systems usually rely on bilingual dictionaries for query translation [1, 13, 15]. Unfortunately, our analysis of Dreamer query log collected in Taiwan (see Section 3.1) showed that 74% of the 20,000 high frequent Web queries can not be found in general-purpose English-Chinese dictionaries (they are called unknown terms in this paper). How to automatically find translations for unknown terms, therefore, has become a major challenge for cross-language Web search. Different from previous works, we focus on investigating new approaches to mining multilingual Web resources [19]. We have proposed a novel approach to extracting translations of Web queries through the  mining of Web anchor texts and link structures [16, 17]. An anchor text is the descriptive part of an out-link of a Web page used to provide a brief description of the linked page. A variety of anchor texts in multiple languages might link to the same pages from all over the world. For example, Figure 1 shows a typical example, in which there are a variety of anchor texts in multiple languages linking to the Yahoo! from all over the world. Such a bundle of anchor texts pointing together to the same page is called an anchor-text set. Web anchor-text sets may contain similar description texts in multiple languages. Thus, for an unknown term appearing in some anchor-text sets, it is likely that its corresponding target translations appear together in the same anchor-text sets. However, discovering translation knowledge from the Web has not been fully explored. In this paper, we intend to investigate another kind of Web resource, search results, and try to combine them with the anchor texts to benefit term translation. Chinese pages on the Web consist of rich texts in a mixture of Chinese (main language) and English (auxiliary language), and many of them contain translations of proper nouns. According to our observations, many search result pages in Chinese Web usually contain snippets of summaries in a mixture of Chinese and English. For example, Figure 2 illustrates the search-result page of the English query “National Palace Museum,” which was submitted to Google for searching Chinese pages, could obtain many relevant results containing both the query itself and its Chinese aliases. To explore search results on extraction of term translation, we have employed two methods: the chi-square test and context-vector analysis. Based on a novel integration of the developed anchor-text- and search-result-based methods, we implemented an experimental system, called LiveTrans, to provide English-Chinese translation suggestion and cross-lingual retrieval of both Web pages and images. The purpose of this paper is to introduce our experiences in developing the methods and implementing the system. 2. Related Work Term translation extraction is an important research problem in the context of MT. A number of related researches [12, 21] have used sentence-aligned parallel corpora to extract translations since the advent of statistical translation model [3]. Although high accuracy can be easily achieved by these techniques, sufficiently large parallel corpora for various subject domains and language-pairs are still hard to be available. On the other hand, some work has been done on term translation extraction from comparable or even unrelated texts [11, 20]. However, using non-parallel corpora is more difficult to effectively extract translations than parallel corpora due to the lack of parallel correlation aligned between documents or sentence pairs.  Figure 1. An illustration showing various anchor texts in multiple languages linking to Yahoo! from all over the world [17].  國立故宮博物院 故宮 Figure 2. An illustration showing translation equivalents, such as “National Palace  Museum”/  ( ), which are included in a search result page  returned from Google.  On the other hand, CLIR has become an important topic in recent research on information retrieval, however, practical cross-language Web search services have not lived up to expectations. This task must face a number of challenges, especially the problem of query translation. To deal with such problem, existing CLIR systems mostly rely on bilingual dictionaries. These dictionary-based techniques are limited in real-world applications since queries often contain unknown query terms, such as personnel names and technical terms [15]. Although some methods integrating dictionary-based techniques with parallel-corpus disambiguation, technology have been proposed and achieved performance improvements [1, 13]. Nonetheless, the unavailability of translations of unknown Web queries in diverse subjects is still a thorny problem.  A page1 modified by Oard lists some CLIR retrieval systems, which can be either used on the Internet or obtained from commercial sources. For example, the Multilingual Summarization and Translation (MuST2) system is a Web-accessible CLIR system that uses English queries to search Indonesian, Spanish, Arabic and Japanese. MTIR is a demonstration search system that accepts queries in Chinese, finds documents in English, and then translates the selected documents into Chinese [1]. These systems generally rely on built-in bilingual dictionaries for query translation. To our knowledge, the proposed LiveTrans system is one of the few CLIR systems which allow the translations of unknown queries to be extracted through the mining of Web resources.  3. LiveTrans System  The LiveTrans3 system is an experimental meta-search engine that provides English-Chinese translation  suggestion and cross-language search for retrieval of both Web pages and images. It was implemented based on  a novel combination of the developed Web mining methods. To use the system, users may select either English,  traditional Chinese or simplified Chinese as the source/target language. For each input source query, the system  will suggest a list of target translations. Since real queries are often short, there is a lack of context information  needed to perform query translation. The system combines the term translation extraction methods and bilingual  lexicons to make suggestions. The users can select the preferred translation and the system will return the  retrieved Web pages and images, and sort them in their order of decreasing relevance to the corresponding  translated queries. The titles of the retrieved pages are also translated word by word to the source languages for  reference (i.e. gloss translation). Like most of the meta-search engines, backend engines can be chosen and the  retrieved results can be merged using a data fusion technique. The system has been used to collect translation  equivalents of a certain portion of users’ queries. Many of the obtained translations are really not easy for  human indexers to compile. For example, in the case shown in Figure 3, the user selected English as the source  language and Chinese as the target language. In this example, the given query was “Academia Sinica” and its  中央研究院 中研院 translations were extracted, i.e.,  and  .  We sometimes refer to the Web as a globally interconnected information infrastructure. At present, however, for someone who reads only English, it is presently the English-Wide-Web, and a reader of only Chinese sees only the Chinese-Wide-Web. With the LiveTrans system, it is easy to see that there are a number  
 This paper proposes a corpora-based approach in comparing the Mapping Principles for economy metaphors in English and Chinese. The Mapping Principles are validated using an upper ontology (SUMO). This research extends on the work of Ahrens, Chung and Huang (2003) by examining the ‘economy’ metaphors in Chinese and English. In Ahrens, Chung and Huang (2003), they proposed to delimit the Mapping Principle via two steps: First, they used a corpora-based analysis on the word jingji ‘economy’ to find out the most prototypical mappings in a metaphor Second, they used an upper ontology (SUMO) to examine whether the mapping principle is a representation of conceptual knowledge in the ontology. This paper goes a step further by examining the similarities and differences of source domains in English and Chinese. Using the Conceptual Mapping Model, this paper looks particularly into the example of ECONOMY IS A PERSON. This paper observes the representation of shared knowledge in the source domain in different languages and explains the similarities and differences by looking into the definition of inference rules in the upper ontology of SUMO. Key Words: Corpora, Conceptual Mapping Model, Mapping Principle, SUMO, ontology  
 Jia-Lin Tsai, Gladys Hsieh and Wen-Lian Hsu Institute of Information Science, Academia Sinica Nankang, Taipei, Taiwan, R.O.C. {tsaijl,gladys,hsu}@iis.sinica.edu.tw  A meaningful noun-verb word-pair in a sentence is called a noun-verb event-frame (NVFE). Previously, we have developed an NVEF word-pair identifier to demonstrate that NVEF knowledge can be used effectively to resolve the Chinese word-sense disambiguation (WSD) problem (with 93.7% accuracy) and the Chinese syllable-to-word (STW) conversion problem (with 99.66% accuracy) on the NVEF related portion. In this paper, we propose a method for automatically acquiring a large scale NVEF knowledge without human intervention. The automatic discovery of NVEF knowledge includes four major processes: (1) segmentation check; (2) Initial Part-of-speech (POS) sequence generation; (3) NV knowledge generation and (4) automatic NVEF knowledge confirmation. Our experimental results show that the precision of the automatically acquired NVEF knowledge reaches 98.52% for the test sentences. In fact, it has automatically discovered more than three hundred thousand NVEF word-pairs from the 2001 United Daily News (2001 UDN) corpus. The acquired NVEF knowledge covers 48% NV-sentences in Academia Sinica Balanced Corpus (ASBC), where an NV-sentence is one including at least a noun and a verb. In the future, we will expand the size of NVEF knowledge to cover more than 75% of NV-sentences in ASBC. We will also apply the acquired NVEF knowledge to support other NLP researches, in particular, shallow parsing, syllable/speech understanding and text indexing.  Keywords: noun-verb event frame (NVEF), machine learning, Hownet, WSD, STW  
In order to achieve fast and high quality Part-of-speech (PoS) tagging, algorithms should be high accuracy and require less manually proofreading. To evaluate a tagging system, we proposed a new criterion of reliability, which is a kind of cost-effective criterion, instead of the conventional criterion of accuracy. The most cost-effective tagging algorithm is judged according to amount of manual editing and achieved final accuracy. The reliability of a tagging algorithm is defined to be the estimated best accuracy of the tagging under a fixed amount of proofreading. We compared the tagging accuracies and reliabilities among different tagging algorithms, such as Markov bi-gram model, Bayesian classifier, and context-rule classifier. According to our experiments, for the best cost-effective tagging algorithm, in average, 20% of samples of ambivalence words need to be rechecked to achieve an estimated final accuracy of 99%. The tradeoffs between amount of proofreading and final accuracy for different algo-  rithms are also compared. It concludes that an algorithm with highest accuracy may not always be the most reliable algorithm. 
In various Asian languages, including Chinese, there is no space between words in texts. Thus, most Chinese NLP systems must perform word-segmentation (sentence tokenization). However, successful word-segmentation depends on having a sufficiently large lexicon. On the average, about 3% of the words in text are not contained in a lexicon. Therefore, unknown word identification becomes a bottleneck for Chinese NLP systems. In this paper, we present a Chinese word auto-confirmation (CWAC) agent. CWAC agent uses a hybrid approach that takes advantage of statistical and linguistic approaches. The task of a CWAC agent is to auto-confirm whether an n-gram input (n ≥ 2) is a Chinese word. We design our CWAC agent to satisfy two criteria: (1) a greater than 98% precision rate and a greater than 75% recall rate and (2) domain-independent performance (F-measure). These criteria assure our CWAC agents can work automatically without human intervention. Furthermore, by combining several CWAC agents designed based on different principles, we can construct a multi-CWAC agent through a building-block approach. Three experiments are conducted in this study. The results demonstrate that, for n-gram frequency ≥ 4 in large corpus, our CWAC agent can satisfy the two criteria and achieve 97.82% precision, 77.11% recall, and 86.24% domain-independent F-measure. No existing systems can achieve such a high precision and domain-independent F-measure. The proposed method is our first attempt for constructing a CWAC agent. We will continue develop other CWAC agents and integrating them into a multi-CWAC agent system. Keywords: natural language processing, word segmentation, unknown word, agent 
This paper presents a maximum entropy based Chinese named entity recognizer (NER): Mencius. It aims to address Chinese NER problems by combining the advantages of rule-based and machine learning (ML) based NER systems. Rule-based NER systems can explicitly encode human comprehension and can be tuned conveniently, while ML-based systems are robust, portable and inexpensive to develop. Our hybrid system incorporates a rule-based knowledge representation and template-matching tool, InfoMap [1], into a maximum entropy (ME) framework. Named entities are represented in InfoMap as templates, which serve as ME features in Mencius. These features are edited manually and their weights are estimated by the ME framework according to the training data. To avoid the errors caused by word segmentation, we model the NER problem as a character-based tagging problem. In our experiments, Mencius outperforms both pure rule-based and pure ME-based NER systems. The F-Measures of person names (PER), location names (LOC) and organization names (ORG) in the experiment are respectively 92.4%, 73.7% and 75.3%. 
The TAP-XL Automated Analyst’s Assistant is an application designed to help an Englishspeaking analyst write a topical report, culling information from a large inflow of multilingual, multimedia data. It gives users the ability to spend their time finding more data relevant to their task, and gives them translingual reach into other languages by leveraging human language technology. 
We will demonstrate a spoken dialogue interface to a Geologist’s Field Assistant that is being developed as part of NASA’s Mobile Agents project. The assistant consists of a robot and an agent system which helps an astronaut wearing a planetary space suit while conducting a geological exploration. The primary technical challanges relating to spoken dialogue systems that arise in this project are speech recognition in noise, open-microphone, and recording voice annotations. This system is capable of discriminating between speech intended for the system and for other purposes.  Start tracking my g p s coordinates. Start logging my bio sensors every ﬁfteen seconds. Where is my my current location? Call this location Asparagus. Create a new sample bag and label it sample bag three. Take a voice note Please begin recording voice note now: This sample originated in a dry creek bed. [pause] Would you like to continue recording the voice note? no Voice note has been created. Associate that voice note with sample bag three. Play the voice note associated with sample bag three. Table 1: Example utterances  
The QCS information retrieval (IR) system is presented as a tool for querying, clustering, and summarizing document sets. QCS has been developed as a modular development framework, and thus facilitates the inclusion of new technologies targeting these three IR tasks. Details of the system architecture, the QCS interface, and preliminary results are presented. 
 ¢  £  £  Institute for Informatics and Telecommunications, NCSR “Demokritos” ¥ vangelis, costass @iit.demokritos.gr ¦  Velti S.A. ¡ Dsouﬂis@velti.net  Division of Informatics, University of Edinburgh ¢ ¥ grover, bhachey @ed.ac.uk ¦  D.I.S.P., Universita di Roma Tor Vergata £ ¥ pazienza, vindigni @info.uniroma2.it ¦ ¤ Lingway ¥ emmanuel.cartier, Jose.Coch @lingway.com ¦  
Columbia’s Newsblaster tracking and summarization system is a robust system that clusters news into events, categorizes events into broad topics and summarizes multiple articles on each event. Here we outline our most current work on tracking events over days, producing summaries that update a user on new information about an event, outlining the perspectives of news coming from different countries and clustering and summarizing non-English sources. 
WordFreak is a natural language annotation tool that has been designed to be easy to extend to new domains and tasks. Speciﬁcally, a plug-in architecture has been developed which allows components to be added to WordFreak for customized visualization, annotation speciﬁcation, and automatic annotation, without re-compilation. The APIs for these plug-ins provide mechanisms to allow automatic annotators or taggers to guide future annotation to supports active learning. At present WordFreak can be used to annotate a number of different types of annotation in English, Chinese, and Arabic including: constituent parse structure and dependent annotations, and ACE named-entity and coreference annotation. The Java source code for WordFreak is distributed under the Mozilla Public License 1.1 via SourceForge at: http://wordfreak.sourceforge.net. This site also provides screenshots, and a web deployable version of WordFreak. 
The JAVELIN system integrates a ﬂexible, planning-based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text. The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus. The operation of the system will be explained in depth through browsing the repository of data objects created by the system during each question answering session. 
We will demonstrate the output of a distributional clustering algorithm called Clustering by Committee that automatically discovers word senses from text1. 
Leximancer is a software system for performing conceptual analysis of text data in a largely language independent manner. The system is modelled on Content Analysis and provides unsupervised and supervised analysis using seeded concept classiﬁers. Unsupervised ontology discovery is a key component. 
 Ralph Grishman  Computer Science Department  New York University  715 Broadway, 7th Floor,  New York, NY  sudo,sekine,grishman @cs.nyu.edu ¡  
In this work, we present a new semantic language modeling approach to model news stories in the Topic Detection and Tracking (TDT) task. In the new approach, we build a unigram language model for each semantic class in a news story. We also cast the link detection subtask of TDT as a two-class classiﬁcation problem in which the features of each sample consist of the generative log-likelihood ratios from each semantic class. We then compute a linear discriminant classiﬁer using the perceptron learning algorithm on the training set. Results on the test set show a marginal improvement over the unigram performance, but are not very encouraging on the whole. 
This paper tests speech recognition using prosody dependent allophone models. The log likehoods of various prosodically labeled phonemes are calculated using Baum-Welsh re-estimation. These log likehoods are then compared to log likehoods of non-prosodically labeled phonemes. Based on the comparison of these log likehoods, it can be concluded that modeling all prosodic information directly in the vowel model leads to improvement in the model. Consonants, on the other hand, split naturally into three categories, strengthened, lengthened and neutral. 1. Introduction Prosody is an important factor in how humans interpret speech. The same word string can have different meanings depending on the way it is said. Many linguists have performed extensive studies of prosody and of the effects of prosodic factors on spoken language. In his dissertation, Cho (2001) investigates how phonetic features are conditioned by prosodic factors by examining pre-boundary, post-boundary, and accented syllables. Cho reports that boundary induced articulatory strengthening occurs in phrase final vowel positions and phrase initial consonant positions. Phrase initial vowels are also more susceptible to coarticulation than phrase final vowels. Cho also hypothesizes that accented syllables are characterized primarily by sonority expansion. An accented vowel is usually not affected by coarticulation with a neighboring vowel. Strengthening effects caused by boundaries and accents cannot be considered the same and Cho discusses several differences between boundary and accent strengthening effects. In a study performed by Edwards et al (1991), the effect of final lengthening at prosodic boundaries was examined by studying articulator movement patterns. It was found that decreasing intragestural  stiffness slows down the syllable, affecting the tempo of the spoken word, causing the syllable to be lengthened. The changing of intergestural phrasing also affects the syllable duration by decreasing the overlap of a vowel gesture with a consonant gesture. This increases the duration of accented syllables comparatively to unaccented syllables and causes the accented syllable to be strengthened. De Jong (1994) investigated the supraglottal correlates of linguistic prominence in English. De Jong suggests that stress involves a localized shift toward hyperarticulated speech. An increase in the duration in the closure and in the aspiration of initial voiceless stops was observed along with an increase in duration of prevoicing in initial voiced stops in stressed syllables. Fougeron and Keating (1997) report that on the edges of prosodic phrase boundaries, final vowels and initial consonants have less reduced lingual articulation. The differences in articulation were manifested in the linguopalatal contact of boundary consonants and vowels. The linguopalatal contact of both consonants and vowels relates directly to the type and size of phrase boundary. Boundary type and size also appear to effect the acoustic duration of post-boundary consonants. Wightman et al (1992) report that there is segmental lengthening in the rhyme of a syllable that directly precedes a phrase boundary. Wightman examines the effect of duration and pause on boundary words and shows that speaking rate effects the distribution of phoneme duration. The lengthening effects of pre-boundary syllables can be used to distinguish several different types of phrase boundaries. These results show that prosody can cause variations not just in pitch, but also in the articulation of phonetic contrasts in different phonemes. These variations can be modeled as a part of the phoneme definition in an automatic speech recognition (ASR) system. However, the question is whether or not modeling prosodic factors with phonemes would lead to improvements in the quality of the phoneme model and thus lead to improvements in both the correctness and accuracy in an ASR system. Most modern speech recognizers function by breaking words up into mathematical features. The recognizer then determines the most likely occurring set  Consonants  b  ch  d  dh  f  g  hh  jh  k  l  m  n  p  r  s  sh  t  v  w  y  z  Vowels  aa  ae  ah  ao  aw  ax  ay  eh  el  er  ey  ih  iy  ow  oy  uh  uw  Figure 1. This figure contains a chart of the 38 different non-prosodically distinguished phonemes used for experimentation.  phn : phrase medial phn! : phrase medial, accented phnB4 : phrase final, unaccented phnB4! : phrase final, accented B4phn : phrase initial, unaccented B4phn! : phrase initial, accented Figure 2. The different prosodic labels. represents some generic phoneme.  “Phn”  of phonemes by comparing these extracted features with its own phoneme models. Phonemes are usually modeled using hidden Markov Models (HMMs). Once the recognizer has identified a set of the most likely occurring phonemes, it then uses a dictionary to match a word or group of words to that set. Prosody can be incorporated into the phoneme model by allowing two different HMMs to represent a single phoneme. One HMM would need to represent the prosody independent version of the phoneme while the other would represent the phoneme in some prosodic context. This could allow the recognizer to do things such as distinguish between accented and unaccented phonemes or distinguish between boundary and nonboundary phonemes. Allowing the recognizer to make such a distinction may reduce the confusability of certain phoneme groups, which in turn could allow for increased recognition rates. The goal of this research is to not only determine if the inclusion of prosody in the phoneme model causes improvement in the model, but also to determine which prosodic factors to model and the best way to model them. This will be accomplished by first splitting phonemes into different prosodically varying groups and then by comparing the log probability of the occurrence of each phoneme in those different groups. Because prosody causes noticeable variations in speech, a phoneme model that includes prosodic factors should differ from models of the same phoneme that do not. This difference will prove to be significant enough to show that prosodic factors should be taken into account for a more accurate phoneme model. 2. The Database Boston University’s Radio News Corpus (1995) was used for all experiments. The speakers from this corpus that were analyzed were F1A, F2B, and M2B. The usable data from these three speakers consisted of 259  wav files containing 18270 words. All the wav files that were used were accompanied by two types of prosodic transcription files, .brk and .ton files. The corpus was labeled according to the ToBI standard. Silverman et al (1992) explain the labeling system in detail. It will not be described in this paper. The .brk files specify a ToBI break index (0-4) for every spoken word in the associated wav file. For the experiments, the only boundary distinguished was the intonational phrase boundary (ToBI index 4). All other boundary types (indices 0-3) were grouped together. There were 3855 intonational phrase boundaries in the data set. The .ton files label the times in which an accented vowel occurs. The most abundant accent label was H* which occurs in a ratio of about 10 H* for every single L*. Other accent types do occur, but most include H* in bitonal accent. 3. Prosodic Annotation The set of 38 different phonemes, shown in figure 1, were used in the experiments. 3.1 Allophone Modeling Recognition experiments were preformed for four different allophone sets: • Tied • Accent • Boundary • Untied The Tied set contained no prosodically labeled data. The Accent set contained monophones that were split into two groups, accented and unaccented. Phonemes were not distinguished on the basis of phrase position.  Monophone Group  Tied All Cons. After Vowel Before Vowel Vowels  Accent All Cons. After Vowel Before Vowel Vowels  Boundary All Cons. After Vowel Before Vowel Vowels  Untied All Cons. After Vowel Before Vowel Vowels  Figure 3. The sixteen experimental conditions  The Boundary set modeled monophones as phrase initial, phrase medial, or phrase final. Accented phonemes were not distinguished from unaccented phonemes. The Untied set distinguish phonemes by both phrasal position and accentuation. A monophone in this group could be labeled as phrase medial, phrase medial accented, phrase initial, phrase initial accented, phrase final or phrase final accented. 3.2 Allophone Definitions Figure 2 contains the six different labels used to represent the allophones of a single imaginary phoneme “phn.” A phrase final phoneme was considered to be any phoneme that occurred in the nucleus or coda of the final syllable of a word directly preceding an intonational phrase boundary. Phrase initial phonemes, on the other hand, were considered to be any phoneme in the onset or nucleus of the initial syllable of a word that followed an intonational phrase boundary. Phase medial phonemes were considered to be any other phoneme. An accented vowel was the lexically stressed vowel in a word containing a transcribed pitch accent. Because accented consonants are not clearly defined, three different labeled sets of accented consonants were developed: • All Consonants • After Vowel • Before Vowel All Consonants considered every consonant in a syllable with an accented vowel to also be accented. After Vowel considered as accented only the coda consonants. Before Vowel recognized only the onset consonants of the accented syllable as being accented. Accents were considered to be limited to a single syllable. Because there were three different groups of accented consonants and because there is only one way a vowel can be labeled as accented, vowels were  beyond b iy y aa n d beyond! b iy y aa! n! d! beyondB4 b iy y aaB4 nB4 dB4 beyondB4! b iy y aaB4! nB4! dB4! B4beyond B4b B4iy y aa n d B4beyond! B4b B4iy y aa! n! d! Figure 4. An example of each of the six word types defined with Untied allophones for the After Vowel experimental condition. Boundary allophones could only be used to define three distinct word types, Accent only two, and Tied only one. a. 0 370000 B4in 370000 760000 nineteen! 760000 1150000 seventy 1150000 1680000 sixB4 1680000 2310000 B4democratic! 2310000 2680000 governor b. 600000 1600000 w 1600000 2400000 aa! 2400000 2900000 n! 2900000 3800000 t 3800000 4900000 axB4 4900000 5300000 dB4 Figure 5a. An example Untied word level transcription b. An example Untied phone level transcription for the After Vowel accent condition. The transcribed word is “wanted.” separated into a fourth group of their own, entitled Vowels. The four groups along with the four different allophone models lead to the sixteen experimental conditions illustrated in figure 3. 3.3 Dictionaries and Transcription Types Each experimental condition required its own dictionary and transcription. Just as each phoneme had six distinct allophones, each word had six distinct types. A word could be phrase initial, medial or final and accented or unaccented. Each word type had its own definition. An example dictionary is shown in figure 4. Every experimental condition had both a word level transcription and a phone level transcription. Figure 5 shows an example of the two different levels of transcription files. 4. Experiments  All Consonants  Merge Separate  ch  b  dB4, B4b  B4d d  dhB4 dh  B4dh f  fB4 g  B4f hh  gB4 jh  B4g jhB4  jhB4 k  kB4 l  B4k m  lB4 n  mB4 nB4  B4m p  pB4 r  B4p rB4  sB4 B4r  B4s s  tB4 sh  v  t  B4w tB4  z  vB4  B4v  w  y  zB4  After Vowel  Merge Separate  dhB4 b  gB4 ch  jhB4 d  kB4 dB4  lB4 dh  mB4 f  nB4 fB4  pB4 g  pB4 jh  sB4 k  sh  l  tB4 m  v  n  p  r  rB4  s  sh  t  vB4  y  z  zB4  Before Vowel  Merge Separate  B4d b  B4f B4b  B4g ch  B4k d  B4m dh  B4n f  B4p g  B4s hh  B4w B4hh  z  jh  k  l  m  n  p  r  B4r  s  sh  t  B4t  v  w  y  Vowels  Merge Separate  aoB4 aa  ax  aaB4  B4eh B4aa  B4ey ae  B4ow aeB4  uh  B4ae  uhB4 ah  B4uh ahB4  B4uw ao  aoB4  aw  ay  ayB4  eh  ehB4  ey  eyB4  ih  ihB4  B4ih  iy  iyB4  B4iy  ow  owB4  oy  uw  uwB4  Table 1. The results of experiments for the Accented allophone sets. The "Merge" column lists phonemes with WA ≥ LL. The "Separate" column indicates phonemes where WA < LL. Due to the relatively small size of the data set, several phonemes are missing from the table.  Experiments were performed using the Hidden Markov Toolkit (HTK), which is distributed by the University of Cambridge (2002). Phonemes were modeled using a three-state HMM with no emitting start and end states. Each emitting state consisted of three mixture Gaussians and no state skipping was allowed. 4.1 Experimental Procedure The Radio News Corpus data was divided into 2 sets: a training set and a test set. The test set was approximately 10% of the size of the training set. The experimental procedure was completed for sixteen experimental conditions. The experimental procedure can be divided into two steps. In step one, the training data was used to re-estimate the HMM definitions for each phoneme. Re-estimation was performed with the HTK tool HRest, which uses Baum-Welsh re-estimation described in detail in the HTK book available from Cambridge University (2002). HMM parameters were re-estimated  until either the log likehood converged or HRest had performed 100 iterations of the re-estimation algorithm. In the second step of the experiments, HRest was used to perform a single iteration of the reestimation algorithm on the test data using the HMM definitions that were updated from the re-estimation of the training set. During re-estimation, the log likehoods of each phoneme were output and saved for later comparisons. 4.2 Post Processing Once all the log likehoods had been recorded, the Untied allophone sets were used as a basis to determine if the considered monophones were better modeled as prosody independent or prosody dependent. To determine the best modeling strategy for a particular monophone, six different weighted averages (WA’s) were calculated from the Untied log likehoods and compared to the computed log likehoods of the Boundary, Accent and Tied models.  a.  Initial  Accented  
This paper describes the construction of language choice models for the microplanning of discourse relations in a Natural Language Generation system that attempts to generate appropriate texts for users with varying levels of literacy. The models consist of constraint satisfaction problem graphs that have been derived from the results of a corpus analysis. The corpus that the models are based on was written for good readers. We adapted the models for poor readers by allowing certain constraints to be tightened, based on psycholinguistic evidence. We describe how the design of microplanner is evolving. We discuss the compromises involved in generating more readable textual output and implications of our design for NLG architectures. Finally we describe plans for future work. 
This paper presents an unsupervised method for discriminating among the senses of a given target word based on the context in which it occurs. Instances of a word that occur in similar contexts are grouped together via McQuitty’s Similarity Analysis, an agglomerative clustering algorithm. The context in which a target word occurs is represented by surface lexical features such as unigrams, bigrams, and second order co-occurrences. This paper summarizes our approach, and describes the results of a preliminary evaluation we have carried out using data from the SENSEVAL-2 English lexical sample and the line corpus. 
This paper presents recent developments of an indexing technique aimed at improving parsing times. Although several methods exist today that serve this purpose, most of them rely on statistical data collected during lengthy training phases. Our goal is to obtain a reliable method that exhibits an optimal efﬁciency/cost ratio, without lengthy training processes. We focus here on static analysis of the grammar, a method that has unworthily received less attention in the last few years in computational linguistics. The paper is organized as follows: ﬁrst, the parsing and indexing problem are introduced, followed by a description of the general indexing strategy for chart parsing; second, a detailed overview and performance analysis of the indexing technique used for typedfeature structure grammars is presented; ﬁnally, conclusions and future work are outlined. 
Large-scale parsing is still a complex and timeconsuming process, often so much that it is infeasible in real-world applications. The parsing system described here addresses this problem by combining ﬁnite-state approaches, statistical parsing techniques and engineering knowledge, thus keeping parsing complexity as low as possible at the cost of a slight decrease in performance. The parser is robust and fast and at the same time based on strong linguistic foundations. 
Word fragments pose serious problems for speech recognizers. Accurate identiﬁcation of word fragments will not only improve recognition accuracy, but also be very helpful for disﬂuency detection algorithm because the occurrence of word fragments is a good indicator of speech disﬂuencies. Different from the previous effort of including word fragments in the acoustic model, in this paper, we investigate the problem of word fragment identiﬁcation from another approach, i.e. building classiﬁers using acoustic-prosodic features. Our experiments show that, by combining a few voice quality measures and prosodic features extracted from the forced alignments with the human transcriptions, we obtain a precision rate of 74.3% and a recall rate of 70.1% on the downsampled data of spontaneous speech. The overall accuracy is 72.9%, which is signiﬁcantly better than chance performance of 50%. 
Topic detection and tracking approaches monitor broadcast news in order to spot new, previously unreported events and to track the development of the previously spotted ones. The dynamical nature of the events makes the use of state-of-the-art methods difﬁcult. We present a new topic deﬁnition that has potential to model evolving events. We also discuss incorporating ontologies into the similarity measures of the topics, and illustrate a dynamic hierarchy that decreases the exhaustive computation performed in the TDT process. This is mainly work-in-progress. 
In this paper we describe a novel approach to lexical chain based segmentation of broadcast news stories. Our segmentation system SeLeCT is evaluated with respect to two other lexical cohesion based segmenters TextTiling and C99. Using the Pk and WindowDiff evaluation metrics we show that SeLeCT outperforms both systems on spoken news transcripts (CNN) while the C99 algorithm performs best on the written newswire collection (Reuters). We also examine the differences between spoken and written news styles and how these differences can affect segmentation accuracy. 
In this paper, we propose a novel Cooperative Model for natural language understanding in a dialogue system. We build this based on both Finite State Model (FSM) and Statistical Learning Model (SLM). FSM provides two strategies for language understanding and have a high accuracy but little robustness and flexibility. Statistical approach is much more robust but less accurate. Cooperative Model incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies. 
 We present a novel system for automatically marking up text documents into XML and discuss the benefits of XML markup for intelligent information retrieval. The system uses the Self-Organizing Map (SOM) algorithm to arrange XML marked-up documents on a twodimensional map so that similar documents appear closer to each other. It then employs an inductive learning algorithm C5 to automatically extract and apply markup rules from the nearest SOM neighbours of an unmarked document. The system is designed to be adaptive, so that once a document is marked-up; its behaviour is modified to improve accuracy. The automatically marked-up documents are again categorized on the Self-Organizing Map. 
We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Signiﬁcantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create ﬁrst-pass bigram lattices or N-best lists, these results are highly relevant. 
Sources of training data suitable for language modeling of conversational speech are limited. In this paper, we show how training data can be supplemented with text from the web ﬁltered to match the style and/or topic of the target recognition task, but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of N-grams. 
We describe initial experiments in combining the output of question answering systems using data from the 2002 TREC Question Answering task. We explore several distance-based combining methods, as well as a number of distance metrics involving both word and character ngrams. 
Story link detection has been regarded as a core technology for other Topic Detection and Tracking tasks such as new event detection. In this paper we analyze story link detection and new event detection in a retrieval framework and examine the effect of a number of techniques, including part of speech tagging, new similarity measures, and an expanded stop list, on the performance of the two detection tasks. We present experimental results that show that the utility of the techniques on the two tasks differs, as is consistent with our analysis. 
 2 Adaptation Methods  In order to boost the translation quality of EBMT based on a small-sized bilingual corpus, we use an out-of-domain bilingual corpus and, in addition, the language model of an indomain monolingual corpus. We conducted experiments with an EBMT system. The two evaluation measures of the BLEU score and the NIST score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model.  EBMT (Nagao, 1984) retrieves the translation examples that are most similar to an input expression and adjusts the examples to obtain the translation. The EBMT system in our approach retrieves not only indomain examples, but also out-of-domain examples. When using out-of-domain examples, suitability to the target domain is considered. We tried the following three types of adaptation methods. (1) Merging equally  
This paper describes an application of active learning methods to the classiﬁcation of phone strings recognized using unsupervised phonotactic models. The only training data required for classiﬁcation using these recognition methods is assigning class labels to the audio ﬁles. The work described here demonstrates that substantial savings in this effort can be obtained by actively selecting examples to be labeled using conﬁdence scores from the BoosTexter classiﬁer. The saving in class labeling effort is evaluated on two different spoken language system domains in terms both of the number of utterances to be labeled and the length of the labeled utterances in phones. We show that savings in labeling effort of around 30% can be obtained using active selection of examples. 
The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using Viterbi search to find the highest probability tag sequence for a given sentence. Further we examine the use of syntactic pattern based re-ranking to further increase performance. We analyze our strategy using both extracted and human generated syntactic features. Experiments indicate 85.7% accuracy using human annotations on a held out test set. 
We combine a surface based approach to discourse parsing with an explicit rhetorical grammar in order to efficiently construct an underspecified representation of possible discourse structures. 
To support summarization of automatically transcribed meetings, we introduce a classiﬁer to recognize agreement or disagreement utterances, utilizing both word-based and prosodic cues. We show that hand-labeling efforts can be minimized by using unsupervised training on a large unlabeled data set combined with supervised training on a small amount of data. For ASR transcripts with over 45% WER, the system recovers nearly 80% of agree/disagree utterances with a confusion rate of only 3%. 
In this paper, we propose an automatic quantitative expansion method for a sentence set that contains sentences of the same meaning (called an equivalent sentence set). This task is regarded as paraphrasing. The features of our method are: 1) The paraphrasing rules are dynamically acquired by Hierarchical Phrase Alignment from the equivalent sentence set, and 2) A large equivalent sentence set is generated by substituting source syntactic structures. Our experiments show that 561 sentences on average are correctly generated from 8.48 equivalent sentences. 
We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton. For our purposes, a hub is a node in a graph with in-degree greater than one and out-degree greater than one. We create a word-trie, transform it into a minimal DFA, then identify hubs. Those hubs mark the boundary between root and suffix, achieving similar performance to more complex mixtures of techniques. 
We report results of experiments aimed at improving the translation quality by incorporating the cognate information into translation models. The results conﬁrm that the cognate identiﬁcation approach can improve the quality of word alignment in bitexts without the need for extra resources. 
We present a syntax-based constraint for word alignment, known as the cohesion constraint. It requires disjoint English phrases to be mapped to non-overlapping intervals in the French sentence. We evaluate the utility of this constraint in two different algorithms. The results show that it can provide a signiﬁcant improvement in alignment quality. 
Human tutors detect and respond to student emotional states, but current machine tutors do not. Our preliminary machine learning experiments involving transcription, emotion annotation and automatic feature extraction from our human-human spoken tutoring corpus indicate that the spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states. 
This paper describes a domain-independent, machine-learning based approach to temporally anchoring and ordering events in news. The approach achieves 84.6% accuracy in temporally anchoring events and 75.4% accuracy in partially ordering them. 
We report on results of combining graphical modeling techniques with Information Extraction resources (Pattern Dictionary and Lexicon) for both frame and semantic role assignment. Our approach demonstrates the use of two human built knowledge bases (WordNet and FrameNet) for the task of semantic extraction. 1. Introduction Portability and domain independence are critical challenges for Natural Language Processing (NLP) systems. The ongoing development of public knowledge bases such as WordNet, FrameNet, CYC, etc. has the potential to support domain independent solutions to NLP. The task of harnessing the appropriate information from these resources for an application remains significant. This paper reports on the use of semantic resources for a necessary component of scalable NLP systems, Semantic Extraction (SE) . Semantic Extraction pertains to the assignment of semantic bindings to short units of text (usually sentences). The SE problem is quite similar to the Information Extraction (IE) task, in that in both cases we are interested only in certain predicates and their argument bindings and not in full understanding. However there are major differences as well. IE is a pre-specified and autonomous task with a narrow domain of focus, where all the information of interest is represented in the extraction template. SE involves finding predicate-argument structures in open domains and is a crucial semantic parsing step in a text understanding task. In this paper we report results obtained from combining IE and graphical modeling techniques, with semantic resources (WordNet and FrameNet) for automatic Semantic Extraction.  2. Background Semantic Extraction has become a strong research focus in the last few years. A good example is the work of Gildea and Jurafsky (2002) (GJ). GJ present a comprehensive empirical approach to the problem of semantic role assignment. Their work looked at the problem of assigning semantic roles to text based on a statistical model of the FrameNet1 data. In their work, GJ assume that the frame of interest is determined a-priori for every sentence. In the IE community, there has been an ongoing effort to build systems that can automatically generate required pattern sets as well as the extraction relevant lexicon. Jones and Riloff (JR) (1999) describe a bootstrapping approach to the problem of IE pattern extension. They use a small seed lexicon and pattern set, to iteratively generate new patterns and expand their lexicon until they achieve an optimized set of patterns and lexicon. In the area of lexicon acquisition, many researchers have employed public knowledge bases such as WordNet in IE systems. Bagga et. al. (1997) and later Harabagiu and Maiorano (HM) (2000) investigated the acquisition of the lexical concept space using WordNet and have applied their methods to the Information Extraction task. In this paper, we describe work that blends the semantic labeling approach exemplified by the GJ effort and the bootstrapping approach of JR and HM. Our work differs from the previous efforts in the following respects. 1) We used FrameNet annotations as seeds both for patterns and for the extraction lexicon. We expand the seed lexicon using WordNet. 2) We built a graphical model for the semantic extraction task, which allows us to integrate automatic frame assignment as part of the extraction. 3) We employed IE methods (including pattern sets and Named Entity Recognition) as initial extraction steps. 
A pseudoword is a composite comprised of two or more words chosen at random; the individual occurrences of the original words within a text are replaced by their conﬂation. Pseudowords are a useful mechanism for evaluating the impact of word sense ambiguity in many NLP applications. However, the standard method for constructing pseudowords has some drawbacks. Because the constituent words are chosen at random, the word contexts that surround pseudowords do not necessarily reﬂect the contexts that real ambiguous words occur in. This in turn leads to an optimistic upper bound on algorithm performance. To address these drawbacks, we propose the use of lexical categories to create more realistic pseudowords, and evaluate the results of different variations of this idea against the standard approach. 
References included in multi-document summaries are often problematic. In this paper, we present a corpus study performed to derive a statistical model for the syntactic realization of referential expressions. The interpretation of the probabilistic data helps us gain insight on how extractive summaries can be rewritten in an efﬁcient manner to produce more ﬂuent and easy-to-read text. 
A novel bootstrapping approach to Named Entity （NE）tagging using concept-based seeds and successive learners is presented. This approach only requires a few common noun or pronoun seeds that correspond to the concept for the targeted NE, e.g. he/she/man/woman for PERSON NE. The bootstrapping procedure is implemented as training two successive learners. First, decision list is used to learn the parsing-based NE rules. Then, a Hidden Markov Model is trained on a corpus automatically tagged by the first learner. The resulting NE system approaches supervised NE performance for some NE types. 
This paper describes an effort to rapidly develop language resources and component technology to support searching Cebuano news stories using English queries. Results from the ﬁrst 60 hours of the exercise are presented. 
This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of-Speech (PoS) tags to novel text. This task is particularly challenging for non-standard corpora, such as Internet lingo, where a large proportion of words are unknown. Previous work reveals that PoS tags depend on a variety of morphological and contextual features. Representing these dependencies in a DBN results into an elegant and effective PoS tagger. 
We investigate the optimal LM treatment of abundant ﬁlled pauses (FP) in spontaneous monologues of a professional dictation task. Questions addressed here are (1) how to deal with FP in the LM history and (2) to which extent can the LM distinguish between positions with high and low FP likelihood. Our results differ partly from observations reported on dialogues. Discarding FP from all LM histories clearly improves the performance. Local perplexities, entropies and word rankings at positions following FP suggest that most FP indicate hesitations rather than restarts. Proper prediction of FP allows to distinguish FP from word positions by a doubled FP probability. Recognition experiments conﬁrm the improvements found in our perplexity studies. 
In this paper we investigate the use of surface text patterns for a Maximum Entropy based Question Answering (QA) system. These text patterns are collected automatically in an unsupervised fashion using a collection of trivia question and answer pairs as seeds. These patterns are used to generate features for a statistical question answering system. We report our results on the TREC-10 question set. 
We present CarmelTC, a novel hybrid text classiﬁcation approach for automatic essay grading. Our evaluation demonstrates that the hybrid CarmelTC approach outperforms two “bag of words” approaches, namely LSA and a Naive Bayes, as well as a purely symbolic approach. 
In this paper, a multi-stream paradigm is proposed to improve the performance of automatic speech recognition (ASR) systems in the presence of highly interfering car noise. It was found that combining the classical MFCCs with some auditory-based acoustic distinctive cues and the main formant frequencies of a speech signal using a multi-stream paradigm leads to an improvement in the recognition performance in noisy car environments. 
This paper presents our experiments in applying Latent Semantic Analysis (LSA) to dialogue act classiﬁcation. We employ both LSA proper and LSA augmented in two ways. We report results on DIAG, our own corpus of tutoring dialogues, and on the CallHome Spanish corpus. Our work has the theoretical goal of assessing whether LSA, an approach based only on raw text, can be improved by using additional features of the text. 
We report here empirical results of a series of studies aimed at automatically predicting information quality in news documents. Multiple research methods and data analysis techniques enabled a good level of machine prediction of information quality. Procedures regarding user experiments and statistical analysis are described. 
 (2a) Remove the groceries from the bag.  We report on our work to automatically build a corpus of instructional text annotated with lexical semantics information. We have coupled the parser LCFLEX with a lexicon and ontology derived from two lexical resources, VerbNet for verbs and CoreLex for nouns. We discuss how we built our lexicon and ontology, and the parsing results we obtained. 
Homograph ambiguity is an original issue in Text-to-Speech (TTS). To disambiguate homograph, several efficient approaches have been proposed such as part-of-speech (POS) n-gram, Bayesian classifier, decision tree, and Bayesian-hybrid approaches. These methods need words or/and POS tags surrounding the question homographs in disambiguation. Some languages such as Thai, Chinese, and Japanese have no word-boundary delimiter. Therefore before solving homograph ambiguity, we need to identify word boundaries. In this paper, we propose a unique framework that solves both word segmentation and homograph ambiguity problems altogether. Our model employs both local and longdistance contexts, which are automatically extracted by a machine learning technique called Winnow. 
In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length.  
This paper describes an initial evaluation of systems that answer questions seeking deﬁnitions. The results suggest that humans agree sufﬁciently as to what the basic concepts that should be included in the deﬁnition of a particular subject are to permit the computation of concept recall. Computing concept precision is more problematic, however. Using the length in characters of a deﬁnition is a crude approximation to concept precision that is nonetheless sufﬁcient to correlate with humans’ subjective assessment of deﬁnition quality. The TREC question answering track has sponsored a series of evaluations of systems’ abilities to answer closed class questions in many domains (Voorhees, 2001). Closed class questions are fact-based, short answer questions. The evaluation of QA systems for closed class questions is relatively simple because a response to such a question can be meaningfully judged on a binary scale of right/wrong. Increasing the complexity of the question type even slightly signiﬁcantly increases the difﬁculty of the evaluation because partial credit for responses must then be accommodated. The ARDA AQUAINT1 program is a research initiative sponsored by the U.S. Department of Defense aimed at increasing the kinds and difﬁculty of the questions automatic systems can answer. A series of pilot evaluations has been planned as part of the research agenda of the AQUAINT program. The purpose of each pilot is to develop an effective evaluation methodology for systems that answer a certain kind of question. One of the ﬁrst pilots to be implemented was the Deﬁnitions Pilot, a pilot to develop an evaluation methodology for questions such as What is mold? and Who is Colin Powell?. 1See http:///www.ic-arda.org/InfoExploit/ aquaint/index.html.  This paper presents the results of the pilot evaluation. The pilot demonstrated that human assessors generally agree on the concepts that should appear in the deﬁnition for a particular subject, and can ﬁnd those concepts in the systems’ responses. Such judgments support the computation of concept recall, but do not support concept precision since it is not feasible to enumerate all concepts contained within a system response. Instead, the length of a response is used to approximate concept precision. An F-measure score combining concept recall and length is used as the ﬁnal metric for a response. Systems ranked by average F score correlate well with assessors’ subjective opinions as to deﬁnition quality. 
This paper describes a method for utterance classiﬁcation that does not require manual transcription of training data. The method combines domain independent acoustic models with off-the-shelf classiﬁers to give utterance classiﬁcation performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription. In our method, unsupervised training is ﬁrst used to train a phone n-gram model for a particular domain; the output of recognition with this model is then passed to a phone-string classiﬁer. The classiﬁcation accuracy of the method is evaluated on three different spoken language system domains. 
Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering. A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking. However, there are some cases where segmentation granularity contradicts the results of morphological analysis and the building units of NEs, so that extraction of some NEs are inherently impossible in this setting. To cope with the unit problem, we propose a character-based chunking method. Firstly, the input sentence is analyzed redundantly by a statistical morphological analyzer to produce multiple (n-best) answers. Then, each character is annotated with its character types and its possible POS tags of the top n-best answers. Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs. This method introduces richer information to the chunker than previous methods that base on a single morphological analysis result. We apply our method to IREX NE extraction task. The cross validation result of the F-measure being 87.2 shows the superiority and effectiveness of the method. 
We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difﬁcult than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems. 
Motivated by the success of ensemble methods in machine learning and other areas of natural language processing, we developed a multistrategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora. The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques. We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels. Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric. 
We examine clarification dialogue, a mechanism for refining user questions with follow-up questions, in the context of open domain Question Answering systems. We develop an algorithm for clarification dialogue recognition through the analysis of collected data on clarification dialogues and examine the importance of clarification dialogue recognition for question answering. The algorithm is evaluated and shown to successfully recognize the occurrence of clarification dialogue in the majority of cases and to simplify the task of answer retrieval. 
Previous work on minimizing weighted ﬁnite-state automata (including transducers) is limited to particular types of weights. We present efﬁcient new minimization algorithms that apply much more generally, while being simpler and about as fast. We also point out theoretical limits on minimization algorithms. We characterize the kind of “well-behaved” weight semirings where our methods work. Outside these semirings, minimization is not well-deﬁned (in the sense of producing a unique minimal automaton), and even ﬁnding the minimum number of states is in general NP-complete and inapproximable. 
 We present improvements to a greedy decod-  ing algorithm for statistical machine translation  that red¢uc¡¤£¦e ¥¨i§ts time complexity from at least  cubic (  when applied na¨ıvely) to prac-  tically linear time1 without sacriﬁcing trans-  lation quality. We achieve this by integrat-  ing hypothesis evaluation into hypothesis cre-  ation, tiling improvements over the translation  hypothesis at the end of each search iteration,  and by imposing restrictions on the amount of  word reordering during decoding.  
The discovery of semantic relations from text becomes increasingly important for applications such as Question Answering, Information Extraction, Text Summarization, Text Understanding, and others. The semantic relations are detected by checking selectional constraints. This paper presents a method and its results for learning semantic constraints to detect part-whole relations. Twenty constraints were found. Their validity was tested on a 10,000 sentence corpus, and the targeted partwhole relations were detected with an accuracy of 83%. 
In this paper we present ONTOSCORE, a system for scoring sets of concepts on the basis of an ontology. We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence. We conducted an annotation experiment and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses. An evaluation of our system against the annotated data shows that, it successfully classiﬁes 73.2% in a German corpus of 2.284 SRHs as either coherent or incoherent (given a baseline of 54.55%). 
We describe our approach to the construction and evaluation of a large-scale database called “CatVar” which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications, our categorial-variation resource may serve as an integral part of a diverse range of natural language applications. Thus, the research reported herein overlaps heavily with that of the machine-translation, lexicon-construction, and information-retrieval communities. We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold standard. This evaluation reveals that the categorial database achieves a high degree of precision and recall. Additionally, we demonstrate that the database improves on the linkability of Porter stemmer by over 30%. 
We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser. The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge. Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions. 
Manually constructing an inventory of word senses has suffered from problems including high cost, arbitrary assignment of meaning to words, and mismatch to domains. To overcome these problems, we propose a method to assign word meaning from a bilingual comparable corpus and a bilingual dictionary. It clusters second-language translation equivalents of a first-language target word on the basis of their translingually aligned distribution patterns. Thus it produces a hierarchy of corpus-relevant meanings of the target word, each of which is defined with a set of translation equivalents. The effectiveness of the method has been demonstrated through an experiment using a comparable corpus consisting of Wall Street Journal and Nihon Keizai Shimbun corpora together with the EDR bilingual dictionary. 
We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to ﬁnd a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efﬁcient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-ﬁrst and ﬁnite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to ﬁnd the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-ﬁrst parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time. 
We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.  method to extract phrase translation pairs? In order to investigate this question, we created a uniform evaluation framework that enables the comparison of different ways to build a phrase translation table. Our experiments show that high levels of performance can be achieved with fairly simple means. In fact, for most of the steps necessary to build a phrase-based system, tools and resources are freely available for researchers in the ﬁeld. More sophisticated approaches that make use of syntax do not lead to better performance. In fact, imposing syntactic restrictions on phrases, as used in recently proposed syntax-based translation models [Yamada and Knight, 2001], proves to be harmful. Our experiments also show, that small phrases of up to three words are sufﬁcient for obtaining high levels of accuracy. Performance differs widely depending on the methods used to build the phrase translation table. We found extraction heuristics based on word alignments to be better than a more principled phrase-based alignment method. However, what constitutes the best heuristic differs from language pair to language pair and varies with the size of the training corpus.  
In this paper, we introduce a generative probabilistic optical character recognition (OCR) model that describes an end-to-end process in the noisy channel framework, progressing from generation of true text through its transformation into the noisy output of an OCR system. The model is designed for use in error correction, with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks. We present an implementation of the model based on ﬁnitestate models, demonstrate the model’s ability to signiﬁcantly reduce character and word error rate, and provide evaluation results involving automatic extraction of translation lexicons from printed text. 
We present a derivation of the alignment template model for statistical machine translation and an implementation of the model using weighted ﬁnite state transducers. The approach we describe allows us to implement each constituent distribution of the model as a weighted ﬁnite state transducer or acceptor. We show that bitext word alignment and translation under the model can be performed with standard FSM operations involving these transducers. One of the beneﬁts of using this framework is that it obviates the need to develop specialized search procedures, even for the generation of lattices or N-Best lists of bitext word alignments and translation hypotheses. We evaluate the implementation of the model on the Frenchto-English Hansards task and report alignment and translation performance. 
Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results. 
Recent TREC results have demonstrated the need for deeper text understanding methods. This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system. The approach is to transform questions and answer passages into logic representations. World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text. Moreover, the trace of the proofs provide answer justiﬁcations. The results show that the prover boosts the performance of the QA system on TREC questions by 30%. 
We investigate single-view algorithms as an alternative to multi-view algorithms for weakly supervised learning for natural language processing tasks without a natural feature split. In particular, we apply co-training, self-training, and EM to one such task and ﬁnd that both selftraining and FS-EM, a new variation of EM that incorporates feature selection, outperform cotraining and are comparatively less sensitive to parameter changes. 
We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. 
We present a simple method for language independent and task independent text categorization learning, based on character-level n-gram language models. Our approach uses simple information theoretic principles and achieves effective performance across a variety of languages and tasks without requiring feature selection or extensive pre-processing. To demonstrate the language and task independence of the proposed technique, we present experimental results on several languages—Greek, English, Chinese and Japanese—in several text categorization problems—language identiﬁcation, authorship attribution, text genre classiﬁcation, and topic detection. Our experimental results show that the simple approach achieves state of the art performance in each case. 
We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation. Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection. Furthermore, we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems. An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings. Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator. 
This paper investigates adapting a lexicalized probabilistic context-free grammar (PCFG) to a novel domain, using maximum a posteriori (MAP) estimation. The MAP framework is general enough to include some previous model adaptation approaches, such as corpus mixing in Gildea (2001), for example. Other approaches falling within this framework are more effective. In contrast to the results in Gildea (2001), we show F-measure parsing accuracy gains of as much as 2.5% for high accuracy lexicalized parsing through the use of out-of-domain treebanks, with the largest gains when the amount of indomain data is small. MAP adaptation can also be based on either supervised or unsupervised adaptation data. Even when no in-domain treebank is available, unsupervised techniques provide a substantial accuracy gain over unadapted grammars, as much as nearly 5% F-measure improvement. 
Conditional random ﬁelds for sequence labeling offer advantages over both generative models like HMMs and classiﬁers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random ﬁeld to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that conﬁrm and strengthen previous results on shallow parsing and training methods for maximum-entropy models. 
Automatic restoration of punctuation from unpunctuated text has application in improving the ﬂuency and applicability of speech recognition systems. We explore the possibility that syntactic information can be used to improve the performance of an HMM-based system for restoring punctuation (speciﬁcally, commas) in text. Our best methods reduce sentence error rate substantially — by some 20%, with an additional 8% reduction possible given improvements in extraction of the requisite syntactic information. 
We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance. 
This paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data. We consider both a mostly-unsupervised approach, co-training, in which two parsers are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 
Statistical measures of word similarity have application in many areas of natural language processing, such as language modeling and information retrieval. We report a comparative study of two methods for estimating word cooccurrence frequencies required by word similarity measures. Our frequency estimates are generated from a terabyte-sized corpus of Web data, and we study the impact of corpus size on the effectiveness of the measures. We base the evaluation on one TOEFL question set and two practice questions sets, each consisting of a number of multiple choice questions seeking the best synonym for a given target word. For two question sets, a context for the target word is provided, and we examine a number of word similarity measures that exploit this context. Our best combination of similarity measure and frequency estimation method answers 6-8% more questions than the best results previously reported for the same question sets. 
We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result. 
Evaluating competing technologies on a common problem set is a powerful way to improve the state of the art and hasten technology transfer. Yet poorly designed evaluations can waste research effort or even mislead researchers with faulty conclusions. Thus it is important to examine the quality of a new evaluation task to establish its reliability. This paper provides an example of one such assessment by analyzing the task within the TREC 2002 question answering track. The analysis demonstrates that comparative results from the new task are stable, and empirically estimates the size of the difference required between scores to conﬁdently conclude that two runs are different. Metric-based evaluations of human language technology such as MUC and TREC and DUC continue to proliferate (Sparck Jones, 2001). This proliferation is not difﬁcult to understand: evaluations can forge communities, accelerate technology transfer, and advance the state of the art. Yet evaluations are not without their costs. In addition to the ﬁnancial resources required to support the evaluation, there are also the costs of researcher time and focus. Since a poorly deﬁned evaluation task wastes research effort, it is important to examine the validity of an evaluation task. In this paper, we assess the quality of the new question answering task that was the focus of the TREC 2002 question answering track. TREC is a workshop series designed to encourage research on text retrieval for realistic applications by providing large test collections, uniform scoring procedures, and a forum for organizations interested in comparing results. The conference has focused primarily on the traditional information retrieval problem of retrieving a ranked list of documents in response to a statement of information need, but also includes other tasks, called  tracks, that focus on new areas or particularly difﬁcult aspects of information retrieval. A question answering (QA) track was started in TREC in 1999 (TREC-8) to address the problem of returning answers, rather than document lists, in response to a question. The task for each of the ﬁrst three years of the QA track was essentially the same. Participants received a large corpus of newswire documents and a set of factoid questions such as How many calories are in a Big Mac? and Who invented the paper clip?. Systems were required to return a ranked list of up to ﬁve [document-id, answerstring] pairs per question such that each answer string was believed to contain an answer to the question. Human assessors read each string and decided whether the string actually did contain an answer to the question. An individual question received a score equal to the reciprocal of the rank at which the ﬁrst correct response was returned, or zero if none of the ﬁve responses contained a correct answer. The score for a submission was then the mean of the individual questions’ reciprocal ranks. Analysis of the TREC-8 track conﬁrmed the reliability of this evaluation task (Voorhees and Tice, 2000): the assessors understood and could do their assessing job; relative scores between systems were stable despite differences of opinion by assessors; and intuitively better systems received better scores. The task for the TREC 2002 QA track changed signiﬁcantly from the previous years’ task, and thus a new assessment of the track is needed. This paper provides that assessment by examining both the ability of the human assessors to make the required judgments and the effect that differences in assessor opinions have on comparative results, plus empirically establishing conﬁdence intervals for the reliability of a comparison as a function of the difference in effectiveness scores. The ﬁrst section deﬁnes the 2002 QA task and provides a brief summary of the system results. The following three sections look at each of the evaluation issues in turn. The ﬁnal sec-  tion summarizes the ﬁndings, and outlines shortcomings of the evaluation that remain to be addressed. 
We propose a gold standard for evaluating two types of information extraction output -- noun phrase (NP) chunks (Abney 1991; Ramshaw and Marcus 1995) and technical terms (Justeson and Katz 1995; Daille 2000; Jacquemin 2002). The gold standard is built around the notion that since different semantic and syntactic variants of terms are arguably correct, a fully satisfactory assessment of the quality of the output must include task-based evaluation. We conducted an experiment that assessed subjects’ choice of index terms in an information access task. Subjects showed significant preference for index terms that are longer, as measured by number of words, and more complex, as measured by number of prepositions. These terms, which were identified by a human indexer, serve as the gold standard. The experimental protocol is a reliable and rigorous method for evaluating the quality of a set of terms. An important advantage of this task-based evaluation is that a set of index terms which is different than the gold standard can ‘win’ by providing better information access than the gold standard itself does. And although the individual human subject experiments are time consuming, the experimental interface, test materials and data analysis programs are completely re-usable. 
This paper describes an unsupervised algorithm for placing unknown words into a taxonomy and evaluates its accuracy on a large and varied sample of words. The algorithm works by ﬁrst using a large corpus to ﬁnd semantic neighbors of the unknown word, which we accomplish by combining latent semantic analysis with part-of-speech information. We then place the unknown word in the part of the taxonomy where these neighbors are most concentrated, using a class-labelling algorithm developed especially for this task. This method is used to reconstruct parts of the existing WordNet database, obtaining results for common nouns, proper nouns and verbs. We evaluate the contribution made by part-of-speech tagging and show that automatic ﬁltering using the class-labelling algorithm gives a fourfold improvement in accuracy. 
A serious bottleneck in the development of trainable text summarization systems is the shortage of training data. Constructing such data is a very tedious task, especially because there are in general many different correct ways to summarize a text. Fortunately we can utilize the Internet as a source of suitable training data. In this paper, we present a summarization system that uses the web as the source of training data. The procedure involves structuring the articles downloaded from various websites, building adequate corpora of (summary, text) and (extract, text) pairs, training on positive and negative data, and automatically learning to perform the task of extraction-based summarization at a level comparable to the best DUC systems. 
• Our approach differs from Roche’s and Abney’s in that it is based on the dependency grammar approach and at the output produces an encoding of the dependency structure of a sentence. The lexical items and the dependency relations are encoded in an intertwined manner and manipulated by grammar rules, as well as structural and linguistic ∗ Faculty of Engineering and Natural Sciences, Sabancı University, Orhanlı, 34956, Tuzla, Istanbul, Turkey. E-mail: oﬂazer@sabanciuniv.edu. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 4  constraints implemented as ﬁnite-state ﬁlters, to arrive at parses. The output of the parser is a ﬁnite-state transducer that compactly packs all the ambiguities as a lattice. • As our approach is an all-parses approach with no statistical component, we have used Lin’s (1995) proposal for ranking the parses based on the total link length and have obtained promising results. For over 48% of the sentences, the correct parse was among the dependency trees with the smallest total link length. • Our approach can employ violable constraints for robust parsing so that when the parser fails to link all dependents to a head, one can use lenient ﬁltering to allow parses with a small number of unlinked dependents to be output. • The rules for linking dependents to heads can specify constraints on the intervening material between them, so that, for instance, certain links may be prevented from crossing barriers such as punctuation or lexical items with certain parts of speech or morphological properties (Collins 1996; Giguet and Vergne 1997; Tapanainen and Ja¨rvinen 1997). We summarize in Figure 1 the basic idea of our approach. This ﬁgure presents in a rather high-level fashion, for a Turkish and an English sentence, the input and output representation for the approach to be presented. For the purposes of this summary, we assume that none of the words in the sentences have any morphological ambiguity and that their morphological properties are essentially obvious from the glosses. We represent the input to the parser as a string of symbols encoding the words with some additional delimiter markers. Panel (a) of Figure 1 shows this input representation for a Turkish sentence, on the top right, and panel (b) shows it for an English sentence. The parser operates in iterations. In the ﬁrst iteration, the parser takes the input string encoding the sentence and manipulates it to produce the intermediate string in which we have three dependency relations encoded by additional symbols (highlighted with boldface type) injected into the string. The partial dependency trees encoded are depicted to the left of the intermediate strings. It should be noted that the sets of dependency relations captured in the ﬁrst iteration are different for Turkish and English. In the Turkish sentence, two determiner links and one object link are encoded in parallel, whereas in the English sentence, two determiner links and one subject link are encoded in parallel. The common property of these links is that they do not “interfere” with each other. The second iteration of the parser takes the output of the ﬁrst iteration and manipulates it to produce a slightly longer string in which symbols encoding a new subject (object) link are injected into the Turkish (English) string. (We again highlight these symbols with boldface type.) Note that in the English string the relative positions of the link start and end symbols indicate that this is a right-to-left link. The dependency structures encoded by these strings are again on their left. After the second iteration, there are no further links that can be added, since in each case there is only one word left without any outgoing links and it happens to be the head of the sentence. The article is structured as follows: After a brief overview of related work, we summarize dependency grammars and aspects of Turkish relevant to this work. We provide a summary of concepts from ﬁnite-state transducers so that subsequent sections can be self-contained. We continue by describing the representation that we have employed for encoding dependency structures, along with the encoding of dependency linking rules operating on these representations and conﬁgurational constraints  516  Oflazer  Dependency Parsing  Figure 1 Dependency parsing by means of iterative manipulations of strings encoding dependency structures. 517  Computational Linguistics  Volume 29, Number 4  for ﬁltering them. We then describe the parser and its operational aspects, with details on how linguistically motivated constraints for further ﬁltering are implemented. We brieﬂy provide a scheme for a robust-parsing extension of our approach using the lenient composition operation. We then provide results from a prototype implementation of the parser and its application to dependency parsing of Turkish. We close with remarks and conclusions. 2. Overview of Related Work Although ﬁnite-state methods have been applied to parsing by many researchers, extended ﬁnite-state techniques were initially used only by Roche (1997), Abney (1996), and the FASTUS group (Hobbs et al. 1997). In the context of dependency parsing with ﬁnite-state machines, Elworthy (2000) has recently proposed a ﬁnite-state parser that produces a dependency output. Roche (1997) presents a top-down approach for parsing context-free grammars implemented with ﬁnite-state transducers. The transducers are based on a syntactic dictionary comprising patterns of lexical and nonlexical items. The input is initially bracketed with sentence markers at both ends and then fed into a transducer for bracketing according to bracketing rules for each of the patterns in the dictionary. The output of the transducer is fed back to the input, and the constituent structure is iteratively reﬁned. When the output of the transducer reaches a ﬁxed point, that is, when no additional brackets can be inserted, parsing ends. Abney (1996) presents a ﬁnite-state parsing approach in which a tagged sentence is parsed by transducers that progressively transform the input into sequences of symbols representing phrasal constituents. In this approach, the input sentence is assumed to be tagged with a part-of-speech tagger. The parser consists of a cascade of stages. Each stage is a ﬁnite-state transducer implementing rules that bracket and transduce the input to an output containing a mixture of unconsumed terminal symbols and nonterminal symbols for the phrases recognized. The output of a stage goes to the next stage in the cascade, which further brackets the input using yet other rules. Each cascade typically corresponds to a level in a standard X-bar grammar. After a certain (ﬁxed) number of cascades, the input is fully bracketed, with the structures being indicated by labels on the brackets. Iterations in Roche’s approach roughly correspond to cascades in Abney’s approach. The grammar, however, determines the number of levels or cascades in Abney’s approach: that is, structure is ﬁxed. A work along the lines of Abney’s is that of Kokkinakis and Kokkinakis (1999) for parsing Swedish. Elworthy (2000) presents a ﬁnite-state parser that can produce a dependency structure from the input. The parser utilizes standard phrase structure rules that are annotated with “instructions” that associate the components of the phrases recognized with dependency grammar–motivated relations. A head is annotated with variables associating it with its dependents. These variables are ﬁlled in by the instructions associated with the rules. These variables are copied or percolated “up” the rules according to special instructions. The approach resembles a uniﬁcation-based grammar in which instead of uniﬁcation, dependency relation features are passed from a dependent to its head. The rules for recognizing phrases and implementing their instructions are implemented as ﬁnite-state transducers. Another notable system for ﬁnite-state parsing is FASTUS (Hobbs et al. 1997). FASTUS uses a ﬁve-stage cascaded system, with each stage consisting of nondeterministic ﬁnite-state machines. FASTUS is mainly for information extraction applications. The early stages recognize complex multiword units such as proper names and colloca-  518  Oflazer  Dependency Parsing  tions and build upon these by grouping them into phrases. Later stages are geared toward recognizing event patterns and building event structures. 3. Dependency Syntax Dependency approaches to syntactic representation use the notion of syntactic relation to associate surface lexical items. Melcˇuk (1988) presents a comprehensive exposition of dependency syntax. Computational approaches to dependency syntax have recently become quite popular (e.g., a workshop dedicated to computational approaches to dependency grammars was held at COLING/ACL’98). Ja¨rvinen and Tapanainen (1998; Tapanainen and Ja¨rvinen 1997) have demonstrated an efﬁcient wide-coverage dependency parser for English. The work of Sleator and Temperley (1991) on link grammar, essentially a lexicalized variant of dependency grammar, has also proved to be interesting in regard to a number of aspects. Dependency-based statistical language modeling and parsing have also become quite popular in statistical natural language processing (Lafferty, Sleator, and Temperley 1992; Eisner 1996; Chelba et al. 1997; Collins 1996; Collins et al. 1999). Robinson (1970) gives four axioms for well-formed dependency structures that have been assumed in almost all computational approaches. These state that, in a dependency structure of a sentence, 1. one and only one word is independent, that is, not linked to some other word; 2. all others depend directly on some word; 3. no word depends on more than one other; and 4. if a word A depends directly on word B, and some word C intervenes between them (in linear order), then C depends directly on A or on B, or on some other intervening word. This last condition of projectivity (or various extensions of it; see, e.g., Lai and Huang [1994]) is usually assumed by most computational approaches to dependency grammars as a constraint for ﬁltering conﬁgurations and has also been used as a simplifying condition in statistical approaches for inducing dependencies from corpora (e.g., Yu¨ ret 1998).1 4. Turkish Turkish is an agglutinative language in which a sequence of inﬂectional and derivational morphemes get afﬁxed to a root (Oﬂazer 1993). At the syntax level, the unmarked constituent order is Subject-Object-Verb, but constituent order may vary as demanded by the discourse context. Essentially all constituent orders are possible, especially at the main sentence level, with very minimal formal constraints. In written text, however, the unmarked order is dominant at both the main-sentence and embedded-clause level. Turkish morphophonology is characterized by a number of processes such as vowel harmony (vowels in sufﬁxes, with very minor exceptions, agree with previous  
Matthew Stone† Rutgers University Alistair Knott§ University of Otago  We argue in this article that many common adverbial phrases generally taken to signal a discourse relation between syntactically connected units within discourse structure instead work anaphorically to contribute relational meaning, with only indirect dependence on discourse structure. This allows a simpler discourse structure to provide scaffolding for compositional semantics and reveals multiple ways in which the relational meaning conveyed by adverbial connectives can interact with that associated with discourse structure. We conclude by sketching out a lexicalized grammar for discourse that facilitates discourse interpretation as a product of compositional rules, anaphor resolution, and inference. 1. Introduction It is a truism that a text means more than the sum of its component sentences. One source of additional meaning are relations taken to hold between adjacent sentences “syntactically” connected within a larger discourse structure. It has been very difﬁcult, however, to say what discourse relations there are, either theoretically (Mann and Thompson 1988; Kehler 2002; Asher and Lascarides 2003) or empirically (Knott 1996). Knott’s empirical attempt to identify and characterize cue phrases as evidence for discourse relations illustrates some of the difﬁculties. Knott used the following theory-neutral test to identify cue phrases: For a potential cue phrase φ in naturally occurring text, consider in isolation the clause in which it appears. If the clause appears incomplete without an adjacent left context, whereas it appears complete if φ is removed, then φ is a cue phrase. Knott’s test produced a nonexhaustive list of about two hundred different phrases from 226 pages of text. He then attempted to characterize the discourse relation(s) conveyed by each phrase by identifying when (always, sometimes, never) one phrase could substitute for another in a way that preserved meaning. He showed how these substitution patterns could be a consequence of a set of semantic features and their values. Roughly speaking, one cue phrase could always substitute for another if it had the same set of features and values, sometimes do so if it was less speciﬁc than the other in terms of its feature values, and never do so if their values conﬂicted for one or more features.  ∗ School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail: bonnie@inf.ed.ac.uk. † Department of Computer Science, Rutgers Universtiy, 110 Frelinghuysen Road, Piscataway, NJ 08854-8019. E-mail: mdstone@cs.rutgers.edu. ‡ Department of Computer & Information Science, University of Pennsylvania, 200 South 33rd Street, Philadelphia, PA 19104-6389. E-mail: joshi@linc.cis.upenn.edu. § Department of Computer Science, University of Otago, P.O. Box 56, DUNEDIN 9015, New Zealand. E-mail: alik@cs.otago.ac.nz. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 4  By assuming that cue phrases contribute meaning in a uniform way, Knott was led to a set of surprisingly complex directed acyclic graphs relating cue phrases in terms of features and their values, each graph loosely corresponding to some family of discourse relations. But what if the relational meaning conveyed by cue phrases could in fact interact with discourse meaning in multiple ways? Then Knott’s substitution patterns among cue phrases may have reﬂected these complex interactions, as well as the meanings of individual cue phrases themselves. This article argues that cue phrases do depend on another mechanism for conveying extrasentential meaning—speciﬁcally, anaphora. One early hint that adverbial cue phrases (called here discourse connectives) might be anaphoric can be found in an ACL workshop paper in which Janyce Wiebe (1993) used the following example to question the adequacy of tree structures for discourse: (1) a. The car was ﬁnally coming toward him. b. He [Chee] ﬁnished his diagnostic tests, c. feeling relief. d. But then the car started to turn right. The problem Wiebe noted was that the discourse connectives but and then appear to link clause (1d) to two different things: then to clause (1b) in a sequence relation (i.e., the car’s starting to turn right being the next relevant event after Chee’s ﬁnishing his tests) and but to a grouping of clauses (1a) and (1c) (i.e., reporting a contrast between, on the one hand, Chee’s attitude toward the car coming toward him and his feeling of relief and, on the other hand, his seeing the car turning right). (Wiebe doesn’t give a name to the relation she posits between (1d) and the grouping of (1a) and (1c), but it appears to be some form of contrast.) If these relations are taken to be the basis for discourse structure, some possible discourse structures for this example are given in Figure 1. Such structures might seem advantageous in allowing the semantics of the example to be computed directly by compositional rules and defeasible inference. However, both structures are directed acyclic graphs (DAGs), with acyclicity the only constraint on what nodes can be connected. Viewed syntactically, arbitrary DAGs are completely unconstrained systems. They substantially complicate interpretive rules for discourse, in order for those rules to account for the relative scope of unrelated operators and the contribution of syntactic nodes with arbitrarily many parents.1 We are not committed to trees as the limiting case of discourse structure. For example, we agree, by and large, with the analysis that Bateman (1999) gives of (2) (vi) The ﬁrst to do that were the German jewellers, (vii) in particular Klaus Burie. (viii) And Morris followed very quickly after, (ix) using a lacquetry technique to make the brooch, (x) and using acrylics, (xi) and exploring the use of colour, (xii) and colour is another thing that was new at that time.  
2. How can this choice be instantiated in a sound probabilistic model? In this article we explore these issues within the framework of generative models, more precisely, the history-based models originally introduced to parsing by Black ∗ MIT Computer Science and Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, 545 Technology Square, Cambridge, MA 02139. E-mail: mcollins@ai.mit.edu. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 4  et al. (1992). In a history-based model, a parse tree is represented as a sequence of decisions, the decisions being made in some derivation of the tree. Each decision has an associated probability, and the product of these probabilities deﬁnes a probability distribution over possible derivations. We ﬁrst describe three parsing models based on this approach. The models were originally introduced in Collins (1997); the current article1 gives considerably more detail about the models and discusses them in greater depth. In Model 1 we show one approach that extends methods from probabilistic context-free grammars (PCFGs) to lexicalized grammars. Most importantly, the model has parameters corresponding to dependencies between pairs of headwords. We also show how to incorporate a “distance” measure into these models, by generalizing the model to a history-based approach. The distance measure allows the model to learn a preference for close attachment, or right-branching structures. In Model 2, we extend the parser to make the complement/adjunct distinction, which will be important for most applications using the output from the parser. Model 2 is also extended to have parameters corresponding directly to probability distributions over subcategorization frames for headwords. The new parameters lead to an improvement in accuracy. In Model 3 we give a probabilistic treatment of wh-movement that is loosely based on the analysis of wh-movement in generalized phrase structure grammar (GPSG) (Gazdar et al. 1985). The output of the parser is now enhanced to show trace coindexations in wh-movement cases. The parameters in this model are interesting in that they correspond directly to the probability of propagating GPSG-style slash features through parse trees, potentially allowing the model to learn island constraints. In the three models a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then follow naturally, leading to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. For this reason we refer to the models as head-driven statistical models. We describe evaluation of the three models on the Penn Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Model 1 achieves 87.7% constituent precision and 87.5% consituent recall on sentences of up to 100 words in length in section 23 of the treebank, and Models 2 and 3 give further improvements to 88.3% constituent precision and 88.0% constituent recall. These results are competitive with those of other models that have been applied to parsing the Penn Treebank. Models 2 and 3 produce trees with information about wh-movement or subcategorization. Many NLP applications will need this information to extract predicate-argument structure from parse trees. The rest of the article is structured as follows. Section 2 gives background material on probabilistic context-free grammars and describes how rules can be “lexicalized” through the addition of headwords to parse trees. Section 3 introduces the three probabilistic models. Section 4 describes various reﬁnments to these models. Section 5 discusses issues of parameter estimation, the treatment of unknown words, and also the parsing algorithm. Section 6 gives results evaluating the performance of the models on the Penn Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Section 7 investigates various aspects of the models in more detail. We give a  
 Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information. We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus. The selectional preferences are speciﬁc to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads. We also investigate use of the one-senseper-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage. Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage. This article is aimed at quantifying the disambiguation performance of automatically acquired selectional preferences in regard to nouns, verbs, and adjectives with respect to a standard test corpus and evaluation setup (SENSEVAL-2) and to identify strengths and weaknesses. Although there is clearly a limit to coverage using preferences alone, because preferences are acquired only with respect to speciﬁc grammatical roles, we show that when dealing with running text, rather than isolated examples, coverage can be increased at little cost in accuracy by using the one-sense-per-discourse heuristic. ∗ Department of Informatics, University of Sussex, Brighton BN1 9QH, UK. E-mail: {dianam, johnca}@sussex.ac.uk c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 4  We acquire selectional preferences as probability distributions over the WordNet (Fellbaum 1998) noun hyponym hierarchy. The probability distributions are conditioned on a verb or adjective class and a grammatical relationship. A noun is disambiguated by using the preferences to give probability estimates for each of its senses in WordNet, that is, for WordNet synsets. Verbs and adjectives are disambiguated by using the probability distributions and Bayes’ rule to obtain an estimate of the probability of the adjective or verb class, given the noun and the grammatical relationship. Previously, we evaluated noun and verb disambiguation on the English all-words task in the SENSEVAL-2 exercise (Cotton et al. 2001). We now present results also using preferences for adjectives, again evaluated on the SENSEVAL-2 test corpus (but carried out after the formal evaluation deadline). The results are encouraging, given that this method does not rely for training on any hand-tagged data or frequency distributions derived from such data. Although a modest amount of English sense-tagged data is available, we nevertheless believe it is important to investigate methods that do not require such data, because there will be languages or texts for which sense-tagged data for a given word is not available or relevant. 2. Motivation The goal of this article is to assess the WSD performance of selectional preference models for adjectives, verbs, and nouns on the SENSEVAL-2 test corpus. There are two applications for WSD that we have in mind and are directing our research. The ﬁrst application is text simpliﬁcation, as outlined by Carroll, Minnen, Pearce et al. (1999). One subtask in this application involves substituting words with thier more frequent synonyms, for example, substituting letter for missive. Our motivation for using WSD is to ﬁlter out inappropriate senses of a word token, so that the substituting synonym is appropriate given the context. For example, in the following sentence we would like to use strategy, rather than dodge, as a substitute for scheme:  A recent government study singled out the scheme as an example to others.  We are also investigating the disambiguation of verb senses in running text before subcategorization information for the verbs is acquired, in order to produce a subcategorization lexicon speciﬁc to sense (Preiss and Korhonen 2002). For example, if subcategorization were acquired speciﬁc to sense, rather than verb form, then distinct senses of ﬁre could have different subcategorization entries:  ﬁre(1) - sack: ﬁre(2) - shoot:  NP V NP NP V NP, NP V  Selectional preferences could also then be acquired automatically from sense-tagged data in an iterative approach (McCarthy 2001). 3. Methodology We acquire selectional preferences from automatically preprocessed and parsed text during a training phase. The parser is applied to the test data as well in the runtime phase to identify grammatical relations among nouns, verbs, and adjectives. The acquired selectional preferences are then applied to the noun-verb and noun-adjective pairs in these grammatical constructions for disambiguation.  640  McCarthy and Carroll  Disambiguating Using Selectional Preferences  Training data where you can drink a single glass with... Test data The men drink here ...  Preprocessor  WordNet  Parser Selectional Preference Acquisition  Grammatical Relations ncsubj drink_VV0 you_PPY dobj drink_VV0 glass_NN1 ncmod glass_NN1 single_JJ  Selectional Preferences model for verbclass drink suck sip  <entity> <time> 0.7 0.0005  <measure> <attribute> 0.08 0.09  Disambiguator  Sense Tagged output instance ID WordNet sense tag d06.s13.t03 man%1:18:00:: d04.s12.t09 drink%2:34:00::  Figure 1 System overview. Solid lines indicate ﬂow of data during training, and broken lines show that at run time.  The overall structure of the system is illustrated in Figure 1. We describe the individual components in sections 3.1–3.3 and 4. 3.1 Preprocessing The preprocessor consists of three modules applied in sequence: a tokenizer, a partof-speech (POS) tagger, and a lemmatizer. The tokenizer comprises a small set of manually developed ﬁnite-state rules for identifying word and sentence boundaries. The tagger (Elworthy 1994) uses a bigram hidden Markov model augmented with a statistical unknown word guesser. When applied to the training data for selectional preference acquisition, it produces the single highest-ranked POS tag for each word. In the run-time phase, it returns multiple tag hypotheses, each with an associated forward-backward probability to reduce the impact of tagging errors. The lemmatizer (Minnen, Carroll, and Pearce 2001) reduces inﬂected verbs and nouns to their base forms. It uses a set of ﬁnite-state rules expressing morphological regularities and subregularities, together with a list of exceptions for speciﬁc (irregular) word forms. 3.2 Parsing The parser uses a wide-coverage uniﬁcation-based shallow grammar of English POS tags and punctuation (Briscoe and Carroll 1995) and performs disambiguation using a context-sensitive probabilistic model (Briscoe and Carroll 1993), recovering from extra-grammaticality by returning partial parses. The output of the parser is a set of grammatical relations (Carroll, Briscoe, and Sanﬁlippo 1998) specifying the syntactic dependency between each head and its dependent(s), taken from the phrase structure tree that is returned from the disambiguation phase. For selectional preference acquisition we applied the analysis system to the 90 million words of the written portion of the British National Corpus (BNC); the parser produced complete analyses for around 60% of the sentences and partial analyses for over 95% of the remainder. Both in the acquisition phase and at run time, we extract from the analyser output subject–verb, verb–direct object, and noun–adjective  641  Computational Linguistics  Volume 29, Number 4  modiﬁer dependencies.1 We did not use the SENSEVAL-2 Penn Treebank–style bracketings supplied for the test data.  3.3 Selectional Preference Acquisition The preferences are acquired for grammatical relations (subject, direct object, and adjective–noun) involving nouns and grammatically related adjectives or verbs. We use WordNet synsets to deﬁne our sense inventory. Our method exploits the hyponym links given for nouns (e.g., cheese is a hyponym of food), the troponym links for verbs 2 (e.g., limp is a troponym of walk), and the “similar-to” relationship given for adjectives (e.g., one sense of cheap is similar to ﬂimsy). The preference models are modiﬁcations of the tree cut models (TCMs) originally proposed by Li and Abe (1995, 1998). The main differences between that work and ours are that we acquire adjective as well as verb models, and also that our models are with respect to verb and adjective classes, rather than forms. We acquire models for classes because we are using the models for WSD, whereas Li and Abe used them for structural disambiguation. We deﬁne a TCM as follows. Let NC be the set of noun synsets (noun classes) in WordNet: NC = {nc ∈ WordNet}, and NS be the set of noun senses 3 in Wordnet: NS = {ns ∈ WordNet}. A TCM is a set of noun classes that partition NS disjointly. We use Γ to refer to such a set of classes in a TCM. A TCM is deﬁned by Γ and a probability distribution:  p(nc) = 1  (8)  nc∈Γ  The probability distribution is conditioned by the grammatical context. In this work, the probability distribution associated with a TCM is conditioned on a verb class (vc) and either the subject or direct-object relation, or an adjective class (ac) and the adjective–noun relation. Let VC be the set of verb synsets (verb classes) in WordNet: VC = {vc ∈ WordNet}. Let AC be the set of adjective classes (which subsume WordNet synsets; we elaborate further on this subsequently). Thus, the TCMs deﬁne a probability distribution over NS that is conditioned on a verb class (vc) or adjective class (ac) and a particular grammatical relation (gr):  p(nc|vc, gr) = 1  (9)  nc∈Γ  Acquisition of a TCM for a given vc and gr proceeds as follows. The data for acquiring the preference are obtained from a subset of the tuples involving verbs in the synset or troponym (subordinate) synsets. Not all verbs that are troponyms or direct members of the synset are used in training. We take the noun argument heads occurring with verbs that have no more than 10 senses in WordNet and a frequency of 20 or more occurrences in the BNC data in the speciﬁed grammatical relationship. The threshold of 10 senses removes some highly polysemous verbs having many sense distinctions that are rather subtle. Verbs that have more than 10 senses include very frequent verbs such as be and do that do not select strongly for their  
This will also emerge in the discussion of the book that is the focus of this review. Learning to Classify Texts Using Support Vector Machines by Thorsten Joachims proposes a theory for automatic learning of text categorization models that has been repeatedly shown to be very successful. At the same time, the approach proposed is based on a rather rough linguistic generalization of (what apparently is) a language-dependent task: topic text classiﬁcation (TC). The result is twofold: on the one hand, a learning theory, based on statistical learnability principles and results, that avoids the limitations of the strong empiricism typical of most text classiﬁcation research; and on the other hand, the application of a naive linguistic model, the bag-of-words representation, to linguistic objects (i.e., the documents) that still achieves impressive accuracy. 2. Several Good Reasons for Reading the Book When you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind: it may be the beginning of knowledge, but you have scarcely in your thoughts advanced to the stage of science. —Lord Kelvin 2.1 Statistical Learning Theory Applied to Text Classiﬁcation Joachims’s book presents the application of a statistical learning model, support vector machines (SVMs) (Vapnik 1995), to the problem of text categorization. As the author emphasizes from the beginning of the book, it affects not only theoretical aspects, and not only empirical ﬁndings or implementation ideas, but all these aspects. The book  Computational Linguistics  Volume 29, Number 4  is the results of the author’s Ph.D. thesis, and this is strongly reﬂected in its overall organization. The original contribution of the Ph.D. thesis is the speciﬁc application of SVMs and some theoretical and algorithmic novelty developed for (but not limited to) text categorization. The author divides the book into four parts: “Notation”, “Theory”, “Methods,” and “Algorithms.” “Notation” introduces the problem and surveys existing approaches to automatic text categorization. First, in chapter 2, feature representation methods are discussed, from bag of words to multiwords or “semantically” justiﬁed features. Then, four traditional methods for learning the categorization function—Naive Bayes (Tzeras and Hartmann 1993), Rocchio (Rocchio 1971), k-nearest neighbors (Yang 1994), and decision trees (Quinlan 1986)—are presented in the same chapter. In this chapter (that is, very early in the book) a speciﬁc notion of performance is deﬁned. It is expressed as a function of the categorization error (i.e., the number of mismatches between the system outcome and the gold standard). Notice how this is a bias to the entire matter, as discussed below. Finally, the basic deﬁnitions for SVMs are given (chapter 3). “Theory” proposes a general model for TC that is based on the distributional properties of (bags of) words. This results in an abstract notion of target categorization (TCat) concept the learnability of which via SVMs derives from their formal properties (chapter 4). Moreover, the TCat concept has an inductive nature, as it is based on the availability of a large set of training examples: Under certain assumptions on the training material, the TCats’ results are linearly separable by an SVM with a controlled amount of error (or loss in predictive accuracy). Accordingly, chapter 5 analyzes methods for estimating the predictive accuracy of the target SVM. The task knowledge embedded in the training examples provides complete information about the training error, and formal results from statistical learnability theory (e.g., Vapnik 1998) (or newly introduced by using Vapnik’s results as inspiration) are directly employed here as an upper bound on the testing error (i.e., the error of the generalization achieved as measured over test data). “Methods” discusses the core technique for inducing the TC functions by means of SVMs. This is ﬁrst done in the tradition of inductive approaches to TC: Training examples are used to induce the maximum margin hyperplane that separates positive from negative examples in a binary setting. The empirical evidence—good performance over three well-known benchmarking data sets—conﬁrms the viability of the SVM induction (chapter 6). In chapter 7 the notion of transductive SVMs (Vapnik 1998) is introduced. It is the inductive task that exploits a consistent set of testing examples as a bias for building the maximum margin hyperplane. Such an approach has several analogies with forms of active learning (e.g., co-training [Blum and Mitchell 1998]), wherein evidence during learning is derived from pieces of more or less weak test evidence (e.g., independent feature spaces are used as selective information on how to sample training examples in co-training). Algorithms and concrete methods for the application of the previous results are then reported in the fourth part, “Algorithms.” In chapters 8 and 9, efﬁcient algorithms for training inductive and transductive SVMs are ﬁnally presented, with reference to the software platform SVMLight, another (nonsecondary) side effect of the author’s Ph.D. thesis. 2.2 Empirically Grounding a Powerful Theory The book has great merit, as much space is given not only to theoretical and experimental aspects, but to an attempt to empirically assess support vector machines as a general learning theory for TC. Grounding formal results on large-scale data is always  656  Book Reviews attempted where possible. It is carried out over the target benchmarking corpora: a collection of Reuters news (Reuters-21578), a collection of medical texts (OHSUMED), and a set of manually classiﬁed Web pages (the WebKB collection).1 This evidence is also used to deﬁne an abstract model of what a target text categorization concept is and how it is learnable by a SVM. The notion of TCat concept tries to capture exactly this. These two aspects are very important, as the reader can better understand the large set of (often mathematically complex) theoretical notions against empirical data and also validate progressively the results of the theory against the evidence derived from real collections. For example, interesting sections (especially for a computational linguistics researcher) are those in which mathematical notions, such as linear separability, training error, and the TCat concept itself, are discussed via estimation against the benchmarking data (e.g., section 3 of chapter 4). Although test data cannot be considered exhaustive samples, TC benchmarks are a rather precious source of information about the distributional and linguistic properties of words. Notice that such combined theoretical and empirical analyses are rare in the literature. The result is an attempt to reconcile IR (often too much focused on empirical performance measures) and AI (more often targeted to theories of learning with weaker possibilities for large-scale empirical assessment). Finally, as large-scale data analysis is required in the study of several NLP tasks (and not only TC), the book is a good example for researchers and practitioners in empirical language processing. 2.3 Theory, Application, and Implementation of Support Vector Machines Covering aspects related not just to methods, but also to theory and efﬁcient implementation of SVMs is a positive aspect of the book. The book helps in building a rather rich picture of the ﬁeld. Introductory matter is presented in a comprehensive and theoretically well-founded way. Then the learning theory based on earlier results from Vapnik (1995) is described, and this has a valuable effect on the methodological contribution. All the aspects of the application of SVMs to TC are thus framed in a larger picture. Examples are the discussion of benchmarking data in section 4 of chapter 5, in which theoretical estimators used to upper-bound the categorization error are compared to their measures as carried out over the benchmarking collections. Almost every speciﬁc parameter of the estimators that raises questions about the quality and applicability of the theory (e.g., dimension of the training set, the error embedded in training material) is analyzed comparatively against the benchmarking data. The analytical estimates are thus compared to the effective measures. This is very relevant for anyone interested in empirical analysis of linguistic data. 2.4 Expressiveness versus Efﬁciency One of the contributions of Joachims’s thesis is the effort spent in making SVM learning for text classiﬁcation feasible. In SVMs, the induction is based on the solution of a quadratic optimization problem in which the size of the matrix is quadratic in the number of available training examples and the different values depend on the choice of the kernel functions.2 The problems here are related to the iterative evaluation of 
 The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground. This special issue of Computational Linguistics explores ways in which this dream is being explored. 1. Introduction The Web is immense, free, and available by mouse click. It contains hundreds of billions of words of text and can be used for all manner of language research. The simplest language use is spell checking. Is it speculater or speculator? Google gives 67 for the former (usefully suggesting the latter might have been intended) and 82,000 for the latter. Question answered. Language scientists and technologists are increasingly turning to the Web as a source of language data, because it is so big, because it is the only available source for the type of language in which they are interested, or simply because it is free and instantly available. The mode of work has increased dramatically from a standing start seven years ago with the Web being used as a data source in a wide range of research activities: The papers in this special issue form a sample of the best of it. This introduction to the issue aims to survey the activities and explore recurring themes. We ﬁrst consider whether the Web is indeed a corpus, then present a history of the theme in which we view the Web as a development of the empiricist turn that has brought corpora center stage in the course of the 1990s. We brieﬂy survey the range of Web-based NLP research, then present estimates of the size of the Web, for English and for other languages, and a simple method for translating phrases. Next we open the Pandora’s box of representativeness (concluding that the Web is not representative of anything other than itself, but then neither are other corpora, and that more work needs to be done on text types). We then introduce the articles in the special issue and conclude with some thoughts on how the Web could be put at the linguist’s disposal rather more usefully than current search engines allow. 1.1 Is the Web a Corpus? To establish whether the Web is a corpus we need to ﬁnd out, discover, or decide what a corpus is. McEnery and Wilson (1996, page 21) say In principle, any collection of more than one text can be called a corpus. . . . But the term “corpus” when used in the context of modern linguistics tends most frequently to have more speciﬁc connotations than this simple deﬁnition provides for. These may be considered un-  ∗ Lewes Rd, Brighton, BN2 4JG, UK. E-mail: Adam.Kilgarriff@itri.brighton.ac.uk † Suite 700, 5001 Baum Blvd, Pittsburgh, PA 15213-1854. E-mail: grefen@clairvoyancecorp.com  c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 3  der four main headings: sampling and representativeness, ﬁnite size, machine-readable form, a standard reference. We would like to reclaim the term from the connotations. Many of the collections of texts that people use and refer to as their corpus, in a given linguistic, literary, or language-technology study, do not ﬁt. A corpus comprising the complete published works of Jane Austen is not a sample, nor is it representative of anything else. Closer to home, Manning and Schu¨ tze (1999, page 120) observe: In Statistical NLP, one commonly receives as a corpus a certain amount of data from a certain domain of interest, without having any say in how it is constructed. In such cases, having more training data is normally more useful than any concerns of balance, and one should simply use all the text that is available. We wish to avoid a smuggling of values into the criterion for corpus-hood. McEnery and Wilson (following others before them) mix the question “What is a corpus?” with “What is a good corpus (for certain kinds of linguistic study)?” muddying the simple question “Is corpus x good for task y?” with the semantic question “Is x a corpus at all?” The semantic question then becomes a distraction, all too likely to absorb energies that would otherwise be addressed to the practical one. So that the semantic question may be set aside, the deﬁnition of corpus should be broad. We deﬁne a corpus simply as “a collection of texts.” If that seems too broad, the one qualiﬁcation we allow relates to the domains and contexts in which the word is used rather than its denotation: A corpus is a collection of texts when considered as an object of language or literary study. The answer to the question “Is the web a corpus?” is yes. 2. History For chemistry or biology, the computer is merely a place to store and process information gleaned about the object of study. For linguistics, the object of study itself (in one of its two primary forms, the other being acoustic) is found on computers. Text is an information object, and a computer’s hard disk is as valid a place to go for its realization as the printed page or anywhere else. The one-million-word Brown corpus opened the chapter on computer-based language study in the early 1960s. Noting the singular needs of lexicography for big data, in the 1970s Sinclair and Atkins inaugurated the COBUILD project, which raised the threshold of viable corpus size from one million to, by the early 1980s, eight million words (Sinclair 1987). Ten years on, Atkins again took the lead with the development (from 1988) of the British National Corpus (BNC) (Burnard 1995), which raised horizons tenfold once again, with its 100 million words and was in addition widely available at low cost and covered a wide spectrum of varieties of contemporary British English.1 As in all matters Zipﬁan, logarithmic graph paper is required. Where corpus size is concerned, the steps of interest are 1, 10, 100, . . . , not 1, 2, 3, . . . Corpora crashed into computational linguistics at the 1989 ACL meeting in Vancouver, but they were large, messy, ugly objects clearly lacking in theoretical integrity in all sorts of ways, and many people were skeptical regarding their role in the discipline. Arguments raged, and it was not clear whether corpus work was an acceptable  
 Parallel corpora have become an essential resource for work in multilingual natural language processing. In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web, ﬁrst reviewing the original algorithm and results and then presenting a set of signiﬁcant enhancements. These enhancements include the use of supervised learning based on structural features of documents to improve classiﬁcation performance, a new contentbased measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a signiﬁcant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the second language, using word-level alignments as the bridge, and then use robust statistical techniques in learning from the resulting noisy annotations (Cabezas, Dorr, and Resnik 2001; Diab and Resnik 2002; Hwa et al. 2002; Lopez et al. 2002; Yarowsky, Ngai, and Wicentowski 2001; Yarowsky and Ngai 2001; Riloff, Schafer, and Yarowsky 2002). For these reasons, parallel corpora can be thought of as a critical resource. Unfortunately, they are not readily available in the necessary quantities. Until very recently, for example, statistical work in machine translation focused heavily on French-English translation because the Canadian parliamentary proceedings (Hansards) in English and French were the only large bitext available. Things have improved somewhat, but it is still fair to say that for all but a relatively few language pairs, parallel corpora tend to be accessible only in specialized forms such as United Nations proceedings (e.g., via the Linguistic Data Consortium, http://www.ldc.upenn.edu ), religious texts (Resnik, Olsen, and Diab 1999), localized versions of software manuals (Resnik and ∗ Department of Linguistics and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742. E-mail: resnik@umd.edu † Department of Computer Science and Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD 21218. E-mail: nasmith@cs.jhu.edu c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 3  Melamed 1997; Menezes and Richardson 2001), and the like. Even for the top handful of majority languages, the available parallel corpora tend to be unbalanced, representing primarily governmental or newswire-style texts. In addition, like other language resources, parallel corpora are often encumbered by fees or licensing restrictions. For all these reasons, it is difﬁcult to follow the “more data are better data” advice of Church and Mercer (1993), abandoning balance in favor of volume, with respect to parallel text. Then there is the World Wide Web. People tend to see the Web as a reﬂection of their own way of viewing the world—as a huge semantic network, or an enormous historical archive, or a grand social experiment. We are no different: As computational linguists working on multilingual issues, we view the Web as a great big body of text waiting to be mined, a huge fabric of linguistic data often interwoven with parallel threads. This article describes our techniques for mining the Web in order to extract the parallel text it contains. It presents, in revised and considerably extended form, our early work on mining the Web for bilingual text (STRAND) (Resnik 1998, 1999), incorporating new work on content-based detection of translations (Smith 2001, 2002), and efﬁcient exploitation of the Internet Archive. In Section 2 we lay out the STRAND architecture, which is based on the insight that translated Web pages tend quite strongly to exhibit parallel structure, permitting them to be identiﬁed even without looking at content; we also show how we have improved STRAND’s performance by training a supervised classiﬁer using structural parameters rather than relying on manually tuned thresholds. In Section 3 we present an approach to detecting translations that relies entirely on content rather than structure, demonstrating performance comparable to STRAND’s using this orthogonal source of information. In Section 4 we describe how we have adapted the STRAND approach to the Internet Archive, dramatically improving our ability to identify parallel Web pages on a large scale. Section 5 puts all the pieces together, using structural and combined content-structure matching of pages on the Internet Archive in order to obtain a sizable corpus of English-Arabic Web document pairs. Finally we present our thoughts on future work and conclusions. 2. The STRAND Web-Mining Architecture STRAND (Resnik 1998, 1999) is an architecture for structural translation recognition, acquiring natural data. Its goal is to identify pairs of Web pages that are mutual translations. In order to do this, it exploits an observation about the way that Web page authors disseminate information in multiple languages: When presenting the same content in two different languages, authors exhibit a very strong tendency to use the same document structure (e.g., Figure 1). STRAND therefore locates pages that might be translations of each other, via a number of different strategies, and ﬁlters out page pairs whose page structures diverge by too much. In this section we describe how STRAND works, and we also discuss several related Web-mining methods, focusing on the overall architecture these systems have in common and the important system-speciﬁc variations. We then show how tuning STRAND’s structural parameters using supervised training can signiﬁcantly increase its performance. 2.1 STRAND Finding parallel text on the Web consists of three main steps: • Location of pages that might have parallel translations  350  Resnik and Smith  The Web as a Parallel Corpus  Figure 1 Example of a candidate pair. • Generation of candidate pairs that might be translations • Structural ﬁltering out of nontranslation candidate pairs We consider each of these steps in turn. 2.1.1 Locating Pages. The original STRAND architecture accomplished the ﬁrst step by using the AltaVista search engine’s http://www.av.com advanced search to search for two types of Web pages: parents and siblings. A parent page is one that contains hypertext links to different-language versions of a document; for example, if we were looking for English and French bitexts, the page at the left in Figure 2 would lead us to one such candidate pair. To perform this search for the English-French language pair, we ask AltaVista for pages in any language that satisfy this Boolean expression: (anchor:"english" OR anchor:"anglais") AND (anchor:"french" OR anchor:"franc¸ais"). A 10-line distance ﬁlter is used to restrict attention to pages on which the English and French pointers occur reasonably close to one another—speciﬁcally, those for which the regular expression (in Perl) /(english|anglais)/ is satisﬁed within 10 lines of the Perl regular expression /(french|fran\w+ais)/ in the HTML source. This helps ﬁlter out a page that contained, for example, a link to “English literature courses” and also contained an unrelated link to “French version” at the top. A sibling page is a page in one language that itself contains a link to a version of the same page in another language; for example, the page at the right of Figure 2 contains a link on the left that says “This page in english.” To perform this search for English pages matching a given French page, we request pages in French that match the Boolean expression anchor:"english" OR anchor:"anglais". More recent versions of STRAND (unpublished) have added a “spider” component for locating pages that might have translations. Given a list of Web sites thought to contain bilingual text for a given language pair (e.g., sites identiﬁed using the AltaVista-based search), it is possible to download all the pages on each site, any 351  Computational Linguistics  Volume 29, Number 3  Figure 2 Excerpts from a parent page (left) and a sibling page (right). The parent page is in Italian and contains links marked “Italiano/Italian,” “Francese/French,” and “Inglese/English.” The sibling page is in Dutch and contains a link marked “This page in english” in the leftmost column. of which might have a translation on that site. Although simple to implement, this method of locating pages shifts the burden of narrowing down the possibilities to the process of generating candidate document pairs. The results reported here do not make use of the spider. 2.1.2 Generating Candidate Pairs. Pairing up potentially translated pages is simple when a search engine has been used to generate parent or sibling pages: One simply pairs the two child pages to which the parent links, or the sibling page together with the page to which it links. When all the pages on a site are under consideration, the process is rather different. The simplest possibility is to separate the pages on a site into the two languages of interest using automatic language identiﬁcation (Ingle 1976; Beesley 1988; Cavnar and Trenkle 1994; Dunning 1994), throwing away any pages that are not in either language, and then generate the cross product. This potentially leads to a very large number of candidate page pairs, and there is no particular reason to believe that most of them are parallel translations, other than the fact that they appear on the same Web site. The spider component of STRAND adds a URL-matching stage, exploiting the fact that the directory structure on many Web sites reﬂects parallel organization when pages are translations of each other. Matching is performed by manually creating a list of substitution rules (e.g., english → big5),1 and for each English URL, applying all possible rules to generate URLs that might appear on the list of pages for the other language. If such a URL is found, the pair with similar URLs is added to the list of candidate document pairs. For example, suppose an English-Chinese site contains a page with URL http://mysite.com/english/home en.html , on which one combination of substitutions might produce the URL http://mysite.com/big5/home ch.html . The original page and the produced URL are probably worth considering as a likely candidate pair. 
 Michel Simard† Universite´ de Montre´al  Although more and more language pairs are covered by machine translation (MT) services, there are still many pairs that lack translation resources. Cross-language information retrieval (CLIR) is an application that needs translation functionality of a relatively low level of sophistication, since current models for information retrieval (IR) are still based on a bag of words. The Web provides a vast resource for the automatic construction of parallel corpora that can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this article, we will investigate the problem of automatically mining parallel texts from the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost. 1. Introduction Finding relevant information in any language on the increasingly multilingual World Wide Web poses a real challenge for current information retrieval (IR) systems. We will argue that the Web itself can be used as a translation resource in order to build effective cross-language IR systems. 1.1 Information Retrieval and Cross-Language Information Retrieval The goal of IR is to ﬁnd relevant documents from a large collection of documents or from the World Wide Web. To do this, the user typically formulates a query, often in free text, to describe the information need. The IR system then compares the query with each document in order to evaluate its similarity (or probability of relevance) to the query. The retrieval result is a list of documents presented in decreasing order of similarity. The key problem in IR is that of effectiveness, that is, how good an IR system is at retrieving relevant documents and discarding irrelevant ones. Because of the information explosion that has occurred on the Web, people are more in need of effective IR systems than ever before. The search engines currently available on the Web are IR systems that have been created to answer this need. By querying these search engines, users are able to identify quickly documents containing the same keywords as the query they enter. However, the existing search engines provide only monolingual IR; that is, they retrieve documents only in the same lan-  ∗ TNO TPD, PO BOX 155, 2600 AD Delft, The Netherlands. E-mail: kraaij@tpd.tno.nl † DIRO, Universite´ de Montre´al, CP. 6128, succ. Centre-ville, Montreal, Qc. H3C 3J7 Canada. E-mail: {nie, simardm}@iro.umontreal.ca c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 3  guage as the query. To be more precise: Search engines usually do not consider the language of the keywords when the keywords of a query are matched against those of the documents. Identical keywords are matched, whatever their languages are. For example, the English word son can match the French word son (‘his’ or ‘her’). Current search engines do not provide the functionality for cross-language IR (CLIR), that is, the ability to retrieve relevant documents written in languages different from that of the query (without the query’s being translated manually into the other language(s) of interest). As the Web has grown, more and more documents on the Web have been written in languages other than English, and many Internet users are non-native English speakers. For many users, the barrier between tbe language of the searcher and the langage in which documents are written represents a serious problem. Although many users can read and understand rudimentary English, they feel uncomfortable formulating queries in English, either because of their limited vocabulary in English, or because of the possible misusage of English words. For example, a Chinese user may use economic instead of cheap or economical or inexpensive in a query because these words have a similar translation in Chinese. An automatic query translation tool would be very helpful to such a user. On the other hand, even if a user masters several languages, it is still a burden for him or her to formulate several queries in different languages. A query translation tool would also allow such a user to retrieve relevant documents in all the languages of interest with only one query. Even for users with no understanding of a foreign language, a CLIR system might still be useful. For example, someone monitoring a competitor’s developments with regard to products similar to those he himself produces might be interested in retrieving documents describing the possible products, even if he does not understand them. Such a user might use machine translation systems to get the gist of the contents of the documents he retrieves through his query. For all these types of users, CLIR would represent a useful tool.  1.2 Possible Approaches to CLIR From an implementation point of view, the only difference between CLIR and the classical IR task is that the query language differs from the document language. It is obvious that to perform in an effective way the task of retrieving documents that are relevant to a query when the documents are written in a different language than the query, some form of translation is required. One might conjecture that a combination of two existing ﬁelds, IR and machine translation (MT), would be satisfactory for accomplishing the combined translation and retrieval task. One could simply translate the query by means of an MT system, then use existing IR tools, obviating the need for a special CLIR system. This approach, although feasible, is not the only possible approach, nor is it necessarily the best one. MT systems try to translate text into a well-readable form governed by morphological, syntactic, and semantic constraints. However, current IR models are based on bag-of-words models. They are insensitive to word order and to the syntactic structure of queries. For example, with current IR models, the query “computer science” will usually produce the same retrieval results as “science computer.” The complex process used in MT for producing a grammatical translation is not fully exploited by current IR models. This means that a simpler translation approach may sufﬁce to implement the translation step. On the other hand, MT systems are far from perfect. They often produce incorrect 382  Kraaij, Nie, and Simard  Embedding Web-Based Statistical Models in CLIR  translations. For example, Systran1 translates the word drug as drogue (illegal substance) in French for both drug trafﬁc and drug administration ofﬁce. Such a translation error will have a substantial negative impact on the effectiveness of any CLIR system that incorporates it. So even if MT systems are used as translation devices, they may need to be complemented by other, more robust translation tools to improve their effectiveness. In the current study, we will use statistical translation models as such a complementary tool. Queries submitted to IR systems or search engines are often very short. In particular, the average length of queries submitted to the search engines on the Web is about two words (Jansen et al. 2001). Such short queries are generally insufﬁcient to describe the user’s information need in a precise and unambiguous way. Many important words are missing from them. For example, a user might formulate the query “Internet connection” in order to retrieve documents about computer networks, Internet service providers, or proxies. However, under the current bag-of-words approach, the relevant documents containing these terms are unlikely to be retrieved. To solve this problem, a common approach used in IR is query expansion, which tries to add synonyms or related words to the original query, making the expanded query a more exhaustive description of the information need. The words added to the query during query expansion do not need to be strict synonyms to improve the query results. However, they do have to be related, to some degree, to the user’s information need. Ideally, the degree of the relatedness should be weighted, with a strongly related word weighted more heavily in the expanded query than a less related one. MT systems act in a way opposite to the query expansion process: Only one translation is generally selected to express a particular meaning. 2 In doing so, MT systems employed in IR systems in fact restrict the possible query expansion effect during the translation process. We believe that CLIR can beneﬁt from query translation that provides multiple translations for the same meaning. In this regard, the tests carried out by Kwok (1999) with a commercial MT system for Chinese-English CLIR are quite interesting. His experiments show that it is much better to use the intermediate translation data produced by the MT system than the ﬁnal translation itself. The intermediate data contain, among other things, all the possible translation words for query terms. Kwok’s work clearly demonstrates that using an MT system as a black box is not the most effective choice for query translation in CLIR. However, few MT systems allow one to access the intermediate stages of the translations they produce. Apart from the MT approach, queries can also be translated by using a machinereadable bilingual dictionary or by exploiting a set of parallel texts (texts with their translations). High-quality bilingual dictionaries are expensive, but there are many free on-line translation dictionaries available on the Web that can be used for query translation. This approach has been applied in several studies (e.g., Hull and Grefenstette 1996; Hiemstra and Kraaij 1999). However, free bilingual dictionaries often suffer from a poor coverage of the vocabulary in the two languages with which they deal, and from the problem of translation ambiguity, because usually no information is provided to allow for disambiguation. Several previous studies (e.g., Nie et al. 1999), have shown that using a translation dictionary alone would produce much lower effectiveness than an MT system. However, a dictionary complemented by a statistical language model (Gao et al. 2001; Xu, Weischedel, and Nguyen 2001) has produced much better results than when the dictionary is used alone.  
 We have developed an example-based machine translation (EBMT) system that uses the World Wide Web for two different purposes: First, we populate the system’s memory with translations gathered from rule-based MT systems located on the Web. The source strings input to these systems were extracted automatically from an extremely small subset of the rule types in the PennII Treebank. In subsequent stages, the source, target translation pairs obtained are automatically transformed into a series of resources that render the translation process more successful. Despite the fact that the output from on-line MT systems is often faulty, we demonstrate in a number of experiments that when used to seed the memories of an EBMT system, they can in fact prove useful in generating translations of high quality in a robust fashion. In addition, we demonstrate the relative gain of EBMT in comparison to on-line systems. Second, despite the perception that the documents available on the Web are of questionable quality, we demonstrate in contrast that such resources are extremely useful in automatically postediting translation candidates proposed by our system. 1. Introduction In quite a short space of time, translation memory (TM) systems have become a very useful tool in the translator’s armory. TM systems store a set of source, target translation pairs in their databases. If a new input string cannot be found exactly in the translation database, a search is conducted for close (or “fuzzy”) matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the ﬁnal, output translation. From this description, it should be clear that TM systems do not translate: Indeed, some researchers consider them to be little more than a search-and-replace engine, albeit a rather sophisticated one (Macklovitch and Russell 2000). We can illustrate this with respect to the TM entries in (1), taken from the Canadian Hansards: (1) a. While most were critical, some contributions were thoughtful and constructive =⇒ La plupart ont formule´ des critiques, mais certains ont fait des observations re´ﬂe´chies et constructives. b. Others were plain meanspirited and some contained errors of fact =⇒ D’autres discours comportaient des propos mesquins et meˆme des erreurs de fait. ∗ School of Computing, Dublin 9, Ireland. E-mail: away@computing.dcu.ie † School of Computing, Dublin 9, Ireland. E-mail: ngough@computing.dcu.ie c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 3  Consider the new source string in (2): (2) While most were critical, some contributions were plain meanspirited. Despite the fact that this new input in (2) is extremely close to the source strings in the TM entries in (1), no TM system containing just these translation pairs in its database would be able to translate (2); the best they could do would be to identify one or both of the two source sentences in the TM in (1) as fuzzy matches and display these, together with their French translations. The translator would then manipulate the target strings in the TM into the ﬁnal translation (3): (3) La plupart ont formule´ des critiques, mais certains ont fait des observations mesquines. An alternative translation that might be derived from the TM entries in (1) is that in (4): (4) La plupart ont formule´ des critiques, mais certains comportaient des observations mesquines. At all stages in the translation process, therefore, the translators themselves are the integral ﬁgures: They are free to accept or reject any suggested matches, they construct the translations, and they may or may not use any translations proposed by the TM system to formulate the translations in the target document. Finally, they are free to insert the translations produced into the TM itself as they see ﬁt: that is, either (3) or (4) could be inserted into the TM with the source string (2), or some other translation, if that were preferred. A prerequisite for TM (and example-based machine translation [EBMT]) applications is a parallel corpus aligned at sentential level. Such a corpus may be presented to translators en bloc, or translators may help construct it themselves. Here too the translator maintains a large degree of autonomy: Using a tool such as Trados WinAlign, for example, he or she may manually overwrite some of the aligner’s decisions by linking source, target sentence pairs using the graphical interface provided. Nevertheless, TM systems are currently falling far short of their potential, given the limitation that the smallest accessible translation units are source, target strings aligned only at sentential level. Consider the fuzzy matching operation, for instance: Translators are able to set a fuzzy match threshold below which no translation pairs are proposed by the TM system. If this threshold is set too low, then potentially useful translation pairs will be presented along with a lot of noise, thereby risking that this useful translation information will be obscured (high recall, low precision); if it is set too low, then good matches will be presented, but potentially useful matches will not be (low recall, high precision). We noted above that faced with the new input in (2), a TM system might be able to present the translator with the fuzzy matches in (1). However, if a translator were to set the level of fuzzy matching at 80% (a not unreasonable level), then neither of the translation pairs in (1) would be deemed to be a suitably good fuzzy match, as only 7/9 (77%) of the words in (1a) match those in (2) exactly, and only 3/9 (33%) of the words in (1b) match those in (2) exactly. Indeed, setting an appropriate fuzzy match level is such a difﬁcult problem that some translators switch off this option and use the TM only to ﬁnd exact matches. If subsentential alignment could be integrated into the TM databases, more useful fragments could be put at the disposal of the translator. If we could fragment the  422  Way and Gough  wEBMT  sententially aligned TM examples in (1) so that subsentential chunks were displayed to the user, then the chance of ﬁnding exact matches or good fuzzy matches would increase considerably. This is currently beyond the scope of TM systems. In contrast, EBMT systems have overcome this constraint by storing subsentential translational correspondences in addition to the sententially aligned pairs from which they are derived. As a consequence, where a TM system can only propose a number of close-scoring matches in its database for the translator to adapt into the ﬁnal translation, an EBMT system can produce translations itself by automatically combining chunks from different translation examples stored in its memories. In Section 2, we describe how we automatically obtain a hierarchy of lexical resources that are used sequentially by our EBMT system, wEBMT, to translate new input. The primary resource gathered is a “phrasal lexicon,” constructed by extracting over 200,000 phrases from the Penn Treebank and having them translated into French by three Web-based machine translation (MT) systems. Each set of translations is stored separately, and for each set the “marker hypothesis” (Green 1979) is used to segment the phrasal lexicon into a “marker lexicon.” The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ”marked” for complex syntactic structure at surface form by a closed set of speciﬁc lexemes and morphemes. That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment. Consider the following example, selected at random from the Wall Street Journal section of the Penn-II Treebank: (5) The Dearborn, Mich., energy company stopped paying a dividend in the third quarter of 1984 because of troubles at its Midland nuclear plant. Here we see that three noun phrases start with determiners and one with a possessive pronoun. The sets of determiners and possessive pronouns are both very small. Furthermore, there are four prepositional phrases, and the set of prepositions is similarly small. A further assumption that could be made is that all words that end with -ed are verbs, such as stopped in (5). The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different language-related tasks, including • language learning (Green 1979; Mori and Moeser 1983; Morgan, Meier, and Newport 1989) • monolingual grammar induction (Juola 1998) • grammar optimization (Juola 1994) • insights into universal grammar (Juola 1998) • machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance. Green’s (1979) work showed that artiﬁcial languages, both with and without speciﬁc marker words, may be learned more accurately and quickly if such psycholinguistic cues exist. The  423  Computational Linguistics  Volume 29, Number 3  research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artiﬁcial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis. Juola’s (1994, 1998) work on grammar optimization and induction shows that context-free grammars can be converted to ”marker-normal form.” However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless, Juola (1998, page 23) observes that “a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.” Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun–case marker segment, once one has identiﬁed the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process. As a byproduct of the chosen methodology, we also derive a standard ”word-level” translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input. In Section 3, we describe in detail the segmentation process, together with the procedure whereby target chunks are combined to produce candidate translations. In Section 4, we report initially on two experiments in which we test different versions of our EBMT system against test sets of NPs and sentences. We then conduct a set of further experiments which show that using the resources developed from more than one on-line MT system may improve both translation coverage and quality. Furthermore, seeding the system databases with more fragments improves translation quality. In addition, we calculate the net gain of our EBMT system by comparing translation quality against that of the three on-line MT systems. Finally, we comment on the relative strengths and weaknesses of the three on-line MT systems used. Like most EBMT systems, our approach suffers from the problem of “boundary friction”: where chunks from different translation examples are recombined, the quality of the resulting translations may be compromised. Assume that the aligned examples in (6) are located in the system database: (6) a. You can attach a phone to the connector =⇒ Vous pouvez re´lier un te´le´phone au connecteur. b. Connect only the keyboard and a mouse =⇒ Connectez uniquement le clavier et une souris. Let us now confront the EBMT system with the new input string in (7): (7) You can attach a mouse to the connector. This could be correctly translated by the EBMT system by isolating the useful translation fragments in (8): (8) a. You can attach =⇒ Vous pouvez re´lier (from (6a)) b. a mouse =⇒ une souris (from (6b)) c. to the connector =⇒ au connecteur (from (6a))  424  Way and Gough  wEBMT  Recombining the French chunks gives us the correct translation in (9): (9) Vous pouvez re´lier une souris au connecteur. However, a number of mistranslations could also ensue, including those in (10): (10) a. *Vous pouvez re´lier un souris au connecteur. b. *Vous pouvez re´lier un souris au le connecteur. The mistranslation (10a) could be formed via the set of inferences in (11): (11) You can attach a =⇒ Vous pouvez re´lier un (from (6a)) mouse =⇒ souris (from (6b)) to the connector =⇒ au connecteur (from (6a)) The mistranslation (10b) could be formed via the set of inferences in (12): (12) You can attach a =⇒ Vous pouvez re´lier un (from (6a)) mouse =⇒ souris (from (6b)) to =⇒ au (from (6a)) the =⇒ le (from (6b)) connector =⇒ connecteur (from (6a)) It is clear, therefore, that unless the process by which the original source, target sentence pairs are fragmented is well deﬁned and strictly controlled, chunks may be combined from different contexts that result in agreement errors such as those in (10).1 Depending on the input string, our wEBMT system may generate thousands of candidate translations, including many mistranslations like those in (10). A major advantage of MT systems based on probabilities is that output translations can be ranked (and pruned, if required): One would hope that such systems would rank good translations such as that in (9) more highly than poor ones such as those in (10). We demonstrate that in almost all experiments, our EBMT system consistently ranks the “best” translation in the top 10 output translations, and always in the top 1% of the translations generated. In order to minimize errors of boundary friction, in Section 5 we develop a novel, post hoc procedure via the World Wide Web to validate and, if necessary, correct translations prior to their being output to the user.2 Finally we conclude and point to areas of future research.  
 This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task. 1. Introduction In two recent papers, Banko and Brill (2001a, 2001b) criticize the fact that current NLP algorithms are typically optimized, tested, and compared on fairly small data sets (corpora with millions of words), even though data sets several orders of magnitude larger are available, at least for some NLP tasks. Banko and Brill (2001a, 2001b) experiment with context-sensitive spelling correction, a task for which large amounts of data can be obtained straightforwardly, as no manual annotation is required. They demonstrate that the learning algorithms typically used for spelling correction beneﬁt signiﬁcantly from larger training sets, and that their performance shows no sign of reaching an asymptote as the size of the training set increases. Arguably, the largest data set that is available for NLP is the Web,1 which currently consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) ﬁndings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora,  ∗ School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Shefﬁeld S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information providers such as Lexis Nexis http://www.lexisnexis.com/ might have databases that are even larger than the Web. Lexis Nexis provides full-text access to news sources (including newspapers, wire services, and broadcast transcripts) and legal data (including case law, codes, regulations, legal news, and law reviews). 2 This is the number of pages indexed by Google in December 2002, as estimated by Search Engine Showdown http://www.searchengineshowdown.com/ . c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 3  and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve language modeling. A particularly interesting application is proposed by Grefenstette (1998), who uses the Web for example-based machine translation. His task is to translate compounds from French into English, with corpus evidence serving as a ﬁlter for candidate translations. An example is the French compound groupe de travail. There are ﬁve translations of groupe and three translations for travail (in the dictionary that Grefenstette [1998] is using), resulting in 15 possible candidate translations. Only one of them, namely, work group, has a high corpus frequency, which makes it likely that this is the correct translation into English. Grefenstette (1998) observes that this approach suffers from an acute data sparseness problem if the counts are obtained from a conventional corpus. However, as Grefenstette (1998) demonstrates, this problem can be overcome by obtaining counts through Web searches, instead of relying on a corpus. Grefenstette (1998) therefore effectively uses the Web as a way of obtaining counts for compounds that are sparse in a given corpus. Although this is an important initial result, it raises the question of the generality of the proposed approach to overcoming data sparseness. It remains to be shown that Web counts are generally useful for approximating data that are sparse or unseen in a given corpus. It seems possible, for instance, that Grefenstette’s (1998) results are limited to his particular task (ﬁltering potential translations) or to his particular linguistic phenomenon (noun-noun compounds). Another potential problem is the fact that Web counts are far more noisy than counts obtained from a well-edited, carefully balanced corpus. The effect of this noise on the usefulness of the Web counts is largely unexplored. Zhu and Rosenfeld (2001) use Web-based n-gram counts for language modeling. They obtain a standard language model from a 103-million-word corpus and employ Web-based counts to interpolate unreliable trigram estimates. They compare their interpolated model against a baseline trigram language model (without interpolation) and show that the interpolated model yields an absolute reduction in word error rate of .93% over the baseline. Zhu and Rosenfeld’s (2001) results demonstrate that the Web can be a source of data for language modeling. It is not clear, however, whether their result carries over to tasks that employ linguistically meaningful word sequences (e.g., head-modiﬁer pairs or predicate-argument tuples) rather than simply adjacent words. Furthermore, Zhu and Rosenfeld (2001) do not undertake any studies that evaluate Web frequencies directly (i.e., without a task such as language modeling). This could be done, for instance, by comparing Web frequencies to corpus frequencies, or to frequencies re-created by smoothing techniques. The aim of the present article is to generalize Grefenstette’s (1998) and Zhu and Rosenfeld’s (2001) ﬁndings by testing the hypothesis that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. Instead of having a particular task in mind (which would introduce a sampling bias), we rely on sets of bigrams that are randomly selected from a corpus. We use a Web-based approach for bigrams that encode meaningful syntactic relations and obtain Web frequencies not only for noun-noun bigrams, but also for adjective-noun and verb-object bigrams. We thus explore whether this approach generalizes to different predicateargument combinations. We evaluate our Web counts in four ways: (a) comparison with actual corpus frequencies from two different corpora, (b) comparison with human plausibility judgments, (c) comparison with frequencies re-created using class-based smoothing, and (d) performance in a pseudodisambiguation task on data sets from the literature.  460  Keller and Lapata  Web Frequencies for Unseen Bigrams  2. Obtaining Frequencies from the Web 2.1 Sampling Bigrams from the BNC The data sets used in the present experiment were obtained from the British National Corpus (BNC) (see Burnard [1995]). The BNC is a large, synchronic corpus, consisting of 90 million words of text and 10 million words of speech. The BNC is a balanced corpus (i.e., it was compiled so as to represent a wide range of present day British English). The written part includes samples from newspapers, magazines, books (both academic and ﬁction), letters, and school and university essays, among other kinds of text. The spoken part consists of spontaneous conversations, recorded from volunteers balanced by age, region, and social class. Other samples of spoken language are also included, ranging from business or government meetings to radio shows and phoneins. The corpus represents many different styles and varieties and is not limited to any particular subject ﬁeld, genre, or register. For the present study, the BNC was used to extract data for three types of predicateargument relations. The ﬁrst type is adjective-noun bigrams, in which we assume that the noun is the predicate that takes the adjective as its argument.3 The second predicate-argument type we investigated is noun-noun compounds. For these, we assume that the rightmost noun is the predicate that selects the leftmost noun as its argument (as compound nouns are generally right-headed in English). Third, we included verb-object bigrams, in which the verb is the predicate that selects the object as its argument. We considered only direct NP objects; the bigram consists of the verb and the head noun of the object. For each of the three predicate-argument relations, we gathered two data sets, one containing seen bigrams (i.e., bigrams that occur in the BNC) and one with unseen bigrams (i.e., bigrams that do not occur in the BNC). For the seen adjective-noun bigrams, we used the data of Lapata, McDonald, and Keller (1999), who compiled a set of 90 bigrams as follows. First, 30 adjectives were randomly chosen from a part-of-speech-tagged and lemmatized version of the BNC so that each adjective had exactly two senses according to WordNet (Miller et al. 1990) and was unambiguously tagged as “adjective” 98.6% of the time. Lapata, McDonald, and Keller used the part-of-speech-tagged version that is made available with the BNC and was tagged using CLAWS4 (Leech, Garside, and Bryant 1994), a probabilistic partof-speech tagger, with error rate ranging from 3% to 4%. The lemmatized version of the corpus was obtained using Karp et al.’s (1992) morphological analyzer. The 30 adjectives ranged in BNC frequency from 1.9 to 49.1 per million words; that is, they covered the whole range from fairly infrequent to highly frequent items. Gsearch (Corley et al. 2001), a chart parser that detects syntactic patterns in a tagged corpus by exploiting a user-speciﬁed context-free grammar and a syntactic query, was used to extract all nouns occurring in a head-modiﬁer relationship with one of the 30 adjectives. Examples of the syntactic patterns the parser identiﬁed are given in Table 1. In the case of adjectives modifying compound nouns, only sequences of two nouns were included, and the rightmost-occurring noun was considered the head. Bigrams involving proper nouns or low-frequency nouns (less than 10 per million words) were discarded. This was necessary because the bigrams were used in experiments involving native speakers (see Section 3.2), and we wanted to reduce the risk of including words unfamiliar to the experimental subjects. For each adjective, the set of bigrams was divided into three frequency bands based on an equal division of the  3 This assumption is disputed in the theoretical linguistics literature. For instance, Pollard and Sag (1994) present an analysis in which there is mutual selection between the noun and the adjective. 461  Computational Linguistics  Volume 29, Number 3  Table 1 Example of patterns used for the extraction of adjective-noun bigrams.  Pattern Example  AN  educational material  A Adv N usual weekly classes  A N N environmental health ofﬁcers  range of log-transformed co-occurrence frequencies. Then one bigram was chosen at random from each band. This procedure ensures that the whole range of frequencies is represented in our sample. Lapata, Keller, and McDonald (2001) compiled a set of 90 unseen adjective-noun bigrams using the same 30 adjectives. For each adjective, Gsearch was used to compile a list of all nouns that did not co-occur in a head-modiﬁer relationship with the adjective. Again, proper nouns and low-frequency nouns were discarded from this list. Then each adjective was paired with three randomly chosen nouns from its list of non-co-occurring nouns. Examples of seen and unseen adjective-noun bigrams are shown in Table 2. For the present study, we applied the procedure used by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and McDonald (2001) to noun-noun bigrams and to verb-object bigrams, creating a set of 90 seen and 90 unseen bigrams for each type of predicate-argument relationship. More speciﬁcally, 30 nouns and 30 verbs were chosen according to the same criteria proposed for the adjective study (i.e., minimal sense ambiguity and unambiguous part of speech). All nouns modifying one of the 30 nouns were extracted from the BNC using a heuristic from Lauer (1995) that looks for consecutive pairs of nouns that are neither preceded nor succeeded by another  Table 2 Example stimuli for seen and unseen adjective-noun, noun-noun, and verb-object bigrams (with log-transformed BNC counts).  Adjective High  Adjective-Noun Bigrams  Medium  Low  Unseen  hungry guilty naughty  animal 1.79 pleasure 1.38 application 0  tradition, innovation, prey  verdict 3.91 secret  2.56 cat  0  system, wisdom, wartime  girl  2.94 dog  1.6 lunch  .69 regime, rival, protocol  Noun-Noun Bigrams  High  Medium  Low  Unseen  Head Noun  process 1.14 user television 1.53 satellite plasma 1.78 nylon  .95 gala  0  collection, clause, coat directory  .95 edition 0  chain, care, vote  broadcast  1.20 unit  .60 fund, theology, minute membrane  Verb-Object Bigrams  Verb  High  Medium  Low  Unseen  fulﬁll  obligation 3.87 goal  intensify problem 1.79 effect  choose name  3.74 law  2.20 scripture .69 participant, muscle, grade  1.10 alarm  0  score, quota, chest  1.61 series  1.10 lift, bride, listener  462  Keller and Lapata  Web Frequencies for Unseen Bigrams  noun. Lauer’s heuristic (see (1)) effectively avoids identifying as two-word compounds noun sequences that are part of a larger compound. (1) C = {(w2, w3) | w1w2w3w4; w1, w4 ∈ N; w2, w3 ∈ N} Here, w1 w2 w3 w4 denotes the occurrence of a sequence of four words and N is the set of words tagged as nouns in the corpus. C is the set of compounds identiﬁed by Lauer’s (1995) heuristic. Verb-object bigrams for the 30 preselected verbs were obtained from the BNC using Cass (Abney 1996), a robust chunk parser designed for the shallow analysis of noisy text. The parser recognizes chunks and simplex clauses (i.e., sequences of nonrecursive clauses) using a regular expression grammar and a part-of-speech-tagged corpus, without attempting to resolve attachment ambiguities. It comes with a largescale grammar for English and a built-in tool that extracts predicate-argument tuples out of the parse trees that Cass produces. The parser’s output was postprocessed to remove bracketing errors and errors in identifying chunk categories that could potentially result in bigrams whose members do not stand in a verb-argument relationship. Tuples containing verbs or nouns attested in a verb-argument relationship only once were eliminated. Particle verbs were retained only if the particle was adjacent to the verb (e.g., come off heroin). Verbs followed by the preposition by and a head noun were considered instances of verb-subject relations. It was assumed that PPs adjacent to the verb headed by any of the prepositions in, to, for, with, on, at, from, of, into, through, and upon were prepositional objects (see Lapata [2001] for details on the ﬁltering process). Only nominal heads were retained from the objects returned by the parser. As in the adjective study, noun-noun bigrams and verb-object bigrams with proper nouns or low-frequency nouns (less than 10 per million words) were discarded. The sets of noun-noun and verb-object bigrams were divided into three frequency bands, and one bigram was chosen at random from each band. The procedure described by Lapata, Keller, and McDonald (2001) was followed for creating sets of unseen noun-noun and verb-object bigrams: for each noun or verb, we compiled a list of all nouns with which it did not co-occur within a noun-noun or verbobject bigram in the BNC. Again, Lauer’s (1995) heuristic and Abney’s (1996) partial parser were used to identify bigrams, and proper nouns and low-frequency nouns were excluded. For each noun and verb, three bigrams were formed by pairing it with a noun randomly selected from the set of the non-co-occurring nouns for that noun or verb. Table 2 lists examples for the seen and unseen noun-noun and verb-object bigrams generated by this procedure. The extracted bigrams are in several respects an imperfect source of information about adjective-noun or noun-noun modiﬁcation and verb-object relations. First notice that both Gsearch and Cass detect syntactic patterns on part-of-speech-tagged corpora. This means that parsing errors are likely to result because of tagging mistakes. Second, even if one assumes perfect tagging, the heuristic nature of our extraction procedures may introduce additional noise or miss bigrams for which detailed structural information would be needed. For instance, our method for extracting adjective-noun pairs ignores cases in which the adjective modiﬁes noun sequences of length greater than two. The heuristic in (1) considers only two-word noun sequences. Abney’s (1996) chunker recognizes basic syntactic units without resolving attachment ambiguities or recovering missing information (such as traces resulting from the movement of constituents). Although parsing is robust and fast (since unlike in traditional parsers, no global optimization takes  463  Computational Linguistics  Volume 29, Number 3  place), the identiﬁed verb-argument relations are undoubtedly somewhat noisy, given the errors inherent in the part-of-speech tagging and chunk recognition procedure. When evaluated against manually annotated data, Abney’s (1996) parser identiﬁed chunks with 87.9% precision and 87.1% recall. The parser further achieved a per-word accuracy of 92.1% (where per-word accuracy includes the chunk category and chunk length identiﬁed correctly). Despite their imperfect output, heuristic methods for the extraction of syntactic relations are relatively common in statistical NLP. Several statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identiﬁed in a heuristic manner and may be noisy. 2.2 Sampling Bigrams from the NANTC We also obtained corpus counts from a second corpus, the North American News Text Corpus (NANTC). This corpus differs in several important respects from the BNC. It is substantially larger, as it contains 350 million words of text. Also, it is not a balanced corpus, as it contains material from only one genre, namely, news text. However, the text originates from a variety of sources (Los Angeles Times, Washington Post, New York Times News Syndicate, Reuters News Service, and Wall Street Journal). Whereas the BNC covers British English, the NANTC covers American English. All these differences mean that the NANTC provides a second, independent standard against which to compare Web counts. At the same time the correlation found between the counts obtained from the two corpora can serve as an upper limit for the correlation that we can expect between corpus counts and Web counts. The NANTC corpus was parsed using MINIPAR (Lin 1994, 2001), a broad-coverage parser for English. MINIPAR employs a manually constructed grammar and a lexicon derived from WordNet with the addition of proper names (130,000 entries in total). Lexicon entries contain part-of-speech and subcategorization information. The grammar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges (i.e., types of syntactic [dependency] relationships). MINIPAR employs a distributedchart parsing algorithm. Instead of a single chart, each node in the grammar network maintains a chart containing partially built structures belonging to the grammatical category represented by the node. Grammar rules are implemented as constraints associated with the nodes and edges. The output of MINIPAR is a dependency tree that represents the dependency relations between words in a sentence. Table 3 shows a subset of the dependencies MINIPAR outputs for the sentence The fat cat ate the door mat. In contrast to Gsearch and Cass, MINIPAR produces all possible parses for a given sentence. The parses are ranked according to the product of the probabilities of their edges, and the most likely parse is returned. Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson 1995), a domain-independent corpus of British English, and achieved a recall of 79% and precision of 89% on the dependency relations.  464  Keller and Lapata  Web Frequencies for Unseen Bigrams  Table 3 Examples of dependencies generated by MINIPAR for The fat cat ate the door mat.  Head Relation Modiﬁer Description  cat N:det:Det the  cat N:mod:A fat  eat V:subj:N cat  eat V:obj:N mat  mat N:det:Det the  mat N:nn:N  door  determiner of noun adjective modiﬁer of noun subject of verb object of verb determiner of noun prenominal modiﬁer of noun  For our experiments, we concentrated solely on adjective-noun, noun-noun, and verb object relations (denoted as N:mod:A, N:nn:N, and V:obj:N in Table 3). From the syntactic analysis provided by the parser, we extracted all occurrences of bigrams that were attested both in the BNC and the NANTC corpus. In this way, we obtained NANTC frequency counts for the bigrams that we had randomly selected from the BNC. Table 4 shows the NANTC counts for the set of seen bigrams from Table 2. Because of the differences in the extraction methodology (chunking versus full parsing) and the text genre (balanced corpus versus news text), we expected that some BNC bigrams would not be attested in the NANTC corpus. More precisely, zero frequencies were returned for 23 adjective-noun, 16 verb-noun, and 37 noun-noun bigrams. The fact that more zero frequencies were observed for noun-noun bigrams than for the other two types is perhaps not surprising considering the ease with which novel compounds are created (Levi 1978). We adjusted the zero counts by setting them to .5. This was necessary because all further analyses were carried out on logtransformed frequencies (see Table 4).  Table 4 Log-transformed NANTC counts for seen adjective-noun, noun-noun, and verb-object bigrams.  Adjective-Noun Bigrams  Adjective High  Medium  Low  hungry guilty naughty High  animal .90 pleasure -.30  verdict 2.82 secret  .95  girl  .69 dog  -.30  Noun-Noun Bigrams  Medium  Low  application .60  cat  -.30  lunch  -.30  Head Noun  process - .30 television 2.70 plasma - .30  Verb  High  user satellite nylon  -.30 gala -.30 edition 0 unit  Verb-Object Bigrams  Medium  -.30 directory -.30 broadcast 0 membrane Low  fulﬁll  obligation 2.38 goal  intensify problem 1.20 effect  choose name  2.25 law  2.04 scripture -.30  .60 alarm  -.30  .90 series  .48  465  Computational Linguistics  Volume 29, Number 3  2.3 Obtaining Web Counts Web counts for bigrams were obtained using a simple heuristic based on queries to the search engines AltaVista and Google. All search terms took into account the inﬂectional morphology of nouns and verbs. The search terms for verb-object bigrams matched not only cases in which the object was directly adjacent to the verb (e.g., fulﬁll obligation), but also cases in which there was an intervening determiner (e.g., fulﬁll the/an obligation). The following search terms were used for adjective-noun, noun-noun, and verb-object bigrams, respectively:  (2) "A N", where A is the adjective and N is the singular or plural form of the noun. (3) "N1 N2", where N1 is the singular form of the ﬁrst noun and N2 is the singular or plural form of the second noun. (4) "V Det N", where V is the inﬁnitive, singular present, plural present, past, perfect, or gerund form of the verb, Det is the determiner the, the determiner a, or the empty string, and N is the singular or plural form of the noun. Note that all searches were for exact matches, which means that the words in the search terms had to be directly adjacent to score a match. This is encoded by enclosing the search term in quotation marks. All our search terms were in lower case. We searched the whole Web (as indexed by AltaVista and Google); that is, the queries were not restricted to pages in English. Based on the Web searches, we obtained bigram frequencies by adding up the number of pages that matched the morphologically expanded forms of the search terms (see the patterns in (2)–(4)). This process can be automated straightforwardly using a script that generates all the search terms for a given bigram, issues an AltaVista or Google query for each of the search terms, and then adds up the resulting number of matches for each bigram. We applied this process to all the bigrams in our data set, covering seen and unseen adjective-noun, noun-noun, and verb-object bigrams (i.e., a set of 540 bigrams in total). The queries were carried out in January 2003 (and thus the counts are higher than those reported in Keller, Lapata, and Ourioupina [2002], which were generated about a year earlier). For some bigrams that were unseen in the BNC, our Web-based procedure returned zero counts; that is, there were no matches for those bigrams in the Web searches. It is interesting to compare the Web and NANTC with respect to zero counts: Both data sources are larger than the BNC and hence should be able to mitigate the data sparseness problem to a certain extent. Table 5 provides the number of zero counts for both Web search engines and compares them to the number of bigrams that yielded no matches in the NANTC. We observe that the Web counts are substantially less sparse than the NANTC counts: In the worst case, there are nine bigrams for which our Web queries returned no matches (10% of the data), whereas up to 82 bigrams were unseen in the NANTC (91% of the data). Recall that the NANTC is 3.5 times larger than the BNC, which does not seem to be enough to substantially mitigate data sparseness. All further analyses were carried out on log-transformed frequencies; hence we adjusted zero counts by setting them to .5. Table 6 shows descriptive statistics for the bigram counts we obtained using AltaVista and Google. For comparison, this table also provides descriptive statistics for the BNC and NANTC counts (for seen bigrams only) and for the counts  466  Keller and Lapata  Web Frequencies for Unseen Bigrams  Table 5 Number of zero counts returned by queries to search engines and in the NANTC (for bigrams unseen in BNC).  Adjective-Noun Noun-Noun Verb-Object  AltaVista 2 Google 2 NANTC 76  9  
 Felisa Verdejo∗ UNED, Madrid  We describe an algorithm that combines lexical information (from WordNet 1.7) with Web directories (from the Open Directory Project) to associate word senses with such directories. Such associations can be used as rich characterizations to acquire sense-tagged corpora automatically, cluster topically related senses, and detect sense specializations. The algorithm is evaluated for the 29 nouns (147 senses) used in the Senseval 2 competition, obtaining 148 (word sense, Web directory) associations covering 88% of the domain-speciﬁc word senses in the test data with 86% accuracy. The richness of Web directories as sense characterizations is evaluated in a supervised word sense disambiguation task using the Senseval 2 test suite. The results indicate that, when the directory/word sense association is correct, the samples automatically acquired from the Web directories are nearly as valid for training as the original Senseval 2 training instances. The results support our hypothesis that Web directories are a rich source of lexical information: cleaner, more reliable, and more structured than the full Web as a corpus. 1. Introduction Combining the size and diversity of the textual material on the World Wide Web with the power and efﬁciency of current search engines is an attractive possibility for acquiring lexical information and corpora. A widespread example is spell-checking: Many Web users routinely use search engines to assess which is the “correct” (i.e. with more hits in the Web) spelling of words. Among NLP researchers, Web search engines have already been used as a point of departure for extraction of parallel corpora, automatic acquisition of sense-tagged corpora, and extraction of lexical information. Extraction of parallel corpora. In Resnik (1999), Nie, Simard, and Foster (2001), Ma and Liberman (1999), and Resnik and Smith (2002), the Web is harvested in search of pages that are available in two languages, with the aim of building parallel corpora for any pair of target languages. This is a very promising technique, as many machine translation (MT) and cross-language information retrieval (CLIR) strategies rely on the existence of parallel corpora, which are still a scarce resource. Such Web-mined parallel corpora have proved to be useful, for instance, in the context of the CLEF (Cross-Language Evaluation Forum) CLIR competition, in which many participants use such parallel corpora (provided by the University of Montreal) to improve the performance of their systems (Peters et al. 2002). Automatic acquisition of sense-tagged corpora. The description of a word sense can be used to build rich queries in such a way that the occurrences of the word in the documents retrieved are, with some probability, associated with the desired sense. If the probability is high enough, it is then possible to acquire sense-tagged corpora  ∗ ETS Ingenier´ıa Informa´tica de la UNED, c/ Juan del Rosal, 16, Ciudad Universitaria, 28040 Madrid, Spain. E-mail: {celina,julio,felisa}@lsi.uned.es c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 3  in a fully automatic fashion. Again, this is an exciting possibility that would solve the current bottleneck of supervised word sense disambiguation (WSD) methods (namely, that sense-tagged corpora are very costly to acquire). One example of this kind of technique is Mihalcea and Moldovan (1999), in which a precision of 91% is reported over a set of 20 words with 120 senses. In spite of the high accuracy obtained, such methodology did not perform well in the comparative evaluation reported in Agirre and Mart´ınez (2000), perhaps indicating that examples obtained from the Web may have topical biases (depending on the word), and that further reﬁnement is required. For instance, a technique that behaves well with a small set of words might fail in the common cases in which a new sense is predominant on the Web (e.g., oasis or nirvana as music groups, tiger as a golfer, jaguar as a car brand). Extraction of lexical information. In Agirre et al. (2000), search engines and the Web are used to assign Web documents to WordNet concepts. The resulting sets of documents are then processed to build topic signatures, that is, sets of words with weights that enrich the description of a concept. In Grefenstette (1999), the number of hits in Web search engines is used as a source of evidence to select optimal translations for multiword expressions. For instance, apple juice is selected as a better translation than apple sap for the German ApfelSaft because apple juice hits a thousand times more documents in AltaVista. Finally, in Joho and Sanderson (2000) and Fujii and Ishikawa (1999), the Web is used as a resource to provide descriptive phrases or deﬁnitions for technical terms. A common problem to all the above applications is how to detect and ﬁlter out all the noisy material on the Web, and how to characterize the rest (Kilgarriff 2001b). Our starting hypotheses is that Web directories (e.g., Yahoo, AltaVista or Google directories, the Open Directory Project [ODP]), in which documents are mostly manually classiﬁed in hierarchical topical clusters, are an optimal source for acquiring lexical information; their size is not comparable to the full Web, but they are still enormous sources of semistructured, semiﬁltered information waiting to be mined. In this article, we describe an algorithm for assigning Web directories (from the Open Directory Project http://dmoz.org ) as characterizations for word senses in WordNet 1.7 noun synsets (Miller 1990). For instance, let us consider the noun circuit, which has six senses in WordNet 1.7. These senses are grouped in synsets, together with their synonym terms, and linked to broader (more general) synsets via hypernymy relations:  6 senses of circuit Sense 1: {circuit, electrical circuit, electric circuit} => {electrical device} Sense 2: {tour, circuit} => {journey, journeying} Sense 3: {circuit} => {path, route, itinerary} Sense 4: {circuit (judicial division)} => {group, grouping} Sense 5: {racing circuit, circuit} => {racetrack, racecourse, raceway, track} Sense 6: {lap, circle, circuit} => {locomotion, travel}  Our algorithm associates circuit 1 (electric circuit) with ODP directories such as business/industries/electronics and electrical/contract manufacturers  486  Santamar´ıa, Gonzalo, and Verdejo  Association of Web Directories with Word Senses  whereas circuit 5 (racing circuit) is tagged with directories such as sports/motorsports/auto racing/tracks sports/equestrian/racing/tracks sports/motorsports/auto racing/formula one Every ODP directory has an associated URL, which contains a description of the directory and a number of Web sites that have been manually listed as pertaining to the directory topic, accompanied by brief descriptions of each site. This information is completed with a list of subdirectories, each containing more Web sites and subdirectories. Finally, some directories also have pointers to the same category in other languages. For instance, the Web page for the directory sports/motorsports/auto racing/tracks can be seen in Figure 1. This directory contains links and descriptions for 846 Web sites organized in 12 subdirectories, a link to a related directory (sports/motorsports/karting/tracks) and a link to the same category in French. The association of word senses with Web directories is related to the assignment of domain labels to WordNet synsets as described in Magnini and Cavaglia (2000), in which WordNet is (manually) enriched with domain categories from the Dewey Decimal Classiﬁcation (DDC). Some clear differences between the two are that directories from the ODP are assigned automatically, are richer and deeper and, more importantly,  Figure 1 Contents of an ODP Web directory associated with circuit 5 (racing circuit). 487  Computational Linguistics  Volume 29, Number 3  come with a large amount of associated information directly retrievable from the Web. DDC categories, on the other hand, are a stable domain characterization compared to Web directories. As WordNet and ODP are both hierarchical structures, connecting them is also related to research in mapping thesauruses for digital libraries, ontologies, and data structures in compatible databases. A salient feature of our task is, however, that we do not intend to map both structures, as they are of a quite different nature (lexicalized English concepts versus topics on the Web). Our goal is rather to associate individual items in a many-to-many fashion. A word sense may be characterized with several Web directories, and a Web directory may be suitable for many word senses. The most direct applications of word sense/Web directory associations are • Clustering of senses with identical or very similar categories. • Reﬁnement of senses into specialized variants (e.g., equestrian circuit and formula one circuit as specializations of racing circuit in the example above). • Extraction of sense-tagged corpora from the Web sites listed under the appropriate directories. In Section 2 we describe the proposed algorithm. In Section 3, we evaluate the precision and recall of the algorithm for the set of nouns used in the Senseval 2 WSD competition. In Section 4, we make a preliminary experiment using the material from ODP directories as training corpora for a supervised WSD system. In section 5, we present the results of applying the algorithm to most WordNet 1.7 nouns. Finally, in Section 6 we draw some conclusions. 2. Algorithm Overall, the system takes a WordNet 1.7 noun as input, generates and submits a set of queries into the ODP, ﬁlters the information obtained from the search engine, and returns a set of ODP directories classiﬁed as (1) pseudo–domain labels for some word sense, (2) noise, and (3) salient noise (i.e., directories that are not suitable for any sense in WordNet but could reveal and characterize a new relevant sense of the noun). In case (1), the WordNet sense ↔ ODP directory association also receives a probability score. A detailed description of the algorithm steps follows. 2.1 Querying ODP Structure For every sense wi of the noun w, a query qi is generated, including w as compulsory term, the synonyms and direct hypernyms of wi as optional terms, and the synonyms of other senses of w as negated (forbidden) terms. These queries are submitted to ODP, and a set of directories is retrieved. For instance, for circuit, the following queries are generated and sent to the ODP search engine:1 q1= [+circuit "electrical circuit" "electric circuit" "electrical device" -tour -"racing circuit" -lap -circle] q2= [+circuit tour journey journeying -"electrical circuit" -"electric circuit" -"electrical device" -"racing circuit" -lap -circle]  
 This article describes a spatial model for matching semantic values between two languages, French and English. Based on semantic similarity links, the model constructs a map that represents a word in the source language. Then the algorithm projects the map values onto a space in the target language. The new space abides by the semantic similarity links speciﬁc to the second language. Then the two maps are projected onto the same plane in order to detect overlapping values. For instructional purposes, the different steps are presented here using a few examples. The entire set of results is available at the following address: http://dico.isc.cnrs.fr. 1. Goals This article presents a spatial model that projects the semantic space of a source language word onto a semantic space in the chosen target language. Although the study presented in this article can be described from various angles, we place it within the framework of artifactual simulations of the translation process, and more speciﬁcally, access to the target language’s lexicon. The model is described as a construction process designed to reproduce cognitive functions and their extensions. Future research will include the study of the psycholinguistic validity of such a spatial representation. Now let us brieﬂy describe the scientiﬁc basis of the study. • Three major areas are generally distinguished in the study of the translation process (see Vinay and Darbelnet [1996]), the lexicon (or the study of notions), sentence generation (putting words together), and the message (which brings communicative factors into play). The ﬁrst area involves choosing the right word, which is usually left up to the intuition and expertise of the translator. Our model deals with accessing the lexicon of the target language starting from a notion in the source language. The utility of this research lies in the fact that different languages break down reality in different ways. • Although the translation process has been mastered by a number of experts, it is usually still dependent upon the utilization of tools like dictionaries. The model proposed here relies on semantic maps and offers an alternative method based on the concepts of lexical access and lexical neighborhood. • The work by Anderson (1983) and Collins and Loftus (1975) on the organization of the lexicon is based on priming and the automatic ∗ UMR 5015 CNRS-Universite´ Lyon I, 67 bd Pinel, F-69 675 Bron Cedex. E-mail:{ploux,ji}@isc.cnrs.fr. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 2  spreading of activation to the prime’s neighboring concepts. As an alternative to these local semantic networks, Masson (1995) proposed a connectionist model that takes into account the subjects’ reaction time during priming experiments (the correspondence is based on the assumption that semantic or phonologic proximity and ease of access are correlated). Rouibah, Ploux, and Ji (2001) showed that experimental data on interactions between phonology and semantics could be simulated by distances on lexical maps. One advantage of this proposal is that experimental and artifactual ﬁndings converge; another is its ability to describe a real lexicon. Although the relevance of our model to the representation of the mental lexicon will not be discussed in this article (attempts to gain insight into this correlation are currently underway in other studies), this point is not unrelated to the suitability of our approach to modeling translation as a cognitive function. 2. Description of the Model No two lexicons are related by a one-to-one correspondence (Abplanalp 1998). In other words, the way words are used to refer to extralinguistic reality varies across languages. Some examples of this are cross-language differences in color naming and, borrowing Chuquet and Paillard’s (1989) English-French examples, differences like: • room: pie`ce, chambre, bureau (or in an abstract domain) • esprit: mind, spirit, wit Certain authors (Abplanalp 1998) insist how impossible it is to translate at the word level and propose recourse to the conceptual level as a theoretical alternative. Concepts are thought to depend on human cognitive abilities that are general and shared by all. Although the correspondence between words and concepts remains a controversial topic of study (Reboul 2000), the concept/word opposition is nevertheless relevant to any model of translation, even an artifactual one like ours. As we shall see, even when heeding the speciﬁc organization and breakdown of each individual language, the matching operation does not take place at the word level but at the substrate level (deﬁned below), where the set of meanings of each word “cuts out” a form. First, we will present the model we devised to describe the organization of languages. Then we will explain the source-to-target spreading method used. 2.1 A Model Based on Semantic Similarity The model was initially developed on the basis of a semantic similarity: synonymy. Note, however, that the data and the model are independent, so this same framework can be used to organize other types of similarity (contextual, phonological [Rouibah, Ploux, and Ji 2001], etc.). Other authors also organize the lexicon or other kinds of knowledge on the basis of similarity. For example, in Edelman’s (1998) spatial model of internal representations of the world’s objects, spatial proximity reﬂects object similarity. WordNet (Fellbaum 1998) and EuroWordNet (Vossen 1998) organize the lexicon conceptually as a network of terms, each of which is associated with a partition into  156  Ploux and Ji  A Model for Matching Semantic Maps  Synsets (a Synset being a small group of synonyms that label a concept). Our model differs from Edelman’s in that it deals with lexical semantics, not perceived objects. It also differs from Miller’s (1990) approach, in three respects: • the grain of the semantic units • the lexical structure generation mode • the resulting geometry and organization Most models1 use separate units to represent words or concepts (symbols, points in a space, nodes on a graph, etc.). Relationships between units are expressed as proximity links (in spatial models) or as arcs between nodes (in networks). Our model is spatial, but it differs from local models in that each term is represented by a region in the space, part of which it shares with other terms. This region is constructed automatically according to lexical similarity links (such as those given by a synonym dictionary). It is not the result of supervised learning, nor is it a manual, ontological description of how the lexicon is organized. The next section will break the semantic-space construction process into steps in presenting the initial data, the granular approach, and the resulting organization. 2.2 Method 2.2.1 Initial Data. Three databases were used: two synonym databases (one containing French terms and one containing English terms) and a translation database (FrenchEnglish, English-French) that maps each term to similar words in the other language. The links between an entry and the terms that follow it were not chosen “by hand.” The data were taken mainly from published dictionaries and thesauruses.2 It is updated and supplemented regularly by the addition of new links between words (synonymy or translation links). The method used to generate the French synonym database (described in detail in Ploux (1997) was applied again to generate the English and translation databases. The ﬁrst step required creating an intermediate database containing the set of all links attested in available work in lexicography. In this preliminary database, a term was deemed similar to another term if at least one lexicographer had established the link. The ﬁnal database was obtained through symmetrization of the links produced in the ﬁrst step. While maintaining the shifts in meaning that occur when there is nontransitivity and that, as we shall see, are essential for developing the model, we created new links to symmetrize any initially one-directional ones.3 Table 1 gives a typical example of the structure of the initial data. Table 2 gives a global evaluation of the number of entries and links in the lexical databases. Note that we are not attempting here to deﬁne the term synonymy. We rely on lexicographic publications, which as Edmonds and Hirst (2002) remarked, “have always treated synonymy  
I start by giving an overview of linguistic aspects of the problems introduced by presuppositional expressions, summarize DRT, and show how BAT accounts for presuppositional expressions (Section 2). I then explain what constitutes a proper representation for presuppositions, give examples of lexical entries for presupposition triggers within a compositional framework, and introduce formal tools required for ∗ Division of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, Scotland UK. E-mail: jbos@cogsci.ed.ac.uk. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 2  resolving presuppositions (Section 3). After presenting this formal machinery, I present an efﬁcient version of the presupposition resolution algorithm in Section 4 and implement the various acceptability constraints imposed by BAT. Finally, in Section 5, I discuss implementational issues and report on the performance of the algorithm against a corpus of route instructions, and I investigate the use of general-purpose ﬁrst-order theorem provers to carry out inference tasks imposed by BAT. 2. Preliminaries To make this article self-contained, I will start by outlining the nature of presuppositions and what kind of problems they impose that need to be solved by any natural language understanding component performing discourse processing. Then I will present BAT and show how it accounts for these problems. Because BAT is presented as an extension of DRT, part of this section will be devoted to summarizing the main features of DRT. 2.1 Introducing Presuppositions Presuppositions are those pieces of information that are taken for granted in a conversation or discourse. For instance, to make sense of (1), (1) Vincent and Jules managed to clean the car. we assume the existence of a car, two persons (named Jules and Vincent), and that the two persons found the car difﬁcult to clean. These implications deviate from ordinary entailments. Note that example (1) entails that the car is clean, whereas example (2), (2) If Vincent and Jules managed to clean the car, Jimmie would feel more comfortable. does not, although example (2) includes some of the implications of example (1), namely, the existence of the two persons (Jules and Vincent) and the car, and that the two persons had difﬁculties in cleaning the car. The propositions that are still implied, even after embedding in the conditional, are called presuppositions. This is the crucial property of presuppositions: They are the result of implications that “survive” under negation and modal operators, in the antecedent of conditionals, and in questions (Van der Sandt 1992). In English, most presuppositions in discourse are lexically driven. This means that there are certain lexical items that give rise to presuppositions, whereas others do not. In example (2), for instance, it is the deﬁnite article the that introduces the presupposition that there is a car and the implicative verb to manage that introduces the presupposition that Jules and Vincent had a hard time cleaning the car. Such expressions are called presupposition triggers, and in the examples that follow, I will underline ones relevant to each example. Presuppositions are an important linguistic device in conversation, because when conveyed in utterances, they put constraints on the discourse context. An appropriate context for example (1) is, for instance, example (3): (3) Jules and Vincent were driving in a car on their way to Jimmie. Due to an accident, it was completely covered with blood, and they had to clean it in short time before Bonnie (Jimmie’s girlfriend) would ﬁnd out. Finally, Vincent and Jules managed to clean the car.  180  Bos  Implementing Binding and Accommodation Theory  Whereas the discourse in example (1), in isolation, presupposes the existence of a car, a context as set up in example (3) does not, for the trivial reason that there is a car introduced in the context itself. So (linguistic) contexts may contain information that “cancels” the presuppositions of a new contribution to the discourse or conversation. Therefore, terminological use in the literature on presupposition includes part-time or elementary presuppositions, to signify that presuppositions, once introduced by the trigger sentence, may be canceled by a context. In contrast, consider the discourse in example (4), which is perceived as slightly odd. This perception of oddness is caused by the incompatibility between the context as set up by the ﬁrst two sentences (namely, that there is a clean car) and the presupposition introduced by the third sentence (that Jules and Vincent had difﬁculties cleaning the car). The concept of incompatibility, or better, inconsistency, plays a central role in BAT, the theory of presuppositions in which I will base the resolution algorithm presented in this article. (4) Jules and Vincent were looking after a car. The car was not dirty at all. But Vincent and Jules managed to clean the car. 2.2 The Binding Problem, the Projection Problem, and Accommodation By and large, there are three important themes related to presupposition: the binding problem, the projection problem, and accommodation. Almost all of the theoretical literature on presuppositions deals with these issues, and any computational account of presupposition has to say something about them. Let’s ﬁrst turn to the presupposition binding problem. An example like (5), (5) A boxer nearly escaped from his apartment.  will clarify what comprises the binding problem. The trigger his induces the presupposition that a male individual has an apartment. However, it does not presuppose that just any male person has an apartment, nor that some boxer or other creature owns an apartment. It is the boxer who escaped who has an apartment. That is, the existentially quantiﬁed noun phrase a boxer ties together two types of information: ordinary asserted information (namely, that a boxer nearly escaped from an apartment) and presuppositional information (the apartment mentioned in the assertion belongs to the boxer mentioned in the assertion). As assertions and presuppositions obey different laws, it is no trivial matter to tie them together, and many accounts of presupposition have been shipwrecked on this rock (Van der Sandt 1992, pages 337–340). The presupposition projection problem manifests itself in complex sentences. Presupposition triggers occurring in complex sentences, such as conditionals or disjunctive sentences, sometimes are projected onto the context, but sometimes disappear. For instance, example (6) (6) If Mia dates Vincent, then her husband is out of town. is a sentence presupposing that Mia has a husband. But the similarly constructed sentence in example (7) does not carry this presupposition: (7) If Mia is married, then her husband is out of town.  181  Computational Linguistics  Volume 29, Number 2  This sentence does not presuppose that Mia has a husband. It is the bringing about of Mia’s marital status in the antecedent of the conditional that neutralizes the presupposition of Mia’s being married. Hence, in complex sentences there is no systematic way for dealing with presupposition triggers, as sometimes subparts of complex sentences carry presuppositions that are canceled in the main sentence. Finally, let us consider presuppositional accommodation, as characterised by Karttunen (1974) and later formalized by Lewis (1979). I believe that accommodation plays a role in two related but different linguistic situations. The ﬁrst of these is one in which presuppositions assert new information to the common ground without violating discourse coherency. The second situation is one best described as a hearer’s discourse “repair strategy.” Examples (8) and (9) illustrate the ﬁrst type: (8) Vincent informed his boss. (9) Butch didn’t realize there was a difference between a tummy and a potbelly. The presuppositions conveyed by these utterances are that Vincent has a boss and that there is a difference between a tummy and a potbelly. Hearers have no problems accommodating these presuppositions into the common ground, even in cases in which the context includes no previous mention of them. Only if the discourse built up so far is incompatible with Vincent’s having a boss (maybe he is a freelancer), then a hearer would probably refuse to accept example (8). But with the absence of information as to whether Vincent has a boss, the hearer adjusts his or her presuppositions to make sense of the new utterance or sentence. This is referred to as presuppositional accommodation. Thus, presuppositions are, under certain circumstances, able to present new information to the discourse. However, the level of acceptance of accommodation differs considerably from context to context and according to the type of trigger used (Beaver 2002) and also depends on whether the hearer has access to context or not. Presuppositions triggered by genitive constructions (as in example (8)) and factives (as in example (9)) are known to accommodate easily. Most other presupposition triggers do not allow accommodation, because doing so would lead to incoherent discourse. Consider the following dialogue between Butch and his girlfriend after Butch has fought a match: (10) Fabian: What about the man you fought? Butch: Floyd retired too. Butch’s utterance in this dialogue presupposes that someone distinct from Floyd retired, a presupposition that is trivially true, as many people have retired already. But spoken without the knowledge that Butch ended his career, example (10) is odd, and a hearer will most likely start a clariﬁcation dialogue in such cases. However, example (10) is completely acceptable when one knows that Butch decided to retire after his ﬁght with Floyd. Nevertheless, although hearing example (10) in an ongoing dialogue without any mention of Butch’s planning to retire will certainly raise some eyebrows, somebody who just joins an ongoing conversation and hears it will probably accommodate the associating presupposition, expecting that one of the topics addressed in this conversation was the retirement of somebody different from Floyd. This is when the other  182  Bos  Implementing Binding and Accommodation Theory  role of presuppositional accommodation comes into play, constituting a situation in which hearers don’t have access to the context and use accommodation as a repair strategy. 2.3 Discourse Representation Theory Presupposition is a genuine discourse phenomenon. It should not come as a surprise that an adequate semantic theory for presuppositions would beneﬁt from a formulation in a dynamic theory of meaning. Indeed, BAT is set in DRT (Kamp 1981; Kamp and Reyle 1993; Van Eijck and Kamp 1997), and because it heavily depends on it, I will brieﬂy summarize the most prominent features of DRT here. DRT is one of several formal semantic frameworks designed to deal with the problems related to discourse anaphora, but it is certainly unrivaled with respect to its impressive coverage of linguistic phenomena. Alternative formalisms for discourse semantics are ﬁle change semantics (Heim 1982) and dynamic predicate logic (Groenendijk and Stokhof 1991). The latter uses the syntax of ordinary ﬁrst-order predicate logic, but with a different “context change potential” semantics, allowing existential quantiﬁers to bind variables outside their syntactic scope. The linguistic phenomena that led to the development of dynamic theories such as DRT were mainly centered on the problems introduced by anaphora and indeﬁnite noun phrases. Because anaphora are able to operate on an intersentential level, the traditional method of assigning closed formulas to sentences caused problems for discourse processing, and one had to resort to a number of ad hoc techniques for constructing the ﬁrst-order formulas for natural language discourses. An appropriate ﬁrstorder logic formula for the sentence A woman snorts would be ∃x(WOMAN(x)∧SNORT(x)), but if one continues the discourse with She collapses, there is a need to alter the formula into something of the form: ∃x(WOMAN(x) ∧ SNORT(x) ∧ COLLAPSE(x)). In other words, one has to extend the scope of the existential quantiﬁer introduced in the translation of the ﬁrst sentence to cover elements introduced in the second sentence. The so-called donkey sentences of Geach (donkeys and farmers were the main characters in the example sentences of Geach, which led to increased study of indeﬁnite noun phrases and pronouns within formal semantics) caused similar compositionality problems. Although indeﬁnite noun phrases normally invoke existential quantiﬁcation, a proper ﬁrst-order translation of Every woman that gets a foot massage enjoys it would result in a formula in which the variable introduced by the pronoun it is bound by the universal quantiﬁer stemming from a foot massage, as shown by example (11):  (11) ∀x∀y(WOMAN(x) ∧ FOOT-MASSAGE(y) ∧ GET(x, y) → ENJOY(x, y)) DRT deals with these problems by introducing an intermediate level of semantic representation: discourse representation structures (DRSs). DRSs are pairwise structures consisting of a set of discourse referents, which stand for the entities that are introduced in the discourse, and a set of conditions, which describe the properties of these entities. Discourse referents function like quantiﬁers, in that they are able to bind variables appearing in DRS-conditions. However, the quantiﬁcational force of discourse referents depends on their structural embedding. DRSs are recursive structures, and DRSs embedded in the antecedent of an implicational condition give universal quantiﬁcation to their discourse referents, whereas all other contexts assign existential quantiﬁcation (the translation from DRSs to formulas of ﬁrst-order logic given in Section 3 illustrates this nicely).  183  Computational Linguistics  Volume 29, Number 2  Traditionally, a DRS is presented as a boxlike structure, with discourse referents in the top part and conditions in the lower part of the box. The DRS for the example given earlier is shown in example (12).  (12) A woman snorts. She collapses. xy WOMAN(x) SNORT(x) y=x COLLAPSE(y) Here x and y are discourse referents for a woman and she, respectively. The anaphoric link between she and a woman is established by the condition y = x, and illustrates the role of discourse referents: They introduce discourse entities to which pronouns potentially can refer. In other words, discourse referents are candidate antecedents for future anaphoric reference. The key idea underlying DRT is that discourse referents appearing in embedded DRSs are not available as antecedents for pronouns. The internal structure of DRSs plays a central role in determining the possibility of anaphoric links between pronouns and their potential antecedents. Indeﬁnite noun phrases always introduce their discourse referents locally1 and hence are not accessible from outside a negation or implication. This is shown in the following examples, in which pronouns marked with an asterisk have no proper anaphoric antecedent (narrow-scope interpretation of the indeﬁnite noun phrases is assumed in the examples, because in certain circumstances a wide-scope interpretation of indeﬁnite noun phrases allows anaphoric links):  (13) Butch has a valuable watch. He keeps it in his apartment. Butch has no valuable watch. He keeps it∗ in his apartment. If Butch has a valuable watch, he will take care of it. He keeps it∗ in his apartment. Mia ordered a ﬁve dollar shake. Vincent tasted it. Mia didn’t order a ﬁve dollar shake. Vincent tasted it∗. In the utterances of example (13), the discourse referents for a valuable watch and a ﬁve dollar shake are introduced in subordinated DRSs, excluding anaphoric links to pronouns in subsequent sentences. The DRS in example (14) shows how DRT deals with these observations: Because the discourse referent y introduced for a ﬁve dollar shake is part of an embedded DRS (introduced by negation), it is not accessible for u, the referent introduced for the pronoun it:  
University of Brighton  We argue the case for abstract document structure as a separate descriptive level in the analysis and generation of written texts. The purpose of this representation is to mediate between the message of a text (i.e., its discourse structure) and its physical presentation (i.e., its organization into graphical constituents like sections, paragraphs, sentences, bulleted lists, ﬁgures, and footnotes). Abstract document structure can be seen as an extension of Nunberg’s “text-grammar”; it is also closely related to “logical” markup in languages like HTML and LaTEX. We show that by using this intermediate representation, several subtasks in language generation and language understanding can be deﬁned more cleanly. 1. Introduction When language is written, it appears as a collection of words set out on one or more (actual or virtual) pages. In fact, much of what we tend to call “text” has a strong graphical component (Schriver 1997; Scott and Power 2001). Not only are the words often accompanied by conventional graphics such as pictures or diagrams, but they themselves form graphical elements such as titles, headings, chapters, sections, captions, paragraphs, and bulleted lists. The overlay of graphics on text is in many ways equivalent to the overlay of prosody on speech. Just as all speech has prosody (even if it is a monotone), so too do all texts have layout (even if it is simple wrapped format, in a single face and font, and makes rudimentary use of white space). And just as prosody undoubtedly contributes to the meaning of utterances, so too does a text’s graphical presentation contribute to its meaning. However, although there is a long tradition and rich linguistic framework for describing and representing speech prosody (e.g., Halliday 1967; Chomsky and Halle 1968; Crystal 1969; Bolinger 1972; Pierrehumbert 1980; ’t Hart, Collier, and Cohen 1990; Ladd 1996), the same is not true for text layout. Perhaps not surprisingly, therefore, few natural language understanding (NLU) systems use graphical presentational features to aid interpretation, and few natural language generation (NLG) systems attempt to render the output texts in a principled way. Of course, since all texts have a graphical dimension, all NLG systems will, by deﬁnition, produce laid-out texts. In all but a few recent cases (the ICONOCLAST system (Power 2000; Bouayad-Agha, Power, and Scott 2000; Bouayad-Agha, Scott, and Power 2001; Bouayad-Agha 2001) and the DArtbio system (Bateman et al. 2001)), this is achieved by mapping directly from the underlying discourse structure (Arens and ∗ Information Technology Research Institute, University of Brighton, Lewes Road, Brighton BN2 4GJ, UK. Email: {ﬁrstname.lastname}@itri.bton.ac.uk. † Departament de Tecnologia, University Pompeu Fabra, Barcelona, Spain. Email: Nadjet.Bouayad@ tecm.upf.es. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 2  Hovy 1990; DiMarco et al. 1995; Paris et al. 1995; Power and Cavallotto 1996; Lavoie and Rambow 1997; Mittal et al. 1998). In other cases, the text is mapped onto predetermined genre-speciﬁc layout patterns—for example, for verbalizing mathematical proofs (Huang and Fiedler 1997) or producing letters for customers (Coch 1996). If we take, as most do, the level of discourse structure as representative of the underlying message of a text, such systems are subject to a fundamental limitation. Simply put, for each message there will be but one possible form of presentation. As an illustration let us brieﬂy consider the well-known consensus architecture for NLG systems proposed by Reiter (1994). This architecture, based on a survey of NLG systems from the 1980s and early 1990s, takes the form of a “pipeline” in which ﬁve modules are applied in sequence: content determination, sentence planning, surface generation, morphology, and formatting. Sentence planning maps “conceptual structures into linguistic ones . . . grouping information into clauses and sentences” (Reiter 1994, page 164), but formatting (speciﬁed, for example, by LaTEX markup) occurs only in the ﬁnal formatting stage. In consequence, the organization of material into paragraphs, bulleted lists, etc., is considered only after the wording has been ﬁxed. Graphical presentation, however, clearly interacts with wording. For example, the section of a message that, at the level of discourse, is composed of a list relation, will be expressed differently depending on whether, at the presentational level, it is mapped onto a vertical or horizontal list. Consider a simple example like the following, taken from a patient information leaﬂet (PIL): (1) Are you taking any of the following: • Anticoagulants? • Lithium? • Methotrexate? • Any other medicines which your doctor does not know about? (Voltarol leaﬂet, Geigy; from APBI 1997) If the very same content were presented instead as a horizontal list, we would expect to get something like: (2) Are you taking anticoagulants, lithium, methotrexate, or any other medicines which your doctor does not know about? Now all the information is packed into one sentence, with some missing and additional words, wildly different punctuation, and less generous use of upper-case letters.1 Mapping directly from discourse structure to graphical presentation during generation therefore limits not only the choice of possible layout, but also the choice of possible wording. There have been some recent attempts to develop NLG systems that generate documents rather than just texts. Instead of producing text plans, they produce document plans. Typically these are the text plans of old (i.e., structures of ordered content elements represented in terms of rhetorical structure theory (Mann and Thompson 1986, 1987)), but extended to include pictures or diagrams as content elements, and with additional annotations for metalevel elements such as paragraph or sentence boundaries. Figure 1 shows the type of document plan proposed by Reiter and Dale (2000).  
 In this article we investigate logical metonymy, that is, constructions in which the argument of a word in syntax appears to be different from that argument in logical form (e.g., enjoy the book means enjoy reading the book, and easy problem means a problem that is easy to solve). The systematic variation in the interpretation of such constructions suggests a rich and complex theory of composition on the syntax/semantics interface. Linguistic accounts of logical metonymy typically fail to describe exhaustively all the possible interpretations, or they don’t rank those interpretations in terms of their likelihood. In view of this, we acquire the meanings of metonymic verbs and adjectives from a large corpus and propose a probabilistic model that provides a ranking on the set of possible interpretations. We identify the interpretations automatically by exploiting the consistent correspondences between surface syntactic cues and meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model’s ranking of meanings correlates reliably with human intuitions. 1. Introduction Much work in lexical semantics has been concerned with accounting for regular polysemy, that is, the regular and predictable sense alternations to which certain classes of words are subject (Apresjan 1973). It has been argued that in some cases, the different interpretations of these words must arise from the interaction between the semantics of the words during syntactic composition, rather than by exhaustively listing all the possible senses of a word in distinct lexical entries (Pustejovsky 1991). The class of phenomena that Pustejovsky (1991, 1995) has called logical metonymy is one such example. In the case of logical metonymy additional meaning arises for particular verb-noun and adjective-noun combinations in a systematic way: the verb (or adjective) semantically selects for an event-type argument, which is a different semantic type from that denoted by the noun. Nevertheless, the value of this event is predictable from the semantics of the noun. An example of verbal logical metonymy is given in (1) and (2): (1a) usually means (1b) and (2a) usually means (2b).  (1) a. b.  Mary ﬁnished the cigarette. Mary ﬁnished smoking the cigarette.  (2) a. b.  Mary ﬁnished her beer. Mary ﬁnished drinking her beer.  ∗ Department of Computer Science, University of Shefﬁeld, Regent Court, 211 Portobello Street, Shefﬁeld S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk. † School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: alex@inf.ed.ac.uk.  c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 2  Note how the events in these examples correspond to the purpose of the object denoted by the noun: the purpose of a cigarette is to smoke it and the purpose of a beer is to drink it. Similarly, (3a) means a problem that is easy to solve, (3b) means a language that is difﬁcult to learn, speak, or write, (3c) means a cook that cooks well, (3d) means a soup that tastes good, (3e) means someone who programmes fast, and (3f) means a plane that ﬂies quickly.  (3) a. b. c. d. e. f.  easy problem difﬁcult language good cook good soup fast programmer fast plane  The interpretations of logical metonymies can typically be rendered with a paraphrase, as we have indicated for the above examples. Verb-nouns are paraphrased with a progressive or inﬁnitive VP that is the complement of the polysemous verb (e.g., smoking in (1b)) and whose object is the NP ﬁguring in the verb-noun combination (e.g., cigarette in (1b)). Adjective-noun combinations are usually paraphrased with a verb modiﬁed by the adjective in question or its corresponding adverb. For example, an easy problem is a problem that is easy to solve or a problem that one can solve easily (see (3a)). Logical metonymy has been extensively studied in the lexical semantics literature. Previous approaches have focused on descriptive (Vendler 1968) or theoretical (Pustejovsky 1991, 1995; Briscoe, Copestake, and Boguraev 1990) accounts, on the linguistic constraints on the phenomenon (Godard and Jayez 1993; Pustejovsky and Bouillon 1995; Copestake and Briscoe 1995; Copestake 2001), and on the inﬂuence of discourse context on the interpretation of metonymies (Briscoe, Copestake, and Boguraev 1990; Lascarides and Copestake 1998; Verspoor 1997). McElree et al. (2001) investigated the on-line processing of metonymic expressions; their results indicate that humans display longer reading times for sentences like (1a) than for sentences like (1b). There are at least two challenges in providing an adequate account of logical metonymy. The ﬁrst concerns semi-productivity: There is a wealth of evidence that metonymic constructions are partially conventionalized, and so resolving metonymy entirely via pragmatic reasoning (e.g., by computing the purpose of the object that is denoted by the noun according to real-world knowledge) will overgenerate the possible interpretations (Hobbs et al. 1993). For example, the logical metonymies in (4) are odd, even though pragmatics suggests an interpretation (because real-world knowledge assigns a purpose to the object denoted by the NP):  (4) a. b. c. d.  ?John enjoyed the dictionary. ?John enjoyed the door. ?John began/enjoyed the highway. ?John began the bridge.  Sentence (4a) is odd because the purpose of dictionaries is to refer to them, or to consult them. These are (pointlike) achievements and cannot easily combine with enjoy, which has to be true of an event with signiﬁcant duration. Domain knowledge assigns doors, highways, and bridges a particular purpose, and so the fact that the sentences in (4b)–(4d) are odd indicates that metonymic interpretations are subject to conventional constraints (Godard and Jayez 1993).  262  Lapata and Lascarides  Logical Metonymy  The second challenge concerns the diversity of possible interpretations of metonymic constructions. This diversity is attested across and within metonymic constructions. Metonymic verbs and adjectives are able to take on different meanings depending on their local context, namely, the noun or noun class they select as objects (in the case of verbs) or modify (in the case of adjectives). Consider the examples in (1), in which the meaning of the verb ﬁnish varies depending on the object it selects. Similarly, the adjective good receives different interpretations when modifying the nouns cook and soup (see (3c) and (3d)). Although we’ve observed that some logical metonymies are odd even though pragmatics suggests an interpretation (e.g., (4c)), Vendler (1968) acknowledges that other logical metonymies have more than one plausible interpretation. In order to account for the meaning of adjective-noun combinations, Vendler (1968, page 92) points out that “in most cases not one verb, but a family of verbs is needed”. For example, fast scientist can mean a scientist who does experiments quickly, publishes quickly, and so on. Vendler (1968) further observes that the noun ﬁguring in an adjective-noun combination is usually the subject or object of the paraphrasing verb. Although fast usually triggers a verb-subject interpretation (see (3e) and (3f)), easy and difﬁcult trigger verbobject interpretations (see (3a) and (3b)). An easy problem is usually a problem that one solves easily (so problem is the object of solve), and a difﬁcult language is a language that one learns, speaks, or writes with difﬁculty (so language is the object of learn, speak, and write). Adjectives like good allow either verb-subject or verb-object interpretations: a good cook is a cook who cooks well, whereas good soup is a soup that tastes good. All of these interpretations of fast scientist, difﬁcult language, or good soup seem highly plausible out of context, though one interpretation may be favored over another in a particular context. In fact, in sufﬁciently rich contexts, pragmatics can even override conventional interpretations: Lascarides and Copestake (1998) suggest that (5c) means (5d) and not (5e):  (5) a. b. c. d. e.  All the ofﬁce personnel took part in the company sports day last week. One of the programmers was a good athlete, but the other was struggling to ﬁnish the courses. The fast programmer came ﬁrst in the 100m. The programmer who runs fast came ﬁrst in the 100m. The programmer who programs fast came ﬁrst in the 100m.  The discourse context can also ameliorate highly marked logical metonymies, such as (4c):  (6) a. b. c. d.  John uses two highways to get to work every morning. He ﬁrst takes H-280 and then H-101. He always enjoys H-280, but the trafﬁc jams on H-101 frustrate him.  Arguably the most inﬂuential account of logical metonymy is Pustejovsky’s (1991, 1995) theory of the generative lexicon. Pustejovsky avoids enumerating the various senses for adjectives like fast and verbs like ﬁnish by exploiting a rich lexical semantics for nouns. The lexical entry for an artifact-denoting noun includes a qualia structure: this speciﬁes key features of the word’s meaning that are in some sense  263  Computational Linguistics  Volume 29, Number 2  derivable from real-world knowledge but are lexicalized so as to inﬂuence conventional processes. The qualia structure includes a telic role (i.e., the purpose of the object denoted by the noun) and an agentive role (i.e., the event that brought the object into existence). Thus the lexical entry for book includes a telic role with a value equivalent to read and an agentive role with a value equivalent to write, whereas for cigarette the telic role is equivalent to smoke and the agentive role is equivalent to roll or manufacture. When ﬁnish combines with an object-denoting NP, a metonymic interpretation is constructed in which the missing information is provided by the qualia structure of the NP. More technically, semantic composition of ﬁnish with cigarette causes the semantic type of the noun to be coerced into its telic event (or its agentive event), and the semantic relation corresponding to the metonymic verb (ﬁnish) predicates over this event. This results in an interpretation of (1a) equivalent to (1b). Verbs like begin and enjoy behave in a similar way. Enjoy the book can mean enjoy reading the book, because of book’s telic role, or enjoy writing the book, because of book’s agentive role. In fact, the agentive reading is less typical for book than the telic one, but for other nouns the opposite is true. For instance, begin the tunnel can mean begin building the tunnel, but the interpretation that is equivalent to begin going through the tunnel is highly marked. There is also variation in the relative likelihood of interpretations among different metonymic verbs. (We will return to this issue shortly.) The adjectivenoun combinations are treated along similar lines. Thus the logical polysemy of words like ﬁnish and fast is not accounted for by exhaustive listing.1 In contrast to the volume of theoretical work on logical metonymy, very little empirical work has tackled the topic. Briscoe et al. (1990) investigate the presence of verbal logical metonymies in naturally occurring text by looking into data extracted from the Lancaster-Oslo/Bergen corpus (LOB, one million words). Verspoor (1997) undertakes a similar study in the British National Corpus (BNC, 100 million words). Both studies investigate how widespread the use of logical metonymy is, and how far the interpretation for metonymic examples can be recovered from the head noun’s qualia structure, assuming one knows what the qualia structure for any given noun is. Neither of these studies is concerned with the automatic generation of interpretations for logical metonymies and the determination of their likelihood. Although conceptually elegant, Pustejovsky’s (1995) theory of the generative lexicon does not aim to provide an exhaustive description of the telic roles that a given noun may have. However, these roles are crucial for interpreting verb-noun and adjective-noun metonymies. In contrast to Vendler (1968), who acknowledges that logical metonymies may trigger more than one interpretation (in other words, that there may be more than one possible event associated with the noun in question), Pustejovsky implicitly assumes that nouns or noun classes have one (perhaps default) telic role without, however, systematically investigating the relative degree of ambiguity of the various cases of logical metonymy (e.g., the out-of-context possible readings for fast scientist suggest that fast scientist exhibits a higher degree of semantic ambiguity than fast plane). One could conceivably represent this by the generality of the semantic type of the telic role in the various nouns (e.g., assign the telic role of scientist a relatively general type of event compared with that for plane). But this simply transfers the problem: The degree of generality in lexical representation is highly idiosyncratic and ideally should be acquired from linguistic evidence; furthermore, for nouns with  
Atkins’s keynote address, entitled “Bilingual Dictionaries: Past, Present and Future,” looks at the various types of information available and needed in various types of bilingual and monolingual dictionaries, categorizes it, and argues for a truly electronic dictionary that can adapt itself to the needs of the “multifarious users.” Atkins identiﬁes the strengths of current dictionaries in their wealth of information, scholarly work, and concern for the needs of the dictionary user. She sees weaknesses in the redundancy, coverage gaps, inﬂexible equivalence and collocational selection, distortion caused by disparate needs of source and target languages and by monolingual infor-  Computational Linguistics  Volume 29, Number 2  mation omitted from bilingual dictionaries, inextensibility of bilingual dictionaries to multilingual dictionaries, lack of integrated thesaural functions, and the user learning curve for dictionary metalanguage. In “Use and Usability of Dictionaries: Common Sense and Context Sensibility?” Krista Varantola discusses the disparate needs of lay dictionary users and language professionals. She suggests adapting frame semantics, as proposed by Fillmore and Atkins (1998), to facilitate tailoring the electronic dictionary to give users what they need in terms they understand, relying less on context-free, impenetrable text deﬁnitions. Alain Duval, in “La me´talangue, un mal ne´cessaire du dictionnaire actif,” the only paper in the volume not in English, addresses the problems of communicating with the user of a bilingual dictionary. The new bilingual dictionary tries to function as an “active dictionary” that supports the user who is trying to generate text in a second language, while still doing the job of the “passive dictionary” that helps the user who is merely trying to understand that language. Duval illustrates the differences between the old and new with examples from several older bilingual dictionaries and points out the advantages of the new approach for users as well as the demands on the user who must understand the expanded metalanguage. In “Word Groups in Bilingual Dictionaries: OHFD and After,” Richard Wakely and Henri Be´joint describe their approach to usage notes in the Oxford-Hachette French Dictionary. They discuss their method of identifying lexical sets exhibiting sufﬁcient size, frequency, and behavior commonality. These sets could then be described once with the entries for each set member pointing to the page containing the usage note. They note the pluses and minuses of this approach in terms of practicality, convenience, and usability. In “Examples and Collocations in the French ‘Dictionnaire de langue,’ ” A. P. Cowie, current editor of the International Journal of Lexicography, looks at the treatment of examples in a number of French monolingual dictionaries, including Dictionnaire du franc¸ais contemporain, Le Petit Robert, Le Grand Robert, and Le Tre´sor. He contrasts their methods of blending examples constructed by lexicographers with quotations, exact or adapted. He contends that “the richness, diversity and ﬁtness for purpose of examples in Le Grand Robert and Le Tre´sor, especially, are among the ﬁnest achievements in modern lexicography.” Juri Apresjan has been a leading ﬁgure in lexicography in the Soviet Union and Russia for over 30 years, since he worked with Igor Mel’cˇuk on the development of the Explanatory-Combinatory Dictionary. More recently he has been head of the major Russian machine translation project. In his paper, “Principles of Systematic Lexicography,” he argues for the importance of building a systematic lexicon that can interact effectively with a system of grammar rules in the ECD tradition, and he sketches a linguistic basis for this effort. Charles Fillmore, the creator of frame semantics and the father of the FrameNet lexical resource (Fillmore and Atkins 1998), discusses the problem of “Lexical Isolates,” lexical items that “appear to be of unique semantic or syntactic type.” He illustrates some of these behaviors with those problem children let alone, mention, else, and ilk. In “Sketching Words,” Adam Kilgarriff and David Tugwell describe their method of identifying English word sketches from a corpus with part-of-speech tags and a shallow parse, producing an automatic summary of a word’s behavior that can assist lexicographers in describing that behavior and can help NLP systems subsequently to perform word sense disambiguation reliably. Each word sketch consists of one of twenty-six word relations, with one, two, or three operands. The salience of a word sketch is deﬁned as a function of mutual information and log frequencies.  318  Book Reviews  In “Good Old-Fashioned Lexicography: Human Judgment and the Limits of Automation,” Michael Rundell, editor-in-chief of the Macmillan English Dictionary, considers whether the advances in automation of dictionary development will lead to the demise of the lexicographer. He argues against this view with several compelling examples, suggesting that each advance identiﬁes new layers of complexity that depend on the lexicographer for analysis. Patrick Hanks, lead editor on a number of Collins and Oxford dictionaries, suggests, in “Mapping Meaning onto Use,” that frame semantics provides a “richer schema for representing meaning than is used in any current dictionary.” He advocates using a syntagmatic organizing principle in adjective and verb dictionary entries “rather than (or rather, in tandem with) perceived meaning.” Gregory Grefenstette is probably best known for his work on cross-language information retrieval and its application to Internet text. Here he presents “The WWW as a Resource for Lexicography” in a wide range of languages and the tools needed to extract lexical information effectively. He argues that it is feasible to port a number of tools such as shallow parsers to other languages, especially those using some variant of the Roman alphabet, and that it is time to get to work on this project, as signiﬁcant amounts of text begin to appear in a number of previously unrepresented languages. Most of the papers in the volume view natural language processing as a tool for building lexicons. In “Lexical Knowledge and Natural Language Processing,” Thierry Fontenelle talks about what is needed in a lexical database to support natural language processing and discusses where that knowledge can be found in existing lexical resources, especially collocational dictionaries, thesauri, and semantic networks. This leads naturally to the problems of representing knowledge about verb alternations and other collocations, using the lexical functions of the ExplanatoryCombinatory Dictionary (Apresjan, Mel’cˇuk, and Zholkovsky 1970) and Fillmore’s frame semantics. Annie Zaenen, principal scientist and area manager for Multilingual Theory and Technology at the Xerox Research Centre in Grenoble and co-author of several books about lexical-functional grammar and natural language understanding, makes a convincing case for a depressing conclusion in her “Musings about the Impossible Electronic Dictionary.” She looks at the complexity and pressures stiﬂing progress in the creation of multifunctional lexicons and concludes that current trends will continue to produce disparate resources for disparate consumption rather than a uniﬁed lexical database. This book is a EURALEX production in every way, and it is certainly a success. Anyone interested in lexicography should read this volume. It might have been even better, however, if the editors had given some of Sue Atkins’s many admirers on other continents a chance to join in. The occasional typographical error should certainly be overlooked in view of the bargain price, which should allow many readers to buy copies of their own.  References Apresjan, Juri D., Igor Mel’cˇuk, and Alexander Zholkovsky. 1970. Semantics and lexicography: Towards a new type of unilingual dictionary. In Ferenc Kiefer, editor, Studies in Syntax and  Semantics, Reidel, Dordrecht, pages 1–33. Fillmore, Charles J. and B. T. S. Atkins. 1998. FrameNet and lexicographic relevance. In Proceedings of the First International Conference on Language Resources and Evaluation, Granada, pages 417–423.  319  Computational Linguistics  Volume 29, Number 2  Woody Haynes is working on problems of word-sense discrimination, which led to his participation in the latest SENSEVAL. Martha Evens, his former thesis advisor and a former president of the Association for Computational Linguistics, is the author of a book on lexicography and the editor of the Cambridge University Press book Relational Models of the Lexicon. Their address is Department of Computer Science, Illinois Institute of Technology, 10 West 31st Street, Chicago, IL 60616; e-mail: skhii@mindspring.com, evens@iit.edu.  320  Book Reviews Multimodality in Language and Speech Systems Bjo¨ rn Granstro¨ m, David House, and Inger Karlsson (editors) (Royal Institute of Technology, Stockholm) Dordrecht: Kluwer Academic Publishers (Text, speech and language technology series, edited by Nancy Ide and Jean Ve´ronis, volume 19), 2002, ix+241 pp; hardbound, ISBN 0-4020-0635-7, $82.00, £56.00, –C89.00 Reviewed by Michael Johnston AT&T Labs—Research Multimodality in Language and Speech Systems is a collection of papers that stem from a summer school on the topic held at the KTH Royal Institute of Technology in Stockholm, Sweden, in July 1999, under the auspices of the European Language and Speech Network (ELSNET). The volume’s chapters address a range of related topics, including taxonomies and descriptive frameworks for analysis and examination of multimodal communication (Allwood, Bernsen), experimental analysis of the relationship between speech and hand gesture (McNeill et al.), audio/visual speech perception (Massaro), multimodality in assistive technology (Edwards), descriptions of implemented systems and architectures that support face-to-face multimodal interaction (Tho´ risson, Granstro¨ m et al.), and an intelligent workspace (Brøndsted et al.). As you might expect, given their origin as summer school presentations, the contributions here primarily do not present new work but rather summarize the authors’ research programs or overviews of subareas of the ﬁeld. As such, the volume is a good introduction to an increasingly important area of speech and language research and provides a solid entry point for more detailed reading. It should be of interest both for use in teaching and for researchers and scholars seeking an introduction to this area. It should be noted that the contributions in this volume focus primarily on face-toface multimodal interaction and do not provide an overview of other areas of multimodal interaction such as pen or voice interfaces to mobile devices. Also, the volume does not provide a detailed overview of computational models of multimodal language understanding and multimodal output generation. Andre´ (2003) provides an overview of these areas and could be used in teaching along with this volume, readings from Maybury and Wahlster (1998) and Cassell et al. (2000) to provide a more complete overview of the issues, theory, and practice of multimodal systems. The chapter by Allwood, “Bodily Communication—Dimensions of Expression and Content,” illustrates how body movements are essential in interactive face-to-face communication and argues for going beyond analysis of signaled, discrete, written symbols to develop a fuller picture of human communication. The article provides an excellent overview of research on bodily communication over the last century and presents a descriptive framework for analysis of multimodal communication. This framework combines Peirce’s division of indexical, iconic, and symbolic information with dimensions of intentionality and awareness (indicate/display/signal). Allwood’s contribution clearly illustrates the complexity of the “simultaneous multidimensional coupling” between multiple media of expression and multiple levels of content in faceto-face communication. This point is highly relevant for computational work, since it 321  Computational Linguistics  Volume 29, Number 2  explains why embodied conversational systems are so challenging to build: Failure to capture this complexity will lead to unnatural and stilted behavior on the part of artiﬁcial-agent communicators. Like Allwood’s, the chapter by Bernsen, “Multimodality in Language and Speech Systems—From Theory to Design Support Tool,” provides a framework that can be used in the analysis of multimodal communication and the design of multimodal interactive systems. Whereas Allwood addresses the complexity of face-to-face communication, Bernsen addresses the broader range of interaction between humans, other humans, and machines, including graphical presentations and haptics. The goal of Bernsen’s research program is to determine the basic properties of input and output modalities and from these to derive a comprehensive, relevant, and intuitive taxonomy of modalities and modality combinations (modality theory) and to use this theory to aid interaction designers in selecting which representational modalities to use for a given task, context, and user. This chapter provides a highly detailed elucidation and exempliﬁcation of a theory and taxonomy of output modalities and brieﬂy describes how this has been used in the development of a hypertext encyclopedic reference tool to aid interaction designers. A number of asymmetries between output modalities and input modalities are addressed but, unlike for output, a comprehensive theory and taxonomy of multimodal input is not yet available. The chapter also summarizes research (Bernsen 1997; Bernsen and Dybkjær 1999) that shows how modality theory accounts for the great majority of claims made in the literature regarding speech functionality. One interesting aspect of Bernsen’s modality theory is that, given the top-down development of the taxonomy from theoretical principles, it enables not just analysis of commonplace modalities, but also exploration of new kinds of modalities and modality combinations. The chapter by McNeill et al., “Dynamic Imagery in Speech and Gesture,” argues that human hand gestures are part of our thinking process and that speech and gesture are ‘co-expressive’: deriving from the same semantic source but able to express different aspects of it. This position is supported by results using the experimental paradigm developed by McNeill, Quek, and colleagues, which combines video-based motion tracking techniques with psycholinguistic analysis of discourse. The chapter presents the experimental method and analysis in detail but provides less detail on the underlying psycholinguistic theory. For this, the reader might want to consult other works (McNeill 1992, 2000). The experimental analysis demonstrates how hand use correlates tightly with the semantic content of discourse. In particular the kind of synchrony (antisymmetry or mirror symmetry) is shown to provide cues for discourse segmentation. Principles are also developed for analysis of the gesture signal, including a ‘dominant motion rule’ used to determine whether small hand movements are signiﬁcant. The chapter by Massaro, “Multimodal Speech Perception: A Paradigm for Speech Science,” presents a very clear overview of work on audio/visual speech perception by Massaro and colleagues. The central tenet of the approach is that when evidence from multiple modes, such as audible and visible speech, are combined, the inﬂuence of one modality is greater to the extent that the other is ambiguous or neutral. This is captured by a formal model, the fuzzy logic model of perception (FLMP). The core of the chapter is the presentation of the results of a series of experiments that validate the FLMP as an accurate description of multimodal perception. The experiments address the combination of audible speech with lip movement, integration of written text and speech, word recognition, combination of paralinguistic and linguistic cues, and the combination of auditory and facial cues in the perception of emotion. The McGurk effect is also addressed. This chapter provides an excellent introduction to  322  Book Reviews the program of research pursued by Massaro and colleagues over the last 20 years and provides an entry point for more detailed reading in various books and articles such as Massaro (1998). Edwards’s chapter, “Multimodal Interaction and People with Disabilities,” provides a clear (and inspiring) overview of the ways multimodal interface technology has been or could be applied to assisting users with sensory disabilities. The chapter starts with a clear presentation of the properties of different sensory channels and their relationship to modalities of communication and goes on to present a series of examples of interfaces that map one mode into another or use a combination of modes in order to assist people with disabilities. The chapter by Tho´ risson, “Natural Turn-Taking Needs No Manual: Computational Theory and Model, from Perception to Action,” addresses the complex problem of modeling turn-taking behavior in multimodal dialog. Starting from literature on human-human interaction, a series of hypotheses are developed regarding the properties of turn-taking behavior. The turn-taking mechanism is characterized as anticipatory, multi-level, highly parallel, and opportunistic. It involves logical combination of multiple sensory features and cues and receives higher (temporal) priority than content analysis and interpretation. Tho´ risson goes on to show how these hypotheses can be captured in a computational model in which interaction processing is split into three cooperating layers (reactive, process control, content) with differing temporal priorities, and he describes the implementation of the model in the Gandalf prototype. This is an interactive guide to the solar system that supports face-to-face multimodal communication with a synthetic character. A great deal of detail on the implementation is provided, though it is quite densely packed, so the reader may also want to consult Tho´ risson (1996; 1999) for a fuller understanding of the approach. The chapter by Granstro¨ m et al., “Speech and Gestures for Talking Faces in Conversational Dialogue Systems,” provides a concise overview of work on audio-visual speech synthesis at KTH. Like Cohen and Massaro (1993), Granstro¨ m et al.’s approach employs direct parameterization of a graphical model of the face (Parke 1982). In addition to presenting their approach to facial animation and audio-visual synthesis, the authors summarize two perceptual experiments. The ﬁrst experiment (Teleface) examines the role of visual synthesis in speech intelligibility and its use as an aid to hearing-impaired individuals. For hearing-impaired subjects, adding a synthetic face in addition to the audio channel was found to be almost as much help as adding the natural face. The second experiment explores the relationship between eyebrow movement and intonational phrasing and prominence. Eyebrow movement was found to serve as an independent cue to prominence. The chapter concludes with a description of ﬁve different experimental dialogue systems that employ the KTH audio-visual synthesizer (Waxholm, Olga, August, AdApt, and a language tutor) and demonstrates the applicability of the technology to a broad range of application domains. The chapter by Brødnsted et al., “Developing Intelligent Multimedia Applications,” describes a platform for building applications that combine speech and vision developed at the University of Aalborg in Denmark. A sample application for providing campus information is presented. The system supports speech input and output, visual input (camera), and visual output (laser pointer). The authors provide an overview of the underlying system architecture, with a brief description of each component and an interesting example of one type of multimodal application. However, this is primarily a system overview and offers little detail on the approach to multimodal language processing and dialog management adopted. 323  Computational Linguistics  Volume 29, Number 2 
Mieko Ogura† Tsurumi University University of California at Berkeley  In this study, optimization models using genetic algorithms (GAs) are proposed to study the conﬁguration of vowels and tone systems. As in previous explanatory models that have been used to study vowel systems, certain criteria, which are assumed to be the principles governing the structure of sound systems, are used to predict optimal vowels and tone systems. In most of the earlier studies only one criterion has been considered. When two criteria are considered, they are often combined into one scalar function. The GA model proposed for the study of tone systems uses a Pareto ranking method that is highly applicable for dealing with optimization problems having multiple criteria. For optimization of tone systems, perceptual contrast and markedness complexity are considered simultaneously. Although the consistency between the predicted systems and the observed systems is not as signiﬁcant as those obtained for vowel systems, further investigation along this line is promising. 1. Introduction Studies of the universal characteristics of sound systems in human languages can be pursued according to two different approaches, an inductive approach and a deductive one. The inductive approach involves analyzing the database built from a survey of a large number of languages to arrive at a list of “universal” features that can be widely observed in the database. The deductive approach hypothesizes a number of principles related to speech production and perception processes and predicts possible systems using these principles. These two approaches, however, are often interwoven. The principles hypothesized by the deductive approach are modiﬁed or falsiﬁed by comparing the predictions with the results from the inductive analysis of real language systems. At the same time, the ultimate aim for inductive analysis is to seek intrinsic mechanisms and principles of human speech to explain the universals found in real systems. For the inductive approach in phonological studies, there are two large-scale databases available. One is the Stanford Phonology Archiving (SPA) Project (Vihman 1977), which initially included 196 languages and was extended to 209 languages in ∗ Department of Electronic Engineering, City University of Hong Kong, Hong Kong. E-mail: jyke@ee. cityu.edu.hk † Linguistics Laboratory, Tsurumi University, Yokohama, Japan; Project on Linguistic Analysis, University of California at Berkeley. E-mail: ogura-m@tsurumi-u.ac.jp ‡ Department of Electronic Engineering, City University of Hong Kong, Hong Kong; Project on Linguistic Analysis, University of California at Berkeley. E-mail: eewsyw@uxmail.cityu.edu.hk c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 1  1978. The other is the Phonological Segment Inventory Database (UPSID) (Maddieson 1984) at the University of California at Los Angeles, which initially included 371 languages and was later extended to 451 languages (Maddieson and Precoda 1990; Ladefoged and Maddieson 1996). Many typological studies have been carried out based on these two databases. For example, in studying vowel systems, Crothers (1978) reported an analysis using the SPA database. Ladefoged and Maddieson (1990) and Schwartz et al. (1997b) reported comprehensive analyses for the vowels systems in UPSID. Along with typological studies of the languages in these databases, explanatory models, which attempt to explore the intrinsic reasons for structures and universals, have also been proposed. In the study of vowel systems, the principle of maximal perceptual contrast has a long tradition in linguistics (Jakobson 1941; Wang 1968). This principle suggests that a vowel system tends to achieve a maximum contrast among the vowels in the system. A number of numerical studies adopting this principle have been proposed (Liljencrants and Lindblom 1972; Crothers 1978; Lindblom 1986). Lindblom (1986) proposed the sufﬁcient perceptual contrast principle, under which more systems are predicted to be consistent with natural systems than is predicted by the maximal perceptual contrast principle. Boe¨, Schwartz, and Valle´e (1994) and Schwartz et al. (1997a) added a new consideration called the focalization principle that is based on the observation that vowels with strong formant convergence would be perceptually preferred. More recently, de Boer (1997, 2000, 2001) proposed a synthesized model in which agents interact with each other through iterative imitation games. With explicit optimization, agents can develop coherent vowel systems that are close to real systems. All of the works cited above are concerned with vowel systems. Far fewer studies are reported on other components of a sound system, including consonants, tones (in tone languages), and pitch accent (in non–tone languages), than on vowels. Lindblom and Maddieson (1988) reported a study on phonetic universals in consonant systems using data from UPSID. They proposed that the structure of consonant systems does not arise from a single principle such as the maximization of perceptual contrast. Instead, articulatory factors interact with perceptual factors. According to their proposal, consonant inventories tend to evolve so as to achieve maximal perceptual distinctiveness at minimum articulatory cost. There are some inductive studies on the universals of tone systems as well. For example, Maddieson (1978) reviewed the phonological universals of tones by analyzing data from SPA. Also, Cheng (1973) reported a detailed analysis of the tone systems in Chinese dialects. We have not, however, found any explanatory models that apply a deductive approach for tone systems in the way that such an approach has been applied for vowel systems. More recently, Redford, Chen, and Miikkulainen (2001) reported their studies on the universal and variations of syllable structures (i.e., the combinations of vowels and consonants). They developed a computational model based on a version of the genetic algorithm (GA) (Holland 1975) to simulate the emergence of syllable systems in a language. A set of functional constraints related to perceptual distinctiveness and articulatory ease are taken into account as optimization objectives. In this study, we report some optimization models using GAs to study optimal vowel and tone systems. In these models, the optimal systems are derived from the models based on various explicit optimization criteria and compared with observed systems. First, in the study of vowel systems, we compare two sets of criteria, one considering only the principle of maximal perceptual contrast (Liljencrants and Lindblom 1972), and the other considering both the intervowel perceptual distance and the intravowel spectral salience, that is, the dispersion-focalization principle proposed by Schwartz et al. (1997a). In the second set of criteria, the two objectives, that is, inter-  2  Ke, Ogura, and Wang  Optimization Models of Sound Systems Using GA  vowel perceptual distance and the intravowel spectral salience, are combined into a scalar function. In comparing our results with those of earlier studies, we ﬁnd that the GA models demonstrate the effectiveness of the GA method in identifying the optimal vowel systems based on the above criteria. Second, we apply the GA method to study tone systems. In our application, two objectives (maximum perceptual contrast and minimum markedness complexity) are taken into account to predict the “optimal” tone systems. Instead of combining the two objectives into one ﬁtness function, we use a multi-objective GA (MOGA) model in which a Pareto ranking method is applied for the ﬁtness function. For comparison, we also try a simple GA model that uses only perceptual distance as the optimization criterion. The predicted systems are compared with the real systems for the two sets of criteria. In the following parts of the article, Section 2 gives a brief introduction to a simple GA and a MOGA. Section 3 reports the simulation we performed for vowel systems and comparisons with previous reports. Section 4 introduces our models for tone systems, together with a new analysis of an available tone systems database. Conclusions and discussion are given in Section 5. 2. Introduction to Genetic Algorithms 2.1 Simple Genetic Algorithm GAs were ﬁrst proposed by John Holland in the 1960s (Holland 1975) and have become widely used in various disciplines. The original goal of Holland’s GAs was to study the phenomena of adaptation formally by importing the mechanisms of natural adaptation into computer simulation models. Most of the current applications of GAs, however, are used for speciﬁc optimization problems in which the focus is on the derivation of optimal solutions to the problem rather than the process of adaptation. The basic idea of GAs is based on “natural selection,” the principle of “survival of the ﬁttest,” which assumes that the individual that is better ﬁtted to a particular environment produces more offspring than others in that environment that are less well suited for it; its “ﬁt” genes are then transmitted to the next generation. A GA operates on a population of chromosomes, each generating a potential solution to the studied problem. The process of a traditional simple GA is as follows: At the beginning of the algorithm, a population is randomly initialized, and the ﬁtness of each chromosome is evaluated according to an objective function (also called a ﬁtness function). A number of chromosomes are selected as parents from the population according to their ﬁtness, and parents then undergo crossover and mutation to produce offspring with certain probabilities. Offspring with better ﬁtness are then inserted into the population, replacing the inferior chromosomes in the previous generation. With this replacement, usually the population size is kept constant. This cycle is repeated for a given number of generations, or stopped when a solution obtained is deemed optimal. This process leads to the evolution of a population in which the individuals are more and more suited to their environment, just as in natural adaptation. Because of its global search mechanism, a GA model usually can ﬁnd global optimal solutions in a more efﬁcient way than traditional optimization methods. 2.2 Multi-objective Genetic Algorithm In a traditional GA, the ﬁtness function deals only with one optimization objective. Many practical problems, however, are concerned with several equally important (and usually conﬂicting) objectives. These types of problems are called multi-objective or multicriteria optimization problems (MOPs) (Stadler 1988).  3  Computational Linguistics  Volume 29, Number 1  Human language is an instance of an MOP. A language system is constrained by many demands and requirements. We can consider the current language system to be the product of an optimization process based on such constraints. The constraints can be divided mainly into three categories—the speaker constraint, the listener constraint, and the learner constraint—which often lead to different directions of development for the system. For example, for a sound system, the requirements that arise from speaking and listening often conﬂict with each other. A sound that is easy for a speaker to produce may not be easy for the listener to perceive. Similarly, perceptually distinctive sounds may be difﬁcult to pronounce. A system with a high perceptual contrast may have a high production cost at the same time, as is the case with the consonant set [ k’ ts E m r ] suggested by Ohala in questioning the effectiveness of the principle of maximum perceptual difference in explaining consonant universals (Lindblom and Maddieson 1988). We can see the effects of such a tug-of-war between conﬂicting requirements in various aspects of a language system. For example, in the perception of tones, a completely level tone is the easiest, from a psychophysical viewpoint, to differentiate from nonlevel tones. It requires much effort on the part of a speaker, however, to produce a perfectly level tone. As a consequence of accommodating a speaker’s effort, the listeners will shift their linguistic perception boundary between level and rising tones away from the psychophysical boundary, to allow the speaker some freedom in articulation (Wang 1976). Also, in syntax, a language with free word order may give the speaker a high degree of ﬂexibility in constructing sentences; however, it places the burden on the listener to ﬁgure out the relationships among the words. This is solved by signaling the roles of words by various case markers. If the case marking system is too complex, however, it will be hard for children to learn as they acquire the language. Therefore there may exist a balance point among the three different constraints involved. The most distinctive characteristic of an MOP is that it does not have one singular optimal solution, but rather a set of nondominated,1 alternative solutions, which is often called the Pareto-optimal set. Recently a set of algorithms, called multi-objective genetic algorithms, have been developed speciﬁcally to solve such multi-objective problems. MOGAs have received much attention, and many scientiﬁc and engineering applications employing them have been reported (Fonseca and Fleming 1998; Van Veldhuizen and Lamont 2000). The simplest and most common way to tackle an MOP is to combine its several objectives into one scalar function as the ﬁtness function. Different objectives in the problem are given different weights based on some a priori knowledge (Stadler 1988). (Early studies on sound system optimization with multiple criteria, such as Redford, Chen, and Miikkulainen [2001] and Schwartz et al. [1997a], adopted this approach.) Such knowledge is very often unavailable, however, and most of the time the weights are chosen by trial and error. Thus the performance of the algorithm usually is sensitive to or biased by the weights assigned to the objectives. Within the GA approach, another method called Pareto ranking is often used in the ﬁtness evaluation. The several objective values of a particular chromosome are maintained as a vector, instead of being combined by means of a scalar function into one single ﬁtness value. The ﬁtness of a chromosome is determined by its ranking  
 We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the ﬁve alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and reﬁnements. These statistical models are compared with two heuristic models based on the Dice coefﬁcient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that reﬁned alignment models with a ﬁrst-order dependence and a fertility model yield signiﬁcantly better results than simple heuristic models. In the Appendix, we present an efﬁcient training algorithm for the alignment models presented. 1. Introduction We address in this article the problem of ﬁnding the word alignment of a bilingual sentence-aligned corpus by using language-independent statistical methods. There is a vast literature on this topic, and many different systems have been suggested to solve this problem. Our work follows and extends the methods introduced by Brown, Della Pietra, Della Pietra, and Mercer (1993) by using reﬁned statistical models for the translation process. The basic idea of this approach is to develop a model of the translation process with the word alignment as a hidden variable of this process, to apply statistical estimation theory to compute the “optimal” model parameters, and to perform alignment search to compute the best word alignment. So far, reﬁned statistical alignment models have in general been rarely used. One reason for this is the high complexity of these models, which makes them difﬁcult to understand, implement, and tune. Instead, heuristic models are usually used. In heuristic models, the word alignments are computed by analyzing some association score metric of a link between a source language word and a target language word. These models are relatively easy to implement. In this article, we focus on consistent statistical alignment models suggested in the literature, but we also describe a heuristic association metric. By providing a detailed description and a systematic evaluation of these alignment models, we give the reader various criteria for deciding which model to use for a given task. ∗ Information Science Institute (USC/ISI), 4029 Via Marina, Suite 1001, Marina del Rey, CA 90292. † Lehrstuhl fu¨ r Informatik VI, Computer Science Department, RWTH Aachen–University of Technology, D-52056 Aachen, Germany. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 1  Figure 1 Example of a word alignment (VERBMOBIL task). We propose to measure the quality of an alignment model by comparing the quality of the most probable alignment, the Viterbi alignment, with a manually produced reference alignment. This has the advantage of enabling an automatic evaluation to be performed. In addition, we shall show that this quality measure is a precise and reliable evaluation criterion that is well suited to guide designing and training statistical alignment models. The software used to train the statistical alignment models described in this article is publicly available (Och 2000). 1.1 Problem Deﬁnition We follow Brown, Della Pietra, Della Pietra, and Mercer (1993) to deﬁne alignment as an object for indicating the corresponding words in a parallel text. Figure 1 shows an example. Very often, it is difﬁcult for a human to judge which words in a given target string correspond to which words in its source string. Especially problematic is the alignment of words within idiomatic expressions, free translations, and missing function words. The problem is that the notion of “correspondence” between words is subjective. It is important to keep this in mind in the evaluation of word alignment quality. We shall deal with this problem in Section 5. The alignment between two word strings can be quite complicated. Often, an alignment includes effects such as reorderings, omissions, insertions, and word-tophrase alignments. Therefore, we need a very general representation of alignment. Formally, we use the following deﬁnition for alignment in this article. We are given a source (French) string f1J = f1, . . . , fj, . . . , fJ and a target language (English) string eI1 = e1, . . . , ei, . . . , eI that have to be aligned. We deﬁne an alignment between the two word strings as a subset of the Cartesian product of the word positions; that is, an 20  Och and Ney  Comparison of Statistical Alignment Models  alignment A is deﬁned as  A ⊆ {(j, i): j = 1, . . . , J; i = 1, . . . , I}  (1)  Modeling the alignment as an arbitrary relation between source and target language positions is quite general. The development of alignment models that are able to deal with this general representation, however, is hard. Typically, the alignment models presented in the literature impose additional constraints on the alignment representation. Typically, the alignment representation is restricted in a way such that each source word is assigned to exactly one target word. Alignment models restricted in this way are similar to the concept of hidden Markov models (HMMs) in speech recognition. The alignment mapping in such models consists of associations j → i = aj from source position j to target position i = aj. The alignment aJ1 = a1, . . . , aj, . . . , aJ may contain alignments aj = 0 with the “empty” word e0 to account for source words that are not aligned with any target word. Constructed in such a way, the alignment is not a relation between source and target language positions, but only a mapping from source to target language positions. In Melamed (2000), a further simpliﬁcation is performed that enforces a one-to-one alignment for nonempty words. This means that the alignment mapping aJ1 must be injective for all word positions aj > 0. Note that many translation phenomena cannot be handled using restricted alignment representations such as this one. Especially, methods such as Melamed’s are in principle not able to achieve a 100% recall. The problem can be reduced through corpus preprocessing steps that perform grouping and splitting of words. Some papers report improvements in the alignment quality of statistical methods when linguistic knowledge is used (Ker and Chang 1997; Huang and Choi 2000). In these methods, the linguistic knowledge is used mainly to ﬁlter out incorrect alignments. In this work, we shall avoid making explicit assumptions concerning the language used. By avoiding these assumptions, we expect our approach to be applicable to almost every language pair. The only assumptions we make are that the parallel text is segmented into aligned sentences and that the sentences are segmented into words. Obviously, there are additional implicit assumptions in the models that are needed to obtain a good alignment quality. For example, in languages with a very rich morphology, such as Finnish, a trivial segmentation produces a high number of words that occur only once, and every learning method suffers from a signiﬁcant data sparseness problem.  1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Uefﬁng, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for reﬁned phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000).  21  Computational Linguistics  Volume 29, Number 1  Another application of word alignments is in the ﬁeld of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers from a language, such as English, for which many tools already exist to languages for which such resources are scarce.  1.3 Overview In Section 2, we review various statistical alignment models and heuristic models. We present a new statistical alignment model, a log-linear combination of the best models of Vogel, Ney, and Tillmann (1996) and Brown, Della Pietra, Della Pietra, and Mercer (1993). In Section 3, we describe the training of the alignment models and present a new training schedule that yields signiﬁcantly better results. In addition, we describe how to deal with overﬁtting, deﬁcient models, and very small or very large training corpora. In Section 4, we present some heuristic methods for improving alignment quality by performing a symmetrization of word alignments. In Section 5, we describe an evaluation methodology for word alignment methods dealing with the ambiguities associated with the word alignment annotation based on generalized precision and recall measures. In Section 6, we present a systematic comparison of the various statistical alignment models with regard to alignment quality and translation quality. We assess the effect of training corpora of various sizes and the use of a conventional bilingual dictionary. In the literature, it is often claimed that the reﬁned alignment models of Brown, Della Pietra, Della Pietra, and Mercer (1993) are not suitable for small corpora because of data sparseness problems. We show that this is not the case if these models are parametrized suitably. In the Appendix, we describe some methods for efﬁcient training of fertility-based alignment models.  2. Review of Alignment Models  2.1 General Approaches We distinguish between two general approaches to computing word alignments: statistical alignment models and heuristic models. In the following, we describe both types of models and compare them from a theoretical viewpoint. The notational convention we employ is as follows. We use the symbol Pr (·) to denote general probability distributions with (almost) no speciﬁc assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·).  2.1.1 Statistical Alignment Models. In statistical machine translation, we try to model the translation probability Pr (f1J | eI1), which describes the relationship between a source language string f1J and a target language string eI1. In (statistical) alignment models Pr (f1J, aJ1 | eI1), a “hidden” alignment aJ1 is introduced that describes a mapping from a source position j to a target position aj. The relationship between the translation model and the alignment model is given by  Pr (f1J | eI1) = Pr (f1J, aJ1 | eI1)  (2)  aJ1  The alignment aJ1 may contain alignments aj = 0 with the empty word e0 to account for source words that are not aligned with any target word. In general, the statistical model depends on a set of unknown parameters θ that is learned from training data. To express the dependence of the model on the parameter  22  Och and Ney  Comparison of Statistical Alignment Models  set, we use the following notation:  Pr (f1J, aJ1 | eI1) = pθ(f1J, aJ1 | eI1)  (3)  The art of statistical modeling is to develop speciﬁc statistical models that capture the relevant properties of the considered problem domain. In our case, the statistical alignment model has to describe the relationship between a source language string and a target language string adequately. To train the unknown parameters θ, we are given a parallel training corpus consisting of S sentence pairs {(fs, es) : s = 1, . . . , S}. For each sentence pair (fs, es), the alignment variable is denoted by a = aJ1. The unknown parameters θ are determined by maximizing the likelihood on the parallel training corpus:  S  θˆ = argmax  pθ(fs, a | es)  (4)  θ  s=1 a  Typically, for the kinds of models we describe here, the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) or some approximate EM algorithm is used to perform this maximization. To avoid a common misunderstanding, however, note that the use of the EM algorithm is not essential for the statistical approach, but only a useful tool for solving this parameter estimation problem. Although for a given sentence pair there is a large number of alignments, we can always ﬁnd a best alignment:  ˆaJ1 = argmax pθˆ(f1J, aJ1 | eI1)  (5)  aJ1  The alignment ˆaJ1 is also called the Viterbi alignment of the sentence pair (f1J, eI1). (For the sake of simplicity, we shall drop the index θ if it is not explicitly needed.) Later in the article, we evaluate the quality of this Viterbi alignment by comparing it to a manually produced reference alignment. The parameters of the statistical alignment models are optimized with respect to a maximum-likelihood criterion, which is not necessarily directly related to alignment quality. Such an approach, however, requires training with manually deﬁned alignments, which is not done in the research presented in this article. Experimental evidence shows (Section 6) that the statistical alignment models using this parameter estimation technique do indeed obtain a good alignment quality. In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel, Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which we call Model 6. All these models use a different decomposition of the probability Pr (f1J, aJ1 | eI1).  2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments use a function of the similarity between the types of the two languages (Smadja, McKeown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently, variations of the Dice coefﬁcient (Dice 1945) are used as this similarity function. For each sentence pair, a matrix including the association scores between every word at every position is then obtained:  dice(i, j)  =  2 · C(ei, fj) C(ei) · C(fj)  (6)  23  Computational Linguistics  Volume 29, Number 1  C(e, f ) denotes the co-occurrence count of e and f in the parallel training corpus. C(e) and C(f ) denote the count of e in the target sentences and the count of f in the source sentences, respectively. From this association score matrix, the word alignment is then obtained by applying suitable heuristics. One method is to choose as alignment aj = i for position j the word with the largest association score:  aj = argmax{dice(i, j)}  (7)  i  A reﬁnement of this method is the competitive linking algorithm (Melamed 2000). In a ﬁrst step, the highest-ranking word position (i, j) is aligned. Then, the corresponding row and column are removed from the association score matrix. This procedure is iteratively repeated until every source or target language word is aligned. The advantage of this approach is that indirect associations (i.e., words that co-occur often but are not translations of each other) occur less often. The resulting alignment contains only one-to-one alignments and typically has a higher precision than the heuristic model deﬁned in equation (7).  2.1.3 A Comparison of Statistical Models and Heuristic Models. The main advantage of the heuristic models is their simplicity. They are very easy to implement and understand. Therefore, variants of the heuristic models described above are widely used in the word alignment literature. One problem with heuristic models is that the use of a speciﬁc similarity function seems to be completely arbitrary. The literature contains a large variety of different scoring functions, some including empirically adjusted parameters. As we show in Section 6, the Dice coefﬁcient results in a worse alignment quality than the statistical models. In our view, the approach of using statistical alignment models is more coherent. The general principle for coming up with an association score between words results from statistical estimation theory, and the parameters of the models are adjusted such that the likelihood of the models on the training corpus is maximized.  2.2 Statistical Alignment Models 2.2.1 Hidden Markov Alignment Model. The alignment model Pr (f1J, aJ1 | eI1) can be structured without loss of generality as follows:  J  Pr (f1J, aJ1 | eI1) = Pr (J | eI1) ·  Pr (fj, aj | f1j−1, aj1−1, eI1)  (8)  j=1  J  = Pr (J | eI1) ·  Pr (aj | f1j−1, aj1−1, eI1) · Pr (fj | f1j−1, aj1, eI1)  (9)  j=1  Using this decomposition, we obtain three different probabilities: a length probability Pr (J | eI1), an alignment probability Pr (aj | f1j−1, aj1−1, eI1) and a lexicon probability Pr (fj | f1j−1, aj1, eI1). In the hidden Markov alignment model, we assume a ﬁrst-order dependence for the alignments aj and that the lexicon probability depends only on the word at position aj:  Pr (aj | f1j−1, aj1−1, eI1) = p(aj | aj−1, I)  (10)  Pr (fj | f1j−1, aj1, eI1) = p(fj | eaj )  (11)  24  Och and Ney  Comparison of Statistical Alignment Models  Later in the article, we describe a reﬁnement with a dependence on eaj−1 in the  alignment model. Putting everything together and assuming a simple length model  Pr (J | eI1) = p(J | I), we obtain the following basic HMM-based decomposition of p(f1J | eI1): J  p(f1J | eI1) = p(J | I) ·  [p(aj | aj−1, I) · p(fj | eaj )]  (12)  aJ1 j=1  with the alignment probability p(i | i , I) and the translation probability p(f | e).  To make the alignment parameters independent of absolute word positions, we  assume that the alignment probabilities p(i | i , I) depend only on the jump width  (i − i ). Using a set of non-negative parameters {c(i − i )}, we can write the alignment  probabilities in the form  p(i | i , I) =  c(i − i )  I i  =1 c(i  −i)  (13)  This form ensures that the alignment probabilities satisfy the normalization constraint for each conditioning word position i , i = 1, . . . , I. This model is also referred to as a homogeneous HMM (Vogel, Ney, and Tillmann 1996). A similar idea was suggested by Dagan, Church, and Gale (1993). In the original formulation of the hidden Markov alignment model, there is no empty word that generates source words having no directly aligned target word. We introduce the empty word by extending the HMM network by I empty words e2I+I 1. The target word ei has a corresponding empty word ei+I (i.e., the position of the empty word encodes the previously visited target word). We enforce the following constraints on the transitions in the HMM network (i ≤ I, i ≤ I) involving the empty word e0:1  p(i + I | i , I) = p0 · δ(i, i )  (14)  p(i + I | i + I, I) = p0 · δ(i, i )  (15)  p(i | i + I, I) = p(i | i , I)  (16)  The parameter p0 is the probability of a transition to the empty word, which has to be optimized on held-out data. In our experiments, we set p0 = 0.2. Whereas the HMM is based on ﬁrst-order dependencies p(i = aj | aj−1, I) for the alignment distribution, Models 1 and 2 use zero-order dependencies p(i = aj | j, I, J):  • Model 1 uses a uniform distribution p(i | j, I, J) = 1/(I + 1):  Pr (f1J, aJ1  |  eI1)  =  p(J | I) (I + 1)J  ·  J  p(fj | eaj )  (17)  j=1  Hence, the word order does not affect the alignment probability. • In Model 2, we obtain  J  Pr (f1J, aJ1 | eI1) = p(J | I) · [p(aj | j, I, J) · p(fj | eaj )]  (18)  j=1  
Sebastiaan van Erk† Eindhoven University of Technology  This article describes a new approach to the generation of referring expressions. We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) There are many attractive and well-understood algorithms for dealing with graph structures (see, e.g., Gibbons [1985], Cormen, Leiserson, and Rivest [1990], or Chartrand and Oellermann [1993]). In this article, we describe a straightforward branch and bound algorithm for ﬁnding the relevant subgraphs in which cost functions are used to guide the search process. (2) By deﬁning different cost functions for the graph perspective, we can simulate (and improve) some of the well-known algorithms for the generation of referring  ∗ Communication and Cognition/Computational Linguistics, Faculty of Arts, Tilburg University, Tilburg, The Netherlands. E-mail: E.J.Krahmer@uvt.nl. † Tijgerstraat 2, NL-5645 CK, Eindhoven, The Netherlands. E-mail: sebster@sebster.com. ‡ Ranonkelstraat 67, NL-5644 LB, Eindhoven, The Netherlands. E-mail: andre@astygian.nl. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 1  expressions mentioned above. This facilitates the formal comparison of these algorithms and makes it easier to transfer results from one algorithm to another. (3) The graph perspective provides a clean solution for some problems that have plagued earlier algorithms. For instance, the generation of relational expressions (i.e., referring expressions that include references to other objects) is enhanced by the fact that both properties and relations are formalized in the same way, namely, as edges in a graph. (4) The combined use of graphs and cost functions paves the way for a natural integration of traditional rule-based approaches to generating referring expressions and more recent statistical approaches, such as Langkilde and Knight (1998) and Malouf (2000), in a single algorithm. The outline of this article is as follows. In Section 2 the content selection problem for generating referring expressions is explained, and some well-known solutions to the problem are discussed. In Section 3, we describe how scenes can be modeled as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem. Section 4 contains a sketch of the basic generation algorithm, which is illustrated with a worked example. In Section 5 various ways to formalize cost functions are discussed and compared. We end with some concluding remarks and a discussion of future research directions in Section 6.  2. Generating Referring Expressions  There are many different algorithms for the generation of referring expressions, each with its own objectives: Some aim at producing the shortest possible description (e.g., Dale’s [1992] full brevity algorithm), others focus on psychological realism (e.g., Dale and Reiter’s [1995] incremental algorithm) or realistic output (e.g., Horacek 1997). The degree of detail in which the various algorithms are described differs considerably. Some algorithms are fully formalized and come with explicit characterizations of their complexity (e.g., Dale and Reiter 1995; van Deemter 2000); others are more conceptual and concentrate on exploring new directions (e.g., Stone and Webber 1998). Despite such differences, most algorithms deal with the same problem deﬁnition. They take as input a single object v (the target object) for which a referring expression is to be generated and a set of objects (the distractors) from which the target object needs to be distinguished (we use the terminology from Dale and Reiter [1995]). The task of the algorithm is to determine which set of properties is needed to single out the target object v from the distractors. This is known as the content determination problem for referring expressions. On the basis of this set of properties a distinguishing description for v can be generated. Most algorithms do not address the surface realization problem (how the selected properties should be realized in natural language) in much detail; it is usually assumed that once the content for a referring expression has been determined, a standard realizer such as KPML (Bateman 1997) or SURGE (Elhaded and Robin 1997) can convert the meaning representation to natural language. Consider the example scene in Figure 1. In this scene, as in any other scene, we see a ﬁnite domain of entities D with properties P. In this particular scene, D = {d1, d2, d3, d4} is the set of entities and P = { dog, cat, brown, black+white, large, small } is the set of properties. A scene is usually represented as a database (or knowledge base) listing the properties of each element in D. Thus:  d1: dog (d1) d2: dog (d2) d3: dog (d3) d4: cat (d4)  small (d1) large (d2) large (d3) small (d4)  brown (d1) brown (d2) black+white (d3) brown (d4)  54  Krahmer, van Erk, and Verleg  Graph-Based Generation  d1  d2  d3  d4  Figure 1 A simple example scene consisting of some domestic animals.  In what is probably the key reference on the topic, Dale and Reiter (1995) describe and discuss a number of algorithms for the generation of referring expressions. One of these is the full brevity algorithm (originally due to Dale 1992). This algorithm ﬁrst tries to generate a distinguishing description for the target object v using one single property. If this fails, it considers all possible combinations of two properties to see if any of these sufﬁces for the generation of a distinguishing description, and so on. It is readily seen that this algorithm will output the shortest possible description, if one exists. Suppose the full brevity algorithm is used to generate a description for d1 in Figure 1. There is no single property that distinguishes the target object d1 from the distractors {d2, d3, d4}. But when considering all pairs of properties the algorithm will ﬁnd that one such pair rules out all distractors, namely, small and dog; “the small dog” is a successful and minimal distinguishing description for d1. Dale and Reiter point out that the full brevity algorithm is both computationally infeasible (NP hard) and psychologically unrealistic. They offer the incremental algorithm as an alternative. The incremental algorithm considers properties for selection in a predetermined order, based on the idea that human speakers and listeners prefer certain kinds of properties (or attributes) when describing objects from a given domain. For instance, when discussing domestic animals, it seems likely that a human speaker would ﬁrst describe an animal by its type (is it a dog? is it a cat?). If that does not sufﬁce, ﬁrst absolute attributes like color are tried, followed by relative ones such as size. In sum: The list of preferred attributes for our example domain would be type, color, size . Essentially, the incremental algorithm iterates through this list, and for each property it encounters, it determines whether adding this property to the properties selected so far would rule out any of the remaining distractors. If so, it is included in the list of selected properties. There is one exception to this general strategy: Type information is always included, even if it rules out no distractors. The algorithm stops when all distractors are ruled out (success) or when the end of the list of preferred attributes is reached (failure). Suppose we apply the incremental algorithm to d1 from Figure 1 with type, color, size as preferred attributes. The type of d1 listed in the database is dog. This property is selected (since type information is always selected). It rules out d4 (which is a cat). Next we consider the color of d1; the animal is brown. This property rules out d3 (which is a black and white dog) and is selected. Finally, we consider the size of our target object, which is small. This properly rules out the remaining distractor d2 (which is a large brown dog) and hence is included as well. At this point, all distractors are ruled out (success!), and the set of selected properties is {dog, brown, small}, which a linguistic realizer might express as “the small brown dog.” This is a successful distinguishing 55  Computational Linguistics  Volume 29, Number 1  ¿  ½  ¾  Figure 2 Another scene: Two dogs and two doghouses (from Krahmer and Theune [2002]).  description but not a minimal one: The property brown is, strictly speaking, made redundant by the later inclusion of the property small. Since there is no backtracking in the incremental algorithm however, every selected property is realized (hence “incremental”). This aspect is largely responsible for the computational efﬁciency of the algorithm (it has a polynomial complexity), but Dale and Reiter (1995, page 248) also claim that it is “psychologically realistic.” They point out that sometimes people may describe an object as “the white bird” even though the simpler “the bird” would have been sufﬁcient (cf. Pechmann [1989]; see, however, Krahmer and Theune [2002] for discussion). Even though there are various useful and interesting algorithms for the generation of referring expressions, a number of open questions remain. Recently there has been an increased interest in statistical approaches to natural language generation. For example, Malouf (2000) has shown that large corpora can be used to determine the order of realization of sequences of prenominal adjectives. It is unclear how such statistical work on generation can be combined with older, rule-based work such as the algorithms just discussed. In addition, many algorithms still have difﬁculties with the generation of relational descriptions (descriptions that include references to other objects to single out the target object from its distractors). To illustrate the problem, consider the scene depicted in Figure 2. In this scene we again see a ﬁnite domain of entities D with certain properties P. Here, D = {d1, d2, d3, d4} is the set of entities, and P = { dog, doghouse, small, large, brown, white } is the set of properties. Clearly no algorithm can generate a distinguishing description referring to d1 on this basis. Intuitively, d1 can be distinguished from d2 only using its relation to the doghouse d3. To facilitate this we extend the scene description with a set of relations R = { left of, right of, contain, in }. A few algorithms have been developed that address the issue of relational descriptions. The earliest is from Dale and Haddock (1992), who offer an extension of the full brevity algorithm. The Dale and Haddock algorithm has a problem with inﬁnite recursions; it may produce descriptions like “the dog in the doghouse that contains a dog that is inside a doghouse. . . .” Dale and Haddock, somewhat ad hoc, solve this problem by stipulating that a property or relation may be used only once. Krahmer and Theune (2002) (see also Theune [2000]) describe an extension of the incremental algorithm that allows for relational descriptions. Their extension suffers from what may be called the problem of forced incrementality: When a ﬁrst relation fails to rule out all remaining distractors, additional relations will be tried incrementally. Although it could be argued that incremental selection of properties is psychologically plausible, it seems less plausible for relations. It is unlikely that someone would describe an 56  Krahmer, van Erk, and Verleg  Graph-Based Generation  chihuahua dog brown small d011 01  next_to left_of right_of  chihuahua dog brown small 0011 d2  contains  next_to  left_of  in  next_to  next_to  next_to right_of  left_of  d3 0011  0101 d4  doghouse  large white  right_of next_to  Figure 3 A graph representation of the scene in Figure 2.  large white doghouse  object as “the dog next to the tree in front of the garage” in a situation in which “the dog in front of the garage” would sufﬁce. As we shall argue, the graph perspective provides a clean solution for these problems. 3. Generating Referring Expressions Using Graphs In the previous section we saw that a scene can be described in terms of a domain of entities D with properties P and relations R. Such a scene can be represented as a labeled directed graph (see, e.g., Wilson [1996] for a gentle introduction or Berge [1985] for a more specialized one). Let L = P ∪ R be the set of labels with P and R disjoint (i.e., P ∩ R = ∅). Then G = VG, EG is a labeled directed graph, where VG ⊆ D is the set of vertices (or nodes) and EG ⊆ VG × L × VG is the set of labeled directed edges (or arcs). Where this can be done without creating confusion, the graph subscript is omitted. Throughout this article we use the following notations. If G = V, E is a graph and e = v, l, w an edge (with l ∈ L), then the extension of G with e, denoted as G + e, is the graph V ∪ {v, w}, E ∪ {e} . Moreover, with EG(v, w) we refer to the set of edges in EG from v to w; that is, EG(v, w) = {e ∈ EG | e = v, l, w , for l ∈ L}. The scene given in Figure 2, for example, can now be represented by the graph in Figure 3. This graph models the respective spatial relations between the two chihuahuas, between the two doghouses, and between each dog and the nearest doghouse. For the sake of transparency we have not modeled the relations between the dogs and the distant doghouses (i.e., between d1 and d4 and between d2 and d3). (It is worth stressing that adding these edges would not result in different outcomes in the discussion below). Note that properties (such as dog) are always modeled as loops, 57  Computational Linguistics  next_to  00110011  0101  Volume 29, Number 1  (i) chihuahua 0101  (ii)  dog 01  in  doghouse 0011  (iii) Figure 4 Some graphs for referring expressions, with circles around the intended referent.  that is, as edges that start and end in the same vertex. Relations may have different start and end vertices, but they do not have to (consider potentially reﬂexive relations such as shave). Finally, note that the graph sometimes contains properties of various levels of speciﬁcity (e.g., chihuahua and dog). This aspect of scene graphs will be further discussed in Section 5. Now the content determination problem for referring expressions can be formulated as a graph construction problem. To decide which information to include in a referring expression for an object v ∈ V, we construct a connected directed labeled graph over the set of labels L and an arbitrary set of vertices, but including v. A graph is connected iff there is a path (a list of vertices in which each vertex has an edge from itself to the next vertex) between each pair of vertices. Informally, we say that a vertex (“the intended referent”) from a graph H refers to a given entity in the scene graph G iff the graph H can be “placed over” the scene graph G in such a way that the vertex being referred to is “placed over” the vertex of the given entity in G and each edge from H with label l can be “placed over” an edge from G with the same label. Furthermore, a vertex-graph pair is distinguishing iff it refers to exactly one vertex in the scene graph. Consider the three vertex-graph pairs in Figure 4, in which circled vertices stand for the intended referent. Graph (i) refers to all vertices of the graph in Figure 3 (every object in the scene is next to some other object), graph (ii) can refer to both d1 and d2, and graph (iii) is distinguishing in that it can refer only to d1. Note that the graphs might be realized as something next to something else, a chihuahua, and the dog in the doghouse, respectively. Here we concentrate on the generation of distinguishing vertex-graph pairs. Formally, the notion that a graph H = VH, EH can be “placed over” another graph G = VG, EG corresponds to the notion of a subgraph isomorphism (see, e.g., 58  Krahmer, van Erk, and Verleg  in  0011  00110011  (i)  dog 01  in  doghouse  0101  dog brown small 01  (ii) in 0101  doghouse white large  Graph-Based Generation  (iii) Figure 5 Three distinguishing vertex-graph pairs referring to d1 in Figure 3.  Read and Corneil [1977] for an overview). H can be “placed over” G iff there exists a subgraph G = VG , EG of G such that H is isomorphic to G . H is isomorphic to G iff there exists a bijection π: VH → VG such that for all vertices v, w ∈ VH and all l ∈ L: (v, l, w) ∈ EH ⇔ (π.v, l, π.w) ∈ EG In words: The bijective function π maps all the vertices in H to corresponding vertices in G in such a way that any edge with label l between vertices v and w in H is matched with an edge with the same label between the G counterparts of v and w (i.e., π.v and π.w, respectively). When H is isomorphic to some subgraph of G by an isomorphism π, we write H π G. Given a graph H and a vertex v in H, and a graph G and a vertex w in G, we deﬁne that the pair (v, H) refers to the pair (w, G) iff H is connected and H π G and π.v = w. Furthermore, (v, H) uniquely refers to (w, G) (i.e., (v, H) is distinguishing) iff (v, H) refers to (w, G) and there is no vertex w in G different from w such that (v, H) refers to (w , G). The problem considered in this article can now be formalized as follows: Given a graph G and a vertex w in G, ﬁnd a pair (v, H) such that (v, H) uniquely refers to (w, G). Consider, for instance, the task of ﬁnding a pair (v, H) that uniquely refers to the vertex labeled d1 in Figure 3. It is easily seen that there are a number of such pairs, three of which are depicted in Figure 5. We would like to have a mechanism that allows us to give certain solutions to this kind of task preference over other solutions. For this purpose we shall use cost functions. In general, a cost function is a function that assigns to each subgraph of a scene graph a non-negative number. As we shall see, by deﬁning cost functions in different ways, we can mimic various algorithms for the generation of referring expressions known from the literature. 59  Computational Linguistics  Volume 29, Number 1  3.1 A Note on the Problem Complexity The basic decision problem for subgraph isomorphism (i.e., testing whether a graph H is isomorphic to a subgraph of G) is known to be NP-complete (see, e.g., Garey and Johnson [1979]). Here we are interested in connected H, but unfortunately that restriction does not reduce the theoretical complexity. Note that this characterization of the worst-case complexity holds for graphs in which all edges have the same label; in that case each edge from H can potentially be matched to any edge from G. The bestcase complexity is given when each edge is uniquely labeled. In practice, the situation will most often be somewhere between these extremes. In general, we can say that the more diverse the labeling of edges in the graph of a particular scene is, the sooner a distinguishing vertex-graph pair will be found. It is worth pointing out that there are various alternatives to full subgraph isomorphism that have a lower complexity. For instance, as soon as an upper bound K is deﬁned on the number of edges in a distinguishing graph, the problem loses its intractability (for relatively small K) and becomes solvable, in the worst case, in polynomial O(nK) time, where n is number of edges in the graph G. Restricting the problem in such a way is rather harmless for our current purposes, as it prohibits the generation only of distinguishing descriptions with more than K properties, and for all practical purposes K can be small (referring expressions usually express a limited number of properties). Deﬁning an upper bound K, however, does have a disadvantage: We lose completeness (see van Deemter [2002]). In particular, the algorithm will fail for objects that can be uniquely described only with K + 1 (or more) edges. Of course, one could argue that in such cases objects should be distinguished using other means (e.g., by pointing). Nevertheless, it is worthwhile to look for classes of graphs for which the subgraph isomorphism problem can be solved more efﬁciently, without postulating upper bounds. For instance, if G and H are planar (simple) graphs the problem can be solved in time linear in the number of vertices of G (Eppstein 1999). Basically, a planar graph is one that can be drawn on a plane in such a way that there are no crossing edges (thus, for instance, the graph in Figure 3 is planar, as is any graph with only four vertices). In general, there is no a priori reason to assume that our scene representations will be planar. Yet every nonplanar graph can be modiﬁed into a closely related planar one. We brieﬂy address planarization of scene graphs in the Appendix. A ﬁnal alternative is worth mentioning. The general approach to the problem of subgraph isomorphism detection assumes that both graphs are given on-line. For our current purposes, however, it may happen that the scene graph is ﬁxed and known beforehand, and only the referring graph is unknown and given on-line. Messmer and Bunke (1995, 1998) describe a method that converts the known graph (or model graph, as they call it) into a decision tree. At run time, the input graph is classiﬁed by the decision tree, which detects subgraph isomorphisms. The disadvantage of this approach is that the decision tree may contain, in the worst case, an exponential number of nodes. But the main advantage is that the complexity of the new subgraph isomorphism algorithm is only quadratic in the number of vertices of the input referring graph. Note that with this approach we do not lose information from the scene graph, nor do we lose completeness. In sum, the basic approach to subgraph isomorphisms is NP-complete, but there exist various reformulations of the problem that can be solved more efﬁciently. Deciding which (combination) of these is the most suitable in practice, however, is beyond the scope of this article. Finally, it is worth stressing that the NP-completeness is due to the presence of edges representing relations between different vertices. If we re-  60  Krahmer, van Erk, and Verleg  Graph-Based Generation  strict the approach to properties (looping edges), testing for subgraph isomorphisms becomes trivial.  4. A Sketch of a Branch and Bound Generation Algorithm  In this section we give a high-level sketch of the graph-based generation algorithm. The algorithm (called makeReferringExpression) consists of two main components, a subgraph construction algorithm (called ﬁndGraph) and a subgraph isomorphism testing algorithm (called matchGraphs). For expository reasons we do not address optimization strategies (but see Section 6).  4.1 The Basic Idea We assume that a scene graph G = VG, EG is given. The algorithm systematically tries all relevant subgraphs H of the scene graph G by starting with the subgraph containing only the vertex v (the target object) and expanding it recursively by trying to add edges from G that are adjacent to the subgraph H constructed up to that point. In this way we know that the results will be a connected subgraph. We refer to this set of adjacent edges as the H neighbors in G (denoted as G.neighbors(H)). Formally:  G.neighbors(H) =  EG(v, w)  v∈VH w∈VG  The algorithm returns the cheapest (least expensive) distinguishing subgraph H that refers to v, if such a distinguishing graph exists; otherwise it returns the undeﬁned null graph ⊥.  4.2 Cost Functions We use cost functions to guide the search process and to give preference to some solutions over others. If H = VH, EH is a subgraph of G, then the costs of H, denoted as cost(H), can be given by summing over the costs associated with the vertices and edges of H. Formally:  cost(H) = cost(v) + cost(e)  v∈VH  e∈EH  In fact, this is only one possible way to deﬁne a cost function. The only hard requirement cost functions have to fulﬁll is monotonicity. That is, adding an edge e to a graph G should never result in a graph cheaper than G. Formally:  ∀G ⊆ G ∀e ∈ EG: cost(G ) ≤ cost(G + e)  The monotonicity assumption helps reduce the search space, since extensions of subgraphs with a cost greater than the best subgraph found up to that point can safely be ignored. Naturally, the costs of the undeﬁned graph (cost(⊥)) are not deﬁned. It is worth stressing that the cost function is global: It determines the costs of entire graphs. This implies that the cheapest distinguishing graph is not necessarily the smallest distinguishing graph; a graph consisting of two or more edges may be cheaper than a graph containing one expensive edge.  4.3 Worked Example We now illustrate the algorithm with an example. Suppose the scene graph G is as given in Figure 3 and that we want to generate a referring expression for object d1  61  Computational Linguistics  Volume 29, Number 1  makeReferringExpression(v) { bestGraph := ⊥; H := {v}, ∅ ; return ﬁndGraph(v, bestGraph, H); } ﬁndGraph(v, bestGraph, H) { if [bestGraph = ⊥ and cost(bestGraph) ≤ cost(H)] then return bestGraph ﬁ; distractors := { n | n ∈ VG ∧ matchGraphs(v, H, n, G) ∧ n = v}; if distractors = ∅ then return H ﬁ; for each edge e ∈ G.neighbors(H) do I := ﬁndGraph(v, bestGraph, H + e); if [bestGraph = ⊥ or cost(I) ≤ cost(bestGraph)] then bestGraph := I ﬁ; rof; return bestGraph; }  Figure 6 Sketch of the main function (makeReferringExpression) and the subgraph construction function (ﬁndGraph).  in this graph. Let us assume for the sake of illustration that the cost function is deﬁned in such a way that adding a vertex or an edge always costs one point. Thus, for each v ∈ VH and for each e ∈ EH: cost(v) = cost(e) = 1. (In the next section we describe a number of more interesting cost functions and discuss the impact these have on the output of the algorithm.) We call the function makeReferringExpression (given in Figure 6) with d1 as parameter. In this function the variable bestGraph (for the best solution found up to that point) is initialized as the null graph (there is no best solution yet), and the variable H (for the distinguishing subgraph under construction) is initialized as the graph containing only vertex d1 (i.e., (i) in Figure 7). Then the function ﬁndGraph (see also Figure 6) is called, with parameters d1, bestGraph, and H. To begin with, whether a ﬁrst non-null bestGraph has been found is checked and, if one has, whether the costs of H (the graph under construction) are higher than the costs of the bestGraph found up to that point. If the costs of H are higher, it is not worth extending H since, due to the monotonicity constraint, it will never end up being cheaper than the current bestGraph. During the ﬁrst iteration we have no non-null bestGraph, so we continue. Next the set of distractors is calculated. In terms of the graph perspective, this is the set of vertices in the scene graph G (other than the target vertex v) to which the graph H refers. It is easily seen that the initial value of H refers to every vertex in G. Hence, as one would expect, the initial set of distractors is VG\{d1} = {d2, d3, d4}. Then the current set of distractors is checked to determine whether it is empty. If it is, we have managed to ﬁnd a distinguishing graph, which is subsequently stored in the variable bestGraph. In the ﬁrst iteration, this is obviously not the case, and we continue, recursively trying to extend H by adding 62  Krahmer, van Erk, and Verleg H =0101 (i)  chihuahua  H =  00110011  left_of  brown 0011  (ii)  in  H = 0011  0011  (iii) Figure 7 Three values for H in the generation process for d1.  Graph-Based Generation  adjacent (neighboring) edges until either a distinguishing graph has been constructed (all distractors are ruled out) or the costs of H exceed the costs of the bestGraph found so far. While bestGraph is still the null graph, the algorithm continues until H is a distinguishing graph. Which is the ﬁrst distinguishing graph to be found (if more than one exists) depends on the order in which the adjacent edges are tried (see also Section 5.1). Suppose for the sake of argument that the ﬁrst distinguishing graph to be found is (ii) in Figure 7. This graph is returned and stored in bestGraph. The costs associated with this graph are ﬁve points (two vertices and three edges). At this stage in the generation process only graphs with costs lower than ﬁve points are worth investigating. In fact, there are only a few distinguishing graphs that cost less than this. After a number of iterations the algorithm will ﬁnd the cheapest solution (given this particular, simple deﬁnition of the cost function), which is (iii) in Figure 7. That this distinguishing graph does not include type information does not necessarily mean that such information should not be realized. It means only that type information is, strictly speaking, not necessary to distinguish the intended referent from the distractors. We return to this issue in Section 5.2. 4.4 Subgraph Isomorphism Testing Figure 8 sketches the part of the algorithm that tests for subgraph isomorphism, matchGraphs. This function is called each time the distractor set is calculated. It tests whether the pair (v, H) can refer to (w, G), or put differently, it checks whether there exists an isomorphism π such that H π G with π.v = w. The function matchGraphs ﬁrst determines whether the looping edges starting from vertex v match those of w. If EH(v, v) is not a subset of EG(w, w) (e.g., v is a dog and w is a doghouse), we can immediately discard the matching. Otherwise we start with the matching π.v = w and try to expand it recursively. At each recursion step a fresh and as yet unmatched vertex y from VH is selected that is adjacent to one of the vertices in the current domain of π (notated dom(π)). For each y we calculate the set Z of possible vertices in G to which y can be 63  Computational Linguistics  Volume 29, Number 1  matchGraphs(v, H, w, G) { if EH(v, v) ⊆ EG(w, w) then return false ﬁ; π := {v → w} ; Y := H.neighbors(v); return matchHelper(π, Y, G, H); } matchHelper(π, Y, G, H) { if | Dom (π)| = |H| then return true ﬁ; if Y = ∅ then return false ﬁ; choose a fresh, unmatched y from Y; Z := {z ∈ VG | y might be matched to z }; for each z ∈ Z do if z is a valid extension of the mapping then if matchHelper(π ∪ {y → z}, Y, G, H) then return true ﬁ; ﬁ; rof; return false; }  Figure 8 Sketch of the function testing for subgraph isomorphism (matchGraphs).  matched. This set consists of all the vertices in G that have the same looping edges as y and the same edges to and from other vertices in the domain of the current matching function π. Formally:  Z :=  {z | z ∈ VG ∧ EH(y, y) ⊆ EG(z, z)∧ ∀h ∈ H.neighbors(y) ∩ dom(π) : [ EH(y, h) ⊆ EG(z, π.h) ∧ EH(h, y) ⊆ EG(π.h, z) ] }  (The H.neighbors(y) are the vertices in H that are adjacent to y, that is, those vertices that are connected to y by an edge.) The matching can now possibly be extended with π.y = z, for each z ∈ Z. The algorithm then branches over all these possibilities. Once a mapping π has been found that has exactly as many elements as H has vertices, we have found a subgraph isomorphism. If there are still unmatched vertices in H, or if all possible extensions with vertex y have been checked and no matching can be found, the test for subgraph isomorphism has failed.  4.5 A Note on the Implementation The algorithm outlined in Figures 6 and 8 has been implemented in Java 2 (J2SE, version 1.4). The implemented version of the algorithm is actually more efﬁcient than the sketch suggests, because various calculations need not be repeated in each iteration (the set of distractors and the set G.neighbors(H), for example). In addition, the user has the possibility of specifying the cost function in whatever way he or she sees ﬁt. A full-ﬂedged performance analysis of the current implementation is beyond the scope of this article. Such an analysis would be complicated by the fact that there  64  Krahmer, van Erk, and Verleg  Graph-Based Generation  Table 1 Average times needed to ﬁnd the cheapest distinguishing referring graphs for objects in seven test scene graphs of increasing complexity.  Test scene Vertices Edges Average times  
 This article describes a corpus-based investigation of quantiﬁer scope preferences. Following recent work on multimodular grammar frameworks in theoretical linguistics and a long history of combining multiple information sources in natural language processing, scope is treated as a distinct module of grammar from syntax. This module incorporates multiple sources of evidence regarding the most likely scope reading for a sentence and is entirely data-driven. The experiments discussed in this article evaluate the performance of our models in predicting the most likely scope reading for a particular sentence, using Penn Treebank data both with and without syntactic annotation. We wish to focus attention on the issue of determining scope preferences, which has largely been ignored in theoretical linguistics, and to explore different models of the interaction between syntax and quantiﬁer scope. 1. Overview This article addresses the issue of determining the most accessible quantiﬁer scope reading for a sentence. Quantiﬁers are elements of natural and logical languages (such as each, no, and some in English and ∀ and ∃ in predicate calculus) that have certain semantic properties. Loosely speaking, they express that a proposition holds for some proportion of a set of individuals. One peculiarity of these expressions is that there can be semantic differences that depend on the order in which the quantiﬁers are interpreted. These are known as scope differences. (1) Everyone likes two songs on this album. As an example of the sort of interpretive differences we are talking about, consider the sentence in (1). There are two readings of this sentence; which reading is meant depends on which of the two quantiﬁed expressions everyone and two songs on this album takes wide scope. The ﬁrst reading, in which everyone takes wide scope, simply implies that every person has a certain preference, not necessarily related to anyone else’s. This reading can be paraphrased as “Pick any person, and that person will like two songs on this album.” The second reading, in which everyone takes narrow scope, implies that there are two speciﬁc songs on the album of which everyone is fond, say, “Blue Moon” and “My Way.” In theoretical linguistics, attention has been primarily focused on the issue of scope generation. Researchers applying the techniques of quantiﬁer raising and Cooper storage have been concerned mainly with enumerating all of the scope readings for a ∗ Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail: dchiggin@alumni.uchicago.edu. † Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail: j-sadock@uchicago.edu. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 1  sentence that are possible, without regard to their relative likelihood or naturalness. Recently, however, linguists such as Kuno, Takami, and Wu (1999) have begun to turn their attention to scope prediction, or determining the relative accessibility of different scope readings. In computational linguistics, more attention has been paid to the factors that determine scope preferences. Systems such as the SRI Core Language Engine (Moran 1988; Moran and Pereira 1992), LUNAR (Woods 1986), and TEAM (Martin, Appelt, and Pereira 1986) have employed scope critics that use heuristics to decide between alternative scopings. However, the rules that these systems use in making quantiﬁer scope decisions are motivated only by the researchers’ intuitions, and no empirical results have been published regarding their accuracy. In this article, we use the tools of machine learning to construct a data-driven model of quantiﬁer scope preferences. For theoretical linguistics, this model serves as an illustration that Kuno, Takami, and Wu’s approach can capture some of the clearest generalizations about quantiﬁer scoping. For computational linguistics, this article provides a baseline result on the task of scope prediction, with which other scope critics can be compared. In addition, it is the most extensive empirical investigation of which we are aware that collects data of any kind regarding the relative frequency of different quantiﬁer scope readings in English text.1 Section 2 brieﬂy discusses treatments of scoping issues in theoretical linguistics, and Section 3 reviews the computational work that has been done on natural language quantiﬁer scope. In Section 4 we introduce the models that we use to predict quantiﬁer scoping, as well as the data on which they are trained and tested. Section 5 combines the scope model of the previous section with a probabilistic context-free grammar (PCFG) model of syntax and addresses the issue of whether these two modules of grammar ought to be combined in serial, with information from the syntax feeding the quantiﬁer scope module, or in parallel, with each module constraining the structures provided by the other. 2. Approaches to Quantiﬁer Scope in Theoretical Linguistics Most, if not all, linguistic treatments of quantiﬁer scope have closely integrated it with the way in which the syntactic structure of a sentence is built up. Montague (1973) used a syntactic rule to introduce a quantiﬁed expression into a derivation at the point where it was to take scope, whereas generative semantic analyses such as McCawley (1998) represented the scope of quantiﬁcation at deep structure, transformationally lowering quantiﬁers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantiﬁers from their surface positions to their scope positions by means of a quantiﬁer-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantiﬁed elements out of the position in which they are found on the surface and raises them to a higher position that reﬂects their scope. The various incarnations of the strategy that  
 In this article, we describe an efﬁcient beam search algorithm for statistical machine translation based on dynamic programming (DP). The search algorithm uses the translation model presented in Brown et al. (1993). Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efﬁcient search algorithm. Word reordering restrictions especially useful for the translation direction German to English are presented. The restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article. 1. Introduction This article is about a search procedure for statistical machine translation (MT). The task of the search procedure is to ﬁnd the most likely translation given a source sentence and a set of model parameters. Here, we will use a trigram language model and the translation model presented in Brown et al. (1993). Since the number of possible translations of a given source sentence is enormous, we must ﬁnd the best output without actually generating the set of all possible translations; instead we would like to focus on the most likely translation hypotheses during the search process. For this purpose, we present a data-driven beam search algorithm similar to the one used in speech recognition search algorithms (Ney et al. 1992). The major difference between the search problem in speech recognition and statistical MT is that MT must take into account the different word order for the source and the target language, which does not enter into speech recognition. Tillmann, Vogel, Ney, and Zubiaga (1997) proposes a dynamic programming (DP)–based search algorithm for statistical MT that monotonically translates the input sentence from left to right. The word order difference is dealt with using a suitable preprocessing step. Although the resulting search procedure is very fast, the preprocessing is language speciﬁc and requires a lot of manual  ∗ IBM T. J. Watson Research Center, Yorktown Heights, NY 10598. E-mail: ctill@us.ibm.com. The research reported here was carried out while the author was with Lehrstuhl fu¨ r Informatik VI, Computer Science Department, RWTH Aachen. † Lehrstuhl fu¨ r Informatik VI, Computer Science Department, RWTH Aachen, D-52056 Aachen, Germany. E-mail: ney@informatik.rwth-aachen.de. c 2003 Association for Computational Linguistics  Computational Linguistics  Volume 29, Number 1  work. Currently, most search algorithms for statistical MT proposed in the literature are based on the A∗ concept (Nilsson 1971). Here, the word reordering can be easily included in the search procedure, since the input sentence positions can be processed in any order. The work presented in Berger et al. (1996) that is based on the A∗ concept, however, introduces word reordering restrictions in order to reduce the overall search space. The search procedure presented in this article is based on a DP algorithm to solve the traveling-salesman problem (TSP). A data-driven beam search approach is presented on the basis of this DP-based algorithm. The cities in the TSP correspond to source positions of the input sentence. By imposing constraints on the possible word reorderings similar to that described in Berger et al. (1996), the DP-based approach becomes more effective: when the constraints are applied, the number of word reorderings is greatly reduced. The original reordering constraint in Berger et al. (1996) is shown to be a special case of a more general restriction scheme in which the word reordering constraints are expressed in terms of simple combinatorical restrictions on the processed sets of source sentence positions.1 A set of four parameters is given to control the word reordering. Additionally, a set of four states is introduced to deal with grammatical reordering restrictions (e.g., for the translation direction German to English, the word order difference between the two languages is mainly due to the German verb group. In combination with the reordering restrictions, a data-driven beam search organization for the search procedure is proposed. A beam search pruning technique is conceived that jointly processes partial hypotheses according to two criteria: (1) The partial hypotheses cover the same set of source sentence positions, and (2) the partial hypotheses cover sets C of source sentence positions of equal cardinality. A partial hypothesis is said to cover a set of source sentence positions when exactly the positions in the set have already been processed in the search process. To verify the effectiveness of the proposed techniques, we report and analyze results for two translation tasks: the German to English Verbmobil task and French to English Canadian Hansards task. The article is structured as follows. Section 2 gives a short introduction to the translation model used and reports on other approaches to the search problem in statistical MT. In Section 3, a DP-based search approach is presented, along with appropriate pruning techniques that yield an efﬁcient beam search algorithm. Section 4 reports and analyzes translation results for the different translation directions. In Section 5, we conclude with a discussion of the achieved results. 2. Previous Work 2.1 IBM Translation Approach In this article, we use the translation model presented in Brown et al. (1993), and the mathematical notation we use here is taken from that paper as well: a source string f1J = f1 · · · fj · · · fJ is to be translated into a target string eI1 = e1 · · · ei · · · eI. Here, I is the length of the target string, and J is the length of the source string. Among all possible target strings, we will choose the string with the highest probability as given by Bayes’  
In this article we discuss a modular design for weighted deductive parsing by distinguishing between a weighted deduction system, on the one hand, which pertains to the choice of grammatical formalism and parsing strategy, and the algorithm that ﬁnds the derivation with the lowest weight, on the other. The latter is Dijkstra’s algorithm for the shortest-path problem (Dijkstra 1959) as generalized by Knuth (1977) for a problem on grammars. It has been argued by, for example, Backhouse (2001), that this algorithm can be used to solve a wide range of problems on context-free grammars. A brief presentation of a very similar algorithm for weighted deductive parsing has been given before by Eisner (2000, Figure 3.5e). Our presentation contrasts with that of Klein and Manning (2001), who offer an indivisible speciﬁcation for a small collection of parsing strategies for weighted contextfree grammars only, referring to a generalization of Dijkstra’s algorithm to hypergraphs by Gallo et al. (1993). This article also addresses the efﬁciency of Knuth’s algorithm for weighted deductive parsing, relative to the more commonly used algorithm by Viterbi. 2. Weighted Deductive Parsing The use of deduction systems for specifying parsers has been proposed by Shieber, Schabes, and Pereira (1995) and Sikkel (1997). As already remarked by Goodman (1999), deduction systems can also be extended to manipulate weights.1 Here we de- ∗ Faculty of Arts, Humanities Computing, University of Groningen, P.O. Box 716, NL-9700 AS Groningen, The Netherlands. E-mail: markjan@let.rug.nl. Secondary afﬁliation is the German Research Center for Artiﬁcial Intelligence (DFKI). 
For computational linguists, the grammar may also evoke the strong impression that its authors are implicitly operating with a covert but precise set of assumptions about the formal mechanisms that underlie the grammar, and some may be drawn to the project of making these assumptions explicit, perhaps by using the book as the basis for a large-scale computational grammar, very likely one in the tradition of Pullum’s earlier work (Gazdar et al. 1985). Such a project, although very worthwhile, would probably be too long-term for most of us, so we now turn to other ways in which the availability of the grammar may enhance the practice of computational linguistics. One obvious role is as a guide to English grammar for people who build grammatical artifacts. Such artifacts include not only large-scale computational grammars (Grover, Carroll, and Briscoe 1993; Copestake and Flickinger 2000) but also treebanks (Marcus, Santorini, and Marcinkiewicz 1994). It is idle to speculate on whether the Penn Treebank or the Alvey Natural Language Tools would have been signiﬁcantly different if the Cambridge Grammar had been around to inﬂuence them, but it should become a routine part of the training of future grammar writers and treebank annotators that they absorb as much as is feasible of this grammar. This will not be too onerous: Few annotation manuals are as enjoyable to read as this grammar.  Book Review Another role for the grammar is as an organized repository for language data. Although corpora were used in the preparation of the grammar, there are also many constructed examples. Indeed, for system builders, the copious collection of negative examples is perhaps the most signiﬁcant aspect of the publication. They encode signiﬁcant linguistic intuition that is not otherwise available in a single package. 
Disfluent speech adds to the difficulty of processing spoken language utterances. In this paper we concentrate on identifying one disfluency phenomenon: fragmented words. Our data, from the Spoken Dutch Corpus, samples nearly 45,000 sentences of human discourse, ranging from spontaneous chat to media broadcasts. We classify each lexical item in a sentence either as a completely or an incompletely uttered, i.e. fragmented, word. The task is carried out both by the IB 1 and RIPPER machine learning algorithms, trained on a variety of features with an extensive optimization strategy. Our best classifier has a 74.9% F-score, which is a significant improvement over the baseline. We discuss why memory-based learning has more success than rule induction in correctly classifying fragmented words. 
This paper presents the integration of cohesive properties of text with coherence relations, to obtain an adequate representation of text for automatic summarization. A summarizer based on Lexical Chains is enchanced with rhetorical and argumentative structure obtained via Discourse Markers. When evaluated with newspaper corpus, this integration yields only slight improvement in the resulting summaries and cannot beat a dummy baseline consisting of the first sentence in the document. Nevertheless, we argue that this approach relies on basic linguistic mechanisms and is therefore genreindependent. 
This paper presents an exploratory data analysis in lexical acquisition for adjective classes using clustering techniques. From a theoretical point of view, this approach provides large-scale empirical evidence for a sound classification. From a computational point of view, it helps develop a reliable automatic subclassification method. Results show that the features used in theoretical work can be successfully modelled in terms of shallow cues. The resulting clusters parallel to a large extent with proposals in the literature, which indicates that automatic acquisition of adjective classes for large-scale lexicons is possible. 
This paper reports a research effort in Information Extraction, especially in template pattern matching. Our approach uses reach domain knowledge in the football (soccer) area and logical form representation for necessary inferences of facts and templates filling. Our system FRET' (Football Reports Extraction Templates) is compatible to the language-engineering environment GATE and handles its internal representations and some intermediate analysis results. 
This paper explores the possibility of using the paradigm of Dynamic Logic (DL) to formalise information states and update processes on information states. In particular, we present a formalisation of the dialogue gameboard introduced by Jonathan Ginzburg. From a more general point of view, we show that DL is particularly well suited to develop rigorous formal foundations for an approach to dialogue dynamics based on information state updates. 
This paper introduces document normalization, and addresses the issue of whether controlled document authoring systems can be used in a reverse mode to normalize legacy documents. A paradigm for deep content analysis using such a system is proposed, and an architecture for a document normalization system is described. 
We present a prototype system aimed at providing spoken dialogue support for complex procedures aboard the International Space Station. The system allows navigation one line at a time or in larger steps. Other user functions include issuing spoken corrections, requesting images and diagrams, recording voice notes and spoken alarms, and controlling audio volume. 
CarSim is an automatic text-to-scene conversion system. It analyzes written descriptions of car accidents and synthesizes 3D scenes of them. The conversion process consists of two stages. An information extraction module creates a tabular description of the accident and a visual simulator generates and animates the scene. We implemented a first version of CarSim that considered a corpus of texts in French. We redesigned its linguistic modules and its interface and we applied it to texts in English from the National Transportation Safety Board in the United States. 
We give a demonstration of an application of XRCE's controlled text authoring system MDA to biological experiment reports. This work is the result of a collaboration between XRCE's Document Content Models team, CNRS's Institut de Biologic Structurale, and Protein'eXpert, a company specialized in biotechnology based in Grenoble. We start with a brief presentation of the partners involved and their respective goals. We then give some technical background on the MDA system. Some novel features of the application are discussed, in particular how MDA can be used for integrating the formalization of an experimental protocol with its associated textual documentation. 
We demonstrate the production of spoken output with contextually appropriate intonation in the information-state based dialogue system GoDiS. We exploit the context representation in the information state to determine the information structure of system utterances, which we use to control the intonation of synthesized spoken output. 
Ambiguous keyboards provide efficient typing with low motor demands. In our project l concerning the development of a communication aid, we emphasize adaptation with respect to the sensory input. At the same time, we wish to impose individualized language models on the text determination process. UKO–II is an open architecture based on the Emacs text editor with a server/client interface for adaptive language models. Not only the group of motor impaired people but also users of watch–sized devices can profit from this ambiguous typing. 
Suregen-2 applications are intended for use as add-on modules for clinical information systems. Currently, Suregen-2 permits refinement of the predefined medical ontology, specification of text plans and description knowledge for objects of the ontology. It has built-in constructs for referential expressions, aggregation, enumeration and recurrent semantic constellations. A first application built with Suregen-2, which currently supports German only, is in routine use. 
In this demo we will present GATE, an architecture and framework for language engineering, and ANNIE, an information extraction system developed within it. We will demonstrate how ANNIE has been adapted to perform NE recognition in different languages, including Indic and Slavonic languages as well as Western European ones, and how the resources can be reused for new applications and languages. 
We present REGULUS, an Open Source environment which compiles typed unification grammars into context free grammar language models compatible with the Nuance Toolkit. The environment includes a large general unification grammar of English and corpus-based tools for creating efficient domainspecific recognisers from it. We will demo applications built using the system, including a speech translator and a command and control system for a simulated robotic domain, and show how the development environment can be used to edit and extend them. 
The natural language spoken dialogue system AGORA (TID's advanced system of services development) has been developed using a Collaborative Dialogue model with Mixed Initiative and Computational Linguistic models and experiences. Thanks to these technologies, the system is highly flexible and it doesn't need keywords or directed menus. In this demo you will see the multilingual ability and the proacti-vity possibilities of the system. You will also observe a multiservice system and a vocal platform with the last advances in data collection of expert subdialogues. 
The paper describes a cross-lingual document retrieval system in the medical domain that employs a controlled vocabulary (UMLS I ) in constructing an XMLbased intermediary representation into which queries as well as documents are mapped. The system assists in the retrieval of English and German medical scientific abstracts relevant to a German query document (electronic patient record). The modularity of the system allows for deployment in other domains, given appropriate linguistic and semantic resources.  XML-based representation by means of a multilingual medical thesaurus. The controlled vocabulary used, the Metathesaurus (or rather the MeSH3 part of this), is one of the three knowledge sources developed within the UMLS containing semantic information about biomedical concepts, their various names and the specific relationships among them (i.e. broader_term, narrower_term, etc.). In addition we used the UMLS Semantic Network as a further knowledge source, which provides a categorization of the Metathesaurus concepts in semantic types and provides links between these types through relationships that are important for the biomedical domain (i.e. location_of, leads_to, etc.). 2 The MuchMore* Platform  
We present a robust summarisation system developed within the GATE architecture that makes use of robust components for semantic tagging and coreference resolution provided by GATE. Our system combines GATE components with well established statistical techniques developed for the purpose of text summarisation research. The system supports "generic" and query-based summarisation addressing the need for user adaptation.  Here, we present a summarisation system that makes use of robust components for semantic tagging and coreference resolution provided by GATE (Cunningham et al., 2002). Our system combines GATE components with well established statistical techniques developed for the purpose of text summarisation. The result is the sentence extraction system shown in Figure 1, the relevant sentences of the document are highlighted in the GATE user interface. The figure also shows semantic information identified within the document (e.g., named entities). All summarisation components developed as part of this research are made available as a Java Library for research purposes 1 .  
We present our work on information extraction from multiple, multi-lingual sources for the Multimedia Indexing and Searching Environment (MUMIS), a project aiming at developing technology to produce formal annotations about essential events in multimedia programme material. The novelty of our approach consists on the use of a merging or cross-document coreference algorithm that aims at combining the output delivered by the information extraction systems. 
CLaRK is an XML-based software system for corpora development. It incorporates several technologies: XML technology; Un i code ; Regular Cascaded Grammars; Constraints over XML Documents. The basic components of the system are: a tagger, a concordancer, an extractor, a grammar processor, a constraint engine. 
The paper describes a software demo integrating Natural Language Generation (NLG) techniques with recent developments in XML web technology. The NLG techniques include a form of template-based generation, transformation of text plan trees to text specification trees, and a multi-stage pipeline architecture. The web technology includes XSLT transformation processors, an XML database, a Java servlet engine, the Cocoon web publishing framework and a Java speech synthesizer. The software is all free, open-source. 
AnswerBus News Engine' is a question answering system using the contents of CNN Web site2 as its knowledge base. Comparing to other question answering systems including its previous versions, it has a totally independent crawling and indexing system and a fully functioning search engine. Because of its dynamic and continuous indexing, it is possible to answer questions on just-happened facts. Again, it reaches high correct answer rate. In this demonstration we will present the living system as well as its new technical features. Keywords: question answering, QA specific indexing, search engine  called AnswerBus News Engine to automatically answer news related questions. We chose CNN Web site as the knowledge base because it has a good archive of news stories since 1996 and the CNN Web site seems having good reputation on timely updating. The goal of this experiment is to use most techniques used in AnswerBus QA system together with some new techniques, such as QA specific indexing described in [2,3] but not fully implemented in original AnswerBus system, and build a QA system to answer time sensitive questions in the real world. Before building the AnswerBus News Engine, we did another experiment4 ([7]) using part of DUC conference corpus as local archive. The result was exciting. The experimental QA system correctly answered 80% questions designed specially for the local archive. 2 New Features  
Spoken word collections promise access to unique and compelling content, and most of the technology needed to realize that promise is now in place. Decreasing storage costs, increasing network capacity, and the availability of software to encode and exchange digital audio make possible physical access to spoken word collections at a previously unimaginable scale. Effective support for intellectual access — the problem of finding what you are looking for — is much more challenging, however. In this talk I will briefly describe work that has been done on this problem at the Text Retrieval Conferences, the Topic Detection and Tracking evaluations, and in individual research projects around the world. I will then describe a unique resource, a collection of 116,000 hours of oral history interviews recorded in 32 languages in 57 countries that has been assembled by the Survivors of the Shoah Visual History Foundation. Nearly 10,000 hours of this audio has been manually segmented, summarized and indexed, making this an unrivaled resource with which we can explore a broad array of data-driven techniques. My main focus will be to explain how we are leveraging this exceptional resource to develop the ability to index similar materials automatically.  The project we call MALACH (Multilingual Access to Large spoken ArCHives) builds on a long heritage of increasingly demanding applications for speech recognition technology. The accented, emotional and elderly speech in the Shoah Foundation's collection are so challenging that state-of-the-art systems initially yielded a 90% word error rate! We now have speech recognition systems that achieve better than half that error rate for two languages, English and Czech. That's nowhere near good enough to produce readable transcripts, but it is approaching a point where other language technologies can begin to make headway. I'll illustrate that point with our latest results from across the project on speech recognition, natural language processing components, and information retrieval system design. The scope of this one project is breathtaking, directly involving nine research teams from six institutions on two continents (Charles University, IBM T.J. Watson Research Lab, Johns Hopkins University, the Shoah Foundation, the University of Maryland, and the University of West Bohemia), with interests that range from the information needs of historians to the modeling of Czech colloquial pronunciation. Virtually every topic in computational linguistics finds expression in that range. We plan to ultimately build speech recognition systems in at least five languages (adding Russian, Polish and Slovak to what we have now), so morphology and language modeling are critical issues. The diverse range of languages in the collection make  
We present a neural-network-based statistical parser, trained and tested on the Penn Treebank. The neural network is used to estimate the parameters of a generative model of left-corner parsing, and these parameters are used to search for the most probable parse. The parser's performance (88.8% Fmeasure) is within 1% of the best current parsers for this task, despite using a small vocabulary size (512 inputs). Crucial to this success is the neural network architecture's ability to induce a finite representation of the unbounded parse history, and the biasing of this induction in a linguistically appropriate way. 
Automated essay scoring is now an established capability used from elementary school through graduate school for purposes of instruction and assessment. Newer applications provide automated diagnostic feedback about student writing. Feedback includes errors in grammar, usage, and mechanics, comments about writing style, and evaluation of discourse structure. This paper reports on a system that evaluates a characteristic of lower quality essay writing style: repetitious word use. This capability is embedded in a commercial writing assessment application, Criterion sm The system uses a machine-learning approach with word-based features to model repetitious word use in an essay. System performance well exceeds several baseline algorithms. Agreement between the system and a single human judge exceeds agreement between two human judges. 
We present some preliminary results of a Czech-English translation system based on dependency trees. The fully automated process includes: morphological tagging, analytical and tectogrammatical parsing of Czech, tectogrammatical transfer based on lexical substitution using word-to-word translation dictionaries enhanced by the information from the English-Czech parallel corpus of WSJ, and a simple rule-based system for generation from English tectogrammatical representation. In the evaluation part, we compare results of the fully automated and the manually annotated processes of building the tectogrammatical representation.' 
Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence. 
We describe the adaptation to French of a machine-learned sentence realization system called Amalgam that was originally developed to be as language independent as possible and was first implemented for German. We discuss the development of the French implementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic  tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned approaches to sentence realization is that they can easily be adapted to new domains and ideally to new languages merely by retraining The architecture of Amalgam was intended to be languageindependent, although the system has previously only been applied to German sentence realization. Adapting this system to French allows us to assess which aspects of the system are truly language-independent and what must be added in order to account for French. The purpose of this paper is to focus on the adaptation of Amalgam to French. Discussions about the general architecture of the system can be found in Corston-Oliver et al. (2002) and Gamon et al. (2002b). 
When translating from languages with hardly any inflectional morphology like English into morphologically rich languages, the English word forms often do not contain enough information for producing the correct fullform in the target language. We investigate methods for improving the quality of such translations by making use of part-ofspeech information and maximum entropy modeling. Results for translations from English into Spanish and Catalan are presented on the LC-STAR corpus which consists of spontaneously spoken dialogues in the domain of appointment scheduling and travel planning. 
We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used. 
In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information. We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages. 
 Seiichi Yamamoto  Masuzo Yanagida  ATR Spoken Language  Doshisha University  Translation Research Laboratories  
Current paper presents the results of a two-year project during which a consortium of the University of Szeged and the MorphoLogic Ltd. Budapest developed a morpho-syntactically parsed and annotated (disambiguated) corpus for Hungarian. For morpho-syntactic encoding, the Hungarian version of MSD (MorphoSyntactic Description) has been used. The corpus contains texts of five different topic areas: schoolchildren's compositions, fiction, computer-related texts, news, and legal texts. During annotation, linguists have checked the morphosyntactic parsing of each word. Finding part-of-speech tagging (disambiguation) rules by machine learning algorithms was also studied by the researchers of the consortium. Due to the fact that the size of the corpus reaches up to 1 million text words without punctuation characters, it may serve as a reference source for numerous future research applications. The corpus can be obtained freely via Internet for research and educational purposes.  
As the amount of on-line scientific literature in the biomedical domain increases, automatic processing has become a promising approach for accelerating research. We are applying syntactic parsing trained on the general domain to identify proteinprotein interactions. One of the main difficulties obstructing the use of language processing is the prevalence of specialized terminology. Accordingly, we have created a specialized dictionary by compiling on-line glossaries, and have applied it for information extraction. We conducted preliminary experiments on one hundred sentences, and compared the extraction performance when (a) using only a general dictionary and (b) using this plus our specialized dictionary. Contrary to our expectation, using only the general dictionary resulted in better performance (recall 93.0%, precision 91.0%) than with the terminology-based approach (recall 92.9%, precision 89.6%). 
This research note reports on the work in progress which regards automatic transformation of phrase-structure syntactic trees of Arabic into dependency-driven analytical ones. Guidelines for these descriptions have been developed at the Linguistic Data Consortium, University of Pennsylvania, and at the Faculty of Mathematics and Physics and the Faculty of Arts, Charles University in Prague, respectively. The transformation consists of (i) a recursive function translating the topology of a phrase tree into a corresponding dependency tree, and (ii) a procedure assigning analytical functions to the nodes of the dependency tree. Apart from an outline of the annotation schemes and a deeper insight into these procedures, model application of the transformation is given herein. 
This paper presents a multilingual system designed to recognize named entities in a wide variety of languages (currently more than 12 languages are concerned). The system includes original strategies to deal with a wide variety of encoding character sets, analysis strategies and algorithms to process these languages. 
The paper discusses the corpora management system (CMS) design that uses Java and Oracle9i DBMS to support strategic corpora analysis. We present the pilot webbased CMS to support linguists in their daily work. The system offers facilities to assist linguists and internet users as they search for relevant material, and then classify and annotate this material. 
Existing algorithms for generating referential descriptions to sets of objects have serious deficits: while incremental approaches may produce ambiguous and redundant expressions, exhaustive searches are computationally expensive. Mediating between these extreme control regimes, we propose a best-first searching algorithm for uniquely identifying sets of objects. We incorporate linguistically motivated preferences and several techniques to cut down the search space. Preliminary results show the effectiveness of the new algorithm. 
In this paper we present a proposal to extend WordNet-like lexical databases by adding phrasets, i.e. sets of free combinations of words which are recurrently used to express a concept (let's call them recurrent free phrases). Phrasets are a useful source of information for different NLP tasks, and particularly in a multilingual environment to manage lexical gaps. Two experiments are presented to check the possibility of acquiring recurrent free phrases from dictionaries and corpora. 
This paper describes the NECA MNLG; a fully implemented Multimodal Natural Language Generation module. The MNLG is deployed as part of the NECA system which generates dialogues between animated agents. The generation module supports the seamless integration of full grammar rules, templates and canned text. The generator takes input which allows for the specification of syntactic, semantic and pragmatic constraints on the output. 
This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity. 
We describe our investigations in generating textual summaries of physiological time series data to aid medical personnel in monitoring babies in neonatal intensive care units. Our studies suggest that summarization is a communicative task that requires data analysis techniques for determining the content of the summary. We describe a prototype system that summarizes physiological time series. 
This paper describes a system of terminological extraction capable of handling multi-word expressions, using a powerful syntactic parser. The system includes a concordancing tool enabling the user to display the context of the collocation, i.e. the sentence or the whole document where the collocation occurs. Since the corpora are multilingual, the system also offers an alignment mechanism for the corresponding translated documents. 
Within the framework of translation knowledge acquisition from WWW news sites, this paper studies issues on the effect of cross-language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora. We experimentally show that it is quite effective to reduce the candidate bilingual term pairs against which bilingual term correspondences are estimated, in terms of both computational complexity and the performance of precise estimation of bilingual term correspondences. 
We define a back-and-forth translation between Hole Semantics and dominance constraints, two formalisms used in underspecified semantics. There are fundamental differences between the two, but we show that they disappear on practically useful descriptions. Our encoding bridges a gap between two underspecification formalisms, and speeds up the processing of Hole Semantics. 
We use the grammatical relations (GRs) described in Carroll et al. (1998) to compare a number of parsing algorithms A first ranking of the parsers is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences: this required an implementation of GR extraction software for Penn Treebank style parsers. In addition, we perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm. This produces a second ranking of the parsers, and we investigate the number of errors that are caused by the incorrect GRs. 
In this paper, a word alignment approach is presented which is based on a combination of clues. Word alignment clues indicate associations between words and phrases. They can be based on features such as frequency, part-of-speech, phrase type, and the actual wordform strings. Clues can be found by calculating similarity measures or learned from word aligned data. The clue alignment approach, which is proposed in this paper, makes it possible to combine association clues taking different kinds of linguistic information into account. It allows a dynamic tokenization into token units of varying size. The approach has been applied to an English/Swedish parallel text with promising results. 
The Constraint Language for Lambda Structures (CLLS) is an expressive tree description language. It provides a uniform framework for underspecified semantics, covering scope, ellipsis, and anaphora. Efficient algorithms exist for the sublanguage that models scope. But so far no terminating algorithm exists for sublanguages that model ellipsis. We introduce well-nested parallelism constraints and show that they solve this problem. 
This paper provides a method for generating compact and efficient code to implement the enforcement of a description in typed feature logic. It does so by viewing information about types through the course of code generation as modes of instantiation — a generalization of the common practice in logic programming of the hi nary instantiated/variable mode decl arations that advanced Prolog compilers use. Section 1 introduces the description language. Sections 2 and 3 motivate the view of mode and compilation taken here, and outline a mode declaration language for typed feature logic. Sections 4 through 7 then present the compiler. An evaluation on two grammars is presented at the end. 
When machine translation (MT) knowledge is automatically constructed from bilingual corpora, redundant rules are acquired due to translation variety. These rules increase ambiguity or cause incorrect MT results. To overcome this problem, we constrain the sentences used for knowledge extraction to "the appropriate bilingual sentences for the MT." In this paper, we propose a method using translation literalness to select appropriate sentences or phrases. The translation correspondence rate (TCR) is defined as the literalness measure. Based on the TCR, two automatic construction methods are tested. One is to filter the corpus before rule acquisition. The other is to split the acquisition process into two phases, where a bilingual sentence is divided into literal parts and the other parts before different generalizations are applied. The effects are evaluated by the MT quality, and about 4.9% of MT results were improved by the latter method. 
We propose a semantic construction method for Feature-Based Tree Adjoining Grammar which is based on the derived tree, compare it with related proposals and briefly discuss some implementation possibilities. 
Syntactic constraints in Koskenniemi's Finite-State Intersection Grammar (FSIG) are logically less complex than their formalism (Koskenniemi et al., 1992) would suggest: It turns out that although the constraints in Voutilainen's (1994) FSIG description of English make use of several extensions to regular expressions, the description as a whole reduces to a finite combination of union, complement and concatenation. This is an essential improvement to the descriptive complexity of ENGFSIG. The result opens a door for further analysis of logical properties and possible optimizations in the FSIG descriptions. The proof contains a new formula for compiling Koskenniemi's restriction operation without any marker symbols. 
The goal of interactive machine translation is to improve the productivity of human translators. An interactive machine translation system operates as follows: the automatic system proposes a translation. Now, the human user has two options: to accept the suggestion or to correct it. During the post-editing process, the human user is assisted by the interactive system in the following way: the system suggests an extension of the current translation prefix. Then, the user either accepts this extension (completely or partially) or ignores it. The two most important factors of such an interactive system are the quality of the proposed extensions and the response time. Here, we will use a fully fledged translation system to ensure the quality of the proposed extensions. To achieve fast response times, we will use word hypotheses graphs as an efficient search space representation. We will show results of our approach on the Verbmobil task and on the Canadian Hansards task. 
In (Kanazawa, 1998) it was shown that rigid Classical Categorial Grammars are learnable (in the sense of (Gold, 1967)) from strings. Surprisingly there are recent negative results for, among others, rigid associative Lamb ek (L) grammars. In this paper the non-lcarnability of the class of rigid grammars in LP (Associative-Commutative Lambek calculus) and LP0 (same, but allowing the empty sequent in derivations) will be shown. 
Previous work on the induction of selectional preferences has been mainly carried out for English and has concentrated almost exclusively on verbs and their direct objects. In this paper, we focus on class-based models of selectional preferences for German verbs and take into account not only direct objects, but also subjects and prepositional complements. We evaluate model performance against human judgments and show that there is no single method that overall performs best. We explore a variety of parametrizations for our models and demonstrate that model combination enhances agreement with human ratings. 
We develop a new approach to learning phrase translations from parallel corpora, and show that it performs with very high coverage and accuracy in choosing French translations of English named-entity phrases in a test corpus of software manuals. Analysis of a subset of our results suggests that the method should also perform well on more general phrase translation tasks. 
The paper shows how Combinatory Categorial Grammar (CCG) can be adapted to take advantage of the extra resourcesensitivity provided by the Categorial Type Logic framework. The resulting reformulation, Multi-Modal CCG, supports lexically specified control over the applicability of combinatory rules, permitting a universal rule component and shedding the need for language-specific restrictions on rules. We discuss some of the linguistic motivation for these changes, define the Multi-Modal CCG system and demonstrate how it works on some basic examples. We furthermore outline some possible extensions and address computational aspects of Multi-Modal CCG. 
The choice of verb features is crucial for the learning of verb classes. This paper presents clustering experiments on 168 German verbs, which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepositional phrase information and selectional preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification. 
This work studies Named Entity Recognition (NER) for Catalan without making use of annotated resources of this language. The approach presented is based on machine learning techniques and exploits Spanish resources, either by first training models for Spanish and then translating them into Catalan, or by directly training bilingual models. The resulting models are retrained on unlabelled Catalan data using bootstrapping techniques. Exhaustive experimentation has been conducted on real data, showing competitive results for the obtained NER systems. 
We present a new grammar formalism for parsing with freer word-order languages, motivated by recent linguistic research in German and the Slavic languages. Unlike CFGs, these grammars contain two primitive notions of constituency that are used to preserve the semantic or interpretational aspects of phrase structure, while at the same time providing a more efficient backbone for parsing based on word-order and contiguity constraints. A simple parsing algorithm is presented, and compilation of grammars into Constraint Handling Rules is also discussed. 
We develop a general feature space for automatic classification of verbs into lexical semantic classes. Previous work was limited in scope by the need for manual selection of discriminating features, through a linguistic analysis of the target verb classes (Merlo and Stevenson, 2001). We instead analyze the classification structure at a higher level, using the possible defining characteristics of classes as the basis for our feature space. The general feature space achieves reductions in error rates of 42— 69%, on a wider range of classes than investigated previously, with comparable performance to feature sets manually selected for the particular classification tasks. Our results show that the approach is generally applicable, and avoids the need for resource-intensive linguistic analysis for each new task. 
We describe how unknown lexical entries are processed in a unification-based framework with large-coverage grammars and how from their usage lexical entries are extracted. To keep the time and space usage during parsing within bounds, information from external sources like Part of Speech (PoS) taggers and morphological analysers is taken into account when information is constructed for unknown words. 
We describe an implementation integrating a spoken dialogue system with a mobile robot, which the user can direct to specific locations, ask for information about its status, and supply information about its environment. The robot uses an internal map for navigation, and communicates its current orientation and accessible locations to the dialogue system using a topological map as interface. 
This paper presents the first SlovenianGerman and German-Slovenian online dictionary and contains evaluation figures for its Slovenian part. Evaluations are based on coverage of a Slovenian newspaper corpus as well as on user queries. 
The aim of this paper is to show how large-scale (computational) grammars of natural language benefit from an organization of semantics which is based on Minimal Recursion Semantics (MRS; Copestake et al. (1999)). This we are doing by providing an account of valence alternations in German based on MRS, showing how such an account makes a computational grammar more efficient and less complicated for the grammar writer. 
This paper describes a novel approach to generate potential foreign-accented phonetic transcriptions using phonological rewrite rules. For each pair of a native language (Li) and a target language (L2), a set of postlexical rules is designed to transform canonical phonetic dictionaries of L2 into adapted dictionaries for native Li speakers. Some general considerations on the design of such a rule-based system are presented. 
This research note describes the early stages of a project to enhance a monolingual English dictionary database as a resource for computational applications. It considers some of the issues involved in deriving formal lexical data from a natural-language dictionary. 
This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus. 
This paper reports the latest performance of components and features of a project named CorpusCentered Computation (C'3), which targets a translation technology suitable for spoken language translation. C3 places corpora at the center of the technology. Translation knowledge is extracted from corpora by both EBMT and SMT methods, translation quality is gauged by referring to corpora, the best translation among multiple-engine outputs is selected based on corpora and the corpora themselves are paraphrased or filtered by automated processes. 
In this paper, we pursue a multimodular, statistical approach to WH dependencies, using a feedforward network as our modeling tool. The empirical basis of this model and the availability of performance measures for our system address deficiencies in earlier computational work on WH gaps, which require richer sources of semantic and lexical information in order to run. The statistical nature of our models allows them to be simply combined with other modules of grammar, such as a syntactic parser. 
SMT systems rely on sufficient amount of parallel corpora to train the translation model. This paper investigates possibilities to use word-to-word and phrase-to-phrase translations extracted not only from clean parallel corpora but also from noisy comparable corpora. Translation results for a Chinese to English translation task are given. 
We explore learning prepositionalphrase attachment in Dutch, to use it as a filter in prosodic phrasing. From a syntactic treebank of spoken Dutch we extract instances of the attachment of prepositional phrases to either a governing verb or noun. Using cross-validated parameter and feature selection, we train two learning algorithms, TB I and RIPPER, 011 making this distinction, based on unigram and bigram lexical features and a cooccurrence feature derived from WWW counts. We optimize the learning on noun attachment, since in a second stage we use the attachment decision for blocking the incorrect placement of phrase boundaries before prepositional phrases attached to the preceding noun. On noun attachment, IB 1 attains an F-score of 82; RIPPER an F-score of 78. When used as a filter for prosodic phrasing, using attachment decisions from IB 1 yields the best improvement on precision (by six points to 71) on phrase boundary placement. 
To investigate the contributions of taggers or chunkers to the performance of a deep syntactic parser, Weighted Constraint Dependency Grammars have been extended to also take into consideration information from external sources. Using a weak information fusion scheme based on constraint optimization techniques, a parsing accuracy has been achieved which is comparable to other (stochastic) parsers. 
We present a method for computerassisted authorship attribution based on character-level n-gram language models. Our approach is based on simple information theoretic principles, and achieves improved performance across a variety of languages without requiring extensive pre-processing or feature selection. To demonstrate the effectiveness and language independence of our approach, we present experimental results on Greek, English, and Chinese data. We show that our approach achieves state of the art performance in each of these cases. In particular, we obtain a 18% accuracy improvement over the best published results for a Greek data set, while using a far simpler technique than previous investigations. 
Topological Dependency Grammar (TDG) is a lexicalized dependency grammar formalism, able to model languages with a relatively free word order. In such languages, word order variation often has an important function: the realization of information structure. The paper discusses how to integrate information structure into TDG, and presents a constraint-based approach to modelling information structure and the various means to realize it, focusing on (possibly simultaneous use of) word order and tune. 
In this paper we compare two approaches to natural language understanding (NLU). The first approach is derived from the field of statistical machine translation (MT), whereas the other uses the maximum entropy (ME) framework. Starting with an annotated corpus, we describe the problem of NLU as a translation from a source sentence to a formal language target sentence. We mainly focus on the quality of the different alignment and ME models and show that the direct ME approach outperforms the alignment templates method. 
Our goal is to improve the contextual appropriateness of spoken output in a dialogue system. We explore the use of the information state to determine the information structure of system utterances. We concentrate on the realization of information structure by intonation. We present the results of evaluating the contextual appropriateness of varied system output produced with a text-to-speech synthesis system that supports intonation annotation. 
In this paper we introduce a dynamic programming algorithm to perform linear text segmentation by global minimization of a segmentation cost function which consists of: (a) within-segment word similarity and (b) prior information about segment length. The evaluation of the segmentation accuracy of the algorithm on Choi's text collection showed that the algorithm achieves the best segmentation accuracy so far reported in the literature. Keywords: Text Segmentation, Document Retrieval, Information Retrieval, Machine Learning. 
We describe experiments with a Naive Bayes text classifier in the context of anti- spam E-mail filtering, using two different statistical event models: a multi-variate Bernoulli model and a multinomial model. We introduce a family of feature ranking functions for feature selection in the multinomial event model that take account of the word frequency information. We present evaluation results on two publicly available corpora of legitimate and spam E-mails. We find that the multinomial model is less biased towards one class and achieves slightly higher accuracy than the multi-variate Bernoulli model. 
The main aim of this project is to explore, develop and evaluate the contribution of language technologies to the development of WEBCOOP, a system that provides intelligent Cooperative responses to Web queries. Such a system requires the integration of knowledge representation and the use of advanced reasoning procedures. 
In this paper, we describe a method for automatic acquisition of script knowledge from a Japanese text collection. Script knowledge represents a typical sequence of actions that occur in a particular situation. We extracted sequences (pairs) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences (pairs) in terms of the frequency of their occurrence. To extract sequences of actions occurring in time order, we constructed a text collection in which texts describing facts relating to a similar situation were clustered together and arranged in time order. We also describe a preliminary experiment with our acquisition system and discuss the results. 
This paper describes the NECA MNLG; a fully implemented Multimodal Natural Language Generation module. The MNLG is deployed as part of the NECA system which generates dialogues between animated agents. The generation module supports the seamless integration of full grammar rules, templates and canned text. The generator takes input which allows for the specification of syntactic, semantic and pragmatic constraints on the output. 
The paper discusses the corpora management system (CMS) design that uses Java and Oracle9i DBMS to support strategic corpora analysis. We present the pilot webbased CMS to support linguists in their daily work. The system offers facilities to assist linguists and internet users as they search for relevant material, and then classify and annotate this material. 
Even though the question answering (QA) field appeared only in recent years, there are systems for English which obtain good results for opendomain questions. The situation is very different for other languages, mainly due to the lack of NLP resources which are normally used by QA systems. In this paper, we present a project which develops a QA system for Romanian. The challenges we face and decisions we have to make are discussed. 
We present a text-based approach for the automatic indexing and retrieval of digital photographs taken at crime scenes. Our research prototype, SOCIS, goes beyond keyword-based approaches and methods that extract syntactic relations from captions; it relies on advanced Natural Language Processing techniques in order to extract relational facts. These relational facts consist of a "pragmatic relation" and the entities this relation connects (triples of the form: ARG1REL- ARG2). In SOCIS, the triples are used as complex image indexing terms; however, the extraction mechanism is used not only for indexing purposes but also for image retrieval using free text queries. The retrieval mechanism computes similarity scores between querytriples and indexing-triples making use of a domain-specific ontology. 
In this paper we propose computeraided summarisation (CAS) as an alternative approach to automatic summarisation, and present an ongoing project which aims to develop a CAS system. The need for such an alternative approach is justified by the relatively poor performance of fully automatic methods used in summarisation. Our system combines several summarisation methods, allowing the user of the system to interact with their parameters and output in order to improve the quality of the produced summary.  In light of this problem, we propose computeraided summarisation (CAS) as an alternative to automatic summarisation (AS). Whereas AS does not require any human input to produce summaries, we argue that CAS is a more feasible approach as it allows the user to postedit the automatic summaries according to their requirements. In this paper we present an ongoing project which in the process of developing CAS environment. The structure of the paper is as follows: In Section 2 we outline related work. Section 3 discusses the objectives of our research, followed by the features of a CAS prototype in the next section. A discussion of current findings and future plans are presented in Section 5, and the paper finishes with concluding remarks.  
We illustrate how the use of metaphorical views for reasoning with metaphor requires the mapping of information such as event shape, event rate and mental/emotional states from the source domain to the target domain. Such mappings are domain-independent and can be implemented by means of rules we call View Neutral Mapping Adjuncts (VNMAs). We give a list of the main VNMAs that appear to be required, and show how they can be incorporated into a pre-existing system (ATT-Meta) for metaphorical reasoning. 
We propose a new method for detecting errors in "gold-standard" part-ofspeech annotation. The approach locates errors with high precision based on n-grams occurring in the corpus with multiple taggings. Two further techniques, closed-class analysis and finitestate tagging guide patterns, are discussed. The success of the three approaches is illustrated for the Wall Street Journal corpus as part of the Penn Treebank. 
Spoken dialogue systems would be more acceptable if they were able to produce backchannel continuers such as mm-hmm in naturalistic locations during the user's utterances. Using the HCRC Map Task Corpus as our data source, we describe models for predicting these locations using only limited processing and features of the user's speech that are commonly available, and which therefore could be used as a lowcost improvement for current systems. The baseline model inserts continuers after a predetermined number of words. One further model correlates back-channel continuers with pause duration, while a second predicts their occurrence using trigram POS frequencies. Combining these two models gives the best results. 
One of the major challenges in TRECstyle question-answering (QA) is to overcome the mismatch in the lexical representations in the query space and document space. This is particularly severe in QA as exact answers, rather than documents, are required in response to questions. Most current approaches overcome the mismatch problem by employing either data redundancy strategy through the use of Web or linguistic resources. This paper investigates the integration of lexical relations and Web knowledge to tackle this problem. The results obtained on TREC11 QA corpus indicate that our approach is both feasible and effective. 
This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar. 
This work is the first systematic investigation of initiative in human-human tutorial dialogue. We studied initiative management in two dialogue strategies: didactic tutoring and Socratic tutoring. We hypothesized that didactic tutoring would be mostly tutor-initiative while Socratic tutoring would be mixedinitiative, and that more student initiative would lead to more learning (i.e., task success for the tutor). Surprisingly, students had initiative more of the time in the didactic dialogues (21% of the turns) than in the Socratic dialogues (10% of the turns), and there was no direct relationship between student initiative and learning. However, Socratic dialogues were more interactive than didactic dialogues as measured by percentage of tutor utterances that were questions and percentage of words in the dialogue uttered by the student, and interactivity had a positive correlation with learning. 
Research on the discovery of terms from corpora has focused on word sequences whose recurrent occurrence in a corpus is indicative of their terminological status, and has not addressed the issue of discovering terms when data is sparse. This becomes apparent in the case of noun compounding, which is extremely productive: more than half of the candidate compounds extracted from a corpus are attested only once. We show how evidence about established (i.e., frequent) compounds can be used to estimate features that can discriminate rare valid compounds from rare nonce terms in addition to a variety of linguistic features than can be easily gleaned from corpora without relying on parsed text. 
Finite structure query (fsq for short) is a tool for querying syntactically annotated corpora. fsq employs a query language of high expressive power, namely full first order logic. It can be used to query arbitrary finite structures, not just trees. 
We present experimental evidence that providing naive users of a spoken dialogue system with immediate help messages related to their out-of-coverage utterances improves their success in using the system. A grammar-based recognizer and a Statistical Language Model (SLM) recognizer are run simultaneously. If the grammar-based recognizer suceeds, the less accurate SLM recognizer hypothesis is not used. When the grammar-based recognizer fails and the SLM recognizer produces a recognition hypothesis, this result is used by the Targeted Help agent to give the user feedback on what was recognized, a diagnosis of what was problematic about the utterance, and a related in-coverage example. The in-coverage example is intended to encourage alignment between user inputs and the language model of the system. We report on controlled ex-  periments on a spoken dialogue system for command and control of a simulated robotic helicopter. 
Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.  Aktionsplan actionplan  Akti on  plan  action plan  Akt  plan act ion plan  Figure 1: Splitting options for the German word Aktionsplan  
In this paper we describe the X-TRACT workbench, which enables efficient termbased querying against a domain-specific literature corpus. Its main aim is to aid domain specialists in locating and extracting new knowledge from scientific literature corpora. Before querying, a corpus is automatically terminologically analysed by the ATRACT system, which performs terminology recognition based on the C/NCvalue method enhanced by incorporation of term variation handling. The results of terminology processing are annotated in XML, and the produced XML documents are stored in an XML-native database. All corpus retrieval operations are performed against this database using an XML query language. We illustrate the way in which the X-TRACT workbench can be utilised for knowledge discovery, literature mining and conceptual information extraction. 
We describe a domain-independent semantic interpretation architecture suitable for spoken dialogue systems, which uses a decision-list method to effect a transparent combination of rule-based and data-driven approaches. The architecture has been implemented and evaluated in the context of a mediumvocabulary command and control task. 
We propose a new formulation of the PP attachment problem as a 4-way classification which takes into account the argument or adjunct status of the PP. Based on linguistic diagnostics, we train a 4-way classifier that reaches an average accuracy of 73.9% (baseline 66.2%). Compared to a sequence of binary classifiers, the 4-way classifier reaches better performance and individuates a verb's arguments more accurately, thus improving the acquisition of a crucial piece of information for many NLP applications. 
The paper describes ongoing work on the evaluation of methods for extracting collocation candidates from large text corpora. Our research is based on a German treebank corpus used as gold standard. Results are available for adjective+noun pairs, which proved to be a comparatively easy extraction task. We plan to extend the evaluation to other types of collocations (e.g., PP+verb pairs). 
This paper discusses the theoretical and practical concerns in part-of-speech (POS) tagging for Chinese. Unlike other languages such as English, Chinese lacks morphological marking in association with categorial alternations. We consider such categorial fluidity a continuum, and any categorial shift a transition, with special focus on the verb-noun shift. Preliminary observations are reported on this phenomenon from empirical data, and we suggest that POS tagging should not only be theoretically valid but also sufficiently capture the extent of categorial fluidity as reflected by the data. 
This paper presents a multilingual system designed to recognize named entities in a wide variety of languages (currently more than 12 languages are concerned). The system includes original strategies to deal with a wide variety of encoding character sets, analysis strategies and algorithms to process these languages. 
This paper describes a system of terminological extraction capable of handling multi-word expressions, using a powerful syntactic parser. The system includes a concordancing tool enabling the user to display the context of the collocation, i.e. the sentence or the whole document where the collocation occurs. Since the corpora are multilingual, the system also offers an alignment mechanism for the corresponding translated documents. 
The paper addresses the problem of automatic enrichment of a thesaurus by classifying new words into its classes. The proposed classification method makes use of both the distributional data about a new word and the strength of the semantic relatedness of its target class to other likely candidate classes. 
This paper presents PEAS, the first comparative evaluation framework for parsers of French whose annotation formalism allows the annotation of both constituents and functional relations. A test corpus containing an assortment of different text types has been built and part of it has been manually annotated. Precision/Recall and crossing brackets metrics will be adapted to our formalism and applied to the parses produced by one parser from academia and another one from industry in order to validate the framework. 
In this paper we report ongoing work on developing an interactive word alignment environment that will assist a user to quickly produce accurate full-coverage word alignment in bitexts for different language engineering tasks, such as MT lexicons and gold standards for evaluation. The system uses a graphical interface, static and dynamic resources as well as machine learning techniques. We also sketch how the system is being integrated with an automatic word aligner. 
 2 Evaluation Method  The paper presents two approaches to partial parsing of German: a tagger trained on dependency tuples, and a cascaded finite-state parser (Abney, 1997). For the tagging approach, the effects of choosing different representations of dependency tuples are investigated. Performance of the finite-state parser is boosted by delaying syntactically unsolvable disambiguation problems via underspecification. Both approaches are evaluated on a 340,000-token corpus. 
Language variationists study how languages vary along geographical or social lines or along lines of age and gender. Variationist data is available and challenging, in particular for DIALECTOLOGY, the study of geographical variation, which will be the focus of this paper, although we present approaches we expect to transfer smoothly to the study of variation correlating with other extralinguistic variables. Techniques from computational linguistics on the one hand, and standard statistical data reduction techniques on the other, not only shed light on this classic linguistic problem, but they also suggest avenues for exploring the question at more abstract levels, and perhaps for seeking the determinants of variation. 
Machine Translation (MT) has long been a controversial topic, the source of illusions, jokes and even serious disputes. But nothing has stopped people from believing in its ability to help us cross the language barrier. Research and development in fully automatic translation has been carried out for fifty years. At regular intervals, researchers from Japan, USA, Russia, Germany, France, Netherlands and other countries have heralded the big breakthrough. This time the breakthrough is real. While academics debate linguistic and statistical approaches to MT, organisations in the public and private sector are putting it to work. Companies like Cisco, Oracle, SAP, Océ, Fortis A.G., DaimlerChrysler, IBM and Ford see tremendous benefits in MT, and rapid returns on investment. The breakthrough is market-driven rather than technical: MT is not perfect, but it has become an economic necessity. We must learn how to use it and how to optimise its benefits in practical business environments. This article provides a snapshot of solutions for automated multilingual communications on corporate Intranets and Extranets and in translation production environments. Machine Translation makes perfect business sense The tide is changing Growing volumes of information and an ever-faster pace of doing business, at home and across borders, make automatic real-time translation an economic necessity for many corporations. Machine Translation sounds like a curse to some and a fantasy to others. And yet many corporations are realising tremendous benefits by integrating MT and other language technologies into their localisation processes, Intranets and customer support and e-commerce sites. While researchers and translators are often disappointed with the output of MT systems, more and more users accept the imperfections and embrace the technology. Every day, portals like Altavista and Google process nearly 10 million requests for automatic translation. Translators, at institutions like the European Commission as well as freelancers, realise that they can produce much more work in a day by post-editing MT instead of translating from scratch. And companies invest in customised MT systems for their specific domains to allow for real-time automatic cross-lingual communication with their customers, employees and suppliers. Language is, quite simply, a business instrument. It helps us to communicate facts and tasks, and where that is the primary goal, MT is often preferable to human translation. Several factors determine the indisputable need for MT, most notably the need to reduce translation costs, shorten translation cycles and improve efficiency. Where the need for translation is instant, or the volumes are high, MT may be the only translation solution available.  Intranet solutions  Intranets go global The impact of globalisation on the external face of business is well documented. There has been less focus, however, on the impact internally, particularly on language requirements. Sixty-seven percent of the largest European companies have employees in more than five countries.1 Even though management may impose English as a “corporate language”, many documents are still written and stored in multiple languages. According to some estimates the volume of content on corporate Intranets is doubling every six to nine months and employees spend half of their time looking for information, while finding it continues to get harder. Every company has a language deficiency factor. This is the proportion of employees who are not fluent in the corporate language. Language deficiency can seriously undermine the efficiency of office and factory workers. The language deficiency factor has a significant negative effect on the retrieval of information on corporate Intranets. MT can be a powerful tool to compensate for corporate language deficiencies - for example: • Daimler-Chrysler uses MT for translation of company email and Intranet web pages • PricewaterhouseCoopers uses MT for translation and sharing of reports and proposals on its Intranet • NCR uses MT for e-learning and HR documents • Administrators at the European Commission use MT to write reports in a secondary language.  Intranet solution: pay-back scenario Investments in Machine Translation for an Intranet can be earned back very quickly. In a typical scenario, 10% of non-local employees of a medium-sized company may lack fluency in the corporate language(s). They may simply miss information published on the Intranet altogether and “reinvent the wheel”, or they may attempt to compensate by translating on their own, with the probability that they will misinterpret information and make errors.  Intranet pay-back scenario - medium-sized company  Corporate assumptions  Number of employees  % of overseas employees  Average labour costs  Intranet information assumptions  Language deficiency factor  Total loss of efficiency  15%  12,000 50% 50,000 10% 4,500,000  Employee self-translation factor 5%  1,500,000  Reinvent the wheel factor  7%  2,100,000  Error factor  3%  900,000  Quality scoring rate  60%  Loss of efficiency  2,700,000  
The paper is divided into three sections. The first gives a description of the reasons for setting up this system, while the second provides user statistics and feedback, in addition to practical examples of how the MT engine is used to obtain a working translation into a language the user knows but in which he is not fully proficient, or for gisting a document in an unknown language. The third section concerns potential improvements to the system in the future. In particular, reference is made to the translation of financial reporting documents using Extensible Business Reporting Language (XBRL), taking advantage of another PwCSYSTRAN joint project which is pioneering the use of XBRL in a web-based service for translating financial documents, and a further XBRL initiative undertaken by PwC with Nasdaq and Microsoft. 1. Background: reasons for implementation Following the world-wide merger between the professional services firms Price Waterhouse and Coopers and Lybrand, a Knowledge Management team headed by senior consultant Jonathan Sage was set up in 1999 to investigate translation services around the world and possibly identify an international supplier of language services to offices in all countries. This was no small aim, since PwC is present in 142 countries, with over 125,000 professionals. During the initial stage of researching the needs of offices around Europe, it was discovered that either individual translators (ranging from secretaries doing occasional translation work to fully qualified professionals) or translation departments already existed in a number of major offices, particularly Paris, Amsterdam and Madrid. Finding a global translation service supplier was therefore no longer relevant, and in fact would probably have been opposed by users of translation services who we may assume were reasonably happy with the service they were receiving locally. Instead, the decision was taken to go ahead with the original concept of a global translation service but via a different channel, making basic translation available to all PwC employees in any location through access to a machine translation engine on the  firm’s Intranet. The Spanish firm, which had an internal translation department and a strong Knowledge Management team, was closely involved in the project from the outset. It was made clear to the Knowledge Management team by the experienced translators working at the Madrid office and elsewhere that the machine translation programs available at the time would not be capable of translating to an acceptable standard the kind of documents that were typically handled by the PwC translation departments (annual accounts, financial reports, international taxation analyses, commercial contracts, promotional materials, etc.) because of their highly technical nature and the complex language involved. MT systems are good at translating weather reports, user guides or help files for electronic consumer products, basically any document with simple syntax and restricted vocabulary. They are not good at translating grammatically complex analyses of different areas of law or subjective advice on how best to minimise the tax burden of foreign investments, to give just two examples. And in any case, the one area where computers were definitely of help for PwC’s translators, this being the use of CAT for documents with high levels of repetition (essentially, annual accounts and certain contracts), was already covered, or would soon be covered, in the major offices through the use of Trados translation memory software. No attempt was made, therefore, to give the impression that MT would provide a universal solution for PwC’s translation needs; rather, it was presented as an alternative to “human” translation for certain circumstances in which high linguistic quality was not an essential prerequisite and fast, dynamic translations were required. The largest industry players were approached and SYSTRAN was chosen from among the leading suppliers as they could combine state-of-the-art technology with the largest breadth of available language pairs. In addition, SYSTRAN's online translation service is a turnkey solution hosted on SYSTRAN's servers. This efficient approach allowed PwC's service to be up and running in a very short timeframe without tapping into the company's IT resources. SYSTRAN were already known to the team from PwC since they provided the basis for the European Commission MT service and also the engine for the popular Babelfish service on the Alta Vista browser. Trials with machine-translated documents were carried out by the Knowledge Management centre with a target group of volunteers in various countries and the site commenced operation on a pilot basis in the Spanish firm in early 2000. Soon after, the on-line service was made available to PwC employees in all countries.  Figure 1: User interface of PwC/SYSTRAN web site The translation engine currently offers 37 language pairs and 21 SYSTRAN dictionaries on specific subject areas. During 200l and 2002 specific dictionaries provided by PwC were incorporated into the system in order to help tailor it, to the extent possible, to the firm’s terminology. A number of English-Spanish dictionaries on areas such as insurance, banking, tax and auditing, migrated from Trados MultiTerm, were provided by the Madrid Translation Service, and English-French and Japanese-English dictionaries were added later on XBRL. Users can choose between these dictionaries and the ones supplied by SYSTRAN. In addition to these specialised dictionaries, PwC employees have the option of creating their own dictionaries, in which they introduce the terms themselves. The procedure for doing this is set out in considerable detail in an on-line user guide. To date, 216 such user dictionaries have been created which are only available to the user that made them. This reflects a substantial interest in the facility on the part of these users. In this connection, it should be stressed that SYSTRAN also offers its customers a range of professional services for high-end customisation. The service utilised by PwC has not been customised in depth in order to maximise the quality of the output since this was not the overriding purpose envisaged for the system by PwC when it was started up.  PwC employees around the world have access to the firm’s intranet, called KnowledgeCurve, which provides them with all sorts of information on the firm itself, its lines of service, clients, markets, etc., in addition to internal services and data specific to employees in the various countries. As shown below, the PwC/SYSTRAN site is accessed on the KnowledgeCurve home page through a list of Quick Links to useful sites or services on the intranet. The Spanish firm’s site also contains a direct link from its main page. Figure 2: KnowledgeCurve home page showing translation link Users have to introduce a user name and password to access Knowledgecurve, and this same password is valid for the MT site. This avoids the need to write in the user ID and password twice. Employees can also access the site from the Internet, rather than via the intranet, in which case their PwC email address serves as the user ID.  2. MT in action: what users think, and why they use the on-line facility This section looks at how, why and by whom the PwC/SYSTRAN MT facility is used, providing data on most popular language combinations, users’ opinions on the worth of the system, and three practical examples of how the facility is actually utilised by PwC professionals. a) Most requested combinations The most popular language combinations of the 37 available pairs, with the number of translations requested, are shown in the following table (September 2003):  Language Pairs English-Spanish Spanish-English German-English English-German French-English English-French Dutch-English Portuguese-English English-Portuguese English-Chinese (simplified) Italian-English English-Italian Japanese-English English-Dutch English-Chinese (traditional) German-French French-Spanish Dutch-French French-German French-Dutch German-French  No. of requests 40327 22727 16756 8567 7304 5278 4764 1942 1541 1485 1396 1360 1273 1271 1244 1015 920 818 697 692 646  Table 1: Most requested language combinations  The most requested language pairs for translations in both directions (e.g. EnglishSpanish and Spanish-English) are reflected in the following graph. The figures for traditional and simplified Chinese have been merged. 70000 60000 50000 40000 30000 20000 10000 0 English/SpEanngislihsh/GerEmnagnlish/FreEnncghlEisnhg/Dlisuhtc/PhortuEgnugesliesh/ChiEnnegsleishE/Intaglilaisnh/JapFarneensceh/GeFrmreannch/SpanFisrehnch/Dutch Figure 3: Most requested language combinations The total number of translations requested for all languages is around 130,000 and some 7,300 PwC employees around the world have used the system. As can be seen from the above data, combined requests for English-Spanish and Spanish-English account for almost half of the total for all languages, and more than double the next most popular combination, German-English-German. This backs up the widely held view in the language community that Spanish is the world’s most international language after English. b) User feedback The PwC/SYSTRAN contains a mechanism, called “linguistic feedback”, whereby users are able to provide information on their use of the facility and their opinions on the results obtained. In addition to data on the type of document (text, file, web site) and language combination used, the feedback form asks users to comment on strengths or weaknesses observed by them and to indicate whether they agree with the following statements: [ ] I could basically understand the translation [ ] I found the translation suitable for my requirements [ ] I am still favourable to using this technology to help in my comprehension needs on FL text With regard to the responses received from users, both directly from the intranet site and via a survey of professionals in the Spanish firm using the same format, 57% had ticked  at least one of the boxes and may therefore be regarded as “positive”, while 43% had no box ticked, and should therefore be considered “negative”.  Of the positive responses, opinions concerning the value of the facility (i.e. boxes ticked) can be analysed as follows:  Response Could basically understand translation Found translation suitable for requirements Still favourable to using this technology  % of respondents 54% 21% 25%  Table 2: User feedback opinions Graphically, this may be represented as follows:  Basically Understand Suitable to needs Approve technology  Figure 4: User feedback opinions Concerning the format options for the original documents (web address, file or text), the vast majority of users (80%) chose the text option, either writing or pasting their source document in the pertinent space. A file was uploaded for translation by 15%, while 5% requested the direct translation of a web site. Specific comments from users reveal that the PwC/SYSTRAN facility is used basically to obtain a rough idea of a document’s content, to prepare drafts in other languages and even to translate single terms. Opinions as to its value vary considerably, as can be appreciated in the verbatim selection of user comments given below, in descending order of enthusiasm: “I tried it out and think it’s amazing!” “I consider this option to be very useful” “I reckon this is a useful tool” “For simple translations it seems pretty good” “It’s useful for certain texts, but the translations are not very accurate” “I think it could be useful for translating or enquiring about single words, but with regard to texts in general the translation is incongruent” “I wouldn't use this machine for more complicated texts, because it's not reliable” “Translation failed utterly”  This last, wholly negative response is not surprising, since the user was trying to translate into English from Dutch using the German-English engine! 
Introduction The irreversible process of globalization and Internet revolution has changed the way people live their lives. The global spread of large-scale improvements of transportation and communication technologies has made the global world smaller than ever. In relation to this, the Internet- especially the World Wide Web- has eliminated the barriers for communication and interaction with people from different countries. Unlike other communication and tools, the Web is available to any person in the world with an Internet connection. However, although the Web traces its roots from the United States, it reaches out to the four corners of the globe, wherein plenty do not know how to speak English. In fact, Internet research for IDC reports that within four years, only thirty percent of the Internet users will speak English as their first language. Because of the trend, translation becomes an important medium to communicate with various users on theWeb [LANN2001]. Language Translation, though an emerging sector, has shown progress in recent years. Part of its improvement lies in the development of productivity enhancing linguistic tools. Machine Translation is one of the major tools that analyzes and converts text from a source language to the target language. It does not require human intervention and preferable when integrated with organizational workflows for best results [LISA2001]. Future trends facing the industry include the need to use technology, process integration and develop a new tool. One development in that direction is the advent of Multilingual Chat Translation systems. These systems offer an opportunity to chat online with colleagues and friends that do not speak the same language. It is faster, reusable and cost effective than human translation [LISA2001]. Statement of the Problem Businessmen encounter problems communicating with countries that speak in a different language.  There are instances when miscommunication occurs. One instance is when one attempts to send a letter from one country to another that speaks in a different language. Because of the language barrier, the receiver might interpret the sender’s message in the wrong manner. Thus, it might lead to confusion and at times arguments. Another instance is when vendors cannot sell a product because their clients do not understand what they are selling. In order for the customer or client to invest in a certain product, the vendor must be able to fully communicate what the product is all about. In cases where language differences are an issue, the transaction may fail. An alternative is to hire a human translator. However, translation is a highly skilled job that requires more than the mere knowledge of a number of languages. In some countries, the translators’ salaries are comparable to highly trained professionals. Thus, it is costly to hire one. In addition, the delays in translation may prove costly. An average translator could only translate four to six pages of good quality translation per day and delays could erode the market lead-time of a new product [ARNO1994]. Also, when one conducts translations from one end of the globe to another, it is quite costly for it to be mediated by an interpreter over the phone. Also, there is an issue of confidentiality between the interpreter and the clients. The fact that the interpreter could acquire information over the course of translation is risky.  The Philippine Style Chat is a proposed instant messenger translation system that works in a real- time environment. Given a scenario where two users do not understand each other due to different languages they used. The system provides a solution through the following steps. First, the users are subdivided into two parts: the sender and the receiver. The sender is connected to the Internet. Using the system, the sender sends a message in his/her own language. The input sentence goes into the system that uses the transfer approach to translation, and gets information from grammar rules and dictionaries. After going through the process of translation, the translated sentence is sent to the receiver in the language he/she understands. The transfer process views translation as a three- phase process. First, analyze the input into a source- language syntactic structural representation. Second, transfer that representation into the corresponding target-language structure. Lastly, synthesize the output from that structure. Although this approach has the disadvantage of requiring another stage of processing, it holds an advantage of approaching the contrastive element of translation for it is at the transfer stage that the differences between the languages are revealed [SONN2000]. Grammar Formalism Grammars are able to describe the syntax of languages. In the field of machine translation, grammar formalisms are treated as mathematical entities that can capture natural language [ BORRA1999]. For this project, context-free grammar will be used. Context-free grammars are type 2 grammars that are widely used for syntactic description [ TRUJ1999]. It is a formal system that depicts a language by describing how a legal text can be derived from symbols called an axiom or sentence symbol [ELI2001]. A context-free grammar consists of the following components: > A set of terminal symbols, which are the characters of the alphabet that appear in the strings generated by the grammar. > A set of non-terminal symbols, which are placeholders for patterns of terminal symbols that can be generated by the non-terminal symbols. > A set of productions, which are rules for replacing (or rewriting) non-terminal symbols (on the left side of the production) in a string with other non-terminal or terminal symbols (on the right side of the production). > A start symbol, which is a special non-terminal symbol that appears in the initial string generated by the grammar [NELS2001].”  To generate a string of terminal symbols from a Context-free grammar, the following steps are done: • First, begin with a string that consists of the start symbol; • Apply one of the productions with the start symbol on the left hand side, replacing the start symbol with the right hand of the production; • Repeat the process of choosing non-terminal symbols in the string, and replacing them with the right hand side of some corresponding production, until all the non-terminal symbols are replaced by terminal symbols [NELS2001].  The close correspondence between the syntactic descriptions of the natural language and the context-free grammars have been made them a useful and popular tool in natural language processing. Also, since they are more convenient in defining syntax of programming language, efficient techniques for processing them have been developed. However, problems occur when situations like unbounded dependencies, which one cannot use context-free grammar to solve alone [TRUJ1999]. Project Objectives  General Objectives The general objective is to develop a real-time instant messenger translation system.  Specific Objectives  The specific objectives are as follows:  > To collect relevant information from the sources and dictionaries that will help in translation.  > To use algorithms that will ensure good quality translation, which solves different types of ambiguity.  > To ensure that the translation will be done in an efficient manner, which takes into consideration the number of users using it.  > To incorporate features that will make translation easier and provide users  with a “user-friendly” interface.  *  > To constantly solicit users’ feedback, so that the system will cater to their diverse needs.  Significance of the Study Philippine Style Chat, being a translator system, dwells largely with the area of machine translation. In general, machine translation can be described as the process of translating from one human language to another. As a topic, machine translation is significant in  various areas - socially, politically, commercially, scientifically, intellectually or philosophically [ARNO1994]. The social or political importance of machine translation is taken from the socio-political importance of translation in communities where more than one language is spoken. In this case, the only option other than hiring translator would be the adoption of a common “lingua franca”. The problem with the creation of such language is that it involves the dominance of a certain language to the disadvantage of other languages becoming second-class or disappearing. Because of this, translation is important to effective communication- for ordinary human interaction and gathering of information one needs to play in society. One problem, however, is the shortage of translators due to the demand of translation. In the process, it seems that automation of translation is a social and political necessity for modern societies who do not wish to impose a common language on their members [ARNO1994]. The commercial importance of machine translation is a result of these factors. First, translation by itself, is commercially important. If a customer is made to choose between a product with an instruction manual written in English and one whose manual is written in Japanese, most English speakers will buy the former. Second, translation is expensive. Translation is a highly skilled job that requires more than knowing a number of languages. In some countries, translator’s salary is comparable to highly trained professionals. Third, delays in translation are costly. Producing high quality translation of difficult material requires considerable time. Estimates would show that a professional translator might average around 4 to 6 pages of translation (200 words) per day. Due to translation delays of manuals of technical documents, it can easily lead to the delay of a product launch [ARNO1994]. Scientifically, machine translation is important, because it is a testing ground for various ideas and applications in Computer Science, Artificial Intelligence and Linguistics [ARNO1994]. Philosophically, machine translation is interesting, because it represents an attempt to automate an activity that requires the full range of human knowledge. One way of approach to machine translation is the extent to which one can automate translation is an indication of the extent to which one can automate “thinking” [ARNO1994]. Scope and Limitations of the Study Scope of the Project The developed system will create a real-time instant messenger translation device wherein users can communicate in real-time environment. Its scope is subdivided to two major parts: translation features and instant messenger related features.  
Dublin 9, Ireland. minako.ohagan@dcu.ie  1.  Background  This study was prompted by three independent developments. One is a widely publicized  criticism of the Japanese subtitles of the film The Lord of the Rings: The Fellowship of the  Ring - the first episode in the J.R.R.Tolkien trilogy. Shortly after the film's release in Japan in  March 2002, complaints about the quality of the subtitles from Japanese fans led to petitions  to the film's Japanese distributor and the director, Peter Jackson (O'Hagan, 2003a). The  fans claimed that the Japanese translation reflected the subtitler's lack of appreciation of the world of the Lord of the Rings (LOTR)1 as created by Tolkien. It was subsequently revealed  that the entire subtitling task had been completed in just one week by a single subtitler who  had never read the book. This highlighted the fact that in the film industry subtitlers are often  at the mercy of the market-driven approach becoming prevalent in the entertainment industry  as a whole. For example, even the translation of the title of the film is not determined by the  
 (sball@europarl.eu.int)  The IATE1 project for the development of a single terminology platform for the European Union’s institutions and agencies began in 2000 and, although the system was originally scheduled to go live in 2001, it has only recently gone into full-scale user testing prior to the production phase. The paper describes the background to the project, the system implemented and the accompanying structures. It also discusses issues outstanding and future needs.  Introduction The title of this paper is taken from the current (December 2002) online OED’s first draft definition of “joined-up” in the figurative sense, viz.: “of thought, speech, etc.: coherent, organised, cogently or articulately developed”. This aptly characterises the aspirations of the IATE project: to reorganise the terminology activities of the European Union (EU) institutions and agencies in a coherent manner, to eliminate the duplication of effort between institutions and consequent duplicate entries in the various terminology databases managed by them and to develop a single database for future activity using resources as rationally as possible, particularly with an eye to the enlargements of the EU scheduled to take place from May 2004 onwards.  It would be difficult to over-emphasise what an ambitious project this has proved to be, since it has meant bringing together the terminology databases of the three largest Community institutions, the European Commission (Eurodicautom), the European Parliament (Euterpe) and the Council of the EU (TIS), together with the databases more recently developed by other participants such as the Translation Centre (EuroTerms), the European Court of Auditors and the Court of Justice of the European Communities. Each had its own structure and philosophy, and it has been no easy matter to bring them together to make a coherent whole.  The paper will cover four main areas, the development of the project, in particular since November 2001 when it was last presented to an Aslib audience at T&C 23, the  
This paper describes the impact that XML will have on the authoring, publishing and translation of documentation and how XML itself can reduce the cost and complexity of the authoring and translation processes. In the beginning... The advent of text in electronic format posed a number of problems for translators. These problems were: 1. How to mange the differing encoding standards and their corresponding font support and availability. 2. How to present the text to translators without having to purchase additional copies of the original creation program. 3. How to translate the text while preserving the formatting. 4. How to build translation memories for these documents to reduce the cost of translation and improve consistency. The problem was exacerbated by the veritable “Tower of Babel” of differing authoring and composition environments from Interleaf through to PageMaker. The typical approach was to write filters that would “lift” the text to be translated from its proprietary embedded environment and to present it to translators in a uniform but equally proprietary translation environment. After translation the text would then be merged with the original document, replacing the source language text. ISO 8879:1986 SGML A serious attempt to tackle the plethora of competing formats and their embedded nature was made in 1986 with the advent of ISO 8879 Standard Generalized Markup Language (SGML). The aim of ISO 8879 was to separate the content of documents from their form. SGML arose at a time of great and rapid change in the IT industry. The architects attempted to make the standard as flexible and open to change as possible. This laudable aim unfortunately produced something that was very  difficult and expensive to implement. In addition SGML only tackled the aspect of content. Form was tackled by ISO/IEC 10179:1996 Document Style Semantics and Specification Language (DSSSL), but this proved equally difficult to implement. HTML The efforts of the ISO 8879 committee were not in vain. SGML allowed for the creation of HTML which enabled the World Wide Web to catapult the Internet from a vehicle used by academics and computer scientists to what we know today. HTML was initially based on strict adherence to the SGML standard, but soon diverged as the limitations of ISO 8879 became apparent. 
What is a web service In simple terms, a web service is very similar to a simple web site. This simple web site does not contain any graphics or user interface, it simply returns text when it is contacted. Like a web site, different pages, or different information can be requested, and information can be passed to the web site, like fields on a form, that the web site then processes in some way to return some results. With the absence of a user interface, web services are designed to be called from programs. In other words, instead of a person requesting a page (as a normal web site is used) web services are requested by programs on computers over the internet. A simple program that tells you the latest weather on your desktop is probably linking to a web service somewhere on the internet, in the background, in order to get its information. The program will be sending some data to the web site to request the forecast, and the web service, at the Met Office or other, is returning a description of the weather to the program on your desktop. That desktop program is then formatting the data and displaying it to you in a user friendly form. This simple use for a web service is fairly trivial, but it does show some important things about web services: 1. They are used to get information from a site on the internet.  2. They have no user interface, so the calling program must read the response and do something appropriate with it. 3. They are invaluable when you need to use information in a program that it is impossible to get locally on your PC. The weather is an excellent example. If the program needs the weather forecast, this cannot be retrieved from anything Microsoft ship with PCs (yet), so the program must look externally for this information. More about Web Services When programmers create programs, they tend not to write the entire program themselves. Instead, they access code that someone else has already written. This is generally called the use of pre-written ‘objects’. If the program that they are writing uses the time and date, they do not need to create their own counters and clocks. Instead, they simply access the system clock. The system clock is presented to the programmer in an easy to use way, as an object that they can call at any time they need it. Other examples of objects a programmer may need to make use of are: 1. The spellchecker in Microsoft Word 2. Email 3. Graph drawing utilities The list is actually endless, but of these three examples above, there is a common thread. They can all be contained on the local machine. For example, if I need to use spellchecking n my program, I simply need to ensure my users have Microsoft Word installed, and then I can access this spellchecker with little problem. Again with e mail, I will probably an object from Outlook to send a mail using the default account on that machine. Objects such as those above are generally easy to use for programmers. The object can be selected with the mouse, and it then exposes itself and its requirements, such as, “if you pass me a word, I’ll return ‘yes’ if I can find it in my spell check dictionary, and ‘no’ if not”. This is obviously an over simplification, but this is the principle. 
garaolaza@codesyntax.com Introduction This paper describes a multilingual document managing system, SARE-Bi, that is based on the use of metadata. In this system, metadata have the role of controlling all phases of a document’s life cycle, from the drafting of the first version up to the reutilization of published material, including all intermediate phases of translation, post-edition, validation, publication, and others. The system has been implemented in the web application server Zope and metadata are based on two XML proposals: TEI, for structural mark-up, and XLIFF, for log control. The paper shows how adequate Zope is as an application to manage the entire life cycle of multilingual contents. Keywords: multilingual document management, metadata, XML, TEI, XLIFF, Zope, machine assisted translation. SARE-Bi system (http://www.deli.deusto.es/SareBi/) Problem description and case study Rapid multilingual delivery of publishable documents is still a challenge for translation technology. Commercial machine translation systems, as for example Systran, Reverso or PAHO's Spanam or Engspan (for the English and Spanish pair), are capable of producing  readable texts, sometimes of unexpected good quality (normally after some period of training or dictionary updating), but which still need rather long and laborious postediting time before the actual output can be published. Furthermore, very often multilingual publication requires more functions than those usually contained in MT packages. The problem would normally arise in institutions that are bound to generate, for some reason, documents in two or more languages. This is the case of the University of Deusto which, as any other public institution in the Basque Country, has to publish every official document in at least two languages, Spanish and Basque (Euskara), but occasionally also in English or French. What we mean by document is any administrative text that has been made public by some department or centre of the institution. Here we include not only long texts of varying complexity (such as internal statutes, regulations, reports, or proceedings), but also simpler texts including calls, announcements, minutes of meetings, letters, notifications or invitations. Some fieldwork was carried out in order to find out the actual procedures of both writers and translators. As a result of this, a number of conditioning factors were discovered. In the first place, we found that the production of bilingual or multilingual documents in our institution follows a rather fixed process. One person writes the original document, almost always in Spanish. This is sent to the translation service that generates the versions in the other languages (Basque or English). Then the text is sent back to the original writer, who carries out final editing and takes responsibility for publication. The original language of most if not all documents is Spanish, because Basque is still a minority language in our community. Although an already large group of people that understand texts in Basque exists, only a few can actually write with sufficient quality in this language. In addition to this, there is a strong tradition, a kind of intertextual inertia, to write in Spanish, particularly in the case of administrative documentation. As a result, the documentation in our organisation is largely written entirely in Spanish, and only later translated into Basque. There is a select but small minority of staff who are capable of producing bilingual documents, although even they on same occasions prefer to use the translation service. Secondly, we observed that, although a significant quantity of documents that are published in the two languages already exists, the number of documents that are not only written but also published solely in Spanish is very large. Translating has a cost, both in economic terms (many documents are not translated because they are not considered sufficiently important), and of time. Many documents, including smaller-sized ones such as short notifications or calls, have to be generated urgently and very often the translation service has no time to do the job. In third place, the importance of the documents could be established with regard to their recipients. Documents that address the entire university community (of more than 15,000 people) are among those qualified as very important. More restricted documents, addressing reduced groups of people (a call for a department meeting, for example), are deemed less important. This distinction could lead to the application of "lesser quality"  translation procedure in the case of restricted documents. But what happens normally is that these documents are published only in Spanish. It would not be appropriate to circulate low quality translations. Finally, one aspect of the fieldwork that attracted our attention was the fact that an increasing number of writers reutilize bilingual published material for some particular types of texts (short letters, calls, announcements or invitations for example). Many short documents like these undergo small changes (of date, place, some parts of the agenda) from one version to the next. We considered this to an interesting starting point. It is normally safe for the editor of the new document to reuse the old version in the text processor, and with little knowledge of Basque to update the changes both in the original Spanish text and in the translation. In view of this situation our objective was to increase the number of multilingual documents generated in our University, thus reducing both cost in terms of time and money that this effort implies. How can MT help? Currently, no system that translates automatically from Spanish into Basque exists, so Machine Translation option cannot be considered. The Basque Government has recently conducted a feasibility study of MT for Basque and, as a consequence, has decided to finance the development of a Spanish/Basque MT system in the near feature. Research groups in the Basque Country (such as IXA, and DELi), as well as two companies from the linguistic-services sector (Elhuyar, Eleka), are expected to take part in the project, although the leading role will be taken by AutomaticTrans, an MT specialised company from Barcelona. In any case, this is a project for the coming years, that will begin at the end of 2003, and which is not presently available. The only options that can be considered stand in the area of Machine Assisted Translation (MAT). The translation service in our University only partly applies some MAT tools, such as term base. Translation memory systems that have been evaluated and acquired, but have still not been put into operation, due to the time span normally needed before such systems become productive. Solution (aspect 1): a document management system Given this situation, we considered that the most practical thing to do was to develop a document-management system with multilingual functionality in which users could find the complete range of document types that are more commonly used within our own institution. This system would allow users to retrieve relevant documents and reuse them to elaborate updated versions. We thought of a cumulative, collaborative system, where different kinds of users like translators or writers, could share their documents, and not only in their definitive form, but also throughout the different stages of elaboration. Such a system would be beneficial not only for its use in the translation process, but also as a document-base fulfilling archiving purposes.  Solution (aspect 2): translation memories Our group has carried out some basic research in the field of automatic feeding of memory-based MAT systems. In the period 2000-2001, we developed the XTRA-Bi toolkit [Jacob et al., 2001], a set of tools that permits the compilation, segmentation and alignment of bilingual texts. These texts are captured from different Internet sources and then converted into TMX output [LISA, 2003]. The final aim was to combine two complementary technologies: multilingual document management with translation memories. A system could then be designed in such a way that text would be stored not as a big, and largely blind, repository of translation segments, but as a categorised set of segmented aligned and well-classified parallel documents. Hence this design adds the power of a multilingual document-base to the functionality of a translation memory manager. Solution (aspect 3): metadata The last aspect that we considered was the conceptual architecture of the system. Such a system should not be able to manage full documents only, but also document segments. So a broader view of information management was required. We found it in some recent initiatives connected with the evolution of Internet and the shift from rudimentary text mark-up (based in HTML) to solutions derived from the implementation of XML technology. One of the most negative consequences of the proliferation of information published on the Internet, in various forms and dialects of HTML, has been the chaotic accumulation of contents, which seriously hinders both management and retrieval of relevant information. In recent years several proposals have been made that try to alleviate this problem. An important line of research has considered the application of linguistic knowledge, firstly trying to make the scope of the search less ambiguous and more precise, and secondly, extending the search either to semantically related terms, or to texts in other languages [Sparck Jones and Willett, 1997]. Another line of research has focused on the notion of metadata and its application to content in all its possible uses [Weibel, 1995; Kashyap and Sheth, 1998]. The use of metadata has increased in popularity in recent years, due partly to the development of XML, as a qualified alternative to HTML, and partly to the appealing effects of the Semantic Web initiative [Berners-Lee, 1998; W3C, 2003; Decker et al., 2000]. In this context, for the purposes of text categorisation and cataloguing we have adopted a mark-up solution that is strongly inspired in the guidelines of the Text Encoding Initiative (TEI), a well-known standard in the field of corpus linguistics [TEI Consortium, 2003; McEnery and Wilson, 1996]. The emphasis in our use of TEI is not so much on the linguistic aspects of the texts, but on basic structural aspects and on the set of metadata that covers cataloguing information on the TEI header.  In sum, SARE-Bi can be defined as a multilingual document management system that allows collaborative and incremental compilation of documents, that uses metadata as a conceptual mechanism for controlling all aspects of the document base and which shows a strong resemblance to memory-based machine translation systems. Figure 1: SARE-Bi's front page  A first tour on SARE-Bi We will start by illustrating the most salient functionality features of SARE-Bi. The system's front page (http://www.deli.deusto.es/SareBi/) is as Figure 1. As can be seen on the menu on the left part of the screenshot, there are two main operations a user could do in SARE-Bi: document search, and adding a new document. Let us concentrate on the first operation, which is indeed the most frequent one, and because of that, it is implemented on the main entry to the system. There are two ways of performing a search: a filter browsing, and a free text search. In the first mode, the user is allowed to retrieve (filter) a list of documents that meet certain criteria, precisely those which have been associated to them through the use of metadata. Using the first form of the main page to perform a filtering, as we will see later, the user could obtain a list of documents like those shown in figure 2. Figure 2: Results of a filtering The user can see the title of the document in each row of the table, along with the values of several metadata associated to it. If the user clicks on the title of a document, then he could visualise its contents, as we can see in figure 3. The user could read the entire document (without any separation, as a whole) in the first part of the visualisation screen. Alternatively, she could look at the document in a segmented, aligned form, below in the same page. In this way, the multilingual correspondence of the parts of the document is made explicit.  Figure 3: Viewing document contents Finally, at the very top of the document page, the user is able to export the contents to the TEI and TMX formats. In the first case, only a monolingual version is generated. In the second, a bilingual set of translation memories is obtained, which of course are immediately ready to feed any MT software using that technology.  The other kind of search that a user could perform is a free text searching, using the second form that appears in the main page (see figure 1). He could write a word in the textbox to obtain a list of segments (parts of the documents) that contain that word. An example result is shown in figure 4. Figure 4: Results of a free text search  The result of this search shows the segments that contain the required word. Actually, not only is the segment of the language of the word shown, but the rest of corresponding segments (in the other languages of the document) as well. In this way, the system acts like a translation memories browser, giving the user the possibility of knowing the translation of some words or expressions in other languages. Following the links associated to the title of each document shown, the user could reach the whole document visualisation page, in the same way as figure 3 shows. The other main operation that a user could perform in SARE-Bi is adding a new document. This is a twostep function: in the first, the user gives the values of non-automatic metadata that will be associated to the document, and creates as many (empty) subdocuments as it has languages (figure 5). Figure 5: Adding a document, first step  In the second step, the user provides the contents of each subdocument (figure 6). Mark-up, segmentation, and alignment of segments are done automatically by the system. Figure 6: Adding a document, second step It is important to mention that this operation may be used to add a monolingual document, and later the same user (or indeed, another) could add the subdocument in another language. In fact, the typical document generation cycle will involve the creation of a monolingual document by a user, and the later addition of a translated subdocument by a translator, for example. To this end, the screenshot of figure 6 is the common modification page for a given document: once created, any user with enough modification permissions can reach this page from the links labelled “Edit” on the pages shown in figures 2 and 3. Conceptual description of SARE-Bi SARE-Bi system contains, first of all, a multilingual annotated, segmented, and aligned corpus of documents. The contents of each document are firstly annotated (in a TEI-like fashion). Fundamentally, this annotation gives a segmentation of the subdocument in  each language, and an alignment of the corresponding segments in different languages. At the moment, segments are paragraphs, and annotation, segmentation, and alignment are automatically carried out by the system. On the other hand, although conceptually the existing document set can be seen as a whole corpus, it is actually divided into different sub-corpora. This division brings an additional degree of structure that users may find helpful. Secondly, a series of metadata is associated to each document, which describe its diverse pragmatic features and which contribute to the functionality that is desired for the system [Caplan, 2001; Wittenburg and Broeder 2002]. The most important metadatum is the one that classifies the document, according to a hierarchic taxonomy of different levels. In the application to the University of Deusto, this taxonomy has three levels that indicate the function, the genre, and the topic of the document (this is inspired in known typological classification proposals [Trosborg, 1997]). For example, a certificate of attendance at a short course is classified at the three levels: the first is at the function level (informative), the second is genre (a certificate) and the third is the topic (the attendance at the course). At the present time, the University of Deusto taxonomy consists of 3 functions, 25 genres and 256 topics. In figure 7, we show the first two levels of the hierarchy, whereas figure 8 contains an excerpt from the full three-level taxonomy.  10000/reglamentar  11000/  autorización  11100/  acuerdo  11200/  instrucciones  11300/  normativa  11400/  bases  11500/  plan  11600/  ceremonial  20000/informar  21100/  aviso  21200/  carta  21300/  saluda  21400/  certificado (por)  21500/  convocatoria  21600/  tarjeta de invitación  21700/  folleto (imprenta y web)  21800/  guía  21900/  memoria  22000/  catálogo  23000/  actas  23100/  anuncios en prensa  23200/  carteles de propaganda  23700/  nombramientos  30000/inquirir  31100/  ficha  31200/  impreso  31300/  cuestionario  31400/  instancia  Figure 7: First two levels of SARE-Bi hierarchical document taxonomy  30000/inquirir 31100/ 31101/ 31102/ 31103/ 31104/ 31105/ 31106/ 31107/ 31108/ 31109/ 31200/ 31201/ 31202/ 31203/ 31204/ 31205/ 31206/ 31207/ 31208/ 31209/ 31210/ 31211/ 31212/ 31213/ 31214/ 31215/ 31216/ 31217/ 31300/ 31301/ 31302/ 31303/ 31304/ 31305/ 31306/ 31307/ 31308/ 31309/ 31310/ 31400/ 31401/ 31402/ 31403/ 31404/ 31405/ 31406/ 31407/ 31408/  ficha aceptación o renuncia de beca boletín de inscripción datos de viaje modelo de pago relación de coordinadores departamentales planificación actividad de profesores prácticas datos estadísticos boletín subscripción revista impreso de solicitud de beca de solicitud de expediente de solicitud de admisión de solicitud de alojamiento de programa Sócrates de matrícula factura recibí petición de fotocopias permiso permiso para asistencia a congreso justificante de examen solicitud proyecto de investigación en líneas priorizadas reclamación de beca solicitud beca BBK solicitud beca Sasakawa solicitud página web cuestionario comunicación evaluación de asignatura evaluación de curso perfil académico profesional evaluación docencia autoevaluación docencia evaluación cursos drogodependencias evaluación Euskal Irakaslegoa satisfacción padres alumnos ESIDE satisfacción empresas con titulados ESIDE instancia inscripción pruebas mayores 25 años solicitud de adaptacón de planes de estudio solicitud de convalidación asignaturas solicitud de reconocimiento complementos solicitud de reconsideración admisión solicitud de título solicitud de traslado expediente solicitud cambio de asignaturas optativas y libre elección  Figure 8: Excerpt from the full three-level hierarchical document taxonomy of SARE-Bi  Metadata that inform of the state and the visibility (confidentiality) of the document are also very important, and special indeed. Metadata other than these are static: they are assigned at the moment of creation of a document, and usually, they do not change (unless there were any mistakes). On the contrary, state and visibility are dynamic: their values change throughout the edition cycle to show the composition/multilinguism situation of the document. To this end, users are given one of the tabs shown in the document modification page (figure 6), as seen in figure 9. Figure 9: Modifying state and visibility metadata The state indicates the stage of translation of the document: it may be non-validated (in the course of translation, or monolingual), validated (already translated, or accepted as a valid translation), and normative (multilingual version of special relevance, that is offered as a model). The visibility assigns the degree of confidentiality of the document: when it is at the elaboration stage, it is a rough draft (visible only to its owner), whereas when it is already finished (at least in a monolingual version) it may be confidential (visible with a lot of restrictions, for documents with sensitive information), shared (visible to all users of the system in the organisation, equivalent to the concept of intranet), and public (visible for any user connected to the system from the web). These two metadata, state and visibility, are directly related to another important component of the system, that is the set of users, which is considered later in this section. Another important metadata is the centre (or department of the organisation) that originates the document, separated into two levels, centre and subcentre. In addition, several dates are stored, that is to say, the original date of the document, the date of inclusion in the corpus, and the date of the last modification.  It is important to point out that the assignment of some metadata (the most relevant - the category, the centre, and the original date of the document) is still not automatic: the user has to assign them when he adds a new document to the corpus. On the other hand, state and visibility have a dynamic behaviour, controlled by the users, throughout the edition cycle, because they are users who should change their values for a document as it reaches the different stages of composition and translation. There is an additional component of the system that is the set of users. In the first stages of the design of the system, kinds of users were associated to the different tasks allowed in it. From that perspective, there are four types of users: 1. Guests are users outside the organisation; they can interact with the system in a “read-only” way, as a demo. 2. Writers are the people who develop new documents. 3. Translators are, obviously, the users from the translation bureau. 4. Administrators are those who maintain the system. We can suppose that, apart from the guests (who can access to the system universally from Internet), the rest of the users are members of the organisation, and that the system works for them more like an intranet. Later on, in the development of the system, we saw the need for defining permissions associated to the tasks performed by users. Then, there is an additional metadata of a document, the owner, which is the user who first created the document. Depending on the owner, and the state and visibility metadata of the documents, we can define a complex set of permissions for the tasks allowed to users: 1. Guests cannot be owners (they cannot add new documents nor modify the existing ones), and they only have the right to visualise the so-called “public” documents: those with “public” visibility and state at least “validated”. 2. Writers can visualise the contents of any document except those with “confidential” or “draft” visibility of another owner. They can add new documents, and remain owners of them; in particular, each user of this kind is responsible for assigning the correct value of the visibility metadatum of their own documents. However, they can only modify documents of their own, and in fact, they cannot access state metadatum. 3. Translators have a larger set of permissions, because of their task. They can, of course, also add new documents, as if they were writers, under the same conditions that apply to this kind of user. They then have the right to visualise and modify the contents of any document except those in elaboration (visibility “draft”); they have specific access to the “confidential” documents. Translators are responsible for setting the correct values of the state metadatum, as the document undergoes the different stages of translation. However, they cannot modify either the visibility or other metadata. 4. Administrators have no restrictions of any kind.  Typical edition cycle Let us complete the description of tasks previously given on the first tour on SARE-Bi with a full view of the typical document generation cycle allowed by the system. To get a clearer idea, let us take the example of the admission letters that are sent to course applicants. The secretary of the centre (a user of the “writer” kind) first performs a filter browsing or a text search to find out if there is any letter of this kind already in the system. Suppose that none is found. Then he creates a new document, only in Spanish, with the required contents. On creation, visibility is “draft” and state is “non-validated” by default. When he finishes content introduction, he must assign the definitive degree of visibility to the document - normally “shared”. Then, he calls the translating bureau, asking them to translate the document to Basque. A user type “translator” does the translation, adding the language Basque and its contents to the document. When the translation is done, the translator assigns the final value of the state metadatum, normally “validated”, and calls the secretary back. After that the multilingual document can be retrieved. But suppose that after performing the filtering, a previous admission letter is found. Accessing its contents, the secretary could reuse it to compose the new letter. Suppose now that the secretary knows Basque well: as the document is not very complex, he could made the necessary changes in the two languages to obtain a bilingual document, without any translation bureau intervention. On the contrary, suppose the secretary knows Basque a little. He could try the changes both in Spanish and in Basque, but as he is not confident of the result, he calls the translator. Note that normally in this case, there will be minor errors in the document, so the translator’s work will be considerably easier. A translator could also assign the “normative” state to a given multilingual document, when it is paradigmatic in some way: it may contain a special important terminology, it may be a template in its category. The normative documents are then those that could be used by bilingual writers with a high degree of confidence. Implementation of SARE-Bi SARE-Bi has been implemented in the Zope system [Zope Community, 2003] under the conceptual scheme of object-oriented databases. Although theoretically a storage based on XML technology seemed appropriate, it was decided to use Zope due to its optimal handling of information, its support for the basic searching and retrieval operations, and its facilities for multilingual web interface construction (by means of the Localizer module [Ibáñez Palomar, 2003]). That way, the system is totally integrated into the web site of the research group, and it may be used collaboratively by any number of users. On the other hand, total XML functionality is achieved by means of supported export operations to the TEI and TMX formats. The design of the object-oriented database has followed the ideas of UML (Unified Modeling Language [OMG, 2003]). The class diagram is shown in figure 10. Basically, a  set of documents, or corpus, is modelled with three classes of objects (DeliTei, DeliLang y DeliSeg), which have no hierarchical relation (i.e., inheritance), but of composite aggregation one (i.e., a “whole/part” relationship): 1. A DeliTei (multilingual document) has several DeliLang (one-language subdocument). 2. A DeliLang has several DeliSeg (segments, which in our implementation are paragraphs). 3. A DeliSeg contains the textual contents of a paragraph. A container supra-class, named DeliCorpus also exists, which can contain as many documents (DeliTei) as required. Then, the database is defined as a set of persistent objects of the DeliCorpus class. These four classes inherit from classes provided by Zope, as ZObject, CatalogAwareBase, and ZObjectManager to achieve part of their functionality.  ZObject  CatalogAwareBase ZObjectManager  DeliCorpus {persistent} id: string add()  DeliTei  {persistent}  
The impact of corpora in translation studies is still only in its infancy. As Baker (1999) states: “Work in this area began in an exploratory fashion in the early nineties and is only now beginning to yield some concrete findings, albeit on a relatively small scale”. What separates translation studies from other fields is the way it can make use of several different types of corpora. Aligned parallel corpora, monolingual translated corpora as well as monolingual original corpora may all be used as aids in the translation process. This article will begin by looking at aligned parallel texts although using this type of data has various shortcomings. Despite its great value in finding equivalents, it not be the most useful resource for a translator. A parallel corpus will contain all the difficulties that a translator is faced with, such as structure mirroring the source language and odd wordchoices. This sometimes gives the text a foreign feel. There are no obvious solutions to problems with quality of the target text, but this paper will propose the monolingual corpus as a superior source of information. Examples will be given in Swedish and  English, taken from parallel texts, comparable texts and, most importantly, large monolingual corpora, such as the Swedish Language Bank and the Bank of English. 2. Parallel texts The unit of analysis in language is often said to be the single word. However, the relevant units, the units of meaning, are often beyond the word and instead take the form of multiword units. This is a fact that most translators are already familiar with. When using the single word as the unit of analysis, we are faced with the problems of apparent ambiguity. I write apparent, as ambiguity is primarily a problem in the automatic treatment of language, or for students in the field of linguistics. Rarely will a translator discover a section of text which is also ambiguous to the eyes of a professional. The problems of ambiguity only come into the translator’s work when we need to look up something in a bilingual dictionary or from lexical databases. Suddenly, the single word found in the text can correspond with a whole list of possible equivalents in the dictionary. This is not a new observation; the short-comings of bilingual dictionaries have been referred to in many previous publications. However, if in combination with the bilingual dictionary we also have parallel texts, we get access to a sea of disambiguating decisions already made by previous translators. The examples below show the Swedish word mål which appears ambiguous when translating into English1. The word corresponds to aims, goals and objectives, in both singular and plural forms. of the Union ' s regional policy [[aims]] . The report shows that av hur unionens regionalpolitiska [[mål]] uppnåtts . Rapporten visar contribution to this great common [[goal]] . Madam President , I will bidrag till detta stora gemensamma [[mål]] . Fru talman ! Jag vill öka You have fulfilled your [[objective]] for today , which is to answer .. Ni har lyckats uppnå dagens [[mål]] : att besvara samtliga frågar . Example 1. The Swedish word mål in parallel texts, corresponding to 'aims', 'goal' and 'objective'. 
I. - Background The rules on the use of languages 1. The rules governing the use of languages at the Court of Justice and at the Court of First Instance of the European Communities are a special feature which distinguishes them not only from other national courts but also from other institutions of the European Union. 2. Proceedings before the Court of Justice and the Court of First Instance may be conducted in any of the languages defined as official in the legislation governing the work of the Community Courts. This means that, from the moment when the application initiating  proceedings is lodged at the Registry, the parties and the Community Courts will use throughout the proceedings (at least for the written submissions) the language indicated as the language of the case in accordance with the Rules of Procedure. 3. Moreover, the uniform application of Community law requires the dissemination of the case-law in all the official languages of the Union, since all citizens of the Union are entitled to have access to it as a source of Community law, in the official language of their country. This dissemination is achieved not only through the publication of the definitive (and only authentic) version in the Court Reports, but also with the instantaneous online reports of the Curia website. 
A course in machine-assisted translation at final-year undergraduate level is the subject of the paper. The course includes a workshop session during which students compile a list of post-editing guidelines to make a text suitable for use in a clearly defined situation, and the paper describes this workshop and considers its place in the course and its future development. Issues of teaching MT to language learners are discussed.
This paper describes how a 45-hour Computers in Translation course is actually taught to 3rd-year translation students at the University of Alacant; the course described started in year 1995{--}1996 and has undergone substantial redesign until its present form. It is hoped that this description may be of use to instructors who are forced to teach a similar subject in such as small slot of time and need some design guidelines.
This paper describes some resources for introducing concepts of statistical machine translation. Students using these resources are not required to have any particular background in computational linguistics or mathematics.
This paper describes a graduate-level machine translation (MT) course taught at the Language Technologies Institute at Carnegie Mellon University. Most of the students in the course have a background in computer science. We discuss what we teach (the course syllabus), and how we teach it (lectures, homeworks, and projects). The course has evolved steadily over the past several years to incorporate refinements in the set of course topics, how they are taught, and how students {``}learn by doing{''}. The course syllabus has also evolved in response to changes in the field of MT and the role that MT plays in various social contexts.
This paper describes the approach used for introducing CAT tools and MT systems into a course offered in translation curricula at the Université de Montréal (Canada). It focuses on the automation of the translation process and presents various strategies that have been developed to help students progressively acquire the knowledge necessary to understand and undertake the tasks involved in the automation of translation. We begin with very basic principles and techniques, and move towards complex processes of advanced CAT and revision tools, including ultimately MT systems. As we will see, teaching concepts related to MT serves both as a wrap-up for the subjects dealt with during the semester and a way to highlight the tasks involved in the transfer phase of translation.
This paper describes a number of {``}toy{''} MT systems written in Prolog, designed as programming exercises and illustrations of various approaches to MT. The systems include a dumb word-for-word system, DCG-based {``}transfer{''} system, an interlingua-based system with an LFG-like interface structure, a first-generation-like Russian-English system, an interactive system, and an implementation based on early example-based MT.
Implementation of machine translation {``}toy{''} systems is a good practical exercise especially for computer science students. Our aim in a series of courses on MT in 2002 was to make students familiar both with typical problems of Machine Translation in particular and natural language processing in general, as well as with software implementation. In order to simulate a software implementation proc- ess as realistic as possible, we introduced more than 20 evaluation criteria to be filled by the students when they evaluated their own products. The criteria go far beyond such {``}toy{''} systems, but they should demonstrate the students, what a real software evaluation means, and which are the particularities of Machine Translation Evaluation.
Empirical methods in Natural Language Processing (NLP) and Machine Translation (MT) have become mainstream in the research field. Accordingly, it is important that the tools and techniques in these paradigms be taught to potential future researchers and developers in University courses. While many dedicated courses on Statistical NLP can be found, there are few, if any courses on Empirical Approaches to MT. This paper presents the development and assessment of one such course as taught to final year undergraduates taking a degree in NLP.
We introduce a new generation of commercial translation software, based primarily on statistical learning and statistical language models.
We describe a Chinese to English Machine Translation system developed at the Johns Hopkins University for the NIST 2003 MT evaluation. The system is based on a Weighted Finite State Transducer implementation of the alignment template translation model for statistical machine translation. The baseline MT system was trained using 100,000 sentence pairs selected from a static bitext training collection. Information retrieval techniques were then used to create specific training collections for each document to be translated. This document-specific training set included bitext and name entities that were then added to the baseline system by augmenting the library of alignment templates. We report translation performance of baseline and IR-based systems on two NIST MT evaluation test sets.
The SYSTRAN Review Manager (SRM) is one of the components that comprise the SYSTRAN Linguistics Platform (SLP), a comprehensive enterprise solution for managing MT customization and localization projects. The SRM is a productivity tool used for the review, quality assessment and maintenance of linguistic resources combined with a SYSTRAN solution. The SRM is used in-house by SYSTRAN{'}s development team and is also licensed to corporate customers as it addresses leading linguistic challenges, such as terminology and homographs, which makes it a key component of the QA process. Extremely flexible, the SRM adapts to localization and MT customization projects from small to large-scale. Its Web-based interface and multi-user architecture enable a centralized and efficient work environment for local and geographically disbursed individual users and teams. Users segment a given corpus to fluidly review and evaluate translations, as well as identify the typology of errors. Corpus metrics, terminology extraction and detailed reporting capabilities facilitate prioritizing tasks, resulting in immediate focus on those issues that significantly impact MT quality. Data and statistics are tracked throughout the customization process and are always available for regression tests and overall project management. This environment is highly conducive to increased productivity and efficient QA in the MT customization effort.
MultiTrans is a translation support and language management solution that is based on a multilingual full-text repository of previously translated content. It has helped global organizations and language-industry professionals to improve translation productivity and quality for all types of content. Unlike traditional translation memory tools, which are based on a database of isolated whole sentences, MultiTrans makes vast collections of legacyfull-text translations searchable fortext stringsof any length in their full usage context.MultiTrans' interactive research agent automates and aggregates the search process, providing users with the most relevant information, maximizing language resource reuse.
This paper describes a Multi-language Translation Example Browser, a type of translation memory system. The system is able to retrieve translation examples from bilingual news databases, which consist of news transcripts of past broadcasts. We put a Japanese-English system to practical use and undertook trial operations of a system of eight language-pairs.
This paper presents the online demo of Matador, a large-scale Spanish-English machine translation system implemented following the Generation-heavy Hybrid Machine Translation (GHMT) approach.
We present a new large-scale database called {``}CatVar{''} (Habash and Dorr, 2003) which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications, our categorial-variation resource may serve as an integral part of a diverse range of natural language applications. Thus, the research reported herein overlaps heavily with that of the machine-translation, lexicon-construction, and information-retrieval communities. We demonstrate this database, embedded in a graphical interface; we also show a GUI for user input of corrections to the database.
In response to growing needs for cross-lingual patent retrieval, we propose PRIME (Patent Retrieval In Multilingual Environment system), in which users can retrieve and browse patents in foreign languages only by their native language. PRIME translates a query in the user language into the target language, retrieves patents relevant to the query, and translates retrieved patents into the user language. To update a translation dictionary, PRIME automatically extracts new translations from parallel patent corpora. In the current implementation, trilingual (J/E/K) patent retrieval is available. We describe the system design and its evaluation.
This paper describes an implementation of Collaborative Translation Environment {`}Yakushite Net{'}. In {`}Yakushite Net{'}, Internet users collaborate in enhancing the dictionaries of their specialty fields, and the system thus improves and expands its accuracy and areas of translations. In the course of realization of this system, we encountered several technical challenges. We would like to first explain those challenges, and then the solutions to them. Our future plan will also be explained at the end.
Combining machine translation (MT), translation memory (TM), XML, and an automation server, the LTC Communicator enables help desk systems to handle multilingual data by providing automatic translation on the fly. The system has been designed to deliver machine-translated questions/answers (trouble tickets/solutions) at an intelligible level. The modular architecture combining automation servers and workflow management gives flexibility and reliability to the overall system. The web server architecture allows remote access and easy integration with existing help desk systems. A trial was funded within the framework of the EU project IMPACT.
This paper presents an overview of the tools provided by KANTOO MT system for controlled source language checking, source text analysis, and terminology management. The steps in each process are described, and screen images are provided to illustrate the system architecture and example tool interfaces.
This paper presents a system overview of an English to Hindi Machine-Aided Translation System named AnglaHindi. Its beta-version has been made available on the internet for free translation at http://anglahindi.iitk.ac.in AnglaHindi is an English to Hindi version of the ANGLABHARTI translation methodology developed by the author for translation from English to all Indian languages. Anglabharti is a pseudo-interlingual rule-based translation methodology. AnglaHindi, besides using the rule-bases, uses example-base and statistics to obtain more acceptable and accurate translation for frequently encountered noun and verb phrasals. This way a limited hybridization of rule-based and example-based approaches has been incorporated.
The aim of TransType2 (TT2) is to develop a new kind of Computer-Assisted Translation (CAT) system that will help solve a very pressing social problem: how to meet the growing demand for high-quality translation. To date, translation technology has not been able to keep pace with the demand for high-quality translation. The innovative solution proposed by TT2 is to embed a data driven Machine Translation (MT) engine within an interactive translation environment. In this way, the system combines the best of two paradigms: the CAT paradigm, in which the human translator ensures high-quality output; and the MT paradigm, in which the machine ensures significant productivity gains.
TWiC is an on-line word and expression translation syste m which uses a powerful parser to (i) properly identify the relevant lexical units, (ii) retrieve the base form of the selected word and (iii) recognize the presence of a multiword expression (compound, idiom, collocation) the selected word may be part of. The conjunction of state-of-the-art natural language parsing, multiword expression identification and large bilingual databases provides a powerful and effective tool for people who want to read on-line material in a foreign language which they are not completely fluent in. A full prototype version of TWiC has been completed for the English-French pair of languages.
Machine translation engines draw on various types of databases. This paper is concerned with Arabic as a source or target language, and focuses on lexical databases. The non-concatenative nature of Arabic morphology, the complex structure of Arabic word-forms, and the general use of vowel-free writing present a real challenge to NLP developers. We show here how and why a stem-grounded lexical database, the items of which are associated with grammar-lexis specifications {--} as opposed to a root-{\&}-pattern database {--}, is motivated both linguistically and with regards to efficiency, economy and modularity. Arguments in favour of databases relying on stems associated with grammar-lexis specifications (such as DIINAR.1 or the Arabic dB under development at SYSTRAN), rather than on roots and patterns, are the following: (a) The latter include huge numbers of rule-generated word-forms, which do not actually appear in the language. (b) Rule-generated lemmas {--} as opposed to existing ones {--} are widely under-specified with regards to grammar-lexis relations. (c) In a Semitic language such as Arabic, the mapping of grammar-lexis specifications that need to be associated with every lexical entry of the database is decisive. (d) These specifications can only be included in a stem-based dB. Points (a) to (d) are crucial and in the context of machine translation involving Arabic.
SYSTRAN started the design and the development of Arabic, Farsi and Urdu to English machine translation systems in July 2002. This paper describes the methodology and implementation adopted for dictionary building and morphological analysis. SYSTRAN{'}s IntuitiveCoding® technology (ICT) for facilitates the creation, update, and maintenance of Arabic, Farsi and Urdu lexical entries, is more modular and less costly. ICT for Arabic, Farsi, and Urdu requires the implementation of stem-based lexical entries, the authentic scripts for each language, a statistical Arabic stem-guesser, and separate declarative modules for internal and external morphology.
A number of corpus-based techniques have been used in the development of natural language processing application. One area in which these techniques have extensively been applied is lexical development. The current work is being undertaken in the context of a machine translation project in which lexical development activities constitute a significant portion of the overall task. In the first part, we applied corpus-based techniques to the extraction of collocations from Amharic text corpus. Analysis of the output reveals important collocations that can usefully be incorporated in the lexicon. This is especially true for the extraction of idiomatic expressions. The patterns of idiom formation which are observed in a small manually collected data enabled extraction of large set of idioms which otherwise may be difficult or impossible to recognize. Furthermore, preliminary results of other corpus-based techniques, that is, clustering and classification, that are currently being under investigation are presented. The results show that clustering performed no better than the frequency base line whereas classification showed a clear performance improvement over the frequency base line. This in turn suggests the need to carry out further experiments using large sets of data and more contextual information.
This paper addresses issues related to employing logic-based semantic composition as a meaning representation for Arabic within a unification-based syntax-semantics interface. Since semantic representation has to be compositional on the level of semantic processing λ-calculus based on Discourse Representation Theory can be utilized as a helpful and practical technique for the semantic construction of ARABIC in Arabic understanding systems. As ARABIC computational linguistics is also short of feature-based compositional syntax-semantics interfaces we hope that this approach might be a further motivation to redirect research to modern semantic construction techniques for developing an adequate model of semantic processing for Arabic and even no existing formal theory is capable to provide a complete and consistent account of all phenomena involved in Arabic semantic processing.
Most words in Modern Hebrew texts are morphologically ambiguous. We describe a method for finding the correct morphological analysis of each word in a Modern Hebrew text. The program first uses a small tagged corpus to estimate the probability of each possible analysis of each word regardless of its context and chooses the most probable analysis. It then applies automatically learned rules to correct the analysis of each word according to its neighbors. Finally, it uses a simple syntactical analyzer to further correct the analysis, thus combining statistical methods with rule-based syntactic analysis. It is shown that this combination greatly improves the accuracy of the morphological analysis{---}achieving up to 96.2{\%} accuracy.
The parsing of Arabic sentence is a necessary prerequisite for many natural language processing applications such as machine translation and information retrieval. In this paper we report our attempt to develop an efficient chart parser for Analyzing Modern Standard Arabic (MSA) sentence. From a practical point of view, the parser is able to satisfy syntactic constraints reducing parsing ambiguity. Lexical semantic features are also used to disambiguate the sentence structure. We explain also an Arabic morphological analyzer based on ATN technique. Both the Arabic parser and the Arabic morphological analyzer are implemented in Prolog. The linguistic rules were acquired from a set of sentences from MSA sentence in the Agriculture domain.
We formulate an original model for statistical machine translation (SMT) inspired by characteristics of the Arabic-English translation task. Our approach incorporates part-of-speech tags and linguistically motivated phrase chunks in a 2-level shallow syntactic model of reordering. We implement and evaluate this model, showing it to have advantageous properties and to be competitive with an existing SMT baseline. We also describe cross-categorial lexical translation coercion, an interesting component and side-effect of our approach. Finally, we discuss the novel implementation of decoding for this model which saves much development work by constructing finite-state machine (FSM) representations of translation probability distributions and using generic FSM operations for search. Algorithmic details, examples and results focus on Arabic, and the paper includes discussion on the issues and challenges of Arabic statistical machine translation.
We describe work in progress whose main objective is to create a collection of resources and tools for processing Hebrew. These resources include corpora of written texts, some of them annotated in various degrees of detail; tools for collecting, expanding and maintaining corpora; tools for annotation; lexicons, both monolingual and bilingual; a rule-based, linguistically motivated morphological analyzer and generator; and a WordNet for Hebrew. We emphasize the methodological issue of well-defined standards for the resources to be developed. The design of the resources guarantees their reusability, such that the output of one system can naturally be the input to another.
This paper experimentally compares two automatic evaluators, RED and BLEU, to determine how close the evaluation results of each automatic evaluator are to average evaluation results by human evaluators, following the ATR standard of MT evaluation. This paper gives several cautionary remarks intended to prevent MT developers from drawing misleading conclusions when using the automatic evaluators. In addition, this paper reports a way of using the automatic evaluators so that their results agree with those of human evaluators.
A hybrid approach to automatic derivation of class-based selectional preferences is proposed. A lexicon of selectional preferences can assist in handling several forms of ambiguity, a major problem for MT. The approach combines knowledge-rich parsing and lexicons, with statistics and corpus data. We illustrate the use of a selectional preference lexicon for anaphora resolution.
Information on subcategorization and selectional restrictions is important for natural language processing tasks such as deep parsing, rule-based machine translation and automatic summarization. In this paper we present a method of adding detailed entries to a bilingual dictionary, based on information in an existing valency dictionary. The method is based on two assumptions: words with similar meaning have similar subcategorization frames and selectional restrictions; and words with the same translations have similar meanings. Based on these assumptions, new valency entries are constructed from words in a plain bilingual dictionary, using entries with similar source-language meaning and the same target-language translations. We evaluate the effects of various measures of similarity in increasing accuracy.
Many corpus-based Machine Translation (MT) systems generate a number of partial translations which are then pieced together rather than immediately producing one overall translation. While this makes them more robust to ill-formed input, they are subject to disfluencies at phrasal translation boundaries even for well-formed input. We address this {``}boundary friction{''} problem by introducing a method that exploits overlapping phrasal translations and the increased confidence in translation accuracy they imply. We specify an efficient algorithm for producing translations using overlap. Finally, our empirical analysis indicates that this approach produces higher quality translations than the standard method of combining non-overlapping fragments generated by our Example-Based MT (EBMT) system in a peak-to-peak comparison.
When multilingual communication through a speech-to-speech translation system is supported by multimodal features, e.g. pen-based gestures, the following issues arise concerning the nature of the supported communication: a) to what extend does multilingual communication differ from {`}ordinary{'} monolingual communication with respect to the dialogue structure and the communicative strategies used by participants; b) the patterns of integration between speech and gestures. Building on the outcomes of a previous work, we present results from a study aimed at addressing those issues. The initial findings confirm that multilingual communication, and the way in which it is realized by actual systems (e.g., with or without the push-to-talk mode) affects the form and structure of the conversation.
We present a syntax-based language model for use in noisy-channel machine translation. In particular, a language model based upon that described in (Cha01) is combined with the syntax based translation-model described in (YK01). The resulting system was used to translate 347 sentences from Chinese to English and compared with the results of an IBM-model-4-based system, as well as that of (YK02), all trained on the same data. The translations were sorted into four groups: good/bad syntax crossed with good/bad meaning. While the total number of translations that preserved meaning were the same for (YK02) and the syntax-based system (and both higher than the IBM-model-4-based system), the syntax based system had 45{\%} more translations that also had good syntax than did (YK02) (and approximately 70{\%} more than IBM Model 4). The number of translations that did not preserve meaning, but at least had good grammar, also increased, though to less avail.
Intelligibility and fidelity are the two key notions in machine translation system evaluation, but do not always provide enough information for system development. Detailed information about the type and number of errors of each type that a translation system makes is important for diagnosing the system, evaluating the translation approach, and allocating development resources. In this paper, we present a fine-grained machine translation evaluation framework that, in addition to the notions of intelligibility and fidelity, includes a typology of errors common in automatic translation, as well as several other properties of source and translated texts. The proposed framework is informative, sensitive, and relatively inexpensive to apply, to diagnose and quantify the types and likely sources of translation error. The proposed fine-grained framework has been used in two evaluation experiments on the LMT English-Spanish machine translation system, and has already suggested one important architectural improvement of the system.
We approach to correcting features in transferred linguistic representations in machine translation. The hybrid approach combines decision trees and transformation-based learning. Decision trees serve as a filter on the intractably large search space of possible interrelations among features. Transformation-based learning results in a simple set of ordered rules that can be compiled and executed after transfer and before sentence realization in the target language. We measure the reduction in noise in the linguistic representations and the results of human evaluations of end-to-end English-German machine translation.
We describe a large-scale investigation of the correlation between human judgments of machine translation quality and the automated metrics that are increasingly used to drive progress in the field. We compare the results of 124 human evaluations of machine translated sentences to the scores generated by two automatic evaluation metrics (BLEU and NIST). When datasets are held constant or file size is sufficiently large, BLEU and NIST scores closely parallel human judgments. Surprisingly, this was true even though these scores were calculated using just one human reference. We suggest that when human evaluators are forced to make decisions without sufficient context or domain expertise, they fall back on strategies that are not unlike determining n-gram precision.
N-gram measures of translation quality, such as BLEU and the related NIST metric, are becoming increasingly important in machine translation, yet their behaviors are not fully understood. In this paper we examine the performance of these metrics on professional human translations into German of two literary genres, the Bible and Tom Sawyer. The most surprising result is that some machine translations outscore some professional human translations. In addition, it can be difficult to distinguish some other human translations from machine translations with only two reference translations; with four reference translations it is much easier. Our results lead us to conclude that much care must be taken in using n-gram measures in formal evaluations of machine translation quality, though they are still valuable as part of the iterative development cycle.
Word Order transfer is a compulsory stage and has a great effect on the translation result of a transfer-based machine translation system. To solve this problem, we can use fixed rules (rule-based) or stochastic methods (corpus-based) which extract word order transfer rules between two languages. However, each approach has its own advantages and disadvantages. In this paper, we present a hybrid approach based on fixed rules and Transformation-Based Learning (or TBL) method. Our purpose is to transfer automatically the English word orders into the Vietnamese ones. The learning process will be trained on the annotated bilingual corpus (named EVC: English-Vietnamese Corpus) that has been automatically word-aligned, phrase-aligned and POS-tagged. This transfer result is being used for the transfer module in the English-Vietnamese transfer-based machine translation system.
Machine Translation (MT) is the most interesting and difficult task which has been posed since the beginning of computer history. The highest difficulty which computers had to face with, is the built-in ambiguity of Natural Languages. Formerly, a lot of human-devised rules have been used to disambiguate those ambiguities. Building such a complete rule-set is time-consuming and labor-intensive task whilst it doesn{'}t cover all the cases. Besides, when the scale of system increases, it is very difficult to control that rule-set. In this paper, we present a new model of learning-based MT (entitled BTL: Bitext-Transfer Learning) that learns from bilingual corpus to extract disambiguating rules. This model has been experimented in English-to-Vietnamese MT system (EVT) and it gave encouraging results.
Structural divergence presents a challenge to the use of syntax in statistical machine translation. We address this problem with a new algorithm for alignment of loosely matched non-isomorphic dependency trees. The algorithm selectively relaxes the constraints of the two tree structures while keeping computational complexity polynomial in the length of the sentences. Experimentation with a large Chinese-English corpus shows an improvement in alignment results over the unstructured models of (Brown et al., 1993).
We describe an experiment in rapid development of a statistical machine translation (SMT) system from scratch, using limited resources: under this heading we include not only training data, but also computing power, linguistic knowledge, programming effort, and absolute time.
The Department of Linguistics of the Centro Ramón Piñeiro para a Investigación en Humanidades (C.R.P.I.H.), headed by Professor Guillermo Rojo, has developed Es-Ga, a machine translation system based on the Metal system which at the present time translates from Spanish into Galician in .rtf, .txt and .html formats. It also contains a number of programmes whose function is to deformat documents that are then translated and, once this process has finished, to reconstruct their original format. The system has a tool bar with linguistic information designed for MS-WORD, the functionality and functioning of which has proven unquestionable as an aid to the posteditor in a context of linguistic interference between two intercomprehensible languages.
This paper proposes a method of automatic transliteration from English to Japanese words. Our method successfully transliterates an English word not registered in any bilingual or pronunciation dictionaries by converting each partial letters in the English word into Japanese katakana characters. In such transliteration, identical letters occurring in different English words must often be converted into different katakana. To produce an adequate transliteration, the proposed method considers chunking of alphabetic letters of an English word into conversion units and considers English and Japanese context information simultaneously to calculate the plausibility of conversion. We have confirmed experimentally that the proposed method improves the conversion accuracy by 63{\%} compared to a simple method that ignores the plausibility of chunking and contextual information.
The theme of controlled translation is currently in vogue in the area of MT. Recent research (Scha ̈ler et al., 2003; Carl, 2003) hypothesises that EBMT systems are perhaps best suited to this challenging task. In this paper, we present an EBMT system where the generation of the target string is filtered by data written according to controlled language specifications. As far as we are aware, this is the only research available on this topic. In the field of controlled language applications, it is more usual to constrain the source language in this way rather than the target. We translate a small corpus of controlled English into French using the on-line MT system Logomedia, and seed the memories of our EBMT system with a set of automatically induced lexical resources using the Marker Hypothesis as a segmentation tool. We test our system on a large set of sentences extracted from a Sun Translation Memory, and provide both an automatic and a human evaluation. For comparative purposes, we also provide results for Logomedia itself.
Divergence is a key aspect of translation between two languages. Divergence occurs when structurally similar sentences of the source language do not translate into sentences that are similar in structures in the target language. Divergence assumes special significance in the domain of Example-Based Machine Translation (EBMT). An EBMT system generates translation of a given sentence by retrieving similar past translation examples from its example base and then adapting them suitably to meet the current translation requirements. Divergence imposes a great challenge to the success of EBMT. The present work provides a technique for identification of divergence without going into the semantic details of the underlying sentences. This identification helps in partitioning the example database into divergence / non-divergence categories, which in turn should facilitate efficient retrieval and adaptation in an EBMT system.
This paper describes and evaluates Matador, an implemented large-scale Spanish-English MT system built in the Generation-Heavy Hybrid Machine Translation (GHMT) approach. An extensive evaluation shows that Matador has a higher degree of robustness and superior output quality, in terms of grammaticality and accuracy, when compared to a primarily statistical approach.
The multilingual machine translation system described in the first part of this paper demonstrates that the translation memory (TM) can be used in a creative way for making the translation process more automatic (in a way which in fact does not depend on the languages used). The MT system is based upon exploitation of syntactic similarities between more or less related natural languages. It currently covers the translation from Czech to Slovak, Polish and Lithuanian. The second part of the paper also shows that one of the most popular TM based commercial systems, TRADOS, can be used not only for the translation itself, but also for a relatively fast and natural method of evaluation of the translation quality of MT systems.
Data-Oriented Translation (DOT), which is based on Data-Oriented Parsing (DOP), comprises an experience-based approach to translation, where new translations are derived with reference to grammatical analyses of previous translations. Previous DOT experiments [Poutsma, 1998, Poutsma, 2000a, Poutsma, 2000b] were small in scale because important advances in DOP technology were not incorporated into the translation model. Despite this, related work [Way, 1999, Way, 2003a, Way, 2003b] reports that DOT models are viable in that solutions to {`}hard{'} translation cases are readily available. However, it has not been shown to date that DOT models scale to larger datasets. In this work, we describe a novel DOT system, inspired by recent advances in DOP parsing technology. We test our system on larger, more complex corpora than have been used heretofore, and present both automatic and human evaluations which show that high quality translations can be achieved at reasonable speeds.
We introduced, for Translation Memory System, a statistical framework, which unifies the different phases in a Translation Memory System by letting them constrain each other, and enables Translation Memory System a statistical qualification. Compared to traditional Translation Memory Systems, our model operates at a fine grained sub-sentential level such that it improves the translation coverage. Compared with other approaches that exploit sub-sentential benefits, it unifies the processes of source string segmentation, best example selection, and translation generation by making them constrain each other via the statistical confidence of each step. We realized this framework into a prototype system. Compared with an existing product Translation Memory System, our system exhibits obviously better performance in the ``assistant quality metric'' and gains improvements in the range of 26.3{\%} to 55.1{\%} in the ``translation efficiency metric''.
The common assertion that MT systems have improved over the last decades is examined by informal comparisons of translations produced by operational systems in the 1960s, 1970s and 1980s and of translations of the same source texts produced by some currently available commercial and online systems. The scarcity of source and target texts for earlier systems means that the conclusions are consequently tentative and preliminary.
CLS Corporate Language Services AG recently began offering the rapid post-editing of raw machine translation output to meet the rising demand for this service among clients. What is meant by rapid post-editing is the rough correction of machine translated texts with emphasis on speed and denotative accuracy. In the preliminary phase of the project, CLS conducted a test among four in-house translators. The objective was to gain practical experience, establish workflow requirements and set up efficient post-editing processes. Text samples were selected from several subject categories, and post-edited in English, German and French. The participants were given 10, 15 and 30 minutes per page to complete their tasks. This paper aims to present the results of the post-editing test at CLS Corporate Language Services AG, and to examine the conditions under which a rapid post-editing service is feasible in a commercial environment.
Inter-word associations like stagger - drunken, or intra-word sense divisions (e.g. write a diary vs. write an article) are difficult to compile using a traditional lexicographic approach. As an alternative, we present a model that reflects this kind of subtle lexical knowledge. Based on the minimal sense of a word (clique), the model (1) selects contextually related words (contexonyms) and (2) classifies them in a multi-dimensional semantic space. Trained on very large corpora, the model provides relevant, organized contexonyms that reflect the fine-grained connotations and contextual usage of the target word, as well as the distinct senses of homonyms and polysemous words. Further study on the neighbor effect showed that the model can handle the data sparseness problem.
This paper describes a framework for multilingual translation using existing translation engines. Our method allows translation between non-English languages through English as a {``}hub language{''}. This hub language method has two major problems: {``}information loss{''} and {``}error accumulation{''}. In order to address these problems, we represent the hub language using the Linguistic Annotation Language (LAL), which contains English syntactic information and source language information. We show the effectiveness of the annotation approach with a series of experiments.
This paper describes an approach to analyzing the lexical structure of OCRed bilingual dictionaries to construct resources suited for machine translation of low-density languages, where online resources are limited. A rule-based, an HMM-based, and a post-processed HMM-based method are used for rapid construction of MT lexicons based on systematic structural clues provided in the original dictionary. We evaluate the effectiveness of our techniques, concluding that: (1) the rule-based method performs better with dictionaries where the font is not an important distinguishing feature for determining information types; (2) the post-processed stochastic method improves the results of the stochastic method for phrasal entries; and (3) Our resulting bilingual lexicons are comprehensive enough to provide the basis for reasonable translation results when compared to human translations.
Many studies have been reported in the domain of speech-to-speech machine translation systems for travel conversation use. Therefore, a large number of travel domain corpora have become available in recent years. From a wider viewpoint, speech-to-speech systems are required for many purposes other than travel conversation. One of these is monologues (e.g., TV news, lectures, technical presentations). However, in monologues, sentences tend to be long and complicated, which often causes problems for parsing and translation. Therefore, we need a suitable translation unit, rather than the sentence. We propose the clause as a unit for translation. To develop a speech-to-speech machine translation system for monologues based on the clause as the translation unit, we need a monologue parallel corpus with clause alignment. In this paper, we describe how to build a Japanese-English monologue parallel corpus with clauses aligned, and discuss the features of this corpus.
This paper presents FEMTI, a web-based Framework for the Evaluation of Machine Translation in ISLE. FEMTI offers structured descriptions of potential user needs, linked to an overview of technical characteristics of MT systems. The description of possible systems is mainly articulated around the quality characteristics for software product set out in ISO/IEC standard 9126. Following the philosophy set out there and in the related 14598 series of standards, each quality characteristic bottoms out in metrics which may be applied to a particular instance of a system in order to judge how satisfactory the system is with respect to that characteristic. An evaluator can use the description of user needs to help identify the specific needs of his evaluation and the relations between them. He can then follow the pointers to system description to determine what metrics should be applied and how. In the current state of the framework, emphasis is on being exhaustive, including as much as possible of the information available in the literature on machine translation evaluation. Future work will aim at being more analytic, looking at characteristics and metrics to see how they relate to one another, validating metrics and investigating the correlation between particular metrics and human judgement.
Pattern-based machine translation systems can be easily customized by adding new patterns. To gain full profits from this character, input of patterns should be both expressive and simple to understand. The pattern-based machine translation system we have developed simplifies the handling of features in patterns by allowing sharing constraints between non-terminal symbols, and implementing an automated scheme of feature inheritance between syntactic classes. To avoid conflicts inherent to the pattern-based approach the system has priority control between patterns and between dictionaries. This approach proved its scalability in the web-based collaborative translation environment {`}Yakushite Net.{'}
We introduce a string-to-string distance measure which extends the edit distance by block transpositions as constant cost edit operation. An algorithm for the calculation of this distance measure in polynomial time is presented. We then demonstrate how this distance measure can be used as an evaluation criterion in machine translation. The correlation between this evaluation criterion and human judgment is systematically compared with that of other automatic evaluation measures on two translation tasks. In general, like other automatic evaluation measures, the criterion shows low correlation at sentence level, but good correlation at system level.
In this paper we show why scalability is one of the most important aspects for the evaluation of Machine Translation (MT) systems and what scalability entails in the framework of MT. We illustrate the issue of scalability by reporting about an MT solution, which has been chosen in the course of a thorough hands-on evaluation and which in the meantime has been developed from a pilot system to a MT turnkey solution for mid-to large-scale enterprises.
This paper presents a source language diagnostic system for controlled translation. Diagnostics were designed and implemented to address the most difficult rewrites for authors, based on an empirical analysis of log files containing over 180,000 sentences. The design and implementation of the diagnostic system are presented, along with experimental results from an empirical evaluation of the completed system. We found that the diagnostic system can correctly identify the problem in 90.2{\%} of the cases. In addition, depending on the type of grammar problem, the diagnostic system may offer a rewritten sentence. We found that 89.4{\%} of the rewritten sentences were correctly rewritten. The results suggest that these methods could be used as the basis for an automatic rewriting system in the future.
This paper raises a neglected issue in the study of ellipsis resolution. The existence of ellipsis under certain constructions is often disguised due to the structure that assigns the nominative marking to what is typically the object. This kind of ellipsis deserves attention in view of the fact that its referent is the agent of the sentence and that these constructions are observed in diverse languages. A problem is posed by virtue of the fact that English is not one of those languages, and it overtly expresses the referent of ellipsis that is implicit in those languages that use those constructions. Hence, the recognition and resolution of such ellipses is of importance particularly in machine translation systems that translate sentences with {``}incognito ellipsis{''} from those languages into English. After presenting the types of constructions, the paper explicates the mechanisms that govern the constructions in Japanese, and proposes a method to resolve such incognito ellipses along with common ellipses in a unified manner.
The paper describes a novel approach to Multi-Engine Machine Translation. We build statistical models of performance of translations and use them to guide us in combining and selecting from outputs from multiple MT engines. We empirically demonstrate that the MEMT system based on the models outperforms any of its component engine.
Statistical techniques for machine translation offer promise for rapid development in response to unexpected requirements, but realizing that potential requires rapid acquisition of required resources as well. This paper reports the results of experiments with resources collected in ten days; about 1.3 million words of parallel text from five types of sources and a bilingual term list with about 20,000 term pairs. Systems were trained with resources individually and in combination, using an approach based on alignment templates. The use of all available resources was found to yield the best results in an automatic evaluation using the BLEU measure, but a single resource (the Bible) coupled with a small amount of in-domain manual translation (less than 6,000 words) achieved more than 85{\%} of that upper baseline. With a concerted effort, such a system could be built in a single day.
Machine-Translation of news headlines is difficult since the sentences are fragmentary and abbreviations and acronyms of proper names are frequently used. Another difficulty is that, since the headline comes at the top of a news article, the context information useful to disambiguate the sense of words and to determine their translation(target word) is not available. This paper proposes a new approach to translating English news headline. In this approach, the abbreviations and acronyms in the headlines are complemented with their coreference in the lead of the article. Moreover, the target word selection is performed by referring to the translation of similar news articles retrieved from a parallel corpus. In the experiment, 100 English headlines are translated into Japanese using a corpus containing 30,000 English-Japanese article pairs, resulting in a 17 {\%} improvement in the target words and a 21 {\%} improvement in the style of translation.
This paper reports on the development of a collocation extraction system that is designed within a commercial machine translation system in order to take advantage of the robust syntactic analysis that the system offers and to use this analysis to refine collocation extraction. Embedding the extraction system also addresses the need to provide information about the source language collocations in a system-specific form to support automatic generation of a collocation rulebase for analysis and translation.
The goal of the AMETRA project is to make a computer-assisted translation tool from the Spanish language to the Basque language under the memory-based translation framework. The system is based on a large collection of bilingual word-segments. These segments are obtained using linguistic or statistical techniques from a Spanish-Basque bilingual corpus consisting of sentences extracted from the Basque Country{'}s of{\pounds}cial government record. One of the tasks within the global information document of the AMETRA project is to study the combination of well-known statistical techniques for the translation of short sequences and techniques for memory-based translation. In this paper, we address the problem of constructing a statistical module to deal with the task of translating segments. The task undertaken in the AMETRA project is compared with other existing translation tasks, This study includes the results of some preliminary experiments we have carried out using well-known statistical machine translation tools and techniques.
This paper reports results from an experiment that was aimed at comparing evaluation metrics for machine translation. Implemented as a workshop at a major conference in 2002, the experiment defined an evaluation task, description of the metrics, as well as test data consisting of human and machine translations of two texts. Several metrics, either applicable by human judges or automated, were used, and the overall results were analyzed. It appeared that most human metrics and automated metrics provided in general consistent rankings of the various candidate translations; the ranking of the human translations matched the one provided by translation professionals; and human translations were distinguished from machine translations.
In machine translation, information on word ambiguities is usually provided by the lexicographers who construct the lexicon. In this paper we propose an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word. The approach is based on the statistics of the distributional similarity between the words in a corpus. Our algorithm works as follows: The 20 strongest first-order associations to the ambiguous word are considered as sense descriptor candidates. All pairs of these candidates are ranked according to the following two criteria: First, the two words in a pair should be as dissimilar as possible. Second, although being dissimilar their co-occurrence vectors should add up to the co-occurrence vector of the ambiguous word scaled by two. Both conditions together have the effect that preference is given to pairs whose co-occurring words are complementary. For best results, our implementation uses singular value decomposition, entropy-based weights, and second-order similarity metrics.
This paper describes a sentence pattern-based English-Korean machine translation system backed up by a rule-based module as a solution to the translation of long sentences. A rule-based English-Korean MT system typically suffers from low translation accuracy for long sentences due to poor parsing performance. In the proposed method we only use chunking information on the phrase-level of the parse result (i.e. NP, PP, and AP). By applying a sentence pattern directly to a chunking result, the high performance of analysis and a good quality of translation are expected. The parsing efficiency problem in the traditional RBMT approach is resolved by sentence partitioning, which is generally assumed to have many problems. However, we will show that the sentence partitioning has little side effect, if any, in our approach, because we use only the chunking results for the transfer. The coverage problem of a pattern-based method is overcome by applying sentence pattern matching recursively to the sub-sentences of the input sentence, in case there is no exact matching pattern to the input sentence.
Prepositional phrase attachment (PP attachment) is a major source of ambiguity in English. It poses a substantial challenge to Machine Translation (MT) between English and languages that are not characterized by PP attachment ambiguity. In this paper we present an unsupervised, bilingual, corpus-based approach to the resolution of English PP attachment ambiguity. As data we use aligned linguistic representations of the English and Japanese sentences from a large parallel corpus of technical texts. The premise of our approach is that with large aligned, parsed, bilingual (or multilingual) corpora, languages can learn non-trivial linguistic information from one another with high accuracy. We contend that our approach can be extended to linguistic phenomena other than PP attachment.
Customization of Machine Translation (MT) is a prerequisite for corporations to adopt the technology. It is therefore important but nonetheless challenging. Ongoing implementation proves that XML is an excellent exchange device between MT modules that efficiently enables interaction between the user and the processes to reach highly granulated structure-based customization. Accomplished through an innovative approach called the SYSTRAN Translation Stylesheet, this method is coherent with the current evolution of the {``}authoring process{''}. As a natural progression, the next stage in the customization process is the integration of MT in a multilingual tool kit designed for the {``}authoring process{''}.
Customizing a general-purpose MT system is an effective way to improve machine translation quality for specific usages. Building a user-specific dictionary is the first and most important step in the customization process. An intuitive dictionary-coding tool was developed and is now utilized to allow the user to build user dictionaries easily and intelligently. SYSTRAN{'}s innovative and proprietary IntuitiveCoding® technology is the engine powering this tool. It is comprised of various components: massive linguistic resources, a morphological analyzer, a statistical guesser, finite-state automaton, and a context-free grammar. Methodologically, IntuitiveCoding® is also a cross-application approach for high quality dictionary building in terminology import and exchange. This paper describes the various components and the issues involved in its implementation. An evaluation frame and utilization of the technology are also presented.
Example-based machine translation (EBMT) is a promising translation method for speech-to-speech translation (S2ST) because of its robustness. However, it has two problems in that the performance degrades when input sentences are long and when the style of the input sentences and that of the example corpus are different. This paper proposes example-based rough translation to overcome these two problems. The rough translation method relies on {``}meaning-equivalent sentences,{''} which share the main meaning with an input sentence despite missing some unimportant information. This method facilitates retrieval of meaning-equivalent sentences for long input sentences. The retrieval of meaning-equivalent sentences is based on content words, modality, and tense. This method also provides robustness against the style differences between the input sentence and the example corpus.
We describe the implementation of two new language pairs (English-French and English-German) which use machine-learned sentence realization components instead of hand-written generation components. The resulting systems are evaluated by human evaluators, and in the technical domain, are equal to the quality of highly respected commercial systems. We comment on the difficulties that are encountered when using machine-learned sentence realization in the context of MT.
While spoken language translation remains a research goal, a crude form of it is widely available commercially for Japanese{--}English as a pipeline concatenation of speech-to-text recognition (SR), text-to-text translation (MT) and text-to-speech synthesis (SS). This paper proposes and illustrates an evaluation methodology for this noisy channel which tries to quantify the relative amount of degradation in translation quality due to each of the contributing modules. A small pilot experiment involving word-accuracy rate for the SR, and a fidelity evaluation for the MT and SS modules is proposed in which subjects are asked to paraphrase translated and/or synthesised sentences from a tourist{'}s phrasebook. Results show (as expected) that MT is the {``}noisiest{''} channel, with SS contributing least noise. The concatenation of the three channels is worse than could be predicted from the performance of each as individual tasks.
We present a method for compositionally translating Japanese NN compounds into English, using a word-level transfer dictionary and target language monolingual corpus. The method interpolates over fully-specified and partial translation data, based on corpus evidence. In evaluation, we demonstrate that interpolation over the two data types is superior to using either one, and show that our method performs at an F-score of 0.68 over translation-aligned inputs and 0.66 over a random sample of 500 NN compounds.
Evaluation of MT evaluation measures is limited by inconsistent human judgment data. Nonetheless, machine translation can be evaluated using the well-known measures precision, recall, and their average, the F-measure. The unigram-based F-measure has significantly higher correlation with human judgments than recently proposed alternatives. More importantly, this standard measure has an intuitive graphical interpretation, which can facilitate insight into how MT systems might be improved. The relevant software is publicly available from http://nlp.cs.nyu.edu/GTM/.
In this paper, we present several confidence measures for (statistical) machine translation. We introduce word posterior probabilities for words in the target sentence that can be determined either on a word graph or on an N best list. Two alternative confidence measures that can be calculated on N best lists are proposed. The performance of the measures is evaluated on two different translation tasks: on spontaneously spoken dialogues from the domain of appointment scheduling, and on a collection of technical manuals.
In this paper we describe the components of our statistical machine translation system. This system combines phrase-to-phrase translations extracted from a bilingual corpus using different alignment approaches. Special methods to extract and align named entities are used. We show how a manual lexicon can be incorporated into the statistical system in an optimized way. Experiments on Chinese-to-English and Arabic-to-English translation tasks are presented.
This paper presents a decoder for statistical machine translation that can take advantage of the example-based machine translation framework. The decoder presented here is based on the greedy approach to the decoding problem, but the search is initiated from a similar translation extracted from a bilingual corpus. The experiments on multilingual translations showed that the proposed method was far superior to a word-by-word generation beam search algorithm.
Recent work in machine translation and information extraction has demonstrated the utility of a level that represents the predicate-argument structure. It would be especially useful for machine translation to have two such Proposition Banks, one for each language under consideration. A Proposition Bank for English has been developed over the last few years, and we describe here our development of a tool for facilitating the development of a Chinese Proposition Bank. We also discuss some issues specific to the Chinese Treebank that complicate the matter of mapping syntactic representation to a predicate-argument level, and report on some preliminary evaluation of the accuracy of the semantic tagging tool.
The statistical Machine Translation Model has two components: a language model and a translation model. This paper describes how to improve the quality of the translation model by using the common word pairs extracted by two asymmetric learning approaches. One set of word pairs is extracted by Viterbi alignment using a translation model, the other set is extracted by Viterbi alignment using another translation model created by reversing the languages. The common word pairs are extracted as the same word pairs in the two sets of word pairs. We conducted experiments using English and Japanese. Our method improves the quality of a original translation model by 5.7{\%}. The experiments also show that the proposed learning method improves the word alignment quality independent of the training domain and the translation model. Moreover, we show that common word pairs are almost as useful as regular dictionary entries for training purposes.
The customization of Machine Translation systems concentrates, for the most part, on MT dictionaries. In this paper, we focus on the customization of complex lexical entries that involve various types of lexical collocations, such as sub-categorization frames. We describe methods and tools that leverage existing parsers and other MT dictionaries for customization of MT dictionaries. This customization process is applied on large-scale customization of several commercial MT systems, including English to Japanese, Chinese, and Korean.
In this paper the authors wish to present a view of translation equivalence related to a pragmatics-based approach to machine translation. We will argue that current evaluation methods which assume that there is a predictable correspondence between language forms cannot adequately account for this view. We will then describe a method for objectively determining the relative equivalence of two texts. However, given the need for both an open world assumption and non-monotonic inferencing, such a method cannot be realistically implemented and therefore certain ``classic'' evaluation strategies will continue to be preferable as practical methods of evaluation.
Two string comparison measures, edit distance and n-gram co-occurrence, are tested for automatic evaluation of translation quality, where the quality is compared to one or several reference translations. The measures are tested in combination for diagnostic evaluation on segments. Both measures have been used for evaluation of translation quality before, but for another evaluation purpose (performance) and with another granularity (system). Preliminary experiments showed that the measures are not portable without redefinitions, so two new measures are defined, WAFT and NEVA. The new measures could be applied for both purposes and granularities.
This paper looks at granularity issues in machine translation evaluation. We start with work by (White, 2001) who examined the correlation between intelligibility and fidelity at the document level. His work showed that intelligibility and fidelity do not correlate well at the document level. These dissimilarities lead to our investigation of evaluation granularity. In particular, we revisit the intelligibility and fidelity relationship at the corpus level. We expect these to support certain assumptions in both evaluations as well as indicate issues germane to future evaluations.
Even with recent, renewed attention to MT evaluation{---}due in part to n-gram-based metrics (Papineni et al., 2001; Doddington, 2002) and the extensive, online catalogue of MT metrics on the ISLE project (Hovy et al., 2001, 2003), few reports involving task-based metrics have surfaced. This paper presents our work on three parts of task-based MT evaluation: (i) software to track and record users' task performance via a browser, run from a desktop computer or remotely over the web, (ii) factorial experimental design with replicate observations to compare the MT engines, based on the accuracy of users' task responses, and (iii) the use of chi-squared and generalized linear models (GLMs) to permit finer-grained data analyses. We report on the experimental results of a six-way document categorization task, used for the evaluation of three Korean-English MT engines. The statistical models of the probabilities of correct responses yield an ordering of the MT engines, with one engine having a statistically significant lead over the other two. Future research will involve testing user performance on linguistically more complex tasks, as well as extending our initial GLMs with the documents' Bleu scores as variables, to test the scores as independent predictors of task results.
The goal of Question-Answering (QA) systems is to find short and factual answers to opendomain questions by searching a large collection of documents. The subject of this research is to formulate complete and natural answer-sentences to questions, given the short answer. The answer-sentences are meant to be self-sufficient; that is, they should contain enough context to be understood without needing the original question. Generating such sentences is important in question-answering as they can be used to enhance existing QA systems to provide answers to the user in a more natural way and to provide a pattern to actually extract the answer from the document collection.
This paper describes the use of decision trees to learn lexical information for the enrichment of our natural language processing (NLP) system. Our approach to lexical learning differs from other approaches in the field in that our machine learning techniques exploit a deep knowledge understanding system. After the introduction we present the overall architecture of our lexical learning module. In the following sections we present a showcase of lexical learning using decision trees: we learn verbs that take a human subject in Spanish and French.
Previous studies on automatic extraction of lexical similarities have considered as semantic unit of text the word. However, the theory of contextual lexical semantics implies that larger segments of text, namely non-compositional multiwords, are more appropriate for this role. We experimentally tested the applicability of this notion applying automatic collocation extraction to identify and merge such multiwords prior to the similarity estimation process. Employing an automatic WordNet-based comparative evaluation scheme along with a manual evaluation procedure, we ascertain improvement of the extracted similarity relations.
A new variant of structured contextual grammar, which generates dependency trees, is introduced. The new generative model, called dependency contextual grammar, improves both the strong and weak generative power of contextual grammars, while being a potential candidate for the mathematical description of dependency-based syntactic models.
This paper presents the French implementation of Amalgam, a machine-learned sentence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models.
In spoken dialog systems, a wellcrafted prompt is important in order to get the user to respond with an expected type of utterance. We identify a new, important area for research in speechto-speech translation, which focuses on the fact that the output of the MT system serves as the prompt for the user on each end. The MT engine used in speech-to-speech translation must pay special attention to its generation component, to such an extent that it makes sense to talk about controlled generation. Some rules for controlled generation are given.  which focuses on the fact that the output of the MT system serves as the prompt for the user on each end. Since prompt design is extremely important in eliciting expected types of responses, the MT engine used in speech-to-speech translation must pay special attention to its generation component, to such an extent that it makes sense to talk about controlled generation. Thus this paper stands at the intersection of three important natural language technologies: Controlled language, machine translation, and speech processing. We will ﬁrst, in Section 2, give some examples demonstrating the importance of a good prompt for SDSs. In Section 3 we relate MT output to dialog prompts, and in Section 4 we give some suggestions for CL rules in the context of speech-to-speech MT.  
A formally defined subset of English known as EasyEnglish is described. Primarily designed for a culturally diverse audience of second-language English speakers, EasyEnglish seeks to clarify meaning through restrictions on vocabulary and grammar, whilst preserving intellectual content. EasyEnglish has been developed by Wycliffe Associates (UK), an organisation that produces Biblical materials for a worldwide audience. Special features of EasyEnglish include: (1) two levels of lexical restriction (approximately 1,200 words and 2,800 words respectively); (2) constraints on the range of meanings allowed for permitted terms; (3) logical reformulation and application of grammatical structures designed to maximise comprehensibility; and (4) application of established techniques for translation across cultural boundaries. Printed and computerised tools for writing are briefly outlined, together with future developments. The efforts we make in making the EasyEnglish texts clear to people from a wide diversity of cultures and who speak a wide range of mother tongues may contribute significantly to the wide acceptance of our material; valid criticisms of EasyEnglish are also pointed out. Three particular current challenges are: selection of corpora from which suitable vocabulary can be drawn; fine-tuning the grammatical rules; and obtaining feedback from users.  
By this paper, we present the INTERLINGUA Project1: its design and current work. The goal of the project is achieving fully-automatic (no pre-edition, no post-edition) translation of emails in the virtual campus of the Open University of Catalonia (UOC). The problem of unsupervised machine translation of emails is discussed. Then we describe the strategy designed to build the system, including a multiple-level evaluation process and the building of several automatic preedition, post-edition and unknown-word extraction modules. Last, the work carried on so far on building such decision-taking modules is presented. 
This paper describes the Cataloguing Tool of the Mkbeem multilingual eCommerce mediation system. The Cataloguing Tool is used by product suppliers to provide necessary product information in a form required by the mediation system. The relevant information includes product articles that are maintained in a pivot language and automatically translated to other supported languages. From the supplier viewpoint the Cataloguing Tool implements "write-once-publish-many" paradigm. Other functionalities of the tool include automatic extraction of product properties from text articles along to an ontological product model and automatic classification of products based on their properties and ontology models. This paper describes the Cataloguing Tool and discusses in more detail about the use of ontologies in automatic interpretation of the semantics of product articles and user queries. 
It is a general assumption that 1) the readability and clarity of LSP texts written in a controlled language are better than uncontrolled texts and 2) that controlled languages produce better results with machine translation than uncontrolled languages. Controlled languages impose lexical, syntactic and pragmatic restrictions on the writing style of the author. We will focus on syntactic restrictions and investigate whether a writing style in which various forms of grammatical metaphors have been dissolved in order to make the text more accessible to human readers, will in fact make the texts more suitable for MT. The basis of our investigation is a small corpus of English LSP texts that have been evaluated regarding their accessibility and acceptability by human users. The MT-system we will introduce as an additional ”user” of the texts is the English-Danish prototype of the Comprendium MT-system.  
This paper presents the results of an analysis of eight Controlled English rule sets. The objective of this analysis was to discover the extent to which Controlled Language rule sets shared common rules and to try to establish a core set of CL rules for English. The analysis reveals that, although there is some commonality of rules across some rule sets, all eight CL rule sets have but one rule in common. Therefore, it is not possible to derive a core set of CL rules for English from this analysis. The lack of a core rule set makes it difficult for organisations who want to implement CL without reinventing the wheel. The author provides a suggestion for the most important rules for controlling English, based on the common rules across the eight CLs analysed here. 
According to common understanding the definition of a Controlled Language (CL) depends on its intended use: if improvement in text quality in terms of readability, clarity and comprehensibility is the primary goal of a CL, its rules will differ from CL rules which aim foremost at the improvement of translatability in an automated translation processing environment. In this article the following questions will be addressed: what is the relationship between the two CL approaches and rules? Are there any overlaps, contradictions, or interactions? Are the two approaches compatible and, if so, can they be combined? The findings presented are based on both the outcome of a CL research project and experiences in industrial CL applications. 
AECMA Simplified English for aircraft maintenance manuals is one of the bestknown examples of a controlled language. In this paper we describe the development of its equivalent for Spanish. We also present a prototype validating parser and outline its evaluation. 
 This paper describes a project currently  under way at SAP dealing with the task of  post-editing MT output. As a concrete re-  sult of the project, a standard post-editing  guide to be used by translator end users is  currently being created. The purpose of  this post-editing guide is not only to train  and support translators in the daily work  with MT but also to make the post-editing  task more accessible to them, thus en-  couraging an open-minded attitude to-  wards  translation  technology.  Furthermore the systematic error typology  underlying the guide serves not only as a  methodological framework for the re-  search on post-editing but also as a diag-  nosis for necessary corrections and  enhancements to be carried out in the cor-  responding MT systems used.  In the context of the project descrip-  tion, the related research in the field of  automated translation processes as well as  the experiences made with MT at SAP are  illustrated.  
This paper presents ECOLE, a look-ahead text editor that supports authors writing seemingly informal specifications in PENG, a computer-processable controlled natural language. ECOLE communicates via a socket interface with the controlled language processor of the PENG system. After each word form entered the lookahead editor displays appropriate lookahead categories. These syntactic hints tell the author what kind of word or syntactic structure can follow the current input string and reduce thereby the cognitive burden to learn and remember the controlled language. While the author types the text word by word and adds unknown content words on the fly to the lexicon, a discourse representation structure and a paraphrase is built up dynamically for the text in a completely compositional manner. The arising specification can be checked automatically for consistency and informativity with the help of third-party reasoning services. 
 florence.beaujard@airbus.com  emmanuelle.cannesson@airbus.com  ABSTRACT_________________________________________________________________  In 1998, Airbus started a project dedicated to the creation of a controlled language for industrial use; in this case enhancing warning texts quality in the cockpit of Airbus aircraft. Another objective was to provide designers a means of facilitating their job while respecting the stringent safety criteria. This project was divided into three parts conducted in the frame of R&D activities, the first one dealing with the terminological aspects, the second devoted to syntax and the third one focused on the acronyms for computers naming. Throughout this innovative linguistic project, Airbus proves to be aware of the constant need to enhance safety.  Keywords: Controlled language – Word order structuring – Terminological standardization – Abbreviations – Procedural texts – Acronyms. ___________________________________________________________________________  Introduction : what is a CL ?  Controlled languages (CLs) are of  vital interest (for safety and economic  reasons, etc.) for industry. Indeed, they  have been created in order to resolve  problems of readability (reducing the  complexity of syntactic structures of a text  increases its readability), of  comprehensibility  (a  lexical  disambiguation  increases  the  comprehensibility of a text) and of  translatability (a syntactic and semantic  control facilitates the shift between two  languages) but not of grammaticality (a  grammatical text written in a given CL will  not necessarily be considered as  grammatical in the corresponding natural  language).  As Goyvaerts (1996) wrote,  “Industry does not need Shakespeare or  Chaucer, industry needs clear, concise  communicative writing - in one word  Controlled Language”.  “A restricted or controlled language  refers to a system that limits language to a  set number of core vocabulary words, and  usually, a set of writing guidelines for  grammar, mechanics, and style. […] A  controlled language attempts to reduce  ambiguities, colloquialisms, and  synonyms” (AECMA (1995)).  English is a very productive natural  language for CLs’ creation as it is the  current international language used for  trade and science. Nevertheless, other  natural languages such as German,  Chinese, Swedish, French, etc. have  generated CLs. A CL is not “simple” or  “baby” English, German, French, etc. but  simplified English, German, French, etc.  The Airbus project In aeronautics, pilots must daily use procedures in any type of situations (normal or abnormal). It was observed that some incidents aboard commercial planes were due to the non compliance with procedures. Whether it be oral or written, a  message will be considered successful and effective when it is in keeping with the mental process implemented to reconstruct and interpret the information contained in this message. But, because one does not expect any individual to master and speak a number of languages with the same level of competence as the one he has reached in his own mother tongue, industry prefers a precise and concise language to the use of a natural language which would allow nonparallel grammatical constructions, possessing inherent ambiguities of various types, etc. In 1998, Airbus started a project dedicated to the creation of a controlled language for industrial use; in this case enhancing warning texts quality in the cockpit of Airbus aircraft. Another objective was to provide designers a means of facilitating their job while respecting the stringent safety criteria. This project was divided into three parts conducted in the frame of R&D activities, the first one dealing with the terminological aspects, the second devoted to syntax and the third one focused on the acronyms for computers naming. For industries willing to create a CL need to be aware of what has already been done, we first built up an overview which could give instant access to information. To achieve it, we had a close look at what has been written in the field of CLs and tried to get in touch with the persons involved in different projects (K. Barthe, E. Johnson, K. Godden, B. Arendse, E. Adolphson, etc.) We encountered different domains such as aircraft, meteorology, emergency services (police, fire, maritime, ambulance, etc.), etc. Our overview is meant to be a help for work. It is open-ended and can be added to. A user with queries about a particular CL can easily and quickly (in a click) get concise and succinct answers such as the rules applied, the company involved in the project, etc. by consulting  the related ID card. These ID cards (about 40 that required half a year to be compiled) consist of the following headings: complete name of the CL, date (of creation, or duration of the study), organization (a company, university, etc. which owns the product), designer (an individual or a pool, a company, etc. who is in charge of the project), objectives / application, elaboration / content, and bibliographical references. Because our aim was not to provide extensive information, we added an appropriate bibliography on each ID card so that the user can find helpful references for more precise information.  (of grammar)  Controlled languages  (of restricted domain)  BASIC English  CFE  TAUM Météo  ACE EEA CC  Douglas Aircraft ILSAM ILAM SE standard PACE  AECMA SE  CTE SDD KISL C. E. on board A3XX  Figure 0: CLs’ overview  Name  DOUGLAS AIRCRAFT  Date Society Designer Purpose Application  1979 McDonnell Douglas Corp. McDonnell Aircraft Company Standardization of readability and translatability of technical and A/C maintenance manuals.  Elaboration Content  2000 words taken from the list of the preferred verbs used in the Navy, in the Air Force, and in McDonnell 50’s technical manuals. This technical vocabulary was one of the sources studied for the creation of the AECMA SE lexicon.  Biblio References  Gingras B. (87) Huijsen W.O. (98a&b) Stewart K.M. (98)  Figure 1: An ID card  Main characteristics of our CL It will not be a translation tool. This CL will be presented in the form of a writing guide (including recommended structures and vocabulary). These constructed sentences are not intended to evolve. Nevertheless, improvements by modification of messages or addition of new ones will be possible, if necessary. Moreover, it is not meant to be computerized for the moment. It will improve comprehensibility by improving the abbreviations and the acronyms, deleting synonyms, reducing and standardizing syntactic structures, making some ellipses explicit. It will take into account the interference between languages because even though texts are written in English (official language for exchange in the cockpit), pilots are from various linguistic origins. Thus, different crew configurations exist: both pilots are native English speakers, or only one of them, or both are non-native English speakers (possibly not sharing the same mother tongue). Theoretical and practical (both semantic and syntactic) choices will be clearly justified at each step of the establishment of this language. Its validation will consist in the checking of objectives and of assessments done by different persons (domain acquainted or not). Theoretical choices will comply with bibliographical references, existing theories and new ones, etc. Practical choices will comply with pilots’ assessments, workgroups, former studies, etc. These choices will be justified and written in order to ensure a good traceability of design rationale. It will respect the Airbus family concept (commonality between aircraft). It is to say the existing use of terms that participates to the upholding of knowledge in a community of speakers and also eases the cross crew qualification.  It will deal with technical constraints such as for instance the restricted space dedicated to the display of warning texts on screen (between 20 to 36 characters according to the type of text concerned). It will be crew oriented. To make sure that it achieves its objectives, all the persons involved (from the designers to the pilots) will meet for workgroup sessions. These sessions will help to collect the comments of potential users and to guarantee homogeneity between cockpit crew-machine interfaces, operational documentation and maintenance. This feedback will be very useful, because “the sooner user requirements are integrated into the design, the quicker it will be possible to iron out snags with the enduser’s help” (Patri (1998)). Figure 2: The working group How we did it? 1. terminological methodology Always concerned by the respect of objectives and constraints, we developed an innovative method intended to determine terms and the form under which they should appear in future warning texts. The terminological study has been conducted along two major thrusts: the terminological standardization principle and the morphological reduction process. It is to be noticed that at each stage of these two axes users are taken into account (via interviews and assessments) in order to formulate recommendations that  correspond to their operational need and experience. 1.1. The terminological standardization principle This principle aimed at normalizing the existing terminology and was based on two postulates: • existing terms are reusable in the future terminology but on the condition they fulfill the different fixed criteria; • existing terms are not implied in a synonymy phenomenon. With the help of the analysis of the corpus (about 3000 sentences and 700 words), a decisional tree (figure 3) has been created with different criteria. These are of different kinds: a) derivational and flexional through: • the keeping of final morphemes such as –ed and –ing as explicit visual marks of in course or accomplished processes; • or the keeping of the final –s as the mark of the plural form; • or the suppression of the negative prefix in favor of the explicit negative expression through the use of not. b) homophonic and homographic through the location of homophones and homographs in the corpus, but also homographs with various languages. c) geographical and genealogical through the preference of terms with an American English tendency and/or terms presenting a Latin root. d) documentary through the checking of the use of these terms in different aeronautical references such as aeronautical regulations, maintenance references, operational documentation and air traffic control references. These criteria permit to tag each term the same way in order to create the decisional matrix. When a term respects all the criteria, it is immediately reused in the new terminology. If it does not respect at least one criterion, a candidate (respectful of that criterion) is proposed. Both term and its candidate undergo the decisional  tree. Then, it must be decided which one of the term or candidate must be kept in the new terminology; this is done through the observation of the decisional matrix and the comparison of all criteria. Figure 3: Existing terms are reusable When a term is involved in a group of synonyms, a specific treatment has been established (figure 4). With the help of an expert of the domain, three categories have been isolated: invalid, valid synonyms and groups of synonyms to be confirmed.  c) synonymy to be confirmed: Because the expert was not able to confirm some synonymies, it was decided to consult more experts of the domain in order to take a decision whether these groups were synonyms or not. This was realized by interviewing eight pilots (airline, flight test and instructors) in the same spirit as it was done with the first expert. After the establishment of the new terminology it was necessary to determine under which form (short or full) the terms should appear in the future warning texts. 1.2. The process of morphological reduction The process of morphological reduction (figure 5) permits to respect the uniqueness criterion: “one word – one meaning – one short form”.  Figure 4: Treatment of synonyms a) invalid synonymy: When a group of synonyms is invalidated, the expert gives a precise definition for each term and they are introduced in the new terminology. b) valid synonymy: Using the contexts of the terms, the expert is able to validate the synonymy and to recommend the use of only one of these terms. Therefore, with the help of the decisional matrix it is possible to select one term representative of the group which would be introduced in the new terminology. The other terms appear in the new terminology as not recommended terms.  Figure 5: Morphological reduction process With the help of the corpus analysis and different linguistic references we created a reductional matrix which is the compilation of abbreviating rules applicable depending on the length of terms. Abbreviated forms are generated by the matrix and submitted to assessment with pilots selected as explained in the syntactic part. Speed and correctness criteria were measured in order to characterize the transparency criterion defined as follows: “an abbreviation is transparent when correctly developed in a minimum of time without any context”. It is important to note that previous assessments have been conducted on existing abbreviations in order to respect the commonality principle; therefore every  existing short form which has been judged as transparent was kept in the new terminology. New short forms generated by the matrix were proposed only for those whose existing short forms failed at the transparency criterion. In the end, for each recommended entry of the new terminology it is recommended to use either a short form, or the full form when we were unable to generate a transparent abbreviation or for specific reasons (safety/rarity/commonality, etc). 2. syntactic process As for terminology, syntax was concerned by the respect of objectives and constraints. Due to the lack of room, one of the main characteristics of the corpus is the quasi systematic lack of grammatical words such as in, of, by, etc. In this context, the parameter “word order” is crucial and must be taken into account. As Slobin (1985) notices, “It is likely that elements such as case inflections, verb inflections, pre- or postpositions, and conjoining and subordinating particles provide major orienting points for the perception of structure”. In figure 6, the shortest1 English sequence “young horse breaker” has two meanings whereas the incorrect sequence “horse young breaker” would have only one. Indeed, depending on what the adjective “young” defines “the horse breaker” or “the horses”, the grammatical nominal phrase “a young horse breaker” can be understood differently, respectively “a young breaker of horses” or “a breaker of young horses”. On the contrary, when “young” is close to “breaker”, the ambiguity disappears and only the first meaning “the young breaker of horses” is now possible. 
The Knowledge Representation (KR) community and the Natural Language Processing community, in our opinion, have common goals yet ﬁnding a language that is expressive enough and capable of efﬁcient reasoning is yet a challenge. We have claimed elsewhere that having a Natural Language(NL)like KR may be a step towards solving that challenge. The NL-like KR we looked at deﬁnes a controlled subset of English that is not trivial and exhibits powerful reasoning properties. Controlled Language for Inference Purposes (CLIP) is a dialect of English that was considered while developing a domain-independent knowledge-based system. The system takes as input ’clippy’ utterances, Ui, and uses a NLlike KR called McLogic to deduce plausible inferences from Ui and give a justiﬁcation for these deductions. 
The paper reviews the extraction of terminology from corpora. It identifies three possible applications, terminology, translation, and retrieval. They differ in the requirements for relevancy of terms to be extracted. Standard evaluation methods based on recall and precision critically depend on the notion of relevancy of a term, which is questionable and possibly outweighed in favour of criteria of usability in practical applications. Two tools, TermExtract and BiExtract, are presented as examples of the integration of extraction tools into different workflows. 
This document gives an overview of the history of the translation industry. The argument is made that the current changing business demands make machine translation an economic necessity. Fifty years after research for machine translation commenced translation automation finally seems to become a reality. The deciding factor is perhaps less so the improvement of the technology but rather the insurmountable need for translated content. To understand the shifting demands for translation through the years we distinguish three phases in the history of the translation industry: the translation phase, the localisation phase and the globalisation phase (Cadieux and Esselink). 
In this paper we report on the set of controlled language specifications defined for Modern Greek and the development of the respective style checker. We will focus on the effectiveness and suitability of these specifications by assessing the performance of a commercial machine translation system over controlled texts and will comment on the evaluation results. For our experiments we have used the SYSTRAN MT system (English-intoGreek language pair). We will show that an improvement in translation is feasible, when a text compliant with controlled language specifications enters a MT system. Finally, we will propose a third parameter for setting CL specifications. 
This paper describes an experiment in controlled natural language interface technology. The system under work ensures a syntactic and semantic correct composition of input sentences, without restricting the user too much. The lexicon as well as the syntactic and semantic restrictions are maintained in flexible structures which can be updated at any time. The application of the system at present is machine translation. The paper presents the general paradigm of menu based natural language interfaces, the architecture of the proposed system (MenuChoice), the status of implementation, as well as further work. 
