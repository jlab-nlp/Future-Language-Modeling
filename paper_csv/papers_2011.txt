 Keywords: temporarily ambiguous sentence, garden path effect, DRT, box merger  
 a Graduate Institute of Linguistics, National Chengchi University, NO.64, Sec.2, ZhiNan Rd., Wenshan District, Taipei City 11605, Taiwan (R.O.C) kyoko@nccu.edu.tw b Department of English, National Chengchi University, NO.64, Sec.2, ZhiNan Rd., Wenshan District, Taipei City 11605, Taiwan (R.O.C) hllai@nccu.edu.tw  Abstract. This paper attempts to provide a refined description with a quantitative analysis about the deontic modals zo3-ded4 (做得) and sii2-ded4 (使得) in Taiwanese Hakka. While the affirmative-negative relationship is symmetrical structurally, it manifests in an asymmetrical manner semantically. The logic notations will be applied to clarify the intriguing interaction of possibility and negation. Under the interaction of semantic meanings and syntactic constructions, the representation of deontic modals in Taiwanese Hakka is therefore clearly elucidated. Keywords: possibility, necessity, negation, modality, Taiwanese Hakka 
25th Paciﬁc Asia Conference on Language, Information and Computation, pages 80–89 80  In this paper, I am going to show how the semantics of resultative constructions in Mandarin Chinese and Cantonese can be linked to the syntax by drawing insights from conceptual semantics (Jackendoff, 1990). Along with a demonstration of the application of conceptual structures in accounting for the mapping phenomenon in Mandarin Chinese and Cantonese resultative constructions, the paper will focus on the strange or marked mapping phenomenon of a Mandarin Chinese sentence discussed in Her’s (2006) paper, “張三 追-累-了 李四 Zhāngsān zhuī-lèi-le Lǐsì” (John chase-tired-ASP Lee)1. I will also show in this paper how the representation of conceptual structures can capture the special properties of resultative constructions and at the same time handle the relevant mapping phenomena, including that of the marked structure mentioned above, naturally. Section 2 The Conceptual Semantics Approach To handle the argument-function mapping of possible ambiguous interpretations of resultative constructions like the sentence above in Mandarin and Cantonese, an approach considering the conceptual semantics as proposed by Jackendoff (1990) seems suitable. Unlike Lexical Mapping Theory ((Levin 1986) and (Bresnan and Zaenen 1990)), which handles mapping by assigning intrinsic values to individual roles and grammatical functions, the conceptual semantics approach handles mapping by looking at the conceptual structure of the sentence as a whole. Thus, each interpretation of the sentence would possess its corresponding conceptual structure. The mapping is done according to the position each participant of the event denoted by the sentence is put. One of the features of Mandarin and Cantonese resultative constructions is that the resulting predicate or the means verb, which is usually adjective-like in a resultative compound contributes to the core meaning of the sentence unlike modifying adjuncts, which only provide additional information. These adjuncts are called “Superordinate Adjuncts” (Jackendoff, 1990). According to Jackendoff’s (1990) analysis, resultative sentences like: (1-i) Peter swung the towel dry. (2-i) The gardener watered the tulips flat. (3-i) The man hit the dog dead. can be rephrased as follows: (1-ii) Peter made the towel dry by watering it. (2-ii) The gardener made the tulips flat by watering them. (3-ii) The man made the dog dead by hitting it. 
 d  Mike Tian-Jian Jiang , Cheng-Wei Shih , Chan-Hung Kuo , Richard Tzong-Han Tsai , and  abc Wen-Lian Hsu  a Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan b Institute of Information Systems and Application, National Tsing Hua University, Hsinchu, Taiwan c Institute of Information Science, Academia Sinica, Taipei, Taiwan {tmjanng, dapi, laybow, hsu}@iis.sinica.edu.tw d Department of Computer Science & Engineering, Yuan Ze University, Taoyuan, Taiwan thtsai@saturn.yzu.edu.te  Abstract. Numerous studies have analyzed the influences of word segmentation (WS) performance on information retrieval (IR) for Mandarin Chinese and have demonstrated a non-monotonic relationship between WS accuracy and IR effectiveness. The usefulness of the compound words that have been a focus of the IR literature is not reflected by common WS evaluation metrics of word-based precision (P) and recall (R). This investigation proposes alternative measurements of WS accuracy, which are based on negative segments that are annotated against four standards of referenced corpora, called true negative rate (TNR) and negative predictive value (NPV), and compares with P and R through search engine simulation,. Accuracy-controlled WS systems segment queries for the simulation including NTCIR collections and "Sogou" logs. Mean average precision (MAP) estimates the similarity of search results between the original and segmented queries. The statistics demonstrate that TNR and NPV are generally more closely correlated with MAP than are P and R. Keywords: Word segmentation, information retrieval, true negative rate. 
 a School of Computer Science & Engineering, BeiHang University, 37# Xueyuan Rd, Haidian District, Beijing 100191, China {chaowenhan, lizj}@buaa.edu.cn  Abstract. In statistical machine translation, the number of sentence pairs in the bilingual corpus is very important to the quality of translation. However, when the quantity reaches some extent, enlarging the corpus has less effect on the translation quality; whereas increasing greatly the time and space complexity to train the translation model, which hinders the development of statistical machine translation. In this paper, we propose a graph-based bilingual corpus selection approach, which makes use of the structural information of corpus to measure and update the importance of each sentence pair, and then selects a sentence pair with the highest importance each time. Our experiments in a Chinese-English translation task show that, selecting only 50% of the whole corpus by the graph-based selection approach as training set, we can obtain the near translation result with the one using the whole corpus. Keywords: Statistical Machine Translation, Corpus Selection, Graph 
 Thet Thet Zin , Khin Mar Soe , and Ni Lar Thein  a University of Computer Studies,Yangon,Myanmar thetthetzin.ucsy@gmail.com b Natural Language Processing Laboratory University of Computer Studies, Yangon, Myanmar  Abstract. This paper presents Myanmar phrases translation model with morphological analysis. The system is based on statistical approach. In statistical machine translation, large amount of information is needed to guide the translation process. When small amount of training data is available, morphological analysis is needed especially for morphology rich language. Myanmar language is inflected language and there are very few creations and researches of corpora in Myanmar, comparing to other language such as English, French, and Czech etc. Therefore, Myanmar phrases translation model is based on syntactic structure and morphology of Myanmar language. Bayes rule is also used to reformulate the translation probability of phrase pairs. Experiment results showed that proposed system can improve translation quality by applying morphological analysis on Myanmar language. Keywords: Morphological Analysis, Statistical Machine Translation, Bayes rule, Syntactic Structure. 
 a Department of Humanities and Social Sciences, IIT Bombay, Powai, Mumbai-400076, India {shalmaleep, vsarma}@iitb.ac.in  Abstract. In this paper, we describe the two tests developed and designed for Marathi using non-words, a) plural formation for non-words b) intuition test for gender assignment in which subjects were asked to assign gender to non-words. We look at the distribution of nouns across noun classes and genders and discuss the congruence between the problematic classes as observed in the tests and the actual class distribution and frequency in the language. Keywords: Marathi, plurals, gender, non-words, morphological rules 
The acquisition of tense-aspect morphology has shown an interesting universal pattern in both first and second language acquisition (Andersen and Shirai, 1996). This universal tendency is referred to as the Aspect Hypothesis (Andersen and Shirai, 1994; Robison, 1995), which claims that there is a universal developmental sequence of tense-aspect markers: past tense forms start with achievement verbs, and progressive forms start with activity verbs. Correlations between tense-aspect markers and the inherent aspect of verbs have been verified in the first language acquisition of various languages (for example, Bloom et al.1980 in English, Antinucci and Miller 1976 in Italian, Bronckart and Sinclair 1973 in French, Weist et al. 1984 in Polish, Stephany 1981 in Greek, Aksu-koc 1988 in Turkish). However, only a few studies were conducted on the L1 acquisition of Asian languages, such as Shirai (1993, 1998) and Shirai and Suzuki (in print) for Japanese, and Seda and Lee (2002) for Korean. In their study on the L1 acquisition of Korean, Seda and Lee (2002) found that both Korean and Turkish children use past marking predominantly with accomplishment and achievement verbs first, supporting the Aspect Hypothesis. However, they only focused on the acquisition of past tense . Further research is needed to verify the Aspect Hypothesis completely, including the acquisition of the progressive tense. This study tests whether the universal developmental pattern proposed by Andersen and  The work reported in this paper was supported by Grant-in-Aid for JSPS Fellows and the Tohoku University International Advanced Research and Education Organization. Copyright 2011 by Ju-yeon Ryu 25th Paciﬁc Asia Conference on Language, Information and Computation, pages 186–195 186  Shirai (1996) holds true for the L1 acquisition of Korean, and also compares it to the L1 acquisition of the Japanese imperfective marker -te i- in order to discover what determines children‟s acquisition of aspect: universal predisposition, the distribution bias hypothesis or individual variation.  2. The Imperfective Aspect System in Korean Imperfective aspect markers in Korean can express two meanings: the progressive and the resultant state (Lee, 1993; Martin, 1992). When expressing an action-in-progress meaning, Korean takes -ko iss- as the progressive marker. Meanwhile, when expressing a resultant state meaning, Korean takes -ko iss- or -a iss-. Syntactically, when expressing a resultative state, both -ko iss- and -a iss- are used, depending on the transitivity of the main verbs. As shown in Table 1, -ko iss- is used with transitive verbs, whereas -a iss- is used with intransitive verbs (Lee, 1991).  Table 1 Imperfective markers -ko iss- and -a iss-  Intransitive verbs Transitive verbs  The progressive marker  -ko iss-  The resultant marker  -a iss-  -ko iss-  -ko iss- has been generally treated as a progressive marker similar to the English progressive marker be -ing, as shown in (1a). However, it is not obligatory to employ the Korean -ko iss- to describe an ongoing event, unlike with be -ing in English and -te i- in Japanese. The simple present form in Korean, in fact, can encode an ongoing event like the Romance languages as well as others, as shown in (1a) and (1b).  (1a) Pap-ul mek-ko iss-ta lunch-Acc eat-Prog-Dec “He(or She) is eating a lunch” (1b) Pap-ul mek-nun-ta lunch-Acc eat-Prs-Dec “He(or She) is eating a lunch”  (AN 1;11)  In addition to its progressive meaning, -ko iss- can also describe a resultant state with transitive verbs, such as verbs of wearing, carrying, and body posture (Ahn, 1995; Kim, 1993; Lee, 1991) .  (2) Wangkwan-ul ssu-ko iss-eyo (JONG 1;11) crown-Acc wear-Resl-Dec “He(or She) is wearing a crown.” or “He(or She) is putting on a crown”  In (2), -ko iss- can be interpreted either as an ongoing event or as a resultant state. This is because in the case of transitive verbs, only -ko iss- can be chosen as the imperfective aspect marker in both the describing of the progressive and the resultant state. -a iss- has been defined as the resultative marker in Korean (Ahn, 1995; Lee, 1993; Martin, 1992). However, whereas -ko iss- is compatible with all transitive verbs, -a iss- can only be used with intransitive verbs in describing a persisting state resulting from a completed action, as in (3) (Lee, 1991).  (3) Khokkili anc-a iss-e elephant sit-Resl-Dec “The elephant is sitting.”  (JONG 1;11)  187  Since we compare the results of Korean L1 acquisition with those of Japanese L1 acquisition, it is necessary to describe the aspectual system of Japanese. It has an aspectual system quite similar to Korean in that the Japanese imperfective marker -te i- can denote both progressive and stative meanings. Generally speaking, the meaning of -te i- is dependent on the inherent aspectual value of the verbs to which -te i- is attached. Dynamic durative verbs (i.e. Vendler‟s (1957) activity and accomplishment verbs) combine with -te i- to express progressive meanings, whereas punctual change of state verbs (Vendler‟s achievement verbs) express a resulting state. The Korean imperfective marker -a iss-, which covers the resultative state meaning denoted by -te i- with achievement verbs in Japanese, is attached almost exclusively to intransitive change-of-state verbs.1 On the other hand, The Korean imperfective marker -ko iss-, which denotes progressive meaning with activity and accomplishment verbs as in Japanese, can have resultative meaning with transitive achievement verbs, as Shown in Table 2.  Table 2 Comparison of Japanese and Korean imperfective aspect markers  Progressive  Resultative  Japanese  -te i-  -te i-  Activity, Accomplishment  Achievement  Korean  -ko iss-  -ko iss-  -a iss-  Activity, Accomplishment transitive intransitive  Achievement Achievement  3. Previous studies on the acquisition of aspect Previous studies on the acquisition of tense/aspect have claimed that the development of tense/aspect morphology in language acquisition is strongly influenced by the inherent semantic aspect of the verbs to which the inflections are attached. This hypothesis, generally referred to as the Aspect Hypothesis (Andersen and Shirai, 1994, 1996; Shirai, 1991; Bardovi-Harlig, 1999, 2000; Li and Shirai, 2000; Salaberry and Shirai, 2002), claims the following: 1. Learners use (perfective) past marking on achievement/accomplishment verbs, eventually extending use to activity and stative verbs. 2. In languages that encode the perfective/imperfective distinction morphologically, the imperfective past appears later than the perfective past, and the imperfect past marking begins with stative and activity (i.e., atelic) verbs, then extends to accomplishment and achievement (i.e., telic) verbs. 3. In languages that have a progressive aspect, progressive marking begins with activity verbs, and then extends to accomplishment/achievement verbs. 4. Progressive marking is rarely incorrectly overextended to stative verbs (in L1 acquisition). Although researchers generally agree on this descriptive observation (e.g. Shirai, Slobin and Weist, 1998), the explanation for this observation is not fully understood. One important hypothesis appeals to a universal predisposition (e.g., Bickerton, 1981), which presupposes that children have a bias to map tense-aspect markers with particular semantic content. Namely, he argued that children use past tense marking to denote punctuality, or telicity in aspectology, and progressive/imperfective marking to mark the lack of telicity, because the PunctualNonPunctual Distriction (PNPD) is a distinction that has a special status in his bio-program hypothesis.. Meanwhile, the Distributional Bias Hypothesis (e.g. Shirai and Andersen, 1995) argues that children make particular associations based on input frequency. In other words, it 
 Tomoki Nagase , Hajime Tsukada , Katsunori Kotani ,  d  e  Nobutoshi Hatanaka , Yoshiyuki Sakamoto  a Fujitsu Laboratories, 1-1, Kamikodanaka, 4-chome, Nakahara-ku, Kawasaki, 211-8588, Japan nagase.tomoki@jp.fujitsu.com b NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan tsukada.hajime@lab.ntt.co.jp c Kansai Gaidai University, 16-1 Nakamiyahigashino-cho, Hirakata, Osaka 573-1001, Japan kkotani@kansaigaidai.ac.jp d Tokyo University of Information Sciences, 4-1 Onaridai,Wakaba-ku,Chiba,265-8501 Japan hatanaka@rsch.tuis.ac.jp e 5-38-7, Akatsutsumi, Setagaya-ku, Tokyo, 156-0044, Japan ysakanlp@ka2.so-net.ne.jp  Abstract. The present paper proposes automatic error analysis methods that use patterns representing grammatical check points. Our method is comparable to or slightly outperforms conventional methods for automatic evaluation metrics. Different from the conventional methods, our method enables error analysis for each grammatical check point. While our method does not depend on languages, we experimentally show its validity by using a Japanese-to-Chinese test set. Errors in existing Japanese-to-Chinese translation systems are also analyzed. Keywords: automatic evaluation, machine translation, error analysis 
 Nay Yee Lin , Khin Mar Soe , and Ni Lar Thein  a,c University of Computer Studies,Yangon, Myanmar {nayyeelynn, nilarthein}@gmail.com b Natural Language Processing Laboratory, University of Computer Studies,Yangon, Myanmar kmsucsy@gmail.com  Abstract. Machine Translation systems expect target language output to be grammatically correct. In Myanmar-English statistical machine translation system, target language output (English) can often be ungrammatical. To address this issue, we propose an ongoing chunk-based grammar checker by using trigram language model and rule based model. It is able to solve distortion, deficiency and make smooth the translated English sentences. We identify the sentences with chunk levels and generate context free grammar (CFG) rules for recognizing grammatical relations of chunks. There are three main processes to build a grammar checker: checking the sentence patterns in chunk level, analyzing the chunk errors and correcting the errors. According to experimental results, this checker can detect simple, compound and complex sentence types for declarative and interrogative sentences. This system is useful for reducing grammar errors of target language in Myanmar-English machine translation system. Keywords: Statistical Machine Translation System, Trigram Language Model, Rule based Model. 
 a Department of Humanities and Social Sciences, IIT Bombay, Powai, Mumbai-400076, India smritismriti@gmail.com vsarma@iitb.ac.in  Abstract. In this paper, we provide a complete description of Hindi verbal inflection within the framework of Distributed morphology. We discuss the categories that are visible on the verb itself and on associated auxiliaries. We show how both analysis and generation are possible using this model. We also discuss the implementation of such linguistically motivated analysis in a morphological analyzer for Hindi, one of the several NLP tools that we have developed for Hindi, and discuss the outcomes of such an implementation. Keywords: Hindi, Distributed Morphology, verbal inflection, morphological analyzer 
Named entity recognition (NER) was first introduced in MUC-6 in 1990’s. According to MUC, NER is divided into 3 main groups. The first one is entity names including person, organization, and location names. The second one is temporal expressions and the last one is numerical expressions which are monetary values and percentages. Among these 3 groups, the first one is the most difficult to be recognized because the structures of the names are complicated and the contexts they occur are varying. As a result, a number of research on NER as well as this study have focused on the first group. Most research on NER using statistical models mainly focused on feature selection since features are the most important part of the models and directly affect the performance of the systems. It was assumed that the answer given to the models has no effect on the system performance. However, in Chinese, some research on NER (Mao et al., 2008; Yu et al., 2008), the work of Yu et al. (2008) proved that the number of answer tags can help improve the system performance. The use of more tags yields better results because it gives more information of entity names’ boundaries to the system for learning. For machine learning, it tends to be that the more information is provided, the better the performance of the system is. Research on Thai NER using statistical models such as SVMs (Suwanno et al., 2007), HMM (Sansing and Prayot, 2010), ME (Chanlekha and Kawtrakul, 2004), CRFs (Tepdang et al.; Tirasaroj and Aroonmanakun, 2009), etc. also focused on feature selections. None has studied the effect of answer patterns for their supervised systems. The answer pattern is the way we give the answer in the training corpus for supervised machine learning system. The answer suggests how much information of NEs provided for the systems. For example, the answer can be only the information of NE types or it can include the NE boundaries. In addition, most of previous ∗ The work reported in this paper was supported by The Thailand Research Fund (TRF) under grant no MSG53Z0008, and partially supported by Chulalongkorn University Centenary Academic Development Project. Moreover, we would like to thank NECTEC for their BEST 2009 corpus and reviewers for their suggestions and comments. Copyright 2011 by Nutcha Tirasaroj and Wirote Aroonmanakun 25th Paciﬁc Asia Conference on Language, Information and Computation, pages 392–399 392  research did not distinguish metonymic names, which are location names referring to organizations and organization names referring to locations. Thus, apart from presenting the NER systems, this study is conducted to find out if the more informative answers can improve the system performance of Thai NER like in Chinese or not. In addition, we will find out the differences of results when metonymic names are marked. Our NER systems are based on CRF models. Recently, there was a comparative study on Thai word segmentation approaches (Haruechaiyasak et al., 2008). The results showed that CRFs yielded the best performance when compared with other machine learning methods, Naïve Bayes, decision tree, and Support Vector Machine. CRFs were also applied to Thai NER (Tepdang et al.; Tirasaroj and Aroonmanakun, 2009) and yielded about 80% of recognition rate or Fmeasure. In this study, we used CRF++0.53 implemented by Taku Kudo. Related Work There are a number of research focusing on Thai named entity recognition. Charoenpornsawat et al. (1998) used feature-based approach such as context words, collocations, and heuristic rules to extract the candidates and then used Winnow algorithm to recognize the entity names. The accuracy was 92.17%. Chanlekha et al. (2002) applied statistical and heuristic rule-based model to POS tagged data. The results showed that the model did have much problem when extract the names from magazines but when the model was applied to the newspapers, the accuracy was quite low. It was due to the written style of magazines that the names were usually in regular patterns. Tirasaroj and Aroonmanakun (2009) compared the performance of the systems between word-based and syllable-based system by applying CRF. The results showed no significant difference. Although in their work, they mentioned about the NE tags that differentiated between common named entities – person, organization, and location names - and metonymic names, the organization names referring to the locations and vice versa. Their answers did not divided into five categories like their tags but just three categories as other previous research. In Thai NER, the researchers have never mentioned about their answer patterns used in their systems but in Chinese, there are some research proving that the number of tag types does have an effect on supervised learning systems. The amount of tags used in 2008 is more than those used in 2006. In 2006, Feng et al. (2006) used BIO tags; B for the beginning of names, I for the inside of names, and O for others. In 2008, Mao et al. (2008) used BIOE and Yu et al. (2008) used BIOES instead of BIO. They added E and S; E for the end of names and S for single character names. From their research, giving more tags could help improve the system performance. In our study, we will apply NE type tags like in Tirasaroj and Aroonmanakun’s work and for NE boundaries, we will adapt from BIOE. We will explain in more detail in the section about patterns of answer. Conditional Random Filed Models Conditional Random Fields (CRFs) (Lafferty et al., 2001), which are developed from MEMMs, is a discriminative undirected graphical model. They are usually used for segmenting and labeling sequence data. CRF models the conditional distribution P(Y|X), where Y is the hidden label sequence or output and X is the observation or input. A linear chain CRF defines the conditional probability of sequence Y when given the observation sequence X as follows: (1) Z is a normalization factor defined as: 393  (2)  where function.  is an arbitrary feature function, is a learned weight for each feature  Corpus  The corpus used in this study is taken from BEST 2009 corpus. BEST 2009 is the corpus  created for training and testing word segmentation systems in “BEST 2009: THAI WORD  SEGMENTATION SOFTWARE CONTEST.” Many genres of data were collected in BEST but  only news genre was used in our study because in the news there are all three kinds of named  entities- person, organization, and location names- and the number of entities are much more  various than those found in other genres like fictions and encyclopedia. Thai word segmentation  program (Aroonmanakun, 2002) was used to segment all data in our corpus. The number of  tokens is about 367,600 tokens.  As we applied supervised learning system, we tagged all answers in the corpus. The data  from BEST 2009 had been tagged the NE boundaries but not NE types. We then had to add NE  types manually. As one of the main purposes of this study was to compare the answer patterns  of the systems, we tried to give as much information of answers as possible. We followed  Tirasaroj and Aroonmakun (2009) so that there were five NE types in our corpus. Apart from  common NE types, person, organization, and location names, there were two more tags for  metonymic names, organization names referring to locations and vice versa. The followings  were NE tags in our corpus:  1. <persName>…</persName> is for person names including first names, last names,  nicknames, alias, etc. The titles and honorifics right before the names are also included such as  <persName>ลุงสมหมาย</persName>  Uncle Sommai  2. <orgName>…</orgName> is for organization names. In case that the full names are  followed by their abbreviated names, they are tagged separately like  <orgName>กระทรวงสาธารณสุข</orgName><orgName><abb>สธ.</abb></orgName>  Ministry of Public Health  MOPH  3. <placeName>…</placeName> is for location names such as  ที่<placeName>นครปฐม</placeName>  at  Nakornpathom  4. <orgName ref=“loc”>…</orgName> is for organization names used to refer to locations.  Generally, if the locations do not have their specific names, their names would undoubtedly be  the same as the organization situated in that place. For example,  เขามารักษาโรคที่<orgName ref=“loc”>โรงพยาบาลสัตว</orgName>  had exam at  Veterinary hospital  5. <placeName ref=“org”>…</placeName> is for location names used as organization  names. Mostly, this kind of names is used to refer to an organization, especially official offices  and the government such as  <placeName ref=“org”>ประเทศไทย</placeName>ไดแจงไปยังองคการระหวางประเทศเรียบรอยแลว  Thailand has already informed International Organizations  Besides NE tags, there is a tag for abbreviations. The tag <abb>…</abb> is used to tag every  abbreviation in the corpus. In case of the abbreviations like titles or honorifics which are parts  394  of names, the abbreviation tags would be tagged inside NE tags like the example in the organization tag above. Features and Patterns of Answer In this study, we applied both open and closed features. The open features are the knowledge we got outside the corpus while the closed features are those extracted from the training and testing data. There are 7 features used in this study; two open features, three closed features, and two baseline features. The open and closed features are all binary features. In this study, the deep linguistic features such as POS, functional features were not used because this is the preliminary stage of our study and these features take a lot of time in preparing the training corpus. The followings are the details of each feature used in this study. The open features: applying only closed features may not be enough for the system as the entity names sometimes occur in ambiguous context or without any clue words. The gazetteers were used to help extract this kind of names. Our gazetteers include the lists of person, organization, and location names, abbreviated organization names, and personal titles and honorifics. If the token matched with any part of the names in the list, this feature would be set to ‘Y’. Another outside knowledge is a general word list. This feature was set from the patterns of names that the names can composed of known and unknown strings. However, although those known strings are words found in dictionary, they are sometimes not words used in daily life. For example, “อัปสร” (Upsorn) is a synonym of ‘นางฟา’ (Nang-faa); both mean an angel, but no one generally calls the angel as “อัปสร” except in literature. Then, words that are not frequently found in daily life are possibly a name or a part of the name. So, in this case, the feature for the token not found in the list was set to ‘Y’. The closed features: there were three closed features we got from the corpus, including abbreviation, context clues, and repeated NE. The details of each feature are as follows: The abbreviation feature was considered from the characteristic of Thai entity names that they are usually preceded by the abbreviation. Person names usually co-occur with titles and honorifics such as น.ส. (Miss), ดร. (Ph.D), organization names with company designators such as บจ. (company limited), and location names with location indicators like ถ. (Road), จ. (province). Those abbreviations can help indicate not only named entities’ boundaries but also their types. For this feature, the sequence of three tokens would be set to ‘Y’ if one of them was an abbreviation. The context clues are the key factors in assigning the types of metonymic names. For example, the location names, in fact, could not co-occur with verbs since only persons and organizations can do some actions. Thus, whenever the location names co-occur with verbs, it implies that these names are used as metonymic names referring to the organization. The list of context clues was extracted from the three tokens before and after the entity names in the training corpus. If the token matched with any token in the list, the feature was set to ‘Y’. Repeated NE is set from our assumption that most named entities do not occur once in the text. Then, if the strings of words appeared several times in the text, we assumed that they would probably be entity names. For this feature, if the three tokens, including the current token and the tokens before and after, occurred together more than three times in the text, they would be set to ‘Y’. The baseline features: apart from the open and closed features, there were baseline features used as templates for running CRF. Our baseline features were unigram and bigram. Besides feature selection, we have to set the patterns of answer. There are 5 answer patterns used in this study. Each pattern has different information of named entities’ boundaries and types. The details are as shown in Table 1: 395  Table 1: Answer patterns  No  Pattern  
 a JiangSu Provincial Key Lab for Computer Information Processing Technology b School of Computer Science and Technology Soochow University, Suzhou(215006) ,China {kongfang, gdzhou}@suda.edu.cn  Abstract. This paper systematically explores the effectiveness of dependency and constituent-based syntactic information for anaphoricity determination. In particular, this paper proposes two ways to combine dependency and constituent-based syntactic information to explore their complementary advantage. One is a dependency-driven constituent-based structured representation, and the other uses a composite kernel. Evaluation on the Automatic Content Extraction (ACE) 2003 corpus shows that dependency and constituent-based syntactic information are quite complementary and proper combination can much improve the performance of anaphoricity determination, and further improve the performance of coreference resolution. Keywords: Dependency Relation, Constituent Parser, Anaphoricity Determination, Coreference Resolution, Tree Kernel, Composite Kernel 
Consecutive interpreting (CI) is “a process in which adequate information is orally presented and transferred into another linguistic and cultural system” (Hu, 2006, p. 3). Among the forms of interpreting, e.g. simultaneous interpreting (SI) featured by the dependence on high-tech booth facilities and remote interpreting (RI) featured by the reliance on high speed internet connection, CI is the most frequently adopted for its low cost and moderate requirement on technical support. It is applied in interpreter-on-site business negotiations, press conferences, parent teacher meetings, interviews, and individual consultations. Responding to the increasing social demand for CI interpreters and English users of certain CI skills, a number of universities and colleges in China have offered CI as a compulsory core course for English majors. In the assessment of CI output, fluency is an important criterion. Since CI output can be regarded as a natural speech delivered in the form of the target language and featured by spontaneous production, CI researchers share the findings on fluency in natural speech studies. Although categorized as disfluencies (Engelhardt et al., 2010; MacGregor et al., 2009; Corley & Stewart, 2008; Shriberg, 1999; Clark & Wasow, 1998), pause, fillers and repetition are by no means tantamount to fluency hindrance. Studies have shown that pause, fillers and repetition may help smooth speech, facilitate understanding and serve different communicative functions (Brennan & Schober, 2001, in Xu, 2010; Tissi, 2000; Hieke, 1981). Further, Clark & Tree (2002) propose moving the status of the fillers um and uh toward that of an interjection. The positive role of disfluecies in spontaneous speech is undeniable. However, it is also beyond dispute that disfluencies impair speech fluency when they exceed acceptable limits. Goffman states that 
We present the S-SSTC framework for machine translation (MT), introduced in 2002 and developed since as a set of working MT systems (SiSTeC-ebmt). Our approach is example-based, but differs from other EBMT approaches in that it uses alignments of string-tree alignments, and in that supervised learning is an integral part of the approach. Our model directly deals with three main difficulties in the traditional treatment of MT that stem from its separation from the "translation task" (the 'world'). First, by allowing the system to learn from real translation examples directly, we avoid the need to indefinitely pursue the elusive goal of writing grammars to exactly describe intermediate syntacticosemantic monolingual representations and their correspondences. Second, we make explicit the dependence of the MT system performance on the input from the environment. That is possible only because the learning process uses feedback from the real translation knowledge when constructing its knowledge representation. Third, such MT systems using an inductively learned knowledge base yield a desirable non-regressive behavior by using translation mistakes to improve their knowledge base. 1. Introduction The S-SSTC-based framework for the construction of MT systems has been introduced in 2002 [Mosleh et. al. 2002] and developed since to an operational state (SiSTeC-ebmt for English-Malay and English-Chinese). In this article, we would like to stress a particular aspect, namely that this approach is better capable of modeling the translation knowledge of human translators than other example-based approaches. Because the translation knowledge is represented as alignments (synchronizations) between string-tree alignments (SSTCs, or structured string-tree correspondences), it is more natural to translators (and post-editors) than direct word-word, string-string or chunk-chunk correspondences used in classical SMT and EBMT models. It is also totally static, hence more understandable than procedural knowledge embedded in almost all RBMT approaches. The learning process which is an integral part of the development of SiSTeC-ebmt MT systems can in fact be viewed as a special case of the study of reasoning reported in [Khardon&Roth 94], because it combines the interfaces to the 'world' used by known learning models with the reasoning task and a performance criterion suitable for it. In such a framework, the intelligent agent is given access to its learning interface, and is also given a grace period in which it can interact with this interface and construct its representation Knowledge Base (KB) of the 'world'. Its reasoning performance is measured only after this period, when it is presented with 'queries' from some query language, relevant to the 'world', and has to answer whether such 'queries' are implied by the learned 'world' model. In our case, the 'world' is the 'translation task' captured in terms of the parallel texts produced by human translators and enriched by their S-SSTCs, and the 'queries' are simply modeled by a predicate Translate(ST,TT) where ST is the source language text and TT is a variable to be instantiated by a target language text if the 'translation' model learned is capable of performing such translation. Our model directly deals with three main difficulties in the traditional treatment of MT which stem from its separation from the "translation task" (the 'world'). First, by allowing the system to learn from real translation examples directly, we avoid the need to indefinitely pursue the elusive goal of writing grammars to exactly describe intermediate syntactico-semantic monolingual representations and their correspondences. Second, we make explicit the dependence of the MT system performance on the input from the environment. This is possible only because the learning process uses feedback from the real translation knowledge when constructing its knowledge representation. Third, such MT systems using an inductively learned knowledge base yield a desirable non-regressive behavior by using translation mistakes to improve their knowledge base. 25th Paciﬁc Asia Conference on Language, Information and Computation, pages 477–484 477  Learning to translate is just like any other machine learning task; it is concerned with modeling and understanding learning phenomena with respect to the 'world' — a central aspect of cognition. Traditional theories of Machine Translation systems, however, have assumed that such cognition can be studied separately from learning. It is assumed that the knowledge is given to the system, stored in some representation language with a well-defined meaning, and that there is some mechanism which can be used to determine what source language text can be translated with respect to the given knowledge; the question of how this knowledge might be acquired and whether this should influence how the performance of the machine translation system is measured is not considered. We prove the usefulness of the ‘learning-to-translate’ approach by showing that through interaction with the world, the agent truly gains additional translating power, over what is possible in more traditional settings. 2. The bilingual knowledge bank base as a set of S-SSTCs Bilingual parallel texts which encode the correspondences between source and target sentences have been used extensively in implementing the so called example-based machine translation systems [Sato 91, Richardson et. al. 2001, Menezes et. al. 2001, Kawahara & Kurohashi 2010]. In order to enhance the quality of example-based systems, sentences of a parallel corpus are normally annotated with their constituent or dependency structures [Sadler&Vendelmans 90], which in turn allows correspondences between source and target sentences to be established at the structural level. Here, we annotate parallel texts based on the Structured String-Tree Correspondence (SSTC) [Boitet&Zaharin 88]. The SSTC is a general structure that can associate, to strings in a language, arbitrary tree structures as desired by the annotator to be the interpretation structures of the strings, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be interpreted for both analysis and synthesis in the machine translation process. These features are very much desired in the design of an annotation scheme, in particular for the treatment of certain non-standard linguistic phenomena, such as unprojectivity or inversion of dominance [Tang&Zaharin 95]. In this paper, we show how to use the good properties of the SSTC annotation scheme for S-SSTCbased MT, using the example of the SiSTeC-ebmt English-Malay Machine Translation system. We have chosen dependency structures as linguistic representations in the SSTCs, since they provide a natural way of annotating both the tree associated to a string as well as the mapping between the two [Goh 96]. We also give a simple means to denote the translation elements between the corresponding source (English) and target (Malay) SSTCs. Similar arguments also appeared in [Sadler&Vendelmans 90] and [Maxwell&Schubert 89]. The dependency structure used here is in fact quite analogous to the use of abstract syntax tree in most of the compiler implementation. However, we note that the SSTCs can easily be extended to keep multiple levels of linguistic representation (e.g. syntagmatic1, functional and logical structures) if that is considered important to enhance the results of the machine translation system. Naturally, the more information annotated in an SSTC, the more difficult is the annotation work; that is why one should try to keep only the annotations contributing most to the task at hand. In the general case, let S be a string (usually a sentence) and T a tree (its linguistic representation). Instead of simply write (S,T), we want to decompose that ‘large’ correspondence into smaller ones (S1, T1)…(Sn, Tn) in a hierarchical fashion; hence the adjective ‘structured’ in ‘SSTC’. If T is an abstract representation of S, some nodes may represent discontinuous words or constituents (e.g. He gives the money back to her), or some words are not directly represented (e.g. auxiliaries, articles), or some words omitted (elided) in S may have been restored in T. [Boitet&Zaharin 88] have shown how to encode such string-tree correspondences in the tree part (T), through 2 functions, SNODE and STREE, even if the trees are ‘abstract’, but provided they obey some formal constraints that are in effect verified by all known kinds of linguistic trees. In the SSTC diagrams presented here, any tree node N bears a pair X/Y where X = SNODE(N) and Y = STREE(N). X and Y are generalized (not necessarily connex) substrings of the string S, and are written as minimal2 left-to-right lists of usual intervals, like 1_3+4_5). SNODE(N) denotes the substring that corresponds to the lexical information contained in 
§  ↑  { g9804, chaolin}@cs.nccu.edu.tw, jshin@csie.ncnu.edu.tw  Abstract. We studied a special case for the translation of English verbs in verb-object pairs. Researchers have studied the effects of the linguistic information about the verbs being translated, and many have reported how considering the objects of the verbs will facilitate the quality of translations. In this study, we took an extreme venue – assuming the availability of the Chinese translations of the English objects. We explored the issue with thousands of samples that we extracted from 2011 NTCIR PatentMT workshop. The results indicated that, when the English verbs and objects were known, the information about the object’s Chinese translation could still improve the quality of the verb’s translations but not quite significantly. Keywords: Machine Translation, Feature Comparison 
a National Institute of Information and Communications Technology, Kyoto, 619-0289 Japan shirado@nict.go.jp b The Institute of Behavioral Sciences, Tokyo, 162-0845 Japan smarumoto@ibs.or.jp c Tottori University, Tottori, 680-8550 Japan murata@ike.tottori-u.ac.jp d Toyohashi University of Technology, Aichi, 441-8580 Japan isahara@tut.ac.jp  Abstract. We propose a system for flexibly judging the misusage of honorifics in Japanese sentence. The system can point out misused words and phrases, and can also indicate how they are misused. The system uses judgment rules whose degrees of validity in modern Japanese society are quantified by psychological experiments. The system can judge sentences flexibly based on the learner's linguistic level by tuning thresholds regarding the degree of validity. The proposed system is expected to be applied in practical computeraided education. Keywords: Japanese, honorifics, misuse, judgment, education 
 b Faculty of Computer Science and Information Technology, Universiti Malaysia Sarawak 94300 Kota Samarahan, Sarawak, Malaysia {mbranaivo, alvin}@fit.unimas.my  Abstract. This paper presents NERSIL, the first Iban named entity recognition using ANNIE, an information extraction tool available within GATE. We proposed a method for building rules and gazetteers for Iban language. Rules were determined based on named entities that were not recognized by ANNIE. Then, the investigation of the contexts of these non-recognized Iban named entities allowed us to write the rules. NERSIL achieves 76.4% F-measure. Keywords: Named Entity Recognition, GATE, Information Extraction, Rules-based, Gazetteers 
 Abstract. This paper deals with how to enhance the performance of a rule-based parser using statistical Information. PP (Prepositional Phrase) attachment ambiguity is one of the main ambiguities found in parsing. We therefore conducted some experiments on extracting statistical information for PP attachment from a corpus, and on applying such information to a rule-based parser. Two types of information are used: supervised learning data and unsupervised learning data. In this paper, we show how we apply these types of information and to what degree they contribute to the PP attachment as well as to the overall parsing performance. The final results show a 5.42% performance improvement in PP attachment, with an 8.7% error reduction ratio in the overall parsing performance. Keywords: PP attachment, rule-based parsing, syntactic ambiguity, statistical information, lexical dependency 
a  Shu Zhang , Jianwei Wu , Dequan Zheng , Yao Meng , Yingju Xia , and Hao Yu  a Fujitsu Research and Development Center Dong Si Huan Zhong Rd, Chaoyang District, Beijing and 0086, China {zhangshu, mengyao, yjxia, yu}@cn.fujitsu.com b School of Computer Science and Technology, Harbin Institute of Technology No.92, Xidazhi Street, Harbin 150001, China {jwwu, dqzheng}@mtlab.hit.edu.cn  Abstract. Twitter is a widespread social media, which rapidly gained worldwide popularity. Pursuing on the problem of finding related tweets to a given organization, we propose supervised and semi-supervised based methods. This is a challenging task due to the potential organization name ambiguity. The tweets and organization contain little information. The organizations in training data are different with those in test data, which leads that we could not train a classifier to a certain organization. Therefore, we induce external resources to enrich the information of organization. Supervised and semisupervised methods are adopted in two stages to classify the tweets. This is a try to utilize both training and test data for this specific task. Our experimental results on WePS-3 are primary and encouraging, they prove the proposed techniques are effective in performing the task. Keywords: Twitter, name ambiguity, online reputation management. 
b ayu@stei.itb.ac.id  Abstract. We developed an open domain QA system that can handle factoid and nonfactoid questions in Indonesian language by using monolingual approaches. EAT classification is done by identifying question word and clue words. Keyword extraction from question is done by looking at POS information of each word in question, eliminating stop words, and stemming. We use articles from Indonesian Wikipedia as corpus and Lucene framework as the base for passage retriever component, with three additional processing: query expansion, boost EAT, and boost term. For factoid questions, answer finding is done by using Named Entity Recognition. Answer scoring is done by calculating keyword occurrences and answer-keywords distance (MRR = 0.6191). For non-factoid questions, answer finding is done by identifying sentence pattern and clue words. Answer scoring is done by considering pattern priority and keyword occurrences (MRR = 0.8079). Keywords: monolingual, open-domain, Indonesian language, factoid, non-factoid. 
There are two approaches to the natural language processing – one is going in width to cover at shallow level (parsing, syntax) the rich linguistic variety found in the natural language, while another is going in depth (semantics, discourse structure) for a monosemous subset of natural language referred to as a controlled natural language (CNL). Today we are nowhere near to bridging the gap between the two approaches. In this presentation I argue that despite elusiveness of this goal, FrameNet might provide a sufficient insight into the deeper semantic layers of the natural language to envision a new kind of a rich CNL narrowing the gap with the true natural language. A blueprint for PAO, a procedural CNL of such new kind is discussed. 
If all we want from a syntactic parser is a dependency tree, what do we gain by ﬁrst computing a different representation such as a phrase structure tree? The principle of parsimony suggests that a simpler model should be preferred over a more complex model, all other things being equal, and the simplest model is arguably one that maps a sentence directly to a dependency tree – a bare-bones dependency parser. In this paper, I characterize the parsing problem faced by such a system, survey the major parsing techniques currently in use, and begin to examine whether the simpler model can in fact rival the performance of more complex systems. Although the empirical evidence is still limited, I conclude that bare-bones dependency parsers fare well in terms of parsing accuracy and often excel in terms of efﬁciency. 
The following work describes a method to automatically classify the sense selection of the complex type Location/Organization –which depends on regular polysemy– using shallow features, as well as a way to increase the volume of sense-selection gold standards by using monosemous data as filler. The classifier results show that grammatical features are the most relevant cues for the identification of sense selection in this instance of regular polysemy. 
In an incremental NLP pipeline every module needs to work incrementally. However, an incremental processing mode can lead to a degradation of accuracy due to the missing context to the right. We discuss three properties of incremental output that can be traded for accuracy, namely timeliness, monotonicity and decisiveness. The consequences of these trade-offs are evaluated systematically for the task of part-of-speech tagging. 
This paper presents work on a comprehensive FrameNet for Danish (cf. www.framenet.dk), with over 12.000 frames, and an almost complete coverage of Danish verb lemmas. We discuss design principles and frame roles as well as the distinctional use of valency, syntactic function and semantic noun classes. By converting frame distinctors into Constraint Grammar rules, we were able to build a robust frame tagger for running Danish text, using DanGram parses as input. The combined context-informed coverage of the parser-frametagger was 94.3%, with an overall F-score for frame senses of 85.12. 
In Danish relative clauses and embedded interrogative clauses are not extraction islands. However, there is an asymmetry between the two clauses. In Danish it is possible to extract the subject out of an embedded interrogative clause. Extraction of the subject out of a relative clause, on the other hand, is not allowed. In this paper we present a formal HPSG analysis of extraction in Danish which treats the extraction out of relative and embedded interrogative clauses in a uniform manner, and the asymmetry between the clauses will be shown to follow from a more general constraint on adjuncts. 
This paper describes the formal structure of the Lithuanian verbs, emphasizing the difference between two kinds of formal patterns called primary and secondary. This short outline attemps to highlight some salient aspects of different descriptive levels (traditionnal model, formalized model and implemented model). 
We present our ongoing work on language technology-based e-science in the humanities, with a focus on text-based research in the historical sciences. Currently, we are working on the adaptation and integration of lexical resources representing different historical stages of Swedish into a lexical and morphological toolbox that will allow us to develop semantically oriented text search applications for historical research on Swedish text. We describe a semantic search prototype which was built using REST web services from this toolbox as components, and which has been evaluated by historians interested in using digitized 19th century novels as primary data for an historical investigation of the emerging consumer society in 19th century Sweden. 
This paper describes a protocol for the evaluation of bilingual terminologies acquired from comparable corpora. The aim of the protocol is to assess the terminologies’added-value in a task of specialized translation. The protocol consists in having specialized texts translated in various situations: without any specialized resource, with an domain-related bilingual terminology or using Internet. By comparing the quality of the segments translated using these various resources, we are able to assess the impact of our bilingual terminologies on the quality of the translation. 
Analysing Nordic persons’ names with respect to language identiﬁcation is a very hard task, as the chosen group of languages is closely related, but provides interesting insights into the structure of names; it is also a task that has many applications in information extraction, speech synthesis, and automatic transliteration. In this paper we present and discuss results obtained by statistical language modelling as well as by a handcrafted deﬁnite clause grammar. 
This paper explores the problem of extracting domain speciﬁc terminology in the ﬁeld of science and education from Lithuanian texts. Four different term extraction approaches have been applied and evaluated. 
We present a learning device able to deduce a set of Danish color and shape terms. Only two data sources are available to the learner: A phonetic transcription of a human informant solving a description task, and a minimal formal model of the picture being described. The system thus contains no preconceived lexical, morphological, or semantic categories. The test data are from the phonetic corpus DanPASS, a standard Danish reference corpus. The learning device, called InShape-2, is an early result of an ambitious research programme at CMOL on data-driven language learning. 
A common problem when combining two bilingual dictionaries to make a third, using one common language as a pivot language, is the emergence of false translations due to lexical ambiguity between words in the languages involved. This paper examines if the translation accuracy improves when using part-of-speech ﬁltering of translation candidates. To examine this, two different Japanese-Swedish lexicons were created, one with part-ofspeech ﬁltering, one without. The results show 33 % less translation candidates and a higher quality lexicon when using part-of-speech ﬁltering. It also resulted in a free lexicon of Swedish translations to 40 716 Japanese entries with a 90 % precision, and the conclusion that part-ofspeech ﬁltering is an easy way of improving the translation quality in this context. 
Word alignment gold standards are an important resource for developing and evaluating word alignment methods. In this paper we present a free English–Swedish word alignment gold standard consisting of texts from Europarl with manually veriﬁed word alignments. The gold standard contains two sets of word aligned sentences, a test set for the purpose of evaluation and a training set that can be used for supervised training. The guidelines used for English–Swedish alignment were created based on guidelines for other language pairs and with statistical machine translation as the targeted application. We also present results of intrinsic evaluation using our gold standard and discuss the relationship to extrinsic evaluation in a statistical machine translation system. 
We present on-going work on estimating the relevance of the results of an Information Extraction (IE) system. Our aim is to build a user-oriented measure of utility of the extracted factual information. We describe experiments using discourse-level features, with classiﬁers that learn from users’ ratings of relevance of the results. Traditional criteria for evaluating the performance of IE focus on correctness of the extracted information, e.g., in terms of recall, precision and F-measure. We introduce subjective criteria for evaluating the quality of the extracted information: utility of results to the end-user. To measure utility, we use methods from text mining and linguistic analysis to identify features that are good predictors of the relevance of an event or a document to a user. We report on experiments in two real-world news domains: business activity and epidemics of infectious disease. 
This paper discusses an investigation into the Norwegian NoWaC corpus. We have compared this web corpus with one corpus of spoken language and one of written language. For nearly all variables that we look at, the web corpus sides with the written corpus, not the spoken one. Thus, despite including language samples from blogs and web forums, NoWaC does not appear to be more speechlike. One exception is interjections, which it does have to some larger extent than the written corpus. It also has taboo words, lacking in the other two. The comparisons have been made purely on the basis of frequency lists, showing that this is a possible and simple way of comparing corpora. We use both a qualitative and quantitative method. In the latter, (log) relative frequency plots show an almost linear relation between NoWaC and the written corpus. 
This article introduces a corpus-based method for improving the process of automatic morphological analysis of a non-standard text variety. More precisely, our paper is concerned with the morphological analysis of Estonian chatroom texts. First, the morphological analyzer designed for the standard written Estonian is used for the analysis of chatroom texts. On the basis of output error analysis a method for improving the process is proposed. We take advantage of the fact that there are deviations with high token frequency, but low type frequency, on the one hand, and deviations with low token frequency, but high type frequency, on the other hand. The first group has to be manually compiled into a user lexicon, whereas the second group of errors can be taken care of by automatic means: automatic preprocessing of texts and automatic complementation of the user lexicon. As a result, the percentage of unknown tokens in the output of the morphological analyzer decreases from 27 to 10.5. 
We describe a system for interactive modiﬁcation of syntax trees by intuitive editing operations on the surface string. The system has a graphical interface, where the user can move, replace, add, and in other ways modify, words or phrases. During editing, the sentence is kept grammatical, by automatically rearranging words and changing inﬂection, if necessary. This is accomplished by combining constraints on syntax trees with a distance measure between trees. 
FinnWordNet is a wordnet for Finnish that complies with the format of the Princeton WordNet (PWN) (Fellbaum, 1998). It was built by translating the Princeton WordNet 3.0 synsets into Finnish by human translators. It is open source and contains 117000 synsets. The Finnish translations were inserted into the PWN structure resulting in a bilingual lexical database. In natural language processing (NLP), wordnets have been used for infusing computers with semantic knowledge assuming that humans already have a sufﬁcient amount of this knowledge. In this paper we present a case study of using wordnets as an electronic dictionary. We tested whether native Finnish speakers beneﬁt from using a wordnet while completing English sentence completion tasks. We found that using either an English wordnet or a bilingual English-Finnish wordnet signiﬁcantly improves performance in the task. This should be taken into account when setting standards and comparing human and computer performance on these tasks. 
This paper describes the collection and annotation of comparable multimodal corpora for Nordic languages in a project involving research groups from Denmark, Estonia, Finland and Sweden. The goal of the project is to provide annotated multimodal resources to study communicative phenomena, such as feedback, turn-taking and sequencing in the languages involved in the project and to compare these phenomena. Studies so far include verbal expressions, head movements and facial expressions related to feedback. 
Since the 1950s, linguists have been using short lists (40–200 items) of basic vocabulary as the central component in a methodology which is claimed to make it possible to automatically calculate genetic relationships among languages. In the last few years these methods have experienced something of a revival, in that more languages are involved, different distance measures are systematically compared and evaluated, and methods from computational biology are used for calculating language family trees. In this paper, we explore how this methodology can be extended in another direction, by using larger word lists automatically extracted from a parallel corpus using word alignment software. We present preliminary results from using the Europarl parallel corpus in this way for estimating the distances between some languages in the Indo-European language family. 
We show that the quality of sentence-level subjectivity classiﬁcation, i.e. the task of deciding whether a sentence is subjective or objective, can be improved by incorporating hitherto unused features: readability measures. Hence we investigate in 6 different readability formulae and propose an own. Their performance is evaluated in a 10-fold cross validation setting using machine learning. Thereby, it is demonstrated that sentence-level subjectivity classiﬁcation beneﬁts from employing readability measures as features in addition to already well-known subjectivity clues. 
Recent research has shown that MT-based sentence alignment is a robust approach for noisy parallel texts. However, using Machine Translation for sentence alignment causes a chicken-and-egg problem: to train a corpus-based MT system, we need sentence-aligned data, and MT-based sentence alignment depends on an MT system. We describe a bootstrapping approach to sentence alignment that resolves this circular dependency by computing an initial alignment with length-based methods. Our evaluation shows that iterative MT-based sentence alignment signiﬁcantly outperforms widespread alignment approaches on our evaluation set, without requiring any linguistic resources other than the to-be-aligned bitext. 
We introduce a framework for POS tagging which can incorporate a variety of different information sources such as statistical models and hand-written rules. The information sources are compiled into a set of weighted ﬁnite-state transducers and tagging is accomplished using weighted ﬁnite-state algorithms. Our aim is to develop a fast and ﬂexible way for trying out different tagger designs and combining them into hybrid systems. We test the applicability of the framework by constructing HMM taggers with augmented lexical models for English and Finnish. We compare our taggers with two existing statistical taggers TnT and Hunpos and ﬁnd that we achieve superior accuracy. 
This paper presents an innovative research resulting in the English-Lithuanian statistical factored phrase-based machine translation system with a spatial ontology. The system is based on the Moses toolkit and is enriched with semantic knowledge inferred from the spatial ontology. The ontology was developed on the basis of the GeoNames database (more than 15 000 toponyms), implemented in the web ontology language (OWL), and integrated into the machine translation process. Spatial knowledge was added as an additional factor in the statistical translation model and used for toponym disambiguation during machine translation. The implemented machine translation approach was evaluated against the baseline system without spatial knowledge. A multifaceted evaluation strategy including automatic metrics, human evaluation and linguistic analysis, was implemented to perform evaluation experiments. The results of the evaluation have shown a slight improvement in the output quality of machine translation with spatial knowledge. 
We have developed an extraction based summarizer based on a word space model and PageRank and compared the readability of the resulting summaries with the original text, using various measures for Swedish and texts from different genres. The measures include among others readability index (LIX), nominal ratio (NR) and word variation index (OVIX). The measures correspond to the vocabulary load, idea density, human interest and sentence structure of the text and can be used to indicate the difﬁculty a reader might have in processing the text. The results show that the summarized texts are more readable, indicating that summarization can be used to reduce the effort to read a text. 
PP attachment has attracted considerable interest the last two decades. The standard set-up has been to extract quadruples of verbs, direct objects, prepositions and prepositional complements from the Wall Street Journal and classify them as either low or high attachments. State-of-the-art results almost equal human performance in the standard set-up. Recently, however, Atterer and Schu¨tze (2007) has questioned this methodology. In this paper, we show that state-of-theart results can be achieved by simpler means than what has previously been shown, using graphical models, but also that state-of-theart parsers perform insigniﬁcantly worse than state-of-the-art PP attachment classiﬁers. This questions the usefulness of previous studies of PP attachment, even if the methodology in these studies is sound.  ones in rows 2–8. A distributional cluster is a set of words that have similar distributions according to a hierarchical clustering algorithm, typically based on probabilities in a bigram language model. The granularity of clusters varies, but we will use a 1000 clusters in our experiments below. Example. To see the complexity of this learning problem, consider the following four examples: (1) (Andy Warhol) painted paintings with 3D-glasses. (2) (Andy Warhol) painted [portraits with 3D-glasses]. (3) (Andy Warhol) painted [paintings with ice-cream]. (4) (Andy Warhol) painted portraits with ice-cream. The square brackets indicate likely low attachment, i.e. that the prepositional phrase most naturally modiﬁes the noun. Note that the four data points have an XOR-like distribution in twodimensional space:  
This report describes a novel approach to modified re-synthesis, by concatenation of speech from different speakers. The system removes an initial voiceless plosive from one utterance, recorded from a child, and replaces it with another voiceless plosive selected from a database of recordings of other child speakers. Preliminary results from a listener evaluation are reported. 
Psycho-acoustical research investigates how human listeners are able to separate sounds that stem from different sources. This ability might be one of the reasons that human speech processing is robust to noise but methods that exploit this are, to our knowledge, not used in systems for automatic formant extraction or in modern speech recognition systems. Therefore we investigate the possibility to use harmonics that are consistent with a harmonic complex as the basis for a robust formant extraction algorithm. With this new method we aim to overcome limitations of most modern automatic speech recognition systems by taking advantage of the robustness of harmonics at formant positions. We tested the effectiveness of our formant detection algorithm on Hillenbrand’s annotated American English Vowels dataset and found that in pink noise the results are competitive with existing systems. Furthermore, our method needs no training and is implementable as a realtime system which contrasts many of the existing systems. 
This paper introduces a modiﬁed version of Random Indexing, a technique for dimensionality reduction based on random projections. We here describe how RI can be efﬁciently implemented using the notion of universal hashing. This eliminates the need to store any random vectors, replacing them instead with a small number of hash-functions, thereby dramatically reducing the memory footprint. We dub this reformulated version of the method Hashed Random Indexing (HRI). 
We consider generative probabilistic models for unsupervised learning of morphology. When training such a model, one has to decide what to include in the training data; e.g., should the frequencies of words affect the likelihood, and should words occurring only once be discarded. We show that for a certain type of models, the likelihood can be parameterized on a function of the word frequencies. Thorough experiments are carried out with Morfessor Baseline, evaluating the resulting quality of the morpheme analysis on English and Finnish test sets. Our results show that training on word types or with a logarithmic function of the word frequencies give similar scores, while a linear function, i.e., training on word tokens, is signiﬁcantly worse. 
This paper presents a disambiguation method for English apostrophe+s contractions. They occur frequently in subtitles and pose special difﬁculties for Machine Translation. We propose to disambiguate these contractions in a pre-processing step and show that this leads to improved translation quality. 
This paper introduces several models for aligning etymological data, or for ﬁnding the best alignment at the sound or symbol level, given a set of etymological data. This will provide us a means of measuring the quality of the etymological data sets in terms of their internal consistency. Since one of our main goals is to devise automatic methods for aligning the data that are as objective as possible, the models make no a priori assumptions—e.g., no preference for vowel-vowel or consonantconsonant alignments. We present a baseline model and successive improvements, using data from Uralic language family. 
In this paper, we explore different linguistic structures encoded as convolution kernels for the detection of subjective expressions. The advantage of convolution kernels is that complex structures can be directly provided to a classiﬁer without deriving explicit features. The feature design for the detection of subjective expressions is fairly difﬁcult and there currently exists no commonly accepted feature set. We consider various structures, such as constituency parse structures, dependency parse structures, and predicateargument structures. In order to generalize from lexical information, we additionally augment these structures with clustering information and the task-speciﬁc knowledge of subjective words. The convolution kernels will be compared with a standard vector kernel. 
A novel technique of adding positionwise ﬂags to one-level ﬁnite state lexicons is presented. The proposed ﬂags are kinds of morphophonemic markers and they constitute a ﬂexible method for describing morphophonological processes with a formalism that is tightly coupled with lexical entries and rule-like regular expressions. The formalism is inspired by the techniques used in two-level rule compilation and it practically compiles all the rules in parallel, but in an efﬁcient way. The technique handles morphophonological processes without a separate morphophonemic representation. The occurrences of the allomorphophonemes in latent phonological strings are tracked through a dynamic data structure into which the most prominent (i.e. the best ranked) ﬂags are collected. The application of the technique is suspected to give advantages when describing the morphology of Bantu languages and dialects. 
Plagiarism detection is a challenge for linguistic models — most current implemented models use simple occurrence statistics for linguistic items. In this paper we report two experiments related to plagiarism detection where we use a model for distributional semantics and of sentence stylistics to compare sentence by sentence the likelihood of a text being partly plagiarised. The result of the comparison are displayed for visual inspection by a plagiarism assessor. 
This paper reports on the implementation of the Latvian grammar checker. It gives a brief introduction of the project scope – Latvian language, the previous implementation of the grammar checker and its limitations. Then, it describes the proposed approach. This paper also describes the Latvian parser used for this project and the quality measurement methods used for the quality assessment of the grammar checking system. Finally, the current state of the grammar checker work is presented. 
In this paper we present a ﬁrst analysis towards better understanding of the query constraining aspects of knowledge, as expressed in the most used public medical bibliographic database MEDLINE. Our results indicate that new terms occur, but also that traditional terms are replaced by more speciﬁc ones and decrease in use as major deﬁning keywords, even though they are still used in abstracts. In other words, as knowledge, including terminology, evolve over time, queries and search methods will have to adapt to these changes to enable ﬁnding recent as well as older research papers in databases. 
Today, spelling and grammar checkers are integrated into modern word processing environments. In some contexts of writing however, these components are not sufficient. In companies that write technical documentation, or when writing a research paper for a specific scientific community, style guides that can only be found in handbooks need to be followed. If such rules are to be implemented in a language-checking framework, they need to be analyzed to identify the requirements on the framework. A categorization scheme for such analysis does not seem to exist, hence the contribution of this paper – a first attempt at a scheme for classifying style guide rules for future implementation. 
Information access from clinical text is a research area which has gained a large amount of interest in recent years. Automatic syntactic analysis for the creation of deeper language models is potentially very useful for such methods. However, syntactic parsers that are tailored to accommodate for the distinctive properties of clinical language are rare and costly to build. We present an initial study on the applicability of an existing parser, pretrained on general Swedish, to clinical text in Swedish. We manually evaluate twelve documents and obtain a 92.4% part-ofspeech tagging accuracy and a 76.6% labeled attachment score for the syntactic dependency parsing. 
We present an algorithm for verb detection of a language in question in a completely unsupervised manner. First, a shallow parser is applied to identify – amongst others – noun and prepositional phrases. Afterwards, a tag alignment algorithm will reveal ﬁxed points within the structures which turn out to be verbs. Results of corresponding experiments are given for English and German corpora employing both supervised and unsupervised algorithms for part-of-speech tagging and syntactic parsing. 
A publically available wordnet browser will, if it does not remain in obscurity, have to cater to two different audiences: the professional lexicographers and the general public. This demonstration paper describes the wordnet browser “Andre ord” which has been developed for the Danish wordnet, DanNet. The first version was released in autumn 2009, followed by the release of a refined version in late 2010. The browser applies the open source framework Ruby on Rails and the graphing toolkit Protovis, and is itself open source. In the paper we discuss what design compromises might be needed when accommodating professionals and non-specialists alike, although our main concern is giving the general public an intuitive impression of the resource. To this aim we adopt the familiar, dictionary-like word-in-synset as the basic unit of the browser idea, but at the same time try to convey the idea that every piece of information in the wordnet is located somewhere in a larger semantic network structure. 
In this paper we present an open source implementation for Finnish morphological parser. We shortly evaluate it against contemporary criticism towards monolithic and unmaintainable finite-state language description. We use it to demonstrate way of writing finite-state language description that is used for varying set of projects, that typically need morphological analyser, such as POS tagging, morphological analysis, hyphenation, spell checking and correction, rule-based machine translation and syntactic analysis. The language description is done using available open source methods for building finitestate descriptions coupled with autotoolsstyle build system, which is de facto standard in open source projects. 
In this paper we demonstrate a hybrid treebank encoding format, derived from the dependency-based format used in Prague Dependency Treebank (PDT). We have specified a Prague Markup Language (PML) profile for the SemTiKamols hybrid grammar model that has been developed for languages with relatively free word order (e.g. Latvian). This has allowed us to exploit the tree editor TrEd that has been used in PDT development. As a proof of concept, a small Latvian treebank has been created by annotating 100 sentences from ―Sophie‘s World‖. 
Linguistic ﬁeldwork data – in the form of basic vocabulary lists – for nine closely related language varieties are compared using an automatic procedure with manual feedback, whose major advantage is its complete consistency. The results of the vocabulary comparison turn out to be in accord with other linguistic features, making this methodology a promising addition to the toolbox of genetic lingusitics. 
This paper presents ongoing research aiming at the automated extraction of knowledge-rich contexts (KRCs) from a Russian language corpus. The notion of KRCs was introduced by Meyer (2001) and refers to a term’s co-text (Sebeok, 1986) as a reservoir of potentially important information about a concept. From a terminological point of view, it seems that KRCs contain exactly the kind of information that should be included into a terminology database. Accordingly, the question how KRCs can be automatically acquired has been widely studied in recent years. However, many languages including Russian still lack thorough study. This paper presents preliminary experimental results obtained on a specialized corpus in the automotive domain. 
Word alignment is necessary for statistical machine translation (SMT), and reordering as a preprocessing step has been shown to improve SMT for many language pairs. In this initial study we investigate if both word alignment and reordering can be improved by iterating these two steps, since they both depend on each other. Overall no consistent improvements were seen on the translation task, but the reordering rules contain different information in the different iterations, leading us to believe that the iterative strategy can be useful. 
Manually performed treebanking is an expensive effort compared with automatic annotation. In return, manual treebanking is generally believed to provide higherquality/value syntactic annotation than automatic methods. Unfortunately, there is little or no empirical evidence for or against this belief, though arguments have been voiced for the high degree of subjectivity in other levels of linguistic analysis (e.g. morphological annotation). We report a double-blind annotation experiment at the level of dependency syntax, using a small Finnish corpus as the analysis data. The results suggest that an interannotator agreement can be reached as a result of reviews and negotiations that is much higher than the corresponding labelled attachment scores (LAS) reported for stateof-the-art dependency parsers. 
An implementation of automatic question generation (QG) from raw Swedish text is presented. QG is here chosen as an alternative to natural query systems where any query can be posed and no indication is given of whether the current text database includes the information sought for. The program builds on parsing with grammatical functions from which corresponding questions are generated and it incorporates the article database of Swedish Wikipedia. The pilot system is meant to work with a text shown in the GUI and autocompletes user input to help find available questions. The act of question generation is here described together with early test results regarding the current produced questions. 
We report the results from a sentencealignment experiment on DanishBulgarian and English-Bulgarian parallel texts applying a method based in part on linguistic motivations as implemented in the TCA2 aligner. Since the presence of cognates has a bearing on the alignment score of candidate sentences we attempt to bridge the gap between source and target languages by transliteration of the Bulgarian text, written originally in Cyrillic. An improvement in F1-measure is achieved in both cases. 
In this paper, we present results of an ongoing investigation of a manually aligned parallel treebank and an automatic tree aligner. We establish the features that show a signiﬁcant correlation with alignment performance. We present those features with the biggest correlation scores and discuss their signiﬁcance, with mention of future applications of these ﬁndings. 
This document describes the method, results and conclusions from my master’s thesis in Nordic studies. My aim was to assess the speech quality of the Norwegian Filibuster text-to-speech system with the synthetic voice Brage. The assessment was carried out with a survey and an intelligibility test at phoneme, word and sentence level. The evaluation criteria used in the study were intelligibility, naturalness, likeability, acceptance and suitability. 
This paper presents the statistical part-ofspeech tagger HunPoS trained on a Persian corpus. The result of the experiments shows that HunPoS provides an overall accuracy of 96.9%, which is the best result reported for Persian part-of-speech tagging. 
The research project presented in this paper aims at identification of context markers for Russian nouns and their use in construction identification. The body of contexts has been extracted from the Russian National Corpus (RNC). The context processing procedure takes into account the lexical and semantic information represented in the corpus annotation. Merged meaning of words are taken into consideration. The reported results contribute to task of building a comprehensive lexicographic resource — the Index of Russian lexical constructions.1 
In this work, we propose to investigate the automatic identification of the relations among portions of several texts suggested by the CST, developing an automated multidocument parser. We explore, in particular, the use of traditional (flat) and hierarchical machine learning techniques in this task, using a corpus of news texts written in Brazilian Portuguese, already annotated according to CST, which allows applying machine learning techniques and testing them. The results obtained are that the performance of some 
• they are easily found on the Web and on document collections. For example, it would be difﬁcult to ignore the number of summaries available in public sources such as Wikipedia or in background pieces of news about particular events which frequently include condensed descriptions, i.e., summaries, of past similar events. 
144 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 144–153, Cuiab´a, MT, Brazil, October 24–26, 2011. c 2011 Sociedade Brasileira de Computa¸c˜ao  reported, named the joint-sequence model, [Bisani and Ney 2008]. In this model graphemes and phonemes are combined into a single state, giving rise to "graphonemes". Although the joint-sequence model has shown to be a powerful tool, we also show in this paper that for the case of the Portuguese the determination of the stressed vowel leads to a substantial improvement in the system performance, as was also reported in [Caseiro et al. 2002]. Thus, we included a linguistically rule based preprocessing stage, for stress assignment, which marks and disambiguates most of the pronunciations. The vocabulary used to generate the pronunciation dictionary is in its previous form of the current "Acordo Ortográfico" (AO). However, we think that this mixedbased G2P can also achieve good performance for European Portuguese (EP) with the AO. The inherent flexibility in dealing with the EP could be extended to other Romanic languages, which makes this an advantageous approach. The remainder of the paper is organized as follows. In Section 2, the jointsequence model is briefly discussed. Section 3 presents how the vocabulary and dictionary were generated while Section 4 describes the linguistic model. In Section 5 experimental results are presented, the main conclusions are summarized and future work directions are foreseen.  2. Joint-Sequence Model  Given a sequence of N graphemes defined by G = G1N = {g1, g2 ,⋯, gN } , the goal is to find a sequence of M phonemes, F = F1M = { f1, f2 ,⋯, fM } , that best describes the phonetic transcription of the original sentence. The statistical approach to this problem corresponds to the determination of the optimal sequence of phonemes, F*, that maximizes the conditional probability of phonemes, F, given a sequence of graphemes, G:  F* = arg max P ( F | G) .  (1)  F  It is difficult to determine F* directly by calculating P(F | G) for all possible sequences  F. However, using the Bayes theorem, we can rewrite the problem as:  F* = arg max P ( F | G) = arg max{P (G | F ) ⋅ P(F) / P(G)}.  (2)  F  F  Since P(G) is common to all sequences F, the problem can be simplified in the following way:  F* = arg max P (G | F ) ⋅ P(F ) .  (3)  F  Using a phonological dictionary, previously created, it is possible to estimate P(G|F) and the a priori probability, P(F), for all sequences F and G found in this dictionary. The Markov based approaches estimate a model for each phoneme and use ngram models to compute P(F). These approaches model the dependency between graphemes and phonemes and the dependency between phonemes, but do not model dependencies between graphemes [Taylor 2005], [Demberg 2006], [Jiampojamarn and  145  Kondrak 2009]. Due to these constraints, other statistical approaches emerged proposing joint probability models P(F,G) to determine the optimal sequence of phonemes [Bisani and Ney 2002], [Galescu and Allen 2001], directly using the expression of the joint probability in (1) in place of the conditional probability. In this approach, all the dependencies present in the dictionary were modeled, resulting in improved performances than those obtained by the other models. 2.1 Alignment between graphemes and phonemes Some graphemes have a univocal correspondence with the phonemes. However, for other graphemes the correspondence to phonemes depends on several factors, such as the grapheme context and the part-of-speech. There are also cases where several graphemes may lead to a single phoneme, and where a single grapheme can lead to several phonemes. All statistical approaches face this problem, being necessary, during the training process, segment and align the two sequences (a phoneme sequence and the corresponding grapheme sequence) with an equal number of segments. The solution is not always trivial or unique and depends on how the alignment algorithms associate graphemes to phonemes of a given word. Alignment can be classified as follows [Jiampojamarn et al. 2007]: 1) "one-to-one" - Each grapheme relates with only one phoneme (segments with one symbol only). A null symbol ('_') is used to deal with the cases in which a grapheme can originate more than one phoneme (the insertion of phonemes) or the cases where more than one grapheme originate only one phoneme (the deletion of phonemes). This alignment is easy to implement using the Levenshtein algorithm, [Navarro 2001]. In the literature these algorithms are called alignment "01-01" if insertions and deletions of phonemes are allowed, or "1-01" if only deletion of phonemes are allowed. This last case corresponds to the alignment used in this work. 2) "many-to-many" - The segments are composed of various symbols, which allow the association of several graphemes to several phonemes. This alignment is more generic and can be used without any prior knowledge of mapping between graphemes and phonemes. It handles insertions and deletions of phonemes without using any special symbol. On the other hand, the resulting model is more difficult to estimate and its performance is generally lower than the model with alignment "one-to-one". These alignments are also known as "n-to-m". 2.2 Statistical model After the alignment, the sequences of graphemes and phonemes have the same number of segments. So, a new entity, born from the association of a segment of graphemes and phonemes can be defined, and is called "graphone(me)" [Bisani and Ney 2002]. A sequence of K graphonemes is annotated as Q(F,G) = {q1, q2 ,⋯,qK } . Given a sequence of K graphonemes, Q(F,G), rather than assuming independence between symbols, the probability of the joint-sequence, P (Q(F,G)) , can be estimated using the so-called "n- grams" (sequences limited to n symbols). 146  2.3 Model estimation The n-gram models are used to estimate the probability of symbols knowing the previous n−1 symbols (history). The estimation of the probability of an n-gram is based on the number of its occurrences. This probability is easy to compute, but there is a problem in assigning a zero probability to the n-grams not seen or with limited number of training examples. To overcome this limitation, it is necessary to model unseen examples (using a discount) or uncommon examples (using smoothing). Thus, a small probability mass must be reserved from the most frequent n-grams to the absent or uncommon n-grams. There are several proposed algorithms to solve this problem of probability mass redistribution, such as Good-Turing [Good 1953], Witten-Bell [Witten and Bell 1991], Kneser-Ney [Kneser and Ney 1995], Ney’s absolute discount [Ney et al. 1994] and Katz’s smoothing [Katz 1987]. In this work we adopted the algorithm implemented by [Demberg et al. 2007], which uses a modified version of Kneser-Ney algorithm [Chen and Goodman 1998]. 3. Pronunciation Dictionary In this work we intend to create a pronunciation dictionary from a given vocabulary. The vocabulary derives from the CETEMPúblico corpus [Santos and Rocha 2001], that corresponds to a collection of newspaper extracts published from 1991 to 1998, annotated in terms of sentences and containing 180 million words in European Portuguese. The process of generating the vocabulary starts by taking all the strings annotated as words, which obey simultaneously to the following criteria: i) start with a letter (a-z, A-Z, á-ú, Á-Ú); ii) do not contain digits; iii) are not all upper case (e.g. acronyms); iv) do not have the character '.' (e.g. URLs); v) end with a letter; vi) the corresponding lemmas do not contain '=' (e.g. compound nouns). From the resulting list, we took the sub-list of words that occur more than 70 times in the corpus, totaling about 50k different words. Foreign words were then removed, using an automatic criteria followed by manual verification. This process results on a vocabulary of 41,586 words. 3.1 Transcription The transcription of the vocabulary words is a result of an iterative procedure. First, a statistical model was estimated, as described in 2.2, using the SpeechDat pronunciation dictionary, [SpeechDAT 1998]. This dictionary contains about 15k entries, from which foreign words were deleted. Some SAMPA transcriptions [Wells 1997] were substituted according to the following directions: 1) we did not use the velar /l~/ and the semivowels /j/ and /w/; and 2) some standardization in the pronunciations was done. The result of applying the statistical model to CETEMPúblico vocabulary was fairly accurate, although with some significant flaws. Then we followed a long procedure of manual verification and correction of the transcriptions. The next pass was to compare the transcriptions with other ones, generated by a commercial speech synthesizer. This comparison allowed us to rely on our results since the majority of the transcriptions agreed. All different transcriptions were analyzed one by one and we found that the transcriptions from our dictionary were the right ones most of the times. 147  This has led to the phonological transcription dictionary referred to as "dic_CETEMP_40k". With the "dic_CETEMP_40k", a new statistical model was built. The test of this model on the training dictionary, allowed us to correct some remaining errors as well as to standardize and regulate some transcription procedures. Throughout the development of this work, the dictionary had been revised and corrected. Although it may still contain some errors, we are confident on its accuracy. We think that this dictionary could be an interesting resource for studies about phonetics and phonology of Portuguese. 3.3 Graphoneme alignment An important step for establishing the statistical model is the alignment between graphemes and phonemes in the form "1-01" (one grapheme leads to zero or one phoneme; see § 2.1). The option "1-01" was chosen from the beginning, because we had identified only six cases where a grapheme could give rise to more than one phoneme. Some cases were the insertion of a yod in some words beginning with <ex->; others were the cases of non-common pronunciations such as <põem> → /po~i~6~i~/ and <têm> → /t 6~i~6~i~/. Defining symbols corresponding to more than one phoneme solved this problem of phoneme insertion. The problem of the phoneme deletions still remains, because there are always graphemes that do not give rise to any phoneme. The alignment between graphemes and phonemes was, then, obtained using the known edit distance or Levenshtein algorithm [Navarro 2001]. This required defining a distance between each phoneme and grapheme. This distance or cost of association was defined using the log probability of this association, which was estimated from an aligned dictionary. 4. Phonetic-phonological restrictions Since the EP is a language with much phonological regularity, we added to the G2P module some linguistic restrictions, which were pertinent to convert graphemes into phonemes. Before any regard on the linguistic rules, an aspect concerning the phonetic/phonological binomial must be clarified. While phonetics gives us the physical and articulatory properties of the sound pronounced (it means the surface structure) phonology studies the sound that has a given role in the pronunciation (the underlying structure). However, any methodological perspective concerning the speech transcription links these two linguistic fields since it deals with the inter-relationship between the units and its distinctive character (phonemes) and the physical reality of those units (phones and allophones) [Crystal 2002]. The studies on the G2P often alternate between the term phone: [Caseiro et al. 2002], [Oliveira and al. 2004] with the term phoneme; [Barros and Weis 2006], without any clarification on the perspective followed. We justify our option to adopt the term phoneme mainly because the procedure to convert the letter into the sound brings us information that derives from the structure of the language (such as, both left and right context which imply the choice of a single unit excluding all other units available in the language). The phoneme that corresponds to the grapheme is well accepted as a class to which may group all allophonic realizations able in EP (which could include all the multi pronunciations). We also considered that the phoneme conversion corresponds to 148  the EP-standard. The phonological neutralization of oppositions is not described in this study and phonemes do not represent any archiphonemes. Algorithms have been constructed based on practical linguistic rules, such as stress marking of the vowel (the syllable nuclei) of any single word and identifying short contexts in which the correspondence between grapheme and phoneme has a good stability.  4.1 Rules for stress assignment Following the theoretical assumptions discussed in [Mateus and d'Andrade 2000], we adopted to mark all vowels, which are stressed (the syllable nuclei) within a word. The importance of the stressed vowel (Vstressed) has been recognized in previous G2P works, such as in [Caseiro et al. 2002]. Since the n-grams context is short and cannot, most of the times retain information about the syllable structure, marking the Vstressed improves the statistical model by expressing graphoneme classes unequivocally. As in [Andrade and Viana 1985], our proposal considered to mark the Vstressed (with the symbol ' " ') and did not require the identification of the syllabic unit. However, the process of identifying the Vstressed that is described in this study was achieved in a very simple way. In the following Table 1, a set of rules for stressing vowels is presented with examples. All contexts were considered, including those without a stressed vowel, such as the prepositions <com>, <de>, <em>, <sem>, <sob>, <do(s)>, <no(s)>; the personal pronouns <me>, <te>, <se>, <nos>, <vos>, <lhe(s)>, <o(s)>, <a(s)>, <lo(s)>, <no(s)>, <vo(s)>, <mo(s)>, <to(s)>, <lho(s)>; the relative pronoun <que>; and the conjunctions <e>, <nem>, <que>, <se>; which are often added to a stressed nuclei within the prosodic unit. Table 1: Rules for stress assignment of the vowels (V)  Rules  Example  
2. Background Part-of-speech tagging is the task of assigning parts-of-speech (morphological classes) to words in a sentence. As in most other NLP fields, the biggest problem faced by POS taggers is ambiguity. Many algorithms have been applied to this problem, generally following rule-based, probabilistic or hybrid approaches. Rule-based approaches use hand-written rules for disambiguation. In probabilistic approaches, ambiguity is resolved by models induced from training corpora. Hybrid is based on disambiguation rules, but in this case the rules are induced from tagged corpora. For Portuguese, rule-based approaches are used in [3] (>99% accuracy) and [10] (~98.6% accuracy), probabilistic approaches in [12] (84.5% accuracy) and [8] (88.7% accuracy), and hybrid approaches in [2] (~89.4% accuracy), [9] (~90% accuracy), [7] (95% accuracy) and [6] (97.2% accuracy). 159 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 159–163, Cuiab´a, MT, Brazil, October 24–26, 2011. c 2011 Sociedade Brasileira de Computa¸c˜ao  The latest taggers developed following probabilistic approach, like HMM, have reached accuracy levels about 96-97% with English. Its advantages against rule-based and hybrid approaches include: (1) it does not require so much manual effort or linguistic knowledge to be employed on tagger development; (2) probabilistic taggers are not constrained by rule set coverage (due to the complexity of natural languages, it is extremely hard to build a rule set covering all cases and exceptions); and (3) since it does not depend on language-specific rules, it can be easily adapted for many languages. 3. Extension to HMM with Character Language Models In this work, we use LingPipe’s HMMs [5] for POS tagging. They are much similar to typical HMM tagger implementations, where hidden states correspond to tags and contextual probabilities are estimated for bigrams (first-order HMMs). The point in which they differ from usual HMMs is the way they estimate emission probabilities: instead of using relative frequency distribution of unique tokens, they use bounded character language models, one for each hidden state. The character language models define probability distributions over strings to be emitted from their respective hidden states. They are bounded language models based on character-level n-grams, where probabilities are normalized over strings of a fixed length. This approach brings the advantages of implicitly including morpheme information in the model and defining a proper probability distribution normalized over the infinite set of possible string emissions (so it can estimate probabilities even for words not seen on training). Further details on LingPipe’s character language models are presented in [4]. 4. The Bosque Corpus For training and testing the tagger, we chose to use Bosque corpus, a subset of the Floresta Sintá(c)tica treebank [1] composed of sentences from newspaper texts, with over 180 thousand words tagged with PALAVRAS parser [3] and fully revised by linguists. 4.1. Corpus Preprocessing In this work we used Bosque version 8.0 in SimTreeML [11] format. Since it is a treebank and not just a POS tagged corpus, it includes much information that is beyond our scope. So the first step was to remove all information but the sentences and the POS tags assigned to each of their tokens. For the purposes of this work two tagsets were considered: one including only word class tags (named TSc) and another combining word class and inflexion tags (named TSi). Additionally, punctuation tags were added to both tagsets (each corresponding to one of the punctuation tokens found in the corpus). 4.2. Final Tagsets After preprocessing the corpus, the final version of TSc tagset presented the following 20 word class tags: {adj, adv, art, conj-c, conj-s, ec, intj, n, n-adj, num, pp, pron-det, pron-indp, pron-pers, prop, prp, v-fin, v-ger, v-inf, v-pcp}. The TSi tagset presented 160  238 tags, each one being a combination of one word class tag and one or more of the following inflexion tags (no more than one of each type): {M, F, M/F} for gender; {S, P, S/P} for number; {NOM, ACC, DAT, PIV, ACC/DAT, NOM/PIV} for case; {1, 2, 3, 1/3} for person; {PR, IMPF, PS, MQP, FUT, COND, PS/MQP} for tense; and {IND, SUBJ, IMP} for mood. Additionally, as stated before, both tagsets presented 19 punctuation tags. 5. Experiment Setup The aim of this work was to evaluate the presented approach for POS tagging regarding accuracy (number of correctly tagged tokens / total number of tokens). In order to do so we have randomly partitioned the corpus sentences in two fixed sets: a training set containing approximately 90% of the corpus tokens and a test set containing the remaining tokens. The first was used for training, testing and tuning the models during development with 10-fold cross-validation, and then the second (unseen during development) was used for testing the final models (trained on the entire training set). Since tagsets may differ between works we firstly defined a lower-bound baseline for comparison purposes. This baseline was a unigram POS tagger. As stated in Section 3, our approach presents HMMs with character language model emissions and those language models are based on n-grams. So, there was one parameter to be tuned in our model: max n-gram size for HMM emissions. This tuning was done by varying max n-gram size on a range of values and, for each value, training and testing a corresponding model. This range has the lower bound 1 (unigram) and some upper bound defined by max length of words in the language (setting max n-gram sizes greater than max word length would be useless). Additionally, confusion matrices were built on each test. They were helpful for error analysis, revealing most common sources of tagging mistakes. 6. Results 6.1. Development Phase The first models trained were the unigram baselines. For TSc the accuracy achieved was approximately 85.8% while for TSi the accuracy achieved was approximately 82.3%. The accuracies obtained for TSc and TSi with the HMMs were plotted in the chart shown in Figure 1. Both present a similar logarithmic growth in accuracy as max n-gram size values are increased until they reach a convergence point. As it can be seen, a max n-gram size 2 is enough to overcome the baselines and with max n-gram size 6 accuracy for TSc surpasses 96%, the same level achieved with English probabilistic POS taggers. For TSi, max accuracy reached is approximately 91.9%, which is not a bad result if compared to other Portuguese probabilistic POS taggers and represents a relatively low decrease since it contains more than 6 times the number of tags in TSc. 6.2. Test Phase After the analysis performed in development phase, we have found that a max n-gram size 10 should be enough to achieve the highest accuracy levels: approximately 96.2% with TSc and 91.9% with TSi. 161  In test phase we validate our results by testing our models on unseen data (the test set mentioned in Section 5). So we have re-trained the models with max n-gram size 10 on the entire training data set and tested it on the test set. The accuracy rates achieved were approximately 96.2% with TSc and 92.0% with TSi, which validates the results obtained during development. From the confusion matrix built from the test ran on TSc, we have found that the most common sources of tagging mistakes – summing to approximately 25% of all erroneous tag assignments – involved the tags adj (adjective), n (noun) and prop (proper noun). This means those are the hardest word classes for the POS tagger to distinguish.  Accuracy  TSc  TSi  1.00 0.99 0.98 0.97 0.96 0.95 0.94 0.93 0.92 0.91 0.90 0.89 0.88 0.87 0.86 0.85 0.84 0.83 0.82 0.81 0.80 0.79 0.78 0.77 0.76 0.75 0.74 0.73 0.72 0.71 0.70  
The annotation of a corpus with semantic role labels consists of three subtasks: 1) identification of the “argument taker”, which may be a single verb or a complex predicate 164 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 164–168, Cuiab´a, MT, Brazil, October 24–26, 2011. c 2011 Sociedade Brasileira de Computa¸c˜ao  (light verb constructions or phrasal verbs, for example); 2) identification and delimitation of arguments associated with the “argument taker”, and 3) assignment of a semantic role label to each of these arguments. Annotation over a syntactic tree eliminates the step of arguments delimitation, as the syntactic constituents delimitated by the parser are kept for arguments annotation. Hence, the quality of SRL annotation is dependant of syntactic parsing quality. Recently there are initiatives to make corpus annotation, following Propbank model, for other languages besides English: Korean (Palmer et al, 2006), Chinese (Xue, 2009), Arabic (Palmer et al, 2008) and Basque (Aldezabal et al. 2010). However, as far as we know, there is not until this date such a corpus of Brazilian Portuguese. To fulfill this gap, we report here the construction of a Brazilian Portuguese Propbank: Propbank-Br. This first step of the research aims to pave the way for a broader and distributed annotation task. Language specific challenges became evident during the annotation task and several decisions have been taken to deal with them. This experience enabled us to customize Propbank guidelines and build frames files for Portuguese verbs, essential resources to guide annotators and ensure inter-annotator agreement. 2. A brief outline of Propbank Propbank (Palmer et al 2005) produced a new layer of annotation, adding semantic role labels in a subcorpus of PennTreebank (the financial subcorpus). Additionally, a verb lexicon with verb senses and rolesets have been built and is available for consultation1. Propbank is a bank of propositions. The underlying idea of the term “proposition” is found in frame semantics proposed by Fillmore (1968). A “proposition” is on the basic structure of a sentence (Fillmore, 1968, p.44), and is a set of relationships between nouns and verbs, without tense, negation, aspect and modality modifiers. Arguments which belong to propositions are annotated by Propbank with numbered role labels (Arg0 to Arg5) and modifiers are annotated with specific ArgMs (Argument Modifiers) role labels. Each verb occurrence in the corpus receives also a sense number, which corresponds to a roleset in the frame file of such verb. A frame file may present several rolesets, depending on how many senses the verb may assume. In the roleset, the numbered arguments are “translated” into verb specific role descriptions. Arg0 of the verb “give”, for example, is described as “giver”. 3. Methods and Tools In the same way as Propbank, our aim is to provide a training corpus to build automatic taggers. For this purpose, it was interesting to annotate a corpus syntactically annotated and manually revised. We decided to annotate the Brazilian portion of Bosque, the manually revised subcorpus of Floresta Sintá(c)tica2 (Affonso et al, 2002), parsed by Palavras (Bick, 2000). Bosque has 9368 sentences and 4213 of them correspond to the Brazilian portion (extracted from the journal Folha de São Paulo of 1994). The annotation tool we have chosen was SALTO (Burchardt et al, 2008) due to a previous successful experience we had on assigning wh-questions to verbal arguments, a 
2. Related Work According to related work, it is possible to note that the alignment of syntactic trees is divided in two steps. First, the lexical alignment is applied to align the leaf nodes, then, the other (internal) nodes are aligned. Furthermore, there is a wellformedness criterion for creating internal alignments which states that an ascendant node in the source tree may only be aligned with an ascendant node in the target tree, regarding the previously aligned node. The same is true for descendant nodes: a descendant node in the source tree can only be aligned with a descendant node in the target tree. 169 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 169–173, Cuiab´a, MT, Brazil, October 24–26, 2011. c 2011 Sociedade Brasileira de Computa¸c˜ao  After the alignment of leaf nodes, the internal nodes are aligned following various approaches and distinct criteria. For instance, the method presented in [Lavie et al. 2008] assigns a prime number to each pair of aligned leaf nodes in source and target trees based on the lexical alignment. This alignment is propagated to the highest nodes in a way that the ascendant nodes receive the product of their children, and the internal nodes of both trees with the same resultant value are aligned. Similarly, in [Tinsley et al. 2007] the alignment of internal nodes is accomplished using the alignment probabilities of leaf nodes generated by GIZA++ [Och and Ney 2003]. In this case, the product of the probabilities of lexical alignment (not prime numbers as [Lavie et al. 2008]) is assigned to parent nodes. In [Menezes and Richardson 2001] and [Groves et al. 2004], the proposed methods automatically align fragments of the source tree with the equivalent target tree fragment quickly and consistently using a best-ﬁrst approach and composition rules. Some other methods, such as [Marecek et al. 2008] and [Tiedemann and Kotze´ 2009], use different resources for the alignment of syntactic trees as: preﬁx analysis, part-of-speech and organization of words in the sentence (linear position). For the experiments presented in this paper, the baseline models were implemented based on [Lavie et al. 2008] and [Tinsley et al. 2007] mainly because they do not require rich resources such as [Marecek et al. 2008] neither use manually created composition rules as [Menezes and Richardson 2001] and [Groves et al. 2004]. 3. Models for Aligning Parallel Syntactic Trees 3.1. Model 1 – Based on [Lavie et al. 2008] Following an idea similar to that described in [Lavie et al. 2008], our implementation (model 1) assigns prime numbers to each pair of aligned terminal nodes1. For those nonaligned terminal nodes, model 1 assigns the value 1 and for those nodes with multiple alignments, it assigns the product of the prime numbers of each alignment. Then, in a second step, the values are propagated to the internal nodes (a bottomup approach): the value assigned to a parent node is the product of the values assigned to its child nodes. Finally, the value of each node in the source tree is compared to the values of each target node and the source and target nodes with the same value are aligned. 
2. Method 2.1 Corpus For the purposes of this study, we selected the full Europarl corpus (Koehn 2005) as source for MWE. It has a relatively large size – if we take into consideration that our results were not only automatic evaluated, but also manually validated – consisting of 43,919,903 running words as of October 2010 (version 4). The size of the Europarl varies from time to time, since it incorporates the sections of the European Parliament and is updated on a regular basis. The selected language was English. 214 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 214–218, Cuiab´a, MT, Brazil, October 24–26, 2011. c 2011 Sociedade Brasileira de Computa¸c˜ao  2.2 Steps of automatic extraction and evaluation This study was developed through a series of steps that we describe below. They range from the preprocessing for and with the mwetoolkit to the automatic evaluation using our developed XML parser. 2.2.1 First step – mwetoolkit (pre)processing The first thing to do is preprocessing the corpus with the Tree Tagger (Schmid 1994; Schmid 1995). Those tags will then be simplified by the mwetoolkit for its own purposes. We then ran the mwetoolkit looking for only five bigram patterns: N + N; N + Num; A + N; V + N; V + P. The extraction was made excluding candidates that occurred less than 10 times. The extraction output is then presented in XML and ARFF files. This first extraction of only bigrams was done because we needed an automatic preclassification for the extraction of rules (as explained next), and mwetoolkit’s gold standard only accounts for bigrams. 2.2.2 Second Step – Extraction of rules After extracting the MWE candidates, the mwetoolkit computes various association measures (Maximum Likelihood Estimator = MLE; Pointwise Mutual Information = PMI; T-score = T; Dice’s coefficient = DICE; and Loglikelihood = LL) for them and also compares them with an internal gold standard for preclassification purposes. At the end of the process, mwetoolkit generates an ARFF file, which contains information on association measures and gold standard preclassification (it marks the MWE candidates as “true” or “false”) and can be used in Weka (Hall et al. 2009) for machine learning. In Weka, we used some implemented algorithms with the ARFF file to obtain the threshold values and decide which of MLE, PMI, T, DICE and/or LL would be useful. And this was the most difficult part. Since the candidates in the ARFF files classified as True by the gold standard were very sparse, we couldn’t extract good results with its raw form. This happened because the results seem much alike the ones of a random validation. So we processed the results a bit further. At first, we excluded the MLE value, because it was much too sparse, and many of the MWE candidates didn’t have this value. The second step was taking away the MWE candidates which didn’t have any of the values. The last filter was the SMOTE (Synthetic Minority Oversampling TEchnique) (Chawla et al. 2002), which is a method of over-sampling the minority class and under-sampling the majority class to achieve a better classification quality with the nearest neighbor value 5. Using T, DICE and LL values we obtained the best results using the JRIP (Cohen 1995), which is an optimized rule-based implementation of the IREP (Fürnfranz and Widmer 1994) algorithm. With JRIP we could extract the following rule, with 67,6% precision: • T values over 5.57; • DICE values over 0.02; • LL values between 51000 and 22712. Although we had three values, the LL formula used by the mwetoolkit is only 215  applicable to bigrams, and our study, as will be shown in the next section, comprised MWE that ranged from bigrams to hexagrams, so the LL value was disregarded. PMI was disregarded in the rule generated by JRIP. 2.2.3 Third step – Full extraction of patterns With this rule for automatic validation, we reprocessed the corpus with the mwetoolkit, but this time we used 26 patterns suggested by a linguist. Although not complete, it represents a good set. N+N / N+N+N / N+N+N+N / A+N / A+N+N / A+A+N / A+A+N+N / A+N+N+N A+N+N+N+N / N+Num / A+A+A+N / A+A+N+N+N / V+N / V+N+N / V+DT+N V+DT+N+N / V+DT+A+N / V+DT+A+N+N / V+P / V+P+N / V+P+DT+N / V+P+DT+N+N V+P+DT+A+N / V+P+DT+A+N+N / V+P+A+N / V+P+A+N+N The result of this extraction was also confronted against the mwetoolkit’s gold standard. Since it only comports bigrams, we needed something more complex to validate the other ngrams. For this purpose, we developed a XML parser, which is explained in the next section. 2.2.4 Fourth step – XML Parser As part of this study, we developed a tool to automate the evaluation process of the XML file generated by the mwetoolkit. This tool aims at classifying each MWE candidate as a true or false MWE. For its development, we used Java. This tool analyses the XML using the Document Object Model (DOM)1 through the Java API for XML Processing (JAXP)2. By using DOM, we had an easy way to manipulate the XML file and execute arbitrary modifications. The candidates are then retrieved and checked against a stoplist of treatment pronouns, so as to remove MWE candidates with those kind of words. After this, the association measures are verified against the thresholds (from Section 2.2.2) and classified as True or False. All candidates marked as False in the previous step are then checked against the Free Dictionary3, if the candidate is present, then it is reclassified as True. 3. Results and validation The final extraction, using 26 patterns of MWE candidates, returned more than 82 thousand MWE candidates, as we can see in Table 1. Since the automatic evaluation with the Free Dictionary is rather time consuming, and our final objective was to retrieve only the necessary amount for manual validation, we divided those candidates and automatically evaluated only the first 17,528 MWE candidates. From these, 1,885 were automatically marked as True (10.75%). After using the XML parser for the automatic evaluation, we started the manual 
The paradigm of parsing as intersection has been used throughout the literature to obtain elegant and general solutions to numerous problems involving grammars and automata. The paradigm has its origins in (Bar-Hillel et al., 1964), where a general construction was used to prove closure of context-free languages under intersection with regular languages. It was pointed out by (Lang, 1994) that such a construction isolates the parsing problem from the recognition problem. The latter can be solved by a reduction of the outcome of intersection. The paradigm has been extended in various ways, by considering more powerful formalisms, such as tree adjoining grammars (Vijay-Shanker and Weir, 1993), simple RCGs (Bertsch and Nederhof, 2001), tree grammars (Nederhof, 2009), and probabilistic extensions of grammatical formalisms (Nederhof and Satta, 2003). Different applications have been identiﬁed, such as computation of distances between languages (Nederhof and Satta, 2008), and parameter estimation of probabilistic models (Nederhof, 2005). The lecture will focus on another application, namely the computation of preﬁx probabilities (Nederhof and Satta, 2011c) and inﬁx probabilities (Nederhof and Satta, 2011a) and will address novel generalisations to linear context-free rewriting systems (Nederhof and Satta, 2011b). References Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In  Y. Bar-Hillel, editor, Language and Information: Selected Essays on their Theory and Application, chapter 9, pages 116–150. Addison-Wesley, Reading, Massachusetts. E. Bertsch and M.-J. Nederhof. 2001. On the complexity of some extensions of RCG parsing. In Proceedings of the Seventh International Workshop on Parsing Technologies, pages 66–77, Beijing, China, October. B. Lang. 1994. Recognition can be harder than parsing. Computational Intelligence, 10(4):486–494. M.-J. Nederhof and G. Satta. 2003. Probabilistic parsing as intersection. In 8th International Workshop on Parsing Technologies, pages 137–148, LORIA, Nancy, France, April. M.-J. Nederhof and G. Satta. 2008. Computation of distances for regular and context-free probabilistic languages. Theoretical Computer Science, 395:235–254. M.-J. Nederhof and G. Satta. 2011a. Computation of inﬁx probabilities for probabilistic context-free grammars. In Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, pages 1213–1221, Edinburgh, Scotland, July. M.-J. Nederhof and G. Satta. 2011b. Preﬁx probabilities for linear context-free rewriting systems. In Proceedings of the 12th International Conference on Parsing Technologies, Dublin, Ireland, October. M.-J. Nederhof and G. Satta. 2011c. Preﬁx probability for probabilistic synchronous context-free grammars. In 49th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 460–469, Portland, Oregon, June. M.-J. Nederhof. 2005. A general technique to train language models on language models. Computational Linguistics, 31(2):173–185. M.-J. Nederhof. 2009. Weighted parsing of trees. In Proceedings of the 11th International Conference on Parsing Technologies, pages 13–24, Paris, France, October. K. Vijay-Shanker and D.J. Weir. 1993. The use of shared forests in tree adjoining grammar parsing. In Sixth  
Regular growth automata form a class of inﬁnite machines, in which all local computations are performed by ﬁnite state automata. We present some results which are relevant to application in practice; apart from runtime, the most important one is modularization, that is, abstraction over subroutines. We use the new techniques to prove some results on substitution. 
Finite-state methods are applied to the Russell-Wiener-Kamp notion of time (based on events) and developed into an account of interval relations and semi-intervals. Strings are formed and collected in regular languages and regular relations that are argued to embody temporal relations in their various underspeciﬁed guises. The regular relations include retractions that reduce computations by projecting strings down to an appropriate level of granularity, and notions of partiality within and across such levels. 
Analyzing the logical structure of a sentence is important for understanding natural language. In this paper, we present a task of Recognition of Requisite Part and Effectuation Part in Law Sentences, or RRE task for short, which is studied in research on Legal Engineering. The goal of this task is to recognize the structure of a law sentence. We empirically investigate how the RRE task is conducted with respect to various supervised machine learning models. We also compared the impact of unlabeled data to RRE tasks. Experimental results for Japanese legal text domains showed that sequence learning models are suitable for RRE tasks and unlabled data also signiﬁcantly contribute to the performance of RRE tasks. 
This paper describes a non-conventional method for compiling (phonological or morpho-syntactic) context restriction (CR) constraints into non-deterministic automata in ﬁnite-state tools and surface parsing systems. The method reduces any CR into a simple one that constraints the occurrences of the empty string and represents right contexts with co-determististic states. In cases where a fully deterministic representation would be exponentially larger, this kind of inward determinism in contexts can bring beneﬁts over various De Morgan approaches where full determinization is necessary. In the method, an accepted word gets a unique path that is a projection of a ladder-shaped structure in the context recognizer. This projection is computed in time that is polynomial to the number of context states. However, it may be difﬁcult to take advantage of the method in a ﬁnite-state library that coerces intermediate results into canonical automata and whose intersection operation assumes deterministic automata. 
We propose an approach to parsing Constraint Grammars using ﬁnite-state transducers and report on a compiler that converts Constraint Grammar rules into transducer representations. The resulting transducers are further optimized by conversion to left and right sequential transducers. Using the method, we show that we can improve on the worstcase asymptotic bound of Constraint Grammar parsing from cubic to quadratic in the length of input sentences. 
In this paper we present a system for named entity recognition and tagging in Serbian that relies on large-scale lexical resources and ﬁnite-state transducers. Our system recognizes several types of name, temporal and numerical expressions. Finite-state automata are used to describe the context of named entities, thus improving the precision of recognition. The widest context was used for personal names and it included the recognition of nominal phrases describing a person’s position. For the evaluation of the named entity recognition system we used a corpus of 2,300 short agency news. Through manual evaluation we precisely identiﬁed all omissions and incorrect recognitions which enabled the computation of recall and precision. The overall recall R = 0.84 for types and R = 0.93 for tokens, and overall precision P = 0.95 for types and P = 0.98 for tokens show that our system gives priority to precision. 
It is well known that context-free parsing can be seen as the intersection of a contextfree language with a regular language (or, equivalently, the intersection of a context-free grammar with a ﬁnite-state automaton). The present article provides a practical efﬁcient way to compute this intersection by converting the grammar into a special ﬁnite-state automaton (the GLR(0)-automaton) which is subsequently intersected with the given ﬁnite-state automaton. As a byproduct, we present a generalisation of Tomita’s algorithm to recognize several inputs simultaneously. 
This paper introduces the Transducersaurus toolkit which provides a set of classes for generating each of the fundamental components of a typical WFST-based ASR cascade, including HMM, Context-dependency, Lexicon, and Grammar transducers, as well as an optional silence class WFST. The toolkit further implements a small scripting language in order to facilitate the construction of cascades via a variety of popular combination and optimization methods and provides integrated support for the T3 and Juicer WFST decoders, and both Sphinx and HTK format acoustic models. New results for two standard WSJ tasks are also provided, and the toolkit is used to compare a variety of construction and optimization algorithms. These results illustrate the ﬂexibility of the toolkit as well as the tradeoffs of various build algorithms. 
This paper uses the task of transliterating an Egyptian Hieroglyphic text into the latin alphabet as a model problem to compare two ﬁnite-state formalisms : the ﬁrst one is a cascade of binary transducers; the second one is a class of multitape transducers expressing simultaneous constraints. The two systems are compared regarding their expressivity and readability. The ﬁrst system tends to produce smaller machines, but is more tricky, whereas the second one leads to more abstract and structured rules. 
Brill’s part-of-speech tagger is deﬁned through a cascade of leftmost rewrite rules. We revisit the compilation of such rules into a single sequential transducer given by Roche and Schabes (Comput. Ling. 1995) and provide a direct construction of the minimal sequential transducer for each individual rule. Keywords. Brill Tagger; Sequential Transducer; POS Tagging 
In this paper we describe our work in progress on FTrace, a tool for finite-state morphology that provides a tracing facility for developers of applications for synchronic and diachronic language descriptions. We discuss not only the current tool for downward tracing, but also the challenges that we face in the further development of FTrace, especially in upward tracing. Finally, we present an example, draw some conclusions, and outline our future work. Keywords: FTrace, tracing, finite-state morphology, xfst, foma, SWI Prolog, Prolog network-interpreter, diachronic language descrip-tion. 
Millstream systems are a non-hierarchical model of natural language. We describe an incremental method for building Millstream conﬁgurations while reading a sentence. This method is based on a lexicon associating words and graph transformation rules. 
One of the approaches to statistical machine translation is based on joint probability distributions over some source and target languages. In this work we propose to model the joint probability distribution by stochastic regular bi-languages. Speciﬁcally we introduce the stochastic k-testable in the strict sense bi-languages to represent the joint probability distribution of source and target languages. With this basis we present a reformulation of the GIATI methodology to infer stochastic regular bi-languages for machine translation purposes. 
In this work, we deﬁne a measure aimed at assessing how well a pronunciation model will function when used as a component of a speech recognition system. This measure, pronunciation entropy, fuses information from both the pronunciation model and the language model. We show how to compute this score by effectively composing the output of a phoneme recognizer with a pronunciation dictionary and a language model, and investigate its role as predictor of pronunciation model performance. We present results of this measure for different dictionaries with and without pronunciation variants and counts. 
Implementations of models of morphologically rich languages such as Arabic typically achieve speed and small memory footprint at the cost of abandoning linguistically abstract and elegant representations. We present a solution to modeling rich morphologies that is both fast and based on linguistically rich representations. In our approach, we convert a linguistically complex and abstract implementation of Arabic verbs in ﬁnite-state machinery into a simple precompiled tabular representation. 
We develop an open-source large-scale ﬁnitestate morphological processing toolkit (AraComLex) for Modern Standard Arabic (MSA) distributed under the GPLv3 license.1 The morphological transducer is based on a lexical database speciﬁcally constructed for this purpose. In contrast to previous resources, the database is tuned to MSA, eliminating lexical entries no longer attested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledgebased pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC’s SAMA (Standard Arabic Morphological Analyser). 
Recognition and translation of named entities (NEs) are two current research topics with regard to the proliferation of electronic documents exchanged through the Internet. The need to assimilate these documents through NLP tools has become necessary and interesting. Moreover, the formal or semi-formal modeling of these NEs may intervene in both processes of recognition and translation. Indeed, the modeling task makes more reliable the constitution of linguistic resources, limits the impact of linguistic specificities and facilitates transformations from one representation to another. In this context, we propose an approach of recognition and translation based on a representation model of Arabic NEs and a set of transducers resolving morphological and syntactical phenomena. The representation model is based on the feature structure independent of lexical categories.  Keywords: Representation Model, NE' recognition, NE' translation, Transducer, Local grammar. 
Recent developments in statistical machine translation (SMT), e.g., the availability of efﬁcient implementations of integrated open-source toolkits like Moses, have made it possible to build a prototype system with decent translation quality for any language pair in a few days or even hours. This is so in theory. In practice, doing so requires having a large set of parallel sentence-aligned bilingual texts (a bi-text) for that language pair, which is often unavailable. Large high-quality bi-texts are rare; except for Arabic, Chinese, and some ofﬁcial languages of the European Union (EU), most of the 6,500+ world languages remain resourcepoor from an SMT viewpoint. This number is even more striking if we consider language pairs instead of individual languages, e.g., while Arabic and Chinese are among the most resource-rich languages for SMT, the Arabic-Chinese language pair is quite resource-poor. Moreover, even resourcerich language pairs could be poor in bi-texts for a speciﬁc domain, e.g., biomedical text, conversational text, etc. Due to the increasing volume of EU parliament debates and the ever-growing European legislation, the ofﬁcial languages of the EU are especially privileged from an SMT perspective. While this includes “classic SMT languages” such as English and French (which were already resource-rich), and some important international ones like Spanish and Portuguese, many of the rest have a limited number of speakers and were resource-poor until a few years ago. Thus, becoming an ofﬁcial language of the EU has turned out to be an easy recipe for getting resource-rich in bi-texts quickly. Our aim is to tap the potential of the EU resources so that they can be used by other non-EU languages that are closely related to one or more ofﬁcial languages of the EU.  We propose to use bi-texts for resource-rich language pairs to build better SMT systems for resource-poor pairs by exploiting the similarity between a resource-poor language and a resourcerich one. We are motivated by the observation that related languages tend to have (1) similar word order and syntax, and, more importantly, (2) overlapping vocabulary, e.g., casa (house) is used in both Spanish and Portuguese; they also have (3) similar spelling. This vocabulary overlap means that the resource-rich auxiliary language can be used as a source of translation options for words that cannot be translated with the resources available for the resource-poor language. In actual text, the vocabulary overlap might extend from individual words to short phrases (especially if the resourcerich languages has been transliterated to look like the resource-poor one), which means that translations of whole phrases could potentially be reused between related languages. Moreover, the vocabulary overlap and the similarity in word order can be used to improve the word alignments for the resource-poor language by biasing the word alignment process with additional sentence pairs from the resource-rich language. We take advantage of all these opportunities: (1) we improve the word alignments for the resource-poor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. Speaker’s Bio Dr. Preslav Nakov is a Research Fellow at the National University of Singapore. He received his PhD in Computer Science from the University of California at Berkeley in 2007. Dr. Nakov’s research interestes are in the areas of Web as a corpus, lexical semantics, machine translation, information extraction, and bioinformatics.  
In this paper, we discuss some linguistic phenomena that pose potential problems for multilevel linguistic annotation of parallel corpora in general and speciﬁcally for data encoding with state-of-art multilevel corpus querying tools such as CQP. We describe the strategy we use for integrating the standard hierarchical XML representation used to annotate such phenomena in our aligned bilingual corpus GECCo into a timeline-based format as used in CQP. Thus, our framework supports efﬁcient multilevel representation as well as corpus exploitation and querying of linguistic data of arbitrary complexity. 
In this paper, we propose the creation of a tagged and aligned corpus for the study of a linguistic phenomenon, the translation of proper names. We try to modify the hypothesis according to which proper names cannot be translated and should therefore appear as borrowings in a targetlanguage. To do so, we introduce a parallel multilingual corpus made of eleven versions in ten different languages of a novel. One of these versions, the French one, which appears to be the source-text, undergoes named entity extraction so as to localize more easily the phenomenon we try to study. We focus on the tools used for the creation of our corpus and present some results refuting the idea that proper names are not translatable. 
The paper introduces an ongoing project for the development of a parallel treebank for Italian, English and French annotated in the pure dependency format of the Turin University Treebank, i.e. Parallel–TUT. We hypothesize that the major features of this annotation format can be of some help in addressing the typical issues related to parallel corpora, e.g. alignment at various levels. Therefore, beneﬁtting from the tools previously used for TUT, we applied the TUT format to a multilingual sample set of sentences from the JRCAcquis Multilingual Parallel Corpus and the whole text of the Universal Declaration of Human Rights. 
The paper describes the basic strategies behind the word and semantic level alignment in the Bulgarian-English treebank. The word level alignment has taken into consideration the experience within other NLP groups in the context of the Bulgarian language specific features. The semantic level alignment builds on the word level alignment and is represented in the framework of the Minimal Recursion Semantics. 
The paper presents the first results, for Bulgarian and English, of a multilingual TransVerba project in progress at the NBU Laboratory for Language Technologies. The project explores the possibility to use Bulgarian translation equivalents in parallel corpora and translation memories as a metalanguage in assigning aspectual values to "non-aspect" language equivalents. The resulting subcorpora of Perfective Aspect and Imperfective Aspect units are then quantitatively analysed and concordanced to obtain parameters of aspectual build-up. 
This paper presents the main features of an annotation tool, the Coreference Annotator, which manages bilingual corpora consisting of aligned texts that can be grouped in collections and subcollections according to their topics and discourse. The tool allows the manual annotation of certain linguistic items in the source text and their translation equivalent in the target text, by entering useful information about these items based on their context. 
Web content management systems (WCMSs) are a popular instrument for gathering, navigating and assessing information in environments such as Digital Libraries or e-Learning. Such environments are characterized not only through a critical amount of documents, but also by their domain heterogeneity, relative to format, domain or date of production, and their multilingual character. Methods from Information and Language Technology are the “plug-ins” necessary to any WCMS in order to ensure a proper functionality, given the features mentioned above. Among these “plug-ins”, machine translation (MT) is a key component, which enables translation of meta-data and content either for the user or for other components of the WCMS (i.e. crosslingual retrieval component). However, the MT task is extremely challenging and lacks frequently the availability of adequate training data. In this paper we will present a WCMS including machine translation, explain the related MT challenges, and discuss the employment of corpora as training material, which are manually and automatically parallel aligned. 
Natural language processing of biomedical text benefits from the ability to recognize broad semantic classes, but the number of semantic types is far bigger than is usually treated in newswire text. A method for broad semantic class assignment using lightweight linguistic analysis is described and evaluated using traditional and novel methods. 
The problem of providing effective computer support for clinical coding has been the target of many research efforts. A recently introduced approach, based on statistical data on co-occurrences of words in clinical notes and assigned diagnosis codes, is here developed further and improved upon. The ability of the word space model to detect and appropriately handle the function of negations is demonstrated to be important in accurately correlating words with diagnosis codes, although the data on which the model is trained needs to be sufﬁciently large. Moreover, weighting can be performed in various ways, for instance by giving additional weight to ‘clinically signiﬁcant’ words or by ﬁltering code candidates based on structured patient records data. The results demonstrate the usefulness of both weighting techniques, particularly the latter, yielding 27% exact matches for a general model (across clinic types); 43% and 82% for two domain-speciﬁc models (ear-nosethroat and rheumatology clinics). 
This paper presents an approach for automatic mapping of International Classification of Diseases 10th revision (ICD-10) codes to diagnoses extracted from discharge letters. The proposed algorithm is designed for processing free text documents in Bulgarian language. Diseases are often described in the medical patient records as free text using terminology, phrases and paraphrases which differ significantly from those used in ICD-10 classification. In this way the task of diseases recognition (which practically means e.g. assigning standardized ICD codes to diseases’ names) is an important natural language processing (NLP) challenge. The approach is based on multiclass Support Vector Machines method, where each ICD-10 4 character classification code is considered as single class. The problem is reduced to multiple binary classifiers and classification is done by a max-wins voting strategy. 
Texts containing personal health information reveal enough data for a third party to be able to identify an individual and his health condition. Detection of personal health information in electronic health records is an essential part of record deidentiﬁcation. Performance evaluation in use today focuses on method’s ability to identify whether a word reveals personal health information or not. In this study, we propose and show that the multi-label classiﬁcation measures better serve the ﬁnal goal of the record de-identiﬁcation. 
We describe experiments with building a recognizer for disease names in Bulgarian clinical epicrises, where both the language and the domain are different from those in mainstream research, which has focused on PubMed articles in English. We show that using a general framework such as GATE and an appropriate pragmatic methodology can yield significant speed up of the manual annotation: we achieve F1=0.81 in just three days. This is the first step towards our ultimate goal: named entity normalization with respect to ICD-10. 
The aim of this study is to assemble and deploy various NLP components and resources in order to parse scientific medical text data and evaluate the degree in which these resources contribute to the overall parsing performance. With parsing we limit our efforts to the identification of unrestricted noun phrases with full phrase structure and investigate the effects of using layers of semantic annotations prior to parsing. Scientific medical texts exhibit complex linguistic structure but also regularities that can be captured by pre-processing the texts with specialized semantically-aware tools. Our results show evidence of improved performance while the complexity of parsing is reduced. Parsed scientific texts and inferred syntactic information can be leveraged to improve the accuracy of higher-level tasks such as information extraction and enhance the acquisition of semantic relations and events. 
This paper describes the latest developments in the design of a tool to monitor Patient Discharge Summaries to detected pieces of evidences related to Hospital Acquired Infections. Anonymization, Named Entity detection, Temporal Expressions analysis and Causality detection methods have been developed and evaluated. They are embedded in a tool designed to work in a Hospital Information Workflow. 
This paper reports about ongoing work in automatic identification of temporal markers and segmentation of patient histories into episodes. We discuss the discourse structure of the Anamneses in Bulgarian hospital discharge letters and present experiments with a corpus of 1,375 anonymised discharge letters of patients with endocrine and metabolic diseases. Our IE prototype discovers 32,445 key terms in the corpus, among them more that 7,000 occurrences of drug names and about 7,500 occurrences of diagnoses. The temporal markers occur 8,248 times usually paired with tokens pointing the direction of time “forward” or “backwards”. Temporal markers are identified with precision 84%, recall 57% and f-measure 67.9%. 
Digital museum databases have extremely heterogeneous data structures which require advanced mapping and vocabulary integration for them to beneﬁt from the interoperability enabled by semantic technologies. In addition to establishing ways of extracting and manipulating digitally encoded cultural material, there exists a need to make this material available and accessible to human users in different forms and languages that are available to them. In this paper we describe a method to manage and access museum data by integrating it within a series of interlinked ontological models. The method allows querying and generation of query results in natural language. We report on the results of applying this method from experiments we have been pursuing. 
In recent years, there has been an increasing amount of literature on query classiﬁcation. Click-through information has been shown to be a useful source for improving this task. However, far too little attention has been paid to queries in very speciﬁc domains such as art, culture and history. We propose an approach that exploits topic models built from a domain speciﬁc corpus as a mean to enrich both the query and the categories against which the query need to be classiﬁed. We take an Art Library as the case study and show that topic model enrichment improves over the enrichment via click-through considerably. 
The Language Archive manages one of the largest and most varied sets of natural language data. This data consists of video and audio enriched with annotations. It is available for more than 250 languages, many of which are endangered. Researchers have a need to access this data conveniently and efﬁciently. We provide several browse and search methods to cover this need, which have been developed and expanded over the years. Metadata and content-oriented search methods can be connected for a more focused search. This article aims to provide a complete overview of the available search mechanisms, with a focus on annotation content search, including a benchmark. 
The article presents the results from the project of the thematic Digital Library of Polish and Poland-related Ephemeral Prints from the 16th, 17th and 18th Centuries, intending to preserve the unique multilingual material, make it available for education and extend it with the joint efforts of historians, philologists, librarians and computer scientists. The paper also puts forward the idea of a living digital library, created with a closed set of resources which can form the basis for their further extension, thus turning traditional digital archives into collaboration platforms. 
This paper deals with normalization of language data from Early New High German. We describe an unsupervised, rulebased approach which maps historical wordforms to modern wordforms. Rules are speciﬁed in the form of context-aware rewrite rules that apply to sequences of characters. They are derived from two aligned versions of the Luther bible and weighted according to their frequency. The evaluation shows that our approach (83%–91% exact matches) clearly outperforms the baseline (65%). 
The dictionaries are among the wellknown tools for applications in everyday life, education, sciences, humanities, and human communication. The recent developments of information technologies contribute to the design and creation of new software tools with a wide range of applications, especially for natural language processing. The paper presents an online bilingual dictionary as a technological tool for applications in digital humanities, and describes the structure and content of the bilingual Lexical Database supporting a Bulgarian-Polish online dictionary. It focuses especially on the presentation of verbs, which form the richest from a specific characteristics viewpoint linguistic category in Bulgarian. The main software modules for webpresentation of this digital dictionary are also shortly described. 
The paper describes an approach for semantic annotation of multimedia objects implemented for the purposes of SINUS Project1. Semantic annotations are supported by semantic annotation models based on ontological presentation of knowledge concerning Bulgarian Iconography. The process of semantic annotation includes automated data-lifting procedure and user-directed approach. The paper pays attention to a specific variant of the semantic annotation process directed by the user - application of Language Technologies for semi-automated creation of semantic text annotations (tags) based on analysis of descriptive texts. The ‘ontology-to-text’ approach has been adapted to the needs of the iconographic domain. Initial experiments are established to support the user during the process of manual semantic annotations in the context of SINUS environment. 
This paper presents an overview of principles and problems connected with the preparation of an electronic edition of the largest Old Church Slavonic manuscript, the Codex Suprasliensis, in the context of a project funded by UNESCO. Specifications of the manuscript, its history, and previous paper-based and electronic editions are discussed, together with a strategy for the preparation of a complete digital edition, including newly acquired digital images, electronic text, analysis and commentaries, parallel Greek text, and updated bibliography. In particular, our paper sheds light on automating the morphosyntactic annotation of the text and the difficulties that had to be resolved in this part of the project.  plies complications in its philological interpretation; see also Abicht and Schmidt, 1896). In section 2, this paper presents information about the content, condition, and history of the manuscript. Section 3 reviews efforts in digitization of the manuscript, and section 4 discusses previous electronic editions of the deciphered text, reviewing problems with representation and availability and solutions adopted by the editors. Section 5 gives an overview of the principles of application of morphosyntactic annotation conditioned by the chosen annotation tool and strategy. The conclusion in section 6 explores distinctions among the publication of a text, digitization of a manuscript, development of language corpora, and a true electronic edition of the text, which is the goal of the UNESCO project.  
We introduce the concept of sentiment profiles, representations of emotional content in texts and the SentiProfiler system for creating and visualizing such profiles. We also demonstrate the practical applicability of the system in literary research by describing its use in analyzing novels in the Gothic fiction genre. Our results indicate that the system is able to support literary research by providing valuable insights into the emotional content of Gothic novels. 
This paper describes the way in which personal relationships between main characters in 19th century Swedish prose fiction can be identified using information guided by named entities, provided by a entity recognition system adapted to the 19th century Swedish language characteristics. Interpersonal relation extraction is based on the context between two relevant, identified person entities. The relationships extraction process also utilizes the content of on-line available lexical semantic resources (suitable vocabularies) and fairly standard context matching methods that provide a basic mechanism for identifying a wealth of interpersonal relations. Such relations can hopefully aid the reader of a 19thcentury Swedish literary work to better understand its content and plot, and get a bird’s eye view on the landscape of the core story. 
In this paper we present the results of a study investigating the diachronic changes of four stylistic features: average sentence length, Automated Readability Index, lexical density and lexical richness in 20th century written English language. All experiments were conducted on the largest existing diachronic corpora of British and American English – the Brown ‘family’ corpora, employing NLP techniques for automatic extraction of the features. Additionally, we compare the trends of changes between the two English varieties and make suggestions for future studies of diachronic language change. 
In the AVATecH project the Max-Planck Institute for Psycholinguistics (MPI) and the Fraunhofer institutes HHI and IAIS aim to significantly speed up the process of creating annotations of audio-visual data for humanities research. For this we integrate state-of-theart audio and video pattern recognition algorithms into the widely used ELAN annotation tool. To address the problem of heterogeneous annotation tasks and recordings we provide modular components extended by adaptation and feedback mechanisms to achieve competitive annotation quality within significantly less annotation time. Currently we are designing a large-scale end-user evaluation of the project. 
The amount of digitized legacy documents has been rising dramatically over the last years due mainly to the increasing number of on-line digital libraries publishing this kind of documents. The vast majority of them remain waiting to be transcribed into a textual electronic format (such as ASCII or PDF) that would provide historians and other researchers new ways of indexing, consulting and querying them. In this work, the state-of-the-art Handwritten Text Recognition techniques are applied for the automatic transcription of these historical documents. We report results for several ancient documents. 
In order to improve OCR quality in texts originally typeset in Gothic script, we have built an automated correction system which is highly specialized for the given text. Our approach includes external dictionary resources as well as information derived from the text itself. The focus lies on testing and improving different methods for classifying words as correct or erroneous. Also, different techniques are applied to find and rate correction candidates. In addition, we are working on a web application that enables users to read and edit the digitized text online. 
The Knowledge Base Population (KBP) task, being run for the past 3 years by the U.S. National Institute of Standards and Technology, is the latest in a series of multi-site evaluations of information extraction, following in the tradition of MUC and ACE. We examine the structure of KBP, emphasizing the basic shift from sentence-by-sentence and document-by-document evaluation to corpus-based extraction and the challenges it raises for cross-sentence and cross-document processing. We consider the problems raised by the limited amount and incompleteness of the training data, and how this has been (partly) addressed through such methods as semi-supervised learning and distant supervision. We describe some of the optional tasks which have been included – rapid task adaptation (last year), temporal analysis (this year), cross-lingual extraction (planned for next year) – and others which have been suggested. 
State of the art semi-supervised entity set expansion algorithms produce noisy results, which need to be refined manually. Sets expanded for intended fine-grained concepts are especially noisy because these concepts are not well represented by the limited number of seeds. Such sets are usually incorrectly expanded to contain elements of a more general concept. We show that fine-grained control is necessary for refining such sets and propose an algorithm which uses both positive and negative user feedback for iterative refinement. Experimental results show that it improves the quality of fine-grained sets significantly. 
In this paper we consider a new approach for domain-specific opinion word extraction in Russian. We suppose that some domains have similar sentiment lexicons and utilize this fact to build an opinion word vocabulary for a group of domains. We train our model in movie domain and then utilize it to book and game domains. Obtained word list quality is comparable with quality of initial domain list. 
In this paper, we investigate the role of predicates in opinion holder extraction. We will examine the shape of these predicates, investigate what relationship they bear towards opinion holders, determine what resources are potentially useful for acquiring them, and point out limitations of an opinion holder extraction system based on these predicates. For this study, we will carry out an evaluation on a corpus annotated with opinion holders. Our insights are, in particular, important for situations in which no labeled training data are available and only rule-based methods can be applied. 
The application of linguistic patterns and rules are one of the main approaches for Information Extraction as well as for highquality ontology population. However, the lack of ﬂexibility of the linguistic patterns often causes low coverage. This paper presents a weakly-supervised rule-based approach for Relation Extraction which performs partial dependency parsing in order to simplify the linguistic structure of a sentence. This simpliﬁcation allows us to apply generic semantic extraction rules, obtained with a distantsupervision strategy which takes advantage of semi-structured resources. The rules are added to a partial dependency grammar, which is compiled into a parser capable of extracting instances of the desired relations. Experiments in different Spanish and Portuguese corpora show that this method maintains the highprecision values of rule-based approaches while improves the recall of these systems. 
This paper discusses a system for automatic clustering of urban-legend texts. Urban legend (UL) is a short story set in the present day, believed by its tellers to be true and spreading spontaneously from person to person. A corpus of Polish UL texts was collected from message boards and blogs. Each text was manually assigned to one story type. The aim of the presented system is to reconstruct the manual grouping of texts. It turned out that automatic clustering of UL texts is feasible but it requires techniques different from the ones used for clustering e.g. news articles. 
The traditional method to evaluate a knowledge extraction system is to measure precision and recall. But this method only partially measures the quality of a knowledge base (KB) as it cannot predict whether a KB is useful or not. One of the ways in which a KB can be useful is if it is able to deduce implicit information from text which standard information extraction techniques cannot extract. We propose a novel, simple evaluation framework called “Machine Reading between the Lines” (MRbtL) which measures the usefulness of extracted KBs by determining how much they can help improve a relation extraction system. In our experiments, we compare two KBs which both have high precision and recall according to annotators who evaluate the knowledge in the KBs independently from any application or context. But, we show that one outperforms the other in terms of MRbtL experiments, as it can accurately deduce more new facts from the output of a relation extractor more accurately. In short, one extracted KB can read between the lines to identify extra information, whereas the other one cannot. 
This paper presents a tool for extracting and normalizing temporal expressions in SMS messages in order to automatically ﬁll in an electronic calendar. The extraction process is based on a library of ﬁnite-state transducers that identify temporal structures and annotate the components needed for the time normalization task. An initial evaluation puts recall at 0.659 and precision at 0.795. 
The application of machine learning techniques to classify text documents into sentiment categories has become an increasingly popular area of research. These techniques rely upon the availability of labelled data, but in certain circumstances the availability of pre-classiﬁed documents may be limited. Limited labelled data can impact the performance of the model induced from it. There are a number of strategies which can compensate for the lack of labelled data, however these techniques may be suboptimal if the initial labelled data selection does not contain a sufﬁcient cross section of the total document collection. This paper proposes a variant of self-training as a strategy to this problem. The proposed technique uses a high precision classiﬁer (linguistic rules) to inﬂuence the selection of training candidates which are labelled by the base learner in an iterative self-training process. The linguistic knowledge encoded in the high precision classiﬁer corrects highconﬁdence errors made by the base classiﬁer in a preprocessing step. This step is followed by a standard self training cycle. The technique was evaluated in three domains: user generated reviews for (1) airline meals, (2) university professors and (3) music against: (1) constrained learning strategies (voting and veto), (2) induction and (3) standard self-training. The evaluation measure was by estimated F-Measure. The results demonstrate clear advantage for the proposed method for classifying text documents into sentiment categories in domains where there is limited amounts of training data.  
In the ﬁeld of subjectivity detection, algorithms automatically classify pieces of text into fact or opinion. Many different approaches have been successfully evaluated on English or Chinese texts. Nevertheless the assumption that these algorithms equally perform on all other languages cannot be veriﬁed yet. It is our intention to encourage more research in other languages, making a start with German. Therefore, this work introduces a German corpus for subjectivity detection on German news articles. We carry out this study in which we choose a number of state of the art subjectivity detection approaches and implement them. Finally we show and compare these algorithms’ performances and give advice on how to use and extend the introduced dataset. 
Most relation extraction methods, especially in the domain of biology, rely on machine learning methods to classify a cooccurring pair of entities in a sentence to be related or not. Such an approach requires a training corpus, which involves expert annotation and is tedious, timeconsuming, and expensive. We overcome this problem by the use of existing knowledge in structured databases to automatically generate a training corpus for protein-protein interactions. An extensive evaluation of different instance selection strategies is performed to maximize robustness on this presumably noisy resource. Successful strategies to consistently improve performance include a majority voting ensemble of classiﬁers trained on subsets of the training corpus and the use of knowledge bases consisting of proven non-interactions. Our best conﬁgured model built without manually annotated data shows very competitive results on several publicly available benchmark corpora. 
This paper describes a simple method to achieve logical constraints on words for topic models based on a recently developed topic modeling framework with Dirichlet forest priors (LDA-DF). Logical constraints mean logical expressions of pairwise constraints, Must-links and Cannot-Links, used in the literature of constrained clustering. Our method can not only cover the original constraints of the existing work, but also allow us easily to add new customized constraints. We discuss the validity of our method by deﬁning its asymptotic behaviors. We verify the effectiveness of our method with comparative studies on a synthetic corpus and interactive topic analysis on a real corpus. 
Co-training, as a semi-supervised learning method, has been recently applied to semantic role labeling to reduce the need for costly annotated data using unannotated data. A main concern in co-training is how to split the problem into multiple views to derive learning features, so that they can effectively train each other. We investigate various feature splits based on two SRL views, constituency and dependency, with different variations of the algorithm. Balancing the feature split in terms of the performance of the underlying classifiers showed to be useful. Also, co-training with a common training set performed better than when separate training sets are used for co-trained classifiers. 
This paper gives two contributions to dependency parsing in Korean. First, we build a Korean dependency Treebank from an existing constituent Treebank. For a morphologically rich language like Korean, dependency parsing shows some advantages over constituent parsing. Since there is not much training data available, we automatically generate dependency trees by applying head-percolation rules and heuristics to the constituent trees. Second, we show how to extract useful features for dependency parsing from rich morphology in Korean. Once we build the dependency Treebank, any statistical parsing approach can be applied. The challenging part is how to extract features from tokens consisting of multiple morphemes. We suggest a way of selecting important morphemes and use only these as features to avoid sparsity. Our parsing approach is evaluated on three different genres using both gold-standard and automatic morphological analysis. We also test the impact of ﬁne vs. coarse-grained morphologies on dependency parsing. With automatic morphological analysis, we achieve labeled attachment scores of 80%+. To the best of our knowledge, this is the ﬁrst time that Korean dependency parsing has been evaluated on labeled edges with such a large variety of data. 
This article evaluates the integration of data extracted from a French syntactic lexicon, the Lexicon-Grammar (Gross, 1994), into a probabilistic parser. We show that by applying clustering methods on verbs of the French Treebank (Abeille´ et al., 2003), we obtain accurate performances on French with a parser based on a Probabilistic Context-Free Grammar (Petrov et al., 2006). 
This paper presents a set of experiments performed on parsing Basque, a morphologically rich and agglutinative language, studying the effect of using the morphological analyzer for Basque together with the morphological disambiguation module, in contrast to using the gold standard tags taken from the treebank. The objective is to obtain a first estimate of the effect of errors in morphological analysis and disambiguation on the parsers. We tested two freely available and state of the art dependency parser generators, MaltParser, and MST, which represent the two dominant approaches in data-driven dependency parsing. 
 S  Recent advances in parsing technology have made treebank parsing with discontinuous constituents possible, with parser output of competitive quality (Kallmeyer and Maier, 2010). We apply Data-Oriented Parsing (DOP) to a grammar formalism that allows for discontinuous trees (LCFRS). Decisions during parsing are conditioned on all possible fragments, resulting in improved performance. Despite the fact that both DOP and discontinuity present formidable challenges in terms of computational complexity, the model is reasonably efﬁcient, and surpasses the state of the art in discontinuous parsing. 
In this paper, we investigated the impact of extracting different types of multiword expressions (MWEs) in improving the accuracy of a data-driven dependency parser for a morphologically rich language (Turkish). We showed that in the training stage, the uniﬁcation of MWEs of a certain type, namely compound verb and noun formations, has a negative effect on parsing accuracy by increasing the lexical sparsity. Our results gave a statistically signiﬁcant improvement by using a variant of the treebank excluding this MWE type in the training stage. Our extrinsic evaluation of an ideal MWE recognizer (for only extracting MWEs of type named entities, duplications, numbers, dates and some predeﬁned list of compound prepositions) showed that the preprocessing of the test data would improve the labeled parsing accuracy by 1.5%. 
Problems for parsing morphologically rich languages are, amongst others, caused by the higher variability in structure due to less rigid word order constraints and by the higher number of different lexical forms. Both properties can result in sparse data problems for statistical parsing. We present a simple approach for addressing these issues. Our approach makes use of self-training on instances selected with regard to their similarity to the annotated data. Our similarity measure is based on the perplexity of part-of-speech trigrams of new instances measured against the annotated training data. Preliminary results show that our method outperforms a self-training setting where instances are simply selected by order of occurrence in the corpus and argue that selftraining is a cheap and effective method for improving parsing accuracy for morphologically rich languages. 
Pragmatics —the aspects of text that signal interpersonal and situational information, complementing semantics— has been almost totally ignored in Natural Language Processing. But in the past five to eight years there has been a surge of research on the general topic of ‘opinion’, also called ‘sentiment’. Generally, research focuses on the determining the author’s opinion/sentiment about some topic within a given fragment of text. Since opinions may differ, it is granted that the author’s opinion is ‘subjective’, and the effectiveness of an opiniondetermination system is measured by comparing against a gold-standard set of human annotations. But what does ‘subjectivity’ actually mean? What are ‘opinion’ and ‘sentiment’? Lately, researchers are also starting to talk about ‘affect’, and even ‘emotion’. What are these notions, and how do they differ from one another? Unfortunately, a survey of the research done to date shows a disturbing lack of clarity on these questions. Very few papers bother to define their terms, but simply take a set of valences such as Good–Neutral–Bad to be sufficient. More recent work acknowledges the need to specify what the opinion actually applies to, and attempts also to determine the theme. Lately, several identify the holder of the opinion. Some even try to estimate the strength of the expressed opinion. The trouble is, the same aspect of the same object can be considered Good by one person and Bad by another, and we can often understand both their points of view. There is much more to opinion/sentiment than simply matching words and phrases that attach to the theme, and computing a polarity score. People give reasons why they like or dislike something, and these reasons pertain to their goals and plans in the case of  opinions) or their deeper emotional states (in the case of affect). In this talk I outline a model of sentiment/opinion and of affect, and show that they appear in text in a fairly structured way, with various components. I show how proper understanding requires the reader to build some kind of person profile of the author, and claim that for systems to do adequate understanding of sentiments, opinions, and affects, they will need to do so as well. This is not a trivial challenge, and it opens the door to a whole new line of research with many fascinating and practical aspects. About The Speaker Dr. Hovy currently holds several positions: Director of the Natural Language Group at Information Sciences Institute (ISI) of the University of Southern California. Deputy Director of the Intelligent Systems Division of ISI. Research Associate Professor of Computer Science at USC. Director of the Center for Knowledge Integration and Discovery (CKID). Director of Research for the Digital Government Research Center (DGRC). Regular High-Level Visiting Scientist, International Guest Academic Talents (IGAT) Program for the Development of University Disciplines in China (111 Program), Jan 2008–Dec 2012. Advisory Professor at the Beijing University of Posts and Telecommunications, Beijing, China. Concurrent Professor at the University of Shenyang, China, Oct 2008–Sep 2011.  
The body of content available on Twitter undoubtedly contains a diverse range of political insight and commentary. But, to what extent is this representative of an electorate? Can we model political sentiment effectively enough to capture the voting intentions of a nation during an election capaign? We use the recent Irish General Election as a case study for investigating the potential to model political sentiment through mining of social media. Our approach combines sentiment analysis using supervised learning and volume-based measures. We evaluate against the conventional election polls and the ﬁnal election result. We ﬁnd that social analytics using both volume-based measures and sentiment analysis are predictive and we make a number of observations related to the task of monitoring public sentiment during an election campaign, including examining a variety of sample sizes, time periods as well as methods for qualitatively exploring the underlying content. 
Most recent studies on emotion analysis and detection focus on how writers express their emotions through textual information. In this paper, we model emotion generation on the Plurk microblogging platform from both writer and reader perspectives. Support Vector Machine (SVM)-based classifiers are used for emotion prediction. To better model emotion generation on such a social network, three types of non-linguistic features are used: social relation, user behavior, and relevance degree, along with textual features. We found that each of the non-linguistic features can be combined with linguistic features to achieve higher performance. In fact, the combination of linguistic, social, and behavioral features performs the best. 
The automatic analysis of emotional content of text has become pervasive and has been applied in many fields of research. The work reported in this paper is in particular interested in modeling antisocial behavior and the emotional states that define it. We introduce the antisocial behavior detection (ASBD) model for portraying the emotions pertaining to antisocial behavior. In addition to describing negative affective states, our model uses the concepts of action tendencies and evidences in order to predict possible acts of antisocial behavior based on input texts. We outline a design for an antisocial behavior detection system based on the ASBD model. 
This paper concentrates on pairing opinion analysis with argument extraction in order to identify why opinions about a certain feature are positive or negative. The objective is to have a better grasp at the underlying elements that support the analysis. In a second stage, given customer recommendations, the goal is to identify the preferences or priorities of customers, e.g. fares over welcome attitude. This induces customers value systems. Finally, we give elements of the implementation based on the <TextCoop> platform, dedicated to discourse analysis. 
In a world in which web users are continuously blasted by ads and often compelled to deal with user-unfriendly interfaces, we sometimes feel like we want to evade from the sensory overload of standard web pages and take refuge in a safe web corner, in which contents and design are in harmony with our current frame of mind. Sentic Corner is an intelligent user interface that dynamically collects audio, video, images and text related to the user’s current feelings and activities as an interconnected knowledge base, which is browsable through a multi-faceted classiﬁcation website. 
This paper explores the ability of senses aligned across languages to carry coherent subjectivity information. We start out with a manual annotation study, and then seek to create an automatic framework to determine subjectivity labeling for unseen senses. We identify two methods that are able to incorporate subjectivity information originating from different languages, namely co-training and multilingual vector spaces, and show that for this task the latter method is better suited and obtains superior results.  cea, 2006; Esuli and Sebastiani, 2006a) and lexical substitution (Su and Markert, 2010). While research in English has underlined that the most robust subjectivity delineation occurs at sense and not at word level (Wiebe and Mihalcea, 2006), we are not aware of this consideration impacting research in other languages. For this reason, in this work we seek to analyze how subjectivity is maintained across sense aligned resources, and identify ways in which subjectivity at sense level may be employed in a multilingual framework to provide a strengthened automatic senselevel classiﬁcation. 2 Related Work  
A method for multilingual review classiﬁcation is described. In this classiﬁcation task, machine translation techniques are used to remove language gaps in the dataset, but many translation errors occur as a side-eﬀect. These errors cause a decrease in the review classiﬁcation performance. To resolve this problem, we introduce a sentiment-oriented sentence ﬁltering module to the process of multilingual review classiﬁcation. Experimental results showed that the proposed method achieved 81.7% classiﬁcation accuracy for the evaluation data. 
The present task collects different statistics of emotions based on the combinations of general variables (intensity, timing and longevity) and physiological variables (psycho-physiological arousals) from the situational statements of the ISEAR (International Survey on Emotion Antecedents and Reactions) dataset. The individual as well as combinational roles of different variables are analyzed. Some interesting observations and insights are found with respect to emotions. The statements of similar emotions are clustered according to different combinations of the variables. Each of the statements of a cluster is passed through two types of emotion tagging systems, a lexicon based baseline system followed by a supervised system. Due to the difficulty of incorporating knowledge regarding physiological variables, the supervised system only considers the roles of general variables from textual statements. The roles of the general variables are played by intensifiers, modifiers and explicitly specified temporal and causal discourse markers. The evaluation indicates that the supervised system based on general variables produces satisfactory results in identifying emotions. 
Online communication is one of the key value propositions of mobile devices. While a variety of instant messaging clients offer users the ability to communicate with other users in real-time, the user experience remains dominated by a basic exchange of textual content. When compared to face-to-face communication, this experience is significantly poorer. In our proposed solution, we seek to enhance the chat experience by using an intelligent adaptive user interface that exploits semantics and sentics, that is the cognitive and affective information, associated with the ongoing communication. In particular, our approach leverages sentiment analysis techniques to process communication content and context and, hence, enable the interface to be adaptive in order to offer users a richer and more immersive chat experience. 
The information overload experienced by people who use online services and read usergenerated content (e.g. product reviews and ratings) to make their decisions has led to the development of the so-called recommender systems. We address the problem of the large increase in the user-generated reviews, which are added to each day and consequently make it difficult for the user to obtain a clear picture of the quality of the facility in which they are interested. In this paper, we describe the TWIN (“Tell me What I Need”) personality-based recommender system, the aim of which is to select for the user reviews which have been written by like-minded individuals. We focus in particular on the task of User Profile construction. We apply the system in the travelling domain, to suggest hotels from the TripAdvisor1 site by filtering out reviews produced by people with similar, or like-minded views, to those of the user. In order to establish the similarity between people we construct a user profile by modelling the user’s personality (according to the Big Five model) based on linguistic cues collected from the user-generated text. 
The repetition of names of persons, places, ideas and events, is used sometimes for emphasis. The same is true of the repetition of affect words - repeated preferentially to show negative/positive sentiment. During an election campaign, this repetition may have a bearing on the electability of politicians and on the reputation of political parties. News media covering an election may be involved in endorsing political parties, attempting to set aspects of election agenda, and may have gender bias. Using Rocksteady, an affect analysis system, we have analyzed samples of news published nationally and regionally by Irish media between 21st December 2010 and 20th Feb. 2011 - in the run up to the Irish General Election on 25th February 2011. Our results show that a diachronic study of the coverage, based on named-entity dictionary crafted from electoral lists and with key ﬁnancial and economic terms added, supplemented by a General Inquirer type dictionary of affect, helped us to distinguish between the winners (two opposition parties that have subsequently formed a coalition government) from the loser (the incumbent party). 
This paper presents the use of Maximum Entropy technique for Chinese sentiment analysis. Berger, Vincent and Stephen (1996) prove that Maximum Entropy is a technique that is effective in a number of natural language processing applications. In this paper, Maximum Entropy classification is used to estimating the polarity of given comments of from electronic product. These messages are classified into either positive or negative. Apart from presenting the results obtained via Maximum Entropy technique, we also analyze the feature selection and pre-processing of the comments for training and testing purpose. 
Two typical approaches to sentiment analysis are lexicon look up and machine learning. Even though recent studies have shown that machine learning approaches in general outperform the lexicon look up approaches, completely ignoring the knowledge encoded in sentiment lexicons may not be optimal. We present an alternative method that incorporates sentiment lexicons as prior knowledge with machine learning approaches such as SVM to improve the accuracy of sentiment analysis. This paper also describes a method to automatically generate domain speciﬁc sentiment lexicons for this learning purpose. Our experiment results show that the domain speciﬁc lexicons we constructed lead to a signiﬁcant accuracy improvement for our sentiment analysis task. 
Sentiment mining and classiﬁcation plays an important role in predicting what people think about products, places, etc. In this piece of work, using basic NLP Techniques like NGram, POS-Tagged NGram we classify movie and product reviews broadly into two polarities: Positive and Negative. We propose a model to address the problem of determining whether a review is positive or negative, we experiment and use several machine learning algorithms Naive Bayes (NB), Multi-Layer Perceptron (MLP), Support Vector Machine (SVM) to have a comparative study of the performance of the method we devised in this work. Along with this we also did negation handling and observed improvements in classiﬁcation. The algorithm we proposed achieved an average accuracy of 78.32% on movie and 70.06% on multi-category dataset. In this paper we focus on the collective study of Ngram and POS tagged information available in the reviews . 
This paper explores how to automatically generate cross-language links between resources in large document collections. The paper presents new methods for Cross-Lingual Link Discovery (CLLD) based on Explicit Semantic Analysis (ESA). The methods are applicable to any multilingual document collection. In this report, we present their comparative study on the Wikipedia corpus and provide new insights into the evaluation of link discovery systems. In particular, we measure the agreement of human annotators in linking articles in different language versions of Wikipedia, and compare it to the results achieved by the presented methods. 
Indian languages are known to have a large speaker base, yet some of these languages have minimal or non-efﬁcient linguistic resources. For example, Kannada is relatively resource-poor compared to Malayalam, Tamil and Telugu, which in-turn are relatively poor compared to Hindi. Many Indian language pairs exhibit high similarities in morphology and syntactic behaviour e.g. Kannada is highly similar to Telugu. In this paper, we show how to build a cross-language part-of-speech tagger for Kannada exploiting the resources of Telugu. We also build large corpora and a morphological analyser (including lemmatisation) for Kannada. Our experiments reveal that a cross-language taggers are as efﬁcient as mono-lingual taggers. We aim to extend our work to other Indian languages. Our tools are efﬁcient and signiﬁcantly faster than the existing monolingual tools. 
With the thriving of the Internet, web users today have access to resources around the world in more than 200 different languages. How to effectively manage multilingual web search results has emerged as an essential problem. In this paper, we introduce the ongoing work of leveraging a CrossLingual Topic Model (CLTM) to integrate the multilingual search results. The CLTM detects the underlying topics of different language results and uses the topic distribution of each result to cluster them into topic-based classes. In CLTM, we unify distributions in topic level by direct translation, thus distinguishing from other multilingual topic models, which mainly concern the parallelism at document or sentence level (Mimno 2009; Ni, 2009). Experimental results suggest that our CLTM clustering method is effective and outperforms the 6 compared clustering approaches. 
Cross-language information retrieval is difﬁcult for languages with few processing tools or resources such as Urdu. An easy way of translating content words is provided by Google Translate, but due to lexicon limitations named entities (NEs) are transliterated letter by letter. The resulting NEs errors (zynydyny zdn for Zinedine Zidane) hurts retrieval. We propose to replace English non-words in the translation output. First, we determine phonetically similar English words with the Soundex algorithm. Then, we choose among them by a modiﬁed Levenshtein distance that models correct transliteration patterns. This strategy yields an improvement of 4% MAP (from 41.2 to 45.1, monolingual 51.4) on the FIRE-2010 dataset. 
While adopting the contextualized hidden Markov model (CHMM) framework for unsupervised Russian POS tagging, we investigate the possibility of utilizing the left, right, and unambiguous context in the CHMM framework. We propose a backoff smoothing method that incorporates all three types of context into the transition probability estimation during the expectation-maximization process. The resulting model with this new method achieves overall and disambiguation accuracies comparable to a CHMM using the classic backoff smoothing method for HMM-based POS tagging from (Thede and Harper, 1999). 
Named Entity Recognition and Classiﬁcation (NERC) is a well-studied NLP task which is typically approached using machine learning algorithms that rely on training data whose creation usually is expensive. The high costs result in the lack of NERC training data for many languages. An approach to create a multilingual NE corpus was presented in Wentland et al. (2008). The resulting resource called HeiNER describes a valuable number of NEs but does not include their types. We present a bootstrap approach based on Wikipedia’s category system to classify the NEs contained in HeiNER that is able to classify more than two million named entities to improve the resource’s quality. 
Back-transliteration based Input Method Editors are very popular for Indian Languages. In this paper we evaluate two such Indic language systems to help understand the challenge of designing a back-transliteration based IME. Through a detailed error-analysis of Hindi, Bangla and Telugu data, we study the role of phonological features of Indian scripts that are reflected as variations and ambiguity in the transliteration. The impact of word-origin on back-transliteration is discussed in the context of codeswitching. We also explore the role of word-level context to help overcome some of these challenges. 
The most popular type of input method in Japan is kana-kanji conversion, conversion from a string of kana to a mixed kanjikana string. However there is no study using discriminative methods like structured SVMs for kana-kanji conversion. One of the reasons is that learning a discriminative model from a large data set is often intractable. However, due to progress of recent researches, large scale learning of discriminative models become feasible in these days. In the present paper, we investigate whether discriminative methods such as structured SVMs can improve the accuracy of kana-kanji conversion. To the best of our knowledge, this is the ﬁrst study comparing a generative model and a discriminative model for kana-kanji conversion. An experiment revealed that a discriminative method can improve the performance by approximately 3%. 
Reducing size of dictionary and language model is critical when applying them to real world applications including machine translation and input method editors (IME). Especially for IME, we have to drastically compress them without sacriﬁcing lookup speed, since IMEs need to be executed on local computers. This paper presents novel lossless compression algorithms for both dictionary and language model based on succinct data structures. Proposed two data structures are used in our product “Google Japanese Input”1, and its open-source version “Mozc”2. 
We live in the age of touch screen gadgets. The future trends also show promising growth for them. Currently available input systems developed for standard PCs have room for improvement in efficiency, visibility and usability etc. particularly for Perso-Arabic scripts e.g., Urdu. In addition, small touch screen devices expose users to health hazards. We put forth Ergonomics in prime focus to reduce potential health hazards. We proposed distinct touch-screen keypads for different devices that are practically applicable for fast, correct and easy composing. We computed the estimated input time and tapcounts using automated procedure to compare contemporary keypads with our proposed keypads. Our experiments on a considerably large Urdu corpus reveal results of ample significance. Our optimization technique for arrangement of alphabets and unique interface for data input is extendable and equally applicable to other natural languages. 
We present an approach to help editors of Japanese on a language learning SNS correct learners’ sentences written in Roman characters by converting them into kana. Our system detects foreign words and converts only Japanese words even if they contain spelling errors. Experimental results show that our system achieves about 10 points higher conversion accuracy than traditional input method (IM). Error analysis reveals some tendencies of the errors speciﬁc to language learners. 
In this paper we describe our approach to building a French text input system, in which no explicit typing of accents is required. This makes typing of French available to a wider range of keyboards and to occasional writers of the language. Our method is built on the noisy-channel model, and achieves 99.8% characterlevel accuracy on the test data consisting of formal and casual writing styles. This system is part of a larger project of making text input easier for multiple languages, including those that have traditionally been the target of input methods (such as Chinese and Japanese) as well as phonetically based non-Roman script languages (such as Greek, Russian, and Indic languages). A demo of this system including these languages will be shown at the workshop. 
We propose a novel phrase extraction system to generate a phrase dictionary for predictive input methods from a large corpus. This system extracts phrases after counting n-grams so that it can be easily maintained, tuned, and re-executed independently. We developed a rule-based ﬁlter based on part-of-speech (POS) patterns to extract Japanese phrases. Our experiment shows usefulness of our system, which achieved a precision of 0.90 and a recall of 0.81, outperforming the N-gram baseline by a large margin. 
This work proposes a novel metric, Maximally Amortized Cost (MAC), for cost evaluations of error correction of predictive Chinese input methods (IMs). With a series of real-time simulation, user correction behaviors are analyzed by estimating generalized backward compatibility of adaptive Chinese IMs. Comparisons between three IMs by using MAC with different context lengths report empirical factors of context length for improving predictive IMs. The error-tolerance level—Futile Effort, Beneficial Effort and Utility—of adaptive IMs is also proposed and analyzed. 
Language resources are really much required for understanding and modeling the language in the present approaches. The language that has a rich language resource gains a big benefit in making a big advance in language processing. On the other hand, the less resource language is struggling with preparing a large enough language resource such as raw text or annotated corpora. It is a labor intensive and time consuming task. Moreover, computerization of the text is another non-trivial effort. There needs a supportive computing environment in inputting, encoding, retrieving, analysis, etc.. Learning from the rich resource languages, we gradually collecting the resource and preparing the necessary tools. Through many efforts in the recent years, we can see some significant outcomes from PAN localization project (2004-2007, 2007-2101, http://www.panl10n.net/), ADD (2006-2010, http://www.tcllab.org/), Asian WordNet (http://asianwordnet.org/), Hindi WordNet (http://www.cfilt.iitb.ac.in/wordnet/webhwn/), BEST (since 2009, Thai Word Segmentation Software Contest, http://thailang.nectec.or.th/ best/) and many NLP summer schools. The activities gain a big potential in leveraging the NLP tools development and research personnel development. It results in a big growth of Asian language resource development and research. With the spirit of sharing on social networking, the resources can efficiently be developed to a satisfied amount in a reasonable time scale. Asian WordNet is an example of developing a set of 13 languages of Wordnet connected via Princeton WordNet. Thai WordNet is open for online collaborative development. About 70K synsets and 80K words of Thai WordNet are available online. ThaiLao conversion is an approach to exhibit the advantage in utilization of language similarity to increase the other language resource. Lao WordNet is created by converting from Thai WordNet by using the phoneme transfer approach. Taking the advantage of language similarity, the language corpus can be obtained by a quick conversion rule. In this case, the study of direct transfer is much more efficient than creating from the scratch. Currently, most of the above mentioned results are open to public for at least research purpose. However, more and more language resources are still needed to improve the language processing. The possible of online collaborative development and sharing is a key factor in the language resource development. 
This document outlines the use of Language Tool for a Tagalog Grammar Checker. Language Tool is an open-source rule-based engine that offers grammar and style checking functionalities. The details of the various linguistic resource requirements of Language Tool for the Tagalog language are outlined and discussed. These are the tagger dictionary and the rule file that use the notation of Language Tool. The expressive power of Language Tool’s notation is analyzed and checked if Tagalog linguistic phenomena are captured or not. The system was tested using a collection of sentences and these are the results: 91% precision rate, 51% recall rate, 83% accuracy rate. 
The Bantay-Wika (Language Watch) project was started in 1994 by the University of the Philippines (UP) - Sentro ng Wikang Filipino1 (SWF) in order to track for long periods of time how the Philippine national language is being used and how it develops, particularly in the Philippine media. The ﬁrst phase of this project, from 1994 to 2004, involved the manual collection and tallying of frequency counts for all the words in eleven major Philippine tablods. With increasing online presence of Philippine news organizations, the project was revived in March 2010, with UP-SWF partnering with UP - Digital Signal Processing (DSP) laboratory. The project objectives were also re-drafted to include the development of software that would automate the process of downloading of Filipino news articles. In this paper, we further detail the goals and the history and of the Bantay-Wika project, its accomplishments and plans for future work. The project ultimately endeavors to build a computational model for language development that can guide language policy makers in a multi-lingual country such as the Philippines, in drafting policies that can effectively promote the use and development of their national language. 
In this paper, we present our on-going grammar development effort towards a linguistically precise and broad coverage grammar for Mandarin Chinese in the framework of HPSG. The use of LinGO Grammar Matrix facilitates the quick start of the development. We propose a series of linguistic treatments for a list of interesting phenomena. The analyses are largely compatible with the HPSG framework. In addition, the grammar also composes semantic representations in Minimum Recursion Semantics. Preliminary tests of the grammar on a phenomenon-oriented test suite show encouraging precision and coverage. 
This paper describes an error detection mechanism which helps in validation of dependency treebank annotation. Consistency in treebank annotation is a must for making data as errorfree as possible and for assuring the usefulness of treebank. This work is aimed at ensuring this consistency and to make the task of validation cost effective by detecting major errors induced during completely manual annotation. We evaluated our system on the Hindi dependency treebank which is currently under development. We could detect 76.63% of errors at dependency level. Results show that our system performs well even when the training data is low. 
This paper attempts to report on developing a WordNet for Urdu on the basis of Hindi WordNet. The resource currently contains about 50000 unique words organized in 28967 synsets. The paper also discusses the problems encountered along the way of transliteration from Hindi WordNet and manual cleaning. It concludes with the planned future work. 
Linguistic code switching (LCS) occurs when speakers mix multiple languages in the same speech utterance. We ﬁnd LCS pervasively in bilingual communities. LCS poses a serious challenge to Natural Language and Speech Processing. With the ubiquity of informal genres online, LCS is emerging as a very widespread phenomenon. This paper presents a ﬁrst attempt at collecting and annotating a large repository of LCS data. We target Hindi English (Hinglish) LCS. We investigate the feasibility of leveraging crowd sourcing as a means for annotating the data on the word level. This paper brieﬂy explains the setup of the experiment and data collection. It also presents statistics representing agreements among annotators over different possible categories of Hinglish words and analyzes the conﬁdence with which a code switched word can be annotated in the correct category by humans. 
The Linguist’s Assistant (LA) is a practical computational paradigm for describing languages. In this paper we describe how to use LA with naturally occurring texts that exemplify interesting target-language linguistic phenomena. We will describe how such texts can be semantically analyzed using a convenient semi-automatic document authoring interface, in effect adding them to LA’s standard semantic-based elicitation corpus. We then exemplify the language description process using a phenomenon that is prevalent in our research: alienable vs. inalienable nominal possession. 
An automated or semi-automated annotation is a practical solution towards largescale corpus construction. However, special characteristics of Thai language, such as lack of word-boundary and sentenceboundary markers trigger several issues in automatic corpus annotation. This paper presents a multi-stage annotation framework, containing two stages of chunking and three stages of tagging. Two chunking stages are named entity extraction by pattern matching and word segmentation by dictionary; and three following tagging stages are dictionary-based, pattern-based and statistical-based tagging. Applying heuristics of ambiguity priority, entity extraction is performed ﬁrst on an original text using a set of patterns, ordered by pattern ambiguity. Later segmenting a sequence of characters into words, the chunks are tagged according to the order of ambiguity, using dictionary, pattern and statistics. Focusing on the reduction of human intervention in corpus construction, our experimental results show that the pattern-based tagging was able to reduce the number of tokens marked as unknown by the dictionary-based tagging by 44.76% and the statistical-based tagging was able to reduce the number of terms identiﬁed as ambiguous by both above methods by 72.44%. The proposed multistage framework reduced the number of tokens requiring human annotation (those that are tagged unknown or with multiple tags) to 16.35% of the entire corpus. 
This paper presents the work being done so far on the building of online corpus for Philippine languages. As for the status, the Philippine Languages Online Corpora (PLOC) now boasts a 250,000-word written corpus of the eight major languages in the archipelago. Some of the issues confronting the corpus building and future directions for this project are likewise discussed in this paper. 
Content-targeted advertising systems are becoming an increasingly important part of the funding for free web services. These programs automatically find relevant keywords on a web page, and then display ads based on those keywords. We propose a method for providing links to ads for travel products (which we call ad links) automatically. We extract keywords from citing areas of travel information links, and provide appropriate ad links. To investigate the effectiveness of our method, we conducted experiments. We obtained a high precision for the extraction of keywords and provision of ad links. 
This paper describes a ﬁrst approach to a computational semantic analyzer for Urdu on the basis of the deep syntactic analysis done by the Urdu grammar ParGram. Apart from the semantic construction, external lexical resources such as an Urdu WordNet and a preliminary VerbNet style resource for Urdu are developed and connected to the semantic analyzer. These resources allow for a deeper level of representation by providing real-word knowledge such as hypernyms of lexical entities and information on thematic roles. We therefore contribute to the overall goal of providing more insights into the computationally efﬁcient analysis of Urdu, in particular to computational semantic analysis. 
To write Punjabi language, Punjabi speakers use two different scripts, Perso-Arabic (referred as Shahmukhi) and Gurmukhi. Shahmukhi is used by the people of Western Punjab in Pakistan, whereas Gurmukhi is used by most people of Eastern Punjab in India. The natural written text in Shahmukhi script has missing short vowels and other diacritical marks. Additionally, the presence of ambiguous character having multiple mappings in Gurmukhi script cause ambiguity at character as well as word level while transliterating Shahmukhi text into Gurmukhi script. In this paper we focus on the word level ambiguity problem. The ambiguous Shahmukhi word tokens have many interpretations in target Gurmukhi script. We have proposed two different algorithms for Shahmukhi word disambiguation. The first algorithm formulates this problem using a state sequence representation as a Hidden Markov Model (HMM). The second approach proposes n-gram model in which the joint occurrence of words within a small window of size ± 5 is used. After evaluation we found that both approaches have more than 92% word disambiguation accuracy. 
Servicization of language resources (LR) and technologies (LT) on an appropriately designed and adequately operated infrastructure is a promising solution for sharing them effectively and efﬁciently. Given this rationale, this position paper reviews relevant attempts around the Language Grid, and presents prospects for an ontologygrounded language service infrastructure. As the associated issues may have substantial depth and stretch, collaborations among international and inter-cultural experts are ﬁnally called for. 
Lexical Resources are a critical component for Natural Language Processing applications. However, the high cost of comparing and merging different resources has been a bottleneck to obtain richer resources and a broader range of potential uses for a significant number of languages. With the objective of reducing cost by eliminating human intervention, we present a new method towards the automatic merging of resources. This method includes both, the automatic mapping of resources involved to a common format and merging them, once in this format. This paper presents how we have addressed the merging of two verb subcategorization frame lexica for Spanish, but our method will be extended to cover other types of Lexical Resources. The achieved results, that almost replicate human work, demonstrate the feasibility of the approach. 
There are many approaches to increase web service based machine translation result. However, perfect result alone does not guarantee the quality of translation service or user satisfaction. This paper proposes framework to improve translation service by using non functional attributes information. In this paper, we present methodology to measure quality of composite translation service using existing services information and also the guideline for selecting the composition web service which has highest quality of service. 
In this paper, we present the semantically enriched (SE) version of the Translation Interoperability Protocol (TIP). TIP is designed to foster and enable the seamless sharing of data and information between different TMS based on open standards for data representations by means of the TIP Package (TIPP) as transport container. SE-TIP is a research sideline that employs Semantic Web technologies to support modeling identification and interaction of sharing tasks, and uses the Web architecture to ensure extensibility and scalability of the SE approach. 
This document describes some of the technological aspects of a project devoted to the creation of a factory for language resources. The project’s objectives are explained, as well as the idea to create a distributed infrastructure of web services. This document focuses on two main topics of the factory: (1) the technological approaches chosen to develop the factory, i.e. software, protocols, servers, etc. (2) and Interoperability as the main challenge is to permit different NLP tools work together in the factory. This document explains why XCES and GrAF are chosen as the main formats used for the linguistic data exchange. 
Standards are fundamental to ex-change, preserve, maintain and integrate data and language resources, and as an essential basis of any language resource infrastructure. This paper promotes an Interoperability Framework as a dynamic environment of standards and guidelines, also intended to support the provision of language-(web)service interoperability. In the past two decades, the need to define common practices and formats for linguistic resources has been increasingly recognized and sought. Today open, collaborative, shared data is at the core of a sound language strategy, and standardisation is actively on the move. This paper first describes the current landscape of standards, and presents the major barriers to their adoption; then, it describes those scenarios that critically involve the use of standards and provide a strong motivation for their adoption; lastly, a series of actions and steps needed to operationalise standards and achieve a full interoperability for Language Resources and Technologies are proposed. 
 META-NET is a Network of Excellence aiming to improve significantly on the number of language technologies that can assist European citizens, by enabling enhanced communication and cooperation across languages. A major outcome will be METASHARE, a searchable network of repositories that collect resources such as language data, tools and related web services, covering a large number of European languages. These resources are intended to facilitate the development and evaluation of a wide range of new language processing applications and services. An important aim of META-SHARE is the promotion of interoperability amongst resources. In this paper, we describe our planned efforts to help to achieve this aim, through the adoption of the UIMA framework and the integration of the U-Compare system within the META-SHARE network. UCompare facilitates the rapid construction and evaluation of NLP applications that make use of interoperable components, and, as such, can help to speed up the development of a new generation of European language technology applications.  
The concept of collective intelligence is contributing significantly to knowledge creation on the Web. While current knowledge creation activities tend to be founded on the approach of assembling content such as texts, images and videos, we propose here the serviceoriented approach. We use the term service grid to refer to a framework of collective intelligence based on Web services. This paper provides an institutional design mainly for non-profit service grids that are open to the public. In particular, we deepen the discussion of 1) intellectual property rights, 2) application systems, and 3) federated operations from the perspective of the following stakeholders: service providers, service users and service grid operators respectively. The Language Grid has been operating, based on the proposed institutional framework, since December 2007. 
The Language Grid is an infrastructure for enabling users to share language services developed by language specialists and end user communities. Users can also create new services to support their intercultural/multilingual activities by composing various language services. In the Language Grid, there are several stakeholders with different incentives: service users, service providers, and a Language Grid operator. For enhancing the language service sharing, it is significant that the Language Grid can coordinate them to match their incentives. However, their incentives vary with the operation model of the Language Grid. To support the various operation models, the Language Grid should employ not a general platform dealing with various types of operation models, but a customizable platform. To this end, we have developed an open-source platform consisting of two types of components: core components and optional components. The former assures interoperability of Language Grids, while the latter provides flexibility of system configuration. It allows developers to extend the platform, and each operator to adapt the platform to his/her operation model by selecting the components. To validate the customizability, we have constructed the private Language Grid for Wikimedia using the same platform as public Language Grid. 
In this paper, we propose a new identiﬁer scheme for Language Resources to provide Language Resources with unique names using a standardised nomenclature. This will also ensure Language Resources to be identiﬁed, and consequently to be recognised as proper references in activities within Human Language Technologies as well as in documents and scientiﬁc papers. 
This paper presents the metadata schema for describing language resources (LRs) currently under development for the needs of META-SHARE, an open distributed facility for the exchange and sharing of LRs. An essential ingredient in its setup is the existence of formal and standardized LR descriptions, cornerstone of the interoperability layer of any such initiative. The description of LRs is granular and abstractive, combining the taxonomy of LRs with an inventory of a structured set of descriptive elements, of which only a minimal subset is obligatory; the schema additionally proposes recommended and optional elements. Moreover, the schema includes a set of relations catering for the appropriate inter-linking of resources. The current paper presents the main principles and features of the metadata schema, focusing on the description of text corpora and lexical / conceptual resources. 
In this paper we outline the general concept of the Language Library, a new initiative that has the purpose of building a huge archive of structured colletion of linguistic information. The Language Library is conceived as a community built repository and as an environment that allows language specialists to share multidimensional and multi-level annotated/processed resources. The ﬁrst steps towards its implementation are brieﬂy sketched. 
Sharing resources in a systematic way is essential for conducting high quality scientific research but it imposes requirements on the documentation, visibility, referability, accessibility, and long term preservation of these resources. Sharing resources only makes sense when others can actually use them, which imposes requirements of interoperability on resources. In this paper we describe how the CLARIN-NL project addresses these issues in order to maximize sharing of resources. We submit that the approach taken in CLARINNL is an exemplary approach that deserves adoption by other research communities, possibly slightly adapted to their own needs and requirements. 
This paper introduces the META-NORD project which develops Nordic and Baltic part of the European open language resource infrastructure. META-NORD works on assembling, linking across languages, and making widely available the basic language resources used by developers, professionals and researchers to build specific products and applications. The goals of the project, overall approach and specific action lines on wordnets, terminology resources and treebanks are described. Moreover, results achieved in first five months of the project, i.e. language whitepapers, metadata specification and IPR management, are presented. 
The system presented in this paper is based upon a phrase-based statistical machine transliteration (SMT) framework. The SMT system’s log-linear model is augmented with a set of features specifically suited to the task of transliteration. In particular our model utilizes a feature based on a joint source-channel model, and a feature based on a maximum entropy model that predicts target grapheme sequences using the local context of graphemes and grapheme sequences in both source and target languages. The segmentation for our approach was performed using a non-parametric Bayesian co-segmentation model, and in this paper we present experiments comparing the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. 
While past research on machine transliteration has focused on a single transliteration task, there exist a variety of supplemental transliterations available in other languages. Given an input for English-toHindi transliteration, for example, transliterations from other languages such as Japanese or Hebrew may be helpful in the transliteration process. In this paper, we propose the application of such supplemental transliterations to English-to-Hindi machine transliteration via an SVM re-ranking method with features based on n-gram alignments as well as system and alignment scores. This method achieves a relative improvement of over 10% over the base system used on its own. We further apply this method to system combination, demonstrating just under 5% relative improvement. 
The purpose of this research is to analyze the patterns of the product names used in Thai economic news and to find clues that could be used to identify the product names’ boundaries and their categories. It is found that the patterns of Thai product names are quite varied. Thirty two patterns are found in this study. While some clues like collocation and the context of names can be used for identifying product names, many of them cannot be identified by these means. This indicates that the task of product named entity recognition is an interesting task for Thai language processing. 
 Named entity (NE) equivalents are useful in many multilingual tasks including MT, transliteration, cross-language IR, etc. Recently, several works have addressed the problem of mining NE equivalents from comparable corpora. These methods usually focus only on single-word NE equivalents whereas, in practice, most NEs are multi-word. In this work, we present a generative model for extracting equivalents of multi-word NEs (MWNEs) from a comparable corpus, given a NE tagger in only one of the languages. We show that our method is highly effective on three language pairs, and provide a detailed error analysis for one of them.  
In this paper a new sequence alignment model is proposed for name transliteration systems. In addition, several new features are introduced to enhance the overall accuracy in a name transliteration system. Discriminative methods are used to train the model. Using this model, we achieve improvements on the transliteration accuracy in comparison with the state-of-the-art alignment models. The 1best name accuracy is also improved using a name selection method from the 10-best list based on the contents of the web. This method leads to a relative improvement of 54% over 1-best transliteration. The experiments are conducted on an EnglishPersian name transliteration task. Furthermore, we reproduce the past studies results under the same conditions. Experiments conducting on English to Persian transliteration show that new features provide a relative improvement of 5% over previous published results. 
This work presents a grapheme-based approach of English-to-Chinese (E2C) transliteration, which consists of many-to-many (M2M) alignment and conditional random fields (CRF) using accessor variety (AV) as an additional feature to approximate local context of source graphemes. Experiment results show that the AV of a given English named entity generally improves effectiveness of E2C transliteration. 
Automatically identifying that different orthographic variants of names are referring to the same name is a signiﬁcant challenge for processing natural language processing since they typically constitute the bulk of the out-of-vocabulary tokens. The problem is exacerbated when the name is foreign. In this paper we address the problem of generating valid orthographic variants for proper names, namely transliterating proper names in different scripts. We attempt to solve the problem for three different language pairs: English → Hindi, English → Persian, and Arabic → English. We adopt a uniﬁed approach to the problem. We frame the problem from a statistical Machine Translation perspective. We further post edit the output applying linguistically informed rules particular to the language pair and re-rank the output using machine learning methods. 
In this paper, we present the KOMODO system which is designed to provide tips (advice and warnings) on the way to realize a task from user queries. Information is extracted from web services. We present the different steps of the system: web page selection, ranking and cleaning, extraction of warnings and advice, relevance analysis and contextualization, and production of a response. The different language processing steps are presented together with an evaluation of the results. The system is fully tested and a demonstration will be made if possible during the presentation. 
Question Answering (QA) systems have profoundly evolved since their inception as natural language interfaces to databases. QA technology has indeed become state-of-the-art on open-domain, unstructured information retrieval and more recently touched the Semantic Web and the problem of querying structured data (e.g. RDF triples) on the Web. A natural new challenge is QA over data services, supporting simple natural language query interfaces to the composition of such services for extracting complex results. This paper discusses the above challenges and illustrates natural language QA over data services with a concrete example. 
In this study, we analyzed how answerers indicated unclear points in questions, and how questioners modiﬁed and resubmitted their questions based on indications of unclear points. 
Typically, Question Classiﬁcation (QC) is the ﬁrst phase in Question Answering (QA) systems. This phase is responsible for ﬁnding out the type of the expected answer by having the answer space reduced by pruning out the extra information that is not relevant for the answer extraction. This paper focuses on some Location based questions and some Entity type questions. Almost all the previous QC algorithms evaluated their work by using the classes deﬁned by Li and Roth (2002). The coarse grained classes Location and Entity both have ﬁne grained class Other. In this paper we target and present the mechanism to create new classes to replace the Other classes in Location and Entity class. Additionally, we also present an automatic hierarchy creation method to add new class nodes using the knowledge resources and shallow language processing. We also show how language processing and knowledge resources are important in the question processing and its advantage on Answer Extraction phase. 
Comparative or evaluative questions are the non-factoid class of questions that contain comparative or evaluative keywords, which may or may not be directly quantifiable. This entails the need for extraction of comparative and evaluative features, identification of semantic meaning of those features and converting them to quantifiable criteria before data can be obtained from the source text. This paper presents the study of the comparative or evaluative questions along with a rule based approach to syntactically extract and semantically analyze comparative or evaluative features, and give a basic idea to generate the answer. 
This paper reports our ongoing research work to create a semantic based question answering system for Thailand tourism information. Our proposed system focuses on mapping expressions in Thai natural language into ontology query language (SPARQL). Topic: Language processing, reasoning aspects 
In this paper we present two stemmers for Gujarati- a lightweight inflectional stemmer based on a hybrid approach and a heavyweight derivational stemmer based on a rule-based approach. Besides using a module for unsupervised learning of stems and suffixes for lightweight stemming, we have also included a module performing POS (Part Of Speech) based stemming and a module using a set of substitution rules, in order to improve the quality of these stems and suffixes. The inclusion of these modules boosted the accuracy of the inflectional stemmer by 9.6% and 12.7% respectively, helping us achieve an accuracy of 90.7%. The maximum index compression obtained for the inflectional stemmer is about 95%. On the other hand, the derivational stemmer is completely rule-based, for which, we attained an accuracy of 70.7% with the help of suffix-stripping, substitution and orthographic rules. Both these systems were developed to be useful in applications such as Information Retrieval, corpus compression, dictionary search and as pre-processing modules in other NLP problems such as WSD. 1. Introduction Stemming is a process of conflating related words to a common stem by chopping off the inflectional and derivational endings. Stemming plays a vital role in Information Retrieval systems by reducing the index size and increasing the recall by retrieving results that contain any of the possible forms of a word present in the query (Harman, 1991). This is especially true in case of a morphologically rich language like Gujarati.  The aim is to ensure that all the related words map to common stem, wherein, the stem may or may not be a meaningful word in the vocabulary of the language. Current state of the art approaches to stemming can be classified into three categories, viz., rule-based, unsupervised and hybrid (Smirnov, 2008). In case of inflectional stemmer, building a completely rule-based system is non-trivial for a language like Gujarati. On the other hand, adopting a purely unsupervised approach, such as take-all-splits discussed in section 4, may fail to take advantage of some language phenomena, such as, the suffixes in a language like Gujarati, are separable based on their parts of speech. For example, the suffix ી (-ī) should be stripped off for verbs (as in case of કર karī ‘did’), but not for nouns (as in case of ઈભાનદાર īmāndārī ‘honesty’). Such characteristics can be easily represented in the form of substitution rules. So, we follow a hybrid approach for the inflectional stemmer taking advantage of both rule-based and unsupervised phenomena. However, in case of derivational stemming, words that are derived, either by adding affixes to the stems or by performing changes at the morpheme boundary, are reduced to their stem forms. To accomplish this task of derivational stemming, we have adopted a completely rule-based approach. The remainder of this paper is organized as follows. We describe the related work in section 2. Next, section 3 explains the morphological structure of Gujarati. We describe our approach to inflectional stemmer in section 4 and to derivational stemmer in section 5. Experiments and results are presented in section 6. Section 7 concludes the paper, pointing also to future work.  
This paper documents recent work carried out for PeEn-SMT, our Statistical Machine Translation system for translation between the English-Persian language pair. We give details of our previous SMT system, and present our current development of significantly larger corpora. We explain how recent tests using much larger corpora helped to evaluate problems in parallel corpus alignment, corpus content, and how matching the domains of PeEn-SMT’s components affect translation output. We then focus on combining corpora and approaches to improve test data, showing details of experimental setup, together with a number of experiment results and comparisons between them. We show how one combination of corpora gave us a metric score outperforming Google Translate for the English-toPersian translation. Finally, we outline areas of our intended future work, and how we plan to improve the performance of our system to achieve higher metric scores, and ultimately to provide accurate, reliable language translation. 
Since Thai has no explicit word boundary, word segmentation is the first thing to do before developing any Thai NLP applications. In order to create large Thai word-segmented corpora to train a word segmentation model, an efficient verification tool is needed to help linguists work more conveniently to check the accuracy and consistency of the corpora. This paper proposes Thai Word Segmentation Verification Tool Version 2.0, which has significantly been improved from the version 1.0 in many aspects. By using hash table in its data structures, the new version works more rapidly and stably. In addition, the new user interfaces have been ameliorated to be more user-friendly too. The description on the new data structures is explained, while the modification of the new user interfaces is described. An experimental evaluation, in comparing with the previous version, shows the improvement in every aspect. 
Economic activities now keep being globalized more and more. Thus we are driven to deal with not only the documents written in English but also those written in other languages. In order to enable us to develop processors of any language quickly, we have been making a framework based on statistical processing and machine learning. At present, we conﬁrmed that part-of-speech (POS) taggers of some target languages can be built by using this framework and the information of source languages. In this paper, we describe the method of acquiring POS lexicons and that of generating supervisors of POS sequences, which are used to learn grammatical models of target languages. We also explain the experimental results of building POS taggers of Portuguese and Indonesian by using some source languages. 
We propose an unsupervised training method to guide the learning of Malay derivational morphology from a set of morphological segmentations produced by a na¨ıve morphological analyzer. Using a morphology-based language model, we ﬁrst estimate the probability of a given segmentation. We train the model with EM to ﬁnd the segmentation that maximizes the probability of each morpheme. We extract the set of afﬁx patterns produced by our algorithm and evaluate them against two references: a list of afﬁx patterns extracted from our hand-segmented derivational wordlist and a derivational history produced by a stemmer. 
This paper concentrates on Punjabi language noun and proper name stemming. The purpose of stemming is to obtain the stem or radix of those words which are not found in dictionary. If stemmed word is present in dictionary, then that is a genuine word, otherwise it may be proper name or some invalid word. In Punjabi language stemming for nouns and proper names, an attempt is made to obtain stem or radix of a Punjabi word and then stem or radix is checked against Punjabi noun and proper name dictionary. An in depth analysis of Punjabi news corpus was made and various possible noun suffixes were identified like ੀ ਆਂ īāṃ, ਿੀਆਂ iāṃ, ੀਆਂ ūāṃ, ੀ ੀਂ āṃ, ੀ ਏ īē etc. and the various rules for noun and proper name stemming have been generated. Punjabi language stemmer for nouns and proper names is applied for Punjabi Text Summarization. The efficiency of Punjabi language noun and Proper name stemmer is 87.37%. 
Urdu is morphologically rich language with different nature of its characters. Urdu text tokenization and sentence boundary disambiguation is difficult as compared to the language like English. Major hurdle for tokenization is improper use of space between words, where as absence of case discrimination makes the sentence boundary detection a difficult task. In this paper some issues regarding both of these language processing tasks have been identified. 
Urdu language raises several challenges to Natural Language Processing (NLP) largely due to its rich morphology. In this language, morphological processing becomes particularly important for Information Retrieval (IR). The core tool of IR is a Stemmer which reduces a word to its stem form. Due to the diverse nature of Urdu, developing stemmer is a challenging task. In Urdu, there are large numbers of variant forms (derivational and inflectional forms) for a single word form. The aim of this paper is to present issues pertaining to the development of Urdu stemmer (rule based stemmer). 1. Introduction Urdu is an Indo-Aryan language. It is the national language of Pakistan and is one of the twentythree official languages of India. It is written in Perso-Arabic script. The Urdu vocabulary consists of several languages including Arabic, English, Turkish, Sanskrit and Farsi (Persian) etc. Urdu’s script is right-to-left and form of a word’s character is context sensitive, means the form of a character is dissimilar in a word because of the position of that character in the word (beginning, centre, on the ending) (Waqas et al., 2006). In Urdu language, morphological processing becomes particularly important for Information Retrieval (IR). Information retrieval system is used to ensure easy access to stored information. It also deals with saving, representation and organization of information objects. Modules of an IR system consist of a group of information objects, a group of requests and a method to decide which information items are most possibly helping to meet the requirements of the requests. Inside IR, the information data which is stored and receives search calls usually corresponds to the lists of identifiers recognized as key terms, keywords. One of the attempts to make the search engines more efficient in information retrieval is the use of stemmer. Stem is the base or root form of a word. Stemmer is an algorithm that reduces  the word to their stem/root form e.g. tested, testing, pretest and tester have the stem “test”. Similarly the Urdu stemmer should stem the words ‫ﮐﻢ‬ ‫( ﻋﻘﻞ‬senseless), ‫( ﻋﻘﻞ ﻣﻨﺪ‬sensible), ‫( ﻋﻘﻞ ﻣﻨﺪﯼ‬sagacity) to Urdu stem word ‫( ﻋﻘﻞ‬sense). Stemming is part of the complex process of taking out the words from text and turning them into index terms in an IR system. Indexing is the process of selecting keywords for representing a document. The smallest units of word which cannot be decomposed further into smaller meaningful units are called Morphemes.1 They are of two kinds: free morphemes and bound morphemes. Morphemes which exist freely (alone) are called free morphemes whereas bound morphemes are made as a result of combination with another morpheme. For instance "flower" is a free morpheme, while "s" is the example of a bound morpheme. The study of internal structure of words is called Morphology.2 Deriving new words from the existing ones is called derivational morphemes e.g. Honour, Honourable, Honourably. Examples in Urdu: The words ‫( ﭼﺎﮨﺖ‬love), ‫ﭼﺎﮨﺘﺎ‬ (to love) and ‫( ﭼﮩﻴﺘﺎ‬lovely) are the derivatives of word ‫( ﭼﺎﮦ‬love). Those morphemes that produce the grammatical formation of a word is called Inflectional morphemes e.g. Boys. Examples in Urdu: The words ‫( ﺳﺨﺖ ﺗﺮ‬harder) and ‫ﺳﺨﺖ ﺗﺮﻳﻦ‬ (hardest) are the inflected forms of word ‫ﺳﺨﺖ‬ (hard). The stemmer is also applicable to other natural language processing applications needing morphological analysis for example spell checkers, word frequency count studies, word parsing etc. The rest of the paper is organized as follows: In section 2, different rule based stemming algorithms are discussed. Section 3 gives an introduction regarding orthographic features. In section 4, several issues pertaining to Urdu stemmer are 
Arabic morphology poses special challenges to computational natural language processing systems. Its rich morphology and the highly complex word formation process of roots and patterns make computational approaches to Arabic very challenging. In this paper we present an approach for morphological analysis and generation of Modern Standard Arabic (MSA). Our approach is based on Arabic morphological automaton technology. We take the special representation of Arabic morphology (root and scheme) to construct a set of morphological automaton which will be used directly in developing a system for Arabic morphological analysis and generation. Our approach for Arabic morphological analysis and generation can be used in different Arabic NLP applications such as Machine Translation (MT) and Information Retrieval (IR). 
Ambiguities arising from alternations of scope in interpretations for multiply quantiﬁed sentences appear to require grammatical operations that compromise the strong assumptions of syntactic/semantic transparency and monotonicity underlying the Frege-Montague approach to the theory of grammar. Examples that have been proposed include covert movement at the level of logical form, abstraction or storage mechanisms, and proliferating type-changing operations. The paper examines some interactions of scope alternation with syntactic phenomena including coordination, binding, and relativization. Starting from the assumption of Fodor and Sag, and others, that many expressions that have been treated as generalized quantiﬁers are in fact referential expressions, and using Combinatory Categorial Grammar (CCG) as a grammatical framework, the paper presents an account of quantiﬁer scope ambiguities according to which the available readings are projected directly from the lexicon by the combinatorics of the syntactic derivation, without any independent manipulation of logical form and without recourse to otherwise unmotivated type-changing operations. As a direct result, scope ambiguity can be efﬁciently processed using packed representations from which the available readings can be simply enumerated. 
We present a formal framework that generalizes a variety of monolingual and synchronous grammar formalisms for parsing and translation. Our framework is based on regular tree grammars that describe derivation trees, which are interpreted in arbitrary algebras. We obtain generic parsing algorithms by exploiting closure properties of regular tree languages. 
Restricting the input or the output of a grammar-induced translation to a given set of trees plays an important role in statistical machine translation. The problem for practical systems is to ﬁnd a compact (and in particular, ﬁnite) representation of said restriction. For the class of synchronous treeadjoining grammars, partial solutions to this problem have been described, some being restricted to the unweighted case, some to the monolingual case. We introduce a formulation of this class of grammars which is effectively closed under input and output restrictions to regular tree languages, i.e., the restricted translations can again be represented by grammars. Moreover, we present an algorithm that constructs these grammars for input and output restriction, which is inspired by Earley’s algorithm. 
The problem of ﬁnding the most probable string for a distribution generated by a weighted ﬁnite automaton or a probabilistic grammar is related to a number of important questions: computing the distance between two distributions or ﬁnding the best translation (the most probable one) given a probabilistic ﬁnite state transducer. The problem is undecidable with general weights and is N P-hard if the automaton is probabilistic. We give a pseudo-polynomial algorithm which computes the most probable string in time polynomial in the inverse of the probability of the most probable string itself, both for probabilistic ﬁnite automata and probabilistic context-free grammars. We also give a randomised algorithm solving the same problem. 
We present a simple and effective way to perform out-of-domain statistical parsing by drastically reducing lexical data sparseness in a PCFG-LA architecture. We replace terminal symbols with unsupervised word clusters acquired from a large newspaper corpus augmented with biomedical targetdomain data. The resulting clusters are effective in bridging the lexical gap between source-domain and target-domain vocabularies. Our experiments combine known self-training techniques with unsupervised word clustering and produce promising results, achieving an error reduction of 21% on a new evaluation set for biomedical text with manual bracketing annotations. 
Instance-weighting has been shown to be effective in statistical machine translation (Foster et al., 2010), as well as crosslanguage adaptation of dependency parsers (Søgaard, 2011). This paper presents new methods to do instance-weighting in stateof-the-art dependency parsers. The methods are evaluated on Danish and English data with consistent improvements over unadapted baselines. 
This paper discusses the difficulties in Chinese deep parsing, by comparing the accuracy of a Chinese HPSG parser to the accuracy of an English HPSG parser and the commonly used Chinese syntactic parsers. Analysis reveals that deep parsing for Chinese is more challenging than for English, due to the shortage of syntactic constraints of Chinese verbs, the widespread pro-drop, and the large distribution of ambiguous constructions. Moreover, the inherent ambiguities caused by verbal coordination and relative clauses make semantic analysis of Chinese more difficult than the syntactic analysis of Chinese. 
We investigate the question whether an explicit feature representation for morphological features is necessary when parsing German with a fully lexicalized, statistical dependency parser. We use two morphosyntactic phenomena of German to show that while lexicalization does indeed sufﬁce to a large extent when recovering the internal structure of noun phrases, an accurate explicit representation can support the correct selection of its grammatical function. 
This paper proposes a framework which uniﬁes graphical model theory and formal language theory through automata theory. Speciﬁcally, we propose Bayesian Network Automata (BNAs) as a formal framework for specifying graphical models of arbitrarily large structures, or equivalently, specifying probabilistic grammars in terms of graphical models. BNAs use a formal automaton to specify how to construct an arbitrarily large Bayesian Network by connecting multiple copies of a bounded Bayesian Network. Using a combination of results from graphical models and formal language theory, we show that, for a large class of automata, the complexity of inference with a BNA is bounded by the complexity of inference in the bounded Bayesian Network times the complexity of inference for the equivalent stochastic automaton. This illustrates that BNAs provide a useful framework for developing and analysing models and algorithms for structure prediction. 
In this paper, we present a model-theoretic description of Property Grammar (PG) with features. Our approach is based on previous work of Duchier et al. (2009), and extends it by giving a model-theoretic account of feature-based properties, which was lacking in the description of Duchier et al. On top of providing a formal deﬁnition of the semantics of feature-based PG, this paper also discusses the various possible interpretations of features (e.g., within the requirement and agreement properties), and show how these interpretations are represented in our framework. This work opens the way for a constraint-based implementation of a parser for PG with features. 
Using semi-supervised EM, we learn ﬁnegrained but sparse lexical parameters of a generative parsing model (a PCFG) initially estimated over the Penn Treebank. Our lexical parameters employ supertags, which encode complex structural information at the pre-terminal level, and are particularly sparse in labeled data – our goal is to learn these for words that are unseen or rare in the labeled data. In order to guide estimation from unlabeled data, we incorporate both structural and lexical priors from the labeled data. We get a large error reduction in parsing ambiguous structures associated with unseen verbs, the most important case of learning lexico-structural dependencies. We also obtain a statistically signiﬁcant improvement in labeled bracketing score of the treebank PCFG, the ﬁrst successful improvement via semi-supervised EM of a generative structured model already trained over large labeled data. 
In this paper, we describe and compare two statistical parsing approaches for the hybrid dependency-constituency syntactic representation used in the Quranic Arabic Treebank (Dukes and Buckwalter, 2010). In our first approach, we apply a multi-step process in which we use a shift-reduce algorithm trained on a pure dependency preprocessed version of the treebank. After parsing, the dependency output is converted into the hybrid representation. This is compared to a novel one-step parser that is able to learn the hybrid representation without preprocessing. We define an extended labelled attachment score (ELAS) as our performance metric for hybrid parsing, and report 87.47% (F1 score) for the multi-step approach, and 89.03% (F1 score) for the onestep integrated algorithm. We also consider the effect of using different sets of morphological features for parsing the Quran, comparing our results to recent work on Modern Standard Arabic. 
This paper proposes a direct parsing of non-local dependencies in English. To this end, we use probabilistic linear context-free rewriting systems for data-driven parsing, following recent work on parsing German. In order to do so, we ﬁrst perform a transformation of the Penn Treebank annotation of non-local dependencies into an annotation using crossing branches. The resulting treebank can be used for PLCFRS-based parsing. Our evaluation shows that, compared to PCFG parsing with the same techniques, PLCFRS parsing yields slightly better results. In particular when evaluating only the parsing results concerning long-distance dependencies, the PLCFRS approach with discontinuous constituents is able to recognize about 88% of the dependencies of type *T* and *T*-PRN encoded in the Penn Treebank. Even the evaluation results concerning local dependencies, which can in principle be captured by a PCFG-based model, are better with our PLCFRS model. This demonstrates that by discarding information on non-local dependencies the PCFG model loses important information on syntactic dependencies in general. 
Among human cognitive abilities, language is singular in the diversity of its manifestations: over 6000 languages are spoken in the world today. Some of the major challenges in modelling how language is processed by the human brain thus lie in explaining (a) how this diversity is handled, and (b) whether there are nevertheless some underlying generalisations that recur across languages of different types. Furthermore, an adequate model should be neurobiologically plausible, i.e., respect what we know about the structure and function of the human brain. In this presentation, I will describe a line of research in which we have attempted to take up these challenges at the level of sentence comprehension. Based on the results of neurophysiological experiments in a range of typologically varied languages, I will argue for a comprehension architecture that is actor-centred, i.e., focused on identifying the participant primarily responsible for the state of affairs under discussion. I will introduce the latest version of a comprehension model (extended Argument Dependency Model, eADM; Bornkessel and Schlesewsky, 2006), the architecture of which is built around actor-centrality as a design principle, and will describe how it accounts for potential universals of comprehension and critical dimensions of variation. References Ina Bornkessel and Matthias Schlesewsky. 2006. The Extended Argument Dependency Model: A neurocognitive approach to sentence comprehension across languages. Psychological Review, 113, 787–821. 117 Proceedings of the 12th International Conference on Parsing Technologies, page 117, October 5-7, 2011, Dublin City University. c 2011 Association for Computational Linguistics 
The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated conﬁdence of RE rules learned from the n best parses can be exploited for parse reranking. The acquired reranking model improves the performance of RE in both training and test phases with the new ﬁrst parses. The obtained signiﬁcant boost of recall does not come from an overall gain in parsing performance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspeciﬁc parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 
Prepositional phrase attachment is an important subproblem of parsing, performance on which suffers from limited availability of labelled data. We present a semi-supervised approach. We show that a discriminative lexical model trained from labelled data, and a generative lexical model learned via Expectation Maximization from unlabelled data can be combined in a product model to yield a PP-attachment model which is better than either is alone, and which outperforms the modern parser of Petrov and Klein (2007) by a signiﬁcant margin. We show that, when learning from unlabelled data, it can be beneﬁcial to model the generation of modiﬁers of a head collectively, rather than individually. Finally, we suggest that our pair of models will be interesting to combine using new techniques for discriminatively constraining EM.  e.g., He ate a salad [PP with a fork] [PP of plastic] Prepositional phrase attachment is an important sub-problem of parsing in and of itself. Structural heuristics perform poorly (cf., Collins and Brooks, 1995), and so lexical knowledge is crucial. Moreover, the highly lexicalized nature of prepositional phrase attachment makes it a kind of microcosm of the general problem of learning dependency structure, and so acts as a computationally less-demanding testing ground on which to try out learning techniques. We have endeavoured to approach the problem with a strategy that might be likely to generalize: a mix of generative and discriminative lexical models, trained using techniques that have worked for parsers. The main contributions of this paper are:  
Current successful probabilistic parsers require large treebanks which are difﬁcult, time consuming, and expensive to produce. Some parts of these data do not contain any useful information for training a parser. Active learning strategies allow to select the most informative samples for annotation. Most existing active learning strategies for parsing rely on selecting uncertain sentences for annotation. We show in this paper that selecting full sentences is not an optimal solution and propose a way to select only subparts of sentences. 
There has been a long history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation. These methods leverage the observation that complex inference problems can often be decomposed into efﬁciently solvable sub-problems. Thus far, however, these methods are not widely used in NLP. In this talk I will describe recent work on inference algorithms for NLP based on Lagrangian relaxation. In the ﬁrst part of the talk I will describe work on non-projective parsing. In the second part of the talk I will describe an exact decoding algorithm for syntax-based statistical translation. If time permits, I will also brieﬂy describe algorithms for dynamic programming intersections (e.g., the intersection of a PCFG and an HMM), and for phrase-based translation. For all of the problems that we consider, the resulting algorithms produce exact solutions, with certiﬁcates of optimality, on the vast majority of examples; the algorithms are efﬁcient for problems that are either NP-hard (as is the case for non-projective parsing, or for phrase-based translation), or for problems that are solvable in polynomial time using dynamic programming, but where the traditional exact algorithms are far too expensive to be practical. While the focus of this talk is on NLP problems, there are close connections to inference methods, in particular belief propagation, for graphical models. Our work was inspired by recent work that has used dual decomposition as an alternative to belief propagation in Markov random ﬁelds. This is joint work with Yin-Wen Chang, Tommi Jaakkola, Terry Koo, Sasha Rush, and David Sontag. 150 
We present a novel method for the computation of preﬁx probabilities for linear context-free rewriting systems. Our approach streamlines previous procedures to compute preﬁx probabilities for context-free grammars, synchronous context-free grammars and tree adjoining grammars. In addition, the methodology is general enough to be used for a wider range of problems involving, for example, several preﬁxes. 
We present a matrix encoding of contextfree grammars, motivated by hardware-level efﬁciency considerations. We ﬁnd efﬁciency gains of 2.5–9× for exhaustive inference and approximately 2× for pruned inference, resulting in high-accuracy parsing at over 20 sentences per second. Our grammar encoding allows ﬁne-grained parallelism during chart cell population; we present a controlled study of several methods of parallel parsing, and ﬁnd nearoptimal latency reductions as core-count increases. 
Low-latency solutions for syntactic parsing are needed if parsing is to become an integral part of user-facing natural language applications. Unfortunately, most state-of-theart constituency parsers employ large probabilistic context-free grammars for disambiguation, which renders them impractical for real-time use. Meanwhile, Graphics Processor Units (GPUs) have become widely available, offering the opportunity to alleviate this bottleneck by exploiting the ﬁnegrained data parallelism found in the CKY algorithm. In this paper, we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm. We use the Compute Uniﬁed Device Architecture (CUDA) programming model to reimplement a state-of-theart parser, and compare its performance on two recent GPUs with different architectural features. Our best results show a 26-fold speedup compared to a sequential C implementation. 
We present a deterministic HPSG parser capable of processing text incrementally with very fast parsing times. Our system demonstrates an efﬁcient data-driven approach that achieves a high level of precision. Through a series of experiments in different conﬁgurations, we evaluate our system and compare it to current state-of-the-art within the ﬁeld, and show that high quality deterministic parsing is realistic even for a ‘deep’ uniﬁcation-based precision grammar. 
We present a novel corpus-driven approach towards grammar approximation for a linguistically deep Head-driven Phrase Structure Grammar. With an unlexicalized probabilistic context-free grammar obtained by Maximum Likelihood Estimate on a largescale automatically annotated corpus, we are able to achieve parsing accuracy higher than the original HPSG-based model. Different ways of enriching the annotations carried by the approximating PCFG are proposed and compared. Comparison to the state-of-the-art latent-variable PCFG shows that our approach is more suitable for the grammar approximation task where training data can be acquired automatically. The best approximating PCFG achieved ParsEval F1 accuracy of 84.13%. The high robustness of the PCFG suggests it is a viable way of achieving full coverage parsing with the hand-written deep linguistic grammars. 
Radically different approaches have been proved to be effective for phrase-structure and dependency parsers in the last decade. Here, we aim to exploit the divergence in these approaches and show the utility of features extracted from the automatic dependency parses of sentences for a discriminative phrase-structure parser. Our experiments show a signiﬁcant improvement over the state-of-the-art German discriminative constituent parser. 
We compare the use of edited text in the form of newswire and unedited text in the form of discussion forum posts as sources for training material in a self-training experiment involving the Brown reranking parser and a test set of sentences from an online sports discussion forum. We ﬁnd that grammars induced from the two automatically parsed corpora achieve similar Parseval fscores, with the grammars induced from the discussion forum material being slightly superior. An error analysis reveals that the two types of grammars do behave differently. 
In this paper, we give a summary of various dependency chart parsing algorithms in terms of the use of parsing histories for a new dependency arc decision. Some parsing histories are closely related to the target dependency arc, and it is necessary for the parsing algorithm to take them into consideration. Each dependency treebank may have some unique characteristics, and it requires for the parser to model them by certain parsing histories. We show in experiments that proper selection of the parsing algorithm which reﬂect the dependency annotation of the coordinate structures improves the overall performance. 
We present a perspective on parser evaluation in a context where the goal of parsing is to extract meaning from a sentence. Using this perspective, we show why current parser evaluation metrics are not suitable for evaluating parsers that produce logical-form semantics and present an evaluation metric that is suitable, analysing some of the characteristics of this new metric. 
We consider the problem of parsing a sentence that is partially annotated with information about where phrases start and end. The application domain is interactive parse selection with probabilistic grammars. It is explained that the main obstacle is spurious ambiguity. The proposed solution is ﬁrst described in terms of appropriately constrained synchronous grammars, and then in terms of a computational model for parsing. Experiments show the feasibility for a practical grammar. 
To detect errors in automatically-obtained dependency parses, we take a grammarbased approach. In particular, we develop methods that incorporate n-grams of different lengths and use information about possible parse revisions. Using our methods allows annotators to focus on problematic parses, with the potential to ﬁnd over half the parse errors by examining only 20% of the data, as we demonstrate. A key result is that methods using a small gold grammar outperform methods using much larger grammars containing noise. To perform annotation error detection on newly-parsed data, one only needs a small grammar. 
As well as explaining the core Reinforcement Learning and user modelling methods and concepts behind this work, I will also cover some recent work from other researchers which ﬁts with this general perspective on NLG. Finally, I discuss some future directions for this research area, for example the issues of incremental generation and generation under uncertainty.  References Srini Janarthanam and Oliver Lemon. 2010a. Learning to adapt to unknown users: Referring expression generation in spoken dialogue systems. In Proceedings of ACL. Srinivasan Janarthanam and Oliver Lemon. 2010b. Adaptive referring expression generation in spoken dialogue systems: evaluation with real users. In Proceedings of SIGDIAL. Srinivasan Janarthanam, Helen Hastie, Oliver Lemon, and Xingkun Liu. 2011. ’the day after the day after tomorrow?’ a machine learning approach to adaptive temporal expression generation: training and evaluation with real users. In Proceedings of SIGDIAL. Oliver Lemon, Srini Janarthanam, and Verena Rieser. 2010. Generation under uncertainty. In Proceedings of the Generation Challenges Session at INLG. Oliver Lemon. 2011. Learning what to say and how to say it: joint optimization of spoken dialogue management and natural language generation. Computer Speech and Language, 25(2). Verena Rieser and Oliver Lemon. 2009. Natural language generation as planning under uncertainty for spoken dialogue systems. In Proceedings of EACL. Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010. Optimising information presentation for spoken dialogue systems. In Proceedings of ACL.  
We present a framework for text simpliﬁcation based on applying transformation rules to a typed dependency representation produced by the Stanford parser. We test two approaches to regeneration from typed dependencies: (a) gen-light, where the transformed dependency graphs are linearised using the word order and morphology of the original sentence, with any changes coded into the transformation rules, and (b) gen-heavy, where the Stanford dependencies are reduced to a DSyntS representation and sentences are generating formally using the RealPro surface realiser. The main contribution of this paper is to compare the robustness of these approaches in the presence of parsing errors, using both a single parse and an n-best parse setting in an overgenerate and rank approach. We ﬁnd that the gen-light approach is robust to parser error, particularly in the n-best parse setting. On the other hand, parsing errors cause the realiser in the genheavy approach to order words and phrases in ways that are disliked by our evaluators. 
This paper presents several affective NLG strategies for generating medical texts for parents of pre-term neonates. Initially, these were meant to be personalised according to a model of the recipient’s level of stress. However, our evaluation showed that all recipients preferred texts generated with the affective strategies, regardless of predicted stress level. 
Evaluations of NLG systems generally are quantiative, that is, based on corpus comparison statistics and/or results of experiments with people. Outcomes of such evaluations are important in demonstrating whether or not an NLG system is successful, but leave gaps in understanding why this is the case. Alternatively, qualitative evaluations carried out by experts provide knowledge on where a system needs to be improved. In this paper we describe two such evaluations carried out for the BT-Nurse system, using two different methodologies (content analysis and discourse analysis). The outcomes of such evaluations are discussed in comparison to what was learnt from a quantitiave evaluation of BT-Nurse. Implications for the role of similar evaluations in NLG are also discussed. 
We describe the application of a framework for salience metrics and linguistic variability with respect to the contextually adequate choice of referring expressions and grammatical roles: Where multiple meaning-equivalent candidate realizations are available that differ in one of these aspects, NLG systems can apply salience metrics to predict contextually adequate realization preferences. We evaluate this claim and a number of parameters of salience metrics found in the theoretical literature on two German newspaper corpora. Key features of the approach described here include the application of a two-dimensional model of salience, how its theoretical predictions can be exploited to develop salience metrics for a particular phenomenon, and that these salience metrics can be subsequently applied to other phenomena. This approach can be applied to develop classiﬁers to predict packaging preferences for phenomena where little training data is available. 
Traditional approaches to referring expression generation (REG) have taken as a fundamental requirement the need to distinguish the intended referent from other entities in the context. It seems obvious that this should be a necessary condition for successful reference; but we suggest that a number of recent investigations cast doubt on the signiﬁcance of this aspect of reference. In the present paper, we look at the role of visual context in determining the content of a referring expression, and come to the conclusion that, at least in the referential scenarios underlying our data, visual context appears not to be a major factor in content determination for reference. We discuss the implications of this surprising ﬁnding. 
This paper presents a cross-linguistic data elicitation study on fully realised referring expressions (REs) in a dialogue context. A webbased experiment was set up in which participants were asked to choose REs to be uttered by one of two agents for identifying ﬁve targets in a scripted dialogue. Participants were told that the agent would point at the referents while uttering their chosen linguistic descriptions. The study was conducted in English, Japanese, Portuguese and Dutch and yielded a total of 1190 referring expressions. Our hypotheses concern sets of objects that need to be considered for identiﬁcation depending on the effect of the pointing gesture. Results show interesting and signiﬁcant differences between the language groups. 
This paper offers a solution to a small problem within a much larger problem. We focus on modelling how people use size in reference, words like “big” and “tall”, which is one piece within the much larger problem of how people refer to visible objects. Examining size in isolation allows us to begin untangling a few of the complex and interacting features that affect reference, and we isolate a set of features that may be used in a hand-coded algorithm or a machine learning approach to generate one of six basic size types. The hand-coded algorithm generates a modiﬁer type with a high correspondence to those observed in human data, and achieves 81.3% accuracy in an entirely new domain. This trails oracle accuracy for this task by just 8%. Features used by the hand-coded algorithm are added to a larger set of features in the machine learning approach, and we do not ﬁnd a statistically signiﬁcant difference between the precision and recall of the two systems. The input and output of these systems are a novel characterization of the factors that affect referring expression generation, and the methods described here may serve as one building block in future work connecting vision to language. 
We present an approach to content selection that works on an ontology-based knowledge base developed independently from the task at hand, i.e., Natural Language Generation. Prior to content selection, a stage akin to signal analysis and data assessment used in the generation from numerical data is performed for identifying and abstracting patterns and trends, and identifying relations between individuals. This new information is modeled as an extended ontology on top of the domain ontology which is populated via inference rules. Content selection leverages the ontology-based description of the domain and is performed throughout the text planning at increasing levels of granularity. It includes a main topic selection phase that takes into account a simple user model, a set of heuristics, and semantic relations that link individuals of the KB. The heuristics are based on weights determined empirically by supervised learning on a corpus of summaries aligned with data. The generated texts are short football match summaries that take into account the user perspective. 
This paper investigates to what extent rhetorical relations can be assigned purely on the basis of propositional content, without any reference to speaker goals or other pragmatic information. This task confronts any NLG system designed to generate coherent text from a set of formally represented statements; we consider it here in the context of an ontology verbaliser, for which the input is a set of axioms encoded in the web ontology language OWL. A simple set-theoretical model of the possible semantic relationships between two statements is proposed; this model allows 46 logically consistent relationships, of which we hypothesise that 11 are rhetorically coherent. This hypothesis is tested through an empirical survey which also provides evidence on how the coherent patterns are expressed linguistically. 
Temporal uncertainty in raw data can impede the inference of temporal and causal relationships between events and compromise the output of data-to-text NLG systems. In this paper, we introduce a framework to reason with and represent temporal uncertainty from the raw data to the generated text, in order to provide a faithful picture to the user of a particular situation. The model is grounded in experimental data from multiple languages, shedding light on the generality of the approach. 
We present evaluation results with human subjects for a novel data-driven approach to Natural Language Generation in spoken dialogue systems. We evaluate a trained Information Presentation (IP) strategy in a deployed tourist-information spoken dialogue system. The IP problem is formulated as statistical decision making under uncertainty using Reinforcement Learning, where both content planning and attribute selection are jointly optimised based on data collected in a Wizard-ofOz study. After earlier work testing and training this model in simulation, we now present results from an extensive online user study, involving 131 users and more than 800 test dialogues, which explores its contribution to overall ‘global’ task success. We ﬁnd that the trained Information Presentation strategy signiﬁcantly improves dialogue task completion, with up to a 9.7% increase (30% relative) compared to the deployed dialogue system which uses conventional, hand-coded presentation prompts. We also present subjective evaluation results and discuss the implications of these results for future work in dialogue management and NLG. 
Language generators in situated domains face a number of content selection, utterance planning and surface realisation decisions, which can be strictly interdependent. We therefore propose to optimise these processes in a joint fashion using Hierarchical Reinforcement Learning. To this end, we induce a reward function for content selection and utterance planning from data using the PARADISE framework, and suggest a novel method for inducing a reward function for surface realisation from corpora. It is based on generation spaces represented as Bayesian Networks. Results in terms of task success and humanlikeness suggest that our uniﬁed approach performs better than a baseline optimised in isolation or a greedy or random baseline. It receives human ratings close to human authors. 
We present an approach to the generation of referring expressions (REs) which computes the unique RE that it predicts to be fastest for the hearer to resolve. The system operates by learning a maximum entropy model for referential success from a corpus and using the model’s weights as costs in a metric planning problem. Our system outperforms the baselines both on predicted RE success and on similarity to human-produced successful REs. A task-based evaluation in the context of the GIVE-2.5 Challenge on Generating Instructions in Virtual Environments veriﬁes the higher RE success scores of the system. 
To evaluate these strategies, we go beyond the typical “overhearer” evaluation methodology, in which participants read or listen to pre-prepared dialogues, which limits the evaluation criteria to users’ perceptions (e.g., informativeness, ease of comprehension). Using a Wizard-of-Oz methodology to evaluate the approaches in an interactive setting, we show that in addition to being preferred by users, the UMSR approach is superior to the Reﬁner approach in terms of both task success and dialogue efﬁciency, even when the user is performing a demanding secondary task. Finally, we hypothesize that UMSR is more effective because it uses linguistic devices to highlight relations (e.g., trade-offs) be- 
This paper describes SimpleNLG for German, a surface realisation engine for German based on SimpleNLG (Gatt and Reiter, 2009). Several features of the syntax of German and their implementation within the current framework are discussed, with a special focus on word order phenomena. Grammatical coverage of the system is demonstrated by means of selected examples. 
This paper introduces EasyText, a fully operational NLG system. This application processes numerical data (in tables) in order to generate speciﬁc analytical commentaries of these tables. We start by describing the context of this particular NLG application (communicative goal, user proﬁles, etc.). We then shortly present the theoretical background which underlies EasyText, before describing its implementation, realization and evaluation. 
We argue that Discourse Representation Structures form a suitable level of languageneutral meaning representation for micro planning and surface realisation. DRSs can be viewed as the output of macro planning, and form the rough plan and structure for generating a text. We present the ﬁrst ideas of building a large DRS corpus that enables the development of broad-coverage, robust text generators. A DRS-based generator imposes various challenges on micro-planning and surface realisation, including generating referring expressions, lexicalisation and aggregation. 
This paper presents a method for tailoring Natural Language Generation according to context in a web-based Virtual Research Environment. We discuss a policy-driven framework for capturing user, project and organisation preferences and describe how it can be used to control the generation of textual descriptions of RDF resources. 
The SWAT TOOLS ontology verbaliser generates a hierarchically organised hypertext designed for easy comprehension and navigation. The document structure, inspired by encyclopedias and glossaries, is organised at a number of levels. At the top level, a heading is generated for every concept in the ontology; at the next level, each entry is subdivided into logically-based headings like ‘Deﬁnition’ and ‘Examples’; at the next, sentences are aggregated when they have parts in common; at the lowest level, phrases are hyperlinked to concept headings. One consequence of this organisation is that some statements are repeated because they are relevant to more than one entry; this means that the text is longer than one in which statements are simply listed. This trade-off between organisation and brevity is investigated in a user study.  they are still easier to understand than formal languages (Kuhn, 2010). We describe here a generic verbaliser2 (applicable to any OWL-DL ontology with English identiﬁers/labels) which delivers its output (e.g., ﬁgure 1) in the form of an organised hypertext,3 akin to an online encyclopedia or glossary, and investigate whether this extra organisation makes the text easier to understand and navigate. Figure 1: A section of SWAT TOOLS encyclopedia-style output, length 7746 words or 25 A4 pages, generated from an ontology about spider anatomy.  
This paper describes preliminary analysis on the influence of the semantic roles in summary generation. The proposed method involves three steps: first, the named entities in the original text are identified using a named entity recognizer; secondly, the sentences are parsed and semantic roles are extracted; thirdly, selection of the sentences containing specific semantic roles for the most relevant entities in text. Although the method is language independent, in order to check its viability, we tested the proposed approach for Romanian summaries. 
This paper presents an ongoing work about the implementation of a CCG grammar for Italian Sign Language. This grammar is part of a generation system used for Italian-LIS translation. 
We present a study that investigates that factors that determine what makes a good lexical substitution. We begin by observing that there is a correlation between the corpus frequency of words and the number of WordNet senses they have, and hypothesise that readers might prefer common, but more ambiguous words over less ambiguous but also less common ones. We identify four properties of a word that determine whether it is a suitable substitution in a given context, and ask volunteers to rank their preferences between two common but ambiguous lexical substitutions, and two uncommon but also unambiguous ones. Preliminary results suggest a slight preference towards the unambiguous. 
A data elicitation study on the type of demonstratives and determiners selected to denote objects in English, Dutch and Portuguese dialogues is presented. Participants were given a scenario and a scripted dialogue in which a furniture seller identiﬁes target objects to a buyer. They were then asked to choose a combination of a determiner or demonstrative and a referring expression to be uttered by the seller and told that the agent would point at the targets while uttering the chosen linguistic descriptions. The study was conducted with native speakers and rendered a total of 920 demonstratives and determiners. It focused on accessibility of the target referents and distance between agents and target referents. Results show that the three language groups largely agree in their preferences and, in contrast to previous work, align with a nearby/far away distinction. 
This paper addresses the task of using natural language generation (NLG) techniques to generate sentences with formal and with informal style. We studied the main characteristics of each style, which helped us to choose parameters that can produce sentences in one of the two styles. We collected some ready-made parallel list of formal and informal words and phrases, from different sources. In addition, we added two more parallel lists: one that contains most of the contractions in English (short forms) and their full forms, and another one that consists in some common abbreviations and their full forms. These parallel lists might help to generate sentences in the preferred style, by changing words or expressions for that style. Our NLG system is built on top of the SimpleNLG package (Gatt and Reiter, 2009). We used templates from which we generated valid English texts with formal or informal style. In order to evaluate the quality of the generated sentences and their level of formality, we used human judges. The evaluation results show that our system can generate formal and informal style successfully, with high accuracy. The main contribution of our work consists in designing a set of parameters that led to good results for the task of generating texts with different formality levels. 
This paper shows how glue rules can be used to increase the robustness of statistical chart realization in a manner inspired by dependency realization. Unlike the use of glue rules in MT—but like previous work with XLE on improving robustness with hand-crafted grammars—they are invoked here as a fallback option when no grammatically complete realization can be found. The method works with Combinatory Categorial Grammar (CCG) and has been implemented in OpenCCG. As the techniques are not overly tied to CCG, they are expected to be applicable to other grammar-based chart realizers where robustness is a common problem. Unlike an earlier robustness technique of greedily assembling fragments, glue rules enable nbest outputs and are compatible with disjunctive inputs. Experimental results indicate that glue rules yield improved realizations in comparison to greedy fragment assembly, though a sizeable gap remains between the quality of grammatically complete realizations and fragmentary ones. 
Hand-crafted approaches to content determination are expensive to port to new domains. Machine-learned approaches, on the other hand, tend to be limited to relatively simple selection of items from data sets. We observe that in time series domains, textual descriptions often aggregate a series of events into a compact description. We present a simple technique for automatically determining sequences of events that are worth reporting, and evaluate its effectiveness. 
Once again, we successfully applied (with the help of support letters from many of last year’s participants and other HLT colleagues) for funding from the Engineering and Physical Sciences Research Council (EPSRC), the main funding body for HLT in the UK. This support helped with all aspects of developing and running the SR Task and organising Generation Challenges 2011. It enabled us to create the SR Task data and to carry out human evaluations, as well as to pay for Deirdre Hogan and Eric Kow’s time spent working on the SR Task. 
We propose a competitive shared evaluation task for Surface Realization in Spanish. The task would be carried out in 2012. It would involve the generation of text in Spanish from a common ground input shared by all systems. Separate corpora for training/development (composed of pairs of common ground input and expected string result) and testing (only common ground input) will be provided. Automatic evaluation procedures will be provided. Submitted results will also be subject to human evaluation. The present proposal is tentative in two different ways. First, the authors intend to revise the proposal in view of the experience and feedback of the Surface Realization Pilot Task currently in process for English, once its results are made public (due in September, 2011). Second, the authors are willing to colaborate both with organizers of equivalent tasks for other languages or more researchers interested in surface realization for Spanish. 
The Surface Realisation (SR) Task was a new task at Generation Challenges 2011, and had two tracks: (1) Shallow: mapping from shallow input representations to realisations; and (2) Deep: mapping from deep input representations to realisations. Five teams submitted six systems in total, and we additionally evaluated human toplines. Systems were evaluated automatically using a range of intrinsic metrics. In addition, systems were assessed by human judges in terms of Clarity, Readability and Meaning Similarity. This report presents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report. 
This article describes the experiments we performed during our participation in the HOO Challenge. We present the adaption we made on two systems, mainly designing new grammatical rules and completing a lexicon. We focused our work on some of the most common errors in the corpus: missing punctuation and inaccurate prepositions. Our best experiment achieved a 0.1097 detection score, a 0.0820 recognition score, and a 0.0557 correction score on the test corpus.  Another approach to error correction consists in using manually developed rules to identify and correct erroneous occurrences. This approach has, for instance, been adopted in the open-source LanguageTool proofreading tool1 (Naber, 2003; Miłkowski, 2010). In this paper, we describe our participation to the HOO2011 challenge. We present our systems and the conﬁgurations we used while participating in the test stage of the challenge. 2 Material and methods  
This paper describes the UKP Lab system participating in the Helping Our Own Challenge 2011. We focus on the correction of realword spelling errors (RWSEs) that are especially hard to detect. Our highly ﬂexible system architecture is based on UIMA (Ferrucci and Lally, 2004) and integrates state-of-the-art approaches for detecting RWSEs. 
GIVE-2.5 evaluates eight natural language generation (NLG) systems that guide human users through solving a task in a virtual environment. The data is collected via the Internet, and to date, 536 interactions of subjects with one of the NLG systems have been recorded. The systems are compared using both task performance measures and subjective ratings by human users. 
These notes describe a contribution to the 2011 GIVE Challenge from the University of Aberdeen. Our contribution focuses on an attempt to increase the extent to which participants felt engaged in the direction giving/following game on which the GIVE challenge focuses. 
This paper presents the Bremen system for the GIVE-2.5 challenge. It is based on decision trees learnt from new annotations of the GIVE corpus augmented with manually speciﬁed rules. Surface realisation is based on context-free grammars. The paper will address advantages and shortcomings of the approach and discuss how the present system can serve as a baseline for a future evaluation with an improved version using hierarchical reinforcement learning with graphical models.  (2011), we suggested to use Hierarchical Reinforcement Learning (HRL) for GIVE and compared it against decision trees. While results (based on simulation and human ratings) showed that the HRL system achieved signiﬁcantly better performance, this paper presents a system that behaves based on decision trees learnt from human data. The system is developed as a reliable baseline for a comprehensive evaluation of an HRL-based system in the future (as part of the author’s PhD thesis). 2 The GIVE Task  
In this paper we describe the C generation system from the Universidad Nacional de Co´rdoba (Argentina) as embodied during the 2011 GIVE 2.5 challenge. The C system has two distinguishing characteristics. First, its navigation and referring strategies are based on the area visible to the player, making the system independent of GIVE’s internal representation of areas (such as rooms). As a result, the system portability to other virtual environments is enhanced. Second, the system adapts classical grounding models to the task of instruction giving in virtual worlds. The simple grounding processes implemented (for referents, game concepts and game progress) seem to have an impact on the evaluation results. 
The CL system uses an algorithm that, given a task-based corpus situated in a virtual world, which contains human instructor’s speech acts and the user’s responses as physical actions, generates a virtual instructor that helps a user achieve a given task in the virtual world. In this report, we explain how this algorithm can be used for generating a virtual instructor for a game-like, task-oriented virtual world such as GIVE’s. 
This paper presents the instruction generation system L submitted by the LORIA and TALARIS team to the GIVE challenge 2011 (GIVE 2.5). The system L takes the same approach to instruction generation than its predecessor the system NA that participated to the GIVE challenge 2010 (GIVE 2), the two systems are almost the same except minor modiﬁcations. We present the strategy of these systems, namely a directive, low level, navigation strategy (“Go left”) and a referring strategy based on focus and sub-contexts (Denis, 2010) (“Not this one! Look for the other one”). These strategies were successful, as shown by the GIVE 2 challenge, but also had some deﬁciences we tried to ﬁx for GIVE 2.5. We explain these deﬁciencies and how we ﬁxed them in GIVE 2.5. We eventually present the preliminary results that show that the system L, like the system NA, achieved a very good result both in objective and in subjective metrics. 
We present the Potsdam natural language generation systems P1 and P2 of the GIVE-2.5 Challenge. The systems implement two different referring expression generation models from Garouﬁ and Koller (2011) while behaving identically in all other respects. In particular, P1 combines symbolic and corpus-based methods for the generation of successful referring expressions, while P2 is based on a purely symbolic model which serves as a qualiﬁed baseline for comparison. We describe how the systems operated in the challenge and discuss the results, which indicate that P1 outperforms P2 in terms of several measures of referring expression success. 
This paper describes the Thumbs Up! Twente system, a natural language generation system designed for the GIVE 2.5 Challenge. The purpose of the system is to guide a user through a virtual 3D environment by generating instructions in real-time. Our system focuses on motivating the user to keep him playing the game and trying to ﬁnd the trophy. 
Sentence compression has attracted much interest in recent years, but most sentence compressors are extractive, i.e., they only delete words. There is a lack of appropriate datasets to train and evaluate abstractive sentence compressors, i.e., methods that apart from deleting words can also rephrase expressions. We present a new dataset that contains candidate extractive and abstractive compressions of source sentences. The candidate compressions are annotated with human judgements for grammaticality and meaning preservation. We discuss how the dataset was created, and how it can be used in generate-and-rank abstractive sentence compressors. We also report experimental results with a novel abstractive sentence compressor that uses the dataset. 
Recent years have seen a trend towards empirically motivated and more data-driven approaches in the ﬁeld of referring expression generation (REG). Much of this work has focussed on initial reference to objects in visual scenes. While this scenario of use is one of the strongest contenders for real-world applications of referring expression generation, existing data sets still only embody very simple stimulus scenes. To move this research forward, we require data sets built around increasingly complex scenes, and we need much larger data sets to accommodate their higher dimensionality. To control the complexity, we also need to adopt a hypothesis-driven approach to scene design. In this paper, we describe GRE3D7, the largest corpus of humanproduced distinguishing descriptions available to date, discuss the hypotheses that underlie its design, and offer a number of analyses of the 4480 descriptions it contains. 
We describe a corpus of human-written English language summaries of line graphs. This corpus is intended to help develop a system to automatically generate summaries capturing the most salient information conveyed by line graphs in popular media, as well as to evaluate the output of such a system. 
Currently there is little agreement about, or even discussion of, methodologies for taskbased evaluation of NLG systems. I discuss one speciﬁc issue in this area, namely the importance of control vs the importance of ecological validity (real-world context), and suggest that perhaps we need to put more emphasis on ecological validity in NLG evaluations. 
Linguistic patterns reﬂect the regularities of Natural Language and their applicability is acknowledged in several Natural Language Processing tasks. Particularly, in the task of Question Generation, many systems depend on patterns to generate questions from text. The approach we follow relies on patterns that convey lexical, syntactic and semantic information, automatically learned from largescale corpora. In this paper we discuss the impact of varying several parameters during pattern learning and matching in the Question Generation task. In particular, we introduce semantics (by means of named entities) in our lexico-syntactic patterns. We evaluate and compare the number and quality of the learned patterns and the matched text segments. Also, we detail the inﬂuence of the patterns in the generation of natural language questions. 
This paper shows that using linguistically motivated features for English that-complementizer choice in an averaged perceptron model for classiﬁcation can improve upon the prediction accuracy of a state-of-the-art realization ranking model. We report results on a binary classiﬁcation task for predicting the presence/absence of a that-complementizer using features adapted from Jaeger’s (2010) investigation of the uniform information density principle in the context of that-mentioning. Our experiments conﬁrm the efﬁcacy of the features based on Jaeger’s work, including information density–based features. The experiments also show that the improvements in prediction accuracy apply to cases in which the presence of a that-complementizer arguably makes a substantial difference to ﬂuency or intelligiblity. Our ultimate goal is to improve the performance of a ranking model for surface realization, and to this end we conclude with a discussion of how we plan to combine the local complementizer-choice features with those in the global ranking model. 
We discuss the preferred ordering of elements of binomials (e.g., conjunctions such as ﬁsh and chips, lager and lime, exciting and interesting) and provide a detailed critique of Benor and Levy’s probabilistic account of English binomials. In particular, we discuss the extent to which their approach is suitable as a model of language generation. We describe resources we have developed for the investigation of binomials using a combination of parsed corpora and very large unparsed corpora. We discuss the use of these resources in developing models of binomial ordering, concentrating in particular on the evaluation issues which arise. 
Reversible stochastic attribute-value grammars (de Kok et al., 2011) use one model for parse disambiguation and ﬂuency ranking. Such a model encodes preferences with respect to syntax, ﬂuency, and appropriateness of logical forms, as weighted features. Reversible models are built on the premise that syntactic preferences are shared between parse disambiguation and ﬂuency ranking. Given that reversible models also use features that are speciﬁc to parsing or generation, there is the possibility that the model is trained to rely on these directional features. If this is true, the premise that preferences are shared between parse disambiguation and ﬂuency ranking does not hold. In this work, we compare and apply feature selection techniques to extract the most discriminative features from directional and reversible models. We then analyse the contributions of different classes of features, and show that reversible models do rely on task-independent features. 
Recent research on multilingual statistical machine translation (SMT) focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs. This paper proposes a new method to translate a dialect language into a foreign language by integrating transliteration approaches based on Bayesian co-segmentation (BCS) models with pivot-based SMT approaches. The advantages of the proposed method with respect to standard SMT approaches are three fold: (1) it uses a standard language as the pivot language and acquires knowledge about the relation between dialects and the standard language automatically, (2) it reduces the translation task complexity by using monotone decoding techniques, (3) it reduces the number of features in the log-linear model that have to be estimated from bilingual data. Experimental results translating four Japanese dialects (Kumamoto, Kyoto, Okinawa, Osaka) into four Indo-European languages (English, German, Russian, Hindi) and two Asian languages (Chinese, Korean) revealed that the proposed method improves the translation quality of dialect translation tasks and outperforms standard pivot translation approaches concatenating SMT engines for the majority of the investigated language pairs. 
This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrasebased SMT system. This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative). A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words. 
Small, manually assembled corpora may be available for less dominant languages and dialects, but producing web-scale resources remains a challenge. Even when considerable quantities of text are present on the web, ﬁnding this text, and distinguishing it from related languages in the same region can be difﬁcult. For example less dominant variants of English (e.g. New Zealander, Singaporean, Canadian, Irish, South African) may be found under their respective national domains, but will be partially mixed with Englishes of the British and US varieties, perhaps through syndication of journalism, or the local reuse of text by multinational companies. Less formal dialectal usage may be scattered more widely over the internet through mechanisms such as wiki or blog authoring. Here we automatically construct a corpus of Hiberno-English (English as spoken in Ireland) using a variety of methods: ﬁltering by national domain, ﬁltering by orthographic conventions, and bootstrapping from a set of Irelandspeciﬁc terms (slang, place names, organisations). We evaluate the national speciﬁcity of the resulting corpora by measuring the incidence of topical terms, and several grammatical constructions that are particular to Hiberno-English. The results show that domain ﬁltering is very effective for isolating text that is topic-speciﬁc, and orthographic classiﬁcation can exclude some non-Irish texts, but that selected seeds are necessary to extract considerable quantities of more informal, dialectal text. 
While most dialectological research so far focuses on phonetic and lexical phenomena, we use recent ﬁeldwork in the domain of dialect syntax to guide the development of multidialectal natural language processing tools. In particular, we develop a set of rules that transform Standard German sentence structures into syntactically valid Swiss German sentence structures. These rules are sensitive to the dialect area, so that the dialects of more than 300 towns are covered. We evaluate the transformation rules on a Standard German treebank and obtain accuracy ﬁgures of 85% and above for most rules. We analyze the most frequent errors and discuss the beneﬁt of these transformations for various natural language processing tasks. 
This paper explores two different methods of learning dialectal morphology from a small parallel corpus of standard and dialect-form text, given that a computational description of the standard morphology is available. The goal is to produce a model that translates individual lexical dialectal items to their standard dialect counterparts in order to facilitate dialectal use of available NLP tools that only assume standard-form input. The results show that a learning method based on inductive logic programming quickly converges to the correct model with respect to many phonological and morphological differences that are regular in nature. 
In this paper we describe a novel feature discovery technique that can be used to model stylistic variation in sociolects. While structural features offer much in terms of expressive power over simpler features used more frequently in machine learning approaches to modeling linguistic variation, they frequently come at an excessive cost in terms of feature space size expansion. We propose a novel form of structural features referred to as “stretchy patterns” that strike a balance between expressive power and compactness in order to enable modeling stylistic variation with reasonably small datasets. As an example we focus on the problem of modeling variation related to gender in personal blogs. Our evaluation demonstrates a significant improvement over standard baselines. 
We explore variability involved in speech with a non-native accent. We first employ a combination of knowledge-based and datadriven approaches for the analysis of pronunciation variants between L1 (German) and target L2 (Slovak). Knowledge gained in this two-step process is then used in adapting acoustic models and the lexicon. We focus on modifications in the pronunciation dictionary and speech rate. Our results show that the recognition of German-accented Slovak is significantly improved with techniques modeling slow L2 speech, and that the adaptation of the pronunciation dictionary yields only insignificant gains. 
This paper describes a method for selecting an appropriate phone set in dialect speech synthesis for a so far undescribed dialect by applying hidden Markov model (HMM) based training and clustering methods. In this pilot study we show how a phone set derived from the phonetic surface can be optimized given a small amount of dialect speech training data. 
This paper reports the results of the WordNet.PTglobal project, an extension of WordNet.PT to all Portuguese varieties. Profiting from a theoretical model of high level explanatory adequacy and from a convenient and flexible development tool, WordNet.PTglobal achieves a rich and multipurpose lexical resource, suitable for contrastive studies and for a vast range of language-based applications covering all Portuguese varieties. 
We introduce BLESS, a data set speciﬁcally designed for the evaluation of distributional semantic models. BLESS contains a set of tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples. It is thus possible to assess the ability of a model to detect truly related word pairs, as well as to perform in-depth analyses of the types of semantic relations that a model favors. We discuss the motivations for BLESS, describe its construction and structure, and present examples of its usage in the evaluation of distributional semantic models. 
Unsupervised methods of semantic relations extraction rely on a similarity measure between lexical units. Similarity measures differ both in kinds of information they use and in the ways how this information is transformed into a similarity score. This paper is making a step further in the evaluation of the available similarity measures within the context of semantic relation extraction. We compare 21 baseline measures – 8 knowledge-based, 4 corpus-based, and 9 web-based metrics with the BLESS dataset. Our results show that existing similarity measures provide signiﬁcantly different results, both in general performances and in relation distributions. We conclude that the results suggest developing a combined similarity measure. 
We present a distributional semantic model combining text- and image-based features. We evaluate this multimodal semantic model on simulating similarity judgments, concept clustering and the BLESS benchmark. When integrated with the same core text-based model, image-based features are at least as good as further text-based features, and they capture different qualitative aspects of the tasks, suggesting that the two sources of information are complementary. 
This paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases. Raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivotbased methods. Our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a signiﬁcantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does. The results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection. 
Distributional approaches are based on a simple hypothesis: the meaning of a word can be inferred from its usage. The application of that idea to the vector space model makes possible the construction of a WordSpace in which words are represented by mathematical points in a geometric space. Similar words are represented close in this space and the deﬁnition of “word usage” depends on the deﬁnition of the context used to build the space, which can be the whole document, the sentence in which the word occurs, a ﬁxed window of words, or a speciﬁc syntactic context. However, in its original formulation WordSpace can take into account only one deﬁnition of context at a time. We propose an approach based on vector permutation and Random Indexing to encode several syntactic contexts in a single WordSpace. Moreover, we propose some operations in this space and report the results of an evaluation performed using the GEMS 2011 Shared Evaluation data. 
We present a distributional vector space model that incorporates Latent Dirichlet Allocation in order to capture the semantic relation holding between adjectives and nouns along interpretable dimensions of meaning: The meaning of adjective-noun phrases is characterized in terms of ontological attributes that are prominent in their compositional semantics. The model is evaluated in a similarity prediction task based on paired adjective-noun phrases from the Mitchell and Lapata (2010) benchmark data. Comparing our model against a high-dimensional latent word space, we observe qualitative differences that shed light on different aspects of similarity conveyed by both models and suggest integrating their complementary strengths. 
Formal and distributional semantic models offer complementary beneﬁts in modeling meaning. The categorical compositional distributional model of meaning of Coecke et al. (2010) (abbreviated to DisCoCat in the title) combines aspects of both to provide a general framework in which meanings of words, obtained distributionally, are composed using methods from the logical setting to form sentence meaning. Concrete consequences of this general abstract setting and applications to empirical data are under active study (Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011). In this paper, we extend this study by examining transitive verbs, represented as matrices in a DisCoCat. We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). 
This paper presents a novel approach for automatic detection of semantic change of words based on distributional similarity models. We show that the method obtains good results with respect to a reference ranking produced by human raters. The evaluation also analyzes the performance of frequency-based methods, comparing them to the similarity method proposed. 
Reading comprehension activities are an authentic task including a rich, language-based context, which makes them an interesting reallife challenge for research into automatic content analysis. For textual entailment research, content assessment of reading comprehension exercises provides an interesting opportunity for extrinsic, real-purpose evaluation, which also supports the integration of context and task information into the analysis. In this paper, we discuss the ﬁrst results for content assessment of reading comprehension activities for German and present results which are competitive with the current state of the art for English. Diving deeper into the results, we provide an analysis in terms of the different question types and the ways in which the information asked for is encoded in the text. We then turn to analyzing the role of the question and argue that the surface-based account of information that is given in the question should be replaced with a more sophisticated, linguistically informed analysis of the information structuring of the answer in the context of the question that it is a response to. 
While modeling entailment at the lexical-level is a prominent task, addressed by most textual entailment systems, it has been approached mostly by heuristic methods, neglecting some of its important aspects. We present a probabilistic approach for this task which covers aspects such as differentiating various resources by their reliability levels, considering the length of the entailed sentence, the number of its covered terms and the existence of multiple evidence for the entailment of a term. The impact of our model components is validated by evaluations, which also show that its performance is in line with the best published entailment systems. 
This paper addresses context matching in textual inference. We formulate the task under the Contextual Preferences framework which broadly captures contextual aspects of inference. We propose a generic classiﬁcationbased scheme under this framework which coherently attends to context matching in inference and may be employed in any inferencebased task. As a test bed for our scheme we use the Name-based Text Categorization (TC) task. We deﬁne an integration of Contextual Preferences into the TC setting and present a concrete self-supervised model which instantiates the generic scheme and is applied to address context matching in the TC task. Experiments on standard TC datasets show that our approach outperforms the state of the art in context modeling for Name-based TC. 
We address two issues related to the development of systems for Recognizing Textual Entailment. The ﬁrst is the impossibility to capitalize on lessons learned over the different datasets available, due to the changing nature of traditional RTE evaluation settings. The second is the lack of simple ways to assess the results achieved by our system on a given training corpus, and ﬁgure out its real potential on unseen test data. Our contribution is the extension of an open-source RTE package with an automatic way to explore the large search space of possible conﬁgurations, in order to select the most promising one over a given dataset. From the developers’ point of view, the efﬁciency and ease of use of the system, together with the good results achieved on all previous RTE datasets, represent a useful support, providing an immediate term of comparison to position the results of their approach. 
Common evaluation metrics for paraphrase patterns do not necessarily correlate with extrinsic recognition task performance. We propose a metric which gives weight to lexical variety in paraphrase patterns; our proposed metric has a positive correlation with paraphrase recognition task performance, with a Pearson correlation of 0.5~0.7 (k=10, with “strict” judgment) in a statistically significant level (p-value<0.01). 
Ambiguities are ubiquitous in natural language and pose a major challenge for the automatic interpretation of natural language expressions. In this paper we focus on different types of lexical ambiguities that play a role in the context of ontology-based question answering, and explore strategies for capturing and resolving them. We show that by employing underspeciﬁcation techniques and by using ontological reasoning in order to ﬁlter out inconsistent interpretations as early as possible, the overall number of interpretations can be effectively reduced by 44 %. 
Intervals and the events that occur in them are encoded as strings, elaborating on a conception of events as “intervals cum description.” Notions of satisfaction in interval temporal logics are formulated in terms of strings, and the possibility of computing these via ﬁnite-state machines/transducers is investigated. This opens up temporal semantics to ﬁnite-state methods, with entailments that are decidable insofar as these can be reduced to inclusions between regular languages.  
Reasoning about ordinary human situations and activities requires the availability of diverse types of knowledge, including expectations about the probable results of actions and the lexical entailments for many predicates. We describe initial work to acquire such a collection of conditional (if–then) knowledge by exploiting presuppositional discourse patterns (such as ones involving ‘but’, ‘yet’, and ‘hoping to’) and abstracting the matched material into general rules. 
Aphasia treatment for the recovery of lost communication functionalities is possible through frequent and intense speech therapy sessions. In this sense, speech and language technology may provide important support in improving the recovery process. The aim of the project Vithea (Virtual Therapist for Aphasia Treatment) is to develop an on-line system designed to behave as a virtual therapist, guiding the patient in performing training exercises in a simple and intuitive fashion. In this paper, the fundamental components of the Vithea system are presented, with particular emphasis on the speech recognition module. Furthermore, we report encouraging automatic word naming recognition results using data collected from speech therapy sessions. 
This paper describes modiﬁcations to acoustic speech signals produced by speakers with dysarthria in order to make those utterances more intelligible to typical listeners. These modiﬁcations include the correction of tempo, the adjustment of formant frequencies in sonorants, the removal of aberrant voicing, the deletion of phoneme insertion errors, and the replacement of erroneously dropped phonemes. Through simple evaluations of intelligibility with na¨ıve listeners, we show that the correction of phoneme errors results in the greatest increase in intelligibility and is therefore a desirable mechanism for the eventual creation of augmentative application software for individuals with dysarthria. 
In this paper, we examine the idea of technology-assisted co-construction, where the communication partner of an AAC user can make guesses about the intended messages, which are included in the user’s word completion/prediction interface. We run some human trials to simulate this new interface concept, with subjects predicting words as the user’s intended message is being generated in real time with speciﬁed typing speeds. Results indicate that people can provide substantial keystroke savings by providing word completion or prediction, but that the savings are not as high as n-gram language models. Interestingly, the language model and human predictions are complementary in certain key ways – humans doing a better job in some circumstances on contextually salient nouns. We discuss implications of the enhanced coconstruction interface for real-time message generation in AAC direct selection devices. 
Advances in natural language generation and speech processing techniques, combined with changes in the commercial landscape, have brought within reach dramatic improvements in Augmentative Alternative Communication (AAC). These improvements, though overwhelmingly positive, amplify a family of personal data use problems. This paper argues that the AAC design and implementation process needs to identify and address personal data use problems. Accordingly, this paper explores personal data management problems and proposes responses. This paper is situated in the context of AAC technology but the responses could be generalised for other communities affected by low digital literacy, low literacy levels and cognitive challenges. 
In this paper, we examine several methods for including dynamic, contextually-sensitive binary codes within indirect selection typing methods using a grid with ﬁxed symbol positions. Using Huffman codes derived from a character n-gram model, we investigate both synchronous (ﬁxed latency highlighting) and asynchronous (self-paced using long versus short press) scanning. Additionally, we look at methods that allow for scanning past a target and returning to it versus methods that remove unselected items from consideration. Finally, we investigate a novel method for displaying the binary codes for each symbol to the user, rather than using cell highlighting, as the means for identifying the required input sequence for the target symbol. We demonstrate that dynamic coding methods for ﬁxed position grids can be tailored for very diverse user requirements. 
This paper describes our work on improving access to the content of multimodal documents containing line graphs in popular media for people with visual impairments. We provide an overview of our implemented system, including our method for recognizing and conveying the intended message of a line graph. The textual description of the graphic generated by our system is presented at the most relevant point in the document. We also describe ongoing work into obtaining additional propositions that elaborate on the intended message, and examine the potential beneﬁts of analyzing the text and graphical content together in order to extend our system to produce summaries of entire multimodal documents. 
This paper describes the integration of commonly used screen readers, namely, NVDA [NVDA 2011] and ORCA [ORCA 2011] with Text to Speech (TTS) systems for Indian languages. A participatory design approach was followed in the development of the integrated system to ensure that the expectations of visually challenged people are met. Given that India is a multilingual country (22 ofﬁcial languages), a uniform framework for an integrated text-to-speech synthesis systems with screen readers across six Indian languages are developed, which can be easily extended to other languages as well. Since Indian languages are syllable centred, syllable-based concatenative speech synthesizers are built. This paper describes the development and evaluation of syllable-based Indian language Text-To-Speech (TTS) synthesis system (around festival TTS) with ORCA and NVDA, for Linux and Windows environments respectively. TTS systems for six Indian Languages, namely, Hindi, Tamil, Marathi, Bengali, Malayalam and Telugu were built. Usability studies of the screen readers were performed. The system usability was evaluated by a group of visually challenged people based on a questionnaire provided to them. And a Mean Opinion Score(MoS) of 62.27% was achieved. 
In this paper, we propose a new approach to readability assessment with a speciﬁc view to the task of text simpliﬁcation: the intended audience includes people with low literacy skills and/or with mild cognitive impairment. READ–IT represents the ﬁrst advanced readability assessment tool for what concerns Italian, which combines traditional raw text features with lexical, morpho-syntactic and syntactic information. In READ–IT readability assessment is carried out with respect to both documents and sentences where the latter represents an important novelty of the proposed approach creating the prerequisites for aligning the readability assessment step with the text simpliﬁcation process. READ–IT shows a high accuracy in the document classiﬁcation task and promising results in the sentence classiﬁcation scenario. 
This paper describes a categorization module for improving the performance of a Spanish into Spanish Sign Language (LSE) translation system. This categorization module replaces Spanish words with associated tags. When implementing this module, several alternatives for dealing with non-relevant words have been studied. Nonrelevant words are Spanish words not relevant in the translation process. The categorization module has been incorporated into a phrase-based system and a Statistical Finite State Transducer (SFST). The evaluation results reveal that the BLEU has increased from 69.11% to 78.79% for the phrase-based system and from 69.84% to 75.59% for the SFST. Keywords: Source language categorization, Speech into Sign Language translation. Lengua de Signos Española (LSE). 
Communication is an essential part of our life. Though, not only communication is the key – it is all about emotional (prosodic) communication. Due to empirical research, people, who are augmentative communicators and speak with a voice output communication aid, want to express their emotions in the same way as everybody else – it is one of their deepest interests (Portnuff, 2006; Hoffmann and Wülfing, 2010). So far, current devices lack the opportunity of emotional utterances. This circumstance leads not only to a huge usability deficit, but furthermore, it is an obstacle to develop emotional competence and to behave as well as regulate one´s emotion adequately (Blackstone and Wilkins, 2009). This article aims to increase the sensitivity for the importance of emotional communication. Furthermore, it tries to give first suggestions for implementing an usable device that supports users with a voice output communication aid to express emotional utterances. This could be done by using phrase-generation, as mentiond by Vanderheyden and Pennigton (1998). 
This paper describes a machine translation system that offers many deaf and hearingimpaired people the chance to access published information in Arabic by translating text into their ﬁrst language, Arabic Sign Language (ArSL). The system was created under the close guidance of a team that included three deaf native signers and one ArSL interpreter. We discuss problems inherent in the design and development of such translation systems and review previous ArSL machine translation systems, which all too often demonstrate a lack of collaboration between engineers and the deaf community. We describe and explain in detail both the adapted translation approach chosen for the proposed system and the ArSL corpus that we collected for this purpose. The corpus has 203 signed sentences (with 710 distinct signs) with content restricted to the domain of instructional language as typically used in deaf education. Evaluation shows that the system produces translated sign sentences outputs with an average word error rate of 46.7% and an average position error rate of 29.4% using leave-oneout cross validation. The most frequent source of errors is missing signs in the corpus; this could be addressed in future by collecting more corpus material. 
 This paper describes an ongoing project where we develop and evaluate a setup involving a communication board and a toy robot, which can communicate with each other via synthesised speech. The purpose is to provide children with communicative disabilities with a toy that is fun and easy to use together with peers, with and without disabilities. When the child selects a symbol on the communication board, the board speaks and the robot responds. This encourages the child to use language and learn to cooperate to reach a common goal. Throughout the project, three children with cerebral palsy and their peers use the robot and provide feedback for further development. The multimodal interaction with the robot is video recorded and analysed together with observational data in activity diaries.  Figure 1: The robot and the communication board and play with each other. As a side eﬀect this can also help them develop their communicative skills. We are developing a remote-controlled robot that can be used by children with severe phys-  
 symbol databases by combining separate basic  A corpus of easy-to-read texts in combination with a base vocabulary pool for Swedish was used in order to build a basic vocabulary. The coverage of these entries by symbols in an existing AAC database was then assessed. We ﬁnally suggest a method for enriching the expressive power of the AAC language by combining existing symbols and in this way illustrate additional concepts.  words from the vocabulary into compounds. 2 Background A fundamental aspect for participation in the society is the possibility to acquire information and to communicate. For the majority of citizens, getting information on every-day issues is hardly a task entailing any speciﬁc problems. There is, however,  
 Lack of ability to understand numerical informa-  tion is an even greater problem than poor literacy.  Numerical information is very common in all kinds of documents from newspapers and magazines to household bills and wage slips. However, many people ﬁnd it difﬁcult to understand, particularly people with poor education and disabilities. Sometimes numerical information is presented with hedges that modify the meaning. A numerical hedge is a word or phrase employed to indicate explicitly that  A U.K. Government Survey in 2003 estimated that 6.8 million adults had insufﬁcient numeracy skills to perform simple everyday tasks such as paying house-hold bills and understanding wage slips, and 23.8 million adults would be unable to achieve grade C in the GCSE maths examination for 16 year-old school children (Williams et al., 2003). A ﬁrst possible approach to solve this impor-  some loss of precision has taken place (e.g., “around”) and it may also indicate the direction of approximation (e.g., “more than”). This paper presents a study of the use of numerical hedges that is part of research investigating the process of rewriting difﬁcult numerical expressions in simpler ways. We carried out a survey in which experts in numer-  tant social problem is making numerical information accessible by rewriting difﬁcult numerical expressions using alternative wordings that are easier to understand. Some loss of precision could have positive advantages for numerate people as well as less numerate. Such an approach would require a set of rewriting strategies yielding expressions that  acy were asked to simplify a range of proportion expressions and analysed the results to obtain guidelines for automating the simpliﬁcation task.  are linguistically correct, easier to understand than the original, and as close as possible to the original meaning. In rewriting, hedges play an important role. For  example,“50.9%” could be rewritten as “just over  
The Simple English Wikipedia provides a simplified version of Wikipedia's English articles for readers with special needs. However, there are fewer efforts to make information in Wikipedia in other languages accessible to a large audience. This work proposes the use of a syntactic simplification engine with high precision rules to automatically generate a Simple Portuguese Wikipedia on demand, based on user interactions with the main Portuguese Wikipedia. Our estimates indicated that a human can simplify about 28,000 occurrences of analysed patterns per million words, while our system can correctly simplify 22,200 occurrences, with estimated f-measure 77.2%. 
We present a nonparametric Bayesian approach to extract a structured database of entities from text. Neither the number of entities nor the ﬁelds that characterize each entity are provided in advance; the only supervision is a set of ﬁve prototype examples. Our method jointly accomplishes three tasks: (i) identifying a set of canonical entities, (ii) inferring a schema for the ﬁelds that describe each entity, and (iii) matching entities to their references in raw text. Empirical evaluation shows that the approach learns an accurate database of entities and a sensible model of name structure. 
The unsupervised Data Oriented Parsing (uDOP) approach has been repeatedly reported to achieve state of the art performance in experiments on parsing of different corpora. At the same time the approach is demanding both in computation time and memory. This paper describes an approach which decreases these demands. First the problem is translated into the generation of probabilistic bottom up tree automata (pBTA). Then it is explained how solving two standard problems for these automata results in a reduction in the size of the grammar. The reduction of the grammar size by using eﬃcient algorithms for pBTAs is the main contribution of this paper. Experiments suggest that this leads to a reduction in grammar size by a factor of 2. This paper also suggests some extensions of the original uDOP algorithm that are made possible or aided by the use of tree automata. 
The development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research. The primary advantage of these methods is that they do not require annotated data to learn a model. However, this advantage makes them difﬁcult to evaluate against a manually labeled gold standard. Using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods. Instead, we argue that the rarely used in-context evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied. Finally, bearing the issue of evaluation in mind, we propose directions for future work in unsupervised natural language processing. 
The automatic generation of entity proﬁles from unstructured text, such as Knowledge Base Population, if applied in a multi-lingual setting, generates the need to align such proﬁles from multiple languages in an unsupervised manner. This paper describes an unsupervised and language-independent approach to mine name translation pairs from entity proﬁles, using Wikipedia Infoboxes as a stand-in for high quality entity proﬁle extraction. Pairs are initially found using expressions that are written in language-independent forms (such as dates and numbers), and new translations are then mined from these pairs. The algorithm then iteratively bootstraps from these translations to learn more pairs and more translations. The algorithm maintains a high precision, over 95%, for the majority of its iterations, with a slightly lower precision of 85.9% and an f-score of 76%. A side effect of the name mining algorithm is the unsupervised creation of a translation lexicon between the two languages, with an accuracy of 64%. We also duplicate three state-of-the-art name translation mining methods and use two existing name translation gazetteers to compare with our approach. Comparisons show our approach can effectively augment the results from each of these alternative methods and resources. 
There is high demand for automated tools that assign polarity to microblog content such as tweets (Twitter posts), but this is challenging due to the terseness and informality of tweets in addition to the wide variety and rapid evolution of language in Twitter. It is thus impractical to use standard supervised machine learning techniques dependent on annotated training examples. We do without such annotations by using label propagation to incorporate labels from a maximum entropy classiﬁer trained on noisy labels and knowledge about word types encoded in a lexicon, in combination with the Twitter follower graph. Results on polarity classiﬁcation for several datasets show that our label propagation approach rivals a model supervised with in-domain annotated tweets, and it outperforms the noisily supervised classiﬁer it exploits as well as a lexicon-based polarity ratio classiﬁer. 
In this paper, we give a treatment to the problem of bilingual part-of-speech induction with parallel data. We demonstrate that na¨ıve optimization of log-likelihood with joint MRFs suffers from a severe problem of local maxima, and suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 
Training efﬁcient statistical approaches for natural language understanding generally requires data with segmental semantic annotations. Unfortunately, building such resources is costly. In this paper, we propose an approach that produces annotations in an unsupervised way. The ﬁrst step is an implementation of latent Dirichlet allocation that produces a set of topics with probabilities for each topic to be associated with a word in a sentence. This knowledge is then used as a bootstrap to infer a segmentation of a word sentence into topics using either integer linear optimisation or stochastic word alignment models (IBM models) to produce the ﬁnal semantic annotation. The relation between automaticallyderived topics and task-dependent concepts is evaluated on a spoken dialogue task with an available reference annotation. 
The amount of data produced in usergenerated content continues to grow at a staggering rate. However, the text found in these media can deviate wildly from the standard rules of orthography, syntax and even semantics and present signiﬁcant problems to downstream applications which make use of this noisy data. In this paper we present a novel unsupervised method for extracting domainspeciﬁc lexical variants given a large volume of text. We demonstrate the utility of this method by applying it to normalize text messages found in the online social media service, Twitter, into their most likely standard English versions. Our method yields a 20% reduction in word error rate over an existing state-of-theart approach. 
Recent years’ most efﬁcient approaches for language understanding are statistical. These approaches beneﬁt from a segmental semantic annotation of corpora. To reduce the production cost of such corpora, this paper proposes a method that is able to match ﬁrst identiﬁed concepts with word sequences in an unsupervised way. This method based on automatic alignment is used by an understanding system based on conditional random ﬁelds and is evaluated on a spoken dialogue task using either manual or automatic transcripts. 
Resolving ambiguity associated with names found on the Web, Wikipedia or medical texts is a very challenging task, which has been of great interest to the research community. We propose a novel approach to disambiguating names using Latent Dirichlet Allocation, where the learned topics represent the underlying senses of the ambiguous name. We conduct a detailed evaluation on multiple data sets containing ambiguous person, location and organization names and for multiple languages such as English, Spanish, Romanian and Bulgarian. We conduct comparative studies with existing approaches and show a substantial improvement of 15 to 35% in task accuracy. 
We present a pilot study on an evaluation method which is able to rank translation outputs with no reference translation, given only their source sentence. The system employs a statistical classiﬁer trained upon existing human rankings, using several features derived from analysis of both the source and the target sentences. Development experiments on one language pair showed that the method has considerably good correlation with human ranking when using features obtained from a PCFG parser. 
This paper proposes a new automatic machine translation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties, and some text processing variants. There is very little linguistic information in AMBER. We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 
 2 TESLA-M  This paper describes the submission from the National University of Singapore to the WMT 2011 Shared Evaluation Task and the Tunable Metric Task. Our entry is TESLA in three different conﬁgurations: TESLA-M, TESLA-F, and the new TESLA-B. 
This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system. 
SemPOS is an automatic metric of machine translation quality for Czech and English focused on content words. It correlates well with human judgments but it is computationally costly and hard to adapt to other languages because it relies on a deep-syntactic analysis of the system output and the reference. To remedy this, we attempt at approximating SemPOS using only tagger output and a few heuristics. At a little expense in correlation to human judgments, we can evaluate MT systems much faster. Additionally, we describe our submission to the Tunable Metrics Task in WMT11. 
We describe our submissions to the WMT11 shared MT evaluation task: MTeRater and MTeRater-Plus. Both are machine-learned metrics that use features from e-rater R , an automated essay scoring engine designed to assess writing proﬁciency. Despite using only features from e-rater and without comparing to translations, MTeRater achieves a sentencelevel correlation with human rankings equivalent to BLEU. Since MTeRater only assesses ﬂuency, we build a meta-metric, MTeRaterPlus, that incorporates adequacy by combining MTeRater with other MT evaluation metrics and heuristics. This meta-metric has a higher correlation with human rankings than either MTeRater or individual MT metrics alone. However, we also ﬁnd that e-rater features may not have signiﬁcant impact on correlation in every case. 
We describe TINE, a new automatic evaluation metric for Machine Translation that aims at assessing segment-level adequacy. Lexical similarity and shallow-semantics are used as indicators of adequacy between machine and reference translations. The metric is based on the combination of a lexical matching component and an adequacy component. Lexical matching is performed comparing bagsof-words without any linguistic annotation. The adequacy component consists in: i) using ontologies to align predicates (verbs), ii) using semantic roles to align predicate arguments (core arguments and modiﬁers), and iii) matching predicate arguments using distributional semantics. TINE’s performance is comparable to that of previous metrics at segment level for several language pairs, with average Kendall’s tau correlation from 0.26 to 0.29. We show that the addition of the shallow-semantic component improves the performance of simple lexical matching strategies and metrics such as BLEU. 
Automatic evaluation metrics are fundamentally important for Machine Translation, allowing comparison of systems performance and efﬁcient training. Current evaluation metrics fall into two classes: heuristic approaches, like BLEU, and those using supervised learning trained on human judgement data. While many trained metrics provide a better match against human judgements, this comes at the cost of including lots of features, leading to unwieldy, non-portable and slow metrics. In this paper, we introduce a new trained metric, ROSE, which only uses simple features that are easy portable and quick to compute. In addition, ROSE is sentence-based, as opposed to document-based, allowing it to be used in a wider range of settings. Results show that ROSE performs well on many tasks, such as ranking system and syntactic constituents, with results competitive to BLEU. Moreover, this still holds when ROSE is trained on human judgements of translations into a different language compared with that use in testing. 
The past few years have seen an increasing interest in using Amazon’s Mechanical Turk for purposes of collecting data and performing annotation tasks. One such task is the mass evaluation of system output in a variety of tasks. In this paper, we present MAISE, a package that allows researchers to evaluate the output of their AI system(s) using human judgments collected via Amazon’s Mechanical Turk, greatly streamlining the process. MAISE is open source, easy to run, and platform-independent. The core of MAISE’s codebase was used for the manual evaluation of WMT10, and the completed package is being used again in the current evaluation for WMT11. In this paper, we describe the main features, functionality, and usage of MAISE, which is now available for download and use. 
 2 System description  This paper describes the development operated into MANY for the 2011 WMT system combination evaluation campaign. Hypotheses from French/English and English/French MT systems were combined with a new version of MANY, an open source system combination software based on confusion networks decoding currently developed at LIUM. MANY has been updated in order to optimize decoder parameters with MERT, which proves to ﬁnd better weights. The system combination yielded signiﬁcant improvements in BLEU score when applied on system combination data from two languages. 
 2 Minimum Bayes risk Decoding  This paper presents the submissions of the pattern recognition and human language technology (PRHLT) group to the system combination task of the sixth workshop on statistical machine translation (WMT 2011). Each submissions is generated by a multi-system minimum Bayes risk (MBR) technique. Our technique uses the MBR decision rule and a linear combination of the component systems’ probability distributions to search for the minimum risk translation among all the sentences in the target language.  SMT can be described as a mapping of a word sequence f in a source language to a word sequence e in a target language; this mapping is produced by the MT decoder D(f ). If the reference translation e is known, the decoder performance can be measured by the loss function L(e, D(f )). Given such a loss function L(e, e ) between an automatic translation e and a reference e, and an underlying probability model P (e|f ), MBR decoding has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004):  
This paper describes our submissions, cmu-heafield-combo, to the ten tracks of the 2011 Workshop on Machine Translation’s system combination task. We show how the combination scheme operates by ﬂexibly aligning system outputs then searching a space constructed from the alignments. Humans judged our combination the best on eight of ten tracks. 
RWTH participated in the System Combination task of the Sixth Workshop on Statistical Machine Translation (WMT 2011). For three language pairs, we combined 6 to 14 systems into a single consensus translation. A three-level metacombination scheme combining six different system combination setups with three different engines was applied on the French–English language pair. Depending on the language pair, improvements versus the best single system are in the range of +1.9% and +2.5% abs. on BLEU, and between −1.8% and −2.4% abs. on TER. Novel techniques compared with RWTH’s submission to WMT 2010 include two additional system combination engines, an additional word alignment technique, meta combination, and additional optimization techniques. 
BBN submitted system combination outputs for Czech-English, German-English, SpanishEnglish, and French-English language pairs. All combinations were based on confusion network decoding. The confusion networks were built using incremental hypothesis alignment algorithm with ﬂexible matching. A novel bi-gram count feature, which can penalize bi-grams not present in the input hypotheses corresponding to a source sentence, was introduced in addition to the usual decoder features. The system combination weights were tuned using a graph based expected BLEU as the objective function while incrementally expanding the networks to bi-gram and 5-gram contexts. The expected BLEU tuning described in this paper naturally generalizes to hypergraphs and can be used to optimize thousands of weights. The combination gained about 0.5-4.0 BLEU points over the best individual systems on the ofﬁcial WMT11 language pairs. A 39 system multisource combination achieved an 11.1 BLEU point gain. 
This paper describes the JHU system combination scheme used in WMT-11. The JHU system combination is based on confusion network alignment, and inherited the framework developed by (Karakos et al., 2008). We improved our core system combination algorithm by making use of TER-plus, which was originally designed for string alignment, for alignment of confusion networks. Experimental results on French-English, GermanEnglish, Czech-English and Spanish-English combination tasks show signiﬁcant improvements on BLEU and TER by up to 2 points on average, compared to the best individual system output, and improvements compared with the results produced by ITG which we used in WMT-10. 
We consider using online language models for translating multiple streams which naturally arise on the Web. After establishing that using just one stream can degrade translations on different domains, we present a series of simple approaches which tackle the problem of maintaining translation performance on all streams in small space. By exploiting the differing throughputs of each stream and how the decoder translates prior test points from each stream, we show how translation performance can equal specialised, per-stream language models, but do this in a single language model using far less space. Our results hold even when adding three billion tokens of additional text as a background language model. 
We present KenLM, a library that implements two data structures for efﬁcient language model queries, reducing both time and memory costs. The PROBING data structure uses linear probing hash tables and is designed for speed. Compared with the widelyused SRILM, our PROBING model is 2.4 times as fast while using 57% of the memory. The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source1, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. 
In past Evaluations for Machine Translation of European Languages, it could be shown that the translation performance of SMT systems can be increased by integrating a bilingual language model into a phrase-based SMT system. In the bilingual language model, target words with their aligned source words build the tokens of an n-gram based language model. We analyzed the effect of bilingual language models and show where they could help to better model the translation process. We could show improvements of translation quality on German-to-English and Arabic-to-English. In addition, for the Arabic-to-English task, training an extra bilingual language model on the POS tags instead of the surface word forms led to further improvements. 
We describe an approach for generating a ranked list of candidate document translation pairs without the use of bilingual dictionary or machine translation system. We developed this approach as an initial, filtering step, for extracting parallel text from large, multilingual—but non-parallel— corpora. We represent bilingual documents in a vector space whose basis vectors are the overlapping tokens found in both languages of the collection. Using this representation, weighted by tf·idf, we compute cosine document similarity to create a ranked list of candidate document translation pairs. Unlike cross-language information retrieval, where a ranked list in the target language is evaluated for each source query, we are interested in, and evaluate, the more difficult task of finding translated document pairs. We first perform a feasibility study of our approach on parallel collections in multiple languages, representing multiple language families and scripts. The approach is then applied to a large bilingual collection of around 800k books. To avoid the computational cost of O(n2 ) document pair comparisons, we employ locality sensitive hashing (LSH) approximation algorithm for cosine similarity, which reduces our time complexity to O(n log n) .  
Languages with rich inﬂectional morphology pose a difﬁcult challenge for statistical machine translation. To address the problem of morphologically inconsistent output, we add uniﬁcation-based constraints to the target-side of a string-to-tree model. By integrating constraint evaluation into the decoding process, implausible hypotheses can be penalised or ﬁltered out during search. We use a simple heuristic process to extract agreement constraints for German and test our approach on an English-German system trained on WMT data, achieving a small improvement in translation accuracy as measured by BLEU.  
The quality of Arabic-English statistical machine translation often suffers as a result of standard phrase-based SMT systems’ inability to perform long-range re-orderings, specifically those needed to translate VSO-ordered Arabic sentences. This problem is further exacerbated by the low performance of Arabic parsers on subject and subject span detection. In this paper, we present two parse “fuzziﬁcation” techniques which allow the translation system to select among a range of possible S–V re-orderings. With this approach, we demonstrate a 0.3-point improvement in BLEU score (69% of the maximum possible using gold parses), and a corresponding improvement in the percentage of syntactically well-formed subjects under a manual evaluation. 
Paraphrases are useful for statistical machine translation (SMT) and natural language processing tasks. Distributional paraphrase generation is independent of parallel texts and syntactic parses, and hence is suitable also for resource-poor languages, but tends to erroneously rank antonyms, trend-contrasting, and polarity-dissimilar candidates as good paraphrases. We present here a novel method for improving distributional paraphrasing by ﬁltering out such candidates. We evaluate it in simulated low and mid-resourced SMT tasks, translating from English to two quite different languages. We show statistically signiﬁcant gains in English-to-Chinese translation quality, up to 1 BLEU from nonﬁltered paraphrase-augmented models (1.6 BLEU from baseline). We also show that yielding gains in translation to Arabic, a morphologically rich language, is not straightforward. 
In many languages the use of compound words is very productive. A common practice to reduce sparsity consists in splitting compounds in the training data. When this is done, the system incurs the risk of translating components in non-consecutive positions, or in the wrong order. Furthermore, a post-processing step of compound merging is required to reconstruct compound words in the output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order. We also propose new heuristic methods for merging components that outperform all known methods, and a learning-based method that has similar accuracy as the heuristic method, is better at producing novel compounds, and can operate with no background linguistic resources. 
Statistical machine translation systems are normally optimised for a chosen gain function (metric) by using MERT to ﬁnd the best model weights. This algorithm suffers from stability problems and cannot scale beyond 20-30 features. We present an alternative algorithm for discriminative training of phrasebased MT systems, SampleRank, which scales to hundreds of features, equals or beats MERT on both small and medium sized systems, and permits the use of sentence or document level features. SampleRank proceeds by repeatedly updating the model weights to ensure that the ranking of output sentences induced by the model is the same as that induced by the gain function.  
We present an empirical study of instance selection techniques for machine translation. In an active learning setting, instance selection minimizes the human effort by identifying the most informative sentences for translation. In a transductive learning setting, selection of training instances relevant to the test set improves the ﬁnal translation quality. After reviewing the state of the art in the ﬁeld, we generalize the main ideas in a class of instance selection algorithms that use feature decay. Feature decay algorithms increase diversity of the training set by devaluing features that are already included. We show that the feature decay rate has a very strong effect on the ﬁnal translation quality whereas the initial feature values, inclusion of higher order features, or sentence length normalizations do not. We evaluate the best instance selection methods using a standard Moses baseline using the whole 1.6 million sentence English-German section of the Europarl corpus. We show that selecting the best 3000 training sentences for a speciﬁc test sentence is sufﬁcient to obtain a score within 1 BLEU of the baseline, using 5% of the training data is sufﬁcient to exceed the baseline, and a ∼ 2 BLEU improvement over the baseline is possible by optimally selected subset of the training data. In out-of-domain translation, we are able to reduce the training set size to about 7% and achieve a similar performance with the baseline.  
Most of the freely available parallel data to train the translation model of a statistical machine translation system comes from very speciﬁc sources (European parliament, United Nations, etc). Therefore, there is increasing interest in methods to perform an adaptation of the translation model. A popular approach is based on unsupervised training, also called self-enhancing. Both only use monolingual data to adapt the translation model. In this paper we extend the previous work and provide new insight in the existing methods. We report results on the translation between French and English. Improvements of up to 0.5 BLEU were observed with respect to a very competitive baseline trained on more than 280M words of human translated parallel data. 
This work presents a simpliﬁed approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training. During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language’s vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM). We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task. Our topic modeling approach is simpler to construct than its counterparts. 
This paper presents the Linguatec submission to the WMT 2011 sixth workshop on statistical machine translation. It describes the architecture of our machine translation system ‘Personal Translator’ (hereinafter also referred to as PT), developed by Linguatec, which is a rule-based translation system, enriched by statistical approaches. We participate for the German-English translation direction. For the current submission we have chosen the latest commercial version of the system, PT14. The translation quality improvement for the submission was done mainly by lexicon tuning: detection of unknown words, extracting of possible translations, partly from the wmt11 training corpora, and enlarging the lexicon by manually coding the chosen transfer candidates. 
This paper describes LIMSI’s submissions to the Sixth Workshop on Statistical Machine Translation. We report results for the FrenchEnglish and German-English shared translation tasks in both directions. Our systems use n-code, an open source Statistical Machine Translation system based on bilingual n-grams. For the French-English task, we focussed on ﬁnding efﬁcient ways to take advantage of the large and heterogeneous training parallel data. In particular, using a simple ﬁltering strategy helped to improve both processing time and translation quality. To translate from English to French and German, we also investigated the use of the SOUL language model in Machine Translation and showed signiﬁcant improvements with a 10-gram SOUL model. We also brieﬂy report experiments with several alternatives to the standard n-best MERT procedure, leading to a signiﬁcant speed-up. 
We present a translation model enriched with shallow syntactic and semantic information about the source language. Base-phrase labels and semantic role labels are incorporated into an hierarchical model by creating shallow semantic “trees”. Results show an increase in performance of up to 6% in BLEU scores for English-Spanish translation over a standard phrase-based SMT baseline. 
We present the results we obtain using our RegMT system, which uses transductive regression techniques to learn mappings between source and target features of given parallel corpora and use these mappings to generate machine translation outputs. Our training instance selection methods perform feature decay for proper selection of training instances, which plays an important role to learn correct feature mappings. RegMT uses L2 regularized regression as well as L1 regularized regression for sparse regression estimation of target features. We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F1 measure over target features as a metric for evaluating translation quality. 
This paper describes the German-English translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation (WMT11). We present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training. 
This paper presents the system we developed for the 2011 WMT Haitian Creole–English SMS featured translation task. Applying standard statistical machine translation methods to noisy real-world SMS data in a low-density language setting such as Haitian Creole poses a unique set of challenges, which we attempt to address in this work. Along with techniques to better exploit the limited available training data, we explore the beneﬁts of several methods for alleviating the additional noise inherent in the SMS and transforming it to better suite the assumptions of our hierarchical phrase-based model system. We show that these methods lead to signiﬁcant improvements in BLEU score over the baseline. 
In this paper we describe our hybrid machine translation system with which we participated in the WMT11 shared translation task for the English→German language pair. Our system was able to outperform its RBMT baseline and turned out to be the best-scored participating system in the manual evaluation. To achieve this, we extended an existing, rule-based MT system with a module for stochastic selection of analysis parse trees that allowed to better cope with parsing errors during the system’s analysis phase. Due to the integration into the analysis phase of the RBMT engine, we are able to preserve the beneﬁts of a rule-based translation system such as proper generation of target language text. Additionally, we used a statistical tool for terminology extraction to improve the lexicon of the RBMT system. We report results from both automated metrics and human evaluation efforts, including examples which show how the proposed approach can improve machine translation quality. 
 1.1 Data Sets  This paper describes the joint QUAERO submission to the WMT 2011 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT German→English task. Each group translated the data sets with their own systems. Then RWTH system combination combines these translations to a better one. In this paper, we describe the single systems of each group. Before we present the results of the system combination, we give a short description of the RWTH Aachen system combination approach. 
We present the Carnegie Mellon University Stat-XFER group submission to the WMT 2011 shared translation task. We built a hybrid syntactic MT system for French–English using the Joshua decoder and an automatically acquired SCFG. New work for this year includes training data selection and grammar ﬁltering. Expanded training data selection signiﬁcantly increased translation scores and lowered OOV rates, while results on grammar ﬁltering were mixed.  ing was carried out in Joshua (Li et al., 2009), an open-source framework for parsing-based MT. We managed our experiments with LoonyBin (Clark and Lavie, 2010), an open-source tool for deﬁning, modifying, and running complex experimental pipelines. We describe our system-building process in more detail in Section 2. In Section 3, we evaluate the system’s performance on WMT development sets and examine the aftermath of training data selection and grammar ﬁltering. Section 4 concludes with possible directions for future work.  
This paper presents our submissions to the shared translation task at WMT 2011. We created two largely independent systems for English-to-French and Haitian Creole-toEnglish translation to evaluate different features and components from our ongoing research on these language pairs. Key features of our systems include anaphora resolution, hierarchical lexical reordering, data selection for language modelling, linear transduction grammars for word alignment and syntaxbased decoding with monolingual dependency information. 
This paper describes the phrase-based SMT systems developed for our participation in the WMT11 Shared Translation Task. Translations for English↔German and English↔French were generated using a phrase-based translation system which is extended by additional models such as bilingual and ﬁne-grained POS language models, POS-based reordering, lattice phrase extraction and discriminative word alignment. Furthermore, we present a special ﬁltering method for the English-French Giga corpus and the phrase scoring step in the training is parallelized. 
This paper describes the statistical machine translation system submitted to the WMT11 Featured Translation Task, which involves translating Haitian Creole SMS messages into English. In our experiments we try to address the issue of noise in the training data, as well as the lack of parallel training data. Spelling normalization is applied to reduce out-of-vocabulary words in the corpus. Using Semantic Role Labeling rules we expand the available training corpus. Additionally we investigate extracting parallel sentences from comparable data to enhance the available parallel data. 
MonoTrans2 is a translation system that combines machine translation (MT) with human computation using two crowds of monolingual source (Haitian Creole) and target (English) speakers. We report on its use in the WMT 2011 Haitian Creole to English translation task, showing that MonoTrans2 translated 38% of the sentences well compared to Google Translate’s 25%. 
This paper describes the statistical machine translation (SMT) systems developed by RWTH Aachen University for the translation task of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation. Both phrasebased and hierarchical SMT systems were trained for the constrained German-English and French-English tasks in all directions. Experiments were conducted to compare different training data sets, training methods and optimization criteria, as well as additional models on dependency structure and phrase reordering. Further, we applied a system combination technique to create a consensus hypothesis from several different systems. 
This paper describes the UPM system for translation task at the EMNLP 2011 workshop on statistical machine translation (http://www.statmt.org/wmt11/), and it has been used for both directions: Spanish-English and English-Spanish. This system is based on Moses with two new modules for pre and post processing the sentences. The main contribution is the method proposed (based on the similarity with the source language test set) for selecting the sentences for training the models and adjusting the weights. With system, we have obtained a 23.2 BLEU for Spanish-English and 21.7 BLEU for EnglishSpanish.  The difference between these techniques and the method that we propose is that we do not search “bad” pairs of sentences, but we search those sentences in source training corpus that are more similar with the language model generated with the source test sentences and we select them for training. Other interesting technique of corpus selection is based on transductive learning (Ueffing, 2007). In this work, authors use of transductive semisupervised methods for the effective use of monolingual data from the source language in order to improve translation quality. The method proposed in this paper is also applied to the validation corpus. There are other works related to select development set (Hui, 2010) that they combine different development sets in order to find the more similar one with test set.  
Accuracy of dependency parsers is one of the key factors limiting the quality of dependencybased machine translation. This paper deals with the inﬂuence of various dependency parsing approaches (and also different training data size) on the overall performance of an English-to-Czech dependency-based statistical translation system implemented in the Treex framework. We also study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of BLEU. 
 2 System overview  We describe our system for the news commentary translation task of WMT 2011. The submitted run for the French-English direction is a combination of two MOSES-based systems developed at LIG and LIA laboratories. We report experiments to improve over the standard phrase-based model using statistical post-edition, information retrieval methods to subsample out-of-domain parallel corpora and ROVER to combine n-best list of hypotheses output by different systems. 
 P (ci|ci−1)  c1  c2  c3  ci  ...  cn  Unsupervised word clustering algorithms —  P (wi|ci)  ci = C(wi)  which form word clusters based on a measure of distributional similarity — have proven to be  w1  w2  w3  wi  ...  wn  useful in providing beneﬁcial features for var-  Figure 1: Bayesian network illustrating the class-  ious natural language processing tasksFiingvuorlev-4-1: Thebacslaesds-lbanasgeudagbeigmraomdelatnhgautaigseumseoddteol, dwehﬁincehtdheeﬁqnueasl-the quality of a  ing supervised utility of such  learning. This word clusters  wasorfkacetxoprslcolriunesststetharei-ng,  tistical machine translation.  repreisteyntoefdaasclausBtaeryiensgianinnethtweoBrkr.own 2005]  algorithm  [Liang,  Although some of the language pairs in this  work clearly beneﬁt from the factor augmen-  2 Unsupervised word clusters  tation, there is no consistent improvemen1Wt einuse the term clustering to refer to a set of clusters.  translation accuracy across the board. For all  Unsupervised word clusters owe their appeal perhaps  language pairs, the word clusters clearly improve translation for some proportion of the sentences in the test set, but has a weak or even detrimental eﬀect on the rest.  mostly to the relative ea4s4e of obtaining them. Obtaining regular morphological, syntactic or semantic analyses for tokens in a text relies on some sort of tagger, either based on manually crafted rules or  It is shown that if one could determine whether or not to use a factor when translating a given sentence, rather substantial improvements in precision could be achieved for all of the language pairs evaluated. While such an “oracle”  trainable on an annotated corpus. Both rule-crafting and corpus annotation are time-consuming and expensive processes, and might not be feasible for a small or resource-scarce language.  method is not identiﬁed, evaluations indicate  For unsupervised word clusters, on the other hand,  that unsupervised word cluster are most bene-  one merely needs a large amount of raw (unanno-  ﬁcial in sentences without unknown words.  tated) text and some processing power. Such cluster-  ing is thus particularly interesting for resource-scarce  
This paper describes the machine translation (MT) system developed by the Transducens Research Group, from Universitat d’Alacant, Spain, for the WMT 2011 shared translation task. We submitted a hybrid system for the Spanish–English language pair consisting of a phrase-based statistical MT system whose phrase table was enriched with bilingual phrase pairs matching transfer rules and dictionary entries from the Apertium shallowtransfer rule-based MT platform. Our hybrid system outperforms, in terms of BLEU, GTM and METEOR, a standard phrase-based statistical MT system trained on the same corpus, and received the second best BLEU score in the automatic evaluation. 
We report results on translation of SMS messages from Haitian Creole to English. We show improvements by applying spell checking techniques to unknown words and creating a lattice with the best known spelling equivalents. We also used a small cleaned corpus to train a cleaning model that we applied to the noisy corpora. 
We present progress on Joshua, an opensource decoder for hierarchical and syntaxbased machine translation. The main focus is describing Thrax, a ﬂexible, open source synchronous context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efﬁcient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats.  traction procedures by providing a ﬂexible and conﬁgurable means of specifying these settings. Section 3 presents a systematic comparison of the two grammars using identical feature sets. In addition, Joshua now includes a single parameterized script that implements the entire MT pipeline, from data preparation to evaluation. This script is built on top of a module called CachePipe. CachePipe is a simple wrapper around shell commands that uses SHA-1 hashes and explicitlyprovided lists of dependencies to determine whether a command needs to be run, saving time both in running and debugging machine translation pipelines.  
We present the DFKI hybrid translation system at the WMT workshop 2011. Three SMT and two RBMT systems are combined at the level of the ﬁnal translation output. The translation results show that our hybrid system signiﬁcantly outperformed individual systems by exploring strengths of both rule-based and statistical translations. 
This paper describes the system presented for the English-Spanish translation task by the collaboration between CEU-UCH and UPV for 2011 WMT. A comparison of independent phrase-based translation models interpolation for each available training corpora were tested, giving an improvement of 0.4 BLEU points over the baseline. Output N -best lists were rescored via a target Neural Network Language Model. An improvement of one BLEU point over the baseline was obtained adding the two features, giving 31.5 BLEU and 57.9 TER for the primary system, computed over lowercased and detokenized outputs. The system was positioned second in the ﬁnal ranking.  
In this paper, we propose that MT is an important technology in crisis events, something that can and should be an integral part of a rapid-response infrastructure. By integrating MT services directly into a messaging infrastructure (whatever the type of messages being serviced, e.g., text messages, Twitter feeds, blog postings, etc.), MT can be used to provide ﬁrst pass translations into a majority language, which can be more effectively triaged and then routed to the appropriate aid agencies. If done right, MT can dramatically increase the speed by which relief can be provided. To ensure that MT is a standard tool in the arsenal of tools needed in crisis events, we propose a preliminary Crisis Cookbook, the contents of which could be translated into the relevant language(s) by volunteers immediately after a crisis event occurs. The resulting data could then be made available to relief groups on the ground, as well as to providers of MT services. We also note that there are signiﬁcant contributions that our community can make to relief efforts through continued work on our research, especially that research which makes MT more viable for under-resourced languages. 
A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps. We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus. We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results.  All of this previous work used heuristics or local statistical tests to extract patterns from corpora. In this paper, we present probabilistic models that generate text using gappy patterns of arbitrary length and with arbitrarily-many gaps. We exploit nonparametric priors and use Bayesian inference to discover the most salient gappy patterns in monolingual and parallel text. We ﬁrst inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results.  
We provide a general algorithmic schema for translation rule extraction and show that several popular extraction methods (including phrase pair extraction, hierarchical phrase pair extraction, and GHKM extraction) can be viewed as speciﬁc instances of this schema. This work is primarily intended as a survey of the dominant extraction paradigms, in which we make explicit the close relationship between these approaches, and establish a language for future hybridizations. This facilitates a generic and extensible implementation of alignment-based extraction methods. 
We present a novel approach for extracting a minimal synchronous context-free grammar (SCFG) for Hiero-style statistical machine translation using a non-parametric Bayesian framework. Our approach is designed to extract rules that are licensed by the word alignments and heuristically extracted phrase pairs. Our Bayesian model limits the number of SCFG rules extracted, by sampling from the space of all possible hierarchical rules; additionally our informed prior based on the lexical alignment probabilities biases the grammar to extract high quality rules leading to improved generalization and the automatic identiﬁcation of commonly re-used rules. We show that our Bayesian model is able to extract minimal set of hierarchical phrase rules without impacting the translation quality as measured by the BLEU score. 
A major weakness of extant statistical machine translation (SMT) systems is their lack of a proper training procedure. Phrase extraction and scoring processes rely on a chain of crude heuristics, a situation judged problematic by many. In this paper, we recast the machine translation problem in the familiar terms of a sequence labeling task, thereby enabling the use of enriched feature sets and exact training and inference procedures. The tractability of the whole enterprise is achieved through an efﬁcient implementation of the conditional random ﬁelds (CRFs) model using a weighted ﬁnite-state transducers library. This approach is experimentally contrasted with several conventional phrase-based systems. 
The Spoken Dialog Challenge 2010 was an exercise to investigate how different spoken dialog systems perform on the same task. The existing Let’s Go Pittsburgh Bus Information System was used as a task and four teams provided systems that were first tested in controlled conditions with speech researchers as users. The three most stable systems were then deployed to real callers. This paper presents the results of the live tests, and compares them with the control test results. Results show considerable variation both between systems and between the control and live tests. Interestingly, relatively high task completion for controlled tests did not always predict relatively high task completion for live tests. Moreover, even though the systems were quite different in their designs, we saw very similar correlations between word error rate and task completion for all the systems. The dialog data collected is available to the research community. 
We investigate how to jointly explain the performance and behavioral differences of two spoken dialogue systems. The Join Evaluation and Differences Identiﬁcation (JEDI), ﬁnds differences between systems relevant to performance by formulating the problem as a multi-task feature selection question. JEDI provides evidence on the usefulness of a recent method, 1/ p-regularized regression (Obozinski et al., 2007). We evaluate against manually annotated success criteria from real users interacting with ﬁve different spoken user interfaces that give bus schedule information. 
This paper describes a general and effective domain selection framework for multi-domain spoken dialogue systems that employ distributed domain experts. The framework consists of two processes: deciding if the current domain continues and estimating the probabilities for selecting other domains. If the current domain does not continue, the domain with the highest activation probability is selected. Since those processes for each domain expert can be designed independently from other experts and can use a large variety of information, the framework achieves both extensibility and robustness against speech recognition errors. The results of an experiment using a corpus of dialogues between humans and a multi-domain dialogue system demonstrate the viability of the proposed framework. 
With the evolution of online communication methods, conversations are increasingly handled via email, internet forums and other such methods. In this paper, we attempt to model lexical information in a context sensitive manner, encoding our belief that the use of language depends on the participants in the conversation. We model the discourse as a combination of the speaker, the addressee and other participants in the conversation as well as a context speciﬁc language model. In order to do this, we introduce a novel method based on an HMM with an exponential state space to capture speaker-addressee context. We also study the performance of topic modeling frameworks in conversational settings. We evaluate the models on the tasks of identifying the set of people present in any conversation, as well as identifying the speaker for every utterance in the conversation, and they show signiﬁcant improvement over the baseline models. 
We present a dialogue collection and enrichment framework that is designed to explore the learning and evaluation of dialogue policies for simple conversational characters using textual training data. To facilitate learning and evaluation, our framework enriches a collection of role-play dialogues with additional training data, including paraphrases of user utterances, and multiple independent judgments by external referees about the best policy response for the character at each point. As a case study, we use this framework to train a policy for a limited domain tactical questioning character, reaching promising performance. We also introduce an automatic policy evaluation metric that recognizes the validity of multiple conversational responses at each point in a dialogue. We use this metric to explore the variability in human opinion about optimal policy decisions, and to automatically evaluate several learned policies in our example domain. 
Human dialogue serves as a valuable model for learning the behavior of dialogue systems. Hidden Markov models’ sequential structure is well suited to modeling human dialogue, and their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process with a layer of implicit, highly influential structure. HMMs have been shown to be effective for a variety of descriptive and predictive dialogue tasks. For task-oriented dialogue, understanding the learning behavior of HMMs is an important step toward building unsupervised models of human dialogue. This paper examines the behavior of HMMs under six experimental conditions including different task-oriented feature sets and preprocessing approaches. The findings highlight the importance of providing HMM learning algorithms with rich task-based information. Additionally, the results suggest how specific metrics should be used depending on whether the models will be employed primarily in a descriptive or predictive manner. 
We present a novel scheme of spoken dialogue systems which uses the up-to-date information on the web. The scheme is based on information extraction which is deﬁned by the predicate-argument (P-A) structure and realized by semantic parsing. Based on the information structure, the dialogue system can perform question answering and also proactive information presentation. Feasibility of this scheme is demonstrated with experiments using a domain of baseball news. In order to automatically select useful domain-dependent P-A templates, statistical measures are introduced, resulting to a completely unsupervised learning of the information structure given a corpus. Similarity measures of P-A structures are also introduced to select relevant information. An experimental evaluation shows that the proposed system can make more relevant responses compared with the conventional ”bag-of-words” scheme. 
Instruction giving can be used in several applications, ranging from trainers in simulated worlds to non player characters for virtual games. In this paper we present a novel algorithm for rapidly prototyping virtual instruction-giving agents from human-human corpora without manual annotation. Automatically prototyping full-ﬂedged dialogue systems from corpora is far from being a reality nowadays. Our approach is restricted in that only the virtual instructor can perform speech acts while the user responses are limited to physical actions in the virtual worlds. We have deﬁned an algorithm that, given a task-based corpus situated in a virtual world, which contains human instructor’s speech acts and the user’s responses as physical actions, generates a virtual instructor that robustly helps a user achieve a given task in the virtual world. We explain how this algorithm can be used for generating a virtual instructor for a game-like, task-oriented virtual world. We evaluate the virtual instructor with human users using task-oriented as well as user satisfaction metrics. We compare our results with both human and rule-based virtual instructors hand-coded for the same task. 
Natural language generators are faced with a multitude of different decisions during their generation process. We address the joint optimisation of navigation strategies and referring expressions in a situated setting with respect to task success and human-likeness. To this end, we present a novel, comprehensive framework that combines supervised learning, Hierarchical Reinforcement Learning and a hierarchical Information State. A human evaluation shows that our learnt instructions are rated similar to human instructions, and signiﬁcantly better than the supervised learning baseline. 
We report on an empirical study of a multiparty turn-taking model for physically situated spoken dialog systems. We present subjective and objective performance measures that show how the model, supported with a basic set of sensory competencies and turn-taking policies, can enable interactions with multiple participants in a collaborative task setting. The analysis brings to the fore several phenomena and frames challenges for managing multiparty turn taking in physically situated interaction. 1. Introduction Effective dialog relies on the coordination of contributions by participants in a conversation via turn taking. The complexity of understanding and managing turns grows significantly in moving from dyadic to multiparty settings, including situations where groups of people converse as they collaborate on shared goals. We are exploring computational methods that can endow dialog systems with the ability to participate in a natural, fluid manner in conversations involving several people. In Bohus and Horvitz (2010a), we presented a computational model for managing multiparty turn taking. The model harnesses multisensory perception and reasoning and includes a set of components and representations. These include methods for tracking multiparty conversational dynamics, for making turn-taking decisions, and for rendering decisions about turns into an appropriate set of  low-level, coordinated gaze, gesture and speech behaviors. We implemented the model and have been testing it in several domains. The investigations have been aimed at characterizing the system’s performance in complex multiparty settings. In Bohus and Horvitz (2010b), we examine data collected during a user study to evaluate the ability of the system to shape the flow of multiparty conversational dynamics. In this paper, we focus our attention on the performance of the inference and decision-making models. We analyze the accuracy of current turn-taking inferences, the influence of inference errors on decisions, and the overall effectiveness of the system’s decision making. We report on subjective and objective measures of the system’s turn-taking performance. We find that the turn-taking methodology enables our system to successfully participate in multiparty interactions, even when relying on relatively coarse models for inference and decision making. The analysis highlights several general phenomena including standing bottlenecks and difficulties, and opportunities for enhancing multiparty turn taking in dialog systems. Based on the results, we discuss challenges and directions for research on turn taking in physically situated dialog. 2. Related Work We begin by placing this work within the larger context of research on multiparty interaction and turn taking. In a seminal paper on turn taking in natural conversations, Sacks, Schegloff and Jefferson (1974) proposed a basic model for the organi-  98  Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 98–109, Portland, Oregon, June 17-18, 2011. c 2011 Association for Computational Linguistics  zation of turns in conversation. The model is centered on the notion of turn-constructional-units, separated by transition relevance places that provide opportunities for speaker changes. In later work, Schegloff (2000) elaborates on several aspects of this model, including interruptions and overlap resolution devices. Other researchers in conversational analysis and psycho-linguistics have highlighted the important role played by gaze, gesture, and other non-verbal communication channels in regulating turn taking. For instance, Duncan (1972) discusses the role of non-verbal signals, and proposes that turn taking is mediated via a set of verbal and non-verbal cues. Wiemann and Knapp (1975) survey prior investigations on turn-taking cues in several conversational settings, in an effort to elucidate differences. Goodwin (1980) discusses various aspects of the relationship between turn taking and attention. More recently, Hjalmarsson (2011) investigates the additive effect turn-taking cues have on listeners in both human and synthetic voices. Within the dialog systems community, efforts have been made on designing and implementing computational models for managing turn taking (e.g., Traum, 1994; Thorrissön, 2002; Raux and Eskenazi, 2009; Selfridge and Heeman, 2010). Moving beyond the dyadic setting, Traum and Rickel (2002) describe a turn management component for supporting dialog between a trainee and multiple virtual humans. Kronlid (2006) describes a Harel state-chart implementation of the original SSJ model. Researchers studying human-robot interaction have developed prototype robots that can interact with multiple human participants (e.g. Matsusaka et al., 2001; Bennewitz et al., 2005). In our previous work Bohus and Horvitz (2009; 2010a; 2010b), we describe a platform that leverages multimodal perception and reasoning to support multiparty dialog in open-world settings. 3. Multiparty Turn-Taking Model We engaged in a set of experiments to probe the inference and decision making competencies of a computational model for multiparty turn taking (Bohus and Horvitz 2010a; 2010b). To set the stage for the analysis to follow, we briefly review the proposed approach. We model turn taking as an interactive, collaborative process by which participants in a conversa-  Sensing S(s) A(s)  Semantic Input Dialog Context  Turn management Dialog management  Ψ Audio-visual evidence  FS(p) FI(p) FA(p)  Speech Gaze Gesture  Behavioral Control Behaviors and Output Management  Decisions CONTRIB System Floor Action  Contribute Semantic Output  Figure 1. Components of turn-taking model.  tion monitor one another and take coordinated actions to ensure that (generally) only one person speaks at a given time. The participant ratified to speak via this process is said to have the floor. Each participant engaged in the interaction continuously produces (i.e. at every time tick) one of four floor management actions: a hold action indicates that a participant is maintaining the floor; a release action indicates that the participant is yielding the floor to a set of other participants (which could be void, allowing for self-selection next turn allocation); a take action indicates that the participant is trying to acquire the floor; finally, a null action indicates that a participant is not making any floor claims. The floor shifts from one participant to another as the result of the joint, cooperative floor management actions taken by the participants. Specifically, a release action must be met with a take action for a floor shift to occur; in all other cases the floor stays with the participant that currently holds it. Figure 1 illustrates the main components and key abstractions in the model. The sensing subcomponent tracks the conversational dynamics, and includes models for detecting spoken signals s, inferring the source S(s) and the set of addressees A(s) for each signal, as well as the floor state FS(p), actions FA(p) and intentions FI(p) of each participant p engaged in a conversation. This information is used in conjunction with higher-level dialog context to decide when the system should generate new contributions and which floor action should be produced at each point in time. Finally, floor actions are rendered by a behavioral component into a set of coordinated gaze, gesture and speech behaviors. By harnessing these different components, the proposed model can enable an  99  embodied conversational agent to handle a broad spectrum of turn-taking phenomena.  4. User Study We implemented an initial set of turn-taking inference and decision making models in the context of a multiparty dialog system, and we conducted a large-scale multiparty interaction user study with this system. The study, described in more detail below, was designed to fulfill two goals: (1) to ascertain an initial performance baseline and identify current bottlenecks and challenges to be addressed moving forward, and (2) to collect a large set of multiparty human-computer dialog data that can be used to study and improve multiparty turn taking in dialog systems. 4.1. System The platform used in these experiments, described in detail in Bohus and Horvitz (2009), takes the form of a multimodal interactive kiosk that displays an avatar head which plays a questions game with multiple participants. The system leverages audiovisual information and employs components for visually tracking multiple people in the scene, sound source localization, speech recognition, conversational scene analysis, behavioral control and dialog management. Figure 2 shows a screen generated by the system, with the rendered avatar and a sample challenge question. Users can collaborate on selecting an answer, and, after a confirmation, the system provides an explanation if the answer is incorrect, before moving on to the next question. Sample interactions are found in Appendix C and videos are available online (Situated Interaction, 2011). 4.2. Turn-Taking Inference and Decisions In the current system, a voice activity detector is used to identify and segment spoken utterances. The source of each utterance is assumed to be the participant who is closest in the horizontal plane to the sound direction identified by the microphone array. The set of addressees is identified by fusing information probabilistically about the focus of attention of the source, as obtained through face detection and head pose tracking, while the utterance is being detected. In addition, the system assumes that non-understandings are addressed to other engaged participants, since initial tests indi-  Figure 2. Questions game: screen and kiosk. cated that in this domain about 80% of utterances that led to non-understandings were in fact addressed to others. Similarly, the system assumes that utterances longer than three seconds are addressed to others (responses addressed to the system tend to be short in this domain) Floor management actions are inferred as follows. If a participant has the floor, we assume they are performing a hold action if speaking and a release action otherwise. The release is assumed to be towards the addressees of the last spoken utterance. Although the latter assumption on releases may not hold in the most general case, it is a reasonable one for the questions game domain. If a participant does not have the floor, the system assumes they perform a take action if speaking or a null action otherwise. The system also assumes that the floor intentions are fully reflected by the floor actions, i.e., a participant intends to have the floor if and only if she performs a hold or take action. Floor states are updated based on the joint, coordinated floor actions of all participants, as described earlier. Turn-taking decisions are based on a simple heuristic policy. The system takes the floor if (1) the floor is being released to it or (2) a participant releases the floor to someone else, but no one claims the floor for a preset duration. In most cases, this duration is set to 3.5 seconds. However, if the floor is released to someone else after the system is interrupted during a question dialog act, the system will try to quickly reacquire the floor should no one else be speaking, so as to finish or restate its question. The waiting duration is set in the latter case to 500 milliseconds. If after 500ms, when the system tries to take the floor another conflict occurs (followed by a floor release to someone else), the waiting duration is increased again to 3.5 seconds. Finally, if a third consecutive conflict oc-  100  curs when the system tries to acquire the floor, the waiting duration is set to a longer, 20 seconds. The system releases the floor at the end of its own outputs. In addition, it has to decide whether it should release the floor when a user performs a take action (i.e. barges in) while the system is speaking. The heuristic policy currently implemented by the system releases the floor only for barge-ins occurring during question dialog acts. Finally, the behavioral models employ policies informed by the existing literature on the role of gaze in regulating turn taking. In particular, the system’s gaze is directed towards the speaking participant, or, if the system is speaking, towards the addressees of the system’s utterance. During silences, the system’s gaze is directed towards the participants that the floor is being released to. The models and policies described above represent a starting point for inference and action, constructed to enable data collection and an initial evaluation in this domain. We are working to update the turn-taking architecture with more sophisticated evidential reasoning and utility-theoretic decision making. Nevertheless, when harnessed as an ensemble within the turn-taking approach that we have described, the current procedures provide for an array of complex, multiparty turn-taking behaviors. For instance, the system can address each participant individually or all participants as a group via controlling the orientation of its head pose. When participants talk amongst themselves, the system can monitor their exchanges and wait until the floor is being released back to it. If an answer is heard during such a side conversation (e.g., one participant suggests an answer to another), the system highlights it on the screen (see Figure 2). If a significant pause is detected during this side conversation, the avatar takes the floor and the initiative, e.g., “So, what do you think is the correct answer?” Once a participant provides an answer, the system seeks confirmation from another participant before moving on. In some cases, the avatar passes back the floor and seeks confirmation non-verbally, by simply turning towards another participant and raising its eyebrows. The system can try to require the floor immediately after being interrupted, but can also back off, giving the participants a chance to finish a side conversation, if successive floor conflicts occur. Sample interactions can be viewed in Appendix C and online (Situated Interaction, 2011).  4.3. Study Design The user study was conducted in a usability lab and involved a total of 60 participants recruited as pairs of people from the general population who previously knew one another (30 male and 30 female, with ages between 18 and 61). The study was structured in 15 one-hour sessions, with each session involving four participants, i.e., two pairs of two previously acquainted participants. In each session, we formed all possible subgroups of size two (6 subgroups) and of size three (4 subgroups) with the four participants. Each subgroup played one game with the system. This setup allowed us to collect a large set of multiparty interactions under diverse conditions (e.g., all-male, all-female, mixed-gender groups; groups where people were previously acquainted vs. not, etc.). At the end of each session, participants filled in a subjective assessment survey. 4.4. Corpus, Annotations, and Cost Assessment In total, 150 multiparty interactions were collected: 90 with two participants and the system, and 60 with three participants and the system. A professional annotator transcribed the utterances detected by the system at runtime, and labeled them with source and addressee information. The system was noted to commit several types of turn-taking errors. To expand the error analysis beyond occurrence statistics and to characterize the impact of various types of errors, we conducted a follow-up study. In this second study, a set of additional participants were recruited to review videos of interactions from the first study and asked to (1) identify the turn-taking errors committed by the system and (2) to assess the costliness of the error on a five-point scale. A total of 9 interactions (5 with two participants and system; 4 with three participants and system) were randomly sampled from the collected corpus, while ensuring that each turn-taking outcome of interest (discussed in Section 5 and summarized in Table 1) was sufficiently represented. Nine participants were recruited via an email request to employees at our organization. Each participant reviewed three interactions, and each interaction was reviewed by three different participants. Prior to the experiment, each of the annotators received a brief review of the turn-taking process in humanhuman interaction. Next, they used a multimodal  101  annotation tool that we created to review the interaction videos. As each video played, the annotator pushed a button at each point they believed that the system had committed a turn-taking error. In a second pass, each annotator was asked to review the errors that they had previously identified and to assess the relative cost of the error, on a scale from 0 (“no error”) to 5 (“worst error”). In a final step, the authors manually aligned each identified turntaking error with a turn-taking decision made by the system and its corresponding outcome. 5. Evaluation We now focus on the various types of turn-taking errors, the outcomes that these errors lead to, and the costs assessed for the outcomes. We begin by focusing on diarization challenges described in Section 5.1. In Sections 5.2 and 5.3, we review the accuracy of the system’s turn-taking inferences and decisions, and their corresponding outcomes. Finally, in Section 5.4, we turn our attention to the subjective assessment results obtained via the postexperiment user survey. Before diving into the details, we note that we eliminated 7 out of the total 150 interactions from the analysis due to significant problems with acoustic echo cancellation. In the remaining 143 interactions, we also identified and eliminated 24 utterances in the transitional engagement stages, e.g., when the users were not ready or properly setup in front of the system. The analysis below is based on the remaining 4379 utterances. 5.1. Diarization The system uses a voice activity detector which leverages energy, acoustics and grammar to detect spoken utterances. Our experiments indicate that this type of black-box solution can make diarization errors, especially in multiparty settings where people may speak simultaneously, at a fast pace, and address each other with language outside the system’s grammar. Results show that only 72% of the detected segments contain speech from a single participant. Another 2% contain background noises incorrectly identified as speech. Most often these are instances where the system heard itself due to acoustic echo-cancellation problems; the ratio grows to about 6% among all utterances detected while the system is speaking. The remaining 26% contain overlapping or successive utterances from  multiple speakers. Inspection of the data reveals that some utterances spoken softly by participants were not detected and that segmentation boundary errors are also sometimes present. While such errors may be mitigated by inferences at higher levels in the turn-taking model, they can significantly influence the system’s ability to track the conversational dynamics and make appropriate turntaking decisions. We plan to pursue more robust audiovisual diarization methods that integrate sound localization as detected by a microphone array, along with higher-level interaction context. 5.2. Take versus Null We now turn our attention to the system’s floor control decisions. The analysis below is based on the utterances and segmentation detected by the system at runtime. We note that a more precise analysis could be conducted with a ground truth segmentation of utterances. Utterances detected by the system can be classified into three categories, based on their relationship to system outputs, as shown in Figure 3: overlaps, which start and end during a system’s output, continuers, which begin during but finish after a system output has ended, and responses, which do not overlap anywhere. With the current policy, the system chooses whether it should take the floor following each detected continuer and response. The dataset contains a total of 3265 such instances. The system’s decision at each of these points hinges on the results of its inferences about the participants’ floor actions, and thus of inferences about the addressees of each utterance. Table 1 displays a tabulation of the release actions performed by the participants versus the actions identified by the system. The release actions are determined from labels assigned manually by the professional annotator. Recall that we make an assumption that the release is towards the set of addressees of an utterance. For segments that were labeled as containing multiple utterances, the release is made to the addressee of the last utterance. The last row in Table 1 corresponds to background noises and system speech incorrectly  turn-internal overlap continuer response  turn-initial overlap (TIO) Actual utterances Detected utterances  System speech  Figure 3. Schematic of different classes of overlap.  102  To System  Labeled Addressee / Release Action  Inferred Addressee / Release Action  To System  Not to System  2063 (64%)  277 (9%)  Take + Verbal Contribution  Take+  1796 (87%)  Non-verbal  Release Turn-initial 0.25 No turn-initial 0.00 267 (13%) 0.42  overlap  overlap  182 (10%)  1614 (90%)  [17 Echo]  Delayed System Take  Other  59 (21%)  Takes  Turn-initial  218 (79%)  1.83 No turn-initial 2.58  0.85  overlap  overlap  22 (37%)  37 (63%)  [0 Echo]  Not to System  305 (9%)  Take + Verbal Contribution  Take+  242 (79%)  Non-verbal  Release  Turn-initial 1.76 No turn-initial 0.42 63 (21%)  0.00  overlap  overlap  101 (42%)  141 (58%)  [0 Echo]  588 (18%)  Delayed System Take  Other  131 (22%)  Takes  Turn-initial  457 (78%)  0.55 No turn-initial 0.00  0.03  overlap  overlap  38 (29%)  93 (71%)  [3 Echo]  Background  10 (<1%)  22 (<1%)  Take + Verbal Contribution  9 (90%)  Turn-initial  No turn-initial  overlap  overlap  3 (33%)  6 (67%)  [0 Echo]  Take+ Non-verbal Release 1 (10%)  Delayed System Take  13 (59%)  Turn-initial  No turn-initial  overlap  overlap  7 (54%)  6 (46%)  [4 Echo]  Other Takes 9 (41%)  Table 1. Decisions to take floor (vs. null), outcomes, and estimated costs (bar graph with confidence intervals). Echo denotes cases where the turn initial overlap is created by utterances where the system hears itself because of errors with echo cancellation.  identified as utterances. On the task of detecting addressees, and thus floor release actions, the results show an error rate of 18%, including 305 false-positives (erroneous detections) and 277 false-negatives (missed detections) of floor releases to the system. These errors influence the quality of turn taking in a variety of ways and underscore the need for more robust inferences about speech source and target, and floor release actions. We believe that more sophisticated models learned from audiovisual information (e.g., prosody, head and body pose, etc.) and attributes of the interaction context (e.g., who spoke last, where is the system looking, etc.) can reduce errors significantly. Table 1 indicates that in 305 (9%) of the cases the system incorrectly inferred that the floor was being released to it. In 79% of these cases, the system took the floor and produced a verbal contribution. Since the floor was not released to the system, such errors can lead to significant turn-taking problems, which often manifest as floor conflicts marked by turn-initial overlaps, where a participant and the system start speaking around the same  time (see Figure 3). Operationally, we define turninitial overlaps as all detected overlaps with an actual onset of less than 300 milliseconds from the beginning of the system’s utterance (see discussion in Appendix A); the other overlaps are dubbed turn-internal. We note that the time at which an overlap is detected by the system lags behind the actual onset of the utterance by an average of about 700 milliseconds, due to core latencies in our audio and speech processing pipeline. Accounting for these computational lags, and others arising at different places in processing pipelines, raise challenges for turn taking in spoken dialog systems. 42% of the verbal takes performed incorrectly by the system led to turn-initial overlaps. This is not surprising, as the system starts speaking when the floor was not released to it. In some of these cases the same participant continues (e.g., diarization errors incorrectly segmented the utterance), or someone else starts speaking. The cost assessment experiment confirmed the impact of these errors – the average estimated cost was 1.76. If no turninitial overlap occurred after the system incorrectly took the floor, the average cost was 0.42. Clearly  103  floor conflicts come with a cost. The specific cost assessments we obtained are perhaps influenced to a degree by the role of game mediator played by the system. With this role, taking the floor in cases when the system was not addressed is perhaps not as costly as it might be in other domains. Note that 182 turn-initial overlaps also occur when the system takes the floor after correctly identifying that the floor was released to it (upperleft quadrant in Table 1). 17 of them are created by the system hearing itself as it starts speaking, due to errors in acoustic echo cancellation; these instances are marked Echo in Table 1. While the relative percentage of turn-initial overlaps is smaller after a floor release to the system (~10%), the majority of all turn-initial overlaps (shaded cells in Table 1) occur in this context, because of the larger incidence of the situation. Often, these utterances contain an immediate answer or a short confirmation from another participant. The cost of these turn-initial overlaps is also much lower: 0.25 versus 1.76 (again, the cost structure is probably sensitive to details of the domain). We believe the turn-initial overlaps that occur when the floor is released to the system can be explained in part by the interpretation of the system’s short delay in responding (per processing) as a signal that the system is not taking the floor, leading other participants to take initiative. As another factor, turn taking is a mixed-initiative process, and other participants might vie for the floor and issue their own contributions immediately after an answer directed to the system. These observations bring to the fore two questions: (1) how can we minimize the number of turn-initial overlaps, and (2) how can the system gracefully handle such overlaps once they occur? One approach to minimizing turn-initial overlaps is to reduce the system’s response delays via faster processing or via the use of predictive models to anticipate the end of turns (e.g. Ferrer et al., 2003; Schlangen, 2006; Raux and Eskenazi, 2008; Skantze and Schlangen, 2009). Multiparty settings require methods for forecasting not only when a current speaker will finish, but also whether any participant will try to take (or release) the floor within a small window of time in the future, i.e., accurately modeling all floor intentions. Our turntaking framework includes components for representing and modeling floor intentions, but these are not used in the current system. We believe there is  promise in learning models to predict floor intentions and the timing of ends of utterances from interaction data. The availability of such predictions can fuel additional turn-taking strategies and also pave the way to more graceful handling of turninitial overlaps after they occur. For instance, if the system can anticipate that someone else might start speaking, it might still decide to take the floor but it might start with a filler, e.g., “So [pause] What do you think?” constructing a natural opportunity for resolving a potential conflict after “So” We plan to investigate the use of decision-theoretic methods to anticipate and resolve such conflicts by introducing and modulating an array of strategies, including the use of fillers, restarts, and acknowledgment gestures. In 21% of the 305 incorrectly detected floor releases to the system, our system immediately performed a non-verbal floor release to another participant by turning the avatar’s face towards them and raising its eyebrows (Take + Non-verbal Release in Table 1). These situations are not costly, as the system’s action does not interrupt the flow of the conversation. Indeed they were never penalized in the cost assessment experiment that we conducted. However, the same action, performed when the floor is actually released to the system (13% of 2063 cases), has the potential to create problems if not properly recognized by the targeted participant as a floor release by the system; the average cost assessed in this case was 0.42. The right-hand column in Table 1 shows cases where the system detected that the floor was not released to it. In these cases, the system waits (performs null) for a specified duration. The cost assessment indicates that waiting in this situation is overall costly, and the cost depends on the ultimate outcome. If no one else takes the floor, the system will eventually do so (Delayed System Take cases in Table 1). In some of these cases, turn-initial overlaps also occur. The 277 cases in which the system fails to detect that the floor was in fact released to it lead to no immediate response from the system. In these cases the system can be perceived as unresponsive and the participants eventually repeat themselves. We believe that performance can be improved with the use of an ongoing decisiontheoretic analysis that continuously reassesses the situation while the system waits. Such an analysis would consider the delay, floor holder’s previous actions, inferences about participants’ floor inten-  104  tions, and cost-benefit tradeoffs of different floor actions. 5.3. Release versus Hold We now turn our attention to the system’s decisions to release the floor. Recall that, according to the current policy, the system performs a floor hold while it is speaking and a floor release at the end of its outputs. In addition, if an overlap (i.e., barge-in) was detected during question dialog acts, the system performed a floor release immediately, interrupting its own output and allowing for the user barge-in. Since such barge-ins were allowed only during the question dialog acts, as Table 2 shows, the current policy leads to an abundance of cases in which the system performs hold when an overlap is detected. Some of these cases are continuers: the overlap only happens at the very end of the system’s output. These cases do not create significant turn-taking problems, as the floor still transitions to the participant relatively quickly (the system releases at the end of its output). However, in a significant number of cases the system appears to ignore the participants (shaded cells in Table 2). About three quarters of these overlaps occur while the system is providing an explanation after an incorrect answer. Observations of the data indicate that in these cases participants may discuss or give their opinion on the answer or some aspect of the system’s explanation, while ignoring the system as it blindly continues the explanation. We have separated in Table 2 turn-initial from turn-internal overlaps. The two types of overlaps reflect different phenomena. As we have discussed, turn-initial overlaps mark floor conflicts, and various strategies could be used to negotiate such conflicts (e.g., Yang and Heeman, 2010). In contrast, turn-internal overlaps may reflect efforts by other participants to take the floor, or might simply be  Turn Initia l  Overlap Type  Action performed by system when overlap detected  HOLD  RELEASE  315 (23%)  Overlap  Continuer  285 (90%)  30 (10%)  [14 Echo]  [3 Echo]  43 (3%) [7 Echo]  968 (69%)  Overlap 828 (86%) [44 Echo]  Continuer 140 (14%) [7 Echo]  73 (5%) [13 Echo]  Table 2. Decisions to release floor (vs. hold).  Turn Internal  backchannels, laughter, exclamations or other lexical or non-lexical events that do not mark a claim for the floor. Making appropriate floor control decisions in this case will require models for reliably distinguishing between the two, i.e., between the take or null floor actions of the participants. This is an especially challenging inference problem as decisions need to be made as early as possible after the onset of an utterance. We note the relatively large incidence of failures in echo cancellation in our microphone array. On the utterances marked Echo in Table 2, the system heard itself and thought a user was speaking. We believe these failures could be significantly reduced with better acoustic echo cancellation.  5.4. Subjective Assessment  Finally, we present results from a subjective assessment of the system by participants, based on a post-experiment survey. The survey included several 7-point Likert scale questions related to turn taking, which are displayed in Figure 4, together with the mean user responses and the corresponding 95% confidence intervals. Generally, participants rated the system’s turn-taking abilities favorably, with scores around 4.5-5. No statistically significant differences were detected in assessments across the participant’s gender or previous familiarity with speech recognition systems. We also note that a parallel human—human interaction study would help us characterize better the system’s performance relative to human dialog.  
Conventional speech recognition approaches usually wait until the user has ﬁnished talking before returning a recognition hypothesis. This results in spoken dialogue systems that are unable to react while the user is still speaking. Incremental Speech Recognition (ISR), where partial phrase results are returned during user speech, has been used to create more reactive systems. However, ISR output is unstable and so prone to revision as more speech is decoded. This paper tackles the problem of stability in ISR. We ﬁrst present a method that increases the stability and accuracy of ISR output, without adding delay. Given that some revisions are unavoidable, we next present a pair of methods for predicting the stability and accuracy of ISR results. Taken together, we believe these approaches give ISR more utility for real spoken dialogue systems. 
We present the novel task of predicting temporal features of continuations of user input, while that input is still ongoing. We show that the remaining duration of an ongoing word, as well as the duration of the next can be predicted reasonably well, and we put this information to use in a system that synchronously completes a user’s speech. While we focus on collaborative completions, the techniques presented here may also be useful for the alignment of back-channels and immediate turn-taking in an incremental SDS, or to synchronously monitor the user’s speech ﬂuency for other reasons. 
This paper provides a ﬁrst assessment of a statistical dialog system in public use. In our dialog system there are four main recognition tasks, or slots – bus route names, bus-stop locations, dates, and times. Whereas a conventional system tracks a single value for each slot – i.e., the speech recognizer’s top hypothesis – our statistical system tracks a distribution of many possible values over each slot. Past work in lab studies has showed that this distribution improves robustness to speech recognition errors; but to our surprise, we found the distribution yielded an increase in accuracy for only two of the four slots, and actually decreased accuracy in the other two. In this paper, we identify root causes for these differences in performance, including intrinsic properties of N-best lists, parameter settings, and the quality of statistical models. We synthesize our ﬁndings into a set of guidelines which aim to assist researchers and practitioners employing statistical techniques in future dialog systems. 
Generating Temporal Expressions (TE) that are easy to understand, unambiguous, and reasonably short is a challenge for humans and Spoken Dialogue Systems. Rather than developing hand-written decision rules, we adopt a data-driven approach by collecting user feedback on a variety of possible TEs in terms of task success, ambiguity, and user preference. The data collected in this work is freely available to the research community. These data were then used to train a simulated user and a reinforcement learning policy that learns an adaptive Temporal Expression generation strategy for a variety of contexts. We evaluate our learned policy both in simulation and with real users and show that this data-driven adaptive policy is a signiﬁcant improvement over a rule-based adaptive policy, leading to a 24% increase in perceived task completion, while showing a small increase in actual task completion, and a 16% decrease in call duration. This means that dialogues are more efﬁcient and that users are also more conﬁdent about the appointment that they have agreed with the system. 
Detecting levels of interest from speakers is a new problem in Spoken Dialog Understanding with signiﬁcant impact on real world business applications. Previous work has focused on the analysis of traditional acoustic signals and shallow lexical features. In this paper, we present a novel hierarchical fusion learning model that takes feedback from previous multistream predictions of prominent seed samples into account and uses a mean cosine similarity measure to learn rules that improve reclassiﬁcation. Our method is domain-independent and can be adapted to other speech and language processing areas where domain adaptation is expensive to perform. Incorporating Discriminative Term Frequency and Inverse Document Frequency (DTFIDF), lexical affect scoring, and low and high level prosodic and acoustic features, our experiments outperform the published results of all systems participating in the 2010 Interspeech Paralinguistic Affect Subchallenge. 
User satisfaction is a common evaluation metric in task-oriented dialogue systems, whereas tutorial dialogue systems are often evaluated in terms of student learning gain. However, user satisfaction is also important for such systems, since it may predict technology acceptance. We present a detailed satisfaction questionnaire used in evaluating the BEETLE II system (REVU-NL), and explore the underlying components of user satisfaction using factor analysis. We demonstrate interesting patterns of interaction between interpretation quality, satisfaction and the dialogue policy, highlighting the importance of more ﬁnegrained evaluation of user satisfaction. 
In this work we describe the modeling and prediction of Interaction Quality (IQ) in Spoken Dialogue Systems (SDS) using Support Vector Machines. The model can be employed to estimate the quality of the ongoing interaction at arbitrary points in a spoken humancomputer interaction. We show that the use of 52 completely automatic features characterizing the system-user exchange signiﬁcantly outperforms state-of-the-art approaches. The model is evaluated on publically available data from the CMU Let’s Go Bus Information system. It reaches a performance of 61.6% unweighted average recall when discriminating between 5 classes (good to very poor). It can be further shown that incorporating knowledge about the user’s emotional state does hardly improve the performance. 
SMS dictation by voice is becoming a viable alternative providing a convenient method for texting in a variety of environments. Contextual knowledge should be used to improve performance. We propose to add topic knowledge as part of the contextual awareness of both texting partners during SMS conversations. Topics can be used for speech applications, if the relation between the conversed topics and the choice of words in SMS dialogs is measurable. In this study, we collected an SMS corpus, developed a topic annotation scheme, and built a topic hierarchy in a tree structure. We validated our topic assignments and tree structure by the Agglomerative Information Bottleneck method, which also proved the measurability of the interrelation between topics and wording. To quantify this relation we propose a naïve classi- fication method based on the calculation of topic distinctive word lists and compare the classifiers‟ topic recognition capabilities for SMS dialogs with unigram language models. The results demonstrate that the relation between topic and wording is significant and can be integrated into SMS dictation. 
Many discourse connectives can signal several types of relations between sentences. Their automatic disambiguation, i.e. the labeling of the correct sense of each occurrence, is important for discourse parsing, but could also be helpful to machine translation. We describe new approaches for improving the accuracy of manual annotation of three discourse connectives (two English, one French) by using parallel corpora. An appropriate set of labels for each connective can be found using information from their translations. Our results for automatic disambiguation are state-of-the-art, at up to 85% accuracy using surface features. Using feature analysis, contextual features are shown to be useful across languages and connectives. 
We propose a method for modelling how dialogue moves inﬂuence and are inﬂuenced by the agents’ preferences. We extract constraints on preferences and dependencies among them, even when they are expressed indirectly, by exploiting discourse structure. Our method relies on a study of 20 dialogues chosen at random from the Verbmobil corpus. We then test the algorithms predictions against the judgements of naive annotators on 3 random unseen dialogues. The average annotator-algorithm agreement and the average inter-annotator agreement show that our method is reliable. 
We present a method of evaluating the immediate performance impact of user state misclassiﬁcations in spoken dialogue systems. We illustrate the method with a tutoring system that adapts to student uncertainty over and above correctness. First we deﬁne a ranking of user states representing local performance. Second, we compare user state trajectories when the ﬁrst state is accurately classiﬁed versus misclassiﬁed. Trajectories are quantiﬁed using a previously proposed metric representing the likelihood of transitioning from one user state to another. Comparison of the two sets of trajectories shows whether user state misclassiﬁcations change the likelihood of subsequent higher or lower ranked states, relative to accurate classiﬁcation. Our tutoring system results illustrate the case where user state misclassiﬁcation increases the likelihood of negative performance trajectories as compared to accurate classiﬁcation. 
Instructional efficacy of automated Conversational Agents designed to help small groups of students achieve higher learning outcomes can be improved by the use of social interaction strategies. These strategies help the tutor agent manage the attention of the students while delivering useful instructional content. Two technical challenges involving the use of social interaction strategies include determining the appropriate policy for triggering these strategies and regulating the amount of social behavior performed by the tutor. In this paper, a comparison of six different triggering policies is presented. We find that a triggering policy learnt from human behavior in combination with a filter that keeps the amount of social behavior comparable to that performed by human tutors offers the most effective solution to the these challenges. 
Mental modeling is crucial for natural humanrobot interactions (HRI). Yet, effective mechanisms that enable reasoning about and communication of mental states are not available. We propose to utilize adverbial cues, routinely employed by humans, for this goal and present a novel algorithm that integrates adverbial modiﬁers with belief revision and expression, phrasing utterances based on Gricean conversational maxims. The algorithm is demonstrated in a simple HRI scenario. 
This paper presents a progressively challenging series of experiments that investigate clariﬁcation subdialogues to resolve the words in noisy transcriptions of user utterances. We focus on user utterances where the user’s speciﬁc intent requires little additional inference, given sufﬁcient understanding of the form. We learned decision-making strategies for a dialogue manager from run-time features of our spoken dialogue system and from observation of human wizards we had embedded within it. Results show that noisy ASR can be resolved based on predictions from context about what a user might say, and that dialogue management strategies for clariﬁcations of linguistic form beneﬁt from access to features from spoken language understanding. 
This paper addresses a ﬁrst step toward a spoken dialogue system that evokes user’s spontaneous backchannels. We construct an HMM-based dialogue-style text-to-speech (TTS) system that generates human-like cues that evoke users’ backchannels. A spoken dialogue system for information navigation was implemented and the TTS was evaluated in terms of evoked user backchannels. We conducted user experiments and demonstrated that the user backchannels evoked by our TTS are more informative for the system in detecting users’ feelings than those by conventional reading-style TTS. 
This paper reports on an experiment that investigates clarification subdialogues in intentionally noisy speech recognition. The architecture learns weights for mixtures of grounding strategies from examples provided by a human wizard embedded in the system. Results indicate that the architecture learns to eliminate misunderstandings reliably despite high word error rate. 
We present a novel annotation scheme for cross-cultural argumentation and persuasion dialogues. This scheme is an adaptation of existing coding schemes on negotiation, following a review of literature on cross-cultural differences in negotiation styles. The scheme has been reﬁned through application to coding both two-party and multi-party negotiation dialogues in three different domains, and is general enough to be applicable to different domains with few if any extensions. Dialogues annotated with the scheme have been used to successfully learn culture-speciﬁc dialogue policies for argumentation and persuasion. 
We present an approach to performing automated evaluations of pipeline architectures in natural language dialogue systems. Our approach addresses some of the difﬁculties that arise in such automated evaluations, including the lack of consensus among human annotators about the correct outputs within the processing pipeline, the availability of multiple acceptable system responses to some user utterances, and the complex relationship between system responses and internal processing results. Our approach includes the development of a corpus of richly annotated target dialogues, simulations of the pipeline processing that could occur in these dialogues, and an analysis of how system responses vary based on internal processing results within the pipeline. We illustrate our approach in two implemented virtual human dialogue systems. 
Linguistic markers of personality traits have been studied extensively, but few crosscultural studies exist. In this paper, we evaluate how native speakers of American English and Arabic perceive personality traits and naturalness of English utterances that vary along the dimensions of verbosity, hedging, lexical and syntactic alignment, and formality. The utterances are the turns within dialogue fragments that are presented as text transcripts to the workers of Amazon’s Mechanical Turk. The results of the study suggest that all four dimensions can be used as linguistic markers of all personality traits by both language communities. A further comparative analysis shows cross-cultural differences for some combinations of measures of personality traits and naturalness, the dimensions of linguistic variability and dialogue acts. 
We present a new approach to dialogue management based on the use of multiple, interconnected policies. Instead of capturing the complexity of the interaction in a single large policy, the dialogue manager operates with a collection of small local policies combined concurrently and hierarchically. The metacontrol of these policies relies on an activation vector updated before and after each turn. 
 pronouns, both personal (I, you, it, they), and deictic  (this, that, these, those, here, there). Hence, this pa-  Within our ongoing effort to develop a computational model to understand multi-modal human dialogue in the ﬁeld of elderly care, this paper focuses on pronominal and deictic co-reference resolution. After describing our data collection effort, we discuss our annotation scheme. We developed a co-reference  per presents our ﬁrst steps toward a full co-reference resolution module, and ultimately, the multi-modal interface. Co-reference resolution is likely the discourse and dialogue processing task that has received the most attention. However, as Eisenstein and Davis  model that employs both a simple notion of  (2006) notes, research on co-reference resolution  markable type, and multiple statistical models. Our results show that knowing the type of the markable, and the presence of simultaneous pointing gestures improve co-reference resolution for personal and deictic pronouns.  has mostly been applied to written text; this task is more difﬁcult in dialogue. First, utterances may be informal, ungrammatical or disﬂuent; second, people spontaneously use hand gestures, body gestures and gaze. Pointing gestures are the eas-  
Many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users’ affect. Previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models. Thus, we wish to investigate how more minor differences, like small dialogue content changes and switching from a wizarded system to a fully automated system, inﬂuence the performance of our affect detection models. We perform a post-hoc experiment where we use various data sets to train multiple models, and compare against a test set from the most recent version of our dialogue system. Analyzing these results strongly suggests that these differences do impact these models’ performance. 
Error-return plots show the rate of error (misunderstanding) against the rate of nonreturn (non-understanding) for Natural Language Processing systems. They are a useful visual tool for judging system performance when other measures such as recall/precision and detection-error tradeoff are less informative, speciﬁcally when a system is judged on the correctness of its responses, but may elect to not return a response. 
We apply a PARADISE-style evaluation to a human-human dialogue corpus that was collected to support the design of a spoken dialogue system for library transactions. The book request dialogue task we investigate is informational in nature: a book request is considered successful if the librarian is able to identify a specific book for the patron. PARADISE assumes that user satisfaction can be modeled as a regression over task success and dialogue costs. The PARADISE model we derive includes features that characterize two types of qualitative features. The first has to do with the specificity of the communicative goals, given a request for an item. The second has to do with the number and location of overlapping turns, which can sometimes signal rapport between the speakers. 
The semantic annotation of dialogue corpora permits building efﬁcient language understanding applications for supporting enjoyable and effective human-machine interactions. Nevertheless, the annotation process could be costly, time-consuming and complicated, particularly the more expressive is the semantic formalism. In this work, we propose a bootstrapping architecture for the semantic annotation of dialogue corpora with rich structures, based on Dependency Syntax and Frame Semantics. 
This paper describes an implemented monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. The system uses mappings between discourse relations in text and dialogue acts in dialogue. These mappings are extracted from a parallel monologue and dialogue corpus. 
We present BEETLE II, a tutorial dialogue system which accepts unrestricted language input and supports experimentation with different dialogue strategies. Our ﬁrst system evaluation compared two dialogue policies. The resulting corpus was used to study the impact of different tutoring and error recovery strategies on user satisfaction and student interaction style. It can also be used in the future to study a wide range of research issues in dialogue systems. 
When a robot is situated in an environment containing multiple possible interaction partners, it has to make decisions about when to engage speciﬁc users and how to detect and react appropriately to actions of the users that might signal the intention to interact. In this demonstration we present the integration of an engagement model in an existing dialog system based on interaction patterns. As a sample scenario, this enables the humanoid robot Nao to play a quiz game with multiple participants.  tions and the more abstract engagement intention). Each user can be engaged in speciﬁc interactions (denoting different “basic unit[s] of sustained, interactive problem-solving”) and there can be multiple such interactions, each with potentially different users. This demonstration shows how an engagement model inspired by these ideas was integrated into an existing dialog system and how it helps in realizing interactive scenarios with a robot that incorporate cues for the dialog from the system’s environment. Section 3 gives more details about this model and how it is used by the dialog.  
This demonstration will illustrate an interactive immersive computer game, POMY, designed to help Korean speakers learn English. This system allows learners to exercise their visual and aural senses, receiving a full immersion experience to increase their memory and concentration abilities to a greatest extent. In POMY, learners can have free conversations with game characters and receive corrective feedback to their errors. Game characters show various emotional expressions based on learners’ input to keep learners motivated. Through this system, learners can repeatedly practice conversations in everyday life setting in a foreign language with no embarrassment. 
We demonstrate a dialogue system and the accompanying authoring tools that are designed to allow authors with little or no experience in building dialogue systems to rapidly build advanced question-answering characters. To date seven such virtual characters have been built by non-experts using this architecture and tools. Here we demonstrate one such character, PFC Sean Avery, which was developed by a non-expert in 3 months. 
The Automatic Content Linking Device is a just-in-time document retrieval system that monitors an ongoing dialogue or monologue and enriches it with potentially related documents from local repositories or from the Web. The documents are found using queries that are built from the dialogue words, obtained through automatic speech recognition. Results are displayed in real time to the dialogue participants, or to people watching a recorded dialogue or a talk. The system can be demonstrated in both settings. 
The paper presents a system for the CoNLL2011 share task of coreference resolution. The system composes of two components: one for mentions detection and another one for their coreference resolution. For mentions detection, we adopted a number of heuristic rules from syntactic parse tree perspective. For coreference resolution, we apply SVM by exploiting multiple syntactic and semantic features. The experiments on the CoNLL-2011 corpus show that our rule-based mention identiﬁcation system obtains a recall of 87.69%, and the best result of the SVM-based coreference resolution system is an average F-score 50.92% of the MUC, B-CUBED and CEAFE metrics. 
Our system treats coreference resolution as an integer linear programming (ILP) problem. Extending Denis and Baldridge (2007) and Finkel and Manning (2008)’s work, we exploit loose transitivity constraints on coreference pairs. Instead of enforcing transitivity closure constraints, which brings Ç´Ò¿µ complexity, we employ a strategy to reduce the number of constraints without large performance decrease, i.e., eliminating coreference pairs with probability below a threshold . Experimental results show that it achieves a better performance than pairwise classiﬁers. 
In this paper, we describe the algorithms and experimental results of Brandeis University in the participation of the CoNLL Task 2011 closed track. We report the features used in our system, and describe a novel cluster-based chaining algorithm to improve performance of coreference identiﬁcation. We evaluate the system using the OntoNotes data set and describe our results. 
This paper describes our entry to the 2011 CoNLL closed task (Pradhan et al., 2011) on modeling unrestricted coreference in OntoNotes. Our system is based on the Reconcile coreference resolution research platform. Reconcile is a general software infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Our entry for the CoNLL closed task is a conﬁguration of Reconcile intended to do well on OntoNotes data. This paper describes our conﬁguration of Reconcile as well as the changes that we had to implement to integrate with the OntoNotes task deﬁnition and data formats. We also present and discuss the performance of our system under different testing conditions on a withheld validation set. 
In this paper, we present our supervised learning approach to coreference resolution in ConLL corpus. The system relies on a maximum entropy-based classifier for pairs of mentions, and adopts a rich linguisitically motivated feature set, which mostly has been introduced by Soon et al (2001), and experiment with alternaive resolution process, preprocessing tools,and classifiers. We optimize the system’s performance for MUC (Vilain et al, 1995), BCUB (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) .  on a rich linguistically motivated feature set. Our system architecture makes it possible to define other kinds of features: atmoic word and markable features. This approach to feature engineering is suitable not only for knowledge-rich but also for knowledge-poor datasets. Finally, we use the bestfirst clustering to create the coreference chains. 2. System Description This section briefly describes our system. First the mention detection is presented. Next, the features which we import are described. Finally, we describled the learning and encoding methods.  1. Introduction The coreference resolution is the task in which all expressions refer to the same entity in a discourse will be identified. As the core of natural language processing, coreference resolution is significant to message understanding, information extraction, text summarization, information retrieval, information filtration, and machine translation. A considerable engineering efforts is needed for the full coreference resolution task, and a significant part of this effort concerns feature engineering. The backbone of our system can be split into two subproblems: mention detection and creation of entitly. We train a mention detector on the training texts. Once the mentions are identified, coreference resolution involves partitioning them into subsets corresponding to the same entity. This problem is cast into the binary classification problem of deciding whether two given mentions are coreferent. Our system relies on maximum entropy-based classifier for pairs of mentions. Our system relies  2.1 Mention Detector The first stage of the coreference resolution process try to identify the occurrence of mentions in document. To detect system mention from a test text, we train a mention detector on the training data. We formulate the mention problem as a classification, by assigning to each token in the text a label, indicating whether it is a mention or not. Hence, to learn the detector, we create one training text and derive its class value (one of b, i, o) from the annotated data. Each instance represents the , the token under consideration, and consists of 19 linguistic features, many of which are modeled after the systems of Bikel et al. (1999) and Florian et al. (2004) , as describled below. (1) Lexical: Tokens in the windows of three words before and after the target word: { ,…, }. (2) Capitalization: Determine whether is IsAllCaP (all the characters of word are capitalized, such as “BBN”), IsInitCap (the word starts with a capitalized character,  127 
In this paper we describe a rule-based system developed for the BioNLP 2011 GENIA event detection task. The system applies Kybots (Knowledge Yielding Robots) on annotated texts to extract bio-events involving proteins or genes. The main goal of this work is to verify the usefulness and portability of the Kybot technology to the domain of biomedicine. 
A growing body of work has highlighted the challenges of identifying the stance a speaker holds towards a particular topic, a task that involves identifying a holistic subjective disposition. We examine stance classiﬁcation on a corpus of 4873 posts across 14 topics on ConvinceMe.net, ranging from the playful to the ideological. We show that ideological debates feature a greater share of rebuttal posts, and that rebuttal posts are signiﬁcantly harder to classify for stance, for both humans and trained classiﬁers. We also demonstrate that the number of subjective expressions varies across debates, a fact correlated with the performance of systems sensitive to sentimentbearing terms. We present results for identiﬁng rebuttals with 63% accuracy, and for identifying stance on a per topic basis that range from 54% to 69%, as compared to unigram baselines that vary between 49% and 60%. Our results suggest that methods that take into account the dialogic context of such posts might be fruitful.  websites vary a great deal. One important contextual variable, discussed at length below, is the percentage of posts that are rebuttals to previous posts, which varies in our data from 34% to 80%. The ability to explicitly rebut a previous post gives these debates both monologic and dialogic properties (Biber, 1991; Crystal, 2001; Fox Tree, 2010); Compare Figure 1 to Figure 2. We believe that discussions containing many rebuttal links require a different type of analysis than other types of debates or discussions. Dialogic Capital Punishment Studies have shown that using the death penalty saves 4 to 13 lives per execution. That alone makes killing murderers worthwhile. What studies? I have never seen ANY evidence that capital punishment acts as a deterrant to crime. I have not seen any evidence that it is “just” either. When Texas and Florida were executing people one after the other in the late 90’s, the murder rates in both states plunged, like Rosie O’donnel off a diet.. . That’s your evidence? What happened to those studies? In the late 90s a LOT of things were different than the periods preceding and following the one you mention. We have no way to determine what of those contributed to a lower murder rate, if indeed there was one. You have to prove a cause and effect relationship and you have failed.  
This paper presents a lexicon model for subjectivity description of Dutch verbs that offers a framework for the development of sentiment analysis and opinion mining applications based on a deep syntactic-semantic approach. The model aims to describe the detailed subjectivity relations that exist between the participants of the verbs, expressing multiple attitudes for each verb sense. Validation is provided by an annotation study that shows that these subtle subjectivity relations are reliably identifiable by human annotators. 
This article reports on the methodology and the development of a complementary information source for the meaning of the synsets of Princeton WordNet 3.0. This encoded information was built following the principles of the Osgoodian differential semantics theory and consists of numerical values which represent the scaling of the connotative meanings along the multiple dimensions defined by pairs of antonyms (factors). Depending on the selected factors, various facets of connotative meanings come under scrutiny and different types of textual subjective analysis may be conducted (opinion mining, sentiment analysis). 
The paper presents a semi-automatic approach to creating sentiment dictionaries in many languages. We ﬁrst produced high-level goldstandard sentiment dictionaries for two languages and then translated them automatically into third languages. Those words that can be found in both target language word lists are likely to be useful because their word senses are likely to be similar to that of the two source languages. These dictionaries can be further corrected, extended and improved. In this paper, we present results that verify our triangulation hypothesis, by evaluating triangulated lists and comparing them to nontriangulated machine-translated word lists. 
We propose a novel method to construct semantic orientation lexicons using large data and a thesaurus. To deal with large data, we use Count-Min sketch to store the approximate counts of all word pairs in a bounded space of 8GB. We use a thesaurus (like Roget) to constrain near-synonymous words to have the same polarity. This framework can easily scale to any language with a thesaurus and a unzipped corpus size ≥ 50 GB (12 billion tokens). We evaluate these lexicons intrinsically and extrinsically, and they perform comparable when compared to other existing lexicons. 
Locating documents carrying positive or negative favourability is an important application within media analysis. This paper presents some empirical results on the challenges facing a machine-learning approach to this kind of opinion mining. Some of the challenges include: the often considerable imbalance in the distribution of positive and negative samples; changes in the documents over time; and effective training and quantiﬁcation procedures for reporting results. This paper begins with three datasets generated by a media-analysis company, classifying documents in two ways: detecting the presence of favourability, and assessing negative vs. positive favourability. We then evaluate a machine-learning approach to automate the classiﬁcation process. We explore the effect of using ﬁve different types of features, the robustness of the models when tested on data taken from a later time period, and the effect of balancing the input data by undersampling. We ﬁnd varying choices for the optimum classiﬁer, feature set and training strategy depending on the task and dataset. 
Sentiment analysis is one of the recent, highly dynamic fields in Natural Language Processing. Most existing approaches are based on word-level analysis of texts and are able to detect only explicit expressions of sentiment. In this paper, we present an approach towards automatically detecting emotions (as underlying components of sentiment) from contexts in which no clues of sentiment appear, based on commonsense knowledge. The resource we built towards this aim – EmotiNet - is a knowledge base of concepts with associated affective value. Preliminary evaluations show that this approach is appropriate for the task of implicit emotion detection, thus improving the performance of sentiment detection and classification in text. 
To assist in the research of social networks in history, we develop machine-learning-based tools for the identiﬁcation and classiﬁcation of personal relationships. Our case study focuses on the Dutch social movement between 1870 and 1940, and is based on biographical texts describing the lives of notable people in this movement. We treat the identiﬁcation and the labeling of relations between two persons into positive, neutral, and negative both as a sequence of two tasks and as a single task. We observe that our machine-learning classiﬁers, support vector machines, produce better generalization performance on the single task. We show how a complete social network can be built from these classiﬁcations, and provide a qualitative analysis of the induced network using expert judgements on samples of the network. 
With the widespread use of email, we now have access to unprecedented amounts of text that we ourselves have written. In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in many types of mail. We create a large word–emotion association lexicon by crowdsourcing, and use it to compare emotions in love letters, hate mail, and suicide notes. We show that there are marked differences across genders in how they use emotion words in work-place email. For example, women use many words from the joy–sadness axis, whereas men prefer terms from the fear–trust axis. Finally, we show visualizations that can help people track emotions in their emails. 
In this paper, we focus on the impressions that people gain from reading articles in Japanese newspapers, and we propose a method for extracting and quantifying these impressions in real numbers. The target impressions are limited to those represented by three bipolar scales, “Happy – Sad,” “Glad – Angry,” and “Peaceful – Strained,” and the strength of each impression is computed as a real number between 1 and 7. First, we implement a method for computing impression values of articles using an impression lexicon. This lexicon represents a correlation between the words appearing in articles and the inﬂuence of these words on the readers’ impressions, and is created from a newspaper database using a word co-occurrence based method. We considered that some gaps would occur between values computed by such an unsupervised method and those judged by the readers, and we conducted experiments with 900 subjects to identify what gaps actually occurred. Consequently, we propose a new approach that uses regression equations to correct impression values computed by the method. Our investigation shows that accuracy is improved by a range of 23.2% to 42.7% by using regression equations. 
Recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon-based approaches where the set of features are generated by humans, to approaches that use general statistical measures where features are selected solely on empirical evidence. The advantage of statistical approaches is that they are fully automatic, however, they often fail to separate features that carry sentiment from those that do not. In this paper we propose a set of new feature selection schemes that use a Content and Syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities. By focusing only on the subjective expressions and ignoring the entities, we can choose more salient features for document-level sentiment analysis. The results obtained from using these features in a maximum entropy classiﬁer are competitive with the state-of-the-art machine learning approaches. 
We introduce a new emotion classiﬁcation task based on Leary’s Rose, a framework for interpersonal communication. We present a small dataset of 740 Dutch sentences, outline the annotation process and evaluate annotator agreement. We then evaluate the performance of several automatic classiﬁcation systems when classifying individual sentences according to the four quadrants and the eight octants of Leary’s Rose. SVM-based classiﬁers achieve average F-scores of up to 51% for 4-way classiﬁcation and 31% for 8-way classiﬁcation, which is well above chance level. We conclude that emotion classiﬁcation according to the Interpersonal Circumplex is a challenging task for both humans and machine learners. We expect classiﬁcation performance to increase as context information becomes available in future versions of our dataset. 
In this paper we explore the use of phrases occurring maximally in text as features for sentiment classification of product reviews. The goal is to find in a statistical way representative words and phrases used typically in positive and negative reviews. The approach does not rely on predefined sentiment lexicons, and the motivation for this is that potentially every word could be considered as expressing something positive and/or negative in different situations, and that the context and the personal attitude of the opinion holder should be taken into account when determining the polarity of the phrase, instead of doing this out of particular context. 
The research described in this work focuses on identifying key components for the task of irony detection. By means of analyzing a set of customer reviews, which are considered as ironic both in social and mass media, we try to ﬁnd hints about how to deal with this task from a computational point of view. Our objective is to gather a set of discriminating elements to represent irony. In particular, the kind of irony expressed in such reviews. To this end, we built a freely available data set with ironic reviews collected from Amazon. Such reviews were posted on the basis of an online viral effect; i.e. contents whose effect triggers a chain reaction on people. The ﬁndings were assessed employing three classiﬁers. The results show interesting hints regarding the patterns and, especially, regarding the implications for sentiment analysis. 
 classiﬁcation of text documents or smaller pieces  In most tasks related to opinion mining and sentiment analysis, it is necessary to compute the semantic orientation (i.e., positive or negative evaluative implications) of certain opinion expressions. Recent works suggest that se-  of text into objective and subjective, classiﬁcation of opinionated documents or individual sentences regarding the overall opinion (into “positive” and “negative” classes, or into a multi-point scale) or extraction of individual opinions from a piece of text  mantic orientation depends on application domains. Moreover, we think that semantic orientation depends on the speciﬁc targets (features) that an opinion is applied to. In this paper, we introduce a technique to build domainspeciﬁc, feature-level opinion lexicons in a semi-supervised manner: we ﬁrst induce a lexicon starting from a small set of annotated documents; then, we expand it automatically from a larger set of unannotated documents, using a new graph-based ranking algorithm. Our method was evaluated in three different domains (headphones, hotels and cars), using a corpus of product reviews which opinions were annotated at the feature level. We conclude that our method produces feature-level opinion lexicons with better accuracy and recall that domain-independent opinion lexicons using only a few annotated documents.  (may include opinion target, holder, polarity or intensity of the opinions, among others). As a key in solving most of these problems, the semantic orientation of some opinion expressions should be computed: a numeric value, usually between −1 and 1, referring to the negative or positive affective implications of a given word or prhase. These values can be collected in an opinion lexicon, so this resource can be accessed when needed. Many recent works (Popescu and Etzioni, 2005; Kanayama and Nasukawa, 2006; Cruz et al., 2010; Qiu et al., 2011) suggest the need for domainspeciﬁc opinion lexicons, containing semantic orientations of opinion expressions when used in a particular domain (e.g., the word “predictable” has opposite semantic orientations when used to deﬁne the driving experience of a car or the plot of a movie).  
 al., 2005). Complex features based on parse trees  The new trend in sentiment classiﬁcation is to use semantic features for representation of documents. We propose a semantic space based on WordNet senses for a supervised document-level sentiment classiﬁer. Not only does this show a better performance for sentiment classiﬁcation, it also opens opportunities for building a robust sentiment classiﬁer. We examine the possibility of using similarity metrics deﬁned on WordNet to address the problem of not ﬁnding a sense in the training corpus. Using three popular similarity metrics, we replace unknown synsets in the test set with a similar synset from the training set. An improvement of 6.2% is seen with respect to baseline using this approach.  have been explored for modeling high-accuracy polarity classiﬁers (Matsumoto et al., 2005). Text parsers have also been found to be helpful in modeling valence shifters as features for classiﬁcation (Kennedy and Inkpen, 2006). In general, the work in the context of supervised SA has focused on (but not limited to) different combinations of bagof-words-based and syntax-based models. The focus of this work is to represent a document as a set of sense-based features. We ask the following questions in this context: 1. Are WordNet senses better features as compared to words? 2. Can a sentiment classiﬁer be made robust with  respect to features unseen in the training cor-  
In this paper, we concentrate on the 3 of the tracks proposed in the NTCIR 8 MOAT, concerning the classification of sentences according to their opinionatedness, relevance and polarity. We propose a method for the detection of opinions, relevance, and polarity classification, based on ISR-WN (a resource for the multidimensional analysis with Relevant Semantic Trees of sentences using different WordNet-based information sources). Based on the results obtained, we can conclude that the resource and methods we propose are appropriate for the task, reaching the level of state-of-the-art approaches. 
In recent years microblogs have taken on an important role in the marketing sphere, in which they have been used for sharing opinions and/or experiences about a product or service. Companies and researchers have become interested in analysing the content generated over the most popular of these, the Twitter platform, to harvest information critical for their online reputation management (ORM). Critical to this task is the efﬁcient and accurate identiﬁcation of tweets which refer to a company distinguishing them from those which do not. The aim of this work is to present and compare two different approaches to achieve this. The obtained results are promising while at the same time highlighting the difﬁculty of this task. 
In this paper we present a method to automatically identify linguistic contexts which contain possible causes of emotions or emotional states from Italian newspaper articles (La Repubblica Corpus). Our methodology is based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge on emotional states and emotion eliciting situations. Our approach has been evaluated with respect to manually annotated data. The results obtained so far are satisfying and support the validity of the methodology proposed. 
 knowledge-based method to create an initial train-  In this study we investigate using an unsupervised generative learning method for subjectivity detection in text across different domains. We create an initial training set using simple lexicon information, and then evaluate a calibrated EM (expectation-maximization)  ing set, and then apply a calibrated EM method to learn from an unannotated corpus. Our experiments show signiﬁcant differences among the three domains: movie, news article, and meeting dialog. This can be explained by the inherent difference of the data, especially the task difﬁculty and classiﬁer’s  method to learn from unannotated data. We evaluate this unsupervised learning approach on three different domains: movie data, news resource, and meeting dialogues. We also perform a thorough analysis to examine impacting factors on unsupervised learning, such as the size and self-labeling accuracy of the initial training set. Our experiments and analysis show inherent differences across domains and performance gain from calibration in EM.  performance for a domain. We demonstrate that for some domains (e.g., movie data) the unsupervised learning methods can rival the supervised approach. 2 Related Work In the early age, knowledge-based methods were widely used for subjectivity detection. They used a lexicon or patterns and rules to predict whether a target is subjective or not. These methods tended  
The aim of this paper is to present an approach to tackle the task of opinion question answering and text summarization. Following the guidelines TAC 2008 Opinion Summarization Pilot task, we propose new methods for each of the major components of the process. In particular, for the information retrieval, opinion mining and summarization stages. The performance obtained improves with respect to the state of the art by approximately 12.50%, thus concluding that the suggested approaches for these three components are adequate. 
News articles have always been a prominent force in the formation of a company’s financial image in the minds of the general public, especially the investors. Given the large amount of news being generated these days through various websites, it is possible to mine the general sentiment of a particular company being portrayed by media agencies over a period of time, which can be utilized to gauge the long term impact on the investment potential of the company. However, given such a vast amount of news data, we need to first separate corporate news from other kinds namely, sports, entertainment, science & technology, etc. We propose a system which takes news as, checks whether it is of corporate nature, and then identifies the polarity of the sentiment expressed in the news. The system is also capable of distinguishing the company/organization which is the subject of the news from other organizations which find mention, and this is used to pair the sentiment polarity with the identified company. Introduction With the rapid advancements in the field of information technology, the amount of information available has increased tremendously. News articles constitute the largest available portion of  factual information about events happening in the world. Corporate news constitutes a major chunk of these news articles. Sentiment Mining applied to the corporate domain would help in various ways like Automatic Recommendation Systems, to help organizations evaluate their market strategies help them frame their advertisement campaigns. Our system tries to address these issues by automating the entire process of news collection, organization/product detection and sentiment mining. This paper is divided into two main parts. The first part describes a way of identifying corporate news from a collection of news articles and then pairing the news with the organization/company which is being talked about in the article. The second part of our paper works on the output of the first part (corporate news) and detects the valence of the identified corporate news articles. It calculates an overall score and identifies valence a s positive, negative or neutral based on this score. The system is immune to addition/mergers of companies, with regards to their identification, as it does not use any name lists. The model uses a machine learning approach to do this task. We extract a set of features from the news and use them to train a set of classifiers. The best model is then used to classify the test data. One advantage of our approach described below is that it only requires a very small amount of annotated training data. We trained the model on the NewsCorp dataset consisting of 860 annotated news articles. The system has shown promising  175  Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 175–181, 24 June, 2011, Portland, Oregon, USA c 2011 Association for Computational Linguistics  results on test data with classification accuracy being 92.05% and a f-measure of 92.00. The final average valence detection accuracy measured was 79.93%. Related Work Much work has been done on text classification.(Barak, 2009; Sebastiani,2002) There have been earlier attempts (Research on Sports Game News Information Extraction, Yonggui YANG,et al) However, they had focused mainly on information extraction and not classification. Earlier attempts on web news classification(Krishnlal et al, 2010) concentrated mainly on classification according to the domain of the news articles. Not much work has been done in the field of corporate news-company pairing. This paper tries to address a more general problem of detecting the main organization being talked about in the articles. Sentiment analysis in computational linguistics has focused on examining what textual features contribute to affective content of text and automatically detecting these features to derive a sentiment metric for a word, sentence or whole text. Niederhoffer (1971) after classifying New York Times headlines into 19 categories evaluated how the markets react to good and bad news. Davis et al (2006) investigate the effects of optimistic or pessimistic language used in financial press releases on future firm performance. Sumbaly et al(2009) used k gram models to detect sentiment in large news datasets. Devitt(2007) improves upon and Melville(2009) have done work on sentiment analysis of web blogs PART I : News Classification Steps involved in news classification 3.1 News Pre-processing The preprocessor merges all the files into one but defines start/end delimiters for each file in the merged file, to enable bulk processing. The merged news file is acted upon by a log-linear part of speech tagger we obtained from the Stanford NLP webpage(Manning,2000).  3.2 Organization detection We follow a two step approach to organization detection: Step 1: We extract the NNP/NNPS1 clusters in the POS-tagged file using reguar expressions. For example, the pos-tagged version of “General Electric Co”, is “ General_NNP Electric_NNP Co_NNP” which is detected as a likely candidate for an organization. Step 2: We use a Named Entity Recognizer[2] to obtain organization names. They are sorted in order of their frequencies and top three organizations are stored for later use. This ensures that even if some names have crept in as organizations due to misclassification by NER tagger, they end up at the bottom of the list and are discarded. Multiple Organization Focus: Let f1,f2 be the frequencies of top 2 organizations. Now if f2>f1/2 then the news article is paired with organizations corresponding to both f1 and f2. Baseline: Using just the frequency of top 3 organizations as features, we get an accuracy of 48.89% which is very low. Therefore, we add additional features which are described below. 3.3 Keyword Detection  The system matches each news article for  occurrence of a set of keywords like “company”,  “share”, “asset”, etc. which have been derived  from statistical observation of corporate news. We  have used POS tags to differentiate between the  contexts in which the keywords have been used.  For example, “share” (verb) is not a keyword but  “share” (noun) is a keyword. We calculate the net  keyword  occurrence  frequency  as  N(key)=  ) where N(key) is the total  keyword frequency and  is the frequency  of each keyword.  3.4 Headline Preprocessing We process the headline and detect likely candidates for organization names and then cross check with the top 3 organization names detected in the step 2.2. We introduce a new feature h_value described as follows: 
This paper presents two instance-level transfer learning based algorithms for cross lingual opinion analysis by transferring useful translated opinion examples from other languages as the supplementary training data for improving the opinion classiﬁer in target language. Starting from the union of small training data in target language and large translated examples in other languages, the Transfer AdaBoost algorithm is applied to iteratively reduce the inﬂuence of low quality translated examples. Alternatively, starting only from the training data in target language, the Transfer Self-training algorithm is designed to iteratively select high quality translated examples to enrich the training data set. These two algorithms are applied to sentence- and document-level cross lingual opinion analysis tasks, respectively. The evaluations show that these algorithms effectively improve the opinion analysis by exploiting small target language training data and large cross lingual training data. 
This paper describes the preliminary results of a system for extracting sentiments opinioned with regard with named entities. It also combines rule-based classification, statistics and machine learning in a new method. The accuracy and speed of extraction and classification are crucial. The service oriented architecture permits the end-user to work with a flexible interface in order to produce applications that range from aggregating consumer feedback on commercial products to measuring public opinion on political issues from blog and forums. The experiment has two versions available for testing, one with concrete extraction results and sentiment calculus and the other with internal metrics validation results.  its subject matter, is a form of information extraction from text, which recently focused a lot of research and growing commercial interest. This paper describes Sentimatrix, a sentiment analysis service, doing sentiment extraction and associating these analyses with named entities, in different languages. We seek to explore how sentiment analysis methods perform across languages, especially Romanian. The main applications that this system experiments with are monitoring the Internet before, during and after a campaign/message release and obtaining consumer feedback on different topics/products. In Section 2 we briefly discuss a state of the art in sentiment analysis, the system’s architecture is described in Section 3 and in Section 4 we focus on identifying opinions on Romanian. Subsequently, we present the experiment results, analysis and discussion in Sections 5 and 6. Future work and conclusions are briefly described in Section 7.  
In this paper we examine the sentence simpliﬁcation problem as an English-to-English translation problem, utilizing a corpus of 137K aligned sentence pairs extracted by aligning English Wikipedia and Simple English Wikipedia. This data set contains the full range of transformation operations including rewording, reordering, insertion and deletion. We introduce a new translation model for text simpliﬁcation that extends a phrasebased machine translation approach to include phrasal deletion. Evaluated based on three metrics that compare against a human reference (BLEU, word-F1 and SSA) our new approach performs signiﬁcantly better than two text compression techniques (including T3) and the phrase-based translation system without deletion. 
In this work, we present a scenario where contextual targeted paraphrasing of sub-sentential phrases is performed automatically to support the task of text revision. Candidate paraphrases are obtained from a preexisting repertoire and validated in the context of the original sentence using information derived from the Web. We report on experiments on French, where the original sentences to be rewritten are taken from a rewriting memory automatically extracted from the edit history of Wikipedia. 
We present a method for the sentence-level alignment of short simpliﬁed text to the original text from which they were adapted. Our goal is to align a medium-sized corpus of parallel text, consisting of short news texts in Spanish with their simpliﬁed counterpart. No training data is available for this task, so we have to rely on unsupervised learning. In contrast to bilingual sentence alignment, in this task we can exploit the fact that the probability of sentence correspondence can be estimated from lexical similarity between sentences. We show that the algoithm employed performs better than a baseline which approaches the problem with a TF*IDF sentence similarity metric. The alignment algorithm is being used for the creation of a corpus for the study of text simpliﬁcation in the Spanish language. 
Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source. 
In our work we use an existing classiﬁer to quantify and analyze the level of speciﬁc and general content in news documents and their human and automatic summaries. We discover that while human abstracts contain a more balanced mix of general and speciﬁc content, automatic summaries are overwhelmingly speciﬁc. We also provide an analysis of summary speciﬁcity and the summary quality scores assigned by people. We ﬁnd that too much speciﬁcity could adversely affect the quality of content in the summary. Our ﬁndings give strong evidence for the need for a new task in abstractive summarization: identiﬁcation and generation of general sentences. 
We examine the task of strict sentence intersection: a variant of sentence fusion in which the output must only contain the information present in all input sentences and nothing more. Our proposed approach involves alignment and generalization over the input sentences to produce a generation lattice; we then compare a standard search-based approach for decoding an intersection from this lattice to an integer linear program that preserves aligned content while minimizing the disﬂuency in interleaving text segments. In addition, we introduce novel evaluation strategies for intersection problems that employ entailmentstyle judgments for determining the validity of system-generated intersections. Our experiments show that the proposed models produce valid intersections a majority of the time and that the segmented decoder yields advantages over the search-based approach. 
We present a system for fusing sentences which are drawn from the same source document but have different content. Unlike previous work, our approach is supervised, training on real-world examples of sentences fused by professional journalists in the process of editing news articles. Like Filippova and Strube (2008), our system merges dependency graphs using Integer Linear Programming. However, instead of aligning the inputs as a preprocess, we integrate the tasks of ﬁnding an alignment and selecting a merged sentence into a joint optimization problem, and learn parameters for this optimization using a structured online algorithm. Evaluation by human judges shows that our technique produces fused sentences that are both informative and readable. 
We propose a new, ambitious framework for abstractive summarization, which aims at selecting the content of a summary not from sentences, but from an abstract representation of the source documents. This abstract representation relies on the concept of Information Items (INIT), which we deﬁne as the smallest element of coherent information in a text or a sentence. Our framework differs from previous abstractive summarization models in requiring a semantic analysis of the text. We present a ﬁrst attempt made at developing a system from this framework, along with evaluation results for it from TAC 2010. We also present related work, both from within and outside of the automatic summarization domain. 
We present a method of creating disjunctive logical forms (DLFs) from aligned sentences for grammar-based paraphrase generation using the OpenCCG broad coverage surface realizer. The method takes as input word-level alignments of two sentences that are paraphrases and projects these alignments onto the logical forms that result from automatically parsing these sentences. The projected alignments are then converted into phrasal edits for producing DLFs in both directions, where the disjunctions represent alternative choices at the level of semantic dependencies. The resulting DLFs are fed into the OpenCCG realizer for n-best realization, using a pruning strategy that encourages lexical diversity. After merging, the approach yields an n-best list of paraphrases that contain grammatical alternatives to each original sentence, as well as paraphrases that mix and match content from the pair. A preliminary error analysis suggests that the approach could beneﬁt from taking the word order in the original sentences into account. We conclude with a discussion of plans for future work, highlighting the method’s potential use in enhancing automatic MT evaluation. 
We present a substitution-only approach to sentence compression which “tightens” a sentence by reducing its character length. Replacing phrases with shorter paraphrases yields paraphrastic compressions as short as 60% of the original length. In support of this task, we introduce a novel technique for re-ranking paraphrases extracted from bilingual corpora. At high compression rates1 paraphrastic compressions outperform a state-of-the-art deletion model in an oracle experiment. For further compression, deleting from oracle paraphrastic compressions preserves more meaning than deletion alone. In either setting, paraphrastic compression shows promise for surpassing deletion-only methods. 
This work surveys existing evaluation methodologies for the task of sentence compression, identiﬁes their shortcomings, and proposes alternatives. In particular, we examine the problems of evaluating paraphrastic compression and comparing the output of different models. We demonstrate that compression rate is a strong predictor of compression quality and that perceived improvement over other models is often a side effect of producing longer output. 
We present a general and simple method to adapt an existing NLP tool in order to enable it to deal with historical varieties of languages. This approach consists basically in expanding the dictionary with the old word variants and in retraining the tagger with a small training corpus. We implement this approach for Old Spanish. The results of a thorough evaluation over the extended tool show that using this method an almost state-of-the-art performance is obtained, adequate to carry out quantitative studies in the humanities: 94.5% accuracy for the main part of speech and 92.6% for lemma. To our knowledge, this is the ﬁrst time that such a strategy is adopted to annotate historical language varieties and we believe that it could be used as well to deal with other non-standard varieties of languages. 
The paper describes a tagger for Old Czech (1200-1500 AD), a fusional language with rich morphology. The practical restrictions (no native speakers, limited corpora and lexicons, limited funding) make Old Czech an ideal candidate for a resource-light crosslingual method that we have been developing (e.g. Hana et al., 2004; Feldman and Hana, 2010). We use a traditional supervised tagger. However, instead of spending years of effort to create a large annotated corpus of Old Czech, we approximate it by a corpus of Modern Czech. We perform a series of simple transformations to make a modern text look more like a text in Old Czech and vice versa. We also use a resource-light morphological analyzer to provide candidate tags. The results are worse than the results of traditional taggers, but the amount of language-speciﬁc work needed is minimal. 
The goal of this study is to evaluate an ‘offthe-shelf’ POS-tagger for modern German on historical data from the Early Modern period (1650-1800). With no specialised tagger available for this particular stage of the language, our ﬁndings will be of particular interest to smaller, humanities-based projects wishing to add POS annotations to their historical data but which lack the means or resources to train a POS tagger themselves. Our study assesses the effects of spelling variation on the performance of the tagger, and investigates to what extent tagger performance can be improved by using ‘normalised’ input, where spelling variants in the corpus are standardised to a modern form. Our ﬁndings show that adding such a normalisation layer improves tagger performance considerably. 
e-Research explores the possibilities offered by ICT for science and technology. Its goal is to allow a better access to computing power, data and library resources. In essence eResearch is all about cyberstructure and being connected in ways that might change how we perceive scientiﬁc creation. The present work advocates open access to scientiﬁc data for linguists and language experts working within the Humanities. By describing the modules of an online application, we would like to outline how a linguistic tool can help the linguist. Work with data, from its creation to its integration into a publication is not rarely perceived as a chore. Given the right tools however, it can become a meaningful part of the linguistic investigation. The standard format for linguistic data in the Humanities is Interlinear Glosses. As such they represent a valuable resource even though linguists tend to disagree about the role and the methods by which data should inﬂuence linguistic exploration (Lehmann, 2004). In describing the components of our system we focus on the potential that this tool holds for real-time datasharing and continuous dissemination of research results throughout the life-cycle of a linguistic project. 
The paper describes a tool developed to process historical (Slovene) text, which annotates words in a TEI encoded corpus with their modern-day equivalents, morphosyntactic tags and lemmas. Such a tool is useful for developing historical corpora of highly-inflecting languages, enabling full text search in digital libraries of historical texts, for modernising such texts for today's readers and making it simpler to correct OCR transcriptions. 
In this paper, we report on how historical events are extracted from text within the Semantics of History research project. The project aims at the creation of resources for a historical information retrieval system that can handle the time-based dynamics and varying perspectives of Dutch historical archives. The historical event extraction module will be used for museum collections, allowing users to search for exhibits related to particular historical events or actors within time periods and geographic areas, extracted from accompanying text. We present here the methodology and tools used for the purpose of historical event extraction alongside with the first evaluation results. 
Cultural heritage institutions are making their digital content available and searchable online. Digital metadata descriptions play an important role in this endeavour. This metadata is mostly manually created and often lacks detailed annotation, consistency and, most importantly, explicit semantic content descriptors which would facilitate online browsing and exploration of available information. This paper proposes the enrichment of existing cultural heritage metadata with automatically generated semantic content descriptors. In particular, it is concerned with metadata encoding archival descriptions (EAD) and proposes to use automatic term recognition and term clustering techniques for knowledge acquisition and content-based document classiﬁcation purposes. 
Most existing HLT pipelines assume the input is pure text or, at most, HTML and either ignore (logical) document structure or remove it. We argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved. 
The ARC project (for Architecture Represented Computationally) is an attempt to reproduce in computer form the architectural historian’s mental model of the Gothic cathedral. This model includes the background information necessary to understand a natural language architectural description. Our ﬁrst task is to formalize the description of Gothic cathedrals in a logical language, and provide a means for translating into this language from natural language. Such a system could then be used by architectural historians and others to facilitate the task of gathering and using information from architectural descriptions. We believe the ARC Project will represent an important contribution to the preservation of cultural heritage, because it will offer a logical framework for understanding the description of landmark monuments of the past. This paper presents an outline of our plan for the ARC system, and examines some of the issues we face in implementing it. 
We present an end-to-end pipeline including a user interface for the production of wordlevel annotations for an opinion-mining task in the information technology (IT) domain. Our pre-annotation pipeline selects candidate sentences for annotation using results from a small amount of trained annotation to bias the random selection over a large corpus. Our user interface reduces the need for the user to understand the “meaning” of opinion in our domain context, which is related to community reaction. It acts as a preliminary buffer against low-quality annotators. Finally, our post-annotation pipeline aggregates responses and applies a more aggressive quality ﬁlter. We present positive results using two different evaluation philosophies and discuss how our design decisions enabled the collection of high-quality annotations under subjective and ﬁne-grained conditions. 
The Voynich Manuscript is an undeciphered document from medieval Europe. We present current knowledge about the manuscript’s text through a series of questions about its linguistic properties. 
Even though historical texts reveal a lot of interesting information on culture and social structure in the past, information access is limited and in most cases the only way to ﬁnd the information you are looking for is to manually go through large volumes of text, searching for interesting text segments. In this paper we will explore the idea of facilitating this timeconsuming manual effort, using existing natural language processing techniques. Attention is focused on automatically identifying verbs in early modern Swedish texts (1550–1800). The results indicate that it is possible to identify linguistic categories such as verbs in texts from this period with a high level of precision and recall, using morphological tools developed for present-day Swedish, if the text is normalised into a more modern spelling before the morphological tools are applied. 
In this paper, we explore the task of automatic text processing applied to collections of historical newspapers, with the aim of assisting historical research. In particular, in this ﬁrst stage of our project, we experiment with the use of topical models as a means to identify potential issues of interest for historians. 
Today we have access to unprecedented amounts of literary texts. However, search still relies heavily on key words. In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in both individual books and across very large collections. We introduce the concept of emotion word density, and using the Brothers Grimm fairy tales as example, we show how collections of text can be organized for better search. Using the Google Books Corpus we show how to determine an entity’s emotion associations from cooccurring words. Finally, we compare emotion words in fairy tales and novels, to show that fairy tales have a much wider range of emotion word densities than novels. 
While the study of the connection between discourse patterns and personal identiﬁcation is decades old, the study of these patterns using language technologies is relatively recent. In that more recent tradition we frame author age prediction from text as a regression problem. We explore the same task using three very different genres of data simultaneously: blogs, telephone conversations, and online forum posts. We employ a technique from domain adaptation that allows us to train a joint model involving all three corpora together as well as separately and analyze differences in predictive features across joint and corpusspeciﬁc aspects of the model. Effective features include both stylistic ones (such as POS patterns) as well as content oriented ones. Using a linear regression model based on shallow text features, we obtain correlations up to 0.74 and mean absolute errors between 4.1 and 6.8 years. 
Academic collaboration has often been at the forefront of scientiﬁc progress, whether amongst prominent established researchers, or between students and advisors. We suggest a theory of the different types of academic collaboration, and use topic models to computationally identify these in Computational Linguistics literature. A set of author-speciﬁc topics are learnt over the ACL corpus, which ranges from 1965 to 2009. The models are trained on a per year basis, whereby only papers published up until a given year are used to learn that year’s author topics. To determine the collaborative properties of papers, we use, as a metric, a function of the cosine similarity score between a paper’s term vector and each author’s topic signature in the year preceding the paper’s publication. We apply this metric to examine questions on the nature of collaborations in Computational Linguistics research, ﬁnding that signiﬁcant variations exist in the way people collaborate within different subﬁelds. 
In this paper, we present a system that automatically generates questions from natural language text using discourse connectives. We explore the usefulness of the discourse connectives for Question Generation (QG) that looks at the problem beyond sentence level. Our work divides the QG task into content selection and question formation. Content selection consists of ﬁnding the relevant part in text to frame question from while question formation involves sense disambiguation of the discourse connectives, identiﬁcation of question type and applying syntactic transformations on the content. The system is evaluated manually for syntactic and semantic correctness. 
Identifying peer-review helpfulness is an important task for improving the quality of feedback received by students, as well as for helping students write better reviews. As we tailor standard product review analysis techniques to our peer-review domain, we notice that peerreview helpfulness differs not only between students and experts but also between types of experts. In this paper, we investigate how different types of perceived helpfulness might inﬂuence the utility of features for automatic prediction. Our feature selection results show that certain low-level linguistic features are more useful for predicting student perceived helpfulness, while high-level cognitive constructs are more effective in modeling experts’ perceived helpfulness. 
This paper presents Genpex, a system for automatic generation of narrative probability exercises. Generation of exercises in Genpex is done in two steps. First, the system creates a speciﬁcation of a solvable probability problem, based on input from the user (a researcher or test developer) who selects a speciﬁc question type and a narrative context for the problem. Then, a text expressing the probability problem is generated. The user can tune the generated text by setting the values of some linguistic variation parameters. By varying the mathematical content of the exercise, its narrative context and the linguistic parameter settings, many different exercises can be produced. Here we focus on the natural language generation part of Genpex. After describing how the system works, we brieﬂy present our ﬁrst evaluation results, and discuss some aspects requiring further investigation. 
Automated testing of spoken language is the subject of much current research. Elicited Imitation (EI), or sentence repetition, is well suited for automated scoring, but does not directly test a broad range of speech communication skills. An Oral Proﬁciency Interview (OPI) tests a broad range of skills, but is not as well suited for automated scoring. Some have suggested that EI can be used as a predictor of more general speech communication abilities. We examine EI for this purpose. A fully automated EI test is used to predict OPI scores. Experiments show strong correlation between predicted and actual OPI scores. Effectiveness of OPI score prediction depends upon at least two important design decisions. One of these decisions is to base prediction primarily on acoustic measures, rather than on transcription. The other of these decisions is the choice of sentences, or EI test items, to be repeated. It is shown that both of these design decisions can greatly impact performance. It is also shown that the effectiveness of individual test items can be predicted. 
Structural events, (i.e., the structure of clauses and disﬂuencies) in spontaneous speech, are important components of human speaking and have been used to measure language development. However, they have not been actively used in automated speech assessment research. Given the recent substantial progress on automated structural event detection on spontaneous speech, we investigated the detection of clause boundaries and interruption points of edit disﬂuencies on transcriptions of non-native speech data and extracted features from the detected events for speech assessment. Compared to features computed on human-annotated events, the features computed on machine-generated events show promising correlations to holistic scores that reﬂect speaking proﬁciency levels. 
For adult readers, an automated system can produce oral reading fluency (ORF) scores (e.g., words read correctly per minute) that are consistent with scores provided by human evaluators (Balogh et al., 2005, and in press). Balogh’s work on NAAL materials used passage-specific data to optimize statistical language models and scoring performance. The current study investigates whether or not an automated system can produce scores for young children’s reading that are consistent with human scores. A novel aspect of the present study is that text-independent rule-based language models were employed (Cheng and Townshend, 2009) to score reading passages that the system had never seen before. Oral reading performances were collected over cell phones from 1st, 2nd, and 3rd grade children (n = 95) in a classroom environment. Readings were scored 1) in situ by teachers in the classroom, 2) later by expert scorers, and 3) by an automated system. Statistical analyses provide evidence that machine Words Correct scores correlate well with scores provided by teachers and expert scorers, with all (Pearson’s correlation coefficient) r’s > 0.98 at the individual response level, and all r’s > 0.99 at the “test” level (i.e., median scores out of 3). 
In this paper, we present an automatic question generation system that can generate gap-ﬁll questions for content in a document. Gap-ﬁll questions are ﬁll-in-the-blank questions with multiple choices (one correct answer and three distractors) provided. The system ﬁnds the informative sentences from the document and generates gap-ﬁll questions from them by ﬁrst blanking keys from the sentences and then determining the distractors for these keys. Syntactic and lexical features are used in this process without relying on any external resource apart from the information in the document. We evaluated our system on two chapters of a standard biology textbook and presented the results. 
We present an empirical study of one-onone human tutoring dialogues in the domain of Computer Science data structures. We are interested in discovering effective tutoring strategies, that we frame as discovering which Dialogue Act (DA) sequences correlate with learning. We employ multiple linear regression, to discover the strongest models that explain why students learn during one-on-one tutoring. Importantly, we deﬁne “ﬂexible” DA sequence, in which extraneous DAs can easily be discounted. Our experiments reveal several cognitively plausible DA sequences which signiﬁcantly correlate with learning outcomes. 
Research has shown that a number of factors, such as maturational constraints, previous language background, and attention, can have an effect on L2 acquisition. One related issue that remains to be explored is what factors make an individual word more easily learned. In this study we propose that word complexity, on both the phonetic and semantic levels, affect L2 vocabulary learning. Two studies showed that words with simple grapheme-to-phoneme ratios were easier to learn than more phonetically complex words, and that words with two or fewer word senses were easier to learn that those with three or more. 
We further work on detecting errors in postpositional particle usage by learners of Korean by improving the training data and developing a complete pipeline of particle selection. We improve the data by ﬁltering non-Korean data and sampling instances to better match the particle distribution. Our evaluation shows that, while the data selection is effective, there is much work to be done with preprocessing and system optimization. 
Language sample analysis is an important technique used in measuring language development. At present, measures of grammatical complexity such as the Index of Productive Syntax (Scarborough, 1990) are used to measure language development in early childhood. Although these measures depict the overall competence in the usage of language, they do not provide for an analysis of the grammatical mistakes made by the child. In this paper, we explore the use of existing Natural Language Processing (NLP) techniques to provide an insight into the processing of child language transcripts and challenges in automatic grammar checking. We explore the automatic detection of 6 types of verb related grammatical errors. We compare rule based systems to statistical systems and investigate the use of different features. We found the statistical systems performed better than the rule based systems for most of the error categories. 
We introduce a method for learning to describe the attendant contexts of a given query for language learning. In our approach, we display phraseological information in the form of a summary of general patterns as well as lexical bundles anchored at the query. The method involves syntactical analyses and inverted file construction. At run-time, grammatical constructions and their lexical instantiations characterizing the usage of the given query are generated and displayed, aimed at improving learners’ deep vocabulary knowledge. We present a prototype system, GRASP, that applies the proposed method for enhanced collocation learning. Preliminary experiments show that language learners benefit more from GRASP than conventional dictionary lookup. In addition, the information produced by GRASP is potentially useful information for automatic or manual editing process. 
Learning a vocabulary word requires seeing it in multiple informative contexts. We describe a system to generate such contexts for a given word sense. Rather than attempt to do word sense disambiguation on example contexts already generated or selected from a corpus, we compile information about the word sense into the context generation process. To evaluate the sense-appropriateness of the generated contexts compared to WordNet examples, three human judges chose which word sense(s) fit each example, blind to its source and intended sense. On average, one judge rated the generated examples as sense-appropriate, compared to two judges for the WordNet examples. Although the system’s precision was only half of WordNet’s, its recall was actually higher than WordNet’s, thanks to covering many senses for which WordNet lacks examples. 
In this paper we present a methodology for creating concept map exercises for students. Concept mapping is a common pedagogical exercise in which students generate a graphical model of some domain. Our method automatically extracts knowledge representations from a textbook and uses them to generate concept maps. The purpose of the study is to generate and evaluate these concept maps according to their accuracy, completeness, and pedagogy. 
This paper investigates two strategies for collecting readability assessments, an Expert Readers application intended to collect ﬁne-grained readability assessments from language experts and a Sort by Readability application designed to be intuitive and open for everyone having internet access. We show that the data sets resulting from both annotation strategies are very similar. We conclude that crowdsourcing is a viable alternative to the opinions of language experts for readability prediction. 
To overcome their substantial barriers to ﬂuent reading, students with dyslexia need to be enticed to read more, and to read texts with carefully controlled lexical content. We describe and show examples from a prototype of the new R2aft story assembly engine, which generates an interactive text that has A) variable plot and B) lexical content which is individualized by decoding pattern. 
We apply a previously reported measure of dialog cohesion to a corpus of spoken tutoring dialogs in which motivation was measured. We ﬁnd that cohesion signiﬁcantly predicts changes in student motivation, as measured with a modiﬁed MSLQ instrument. This suggests that non-intrusive dialog measures can be used to measure motivation during tutoring. 
Semantic distance is the degree of closeness between two pieces of text determined by their meaning. Semantic distance is typically measured by analyzing a set of documents or a list of terms and assigning a metric based on the likeness of their meaning or the concept they represent. Although related research provides some semantic-based algorithms, few applications exist. This work proposes a semanticbased approach for automatically identifying potential course equivalencies given their catalog descriptions. The method developed by Li et al. (2006) is extended in this paper to take a course description from one university as the input and suggest equivalent courses offered at another university. Results are evaluated and future work is discussed.  of institutions, it is not always up to date and the data set is sparse and non-uniformed. This work proposes an approach to automatically identify course equivalencies by analyzing the course descriptions and comparing their semantic distance. The course descriptions are ﬁrst pruned and unrelated contexts are removed. Given a course from another university, the algorithm measures word, sentence, and paragraph similarities to suggest a list of potentially equivalent courses offered by UML. This work has two goals: (1) to efﬁciently and accurately suggest equivalent courses to reduce the workload of transfer coordinators, and (2) to explore new applications using semantic distance to move toward the Semantic Web, i.e., to turn existing resources into knowledge structures.  
We present a method that ﬁlters out nonscorable (NS) responses, such as responses with a technical difﬁculty, in an automated speaking proﬁciency assessment system. The assessment system described in this study ﬁrst ﬁlters out the non-scorable responses and then predicts a proﬁciency score using a scoring model for the remaining responses. The data were collected from non-native speakers in two different countries, using two different item types in the proﬁciency assessment: items that elicit spontaneous speech and items that elicit recited speech. Since the proportion of NS responses and the features available to the model differ according to the item type, an item type speciﬁc model was trained for each item type. The accuracy of the models ranged between 75% and 79% in spontaneous speech items and between 95% and 97% in recited speech items. Two different groups of features, signal processing based features and automatic speech recognition (ASR) based features, were implemented. The ASR based models achieved higher accuracy than the non-ASR based models. 
This paper presents a method for identifying non-English speech, with the aim of supporting an automated speech proﬁciency scoring system for non-native speakers. The method uses a popular technique from the language identiﬁcation domain, a single phone recognizer followed by multiple languagedependent language models. This method determines the language of a speech sample based on the phonotactic differences among languages. The method is intended for use with nonnative English speakers. Therefore, the method must be able to distinguish nonEnglish responses from non-native speakers’ English responses. This makes the task more challenging, as the frequent pronunciation errors of non-native speakers may weaken the phonetic and phonotactic distinction between English responses and non-English responses. In order to address this issue, the speaking rate measure was used to complement the language identiﬁcation based features in the model. The accuracy of the method was 98%, and there was 45% relative error reduction over a system based on the conventional language identiﬁcation technique. The model using both feature sets furthermore demonstrated an improvement in accuracy for speakers at all English proﬁciency levels. 
We present a novel noisy channel model for correcting text produced by English as a second language (ESL) authors. We model the English word choices made by ESL authors as a random walk across an undirected bipartite dictionary graph composed of edges between English words and associated words in an author’s native language. We present two such models, using cascades of weighted ﬁnitestate transducers (wFSTs) to model language model priors, random walk-induced noise, and observed sentences, and expectation maximization (EM) to learn model parameters after Park and Levy (2011). We show that such models can make intelligent word substitutions to improve grammaticality in an unsupervised setting. 
We address the problem of detecting English language learner errors by using a discriminative high-order sequence model. Unlike most work in error-detection, this method is agnostic as to specific error types, thus potentially allowing for higher recall across different error types. The approach integrates features from many sources into the error-detection model, ranging from language model-based features to linguistic analysis features. Evaluation results on a large annotated corpus of learner writing indicate the feasibility of our approach on a realistic, noisy and inherently skewed set of data. High-order models consistently outperform low-order models in our experiments. Error analysis on the output shows that the calculation of precision on the test set represents a lower bound on the real system performance. 1. Introduction Systems for automatic detection and correction of errors in native writing have been developed for many decades. Early in the development of these systems, the approach was exclusively based on knowledge engineering. Hand-crafted grammars would analyze a sentence and would contain special mechanisms for rule or constraint relaxation that allow ungrammatical sentences to produce a parse, while at the same time indicating that a grammatical error is present. More recently, datadriven methods have assumed prominence and there has been an emerging area of research into the challenge of detecting and correcting errors in learner language (for an overview see Leacock et al. 2010). Data-driven methods offer the familiar  set of advantages: they can be more flexible than a manually maintained set of rules and they tend to cope better with noisy input. Drawbacks include the inability to handle linguistically more complex errors that involve long distance dependencies such as subject-verb agreement. Learner errors as a target for error detection and correction pose a particular challenge but also offer some unique opportunities. The challenge lies in the density of errors (much higher than in native writing), the variety of errors (a superset of typical native errors) and the generally more non-idiomatic writing. On the other hand, the availability of annotated corpora, often comprised of manually corrected learner essays or scripts, provides a big advantage for the evaluation and training of data-driven systems. Data-driven systems for English learner error detection and correction typically target a specific set of error types and contain a machine learned component for each error type. For example, such a system may have a classifier that determines the correct choice of preposition given the lexical and syntactic part-of-speech (POS) context and hence can aid the learner with the notoriously difficult problem of identifying an appropriate preposition. Similarly, a classifier can be used to predict the correct choice of article in a given context. Such targeted systems have the advantage that they often achieve relatively high precision at, of course, the cost of recall. However, while there are a few major learner error categories, such as prepositions and articles, there is also a long tail of content word and other errors that is not amenable to a targeted approach. In this paper, we depart from the error-specific paradigm and explore a sequence modeling approach to general error detection in learner writing. This approach is completely agnostic as to the error type. It attempts to predict the location of an  180  Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 180–189, Portland, Oregon, 24 June 2011. c 2011 Association for Computational Linguistics  error in a sentence based on observations gathered from a supervised training phase on an errorannotated learner corpus. Features used here are based on an n-gram language model, POS tags, simple string features that indicate token length and capitalization, and linguistic analysis by a constituency parser. We train and evaluate the method on a sizeable subset of the corpus. We show the contribution of the different feature types and perform a manual error analysis to pinpoint shortcomings of the system and to get a more accurate idea of the system’s precision. 2. Related work Error-specific approaches comprise the majority of recent work in learner error detection. Two of the most studied error types in learner English are preposition and article errors since they make up a large percentage of errors in learner writing (16% and 13% respectively in the Cambridge Learner Corpus, without considering spelling and punctuation errors). The most widely used approach for detecting and correcting these errors is classification, with lexical and POS features gleaned from a window around the potential preposition/article site in a sentence. Some recent work includes Chodorow et al. (2007), De Felice and Pulman (2008), Gamon (2010), Han et al. (2010), Izumi et al. (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). Gamon et al. (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a metaclassifier. These error-specific methods achieve high precision (up to 80-90% on some corpora) but only capture highly constrained error types such as preposition and determiner errors. There has also been research on error-detection methods that are not designed to identify a specific error type. The basic idea behind these erroragnostic approaches is to identify an error where there is a particularly unlikely sequence compared to the patterns found in a large well-formed corpus. Atwell (1986) used low-likelihood sequences of POS tags as indicators for the presence of an error. Sjöbergh (2005) used a chunker to detect unlikely chunks in native Swedish writing compared to the chunks derived from a large corpus of well-formed Swedish writing. Bigert and Knutsson (2002) employed a statistical method to identify a variety of  errors in Swedish writing as rare sequences of morpho-syntactic tags. They significantly reduced false positives by using additional methods to determine whether the unexpected sequence is due to phrase or sentence boundaries or due to rare single tags. Chodorow and Leacock (2000) utilized mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus. Comparing these statistics to the ones found in a novel sentence, they could identify unlikely contexts for the targeted words that were often good indicators of the presence of an error. Sun et al. (2007) mined for patterns that consist of POS tags and function words. The patterns are of variable length and can also contain gaps. Patterns were then combined in a classifier to distinguish correct from erroneous sentences. Wagner et al. (2007) combined parse probabilities from a set of statistical parsers and POS tag n-gram probabilities in a classifier to detect ungrammatical sentences. Okanohara and Tsujii (2007) differed from the previous approaches in that they directly used discriminative language models to distinguish correct from incorrect sentences, without the direct modeling of errorindicating patterns. Park and Levy (2011) use a noisy channel model with a base language model and a set of error-specific noise models for error detection and correction. In contrast to previous work, we cast the task as a sequence modeling problem. This provides a flexible framework in which multiple statistical and linguistic signals can be combined and calibrated by supervised learning. The approach is error-agnostic and can easily be extended with additional statistical or linguistic features. 3. Error detection by sequence modeling Errors consist of a sub-sequence of tokens in a longer token sequence. They can be identified by a combination of internal and contextual features, the latter requiring a notion of Markov window (a window around a token in which relevant information is likely to be found). This is similar to tasks such as named entity recognition (NER) or part-of-speech tagging, where sequence modeling has proven to be successful. We choose a Maximum Entropy Markov Model (MEMM, McCallum et al. 2000) as the modeling technique. In NER, the annotation convention uses  181  three labels for a token “O” (outside of NE), “B” (beginning of NE), and “I” (inside of NE). For our purpose we reduced the set of labels to just “O” and “I” since most of the errors are relatively short. Conditional Random Fields (Lafferty et al. 2001) are considered to be superior to MEMMs in learning problems affected by label bias (Bottou 1991). In our scheme, however, there are only two states “O” and “I”, and both states can transition to each other. Since there are no states with asymmetric transition properties that would introduce a bias towards states with fewer transitions, label bias is not a problem for us. Figure 1 shows the structure of our MEMM with a Markov order of five (the diagram only shows the complete set of arcs for the last state). The input sentence contains the token sequence the past year I was stayed … with the error was stayed. Instead of using the tokens themselves as observations, we chose to use POS tags assigned by an automatic tagger (Toutanova et al. 2003). This choice was motivated by data sparseness. Learning a model that observes individual lexical items and predicts a sequence of error/non-error tags would be ideal, but given the many different error types and triggering contexts for an error, such a model would require much more training data. A large set of features that serve as constraints on the state transition models are extracted for each state. These features are described in Section 5. Note that the model structure would lend itself to a factorial conditional random field (McCallum et al. 2003) which allows the joint labeling of POS tags and state labels. This would, however, require training data that is labeled for both errors and POS tags. Figure 1: MEMM model for error detection, the full set of dependencies is only shown for the last state.  4. Detecting errors in the Cambridge Learner Corpus  The learner corpus used to train and evaluate the  system is the Cambridge Learner Corpus (CLC). It  consists of essays (scripts) written as part of the  University of Cambridge English for Speakers of  Other Languages (ESOL) examinations. The cor-  pus contains about 30 million words of learner  English. All errors are annotated and include, when  possible, a single suggested correction. Errors are  categorized into 87 error types.  We performed a number of preprocessing steps  on the data. On the assumption that learners have  access to a spell checker, errors that were marked  as spelling errors were corrected based on the an-  notations. Confused words (their/there) were treat-  ed in the same way, given that they are corrected  by a modern proofing tool such as the one in Mi-  crosoft Word. In addition, British English spelling  conventions were changed to those of American  English. Sentences containing errors that had no  suggested rewrite were eliminated. Finally, only  lexical errors are covered in this work. For punctu-  ation and capitalization we removed the error an-  notations, retaining the original (erroneous)  punctuation and capitalization.  We grouped the remaining 60 error classifica-  tions into eight categories: Content word, Inflec-  tional morphology, Noun phrase errors,  Preposition errors, Multiple errors, Other errors  involving content words, Other errors involving  function words and Derivational morphology. The  distribution of error categories is shown in Table 1.  Error Class  Freq Pct  Content word insertion, deletion or choice  185,201 21%  Inflectional morphology and agreement of content words  157,660 18%  Noun phrase formation: Determiners and quantifiers  130,829 15%  Preposition error  124,902 14%  Multiple: Adjacent and nested annotations  113,615 13%  Other content word errors  79,596 9%  Other function word errors: anaphors and conjunctions  65,034 7%  Derivational morphology of content words  39,213 4%  Table 1: Error types in the CLC.  182  The multiple error class includes any combination of error types where the error annotations are either nested or adjacent. The other categories are more focused: the errors are of a particular class and their adjacent context is correct, although there may be another error annotation a single token away. Content word errors involve the insertion, deletion and substitution of nouns, verbs, adjectives and adverbs. Further analysis of this error category on a random sample of 200 instances reveals that the majority (72%) of content word errors involve substitutions, while deletions account for 10% of the errors and insertions for 18%. Most substitutions (63%) involve the wrong choice of a word that is somewhat semantically related to the correct choice. Inflectional morphology includes all inflection errors for content words as well as subject-verb agreement errors. The inflectional errors include many cases of what might be considered spelling errors, for example *dieing/dying. Similarly, the derivational morphology errors include all derivational errors for content words – and also include many errors that may be considered as spelling errors. Noun formation errors include all annotations involving determiners and quantifiers: inflection, derivation, countability, word form and noun-phrase-internal agreement. Preposition errors include all annotations that involve prepositions: insertion, deletion, substitution and a non-preposition being used in place of a preposition. There are two other categories: those involving the remaining function words (anaphors and conjunctions) and those involving remaining content words (collocation, idiom, negative formation, argument structure, word order, etc.). It is important to highlight the challenges inherent in this data set. First of all, the problem is highly skewed since only 7.3% of tokens in the test set are involved in an error. Second, since we included correct learner sentences in the development and test sets in the proportion they occur in the overall corpus, only 47% of sentences in the test set contain error annotations, greatly increasing the likelihood of false positives. 5. Features 5.1 Language model features The language model (LM) features comprise a total of 29 features. Each of these features is calculated from n-gram probabilities observed at and  around the current token. All LM features are based on scores from a 7-gram language model with absolute discount smoothing built from the Gigaword corpus (Gao et al. 2001, Nguyen et al. 2007). We group the language model features conceptually into five categories: basic features, ratio features, drop features, entropy delta features and miscellaneous. All probabilities are log probabilities, and n in the n-grams ranges from 1 to 5. All features are calculated for each token w of the tokens w0…wi in a sentence. Basic LM features consist of two features: the unigram probability of w and the average n-gram probability of all n-grams in the sentence that contain w. Ratio features are based on the intuition that errors can be characterized as involving tokens that have a very low ratio of higher order n-gram probabilities to lower order n-gram probabilities. In other words, these are tokens that are part of an unlikely combination of otherwise likely smaller ngrams. These features are calculated as the ratio of the average x-gram probability of all x-grams containing w to the average y-gram probability of all y-grams containing w. The values for x and y are: 5 and 1, 4 and 1, 3 and 1, 2 and 1, 5 and 4, 4 and 3, 3 and 2. Drop features measure either the drop or increase in n-gram probability across token w. For example, the bigram drop at wi is the delta between the bigram probability of the bigram starting at i-1 to the bigram probability of the bigram starting at i. Drop features are calculated for n-grams with 2 ≤ n ≤ 5. Entropy delta features offer another way to look at the changes of n-gram probability across a token w. Forward entropy for wi is defined as the entropy of the string wi…wn where n is the index of the last token in the sentence. We calculate the entropy of an n-gram as the language model probability of string wi…wn divided by the number of tokens in that string. Backward entropy is calculated analogously for w0…wi. For n-grams with 1 ≤ n ≤ 5, we also calculate, at each index i into the token array, the delta between the n-gram entropy of the n-gram starting at i and the n-gram starting at i+1 (forward sliding entropy). Similarly the delta between the ngram entropy of the n-gram starting at i and the ngram starting at i-1 (backward sliding entropy) is calculated.  183  There are four miscellaneous language model features. Three of them, minimum ratio to random, average ratio to random, and overall ratio to random address the fact that a “good” n-gram is likely to have a much higher probability than an n-gram with the same tokens in random order. For all ngrams where 2 ≤ n ≤ 5 we calculate the ratio between the n-gram probability and the sum of the unigram probabilities. For a token wi we produce the minimum ratio to random (the minimum ratio of all n-grams including w) and the average ratio to random (the average of all ratios of the n-grams including w). Overall ratio to random is obtained by looping through each n-gram where 2 ≤ n ≤ 5 that includes wi and summing the n-gram probabilities (sum1) as well as the unigram probabilities of all unigrams in these n-grams (sum2). The ratio feature is then sum1/sum2. The final feature addresses the intuition that an erroneous word may cause n-grams that contain the word to be less likely than adjacent but non-overlapping n-grams. Overlap to adjacent ratio is the sum of probabilities of n-grams including wi, divided by the sum of probabilities of n-grams that are adjacent to wi but do not include it. Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al. (2008) and Gamon (2010). 5.2 String features String features capture information about the characters in a token and the tokens in a sentence. Two binary features indicate whether a token is capitalized (initial capitalization or all capitalized), one feature indicates the token length in characters and one feature measures the number of tokens in the sentence. 5.3 Linguistic Analysis features Each sentence is linguistically analyzed by a PCFG-LA parser (Petrov et al., 2006) trained on the Penn Treebank (Marcus et al., 1993). A number of features are extracted from the constituency tree to assess the syntactic complexity of the whole sentence, the syntactic complexity of the local environment of a token, and simple constituency information for each token. These features are: label  of the parent and grandparent node, number of sibling nodes, number of siblings of the parent, presence of a governing head node, label of the governing head node, and length of path to the root. An additional feature indicates whether the POS tag assigned by the parser does not match the tag assigned by the POS tagger, which may indicate a tagging error. 6. Experiments 6.1 Design For our experiments we use three different mutually exclusive random subsets of CLC. 50K sentences are used for training of the models (larger data sets exceeded the capabilities of our MEMM trainer). In this set, we only include sentences that contain at least one annotated error. We also experimented using a mix of error-free and erroneous sentences, but the resulting models turned out to be extremely skewed towards always predicting the majority state “O” (no error). 20K sentences (including both erroneous and correct sentences) are used for parameter tuning and testing, respectively. Each token in the data is annotated with one of the states “O” or “I”. Performance is measured on a per token basis, i.e. each mismatch between the predicted state and the annotated state is counted as an error, each match is counted as a correct prediction. We use the development set to tune two parameters: the size of the Markov window and a prior to prevent overfitting. The latter is a Gaussian prior (or quadratic regularizer) where the mean is fixed to zero and the variance is left as a free parameter. We perform a grid search to find values for the parameters that optimize the model’s F1 score on the development data. In order to be able to report precision and recall curves, we use a technique similar to the one described in Minkov et al. (2010): we introduce an artificial feature with a constant value at training time. At test time we perform multiple runs, modifying the weight on the artificial feature. This weight variation influences the model’s prior propensity to assign each of the two states, allowing us to measure a precision/recall tradeoff.  184  6.2 Performance of feature sets Figure 2 illustrates the performance of three different feature sets and combinations. The baseline is using only language model features and standard POS tags, which tops out at about 20% precision. Adding the string features discussed in the previous section, and partially lexicalized (PL) POS tags, where we used POS tags for content word tokens and the lexicalized token for function words, we get a small but consistent improvement. We obtain the best performance when all features are used, including the linguistic analysis features (DepParse). We found that a high-order model with a Markov window size of 14 performed best for all experiments with a top F1 score. F1 at lower orders was significantly worse. Training time for the best models was less than one hour. 6.3 Predicting error types In our next experiment, we tried to determine how the sequence modeling approach performs for individual error types. Here we trained eight different models, one for each of the error types in Table 1. As in the previous experiments, the development and test files contained error-free sentences. The optimal Markov window size ranged from 8 to 15. Note that our general sequence model described in the previous sections does not recognize different error types, so it was necessary to train one model per error type for the experiments in this section. Figure 3 shows the results from this series of experiments. We omit the results for other content word error, other function word and multiple errors in this graph since these relatively ill-defined error classes performed rather poorly. As Figure 3 illustrates, derivational errors and preposition errors achieve by far the best results. The fact that the individual precision never reaches the level of the general sequence model (Figure 2) can be attributed to the much smaller overall set of errors in each of the eight training sets. In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set. That system consists of a preposition-specific classifier, a language model and a meta-classifier that combines evidence from the classifier and the language model. The sequence model approach outperforms the classifier of that system, but the full system including lan-  guage model and meta-classifier achieves much higher precision than the sequence modeling approach. 6.4 Learning curve experiments An obvious question that arises is how much training data we need for an error detection sequence model, i.e. how does performance degrade as we decrease the amount of training data from the 50K error-annotated sentences that were used in the previous experiments. To this end we produced random subsets of the training data in 20% increments. For each of these training sets, we determined the resulting F1 score by first performing parameter tuning on the development set and then measuring precision and recall of the best model on the test set. Results are shown in Figure 5: at 20% of training data, precision starts to increase at the cost of recall. At 80% of the training data, recall starts to trend up as well. This upward trend of both precision and recall indicates that increasing the amount of training data is likely to further improve results. 6.5 Error analysis The precision values obtained in our experiments are low, but they are also based on the strictest possible measure of accuracy: an error prediction is only counted as correct if it exactly matches a location and annotation in the CLC. A manual analysis of 400 randomly selected sentences containing “false positives”, where the system had 29% precision and 10% recall, by the strictest calculation, showed that 14% of the “false positives” identified an error that was either not annotated in CLC or was an error type not covered by the system such as punctuation or case (recall from Section 4 that for these errors we removed the error annotations but retained the original string). An additional 16% were adjacent to an error annotation. 12% had error annotations within 2-4 tokens from the predicted error. Foreign language and other unknown proper names comprised an additional 6%. Finally, 9% were due to tokenization problems or all-upper case input that throws off the POS tagger. Thus the precision reported in Figure 2 through Figure 6 is really a lower bound. 30% of the “false positives” either identify, or are adjacent to, an error.  185  Sentence length has a strong influence on the accuracy of the sequence model. For sentences less than 7 tokens long, average precision is approximately 7%, whereas longer sentences average at 29% precision. This observation fits with the fact that high-order models perform best in the task, i.e. the more context a model can access, the more reliable its predictions are. Shorter sentences are also less likely to contain an error: only 12% of short sentences contain an error, as opposed to 46% of sentences of seven tokens or longer. For sentences that are at least 7 tokens long, error predictions on the first and last two tokens (the last token typically being punctuation) have an average precision of 22% as compared to an average of 30% at all other positions. Other unreliable error  predictions include those involving non-alphabetic characters (quotes, parentheses, symbols, numbers) with 1% precision and proper name tags with 10% precision. Many of the predictions on NNP tags identify, by and large, unknown or foreign names (Cricklewood, Cajamarca). Ignoring system flags on short sentences, symbols and NNP tags would improve precision with little cost to recall. We also experimented with a precision/recall metric that is less harsh but at the same time realistic for error detection. For this “soft metric” we count correct and incorrect predictions at the error level instead of the token level. An error is defined as a consecutive sequence of n error tags, where n ≥ 1.  Precision and recall 0.5  0.4  precision  0.3  0.2  0.1  0  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0  recall  LM  LM+ String + PL  LM + String + PL + DepParse  Figure 2: Precision and recall of different feature sets.  0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 0  Precision and Recall per Error Type  0.1 content  0.2 deriv  0.3 recall 0.4 inflect  0.5 nounphrase  Figure 3: Precision and recall of different error models.  0.6  0.7  preposition  186  precision  precision  0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0  Precision and Recall Prepositions  0.1 sequence model  0.2  0.3  0.4  recall full system Gamon (2010)  0.5  0.6  0.7  classifier only Gamon (2010)  Figure 4: Preposition precision and recall.  Precision, recall and amount of training data 0.35  0.3  0.25  0.2  0.15  0.1  0.05  0  0  10  20  30  40  50  60  70  80  90  100  percent of training data  Precision  Recall  Figure 5: Learning curve.  Precision and recall: soft metric and per sentence accuracy 
In this paper, we present a ﬁrst attempt to characterize the semantic deviance of composite expressions in distributional semantics. Speciﬁcally, we look for properties of adjective-noun combinations within a vectorbased semantic space that might cue their lack of meaning. We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). For each model, we generate composite vectors for a set of AN combinations unattested in the source corpus and which have been deemed either acceptable or semantically deviant. We then compute measures that might cue semantic anomaly, and compare each model’s results for the two classes of ANs. Our study shows that simple, unsupervised cues can indeed signiﬁcantly tell unattested but acceptable ANs apart from impossible, or deviant, ANs, and that the simple additive and multiplicative models are the most effective in this task. 
Stemming from distributed representation theories, we investigate the interaction between distributed structure and distributional meaning. We propose a pure distributed tree (DT) and distributional distributed tree (DDT). DTs and DDTs are exploited for deﬁning distributed tree kernels (DTKs) and distributional distributed tree kernels (DDTKs). We compare DTKs and DDTKs in two tasks: approximating tree kernels TK (Collins and Duffy, 2002); performing textual entailment recognition (RTE). Results show that DTKs correlate with TKs and perform in RTE better than DDTKs. Then, including distributional vectors in distributed structures is a very difﬁcult task. 
Since its introduction into the NLP community, pointwise mutual information has proven to be a useful association measure in numerous natural language processing applications such as collocation extraction and word space models. In its original form, it is restricted to the analysis of two-way co-occurrences. NLP problems, however, need not be restricted to twoway co-occurrences; often, a particular problem can be more naturally tackled when formulated as a multi-way problem. In this paper, we explore two multivariate generalizations of pointwise mutual information, and explore their usefulness and nature in the extraction of subject verb object triples. 
This paper reports on the participation of the NLP GROUP at UNED in the DiSCo’2011 compositionality evaluation task. The aim of the task is to predict compositionality judgements assigned by human raters to candidate phrases, in English and German, from three common grammatical relations: adjectivenoun, subject-verb and subject-object. Our participation is restricted to adjectivenoun relations in English. We explore the use of syntactic-based contexts obtained from large corpora to build classiﬁers that model the compositionality of the semantics of such pairs. 
Machine translation began in 1947 with an influential memo by Warren Weaver. In that memo, Weaver noted that human code-breakers could transform ciphers into natural language (e.g., into Turkish)  without access to parallel ciphertext/plaintext data, and  without knowing the plaintext language’s syntax and semantics. Simple word- and letter-statistics seemed to be enough for the task. Weaver then predicted that such statistical methods could also solve a tougher problem, namely language translation. This raises the question: can sufficient translation knowledge be derived from comparable (non-parallel) data? In this talk, I will discuss initial work in treating foreign language as a code for English, where we assume the code to involve both word substitutions and word transpositions. In doing so, I will quantitatively estimate the value of non-parallel data, versus parallel data, in terms of end-to-end accuracy of trained translation systems. Because we still know very little about solving word-based codes, I will also describe successful techniques and lessons from the realm of letter-based ciphers, where the nonparallel resources are (1) enciphered text, and (2) unrelated plaintext. As an example, I will describe how we decoded the Copiale cipher with limited “computer-like” knowledge of the plaintext language. The talk will wrap up with challenges in exploiting comparable data at all levels: letters, words, phrases, syntax, and semantics. 
Using comparable corpora to ﬁnd new word translations is a promising approach for extending bilingual dictionaries (semi-) automatically. The basic idea is based on the assumption that similar words have similar contexts across languages. The context of a word is often summarized by using the bag-of-words in the sentence, or by using the words which are in a certain dependency position, e.g. the predecessors and successors. These different context positions are then combined into one context vector and compared across languages. However, previous research makes the (implicit) assumption that these different context positions should be weighted as equally important. Furthermore, only the same context positions are compared with each other, for example the successor position in Spanish is compared with the successor position in English. However, this is not necessarily always appropriate for languages like Japanese and English. To overcome these limitations, we suggest to perform a linear transformation of the context vectors, which is deﬁned by a matrix. We deﬁne the optimal transformation matrix by using a Bayesian probabilistic model, and show that it is feasible to ﬁnd an approximate solution using Markov chain Monte Carlo methods. Our experiments demonstrate that our proposed method constantly improves translation accuracy. 
This paper presents a series of experiments aimed at inducing and evaluating domainspecific bilingual lexica from comparable corpora. First, a small English-Slovene comparable corpus from health magazines was manually constructed and then used to compile a large comparable corpus on health-related topics from web corpora. Next, a bilingual lexicon for the domain was extracted from the corpus by comparing context vectors in the two languages. Evaluation of the results shows that a 2-way translation of context vectors significantly improves precision of the extracted translation equivalents. We also show that it is sufficient to increase the corpus for one language in order to obtain a higher recall, and that the increase of the number of new words is linear in the size of the corpus. Finally, we demonstrate that by lowering the frequency threshold for context vectors, the drop in precision is much slower than the increase of recall. 
In this article, we present a simple and effective approach for extracting bilingual lexicon from comparable corpora enhanced with parallel corpora. We make use of structural characteristics of the documents comprising the comparable corpus to extract parallel sentences with a high degree of quality. We then use state-of-the-art techniques to build a specialized bilingual lexicon from these sentences and evaluate the contribution of this lexicon when added to the comparable corpus-based alignment technique. Finally, the value of this approach is demonstrated by the improvement of translation accuracy for medical words. 
In this article we present a novel way of looking at the problem of automatic acquisition of pairs of translationally equivalent words from comparable corpora. We ﬁrst present the standard and extended approaches traditionally dedicated to this task. We then reinterpret the extended method, and motivate a novel model to reformulate this approach inspired by the metasearch engines in information retrieval. The empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach. 
In this paper, we present two methods to use a noisy parallel news corpus to improve statistical machine translation (SMT) systems. Taking full advantage of the characteristics of our corpus and of existing resources, we use a bootstrapping strategy, whereby an existing SMT engine is used both to detect parallel sentences in comparable data and to provide an adaptation corpus for translation models. MT experiments demonstrate the beneﬁts of various combinations of these strategies. 
We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different articles about the same topics or events. The procedure consists of document pair extraction, sentence pair extraction, and fragment pair extraction. At each stage, we evaluate the intermediate results manually, and tune the later stages accordingly. With this minimally supervised approach, we achieve 62% of accuracy on the paraphrase fragment pairs we collected and 67% extracted from the MSR corpus. The results look promising, given the minimal supervision of the approach, which can be further scaled up. 
Mining parallel data from comparable corpora is a promising approach for overcoming the data sparseness in statistical machine translation and other NLP applications. Even if two comparable documents have few or no parallel sentence pairs, there is still potential for parallelism in the sub-sentential level. The ability to detect these phrases creates a valuable resource, especially for low-resource languages. In this paper we explore three phrase alignment approaches to detect parallel phrase pairs embedded in comparable sentences: the standard phrase extraction algorithm, which relies on the Viterbi path; a phrase extraction approach that does not rely on the Viterbi path, but uses only lexical features; and a binary classiﬁer that detects parallel phrase pairs when presented with a large collection of phrase pair candidates. We evaluate the effectiveness of these approaches in detecting alignments for phrase pairs that have a known alignment in comparable sentence pairs. The results show that the Non-Viterbi alignment approach outperforms the other two approaches on F1 measure. 
Supervised learning algorithms for identifying comparable sentence pairs from a dominantly non-parallel corpora require resources for computing feature functions as well as training the classiﬁer. In this paper we propose active learning techniques for addressing the problem of building comparable data for low-resource languages. In particular we propose strategies to elicit two kinds of annotations from comparable sentence pairs: class label assignment and parallel segment extraction. We also propose an active learning strategy for these two annotations that performs signiﬁcantly better than when sampling for either of the annotations independently. 
In this paper, we question the homogeneity of a large parallel corpus by measuring the similarity between various sub-parts. We compare results obtained using a general measure of lexical similarity based on χ2 and by counting the number of discourse connectives. We argue that discourse connectives provide a more sensitive measure, revealing differences that are not visible with the general measure. We also provide evidence for the existence of specific characteristics defining translated texts as opposed to nontranslated ones, due to a universal tendency for explicitation. 
While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. We show that this system outperforms a fair baseline (Enright and Kondrak, 2007) in a number of controlled tasks. We applied it on the FrenchEnglish cross-language linked article pairs of Wikipedia in order see whether parallel articles in this resource are available, and if our system is able to locate them. According to some manual evaluation we conducted, a fourth of the article pairs in Wikipedia are indeed in translation relation, and PARADOCS identiﬁes parallel or noisy parallel article pairs with a precision of 80%. 
As the title suggests, our paper deals with web discussion fora, whose content can be considered to be a special type of comparable corpora. We discuss the potential of this vast amount of data available now on the World Wide Web nearly for every language, regarding both general and common topics as well as the most obscure and speciﬁc ones. To illustrate our ideas, we propose a case study of seven wedding discussion fora in ﬁve languages. 
In this paper we investigate automatic datatext alignment, i.e. the task of automatically aligning data records with textual descriptions, such that data tokens are aligned with the word strings that describe them. Our methods make use of log likelihood ratios to estimate the strength of association between data tokens and text tokens. We investigate datatext alignment at the document level and at the sentence level, reporting results for several methodological variants as well as baselines. We ﬁnd that log likelihood ratios provide a strong basis for predicting data-text alignment. 
This paper introduces a new task of crosslingual slot ﬁlling which aims to discover attributes for entity queries from crosslingual comparable corpora and then present answers in a desired language. It is a very challenging task which suﬀers from both information extraction and machine translation errors. In this paper we analyze the types of errors produced by ﬁve diﬀerent baseline approaches, and present a novel supervised rescoring based validation approach to incorporate global evidence from very large bilingual comparable corpora. Without using any additional labeled data this new approach obtained 38.5% relative improvement in Precision and 86.7% relative improvement in Recall over several state-of-the-art approaches. The ultimate system outperformed monolingual slot ﬁlling pipelines built on much larger monolingual corpora. 
We describe the design of a comparable corpus that spans all of the world’s languages and facilitates large-scale cross-linguistic processing. This Universal Corpus consists of text collections aligned at the document and sentence level, multilingual wordlists, and a small set of morphological, lexical, and syntactic annotations. The design encompasses submission, storage, and access. Submission preserves the integrity of the work, allows asynchronous updates, and facilitates scholarly citation. Storage employs a cloud-hosted ﬁlestore containing normalized source data together with a database of texts and annotations. Access is permitted to the ﬁlestore, the database, and an application programming interface. All aspects of the Universal Corpus are open, and we invite community participation in its design and implementation, and in supplying and using its data. 
The paper presents an Expectation Maximization (EM) algorithm for automatic generation of parallel and quasi-parallel data from any degree of comparable corpora ranging from parallel to weakly comparable. Specifically, we address the problem of extracting related textual units (documents, paragraphs or sentences) relying on the hypothesis that, in a given corpus, certain pairs of translation equivalents are better indicators of a correct textual unit correspondence than other pairs of translation equivalents. We evaluate our method on mixed types of bilingual comparable corpora in six language pairs, obtaining state of the art accuracy figures. 
We describe a set of techniques that have been developed while collecting parallel texts for Russian-English language pair and building a corpus of parallel sentences for training a statistical machine translation system. We discuss issues of verifying potential parallel texts and filtering out automatically translated documents. Finally we evaluate the quality of the 1-millionsentence corpus which we believe may be a useful resource for machine translation research. 
Cross lingual information access (CLIA) systems are required to access the large amounts of multilingual content generated on the world wide web in the form of blogs, news articles and documents. In this paper, we discuss our approach to query formation for CLIA systems where language resources are replaced by Wikipedia. We claim that Wikipedia, with its rich multilingual content and structure, forms an ideal platform to build a CLIA system. Our approach is particularly useful for under-resourced languages, as all the languages don’t have the resources(tools) with sufﬁcient accuracies. We propose a context aware language-independent query formation method which, with the help of bilingual dictionaries, forms queries in the target language. Results are encouraging with a precision of 69.75% and thus endorse our claim on using Wikipedia for building CLIA systems.  are in need. Language independent approach is particularly useful for languages that fall into the category of under-resourced (African, few Asian languages), that doesn’t have sufﬁcient resources. In our approach towards language-independent CLIA system, we have developed context aware query translation using Wikipedia. Due to voluntary contribution of millions of users, Wikipedia gathers very signiﬁcant amount of updated knowledge and provides a structured way to access it.  
This paper proposes a novel application of a supervised topic model to do entity relation detection (ERD). We adapt Maximum Entropy Discriminant Latent Dirichlet Allocation (MEDLDA) with mixed membership for relation detection. The ERD task is reformulated to ﬁt into the topic modeling framework. Our approach combines the beneﬁts of both, maximum-likelihood estimation (MLE) and max-margin estimation (MME), and the mixed membership formulation enables the system to incorporate heterogeneous features. We incorporate different features into the system and perform experiments on the ACE 2005 corpus. Our approach achieves better overall performance for precision, recall and Fmeasure metrics as compared to SVM-based and LLDA-based models. 
We propose the use of a nonparametric Bayesian model, the Hierarchical Dirichlet Process (HDP), for the task of Word Sense Induction. Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. We ﬁnd that the two models achieve similar levels of induction quality, while the HDP confers the advantage of automatically inducing a variable number of senses per word, as compared to manually ﬁxing the number of senses a priori, as in LDA. This ﬂexibility allows for the model to adapt to terms with greater or lesser polysemy, when evidenced by corpus distributional statistics. When trained on out-of-domain data, experimental results conﬁrm the model’s ability to make use of a restricted set of topically coherent induced senses, when then applied in a restricted domain. 
Edges of graphs that model real data can be seen as judgements whether pairs of objects are in relation with each other or not. So, one can evaluate the similarity of two graphs with a measure of agreement between judges classifying pairs of vertices into two categories (connected or not connected). When applied to synonymy networks, such measures demonstrate a surprisingly low agreement between various resources of the same language. This seems to suggest that the judgements on synonymy of lexemes of the same lexicon radically differ from one dictionary editor to another. In fact, even a strong disagreement between edges does not necessarily mean that graphs model a completely different reality: although their edges seem to disagree, synonymy resources may, at a coarser grain level, outline similar semantics. To investigate this hypothesis, we relied on shared common properties of real world data networks to look at the graphs at a more global level by using random walks. They enabled us to reveal a much better agreement between dense zones than between edges of synonymy graphs. These results suggest that although synonymy resources may disagree at the level of judgements on single pairs of words, they may nevertheless convey an essentially similar semantic information. 
 2 The Document Network  A graph-based distance between Wikipedia articles is deﬁned using a random walk model, which estimates visiting probability (VP) between articles using two types of links: hyperlinks and lexical similarity relations. The VP to and from a set of articles is then computed, and approximations are proposed to make tractable the computation of semantic relatedness between every two texts in a large data set. The model is applied to document clustering on the 20 Newsgroups data set. Precision and recall are improved in comparison with previous textual distance algorithms. 
In this paper, we present GrawlTCQ, a new bootstrapping algorithm for building specialized terminology, corpora and queries, based on a graph model. We model links between documents, terms and queries, and use a random walk with restart algorithm to compute relevance propagation. We have evaluated GrawlTCQ on an AFP English corpus of 57,441 news over 10 categories. For corpora building, GrawlTCQ outperforms the BootCaT tool, which is vastly used in the domain. For 1,000 documents retrieved, we improve mean precision by 25%. GrawlTCQ has also shown to be faster and more robust than BootCaT over iterations. 
A key problem in document classiﬁcation and clustering is learning the similarity between documents. Traditional approaches include estimating similarity between feature vectors of documents where the vectors are computed using TF-IDF in the bag-of-words model. However, these approaches do not work well when either similar documents do not use the same vocabulary or the feature vectors are not estimated correctly. In this paper, we represent documents and keywords using multiple layers of connected graphs. We pose the problem of simultaneously learning similarity between documents and keyword weights as an edge-weight regularization problem over the different layers of graphs. Unlike most feature weight learning algorithms, we propose an unsupervised algorithm in the proposed framework to simultaneously optimize similarity and the keyword weights. We extrinsically evaluate the performance of the proposed similarity measure on two different tasks, clustering and classiﬁcation. The proposed similarity measure outperforms the similarity measure proposed by (Muthukrishnan et al., 2010), a state-of-theart classiﬁcation algorithm (Zhou and Burges, 2007) and three different baselines on a variety of standard, large data sets. 
We present the first work on applying statistical techniques to unrestricted Quantifier Scope Disambiguation (QSD), where there is no restriction on the type or the number of quantifiers in the sentence. We formulate unrestricted QSD as learning to build a Directed Acyclic Graph (DAG) and define evaluation metrics based on the properties of DAGs. Previous work on statistical scope disambiguation is very limited, only considering sentences with two explicitly quantified noun phrases (NPs). In addition, they only handle a restricted list of quantifiers. In our system, all NPs, explicitly quantified or not (e.g. definites, bare singulars/plurals, etc.), are considered for possible scope interactions. We present early results on applying a simple model to a small corpus. The preliminary results are encouraging, and we hope will motivate further research in this area. 
Usually unsupervised dependency parsing tries to optimize the probability of a corpus by modifying the dependency model that was presumably used to generate the corpus. In this article we explore a different view in which a dependency structure is among other things a partial order on the nodes in terms of centrality or saliency. Under this assumption we model the partial order directly and derive dependency trees from this order. The result is an approach to unsupervised dependency parsing that is very different from standard ones in that it requires no training data. Each sentence induces a model from which the parse is read off. Our approach is evaluated on data from 12 different languages. Two scenarios are considered: a scenario in which information about part-of-speech is available, and a scenario in which parsing relies only on word forms and distributional clusters. Our approach is competitive to state-of-the-art in both scenarios. 
We present a model for the inclusion of semantic role annotations in the framework of conﬁdence estimation for machine translation. The model has several interesting properties, most notably: 1) it only requires a linguistic processor on the (generally well-formed) source side of the translation; 2) it does not directly rely on properties of the translation model (hence, it can be applied beyond phrase-based systems). These features make it potentially appealing for system ranking, translation re-ranking and user feedback evaluation. Preliminary experiments in pairwise hypothesis ranking on ﬁve conﬁdence estimation benchmarks show that the model has the potential to capture salient aspects of translation quality. 
We argue that failing to capture the degree of contribution of each semantic frame in a sentence explains puzzling results in recent work on the MEANT family of semantic MT evaluation metrics, which have disturbingly indicated that dissociating semantic roles and fillers from their predicates actually improves correlation with human adequacy judgments even though, intuitively, properly segregating event frames should more accurately reflect the preservation of meaning. Our analysis finds that both properly structured and flattened representations fail to adequately account for the contribution of each semantic frame to the overall sentence. We then show that the correlation of HMEANT, the human variant of MEANT, can be greatly improved by introducing a simple length-based weighting scheme that approximates the degree of contribution of each semantic frame to the overall sentence. The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame’s contribution gives HMEANT higher correlations than the previously bestperforming flattened model, as well as HTER. 
To facilitate the application of semantics in statistical machine translation, we propose a broad-coverage predicate-argument structure mapping technique using automated resources. Our approach utilizes automatic syntactic and semantic parsers to generate Chinese-English predicate-argument structures. The system produced a many-to-many argument mapping for all PropBank argument types by computing argument similarity based on automatic word alignment, achieving 80.5% F-score on numbered argument mapping and 64.6% F-score on all arguments. By measuring predicate-argument structure similarity based on the argument mapping, and formulating the predicate-argument structure mapping problem as a linear-assignment problem, the system achieved 84.9% F-score using automatic SRL, only 3.7% F-score lower than using gold standard SRL. The mapping output covered 49.6% of the annotated Chinese predicates (which contains predicateadjectives that often have no parallel annotations in English) and 80.7% of annotated English predicates, suggesting its potential as a valuable resource for improving word alignment and reranking MT output. 
To increase the model coverage, sourcelanguage paraphrases have been utilized to boost SMT system performance. Previous work showed that word lattices constructed from paraphrases are able to reduce out-ofvocabulary words and to express inputs in different ways for better translation quality. However, such a word-lattice-based method suffers from two problems: 1) path duplications in word lattices decrease the capacities for potential paraphrases; 2) lattice decoding in SMT dramatically increases the search space and results in poor time efﬁciency. Therefore, in this paper, we adopt word confusion networks as the input structure to carry source-language paraphrase information. Similar to previous work, we use word lattices to build word confusion networks for merging of duplicated paths and faster decoding. Experiments are carried out on small-, medium- and large-scale English– Chinese translation tasks, and we show that compared with the word-lattice-based method, the decoding time on three tasks is reduced signiﬁcantly (up to 79%) while comparable translation quality is obtained on the largescale task.  ously, if the following two conditions could be satisﬁed, namely: • the words in the parallel corpus are highly aligned so that the phrase alignment can be performed well; • the coverage of the input sentence by the parallel corpus is high; then the “exact phrase match” translation method could bring a good translation. However, for some language pairs, it is not easy to obtain a huge amount of parallel data, so it is not that easy to satisfy these two conditions. To alleviate this problem, paraphrase-enriched SMT systems have been proposed to show the effectiveness of incorporating paraphrase information. In terms of the position at which paraphrases are incorporated in the MT-pipeline, previous work can be organized into three different categories: • Translation model augmentation with paraphrases (Callison-Burch et al., 2006; Marton et al., 2009). Here the focus is on the translation of unknown source words or phrases in the input sentences by enriching the translation table with paraphrases.  
 Translation requires non-isomorphic transformation from the source to the target. However, non-isomorphism can be reduced by learning multi-word units (MWUs). We present a novel way of representating sentence structure based on MWUs, which are not necessarily continuous word sequences. Our proposed method builds a simpler structure of MWUs than words using words as vertices of a dependency structure. Unlike previous studies, we collect many alternative structures in a packed forest. As an application of our proposed method, we extract translation rules in form of a source MWU-forest to the target string, and verify the rule coverage empirically. As a consequence, we improve the rule coverage compare to a previous work, while retaining the linear asymptotic complexity. 
Mistranslation of an ambiguous word can have a large impact on the understandability of a given sentence. In this article, we describe a thorough evaluation of the translation quality of ambiguous nouns in three different setups. We compared two statistical Machine Translation systems and one dedicated Word Sense Disambiguation (WSD) system. Our WSD system incorporates multilingual information and is independent from external lexical resources. Word senses are derived automatically from word alignments on a parallel corpus. We show that the two WSD classiﬁers that were built for these experiments (English– French and English–Dutch) outperform the SMT system that was trained on the same corpus. This opens perspectives for the integration of our multilingual WSD module in a statistical Machine Translation framework, in order to improve the automated translation of ambiguous words, and by consequence make the translation output more understandable. 
In this paper we propose several novel approaches to improve phrase reordering for statistical machine translation in the framework of maximum-entropy-based modeling. A smoothed prior probability is introduced to take into account the distortion effect in the priors. In addition to that we propose multiple novel distortion features based on syntactic parsing. A new metric is also introduced to measure the effect of distortion in the translation hypotheses. We show that both smoothed priors and syntax-based features help to signiﬁcantly improve the reordering and hence the translation performance on a large-scale Chinese-to-English machine translation task.  
We show that reifying the rules from hyperedge weights to ﬁrst-class graph nodes automatically gives us rule expectations in any kind of grammar expressible as a deductive system, without any explicit algorithm for calculating rule expectations (such as the insideoutside algorithm). This gives us expectation maximization training for any grammar class with a parsing algorithm that can be stated as a deductive system, for free. Having such a framework in place accelerates turnover time for experimenting with new grammar classes and parsing algorithms—to implement a grammar learner, only the parse forest construction has to be implemented.  For symbols participating in a parse, we could state it like this: The contextual probability of an item is the sum of the probabilities of all contexts where it was used. . . . which is exactly what we mean with outside probability. In semiring (bi-) parsing, this quantity is called reverse value, but in this framework it is also deﬁned for rules, which means that we could restate our boxed statement as: The contextual probability of a rule is the sum of the probabilities of all contexts where it was used.  
We present a translation model based on dependency trees. The model adopts a treeto-string approach and extends PhraseBased translation (PBT) by using the dependency tree of the source sentence for selecting translation options and for reordering them. Decoding is done by translating each node in the tree and combining its translations with those of its head in alternative orders with respect to its siblings. Reordering of the siblings exploits a heuristic based on the syntactic information from the parse tree which is learned from the corpus. The decoder uses the same phrase tables produced by a PBT system for looking up translations of single words or of partial sub-trees. A mathematical model is presented and experimental results are discussed. 
We use hand-coded rules and graph-aligned logical dependencies to reorder English text towards Chinese word order. We obtain a 1.5% higher F-score for Giza++ compared to running with unprocessed text. We describe this research and its implications for SMT. 
We consider SCFG-based MT systems that get syntactic category labels from parsing both the source and target sides of parallel training data. The resulting joint nonterminals often lead to needlessly large label sets that are not optimized for an MT scenario. This paper presents a method of iteratively coarsening a label set for a particular language pair and training corpus. We apply this label collapsing on Chinese–English and French–English grammars, obtaining test-set improvements of up to 2.8 BLEU, 5.2 TER, and 0.9 METEOR on Chinese–English translation. An analysis of label collapsing’s effect on the grammar and the decoding process is also given. 
In this paper we present a novel approach of utilizing Semantic Role Labeling (SRL) information to improve Hierarchical Phrasebased Machine Translation. We propose an algorithm to extract SRL-aware Synchronous Context-Free Grammar (SCFG) rules. Conventional Hiero-style SCFG rules will also be extracted in the same framework. Special conversion rules are applied to ensure that when SRL-aware SCFG rules are used in derivation, the decoder only generates hypotheses with complete semantic structures. We perform machine translation experiments using 9 different Chinese-English test-sets. Our approach achieved an average BLEU score improvement of 0.49 as well as 1.21 point reduction in TER. 
 cimiano@cit-ec.uni-bielefeld.de ing Standards Board, 2007), bio-medicine (Col-  Ontologies and taxonomies are widely used to organize concepts providing the basis for activities such as indexing, and as background knowledge for NLP tasks. As such, translation of these resources would prove useful to adapt these systems to new languages.  lier et al., 2008) (Ashburner et al., 2000) and libraries (Mischo, 1982). These resources normally attach labels in natural language to the concepts and relations that deﬁne their structure, and these labels can be used for a number of purposes, such as providing user interface localization (McCrae et  However, we show that the nature of these resources is signiﬁcantly different from the “free-text” paradigm used to train most statistical machine translation systems. In particular, we see signiﬁcant differences in the linguistic nature of these resources and such resources have rich additional semantics. We demonstrate that as a result of these linguistic differences, standard SMT methods, in partic-  al., 2010), multilingual data access (Declerck et al., 2010), information extraction (Mu¨ller et al., 2004) and natural language generation (Bontcheva, 2005). It seems natural that for applications that use such ontologies and taxonomies, translation of the natural language descriptions associated with them is required in order to adapt these methods to new languages. Currently, there has been some work on  ular evaluation metrics, can produce poor performance. We then look to the task of leveraging these semantics for translation, which we approach in three ways: by adapting the translation system to the domain of the resource; by examining if semantics can help to predict the syntactic structure used in translation; and by evaluating if we can use existing translated  this in the context of ontology localisation, such as Espinoza et al. (2008) and (2009), Cimiano et al. (2010), Fu et al. (2010) and Navigli and Penzetto (2010). However, this work has focused on the case in which exact or partial translations are found in other similar resources such as bilingual lexica. Instead, in this paper we look at how we may gain an  taxonomies to disambiguate translations. We present some early results from these experiments, which shed light on the degree of success we may have with each approach.  adequate translation using statistical machine translation approaches that also utilise the semantic information beyond the label or term describing the concept, that is relations among the concepts in the  
A semantic feature for statistical machine translation, based on Latent Semantic Indexing, is proposed and evaluated. The objective of the proposed feature is to account for the degree of similarity between a given input sentence and each individual sentence in the training dataset. This similarity is computed in a reduced vectorspace constructed by means of the Latent Semantic Indexing decomposition. The computed similarity values are used as an additional feature in the log-linear model combination approach to statistical machine translation. In our implementation, the proposed feature is dynamically adjusted for each translation unit in the translation table according to the current input sentence to be translated. This model aims at favoring those translation units that were extracted from training sentences that are semantically related to the current input sentence being translated. Experimental results on a Spanish-to-English translation task on the Bible corpus demonstrate a significant improvement on translation quality with respect to a baseline system. 
We present a rule extractor for SCFG-based MT that generalizes many of the contraints present in existing SCFG extraction algorithms. Our method’s increased rule coverage comes from allowing multiple alignments, virtual nodes, and multiple tree decompositions in the extraction process. At decoding time, we improve automatic metric scores by significantly increasing the number of phrase pairs that match a given test set, while our experiments with hierarchical grammar ﬁltering indicate that more intelligent ﬁltering schemes will also provide a key to future gains. 
Shallow semantic analyzers, such as semantic role labeling and sense tagging, are increasing in accuracy and becoming commonplace. However, they only provide limited and local representations of local words and individual predicate-argument structures. This talk will address some of the current challenges in producing deeper, connected representations of eventualities. Available resources, such as VerbNet, FrameNet and TimeBank, that can assist in this process will also be discussed, as well as some of their limitations. Speaker’s Bio Martha Palmer is a Full Professor at the University of Colorado with joint appointments in Linguistics and Computer Science and is an Institute of Cognitive Science Faculty Fellow. She recently won a Boulder Faculty Assembly 2010 Research Award. Beginning with her dissertation work at Edinburgh and her ﬁrst job as a Research Scientist at Unisys, her research has been focused on trying to capture the meanings of words in representations that the computer can use to build up meanings of complex sentences and documents. These representations can in turn be used to improve the computer’s ability to perform question answering, information retrieval, and machine translation. Current approaches rely on techniques for applying supervised machine learning algorithms, which use vast amounts of annotated training data. Therefore, she and her students, both at Colorado and previously at the University of Pennsylvania, are engaged in providing data with word sense tags and semantic role labels for English, Chinese, Arabic, and Hindi, funded by DARPA and NSF. They also use machine learning algorithms to develop automatic sense taggers and semantic role labelers, and to extract bilingual lexicons from parallel corpora. A more recent focus is the application of these methods to biomedical journal articles and clinical notes, funded by NIH. She is a co-editor for both the Journal of Natural Language Engineering and LiLT, Linguistic Issues in Language Technology. She is a past President of the Association for Computational Linguistics, past Chair of SIGLEX and SIGHAN, and is currently the Director of the 2011 Linguistics Institute to be held in Boulder, Colorado. 
We introduce several ideas that improve the performance of supervised information extraction systems with a pipeline architecture, when they are customized for new domains. We show that: (a) a combination of a sequence tagger with a rule-based approach for entity mention extraction yields better performance for both entity and relation mention extraction; (b) improving the identiﬁcation of syntactic heads of entity mentions helps relation extraction; and (c) a deterministic inference engine captures some of the joint domain structure, even when introduced as a postprocessing step to a pipeline system. All in all, our contributions yield a 20% relative increase in F1 score in a domain signiﬁcantly different from the domains used during the development of our information extraction system. 
Many recent studies have been dedicated to the extraction of semantic connections between words. Using such information at semantic level is likely to improve the performance of Natural Language Processing (NLP) systems, such as text categorization, question answering, information extraction, etc. The scarcity of such resources in Turkish, obstructs new improvements. There are many examples of semantic networks for English and other widely-used languages to lead the way for studies in Turkish. In this study, developing a semantic network for Turkish is aimed by using structural and string patterns in a dictionary. The results are promising, so that approximately two relations can be extracted from 3 definitions. The overall accuracy is 86% if we consider the correct sense assignment, 94% without considering word sense disambiguation. 
In this paper, we have identified event and sentiment expressions at word level from the sentences of TempEval-2010 corpus and evaluated their association in terms of lexical equivalence and co-reference. A hybrid approach that consists of Conditional Random Field (CRF) based machine learning framework in conjunction with several rule based strategies has been adopted for event identification within the TimeML framework. The strategies are based on semantic role labeling, WordNet relations and some handcrafted rules. The sentiment expressions are identified simply based on the cues that are available in the sentiment lexicons such as Subjectivity Wordlist, SentiWordNet and WordNet Affect. The identification of lexical equivalence between event and sentiment expressions based on the part-of-speech (POS) categories is straightforward. The emotional verbs from VerbNet have also been employed to improve the coverage of lexical equivalence. On the other hand, the association of sentiment and event has been analyzed using the notion of co-reference. The parsed dependency relations along with basic rhetoric knowledge help to identify the co-reference between event and sentiment expressions. Manual evaluation on the 171 sentences of TempEval-2010 dataset yields the precision, recall and F-Score values of 61.25%, 70.29% and 65.23% respectively. 
 This paper introduces Vignette Semantics, a lexical semantic theory based on Frame Semantics that represents conceptual and graphical relations. We also describe a lexical resource that implements this theory, VigNet, and its application in text-to-scene generation. 
This paper suggests two ways of improving semantic role labeling (SRL). First, we introduce a novel transition-based SRL algorithm that gives a quite different approach to SRL. Our algorithm is inspired by shift-reduce parsing and brings the advantages of the transitionbased approach to SRL. Second, we present a self-learning clustering technique that effectively improves labeling accuracy in the test domain. For better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the ﬁnal labeling decisions. All approaches are evaluated on the CoNLL’09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks. 
Many prior studies have investigated the recovery of semantic arguments for nominal predicates. The models in many of these studies have assumed that arguments are independent of each other. This assumption simpliﬁes the computational modeling of semantic arguments, but it ignores the joint nature of natural language. This paper presents a preliminary investigation into the joint modeling of implicit arguments for nominal predicates. The joint model uses propositional knowledge extracted from millions of Internet webpages to help guide prediction. 
We take the first steps towards augmenting a lexical resource, VerbNet, with probabilistic information about coercive constructions. We focus on CAUSEDMOTION as an example construction occurring with verbs for which it is a typical usage or for which it must be interpreted as extending the event semantics through coercion, which occurs productively and adds substantially to the relational semantics of a verb. However, through annotation we find that VerbNet fails to accurately capture all usages of the construction. We use unsupervised methods to estimate probabilistic measures from corpus data for predicting usage of the construction across verb classes in the lexicon and evaluate against VerbNet. We discuss how these methods will form the basis for enhancements for VerbNet supporting more accurate analysis of the relational semantics of a verb across productive usages. 
Topic modelling is a popular approach to joint clustering of documents and terms, e.g. via Latent Dirichlet Allocation. The standard document representation in topic modelling is a bag of unigrams, ignoring both macro-level document structure and micro-level constituent structure. In this talk, I will discuss recent work on consolidating the microlevel document representation with multiword expressions, and present experimental results which demonstrate that linguistically-richer document representations enhance topic modelling.  BA(Linguistics/Japanese) at the University of Melbourne in 1995, and an MEng(CS) and PhD(CS) at the Tokyo Institute of Technology in 1998 and 2001, respectively. Prior to commencing his current position at the University of Melbourne, he was a Senior Research Engineer at the Center for the Study of Language and Information, Stanford University (2001-2004).  Biography Tim Baldwin is an Associate Professor and Deputy Head of the Department of Computer Science and Software Engineering, University of Melbourne and a contributed research staff member of the NICTA Victoria Research Laboratories. He has previously held visiting positions at the University of Washington, University of Tokyo, University of Saarland, and NTT Communication Science Laboratories. His research interests cover topics including deep linguistic processing, multiword expressions, deep lexical acquisition, computer-assisted language learning, information extraction and web mining, with a particular interest in the interface between computational and theoretical linguistics. Current projects include web user forum mining, information personalisation in museum contexts, biomedical text mining, online linguistic exploration, and intelligent interfaces for Japanese language learners. He is President of the Australasian Language Technology Association in 2011-2012. Tim completed a BSc(CS/Maths) and  
Taking as a starting-point the development on cooccurrence techniques for several languages, we focus on the aspects that should be considered in a NV extraction task for Basque. In Basque, NV expressions are considered those combinations in which a noun, inﬂected or not, is co-occurring with a verb, as erabakia hartu (‘to make a decision’), kontuan hartu (‘to take into account’) and buruz jakin (‘to know by heart’). A basic extraction system has been developed and evaluated against two references: a) a reference which includes NV entries from several lexicographic works; and b) a manual evaluation by three experts of a random sample from the n-best lists. 
One of the key issues in both natural language understanding and generation is the appropriate processing of Multiword Expressions (MWEs). MWE can be defined as a semantic issue of a phrase where the meaning of the phrase may not be obtained from its constituents in a straightforward manner. This paper presents an approach of identifying bigram noun-noun MWEs from a medium-size Bengali corpus by clustering the semantically related nouns and incorporating a vector space model for similarity measurement. Additional inclusion of the English WordNet::Similarity module also improves the results considerably. The present approach also contributes to locate clusters of the synonymous noun words present in a document. Experimental results draw a satisfactory conclusion after analyzing the Precision, Recall and F-score values. 
In this paper we present preliminary experiments that aim to reduce lexical data sparsity in statistical parsing by exploiting information about named entities. Words in the WSJ corpus are mapped to named entity clusters and a latent variable constituency parser is trained and tested on the transformed corpus. We explore two different methods for mapping words to entities, and look at the effect of mapping various subsets of named entity types. Thus far, results show no improvement in parsing accuracy over the best baseline score; we identify possible problems and outline suggestions for future directions. 
Multi-Word Expressions (MWEs) are prevalent in text and are also, on average, less polysemous than mono-words. This suggests that accurate MWE detection should lead to a nontrivial improvement in Word Sense Disambiguation (WSD). We show that a straightforward MWE detection strategy, due to Arranz et al. (2005), can increase a WSD algorithm’s baseline f-measure by 5 percentage points. Our measurements are consistent with Arranz’s, and our study goes further by using a portion of the Semcor corpus containing 12,449 MWEs - over 30 times more than the approximately 400 used by Arranz. We also show that perfect MWE detection over Semcor only nets a total 6 percentage point increase in WSD f-measure; therefore there is little room for improvement over the results presented here. We provide our MWE detection algorithms, along with a general detection framework, in a free, open-source Java library called jMWE. Multi-word expressions (MWEs) are prevalent in text. This is important for the classic task of Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2007), in which an algorithm attempts to assign to each word in a text the appropriate entry from a sense inventory. A WSD algorithm that cannot correctly detect the MWEs that are listed in its sense inventory will not only miss those sense assignments, it will also spuriously assign senses to MWE constituents that themselves have sense entries, dealing a double-blow to WSD performance. Beyond this penalty, MWEs listed in a sense in-  ventory also present an opportunity to WSD algorithms - they are, on average, less polysemous than mono-words. In Wordnet 1.6, multi-words have an average polysemy of 1.07, versus 1.53 for monowords. As a concrete example, consider sentence She broke the world record. In Wordnet 1.6 the lemma world has nine different senses and record has fourteen, while the MWE world record has only one. If a WSD algorithm correctly detects MWEs, it can dramatically reduce the number of possible senses for such sentences.  Measure Number of MWEs Fraction of MWEs WSD impr. (Best v. Baseline) WSD impr. (Baseline v. None) WSD impr. (Best v. None) WSD impr. (Perfect v. None)  Us 12,449 7.4% 0.016F1 0.033F1 0.050F1 0.061F1  Arranz 382 9.4% 0.012F1 -  Table 1: Improvement of WSD f-measures over an MWE-unaware WSD strategy for various MWE detection strategies. Baseline, Best, and Perfect refer to the MWE detection strategy used in the WSD preprocess.  With this in mind, we expected that accurate MWE detection will lead to a small yet non-trivial improvement in WSD performance, and this is indeed the case. Table 1 summarizes our results. In particular, a relatively straightforward MWE detection strategy, here called the ‘best’ strategy and due to Arranz et al. (2005), yielded a 5 percentage point improvement1 in WSD f-measure. We also measured an improvement similar to that of Arranz when 1For example, if the WSD algorithm has an f-measure of  20  Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 20–24, Portland, Oregon, USA, 23 June 2011. c 2011 Association for Computational Linguistics  moving from a Baseline MWE detection strategy to the Best strategy, namely, 1.6 percentage points to their 1.2. We performed our measurements over the brown1 and brown2 concordances2 of the Semcor corpus (Fellbaum, 1998), which together contain 12,449 MWEs, over 30 times as many as the approximately 400 contained in the portion of the XWN corpus used by Arranz. We also measured the improvement for WSD f-measure for Baseline and Perfect MWE detection strategies. These strategies improved WSD f-measure by 3.3 and 6.1 percentage points, respectively, showing that the relatively straightforward Best MWE detection strategy, at 5.0 percentage points, leaves little room for improvement. 
Multi-word expressions (MWEs) account for a large portion of the language used in dayto-day interactions. A formal system that is ﬂexible enough to model these large and often syntactically-rich non-compositional chunks as single units in naturally occurring text could considerably simplify large-scale semantic annotation projects, in which it would be undesirable to have to develop internal compositional analyses of common technical expressions that have speciﬁc idiosyncratic meanings. This paper will ﬁrst deﬁne a notion of functorargument decomposition on phrase structure trees analogous to graph coloring, in which the tree is cast as a graph, and the elementary structures of a grammar formalism are colors. The paper then presents a formal argument that tree-rewriting systems, a class of grammar formalism that includes Tree Adjoining Grammars, are able to produce a proper superset of the functor-argument decompositions that string-rewriting systems can produce. 
In this paper, we investigate a supervised machine learning framework for automatically learning of English Light Verb Constructions (LVCs). Our system achieves an 86.3% accuracy with a baseline (chance) performance of 52.2% when trained with groups of either contextual or statistical features. In addition, we present an in-depth analysis of these contextual and statistical features and show that the system trained by these two types of cosmetically different features reaches similar performance empirically. However, in the situation where the surface structures of candidate LVCs are identical, the system trained with contextual features which contain information on surrounding words performs 16.7% better. In this study, we also construct a balanced benchmark dataset with 2,162 sentences from BNC for English LVCs. And this data set is publicly available and is also a useful computational resource for research on MWEs in general. 
In this paper, I present a lexical representation of the light verb ha 'do' used in two types of Korean light verb constructions (LVCs). These two types of the constructions have the typical theoretical and implementation problems as multiword expressions (MWEs): lexical proliferation of the possible light verb senses in the lexicon, potential overgeneration of illformed LVCs, and the semantic compositionality issue. Adopting and adapting the idea of qualia structure (Pustejovsky, 1991) into a typed-feature structure grammar (Copestake, 1993; Copestake, 2002; Sag et al., 2003), I suggest that some Korean common nouns have their associated predicate information in their lexical entries (e.g., the predicate meaning cook is included in the lexical entry of the common noun pap 'rice'). Thus such common nouns provide an appropriate predicate meaning to the light verb. The lexical constraints on the light verb and common nouns, and relevant phrase structure rules allow me to capture the generalizations and idiosyncrasies regarding LVCs in a systematic way. 
This paper describes a new part-of-speech tagger including multiword unit (MWU) identiﬁcation. It is based on a Conditional Random Field model integrating language-independent features, as well as features computed from external lexical resources. It was implemented in a ﬁnite-state framework composed of a preliminary ﬁnite-state lexical analysis and a CRF decoding using weighted ﬁnitestate transducer composition. We showed that our tagger reaches state-of-the-art results for French in the standard evaluation conditions (i.e. each multiword unit is already merged in a single token). The evaluation of the tagger integrating MWU recognition clearly shows the interest of incorporating features based on MWU resources. 
 Recent research has focused on non-English languages such as Spanish, Dutch, and German (Meul-  Most work on evaluation of named-entity recognition has been done in the context of competitions, as a part of Information Extraction. There has been little work on any form of extrinsic evaluation, and how one tagger compares with another on the major classes: PERSON, ORGANIZATION, and LOCATION. We report on a comparison of three state-ofthe-art named entity taggers: Stanford, LBJ, and IdentiFinder. The taggers were compared with respect to: 1) Agreement rate on the classiﬁcation of entities by class, and 2) Percentage of ambiguous entities (belonging to more than one class) co-occurring in a document. We found that the agreement between the taggers ranged from 34% to 58%, depending on the class and that more than 40% of the globally ambiguous entities co-occur within the same document. We also propose a unit test based on the problems we encountered.  der et al., 2002; Carreras et al., 2003; Rossler, 2004), and on improving the performance of unsupervised learning methods (Nadeau et al., 2006; Elsner et al., 2009). There are no well-established standards for evaluation of NER. Since criteria for membership in the classes can change from one competition to another, it is often not possible to compare performance directly. Moreover, since some of the systems in the competition may use proprietary software, the results in a competition might not be replicable by others in the community; however, this applies to the state of the art for most NLP applications rather than just NER. Our work is motivated by a vocabulary assessment project in which we needed to identify multi-word expressions and determine their association with other words and phrases. However,  we found that state-of-the-art software for named-  
In this paper I argue in favour of a collocation extraction approach to the acquisition of relational nouns in German. We annotated frequency-based best lists of nounpreposition bigrams and subsequently trained different classiﬁers using (combinations of) association metrics, achieving a maximum Fmeasure of 69.7 on a support vector machine (Platt, 1998). Trading precision for recall, we could achieve over 90% recall for relational noun extraction, while still halving the annotation effort.  a valuable resource for machine translation, separating the more semantic task of translating modifying prepositions from the more syntactic task of translating subcategorised for prepositions. Despite its relevance for accurate deep parsing, the German HPSG grammar developed at DFKI (Mu¨ller and Kasper, 2000; Crysmann, 2003; Crysmann, 2005) currently only includes 107 entries for proposition taking nouns, and lacks entries for PP-taking nouns entirely. In terms of subcategorisation properties, relational nouns in German can be divided up into 3 classes:  
Semantic Role Labeling annotation task depends on the correct identiﬁcation of predicates, before identifying arguments and assigning them role labels. However, most predicates are not constituted only by a verb: they constitute Complex Predicates (CPs) not yet available in a computational lexicon. In order to create a dictionary of CPs, this study employs a corpus-based methodology. Searches are guided by POS tags instead of a limited list of verbs or nouns, in contrast to similar studies. Results include (but are not limited to) light and support verb constructions. These CPs are classiﬁed into idiomatic and less idiomatic. This paper presents an in-depth analysis of this phenomenon, as well as an original resource containing a set of 773 annotated expressions. Both constitute an original and rich contribution for NLP tools in Brazilian Portuguese that perform tasks involving semantics.  identiﬁed by a parser are usually used to automatically identify argument takers, but do no sufﬁce. A lexicon of CPs, as well as the knowledge about verbal chains composition, would complete a fully automatic identiﬁcation of argument takers. Consequently, the possibility of disagreement between SRL annotators would rely only on the assignment of role labels to arguments. This paper reports the investigation of such multi-word units, in order to meet the needs arisen from an SRL annotation task in a corpus of Brazilian Portuguese1. To stress the importance of these CPs for SRL, consider the sentence John takes care of his business in three alternatives of annotation:  
The identiﬁcation and extraction of Multiword Expressions (MWEs) currently deliver satisfactory results. However, the integration of these results into a wider application remains an issue. This is mainly due to the fact that the association measures (AMs) used to detect MWEs require a critical amount of data and that the MWE dictionaries cannot account for all the lexical and syntactic variations inherent in MWEs. In this study, we use an alternative technique to overcome these limitations. It consists in deﬁning an n-gram frequency database that can be used to compute AMs on-theﬂy, allowing the extraction procedure to efﬁciently process all the MWEs in a text, even if they have not been previously observed. 
 2 Semantic transfer  This paper presents a procedure for extracting transfer rules for multiword expressions from parallel corpora for use in a rule based Japanese-English MT system. We show that adding the multi-word rules improves translation quality and sketch ideas for learning more such rules. 
The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit. 
Multi-word expressions (MWEs) play an important role in all tasks that involve natural language processing. MWEs in Hindi are quite varied and many of these are of the types that are not encountered in English. In this paper, we examine different types of MWEs encountered in Hindi. Many of these have not received adequate attention of investigators. For example, ‘vaalaa’ constructs, doublets (word-pairs), replication, and a variety of verb group forms have not been explored as MWEs. We examine these MWEs from machine translation viewpoint. Many of these are frequently used in day-to-day conversations and informal communication but are not that frequently encountered in a formal textual corpus. Most of the conventional statistical methods for MWE identification use corpus with limited linguistic cues. These are found to be inadequate for detecting all types of MWEs that exist in real life. In this paper, we present a stepwise methodology for mining Hindi MWEs using linguistic knowledge. Interpretation and representation for some of these from machine translation perspective have also been explored. 
In this paper, we describe our methods to detect noun compounds and light verb constructions in running texts. For noun compounds, dictionary-based methods and POStagging seem to contribute most to the performance of the system whereas for light verb constructions, the combination of POStagging, syntactic information and restrictions on the nominal and verbal component yield the best result. However, focusing on deverbal nouns proves to be beneﬁcial for both types of MWEs. The effect of syntax is negligible on noun compound detection whereas it is unambiguously helpful for identifying light verb constructions. 
jMWE is a Java library for implementing and testing algorithms that detect Multi-Word Expression (MWE) tokens in text. It provides (1) a detector API, including implementations of several detectors, (2) facilities for constructing indices of MWE types that may be used by the detectors, and (3) a testing framework for measuring the performance of a MWE detector. The software is available for free download. jMWE is a Java library for constructing and testing Multi-Word Expression (MWE) token detectors. The original goal of the library was to detect tokens (instances) of MWE types in a token stream, given a list of types such as those that can be extracted from an electronic dictionary such as WordNet (Fellbaum, 1998). The purpose of the library is not to discover new MWE types, but rather ﬁnd instances of a set of given types in a given text. The library also supports MWE detectors that are not list-based. The functionality of the library is basic, but it is a necessary foundation for any system that wishes to use MWEs in later stages of language processing. It is a natural complement to software for discovering MWE types, such as mwetoolkit (Ramisch et al., 2010) or the NSP package (Banerjee and Pedersen, 2003). jMWE is available online for free download (Finlayson and Kulkarni, 2011a). 
We introduce FipsCoView, an on-line interface for dictionary-like visualisation of collocations detected from parallel corpora using a syntactically-informed extraction method. 
This demo introduces a suite of web-based English lexical knowledge resources, called StringNet and StringNet Navigator (http://nav.stringnet.org), designed to provide access to the immense territory of multiword expressions that falls between what the lexical entries encode in lexicons on the one hand and what productive grammar rules cover on the other. StringNet’s content consists of 1.6 billion hybrid n-grams, strings in which word forms and parts of speech grams can cooccur. Subordinate and super-ordinate relations among hybrid n-grams are indexed, making StringNet a navigable web rather than a list. Applications include error detection and correction tools and web browser-based tools that detect patterns in the webpages that a user browses. 
 2 count.pl  The Ngram Statistics Package (Text::NSP) is freely available open-source software that identiﬁes ngrams, collocations and word associations in text. It is implemented in Perl and takes advantage of regular expressions to provide very ﬂexible tokenization and to allow for the identiﬁcation of non-adjacent ngrams. It includes a wide range of measures of association that can be used to identify collocations. 
We present an experimental environment for computer-assisted extraction of Multiword Expressions (MWEs) from corpora. Candidate extraction works in two steps: generation and ﬁltering. We focus on recent improvements in the former, for which we increased speed and ﬂexibility. We present examples that show the potential gains for users and applications.  The main contribution of our tool, rather than a novel approach to MWE extraction, is an environment that systematically integrates the functionalities found in other tools, that is, sophisticated corpus queries like in CQP (Christ, 1994) and Manatee (Rychly´ and Smrz, 2004), candidate generation like in Text::NSP (Banerjee and Pedersen, 2003), and ﬁltering like in UCS (Evert, 2004). The pattern matching and n-gram counting steps are the focus of the improvements described in this paper.  
What is a multiword expression (MWE) and how many are there? What is a MWE? What is many? Mark Liberman gave a great invited talk at ACL-89 titled “how many words do people know?” where he spent the entire hour questioning the question. Many of these same questions apply to multiword expressions. What is a word? What is many? What is a person? What does it mean to know? Rather than answer these questions, this paper will use these questions as Liberman did, as an excuse for surveying how such issues are addressed in a variety of fields: computer science, web search, linguistics, lexicography, educational testing, psychology, statistics, etc. 
Abstract A growing body of research analyzes the linguistic and discourse properties of communication in online social media. Most of the analysis, especially at the discourse level, is done manually by human researchers. This talk explores how the findings and techniques of computer-mediated discourse analysis (CMDA), a paradigm I have been developing and teaching for 18 years, can inform computational approaches to communication in social media. I start by reviewing established automation approaches, which mainly focus on structural linguistic phenomena, and emergent approaches, such as machine learning models that identify semantically- and pragmatically-richer phenomena, through the lens of CMDA, pointing out the strengths and limitations of each. The basic problem is that patterns in the discourse of social media users can be identified by humans that do not appear to lend themselves to reliable automated identification using existing approaches. To begin to address this problem, I draw on examples of recent work on Twitter, Wikipedia, and web-based discussion forums to suggest an approach that synthesizes linguistically-informed manual analysis and existing automated techniques. I consider how such an approach could scale up, while still making use of human analysts, and I identify a number of real-world problems that automated CMDA could help address. 
The recent proliferation of political and social forums has given rise to a wealth of freely accessible naturalistic arguments. People can “talk” to anyone they want, at any time, in any location, about any topic. Here we use a Mechanical Turk annotated corpus of forum discussions as a gold standard for the recognition of disagreement in online ideological forums. We analyze the utility of meta-post features, contextual features, dependency features and word-based features for signaling the disagreement relation. We show that using contextual and dialogic features we can achieve accuracies up to 68% as compared to a unigram baseline of 63%. 
Political blogs as a form of social media allow for an uniquely interactive form of political discourse. This is especially evident in focused blogs with a strong ideological identity. We investigate techniques to identify topics within the context of the community, which when discussed in a blog post evoke a discernible positive or negative collective opinion from readers who respond to posts in comments. This is done by using computational methods to assign sentiment polarity to blog comments and learning community speciﬁc models that summarize issues tackled by blogs and predict the polarity based on the topics discussed in a blog post. 
Microtexts, like SMS messages, Twitter posts, and Facebook status updates, are a popular medium for real-time communication. In this paper, we investigate the writing conventions that different groups of users use to express themselves in microtexts. Our empirical study investigates properties of lexical transformations as observed within Twitter microtexts. The study reveals that different populations of users exhibit different amounts of shortened English terms and different shortening styles. The results reveal valuable insights into how human language technologies can be effectively applied to microtexts. 
We examine sentiment analysis on Twitter data. The contributions of this paper are: (1) We introduce POS-speciﬁc prior polarity features. (2) We explore the use of a tree kernel to obviate the need for tedious feature engineering. The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline. 
This paper explores the problem of detecting sentence-level forum authority claims in online discussions. Using a maximum entropy model, we explore a variety of strategies for extracting lexical features in a sparse training scenario, comparing knowledge- and datadriven methods (and combinations). The augmentation of lexical features with parse context is also investigated. We ﬁnd that certain markup features perform remarkably well alone, but are outperformed by data-driven selection of lexical features augmented with parse context. 
We present the AAWD corpus, a collection of 365 discussions drawn from Wikipedia talk pages and annotated with labels capturing two kinds of social acts: alignment moves and authority claims. We describe these social acts and our annotation process, and analyze the resulting data set for interactions between participant status and social acts and between the social acts themselves. 
Hashtags are used in Twitter to classify messages, propagate ideas and also to promote specific topics and people. In this paper, we present a linguistic-inspired study of how these tags are created, used and disseminated by the members of information networks. We study the propagation of hashtags in Twitter grounded on models for the analysis of the spread of linguistic innovations in speech communities, that is, in groups of people whose members linguistically influence each other. Differently from traditional linguistic studies, though, we consider the evolution of terms in a live and rapidly evolving stream of content, which can be analyzed in its entirety. In our experimental results, using a large collection crawled from Twitter, we were able to identify some interesting aspects – similar to those found in studies of (offline) speech – that led us to believe that hashtags may effectively serve as models for characterizing the propagation of linguistic forms, including: (1) the existence of a “preferential attachment process”, that makes the few most common terms ever more popular, and (2) the relationship between the length of a tag and its frequency of use. The understanding of formation patterns of successful hashtags in Twitter can be useful to increase the effectiveness of real-time streaming search algorithms.  to the topic of the message. They can be used not only to add context and metadata to the posts, but also for promotion and publicity. By simply adding a hash symbol (#) before a string of letters, numerical digits or underscore signs (_), it is possible to tag a message, helping other users to find tweets that have a common topic. Hashtags allow users to create communities of people interested in the same topic by making it easier for them to find and share information related to it (Kricfalusi, 2009). Figure 1 shows an example of query for the tag “#basketball”, which returns the newest tweets with this hashtag.  
User-contributed content is creating a surge on the Internet. A list of “buzzing topics” can effectively monitor the surge and lead people to their topics of interest. Yet a topic phrase alone, such as “SXSW”, can rarely present the information clearly. In this paper, we propose to explore a variety of text sources for summarizing the Twitter topics, including the tweets, normalized tweets via a dedicated tweet normalization system, web contents linked from the tweets, as well as integration of different text sources. We employ the concept-based optimization framework for topic summarization, and conduct both automatic and human evaluation regarding the summary quality. Performance differences are observed for different input sources and types of topics. We also provide a comprehensive analysis regarding the task challenges. 
In this paper we investigate the connection between language and community membership of long time community participants through computational modeling techniques. We report on ﬁndings from an analysis of language usage within a popular online discussion forum with participation of thousands of users spanning multiple years. We ﬁnd community norms of long time participants that are characterized by forum speciﬁc jargon and a style that is highly informal and shows familiarity with speciﬁc other participants and high emotional involvement in the discussion. We also ﬁnd quantitative evidence of persistent shifts in language usage towards these norms across users over the course of the ﬁrst year of community participation. Our observed patterns suggests language stabilization after 8 or 9 months of participation. 
Email is an important way of communication in our daily life and it has become the subject of various NLP and social studies. In this paper, we focus on email formality and explore the factors that could affect the sender’s choice of formality. As a case study, we use the Enron email corpus to test how formality is affected by social distance, relative power, and the weight of imposition, as deﬁned in Brown and Levinson’s model of politeness (1987). Our experiments show that their model largely holds in the Enron corpus. We believe that the methodology proposed in the paper can be applied to other social media domains and be used to test other linguistic or social theories. 
Models of the acquisition of word segmentation are typically evaluated using phonemically transcribed corpora. Accordingly, they implicitly assume that children know how to undo phonetic variation when they learn to extract words from speech. Moreover, whereas models of language acquisition should perform similarly across languages, evaluation is often limited to English samples. Using child-directed corpora of English, French and Japanese, we evaluate the performance of state-of-the-art statistical models given inputs where phonetic variation has not been reduced. To do so, we measure segmentation robustness across different levels of segmental variation, simulating systematic allophonic variation or errors in phoneme recognition. We show that these models do not resist an increase in such variations and do not generalize to typologically different languages. From the perspective of early language acquisition, the results strengthen the hypothesis according to which phonological knowledge is acquired in large part before the construction of a lexicon. 
The mapping from phonetic categories to acoustic cue values is highly ﬂexible, and adapts rapidly in response to exposure. There is currently, however, no theoretical framework which captures the range of this adaptation. We develop a novel approach to modeling phonetic adaptation via a belief-updating model, and demonstrate that this model naturally uniﬁes two adaptation phenomena traditionally considered to be distinct. 
Learning to group words into phrases without supervision is a hard task for NLP systems, but infants routinely accomplish it. We hypothesize that infants use acoustic cues to prosody, which NLP systems typically ignore. To evaluate the utility of prosodic information for phrase discovery, we present an HMMbased unsupervised chunker that learns from only transcribed words and raw acoustic correlates to prosody. Unlike previous work on unsupervised parsing and chunking, we use neither gold standard part-of-speech tags nor punctuation in the input. Evaluated on the Switchboard corpus, our model outperforms several baselines that exploit either lexical or prosodic information alone, and, despite producing a ﬂat structure, performs competitively with a state-of-the-art unsupervised lexicalized parser, with a substantial advantage in precision. Our results support the hypothesis that acoustic-prosodic cues provide useful evidence about syntactic phrases for languagelearning infants. 
We propose a statistical test for measuring grammatical productivity. We show that very young children’s knowledge is consistent with a systematic grammar that independently combines linguistic units. To a testable extent, the usage-based approach to language and language learning, which emphasizes the role of lexically speciﬁc memorization, is inconsistent with the child language data. We also discuss the connection of this research with developments in computational and theoretical linguistics. 
This paper deﬁnes a normal form for MCFGs that includes strongly equivalent representations of many MG variants, and presents an incremental priority-queue-based TD recognizer for these MCFGs. After introducing MGs with overt phrasal movement, head movement and simple adjunction are added without change in the recognizer. The MG representation can be used directly, so that even rather sophisticated analyses of properly non-CF languages can be deﬁned very succinctly. As with the similar stack-based CFmethods, ﬁnite memory sufﬁces for the recognition of inﬁnite languages, and a fully connected left context for probabilistic analysis is available at every point. 
Greater learnability has been offered as an explanation as to why certain properties appear in human languages more frequently than others. Languages with greater learnability are more likely to be accurately transmitted from one generation of learners to the next. We explore whether such a learnability bias is sufﬁcient to result in a property becoming prevalent across languages by formalizing language transmission using a linear model. We then examine the outcome of repeated transmission of languages using a mathematical analysis, a computer simulation, and an experiment with human participants, and show several ways in which greater learnability may not result in a property becoming prevalent. Both the ways in which transmission failures occur and the relative number of languages with and without a property can affect whether the relationship between learnability and prevalence holds. Our results show that simply ﬁnding a learnability bias is not sufﬁcient to explain why a particular property is a linguistic universal, or even frequent among human languages. 
The aim of this paper is to present a computational model of the dynamic composition and update of verb argument expectations using Distributional Memory, a state-of-the-art framework for distributional semantics. The experimental results conducted on psycholinguistic data sets show that the model is able to successfully predict the changes on the patient argument thematic ﬁt produced by different types of verb agents. 
This paper presents a study of the effect of working memory load on the interpretation of pronouns in different discourse contexts: stories with and without a topic shift. We present a computational model (in ACT-R, Anderson, 2007) to explain how referring subjects are used and interpreted. We furthermore report on an experiment that tests predictions that follow from simulations. The results of the experiment support the model predictions that WM load only affects the interpretation of pronouns in stories with a topic shift, but not in stories without a topic shift. 
Conversational participants tend to immediately and unconsciously adapt to each other’s language styles: a speaker will even adjust the number of articles and other function words in their next utterance in response to the number in their partner’s immediately preceding utterance. This striking level of coordination is thought to have arisen as a way to achieve social goals, such as gaining approval or emphasizing difference in status. But has the adaptation mechanism become so deeply embedded in the language-generation process as to become a reﬂex? We argue that ﬁctional dialogs offer a way to study this question, since authors create the conversations but don’t receive the social beneﬁts (rather, the imagined characters do). Indeed, we ﬁnd signiﬁcant coordination across many families of function words in our large movie-script corpus. We also report suggestive preliminary ﬁndings on the effects of gender and other features; e.g., surprisingly, for articles, on average, characters adapt more to females than to males. 
Atypical or idiosyncratic language is a characteristic of autism spectrum disorder (ASD). In this paper, we discuss previous work identifying language errors associated with atypical language in ASD and describe a procedure for reproducing those results. We describe our data set, which consists of transcribed data from a widely used clinical diagnostic instrument (the ADOS) for children with autism, children with developmental language disorder, and typically developing children. We then present methods for automatically extracting lexical and syntactic features from transcripts of children’s speech to 1) identify certain syntactic and semantic errors that have previously been found to distinguish ASD language from that of children with typical development; and 2) perform diagnostic classiﬁcation. Our classiﬁers achieve results well above chance, demonstrating the potential for using NLP techniques to enhance neurodevelopmental diagnosis and atypical language analysis. We expect further improvement with additional data, features, and classiﬁcation techniques. 
Since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complimented with the use of appropriate colours in information visualization and product marketing. Yet, there is no comprehensive resource that captures concept–colour associations. We present a method to create a large word–colour association lexicon by crowdsourcing. We focus especially on abstract concepts and emotions to show that even though they cannot be physically visualized, they too tend to have strong colour associations. Finally, we show how word–colour associations manifest themselves in language, and quantify usefulness of co-occurrence and polarity cues in automatically detecting colour associations.1 
Survival analysis is often used in medical and biological studies to examine the time until some speciﬁed event occurs, such as the time until death of terminally ill patients. In this paper, however, we apply survival analysis to eye movement data in order to model the survival function of ﬁxation time distributions in reading. Semiparametric regression modeling and novel evaluation methods for probabilistic models of eye movements are presented. Survival models adjusting for the inﬂuence of linguistic and cognitive effects are shown to reduce prediction error within a critical time period, roughly between 150 and 250 ms following ﬁxation onset. 
We describe the beginning stages of our work on summarizing chat, which is motivated by our observations concerning the information overload of US Navy watchstanders. We describe the challenges of summarizing chat and focus on two chat-speciﬁc types of summarizations we are interested in: thread summaries and temporal summaries. We then discuss our plans for addressing these challenges and evaluation issues. 
We present a novel unsupervised approach to the problem of multi-document summarization of scientiﬁc articles, in which the document collection is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each co-cited article and relevance ranking using a query generated from the context surrounding the cocited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. We present a system called SciSumm that embodies this approach and apply it to the 2008 ACL Anthology. We evaluate this summarization system for relevant content selection using gold standard summaries prepared on principle based guidelines. Evaluation with gold standard summaries demonstrates that our system performs better in content selection than an existing summarization system (MEAD). We present a detailed summary of our ﬁndings and discuss possible directions for future research. 
 This paper addresses the problem of summarizing decisions in spoken meetings: our goal is to produce a concise decision abstract for each meeting decision. We explore and compare token-level and dialogue act-level automatic summarization methods using both unsupervised and supervised learning frameworks. In the supervised summarization setting, and given true clusterings of decisionrelated utterances, we ﬁnd that token-level summaries that employ discourse context can approach an upper bound for decision abstracts derived directly from dialogue acts. In the unsupervised summarization setting,we ﬁnd that summaries based on unsupervised partitioning of decision-related utterances perform comparably to those based on partitions generated using supervised techniques (0.22 ROUGE-F1 using LDA-based topic models vs. 0.23 using SVMs). 
Abstractive summarization has been a longstanding and long-term goal in automatic summarization, because systems that can generate abstracts demonstrate a deeper understanding of language and the meaning of documents than systems that merely extract sentences from those documents. Genest (2009) showed that summaries from the top automatic summarizers are judged as comparable to manual extractive summaries, and both are judged to be far less responsive than manual abstracts, As the state of the art approaches the limits of extractive summarization, it becomes even more pressing to advance abstractive summarization. However, abstractive summarization has been sidetracked by questions of what qualiﬁes as important information, and how do we ﬁnd it? The Guided Summarization task introduced at the Text Analysis Conference 2010 attempts to neutralize both of these problems by introducing topic categories and lists of aspects that a responsive summary should address. This design results in more similar human models, giving the automatic summarizers a more focused target to pursue, and also provides detailed diagnostics of summary content, which can can help build better meaningoriented summarization systems. 
We establish a novel task in the spirit of news summarization and topic detection and tracking (TDT): daily determination of the topics newly popular with Wikipedia readers. Central to this effort is a new public dataset consisting of the hourly page view statistics of all Wikipedia articles over the last three years. We give baseline results for the tasks of: discovering individual pages of interest, clustering these pages into coherent topics, and extracting the most relevant summarizing sentence for the reader. When compared to human judgements, our system shows the viability of this task, and opens the door to a range of exciting future work. 
Information graphics (bar charts, line graphs, etc.) in popular media generally have a discourse goal that contributes to achieving the communicative intent of a multimodal document. This paper presents our work on abstractive summarization of line graphs. Our methodology involves hypothesizing the intended message of a line graph and using it as the core of a summary of the graphic. This core is then augmented with salient propositions that elaborate on the intended message. 
Unsupervised approaches to multi-document summarization consist of two steps: ﬁnding a content model of the documents to be summarized, and then generating a summary that best represents the most salient information of the documents. In this paper, we present a sentence selection objective for extractive summarization in which sentences are penalized for containing content that is speciﬁc to the documents they were extracted from. We modify an existing system, HIERSUM (Haghighi & Vanderwende, 2009), to use our objective, which signiﬁcantly outperforms the original HIERSUM in pairwise user evaluation. Additionally, our ROUGE scores advance the current state-of-the-art for both supervised and unsupervised systems with statistical signiﬁcance. 
In this article we present the RST Spanish Treebank, the first corpus annotated with rhetorical relations for this language. We describe the characteristics of the corpus, the annotation criteria, the annotation procedure, the inter-annotator agreement, and other related aspects. Moreover, we show the interface that we have developed to carry out searches over the corpus’ annotated texts. 
This paper describes the modeling of the morphosyntactic annotations of the MULTEXT-East corpora and lexicons as an OWL/DL ontology. Formalizing annotation schemes in OWL/DL has the advantages of enabling formally specifying interrelationships between the various features and making logical inferences based on the relationships between them. We show that this approach provides us with a top-down perspective on a large set of morphosyntactic speciﬁcations for multiple languages, and that this perspective helps to identify and to resolve conceptual problems in the original speciﬁcations. Furthermore, the ontological modeling allows us to link the MULTEXT-East speciﬁcations with repositories of annotation terminology such as the General Ontology of Linguistics Descriptions or the ISO TC37/SC4 Data Category Registry. 
This paper makes two contributions. First, we describe the Hindi Proposition Bank that contains annotations of predicate argument structures of verb predicates. Unlike PropBanks in most other languages, the Hind PropBank is annotated on top of dependency structure, the Hindi Dependency Treebank. We explore the similarities between dependency and predicate argument structures, so the PropBank annotation can be faster and more accurate. Second, we present a probabilistic rule-based system that maps syntactic dependents to semantic arguments. With simple rules, we classify about 47% of the entire PropBank arguments with over 90% conﬁdence. These preliminary results are promising; they show how well these two frameworks are correlated. This can also be used to speed up our annotations. 
There has been a great deal of excitement recently about using the “wisdom of the crowd” to collect data of all kinds, quickly and cheaply (Howe, 2008; von Ahn and Dabbish, 2008). Snow et al. (Snow et al., 2008) were the ﬁrst to give a convincing demonstration that at least some kinds of linguistic data can be gathered from workers on the web more cheaply than and as accurately as from local experts, and there has been a steady stream of papers and workshops since then with similar results. e.g. (Callison-Burch and Dredze, 2010). Many of the tasks which have been successfully crowdsourced involve judgments which are similar to those performed in everyday life, such as recognizing unclear writing (von Ahn et al., 2008), or, for those tasks that require considerable judgment, the responses are usually binary or from a small set of responses, such as sentiment analysis (Mellebeek et al., 2010) or ratings (Heilman and Smith, 2010). Since the FrameNet process is known to be relatively expensive, we were interested in whether the FrameNet process of ﬁne word sense discrimination and marking of dependents with semantic roles could be performed more cheaply and equally accurately using Amazon’s Mechanical Turk (AMT) or similar resources. We report on a partial success in this respect and how it was achieved. 
For the implementation of the prosody prediction model, large scale annotated speech corpora have been widely applied. Reliability among transcribers, however, was too low for successful learning of an automatic prosodic prediction. This paper reveals our observations on performance deterioration of the learning model due to inconsistent tagging of prosodic breaks in the established corpora. Then, we suggest a method for consistent prosodic labeling among multiple transcribers. As a result, we obtain a corpus with consistent annotation of prosodic breaks. The estimated pairwise agreement of annotation of the main corpus is between 0.7477 and 0.7916, and the value of K is between 0.7057 and 0.7569. Considering the estimated K, annotation of the main corpus has reliable consistency among multiple transcribers. 
 2 Motivation and Background  BiasML is a novel annotation scheme with the purpose of identifying the presence as well as nuances of biased language within the subset of Wikipedia articles dedicated to service providers. Whereas Wikipedia currently uses only manual ﬂagging to detect possible bias, our scheme provides a foundation for the automating of bias ﬂagging by improving upon the methodology of annotation schemes in classic sentiment analysis. We also address challenges unique to the task of identifying biased writing within the speciﬁc context of Wikipedia’s neutrality policy. We perform a detailed analysis of inter-annotator agreement, which shows that although the agreement scores for intra-sentential tags were relatively low, the agreement scores on the sentence and entry levels were encouraging (74.8% and 66.7%, respectively). Based on an analysis of our ﬁrst implementation of our scheme, we suggest possible improvements to our guidelines, in hope that further rounds of annotation after incorporating them could provide appropriate data for use within a machine learning framework for automated detection of bias within Wikipedia. 
We describe a new interactive annotation scheme between a human annotator who carries out simpliﬁed annotations on CFG trees, and a statistical parser that converts the human annotations automatically into a richly annotated HPSG treebank. In order to check the proposed scheme’s effectiveness, we performed automatic pseudo-annotations that emulate the system’s idealized behavior and measured the performance of the parser trained on those annotations. In addition, we implemented a prototype system and conducted manual annotation experiments on a small test set. 
The quality of annotated data is crucial for supervised learning. To eliminate errors in single annotated data, a second round of annotation is often used. However, is it absolutely necessary to double annotate every example? We show that it is possible to reduce the amount of the second round of annotation by more than half without sacriﬁcing the performance. 
In this paper, we propose a crowdsourcing methodology for a single-step construction of both an empirically-derived sense inventory and the corresponding sense-annotated corpus. The methodology taps the intuitions of non-expert native speakers to create an expertquality resource, and natively lends itself to supplementing such a resource with additional information about the structure and reliability of the produced sense inventories. The resulting resource will provide several ways to empirically measure distances between related word senses, and will explicitly address the question of fuzzy boundaries between them. 
 (Leech, 1993; Ide and Brew, 2000; Wynne, 2005;  Cohen et al., 2005a; Cohen et al., 2005b) that ad-  This paper presents an evaluation of an automated quality assurance technique for a type of semantic representation known as a predicate argument structure. These representations are crucial to the development of an important class of corpus known as a proposition bank. Previous work (Cohen and Hunter, 2006) proposed and tested an analytical technique based on a simple discovery procedure inspired by classic structural linguistic methodology. Cohen and Hunter applied the technique manually to a small set of representations. Here we test the feasibility of automating the technique, as well as the ability of the technique to scale to a set of semantic representations and to a corpus many times  dress architectural, sampling, and procedural issues, as well as publications such as (Hripcsak and Rothschild, 2005; Artstein and Poesio, 2008) that address issues in inter-annotator agreement. However, there is not yet a signiﬁcant body of work on the subject of quality assurance for corpora, or for that matter, for many other types of linguistic resources. (Meyers et al., 2004) describe three error-checking measures used in the construction of NomBank, and the use of inter-annotator agreement as a quality control measure for corpus construction is discussed at some length in (Marcus et al., 1993; Palmer et al., 2005). However, discussion of quality control for corpora is otherwise limited or nonexistent.  larger than that used by Cohen and Hunter. We conclude that the technique is completely automatable, uncovers missing sense distinctions and other bad semantic representations, and does scale well, performing at an accuracy of 69% for identifying bad representations. We also report on the implications of  With the exception of the inter-annotatoragreement-oriented work mentioned above, none of this work is quantitative. This is a problem if our goal is the development of a true science of annotation. Work on quality assurance for computational lex-  our ﬁndings for the correctness of the semantic representations in PropBank.  ical resources other than ontologies is especially lacking. However, the body of work on quality as-  surance for ontologies (Kohler et al., 2006; Ceusters  
Within the framework of the construction of a fact database, we deﬁned guidelines to extract named entities, using a taxonomy based on an extension of the usual named entities deﬁnition. We thus deﬁned new types of entities with broader coverage including substantivebased expressions. These extended named entities are hierarchical (with types and components) and compositional (with recursive type inclusion and metonymy annotation). Human annotators used these guidelines to annotate a 1.3M word broadcast news corpus in French. This article presents the deﬁnition and novelty of extended named entity annotation guidelines, the human annotation of a global corpus and of a mini reference corpus, and the evaluation of annotations through the computation of inter-annotator agreements. Finally, we discuss our approach and the computed results, and outline further work. 
The creation of a gold standard corpus (GSC) is a very laborious and costly process. Silver standard corpus (SSC) annotation is a very recent direction of corpus development which relies on multiple systems instead of human annotators. In this paper, we investigate the practical usability of an SSC when a machine learning system is trained on it and tested on an unseen benchmark GSC. The main focus of this paper is how an SSC can be maximally exploited. In this process, we inspect several hypotheses which might have inﬂuenced the idea of SSC creation. Empirical results suggest that some of the hypotheses (e.g. a positive impact of a large SSC despite of having wrong and missing annotations) are not fully correct. We show that it is possible to automatically improve the quality and the quantity of the SSC annotations. We also observe that considering only those sentences of SSC which contain annotations rather than the full SSC results in a performance boost. 
Subjectivity and sentiment analysis (SSA) is an area that has been witnessing a ﬂurry of novel research. However, only few attempts have been made to build SSA systems for morphologically-rich languages (MRL). In the current study, we report efforts to partially bridge this gap. We present a newly labeled corpus of Modern Standard Arabic (MSA) from the news domain manually annotated for subjectivity and domain at the sentence level. We summarize our linguisticallymotivated annotation guidelines and provide examples from our corpus exemplifying the different phenomena. Throughout the paper, we discuss expression of subjectivity in natural language, combining various previously scattered insights belonging to many branches of linguistics. 
We describe our efforts to apply the Penn Discourse Treebank guidelines on a Tamil corpus to create an annotated corpus of discourse relations in Tamil. After conducting a preliminary exploratory study on Tamil discourse connectives, we show our observations and results of a pilot experiment that we conducted by annotating a small portion of our corpus. Our ultimate goal is to develop a Tamil Discourse Relation Bank that will be useful as a resource for further research in Tamil discourse. Furthermore, a study of the behavior of discourse connectives in Tamil will also help in furthering the cross-linguistic understanding of discourse connectives. 
This paper describes an annotated gold standard sample corpus of Early Modern German containing over 50,000 tokens of text manually annotated with POS tags, lemmas, and normalised spelling variants. The corpus is the ﬁrst resource of its kind for this variant of German, and represents an ideal test bed for evaluating and adapting existing NLP tools on historical data. We describe the corpus format, annotation levels, and challenges, providing an example of the requirements and needs of smaller humanities-based corpus projects. 
MAE and MAI are lightweight annotation and adjudication tools for corpus creation. DTDs are used to deﬁne the annotation tags and attributes, including extent tags, link tags, and non-consuming tags. Both programs are written in Java and use a stand-alone SQLite database for storage and retrieval of annotation data. Output is in stand-off XML. 
In this paper, we ﬁrst analyze and classify the empty categories in a Hindi dependency treebank and then identify various discovery procedures to automatically detect the existence of these categories in a sentence. For this we make use of lexical knowledge along with the parsed output from a constraint based parser. Through this work we show that it is possible to successfully discover certain types of empty categories while some other types are more difﬁcult to identify. This work leads to the state-of-the-art system for automatic insertion of empty categories in the Hindi sentence. 
This paper presents the annotation guidelines and specifications which have been developed for the creation of the Italian TimeBank, a language resource composed of two corpora manually annotated with temporal and event information. In particular, the adaptation of the TimeML scheme to Italian is described, and a special attention is given to the methodology used for the realization of the annotation specifications, which are strategic in order to create good quality annotated resources and to justify the annotated items. The reliability of the It-TimeML guidelines and specifications is evaluated on the basis of the results of the inter-coder agreement performed during the annotation of the two corpora.  Introduction 
In this paper, we discuss some of the challenges of adequately applying a speciﬁcation language to an annotation task, as embodied in a speciﬁc guideline. In particular, we discuss some issues with TimeML motivated by error analysis on annotated TLINKs in TimeBank. We introduce a document level information structure we call a narrative container (NC), designed to increase informativeness and accuracy of temporal relation identiﬁcation. The narrative container is the default interval containing the events being discussed in the text, when no explicit temporal anchor is given. By exploiting this notion in the creation of a new temporal annotation over TimeBank, we were able to reduce inconsistencies and increase informativeness when compared to existing TLINKs in TimeBank. 
We describe an experiment on a temporal ordering task in this paper. We show that by selecting event pairs based on discourse structure and by modifying the pre-existent temporal classiﬁcation scheme to ﬁt the data better, we signiﬁcantly improve inter-annotator agreement, as well as broaden the coverage of the task. We also present analysis of the current temporal classiﬁcation scheme and propose ways to improve it in future work. 
The connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems. This paper proposes a novel model for morphological segmentation that is driven by this connection. Our model learns that words with common afﬁxes are likely to be in the same syntactic category and uses learned syntactic categories to reﬁne the segmentation boundaries of words. Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic. 1  tic information, because morphological segmentation precedes other forms of sentence analysis. In this paper, we demonstrate that morphological analysis can utilize this connection without assuming access to full-ﬂedged syntactic information. In particular, we focus on two aspects of the morphosyntactic connection: • Morphological consistency within POS categories. Words within the same syntactic category tend to select similar afﬁxes. This linguistic property signiﬁcantly reduces the space of possible morphological analyses, ruling out assignments that are incompatible with a syntactic category.  
We show how punctuation can be used to improve unsupervised dependency parsing. Our linguistic analysis conﬁrms the strong connection between English punctuation and phrase boundaries in the Penn Treebank. However, approaches that naively include punctuation marks in the grammar (as if they were words) do not perform well with Klein and Manning’s Dependency Model with Valence (DMV). Instead, we split a sentence at punctuation and impose parsing restrictions over its fragments. Our grammar inducer is trained on the Wall Street Journal (WSJ) and achieves 59.5% accuracy out-of-domain (Brown sentences with 100 or fewer words), more than 6% higher than the previous best results. Further evaluation, using the 2006/7 CoNLL sets, reveals that punctuation aids grammar induction in 17 of 18 languages, for an overall average net gain of 1.3%. Some of this improvement is from training, but more than half is from parsing with induced constraints, in inference. Punctuation-aware decoding works with existing (even already-trained) parsing models and always increased accuracy in our experiments. 
While many computational models have been created to explore how children might learn to segment words, the focus has largely been on achieving higher levels of performance and exploring cues suggested by artiﬁcial learning experiments. We propose a broader focus that includes designing models that display properties of infants’ performance as they begin to segment words. We develop an efﬁcient bootstrapping online learner with this focus in mind, and evaluate it on child-directed speech. In addition to attaining a high level of performance, this model predicts the error patterns seen in infants learning to segment words. 
During language acquisition, children learn to segment speech into phonemes, syllables, morphemes, and words. We examine word segmentation speciﬁcally, and explore the possibility that children might have generalpurpose chunking mechanisms to perform word segmentation. The Voting Experts (VE) and Bootstrapped Voting Experts (BVE) algorithms serve as computational models of this chunking ability. VE ﬁnds chunks by searching for a particular information-theoretic signature: low internal entropy and high boundary entropy. BVE adds to VE the ability to incorporate information about word boundaries previously found by the algorithm into future segmentations. We evaluate the general chunking model on phonemicallyencoded corpora of child-directed speech, and show that it is consistent with empirical results in the developmental literature. We argue that it offers a parsimonious alternative to specialpurpose linguistic models. 
Automatic extraction of opinion holders and targets (together referred to as opinion entities) is an important subtask of sentiment analysis. In this work, we attempt to accurately extract opinion entities from Urdu newswire. Due to the lack of resources required for training role labelers and dependency parsers (as in English) for Urdu, a more robust approach based on (i) generating candidate word sequences corresponding to opinion entities, and (ii) subsequently disambiguating these sequences as opinion holders or targets is presented. Detecting the boundaries of such candidate sequences in Urdu is very different than in English since in Urdu, grammatical categories such as tense, gender and case are captured in word inflections. In this work, we exploit the morphological inflections associated with nouns and verbs to correctly identify sequence boundaries. Different levels of information that capture context are encoded to train standard linear and sequence kernels. To this end the best performance obtained for opinion entity detection for Urdu sentiment analysis is 58.06% F-Score using sequence kernels and 61.55% F-Score using a combination of sequence and linear kernels. 
Crisis-affected populations are often able to maintain digital communications but in a sudden-onset crisis any aid organizations will have the least free resources to process such communications. Information that aid agencies can actually act on, ‘actionable’ information, will be sparse so there is great potential to (semi)automatically identify actionable communications. However, there are hurdles as the languages spoken will often be underresourced, have orthographic variation, and the precise deﬁnition of ‘actionable’ will be response-speciﬁc and evolving. We present a novel system that addresses this, drawing on 40,000 emergency text messages sent in Haiti following the January 12, 2010 earthquake, predominantly in Haitian Kreyol. We show that keyword/ngram-based models using streaming MaxEnt achieve up to F=0.21 accuracy. Further, we ﬁnd current state-ofthe-art subword models increase this substantially to F=0.33 accuracy, while modeling the spatial, temporal, topic and source contexts of the messages can increase this to a very accurate F=0.86 over direct text messages and F=0.90-0.97 over social media, making it a viable strategy for message prioritization. 
Sociolinguistic theories (e.g., Lakoff (1973)) postulate that women’s language styles differ from that of men. In this paper, we explore statistical techniques that can learn to identify the gender of authors in modern English text, such as web blogs and scientiﬁc papers. Although recent work has shown the efﬁcacy of statistical approaches to gender attribution, we conjecture that the reported performance might be overly optimistic due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier. Our work is the ﬁrst that consciously avoids gender bias in topics, thereby providing stronger evidence to gender-speciﬁc styles in language beyond topic. In addition, our comparative study provides new insights into robustness of various stylometric techniques across topic and genre. 
Subjectivity word sense disambiguation (SWSD) is automatically determining which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. SWSD has been shown to improve the performance of contextual opinion analysis, but only on a small scale and using manually developed integration rules. In this paper, we scale up the integration of SWSD into contextual opinion analysis and still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater beneﬁts from SWSD are achieved than in previous work. We thus more ﬁrmly demonstrate the potential of SWSD to improve contextual opinion analysis.  
We present a computational model of language learning via a sequence of interactions between a teacher and a learner. Experiments learning limited sublanguages of 10 natural languages show that the learner achieves a high level of performance after a reasonable number of interactions, the teacher can produce meaning-preserving corrections of the learner’s utterances, and the learner can detect them. The learner does not treat corrections specially; nonetheless in several cases, signiﬁcantly fewer interactions are needed by a learner interacting with a correcting teacher than with a non-correcting teacher. 
Feature feedback is an alternative to instance labeling when seeking supervision from human experts. Combination of instance and feature feedback has been shown to reduce the total annotation cost for supervised learning. However, learning problems may not beneﬁt equally from feature feedback. It is well understood that the beneﬁt from feature feedback reduces as the amount of training data increases. We show that other characteristics such as domain, instance granularity, feature space, instance selection strategy and proportion of relevant text, have a signiﬁcant effect on beneﬁt from feature feedback. We estimate the maximum beneﬁt feature feedback may provide; our estimate does not depend on how the feedback is solicited and incorporated into the model. We extend the complexity measures proposed in the literature and propose some new ones to categorize learning problems, and ﬁnd that they are strong indicators of the beneﬁt from feature feedback. 
In this paper we present ULISSE, an unsupervised linguistically–driven algorithm to select reliable parses from the output of a dependency parser. Different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains. In all cases, ULISSE appears to outperform the baseline algorithms. 
Finding the right representation for words is critical for building accurate NLP systems when domain-speciﬁc labeled data for the task is scarce. This paper investigates language model representations, in which language models trained on unlabeled corpora are used to generate real-valued feature vectors for words. We investigate ngram models and probabilistic graphical models, including a novel lattice-structured Markov Random Field. Experiments indicate that language model representations outperform traditional representations, and that graphical model representations outperform ngram models, especially on sparse and polysemous words. 
Keyphrase extraction aims to select a set of terms from a document as a short summary of the document. Most methods extract keyphrases according to their statistical properties in the given document. Appropriate keyphrases, however, are not always statistically signiﬁcant or even do not appear in the given document. This makes a large vocabulary gap between a document and its keyphrases. In this paper, we consider that a document and its keyphrases both describe the same object but are written in two different languages. By regarding keyphrase extraction as a problem of translating from the language of documents to the language of keyphrases, we use word alignment models in statistical machine translation to learn translation probabilities between the words in documents and the words in keyphrases. According to the translation model, we suggest keyphrases given a new document. The suggested keyphrases are not necessarily statistically frequent in the document, which indicates that our method is more ﬂexible and reliable. Experiments on news articles demonstrate that our method outperforms existing unsupervised methods on precision, recall and F-measure. 
In this paper, we introduce a knowledge-based method to disambiguate biomedical acronyms using second-order co-occurrence vectors. We create these vectors using information about a long-form obtained from the Uniﬁed Medical Language System and Medline. We evaluate this method on a dataset of 18 acronyms found in biomedical text. Our method achieves an overall accuracy of 89%. The results show that using second-order features provide a distinct representation of the long-form and potentially enhances automated disambiguation. 
The ﬁrst step in graph-based semi-supervised classiﬁcation is to construct a graph from input data. While the k-nearest neighbor graphs have been the de facto standard method of graph construction, this paper advocates using the less well-known mutual k-nearest neighbor graphs for high-dimensional natural language data. To compare the performance of these two graph construction methods, we run semi-supervised classiﬁcation methods on both graphs in word sense disambiguation and document classiﬁcation tasks. The experimental results show that the mutual k-nearest neighbor graphs, if combined with maximum spanning trees, consistently outperform the knearest neighbor graphs. We attribute better performance of the mutual k-nearest neighbor graph to its being more resistive to making hub vertices. The mutual k-nearest neighbor graphs also perform equally well or even better in comparison to the state-of-the-art b-matching graph construction, despite their lower computational complexity. 
In this paper we present methods for automatically acquiring training examples for the task of entity extraction. Experimental evidence show that: (1) our methods compete with a current heavily supervised state-of-the-art system, within 0.04 absolute mean average precision; and (2) our model signiﬁcantly outperforms other supervised and unsupervised baselines by between 0.15 and 0.30 in absolute mean average precision. 
This paper makes two contributions to the area of single-word based word alignment for bilingual sentence pairs. Firstly, it integrates the – seemingly rather different – works of (Bodrumlu et al., 2009) and the standard probabilistic ones into a single framework. Secondly, we present two algorithms to optimize the arising task. The ﬁrst is an iterative scheme similar to Viterbi training, able to handle large tasks. The second is based on the inexact solution of an integer program. While it can handle only small corpora, it allows more insight into the quality of the model and the performance of the iterative scheme. Finally, we present an alternative way to handle prior dictionary knowledge and discuss connections to computing IBM-3 Viterbi alignments. 
The problem of authorship attribution – attributing texts to their original authors – has been an active research area since the end of the 19th century, attracting increased interest in the last decade. Most of the work on authorship attribution focuses on scenarios with only a few candidate authors, but recently considered cases with tens to thousands of candidate authors were found to be much more challenging. In this paper, we propose ways of employing Latent Dirichlet Allocation in authorship attribution. We show that our approach yields state-of-the-art performance for both a few and many candidate authors, in cases where these authors wrote enough texts to be modelled effectively. 
We describe the extension and objective evaluation of a network1 of semantically related noun senses (or concepts) that has been automatically acquired by analyzing lexical cooccurrence in Wikipedia. The acquisition process makes no use of the metadata or links that have been manually built into the encyclopedia, and nouns in the network are automatically disambiguated to their corresponding noun senses without supervision. For this task, we use the noun sense inventory of WordNet 3.0. Thus, this work can be conceived of as augmenting the WordNet noun ontology with unweighted, undirected relatedto edges between synsets. Our network contains 208,832 such edges. We evaluate our network’s performance on a word sense disambiguation (WSD) task and show: a) the network is competitive with WordNet when used as a stand-alone knowledge source for two WSD algorithms; b) combining our network with WordNet achieves disambiguation results that exceed the performance of either resource individually; and c) our network outperforms a similar resource that has been automatically derived from semantic annotations in the Wikipedia corpus. 
We investigate the use of Semi-Supervised Learning (SSL) in opinion detection both in sparse data situations and for domain adaptation. We show that co-training reaches the best results in an in-domain setting with small labeled data sets, with a maximum absolute gain of 33.5%. For domain transfer, we show that self-training gains an absolute improvement in labeling accuracy for blog data of 16% over the supervised approach with target domain training data. 
We propose a normalized-cut model for the problem of aligning a known hierarchical browsing structure, e.g., electronic slides of lecture recordings, with the sequential transcripts of the corresponding spoken documents, with the aim to help index and access the latter. This model optimizes a normalizedcut graph-partitioning criterion and considers local tree constraints at the same time. The experimental results show the advantage of this model over Viterbi-like, sequential alignment, under typical speech recognition errors. 
In recent years Bayesian techniques have made good inroads in computational linguistics, due to their protection against overﬁtting and expressiveness of the Bayesian modeling language. However most Bayesian models proposed so far have used pretty simple prior distributions, chosen more for computational convenience than as reﬂections of real prior knowledge. 
Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the speciﬁc content of an image, while permitting creativity in the description – making for more human-like annotations than previous approaches. 
Natural language systems trained on labeled data from one domain do not perform well on other domains. Most adaptation algorithms proposed in the literature train a new model for the new domain using unlabeled data. However, it is time consuming to retrain big models or pipeline systems. Moreover, the domain of a new target sentence may not be known, and one may not have signiﬁcant amount of unlabeled data for every new domain. To pursue the goal of an Open Domain NLP (train once, test anywhere), we propose ADUT (ADaptation Using label-preserving Transformation), an approach that avoids the need for retraining and does not require knowledge of the new domain, or any data from it. Our approach applies simple label-preserving transformations to the target text so that the transformed text is more similar to the training domain; it then applies the existing model on the transformed sentences and combines the predictions to produce the desired prediction on the target text. We instantiate ADUT for the case of Semantic Role Labeling (SRL) and show that it compares favorably with approaches that retrain their model on the target domain. Speciﬁcally, this “on the ﬂy” adaptation approach yields 13% error reduction for a single parse system when adapting from the news wire text to ﬁction. 
This paper shows that the performance of history-based models can be signiﬁcantly improved by performing lookahead in the state space when making each classiﬁcation decision. Instead of simply using the best action output by the classiﬁer, we determine the best action by looking into possible sequences of future actions and evaluating the ﬁnal states realized by those action sequences. We present a perceptron-based parameter optimization method for this learning framework and show its convergence properties. The proposed framework is evaluated on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random ﬁelds (CRFs) and structured perceptrons. 
Traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms. We propose a novel discriminative training method that projects the raw term vectors into a common, low-dimensional vector space. Our approach operates by ﬁnding the optimal matrix to minimize the loss of the pre-selected similarity function (e.g., cosine) of the projected vectors, and is able to efﬁciently handle a large number of training examples in the highdimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efﬁcient. 
The extraction of protein-protein interactions (PPIs) reported in scientiﬁc publications is one of the most studied topics in Text Mining in the Life Sciences, as such algorithms can substantially decrease the effort for databases curators. The currently best methods for this task are based on analyzing the dependency tree (DT) representation of sentences. Many approaches exploit only topological features and thus do not yet fully exploit the information contained in DTs. We show that incorporating the grammatical information encoded in the types of the dependencies in DTs noticeably improves extraction performance by using a pattern matching approach. We automatically infer a large set of linguistic patterns using only information about interacting proteins. Patterns are then reﬁned based on shallow linguistic features and the semantics of dependency types. Together, these lead to a total improvement of 17.2 percent points in F1, as evaluated on ﬁve publicly available PPI corpora. More than half of that improvement is gained by properly handling dependency types. Our method provides a general framework for building task-speciﬁc relationship extraction methods that do not require annotated training data. Furthermore, our observations offer methods to improve upon relation extraction approaches. 
Entailment detection systems are generally designed to work either on single words, relations or full sentences. We propose a new task – detecting entailment between dependency graph fragments of any type – which relaxes these restrictions and leads to much wider entailment discovery. An unsupervised framework is described that uses intrinsic similarity, multi-level extrinsic similarity and the detection of negation and hedged language to assign a conﬁdence score to entailment relations between two fragments. The ﬁnal system achieves 84.1% average precision on a data set of entailment examples from the biomedical domain. 
Accurate phenotype mapping will play an important role in facilitating Phenome-Wide Association Studies (PheWAS), and potentially in other phenomics based studies. The PheWAS approach investigates the association between genetic variation and an extensive range of phenotypes in a high-throughput manner to better understand the impact of genetic variations on multiple phenotypes. Herein we deﬁne the phenotype mapping problem posed by PheWAS analyses, discuss the challenges, and present a machine-learning solution. Our key ideas include the use of weighted Jaccard features and term augmentation by dictionary lookup. When compared to string similarity metric-based features, our approach improves the F-score from 0.59 to 0.73. With augmentation we show further improvement in F-score to 0.89. For terms not covered by the dictionary, we use transitive closure inference and reach an F-score of 0.91, close to a level sufﬁcient for practical use. We also show that our model generalizes well to phenotypes not used in our training dataset. 
In comparative genomics, functional annotations are transferred from one organism to another relying on sequence similarity. With more than 20 million citations in PubMed, text mining provides the ideal tool for generating additional large-scale homology-based predictions. To this end, we have reﬁned a recent dataset of biomolecular events extracted from text, and integrated these predictions with records from public gene databases. Accounting for lexical variation of gene symbols, we have implemented a disambiguation algorithm that uniquely links the arguments of 11.2 million biomolecular events to well-deﬁned gene families, providing interesting opportunities for query expansion and hypothesis generation. The resulting MySQL database, including all 19.2 million original events as well as their homology-based variants, is publicly available at http://bionlp.utu.fi/. 
A simple and accurate method for assigning broad semantic classes to text strings is presented. The method is to map text strings to terms in ontologies based on a pipeline of exact matches, normalized strings, headword matching, and stemming headwords. The results of three experiments evaluating the technique are given. Five semantic classes are evaluated against the CRAFT corpus of full-text journal articles. Twenty semantic classes are evaluated against the corresponding full ontologies, i.e. by reﬂexive matching. One semantic class is evaluated against a structured test suite. Precision, recall, and F-measure on the corpus when evaluating against only the ontologies in the corpus is micro-averaged 67.06/78.49/72.32 and macro-averaged 69.84/83.12/75.31. Accuracy on the corpus when evaluating against all twenty semantic classes ranges from 77.12% to 95.73%. Reﬂexive matching is generally successful, but reveals a small number of errors in the implementation. Evaluation with the structured test suite reveals a number of characteristics of the performance of the approach. 
Traditionally, automated triage of papers is performed using lexical (unigram, bigram, and sometimes trigram) features. This paper explores the use of information extraction (IE) techniques to create richer linguistic features than traditional bag-of-words models. Our classiﬁer includes lexico-syntactic patterns and more-complex features that represent a pattern coupled with its extracted noun, represented both as a lexical term and as a semantic category. Our experimental results show that the IE-based features can improve performance over unigram and bigram features alone. We present intrinsic evaluation results of full-text document classiﬁcation experiments to determine automatically whether a paper should be considered of interest to biologists at the Mouse Genome Informatics (MGI) system at the Jackson Laboratories. We also further discuss issues relating to design and deployment of our classiﬁers as an application to support scientiﬁc knowledge curation at MGI. 
Medical Entity Recognition is a crucial step towards efﬁcient medical texts analysis. In this paper we present and compare three methods based on domain-knowledge and machine-learning techniques. We study two research directions through these approaches: (i) a ﬁrst direction where noun phrases are extracted in a ﬁrst step with a chunker before the ﬁnal classiﬁcation step and (ii) a second direction where machine learning techniques are used to identify simultaneously entities boundaries and categories. Each of the presented approaches is tested on a standard corpus of clinical texts. The obtained results show that the hybrid approach based on both machine learning and domain knowledge obtains the best performance. 
Named Entity Recognition (NER) is an important ﬁrst step for BioNLP tasks, e.g., gene normalization and event extraction. Employing supervised machine learning techniques for achieving high performance recent NER systems require a manually annotated corpus in which every mention of the desired semantic types in a text is annotated. However, great amounts of human effort is necessary to build and maintain an annotated corpus. This study explores a method to build a high-performance NER without a manually annotated corpus, but using a comprehensible lexical database that stores numerous expressions of semantic types and with huge amount of unannotated texts. We underscore the effectiveness of our approach by comparing the performance of NERs trained on an automatically acquired training data and on a manually annotated corpus. 
Semantic Role Labeling (SRL) plays a key role in many NLP applications. The development of SRL systems for the biomedical domain is frustrated by the lack of large domainspeciﬁc corpora that are labeled with semantic roles. Corpus development has been very expensive and time-consuming. In this paper we propose a method for building frame-based corpus on the basis of domain knowledge provided by ontologies. We believe that ontologies, as a structured and semantic representation of domain knowledge, can instruct and ease the tasks in building the corpora. In the paper we present a corpus built by using the method. We compared it to BioFrameNet, and examined the gaps between the semantic classiﬁcation of the target words in the domainspeciﬁc corpus and in FrameNet and PropBank/VerbNet. 
One of the reasons for which the resolution of coreferences has remained a challenging information extraction task, especially in the biomedical domain, is the lack of training data in the form of annotated corpora. In order to address this issue, we developed the HANAPIN corpus. It consists of full-text articles from biochemistry literature, covering entities of several semantic types: chemical compounds, drug targets (e.g., proteins, enzymes, cell lines, pathogens), diseases, organisms and drug effects. All of the coreferring expressions pertaining to these semantic types were annotated based on the annotation scheme that we developed. We observed four general types of coreferences in the corpus: sortal, pronominal, abbreviation and numerical. Using the MASI distance metric, we obtained 84% in computing the inter-annotator agreement in terms of Krippendorff’s alpha. Consisting of 20 full-text, open-access articles, the corpus will enable other researchers to use it as a resource for their own coreference resolution methodologies. 
The paper discuses problems in annotating a corpus containing Polish clinical data with low level linguistic information. We propose an approach to tokenization and automatic morphologic annotation of data that uses existing programs combined with a set of domain speciﬁc rules and vocabulary. Finally we present the results of manual veriﬁcation of the annotation for a subset of data. 
Research in the biomedical domain can have a major impact through open sharing of data produced. In this study, we use machine learning for the automatic identification of data deposition sentences in research articles. Articles containing deposition sentences are correctly identified with 73% f-measure. These results show the potential impact of our method for literature curation. 
Kernel methods are considered the most effective techniques for various relation extraction (RE) tasks as they provide higher accuracy than other approaches. In this paper, we introduce new dependency tree (DT) kernels for RE by improving on previously proposed dependency tree structures. These are further enhanced to design more effective approaches that we call mildly extended dependency tree (MEDT) kernels. The empirical results on the protein-protein interaction (PPI) extraction task on the AIMed corpus show that tree kernels based on our proposed DT structures achieve higher accuracy than previously proposed DT and phrase structure tree (PST) kernels. 
Increasingly, as full-text scientiﬁc papers are becoming available, scientiﬁc queries have shifted from looking for facts to looking for arguments. Researchers want to know when their colleagues are proposing theories, outlining evidentiary relations, or explaining discrepancies. We show here that sentence-level annotation with the CISP schema adapts well to a corpus of biomedical articles, and we present preliminary results arguing that the CISP schema is uniquely suited to recovering common types of scientiﬁc arguments about hypotheses, explanations, and evidence. 
In this study we investigate the merits of fast approximate string matching to address challenges relating to spelling variants and to utilise large-scale lexical resources for semantic class disambiguation. We integrate string matching results into machine learning-based disambiguation through the use of a novel set of features that represent the distance of a given textual span to the closest match in each of a collection of lexical resources. We collect lexical resources for a multitude of semantic categories from a variety of biomedical domain sources. The combined resources, containing more than twenty million lexical items, are queried using a recently proposed fast and efﬁcient approximate string matching algorithm that allows us to query large resources without severely impacting system performance. We evaluate our results on six corpora representing a variety of disambiguation tasks. While the integration of approximate string matching features is shown to substantially improve performance on one corpus, results are modest or negative for others. We suggest possible explanations and future research directions. Our lexical resources and implementation are made freely available for research purposes at: http://github.com/ninjin/ simsem 
We present an end-to-end system that processes narrative clinical records, constructs timelines for the medical histories of patients, and visualizes the results. This work is motivated by real clinical records and our general approach is based on deep semantic natural language understanding. 
Suppose we have a large collection of documents most of which are unlabeled. Suppose further that we have a small subset of these documents which represent a particular class of documents we are interested in, i.e. these are labeled as positive examples. We may have reason to believe that there are more of these positive class documents in our large unlabeled collection. What data mining techniques could help us find these unlabeled positive examples? Here we examine machine learning strategies designed to solve this problem. We find that a proper choice of machine learning method as well as training strategies can give substantial improvement in retrieving, from the large collection, data enriched with positive examples. We illustrate the principles with a real example consisting of multiword UMLS phrases among a much larger collection of phrases from Medline. 
This paper presents our preliminary work on adaptation of parsing technology toward natural language query processing for biomedical domain. We built a small treebank of natural language queries, and tested a state-of-theart parser, the results of which revealed that a parser trained on Wall-Street-Journal articles and Medline abstracts did not work well on query sentences. We then experimented an adaptive learning technique, to seek the chance to improve the parsing performance on query sentences. Despite the small scale of the experiments, the results are encouraging, enlightening the direction for effective improvement. 
Ontology authoring is a specialised task requiring amongst other things a deep knowledge of the ontology language being used. Understanding and reusing ontologies can thus be difficult for domain experts, who tend not to be ontology experts. To address this problem, we have developed a Natural Language Generation system for transforming the axioms that form the definitions of ontology classes into Natural Language paragraphs. Our method relies on deploying ontology axioms into a top-level Rhetorical Structure Theory schema. Axioms are ordered and structured with specific rhetorical relations under rhetorical structure trees. We describe here an implementation that focuses on a sub-module of SNOMED CT. With some refinements on articles and layout, the resulting paragraphs are fluent and coherent, offering a way for subject specialists to understand an ontology’s content without need to understand its logical representation. 
Word sense disambiguation (WSD) is an intermediate task within information retrieval and information extraction, attempting to select the proper sense of ambiguous words. Due to the scarcity of training data, semi-supervised learning, which proﬁts from seed annotated examples and a large set of unlabeled data, are worth researching. We present preliminary results of two semi-supervised learning algorithms on biomedical word sense disambiguation. Both methods add relevant unlabeled examples to the training set, and optimal parameters are similar for each ambiguous word. 
We present MedstractPlus, a resource for mining relations from the Medline bibliographic database. It was built on the remains of Medstract, a previously created resource that included a bio-relation server and an acronym database. MedstractPlus uses simple and scalable natural language processing modules to structure text and is designed with reusability and extendibility in mind. 
Thai Traditional Medicine (TTM) has a long history in Thailand and is nowadays considered an effective alternative approach to the modern medicine. One of the main knowledge in Thai traditional medicine is the use of various types of herbs to form medicines. Our main goal is to bridge the gap between the traditional knowledge and the modern biomedical knowledge. Using text mining and visualization techniques, some implicit relations from one source could be used to verify and enhance the knowledge discovery in another source. In this paper, we present our ongoing work, ThaiHerbMiner, a Thai herbal medicine mining and visualizing tool. ThaiHerbMiner applies text mining to extract some salient relations from a collection of PubMed articles related to Thai herbs. The extracted relations can be browsed and viewed using information visualization. Our proposed tool can also recommend a list of herbs which have similar medical properties. 
This paper presents an update semantic for dialogue acts, deﬁned in terms of combinations of very simple ‘elementary update functions’. This approach allows ﬁne-grained distinctions to be made between related types of dialogue acts, and relations like entailment and exclusion between dialogue acts to be established. The approach is applied to dialogue act representations as deﬁned in the Dialogue Act Markup Language (DiAML), part of the recently proposed ISO standard 24617-2 for dialogue act annotation. 
We present a method for training a statistical model for mapping natural language sentences to semantic expressions. The semantics are expressions of an underspeciﬁed logical form that has properties making it particularly suitable for statistical mapping from text. An encoding of the semantic expressions into dependency trees with automatically generated labels allows application of existing methods for statistical dependency parsing to the mapping task (without the need for separate traditional dependency labels or parts of speech). The encoding also results in a natural per-word semantic-mapping accuracy measure. We report on the results of training and testing statistical models for mapping sentences of the Penn Treebank into the semantic expressions, for which per-word semantic mapping accuracy ranges between 79% and 86% depending on the experimental conditions. The particular choice of algorithms used also means that our trained mapping is deterministic (in the sense of deterministic parsing), paving the way for large-scale text-to-semantic mapping. 
We present a system to translate natural language sentences to formulas in a formal or a knowledge representation language. Our system uses two inverse λ-calculus operators and using them can take as input the semantic representation of some words, phrases and sentences and from that derive the semantic representation of other words and phrases. Our inverse λ operator works on many formal languages including ﬁrst order logic, database query languages and answer set programming. Our system uses a syntactic combinatorial categorial parser to parse natural language sentences and also to construct the semantic meaning of the sentences as directed by their parsing. The same parser is used for both. In addition to the inverse λ-calculus operators, our system uses a notion of generalization to learn semantic representation of words from the semantic representation of other words that are of the same category. Together with this, we use an existing statistical learning approach to assign weights to deal with multiple meanings of words. Our system produces improved results on standard corpora on natural language interfaces for robot command and control and database queries. 
This paper presents a model to compose semantic relations. The model is independent of any particular set of relations and uses an extended deﬁnition for semantic relations. This extended definition includes restrictions on the domain and range of relations and utilizes semantic primitives to characterize them. Primitives capture elementary properties between the arguments of a relation. An algebra for composing semantic primitives is used to automatically identify the resulting relation of composing a pair of compatible relations. Inference axioms are obtained. Axioms take as input a pair of semantic relations and output a new, previously ignored relation. The usefulness of this proposed model is shown using PropBank relations. Eight inference axioms are obtained and their accuracy and productivity are evaluated. The model offers an unsupervised way of accurately extracting additional semantics from text.  
Abduction is a method for ﬁnding the best explanation for observations. Arguably the most advanced approach to abduction, especially for natural language processing, is weighted abduction, which uses logical formulas with costs to guide inference. But it has no clear probabilistic semantics. In this paper we propose an approach that implements weighted abduction in Markov logic, which uses weighted ﬁrst-order formulas to represent probabilistic knowledge, pointing toward a sound probabilistic semantics for weighted abduction. Application to a series of challenge problems shows the power and coverage of our approach. 
Taking an asynchronous perspective on the syntax-semantics interface, we propose to use modular graph rewriting systems as the model of computation. We formally deﬁne them and demonstrate their use with a set of modules which produce underspeciﬁed semantic representations from a syntactic dependency graph. We experimentally validate this approach on a set of sentences. The results open the way for the production of underspeciﬁed semantic dependency structures from corpora annotated with syntactic dependencies and, more generally, for a broader use of modular rewriting systems for computational linguistics. Introduction The aim of our work is to produce a semantic representation of sentences on a large scale using a formal and exact approach based on linguistic knowledge. In this perspective, the design of the syntax-semantics interface is crucial. Based on the compositionality principle, most models of the syntax-semantics interface use a synchronous approach: the semantic representation of a sentence is built step by step in parallel with its syntactic structure. According to the choice of the syntactic formalism, this approach is implemented in different ways: in a Context-Free Grammars (CFG) style framework, every syntactic rule of a grammar is associated with a semantic composition rule, as in the classical textbook by Heim and Kratzer (1998); following the principles introduced by Montague, Categorial Grammars use an homomorphism from the syntax to the semantics (Carpenter (1992)). HPSG integrates the semantic and syntactic representations in feature structures which combine by uniﬁcation (Copestake et al. (2005)). LFG follows a similar principle (Dalrymple (2001)). In a synchronous approach, the syntax-semantics interface closely depends on the grammatical formalism. Building such an interface can be very costly, especially if we aim at a large coverage for the grammar. In our work, we have chosen an asynchronous approach in the sense that we start from a given syntactic analysis of a sentence to produce a semantic representation. With respect to the synchronous approach, a drawback is that the reaction of the semantics on the syntax is delayed. On the other hand, the computation of the semantics is made relatively independent from the syntactic formalism. The only constraint is the shape of the output of the syntactic analysis. In the formalisms mentioned above, the syntactic structure most often takes the form of a phrase structure, but the choice of constituency for the syntax makes the relationship with the semantics more complicated. We have chosen dependency graphs, because syntactic dependencies are closely related to predicate-argument relations. Moreover, they can be enriched with relations derived from the syntax, which are usually ignored, such as the arguments of inﬁnitives or the anaphora determined by the syntax. One may observe that our syntactic representation of sentences involves plain graphs and not trees. Indeed, these relations can give rise to multiple governors and dependency cycles. On the semantic side, 65  we have also chosen graphs, which are widely used in different formalisms and theories, such as DMRS (Copestake (2009)) or MTT (Mel’cˇuk (1988)) . The principles being ﬁxed, our problem was then to choose a model of computation well suited to transforming syntactic graphs into semantic graphs. The λ-calculus, which is widely used in formal semantics, is not a good candidate because it is appropriate for computing on trees but not on graphs. Our choice naturally went to graph rewriting. Graph rewriting is barely used in computational linguistics; it could be due to the difﬁculty to manage large sets of rules. Among the pioneers in the use of graph rewriting, we mention Hyvo¨nen (1984); Bohnet and Wanner (2001); Crouch (2005); Jijkoun and de Rijke (2007); Be´daride and Gardent (2009); Chaumartin and Kahane (2010). A graph rewriting system is deﬁned as a set of graph rewrite rules and a computation is a sequence of rewrite rule applications to a given graph. The application of a rule is triggered via a mechanism of pattern matching, hence a sub-graph is isolated from its context and the result is a local modiﬁcation of the input. This allows a linguistic phenomenon to be easily isolated for applying a transformation. Since each step of computation is ﬁred by some local conditions in the whole graph, it is well known that one has no grip on the sequence of rewriting steps. The more rules, the more interaction between rules, and the consistency of the whole rule system becomes difﬁcult to maintain. This bothers our ambition of a large coverage for the grammar. To solve this problem, we propose to organize rules in modules. A module is a set of rules that is linguistically consistent and represents a particular step of the transformation. For instance, in our proposal, there is a module transforming the syntactic arguments of verbs, predicative nouns and adjectives into their semantic arguments. Another module resolves the anaphoric links which are internal to the sentence and determined by the syntax. From a computational point of view, the grouping of a small number of rules inside a module allows some optimizations in their application, thus leading to efﬁciency. For instance, the conﬂuence of rewriting is a critical feature — one computes only one normal form, not all of them — for the performance of the program. Since the underlying relation from syntax to semantics is not functional but relational, the system cannot be globally conﬂuent. Then, it is particularly interesting to isolate subsets of conﬂuent rules. Second point, with a small number of rules, one gets much more control on their output. In particular, it is possible to automatically infer some invariant properties of graphs along the computation within a particular module. Thus, it simpliﬁes the writing of the rules for the next modules. It is also possible to plan a strategy in the global evaluation process. It is well known that syntactic parsers produce outputs in various formats. As a by-product of our approach, we show that the choice of the input format (that is the syntax) seems to be of low importance overall. Indeed, as far as two formats contain the same linguistic information with different representations, a system of rewrite rules can be designed to transform any graph from one format to another as a preliminary step. The same remark holds for the output formats. To illustrate our proposal, we have chosen the Paris7 TreeBank (hereafter P7TB) dependency format deﬁned by Candito et al. (2010) as the syntactic input format and the Dependency MRS format (hereafter DMRS) deﬁned by Copestake (2009) as the semantic output format. We chose those two formats because the information they represent, if it is not complete, is relatively consensual and because both draw on large scale experiments: statistical dependency parsing for French1 on the one hand and the DELPH-IN project2 on the other hand. Actually, in our experiments, since we do not have an appropriate corpus annotated according to the P7TB standard, we used our syntactic parser LEOPAR3 whose outputs differ from this standard and we designed a rewriting system to go from one format to the other. The paper is organized as follows. In section 1, we deﬁne our graph rewriting calculus, the β-calculus. In Section 2, we describe the particular rewriting system that is used to transform graphs from the syntactic P7TB format into the DMRS semantic format. In Section 3, we present experimental results on a test suite of sentences. 1http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html 2http://www.delph-in.net/ 3http://leopar.loria.fr 66  
In three experiments, we investigated the computational complexity of German reciprocal sentences with different quantiﬁcational antecedents. Building upon the tractable cognition thesis (van Rooij, 2008) and its application to the veriﬁcation of quantiﬁers (Szymanik, 2010) we predicted complexity differences among these sentences. Reciprocals with all-antecedents are expected to preferably receive a strong interpretation (Dalrymple et al., 1998), but reciprocals with proportional or numerical quantiﬁer antecedents should be interpreted weakly. Experiment 1, where participants completed pictures according to their preferred interpretation, provides evidence for these predictions. Experiment 2 was a picture veriﬁcation task. The results show that the strong interpretation was in fact possible for tractable all but one-reciprocals, but not for exactly n. The last experiment manipulated monotonicity of the quantiﬁer antecedents. Formal semantics hasn’t paid much attention to issues of computational complexity when the meaning of an expression is derived. However, when it comes to semantic processing in humans (and computers) with limited processing resources, computational tractability becomes one of the most important constraints a cognitively realistic semantics must face. Two consequences come to mind immediately. If there is a choice between algorithms, we should choose tractable ones over intractable ones. And secondly, meanings which cannot be effectively computed shouldn’t be posited for natural language expressions. In this paper we present three psycholinguistic experiments investigating the latter aspect. Following traditions in computer science, a number of cognitive scientists have deﬁned computational tractability as polynomial-time-computability (for an overview see van Rooij, 2008) leading to the P-Cognition Hypothesis (PCH): cognitive capacities are limited to those functions that can be computed in polynomial time. These functions are input-output functions in the sense of Marr (1982)’s ﬁrst level. One objection against the PCH is that computational complexity is deﬁned in terms of limit behavior as the input increases. In practice, however, the input may be rather small. van Rooij (2008) points out that the input size can be parametrized turning a problem that is intractable for a large input size into a tractable one for small inputs. We manipulated the input size in an experiment to test this more reﬁned version of the PCH. An interesting test case for the PCH are quantiﬁed sentences containing reciprocal expressions of the form Q of the As R each other. Consider (1-a) – (1-c). 75  (1) a. Most of the dots are connected to each other. b. Four of the dots are connected to each other. c. All dots are connected to each other.  It has been commonly observed that such sentences are highly ambiguous (see eg. Dalrymple et al.,  1998). For instance, under its logically strongest interpretation (1-a) is true iff given n dots there is a  subset  of  more  than  n 2  dots  which  
Entailment pairs are sentence pairs of a premise and a hypothesis, where the premise textually entails the hypothesis. Such sentence pairs are important for the development of Textual Entailment systems. In this paper, we take a closer look at a prominent strategy for their automatic acquisition from newspaper corpora, pairing ﬁrst sentences of articles with their titles. We propose a simple logistic regression model that incorporates and extends this heuristic and investigate its robustness across three languages and three domains. We manage to identify two predictors which predict entailment pairs with a fairly high accuracy across all languages. However, we ﬁnd that robustness across domains within a language is more difﬁcult to achieve. 
First-order logic provides a powerful and ﬂexible mechanism for representing natural language semantics. However, it is an open question of how best to integrate it with uncertain, probabilistic knowledge, for example regarding word meaning. This paper describes the ﬁrst steps of an approach to recasting ﬁrst-order semantics into the probabilistic models that are part of Statistical Relational AI. Speciﬁcally, we show how Discourse Representation Structures can be combined with distributional models for word meaning inside a Markov Logic Network and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context. 
This paper presents a novel approach to semantic role annotation implementing an entailmentbased view of the concept of semantic role. I propose to represent arguments of predicates with grammatically relevant primitive properties entailed by the semantics of predicates. Such meaning components generalise over a range of semantic relations which humans tend to express systematically through language. In a preliminary study, I show that we can model linguistic knowledge at a general, principled syntax-semantics interface by incorporating a layer of skeletal, entailment-based representation of word meaning in large-scale corpus annotation. 
Coecke, Sadrzadeh, and Clark [3] developed a compositional model of meaning for distributional semantics, in which each word in a sentence has a meaning vector and the distributional meaning of the sentence is a function of the tensor products of the word vectors. Abstractly speaking, this function is the morphism corresponding to the grammatical structure of the sentence in the category of ﬁnite dimensional vector spaces. In this paper, we provide a concrete method for implementing this linear meaning map, by constructing a corpus-based vector space for the type of sentence. Our construction method is based on structured vector spaces whereby meaning vectors of all sentences, regardless of their grammatical structure, live in the same vector space. Our proposed sentence space is the tensor product of two noun spaces, in which the basis vectors are pairs of words each augmented with a grammatical role. This enables us to compare meanings of sentences by simply taking the inner product of their vectors. 
This article introduces and evaluates an approach to semantic compositionality in computational linguistics based on the combination of Distributional Semantics and supervised Machine Learning. In brief, distributional semantic spaces containing representations for complex constructions such as Adjective-Noun and Verb-Noun pairs, as well as for their constituent parts, are built. These representations are then used as feature vectors in a supervised learning model using multivariate multiple regression. In particular, the distributional semantic representations of the constituents are used to predict those of the complex structures. This approach outperforms the rivals in a series of experiments with Adjective-Noun pairs extracted from the BNC. In a second experimental setting based on Verb-Noun pairs, a comparatively much lower performance was obtained by all the models; however, the proposed approach gives the best results in combination with a Random Indexing semantic space. 
{agusev,nc,pranavkh,divyeraj,bethard,jurafsky}@cs.stanford.edu Abstract We present the ﬁrst approach to learning the durations of events without annotated training data, employing web query patterns to infer duration distributions. For example, we learn that “war” lasts years or decades, while “look” lasts seconds or minutes. Learning aspectual information is an important goal for computational semantics and duration information may help enable rich document understanding. We ﬁrst describe and improve a supervised baseline that relies on event duration annotations. We then show how web queries for linguistic patterns can help learn the duration of events without labeled data, producing ﬁne-grained duration judgments that surpass the supervised system. We evaluate on the TimeBank duration corpus, and also investigate how an event’s participants (arguments) effect its duration using a corpus collected through Amazon’s Mechanical Turk. We make available a new database of events and their duration distributions for use in research involving the temporal and aspectual properties of events. 
This paper proposes a framework for representing cross-lingual/interlingual lexical semantic correspondences that are expected to be recovered through a series of on-demand/on-the-ﬂy invocations of a lexical semantic matching process. One of the central notions of the proposed framework is a pseudo synset, which is introduced to represent a cross-lingual/multilingual lexical concept, jointly denoted by word senses in more than one language. Another important ingredient of the proposed framework is a framework for semantifying bilingual lexical resource entries. This is a necessary substep when associating and representing corresponding lexical concepts in different languages by using bilingual lexical resources. Based on these devices, this paper further discusses possible extensions to the ISO standard lexical markup framework (LMF). These extensions would enable recovered correspondences to be organized as a dynamic secondary language resource, while keeping the existing primary language resources intact. 
This paper argues that all subject noun phrases can be given a quantiﬁed formalisation in terms of the intersection between their denotation set and the denotation set of their verbal predicate. The majority of subject noun phrases, however, are only implicitely quantiﬁed and the task of retrieving the most plausible quantiﬁer for a given NP is non-trivial. We propose a formalisation which captures the underspeciﬁcation of the quantiﬁer in subject NPs and we show that this formalisation is widely applicable, including in statements involving kinds. We then present a baseline for a quantiﬁcation resolution system using syntactic features as basis for classiﬁcation. Although the syntactic baseline provides a respectable 78% precision, our error analysis shows that obtaining true performance on the task requires information beyond syntax. 
We present the results of several machine learning tasks that exploit explicit spatial language to classify rhetorical relations and the spatial information of narrative events. Three corpora are annotated with ﬁgure and ground (granularity) relationships, mereotopologically classiﬁed verbs and prepositions, and frames of reference. For rhetorical relations, Na¨ıve Bayesian models achieve 84.90% and 57.87% accuracy in classifying NARRATION and BACKGROUND / ELABORATION relations respectively (16% and 23% above baseline). For the spatial information of narrative events, K* models achieve 55.68% average accuracy (12% above baseline) for all spatial information types. This result is boosted to 71.85% (28% above baseline) when inertial spatial reference and text sequence information are considered. Overall, spatial information is shown to be central to narrative discourse structure and prediction tasks. 
Measures of similarity have traditionally focused on computing the semantic relatedness between pairs of words and texts. In this paper, we construct an evaluation framework to quantify cross-modal semantic relationships that exist between arbitrary pairs of words and images. We study the effectiveness of a corpus-based approach to automatically derive the semantic relatedness between words and images, and perform empirical evaluations by measuring its correlation with human annotators. 
We propose a method to automatically align WordNet synsets and Wikipedia articles to obtain a sense inventory of higher coverage and quality. For each WordNet synset, we ﬁrst extract a set of Wikipedia articles as alignment candidates; in a second step, we determine which article (if any) is a valid alignment, i.e. is about the same sense or concept. In this paper, we go signiﬁcantly beyond stateof-the-art word overlap approaches, and apply a threshold-based Personalized PageRank method for the disambiguation step. We show that WordNet synsets can be aligned to Wikipedia articles with a performance of up to 0.78 F1-Measure based on a comprehensive, well-balanced reference dataset consisting of 1,815 manually annotated sense alignment candidates. The fully-aligned resource as well as the reference dataset is publicly available.1 
In the Recognizing Textual Entailment (RTE) task, sentence pairs are classiﬁed into one of three semantic relations: ENTAILMENT, CONTRADICTION or UNKNOWN. While we ﬁnd some sentence pairs hold full entailments or contradictions, there are a number of pairs that partially entail or contradict one another depending on a speciﬁc situation. These partial contradiction sentence pairs contain useful information for opinion mining and other such tasks, but it is difﬁcult for Internet users to access this knowledge because current frameworks do not differentiate between full contradictions and partial contradictions. In this paper, under current approaches to semantic relation recognition, we deﬁne a new semantic relation known as CONFINEMENT in order to recognize this useful information. This information is classiﬁed as either CONTRADICTION or ENTAILMENT. We provide a series of semantic templates to recognize CONFINEMENT relations in Web texts, and then implement a system for recognizing CONFINEMENT between sentence pairs. We show that our proposed system can obtains a F-score of 61% for recognizing CONFINEMENT in Japanese-language Web texts, and it outperforms a baseline which does not use a manually compiled list of lexico-syntactic patterns to instantiate the semantic templates. 
This paper presents a discourse processing framework based on weighted abduction. We elaborate on ideas described in Hobbs et al. (1993) and implement the abductive inference procedure in a system called Mini-TACITUS. Particular attention is paid to constructing a large and reliable knowledge base for supporting inferences. For this purpose we exploit such lexical-semantic resources as WordNet and FrameNet. We test the proposed procedure and the obtained knowledge base on the Recognizing Textual Entailment task using the data sets from the RTE-2 challenge for evaluation. In addition, we provide an evaluation of the semantic role labeling produced by the system taking the Frame-Annotated Corpus for Textual Entailment as a gold standard. 
This paper presents a machine learning-based approach to the incremental understanding of dialogue utterances, with a focus on the recognition of their communicative functions. A token-based approach combining the use of local classiﬁers, which exploit local utterance features, and global classiﬁers which use the outputs of local classiﬁers applied to previous and subsequent tokens, is shown to result in excellent dialogue act recognition scores for unsegmented spoken dialogue. This can be seen as a signiﬁcant step forward towards the development of fully incremental, on-line methods for computing the meaning of utterances in spoken dialogue. 
We use data from a virtual world game for automated learning of words and grammatical constructions and their meanings. The language data are an integral part of the social interaction in the game and consist of chat dialogue, which is only constrained by the cultural context, as set by the nature of the provided virtual environment. Building on previous work, where we extracted a vocabulary for concrete objects in the game by making use of the non-linguistic context, we now target NP/DP grammar, in particular determiners. We assume that we have captured the meanings of a set of determiners if we can predict which determiner will be used in a particular context. To this end we train a classiﬁer that predicts the choice of a determiner on the basis of features from the linguistic and non-linguistic context. 
We consider the problem of distinguishing polysemous from homonymous nouns. This distinction is often taken for granted, but is seldom operationalized in the shape of an empirical model. We present a ﬁrst step towards such a model, based on WordNet augmented with ontological classes provided by CoreLex. This model provides a polysemy index for each noun which (a), accurately distinguishes between polysemy and homonymy; (b), supports the analysis that polysemy can be grounded in the frequency of the meaning shifts shown by nouns; and (c), improves a regression model that predicts when the “one-sense-per-discourse” hypothesis fails. 
WordNet is extensively used as a major lexical resource in NLP. However, its quality is far from perfect, and this alters the results of applications using it. We propose here to complement previous efforts for “cleaning up” the top-level of its taxonomy with semi-automatic methods based on the detection of errors at the lower levels. The methods we propose test the coherence of two sources of knowledge, exploiting ontological principles and semantic constraints. 
Distributed models of semantics assume that word meanings can be discovered from “the company they keep.” Many such approaches learn semantics from large corpora, with each document considered to be unstructured bags of words, ignoring syntax and compositionality within a document. In contrast, this paper proposes a structured vectorial semantic framework, in which semantic vectors are deﬁned and composed in syntactic context. As such, syntax and semantics are fully interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse. Evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy.  
This paper reports on an exploratory investigation as to whether classes of Urdu N-V complex predicates can be identiﬁed on the basis syntactic patterns and lexical choices associated with the N-V complex predicates. Working with data from a POS annotated corpus, we show that choices with respect to the number of arguments, case marking on subjects and which light verbs are felicitous with which nouns depend heavily on the semantics of the noun in the N-V complex predicate. This initial work represents an important step towards identifying semantic criteria relevant for complex predicate formation. Identifying the semantic criteria and being able to systematically code them in turn represents a ﬁrst step towards building up a lexical resource for nouns as part of developing natural language processing tools for the underresourced South Asian language Urdu. 
In this paper we describe DISCUSS, a dialogue move taxonomy layered over semantic representations. We designed this scheme to enable development of computational models of tutorial dialogues and to provide an intermediate representation suitable for question and tutorial act generation. As such, DISCUSS captures semantic and pragmatic elements across four dimensions: Dialogue Act, Rhetorical Form, Predicate Type, Semantic Roles. Together these dimensions provide a summary of an utterance’s propositional content and how it may change the underlying information state of the conversation. This taxonomy builds on previous work in both general dialogue act taxonomies as well as work in tutorial act and tutorial question categorization. The types and values found within our taxonomy are based on preliminary observations and on-going annotation from our corpus of multimodal tutorial dialogues for elementary school science education. 
Semantic change has mostly been studied by historical linguists and typically at the scale of centuries. Here we study semantic change at a ﬁner-grained level, the decade, making use of recent newspaper corpora. We detect semantic change candidates by observing context shifts which can be triggered by topic salience or may be independent from it. To discriminate these phenomena with accuracy, we combine variation ﬁlters with a series of indices which enable building a coherent and ﬂexible semantic change detection model. The indices include widely adaptable tools such as frequency counts, co-occurrence patterns and networks, ranks, as well as model-speciﬁc items such as a variability and cohesion measure and graphical representations. The research uses ACOM, a co-occurrence based geometrical model, which is an extension of the Semantic Atlas. Compared to other models of semantic representation, it allows for extremely detailed analysis and provides insight as to how connotational drift processes unfold. 
In the Textual Entailment community, a shared effort towards a deeper understanding of the core phenomena involved in textual inference is recently arose. To analyse how the common intuition that decomposing TE would allow a better comprehension of the problem from both a linguistic and a computational viewpoint, we propose a deﬁnition for strong component-based TE, where each component is in itself a complete TE system, able to address a TE task on a speciﬁc phenomenon in isolation. We review the literature according to our deﬁnition, trying to position relevant work as more or less close to our idea of strong component-based TE. Several dimensions of the problem are discussed: i) the implementation of system components to address speciﬁc inference types, ii) the analysis of the phenomena relevant to component-based TE, and iii) the development of evaluation methodologies to assess TE systems capabilities to address single phenomena in a pair. 
The question of how to compose meaning in distributional representations of meaning has recently been recognised as a central issue in computational linguistics. In this paper we describe three general and powerful tools that can be used to describe composition in distributional semantics: quotient algebras, learning of ﬁnite dimensional algebras, and the construction of algebras from semigroups. 
Question classiﬁers are used within Question Answering to predict the expected answer type for a given question. This paper describes the ﬁrst steps towards applying a similar methodology to identifying question classes in dialogue contexts, beginning with a study of questions drawn from the Enron email corpus. Human-annotated data is used as a gold standard for assessing the output from an existing, open-source question classiﬁer (QA-SYS). Problem areas are identiﬁed and potential solutions discussed.  
The paper presents an ongoing research that aims at OWL ontology authoring and verbalization using a deterministic controlled natural language (CNL) that would be as natural and intuitive as possible. Moreover, we focus on a multilingual CNL interface to OWL by considering both highly analytical and highly synthetic languages (namely, English and Latvian). We propose a ﬂexible twolevel translation approach that is enabled by the Grammatical Framework and that has allowed us to develop a more natural, but still predictable multilingual CNL on top of the widely used Attempto Controlled English (its subset for OWL, ACE-OWL). This has also allowed us to exploit the readily available ACE parser and verbalizer not only for the modiﬁed and extended version of ACE-OWL, but also for the corresponding controlled Latvian. 
In this paper, we describe the Baseball Announcers’ Language Linked with General Annotation of Meaningful Events (BALLGAME) project – a text corpus for research in computional semantics. We collected pitch-by-pitch event data for a sample of baseball games and used this data to build an annotated corpus composed of transcripts of radio broadcasts of these games. Our annotation links text from the broadcast to events in a formal representation of the semantics of the baseball game. We describe our corpus model, the annotation tool used to create the corpus, and conclude by discussing applications of this corpus in semantics research and natural language processing. 
Abstract In this paper we present some features of an architecture for the translation (Italian – Italian Sign Language) that performs syntactic analysis, semantic interpretation and generation. Such architecture relies on an ontology that has been used to encode the domain of weather forecasts as well as information on language as part of the world knowledge. We present some general issues of the ontological semantic interpretation and discuss the analysis of ordinal numbers. 
In this paper we summarize existing work on the recently introduced task of processing the scope of negation and modality cues; we analyse the scope model that existing systems can process, which is mainly the model reﬂected in the annotations of the biomedical corpus on which the systems have been trained; and we point out aspects of the scope ﬁnding task that would be different based on observations from a corpus from a different domain and nature. 
In the effort of building a verb lexicon classifying the most used verbs in Arabic and providing information about their syntax and semantics (Mousser, 2010), the problem of classes over-generation arises because of the overt morphology of Arabic, which codes not only agreement and inﬂection relations but also semantic information related to thematic arity or other semantic information like ”intensity”, ”pretension”, etc. The hierarchical structure of verb classes and the inheritance relation between their subparts expels derived verbs from the main class, although they share most of its properties. In this article we present a way to adapt the verb class approach to a language with a productive (verb) morphology by introducing sibling classes.  
This paper describes recent work on the DynDial project∗ towards incremental semantic interpretation in dialogue. We outline our domain-general grammar-based approach, using a variant of Dynamic Syntax integrated with Type Theory with Records and a Davidsonian event-based semantics. We describe a Java-based implementation of the parser, used within the Jindigo framework to produce an incremental dialogue system capable of handling inherently incremental phenomena such as split utterances, adjuncts, and mid-sentence clariﬁcation requests or backchannels.  
Recent work on evaluativity or sentiment in the language sciences has focused on the contributions that lexical items provide. In this paper, we discuss contextual evaluativity, stance that is inferred from lexical meaning and pragmatic environments. Focusing on assessor-grounding claims like We liked him because he so clearly disliked Margaret Thatcher, we build a corpus and construct a system employing compositional principles of evaluativity calculation to derive that we dislikes Margaret Thatcher. The resulting system has an F-score of 0.90 on our dataset, outperforming reasonable baselines, and indicating the viability of inferencing in the evaluative domain.  
The MultiModal Interface Language formalism (MMIL) has been selected as the High Level Semantic (HLS) formalism for annotating the French MEDIA dialogue corpus. This corpus is composed of human-machine dialogues in the domain of hotel reservation and tourist information. Utterances in dialogues have been previously annotated with a concept-value ﬂat semantics for studying and evaluating spoken language understanding modules in dialogue systems. We are now interested in investigating the use of more complex representations to improve the understanding capability. The MMIL intermediate language is a high level semantic formalism that bears relevant linguistic information, from syntax up to discourse. This representation should increase the expressivity of the current annotation though at the expense of the annotation process complexity. In this paper we present our ﬁrst attempt in deﬁning the annotation guidelines for the HLS annotation of the MEDIA corpus and its effect on the annotation process itself, revealed by annotators’ disagreements due to the different levels of hierarchy and the granularity of the features deﬁned in MMIL. 
Most techniques that calculate the relatedness between two concepts use a semantic network, such as Wikipedia, WordNet, or ConceptNet, to ﬁnd the shortest intermediate pathway between two nodes. These techniques assume that a low number of edges on the shortest pathway indicates conceptual similarity. Although this technique has proven valid in conforming to psychological data, we test the usefulness of additional pathway variables in ConceptNet, such as edge type and user-rated score. Our results show strong evidence for the application of additional pathway variables in calculating semantic similarity. 
We introduce a novel approach to measuring semantic relatedness of terms based on an automatically generated, large-scale semantic network. We present promising ﬁrst results that indicate potential competitiveness with approaches based on manually created resources. 
Discovery in or understanding of a text collection can be viewed from many angles: the text aspect of the data mining paradigm, the discover aspect of the information seeking paradigm, or the text content aspect of visualisation. This talk will view topic models as a technique within these paradigms. Some visualisations will be reviewed, as well as a variety of different topic models, and some of the natural language processing issues involved in working with the models. Finally, some of the non-parametric statistical methods underlying the analysis will be reviewed because they are fascinating as well. 
Since 2008 when we organised the First Australian Computation and Linguistic Olympiad (OzCLO), this high-school competition has become an annual event, with almost 800 participants across Australia in 2011. For the third time, we sent an Australian team to the International Linguistics Olympiad (ILO) and the team came back with one silver medal. In this talk, I will give an overview of OzCLO, ﬁrst presenting the background and history of the competition internationally and in Australia, then explaining how it is organised and run in Australia, and ﬁnally discussing the impact and importance of reaching out to highschool students in our discipline. Short Biography 
Bayesian models are usually learned using batch algorithms that have to iterate multiple times over the full dataset. This is both computationally expensive and, from a cognitive point of view, highly implausible. We present a novel online algorithm for the word segmentation models of Goldwater et al. (2009) which is, to our knowledge, the ﬁrst published version of a Particle Filter for this kind of model. Also, in contrast to other proposed algorithms, it comes with a theoretical guarantee of optimality if the number of particles goes to inﬁnity. While this is, of course, a theoretical point, a ﬁrst experimental evaluation of our algorithm shows that, as predicted, its performance improves with the use of more particles, and that it performs competitively with other online learners proposed in Pearl et al. (2011).1 
This paper introduces tree transducers as a unifying theory for semantic parsing models based on tree transformations. Many existing models use tree transformations, but implement specialized training and smoothing methods, which makes it difﬁcult to modify or extend the models. By connecting to the rich literature on tree automata, we show how semantic parsing models can be developed using completely general estimation methods. We demonstrate the approach by reframing and extending one state-of-the-art model as a tree automaton. Using a variant of the inside-outside algorithm with variational Bayesian estimation, our generative model achieves higher raw accuracy than existing generative and discriminative approaches on a standard data set. 
This paper examines the ways in which parallelism can be used to speed the parsing of dense PCFGs. We focus on two kinds of parallelism here: Symmetric Multi-Processing (SMP) parallelism on shared-memory multicore CPUs, and Single-Instruction MultipleThread (SIMT) parallelism on GPUs. We describe how to achieve speed-ups over an already very efﬁcient baseline parser using both kinds of technology. For our dense PCFG parsing task we obtained a 60×speed-up using SMP and SSE parallelism coupled with a cache-sensitive algorithm design, parsing section 24 of the Penn WSJ treebank in a little over 2 secs. 
The N400 is a human neuroelectric response to semantic incongruity in on-line sentence processing, and implausibility in context has been identiﬁed as one of the factors that inﬂuence the size of the N400. In this paper we investigate whether predictors derived from Latent Semantic Analysis, language models, and Roark’s parser are signiﬁcant in modeling of the N400m (the neuromagnetic version of the N400). We also investigate signiﬁcance of a novel pairwise-priming language model based on the IBM Model 1 translation model. Our experiments show that all the predictors are signiﬁcant. Moreover, we show that predictors based on the 4-gram language model and the pairwise-priming language model are highly correlated with the manual annotation of contextual plausibility, suggesting that these predictors are capable of playing the same role as the manual annotations in prediction of the N400m response. We also show that the proposed predictors can be grouped into two clusters of signiﬁcant predictors, suggesting that each cluster is capturing a different characteristic of the N400m response. 
Due to its convenience and low–cost, short message service (SMS) has been a very popular medium for communication for quite some time. Unfortunately, however, SMS messages are sometimes used in illicit acts, such as communication between drug dealers and buyers, extortion, fraud, scam, hoax, false reports of terrorist threats, and many more. This study is a forensic study on the authorship classiﬁcation of SMS messages in the Likelihood Ration (LR) framework with the N–gram modelling technique. The aims of this study are to investigate 1) how accurately it is possible to classify the authors of SMS messages; 2) what degree of strength of evidence (LR) can be obtained from SMS messages and 3) how the classiﬁcation performance and the LRs are affected by the sample size for modelling. The resultant LRs are calibrated by means of the logistic regress calibration technique. The results of the classiﬁcation tests will be rigorously assessed from different angles, using the techniques proposed for automatic speaker recognition and forensic voice comparison. 
Automatically building domain-speciﬁc ontologies is a highly challenging task as it requires extracting domain-speciﬁc terms from a corpus and assigning them relevant domain concept labels. In this paper, we focus on the second task: i.e., assigning domain concepts to domain-speciﬁc terms. Motivated by previous approaches in related research (such as word sense disambiguation (WSD) and named entity recognition (NER)) that use semantic similarity among domain concepts, we explore three types of features — contextual, domain concepts, topics — to measure the semantic similarity of terms; we then assign the domain concepts from the best matching terms. As evaluation, we collected domainspeciﬁc terms from FOLDOC, a freely available on-line dictionary for the the Computing domain, and deﬁned 9 domain concepts for this domain. Our results show that beyond contextual features, using domain concepts and topics derived from domain-speciﬁc terms helps to improve assigning domain concepts to the terms. 
We apply the graph-structured stack (GSS) to shift-reduce parsing in a Combinatory Categorial Grammar (CCG) parser. This allows the shift-reduce parser to explore all possible parses in polynomial time without resorting to heuristics, such as beam search. The GSSbased shift-reduce parser is 34% slower than CKY in the ﬁnely-tuned C&C parser. We perform frontier pruning on the GSS, increasing the parsing speed to be competitive with the C&C parser with a small accuracy penalty. 
Web user forums are valuable means for users to resolve speciﬁc information needs, both interactively for participants and statically for users who search/browse over historical thread data. However, the complex structure of forum threads can make it difﬁcult for users to extract relevant information. Thread linking structure has the potential to help tasks such as information retrieval (IR) and threading visualisation of forums, thereby improving information access. Unfortunately, thread linking structure is not always available in forums. This paper proposes an unsupervised approach to predict forum thread linking structure using lexical chaining, a technique which identiﬁes lists of related word tokens within a given discourse. Three lexical chaining algorithms, including one that only uses statistical associations between words, are experimented with. Preliminary experiments lead to results which surpass an informed baseline. 
In this paper we introduce some of the key NLP-related problems related to the practice of Evidence Based Medicine and propose the task of multi-document query-focused summarisation as a key approach to solve these problems. We have completed a corpus for the development of such multi-document queryfocused summarisation task. The process to build the corpus combined the use of automated extraction of text, manual annotation, and crowdsourcing to ﬁnd the reference IDs. We perform a statistical analysis of the corpus for the particular use of single-document summarisation and show that there is still a lot of room for improvement from the current baselines. 
In a collocation, the choice of one lexical item depends on the choice made for another. This poses a problem for simple approaches to lexicalisation in natural language generation systems. In the Meaning-Text framework, recurrent patterns of collocations have been characterised by lexical functions, which offer an elegant way of describing these relationships. Previous work has shown that using lexical functions in the context of multilingual natural language generation allows for a more efﬁcient development of linguistic resources. We propose a way to encode lexical functions in the Lexical Functional Grammar framework. 
A medical publication may or may not present an outcome. When an outcome is present, its polarity may be positive, negative or neutral. Information about the polarity of an outcome is a vital one, particularly for practitioners who use the outcome information for decision making. We model the problem of automatic outcome polarity identiﬁcation as a three-way document classiﬁcation problem and attempt to solve it via supervised machine learning. We combine domain knowledge and linguistic features of medical text, and apply natural language processing to extract features for the chosen classiﬁers. We introduce two novel features — Relative Average Negation Count and Sentence Signature — and show that they are effective in improving classiﬁcation accuracy. We also include features, such as n-grams and semantic orientation of terms, that have been used for similar text classiﬁcation problems in other domains. Using these features, we obtain a maximum accuracy of 74.9% for the classiﬁcation problem. Our experiments suggest that through careful feature selection, machine learning can be used to solve this problem. 
Native language identiﬁcation (NLI) is the task of determining the native language of an author writing in a second language. Several pieces of earlier work have found that features such as function words, part-of-speech n-grams and syntactic structure are helpful in NLI, perhaps representing characteristic errors of different native language speakers. This paper looks at the idea of using Latent Dirichlet Allocation as a feature clustering technique over lexical features to see whether there is any evidence that these smaller-scale features do cluster into more coherent latent factors, and investigates their effect in a classiﬁcation task. We ﬁnd that although (not unexpectedly) classiﬁcation accuracy decreases, there is some evidence of coherent clustering, which could help with much larger syntactic feature spaces. 
This paper describes a supervised algorithm for diacritic restoration based on naive Bayes classifiers that act at wordlevel. Classifications are based on a rich set of features, extracted automatically from training data in the form of diacritically marked text. The method requires no additional resources, which makes it language independent. The algorithm was evaluated on one language, namely Māori and an accuracy exceeding 99% was observed. 
This paper presents an experimental study on the interpretation of the complement anaphora the others in inter-sentential discourse. It aims to offer an answer to the following two empirical questions. First, how complement anaphora denote the “complement set”, a set of referents that includes those referents not denoted by the matching anaphoric antecedent. Second, what are the exact interpretation principles that govern the anaphoric potential of complement anaphora. The answers to these two questions shed light on how complement anaphora ﬁt into a broader theory of anaphora resolution, and what is the most accurate logical and psychological model of this aspect of grammar. 
This paper presents an experimental study on the interpretation of plural pronoun they in discourse, and offers an answer to two questions. The ﬁrst question is whether the anaphoric interpretation of they corresponds to that of its antecedent NP(maximal interpretation), or by the “whole” previous sentence (reference interpretation). The second question is whether speakers may access only one interpretation or both, although at different “moments” in discourse. The answers to these questions suggest that an accurate logical and psychological model of anaphora resolution includes two principles. A ﬁrst principle ﬁnds a “default” interpretation, a second principle determines when the “alternative” interpretation can (and must) be accessed. 
Providing timely and individualised feedback to students in large undergraduate classes is problematic. In this paper we describe our approach to creating a simple, surface-based, domain-independent natural language tutor which uses simple machine learning techniques as a step towards resolving this issue. The focus of our efforts was on developing a high-quality tutorial dialogue plan, creating well-designed questions, and building a model of student responses derived from real student data. We present some early evaluation results and brieﬂy outline the opportunities that our approach and the new tutorial dialogue system present. 
Detection of child exploitation in Internet chatting is an important issue for the protection of children from prospective online paedophiles. This paper investigates the effectiveness of text classifiers to identify Child Exploitation (CE) in chatting. As the chatting occurs among two or more users by typing texts, the text of chat-messages can be used as the data to be analysed by text classifiers. Therefore the problem of identification of CE chats can be framed as the problem of text classification by categorizing the chatlogs into predefined CE types. Along with three traditional text categorizing techniques a new approach has been made to accomplish the task. Psychometric and categorical information by LIWC (Linguistic Inquiry and Word Count) has been used and improvement of performance in some classifier has been found. For the experiments of current research the chat logs are collected from various websites open to public. Classification-viaRegression, J-48-Decision-Tree and NaïveBayes classifiers are used. Comparison of the performance of the classifiers is shown in the result. 
 There is a rapidly growing body of work in the use of Embodied Conversational Agents (ECA) to convey complex contextual relationships through verbal and non-verbal communication, in domains ranging from military C2 to e-learning. In these applications the subject matter expert is often naïve to the technical requirements of ECAs. ENGAGE (the Extensible Natural Gesture Animation Generation Engine) is designed to automatically generate appropriate and ‘realistic’ animation for ECAs based on the content provided to them. It employs syntactic analysis of the surface text and uses predefined behaviour models to generate appropriate behaviours for the ECA. We discuss the design of this system, its current applications and plans for its future development. 
In this paper, we show how our methods developed for identifying light verb constructions can be adapted to different domains and different types of texts. We both experiment with rule-based methods and machine learning approaches. Our results indicate that existing solutions for detecting light verb constructions can be successfully applied to other domains as well and we conclude that even a little amount of annotated target data can notably contribute to performance if a bigger corpus from another domain is also exploited when training. 
This paper describes a study in the purpose of annotation of event names in French texts. It presents a theoretical study about the notion of Event and deﬁnes the types of event names under study. It then presents related works about Events in NLP. Afterwards, we ﬁrst use manually supervised lexicons that provide lists of nouns representing events, and demonstrate the limitations of lexicons in the task of event recognition. Further experiments are presented to propose an automatic method for building a weighted lexicon of event names.1 
Since the 1990s, the Brown ‘family’ corpora have been widely used for various diachronic studies of 20th century English language. However, the existing methodologies failed to exploit its full potential as they only used the four main text categories. In this paper, we present the results of two experiments on diachronic changes of the Coleman-Liau readability Index (CLI) in British and American English in the period 1961–1991/2. The ﬁrst experiment used all ﬁfteen ﬁne-grained text genres, while the second only used the four main text categories. The comparison of the results of these two experiments demonstrated the importance of using all ﬁfteen ﬁnegrained text genres for obtaining a better understanding of how language changes. 
We present our ﬁndings on projecting part of speech (POS) information from a well resourced language, Farsi, to help tag a lower resourced language, Pashto, following Feldman and Hana (2010). We make a series of modiﬁcations to both tag transition and lexical emission parameter ﬁles generated from a hidden Markov model tagger, TnT, trained on the source language (Farsi). Changes to the emission parameters are immediately effective, whereas changes made to the transition information are most effective when we introduce a custom tagset. We reach our best results of 70.84% when we employ all emission and transition modiﬁcations to the Farsi corpus with the custom tagset. 
This work presents an extension to phrasebased statistical machine translation models which incorporates linguistic knowledge, namely part-of-speech information. Scores are added to the standard phrase table which represent how the phrases correspond to their translations on the partof-speech level. We suggest two different kinds of scores. They are learned from a POS-tagged version of the parallel training corpus. The decoding strategy does not have to be modiﬁed. Our experiments show that our extended models achieve similar BLEU and NIST scores compared to the standard model. Additional manual investigation reveals local improvements in the translation quality. 
In this paper, a system for the extraction of key argument phrases – which make the opinion holder feel negative or positive towards a particular product – from product reviews is introduced. Since the necessary amount of training examples from any arbitrary product type (target domain) is not always available, the possible usage of domain adaptation in the task of opinion phrase extraction is also examined. Experimental results show that models relying on training examples mainly from a different domain can still yield results that are comparable to that of the intra-domain settings. 
The aim of the current work is to see how well existing techniques for textual entailment work when applied to Arabic, and to propose extensions which deal with the speciﬁc problems posed by the language. Arabic has a number of characteristics, described below, which make it particularly challenging to determine the relations between sentences. In particular, the lack of diacritics means that determining which sense of a word is intended in a given context is extremely difﬁcult, since many related senses have the same surface form; and the syntactic ﬂexibility of the language, notably the combination of free word-order, pro-drop subjects, verbless sentences, and compound NPs of various kinds, means that it is also extremely difﬁcult to determine the relationships between words. 
One of the purposes of semantic web is to provide machine understandable content and this can be achieved by annotating information. At the moment, annotations can be created manually and also automatically, both of the approaches having advantages and disadvantages. The goal of this article is to present a new semi-automatic annotation tool, which given a text will annotate words with concepts from an ontology. 
In this paper, we model the corpus-based relation extraction task, namely protein-protein interaction, as a classiﬁcation problem. In that framework, we ﬁrst show that standard machine learning systems exploiting representations simply based on shallow linguistic information can rival state-of-the-art systems that rely on deep linguistic analysis. We also show that it is possible to obtain even more effective systems, still using these easy and reliable pieces of information, if the speciﬁcs of the extraction task and the data are taken into account. Our original method combining lazy learning and language modelling out-performs the existing systems when evaluated on the LLL2005 protein-protein interaction extraction task data1. 
There is no doubt that in the last couple of years corpus-based machine translation (CBMT) approaches have been in focus. Each of the approaches has its advantages and disadvantages. Therefore, hybrid approaches have been developed. This paper presents a comparative study of CBMT approaches, using three types of systems: a statistical MT (SMT) system, an example-based MT (EBMT) system and a hybrid (EBMT-SMT) system. We considered for our experiments three languages, from different language families: Romanian, German and English. Two different types of corpora have been used: while the ﬁrst is manually created, the latter is automatically built. 
Question processing is a key step in Question Answering systems. For this task, it has been shown that a good syntactic analysis of questions helps to improve the results. However, general parsers seem to present some disadvantages in question analysis. We present a speciﬁc tool under development for Spanish question analysis in a QA context: SpQA. SpQA is a parser designed to deal with the special syntactic features of Spanish questions and to cover some needs of question analysis in QA systems such as target identiﬁcation. The system has been evaluated together with three Spanish general parsers. In this comparative evaluation, SpQA shows the best results in Spanish question analysis. 
This paper presents the on-going development of a model of incremental semantics driven natural language generation (NLG) for incremental dialogue systems. The approach is novel in its tight integration of incremental goal-driven semantics and syntactic construction, utilizing Type Theory with Records (TTR) record types for goal concepts as its input and the grammar formalism Dynamic Syntax (DS) for a wordby-word tactical generation procedure. The characterization of generation in terms of semantic input and word output graphs allows an integration into the incremental dialogue system Jindigo and facilitates the generation of human-like self-repairs in a semantically and syntactically motivated way. 
A language-independent method of figure-ofspeech extraction is proposed in order to reinforce rhetoric-oriented considerations in natural language processing studies. The method is based upon a translation of a canonical form of repetition-based figures of speech into the language of PERL-compatible regular expressions. Anadiplosis, anaphora, antimetabole figures were translated into the form exploiting the backreference properties of PERL-compatible regular expression while epiphora was translated into a formula exploiting recursive properties of this very concise artificial language. These four figures alone matched more than 7000 strings when applied on dramatic and poetic corpora written in English, French, German and Latin. Possible usages varying from stylometric evaluation of translation quality of poetic works to more complex problem of semi-supervised figure of speech induction are briefly discussed. 
In this work we research the effect of micro-context on a memory-based learning (MBL) system for word sense disambiguation. We report results achieved on the data set provided by the English Lexical Sample Task introduced in the Senseval 3 competition. Our study revisits the belief that the disambiguation task proﬁts more from a wider context and indicates that in reality system performance is highest when a narrower context is considered. Keywords word sense disambiguation, memory-based learning, supervised learning 
One of the most recent developments in NLP is the emergence of linguistic annotation metasystems which make use of existing processing tools and implement pipelined architecture. In this paper we describe a system that offers a new perspective in exploiting NLP meta-systems by providing a common processing framework. This framework supports most of common NLP tasks by chaining tools that are able to communicate on the basis of common formats. As a demonstration of the effectiveness of the system to manage heterogeneous NLP tools, we developed an English processing chain, pipelining OpenNLP-based and C++ NLP implementations. Furthermore, we conducted experiments to test the stability and measure the performance of the English processing chain. A baseline processing chain for the Bulgarian language illustrates the capabilities of the system to support and manage processing chains for more languages. 
In this paper, we describe how ontologies can be built automatically from deﬁnitions obtained by searching Wikipedia for lexico-syntactic patterns based on the hyponymy relation. First, we describe how deﬁnitions are retrieved and processed while taking into account both recall and precision. Further, concentrating only on precision, we show how a consistent and useful domain ontology can be created with a beneﬁcial precision of 80%. 
In this paper, we evaluate different lexico-syntactic patterns in regard to their usefulness for ontology building. Each pattern is analysed individually to determine its respective probability to return the hy- ponymy relation. We also create different ontolo- gies according to this accuracy criteria to show how it inﬂuences the resulting ontology. Using patterns with a success rate over 80% leads to an approxi- mate accuracy of 77% in the ﬁnal ontology. 
A novel approach to learning metaphors without any prior knowledge is proposed, in which ideas are acquired as concrete concepts and later on develop their abstraction. A grounded model of linguistic concepts and a hierarchical probability map is used to interpret/generate ontological metaphors. 
In this paper we investigate the possibility of creating a PoS tagger for Modern Standard Arabic by integrating open-source tools. In particular a morphological analyser, used in the disambiguation process with a PoS tagger trained on classical Arabic. The investigation shows the scarcity of open-source tools and resources, which complicated the integration process. Among the problems are different input/output formats of each tool, granularity of tag sets and different tokenisation schemes. The ﬁnal prototype of the PoS tagger was trained on classical Arabic and tested on a sample text of modern standard Arabic. The results are not that impressive, only an accuracy of 73% is achieved. This paper however outlines the difﬁculties of integrating tools today and proposes ideas for future work in the ﬁeld and shows that classical Arabic is not sufﬁcient as training data for an Arabic tagger. 
In this paper we present initial work on cross-language word sense disambiguation for translating adjectives from Spanish to Quechua and situate CLWSD as part of the translation task. While there are many available resources for training Spanish-language NLP systems, linguistic resources for Quechua, especially Spanish-Quechua bitext, are quite limited, so some ingenuity is required in developing Spanish-Quechua systems. This work makes use of only freely available resources and compares a few different techniques for CLWSD, including classiﬁers with simple word context features, features from a Spanish-language dependency parser, a multilingual version of the Lesk algorithm, and a distance metric based on the Spanish wordnet. 
The paper presents the annotation of negation and speculation which is important for many NLP applications. Unlike previous research focusing on the medical domain, we investigate the review domain and attempt to annotate the SFU Review Corpus. In order to guarantee consistent annotation, we develop speciﬁc guidelines. Given the lack of research into annotation in the review domain, we explore the possibility of adapting the existing BioScope guidelines for the domain of interest. In order to reveal cases that need additional investigation, initially a small part of the corpus was annotated and this information was used for developing the guidelines. The paper describes the general principles our guidelines are based on and discusses differences with those in BioScope. It discusses the cases which were difﬁcult to annotate. We include some insight into future work in order to improve the annotation process as well. 
Authorship attribution studies consider author's identification of an anonymous text. This is a long history problem with a great number of various approaches. Those ones based on n-grams single out by their performances and good results. A n-gram approach is language independent but the selection of a number n is actually not. The focus of this paper is determination of a set of optimal values for number n for specific task of classification of newspaper articles written in Serbian according to authorship. We combine two different algorithms: the first one is based on counting common n-grams and the another one is based on relative frequency of n-grams. Experimental results are obtained for pairs of n-gram and profile sizes and it can be concluded that for all profile sizes the best results are obtained for 3≤n≤7.  A goal of this paper is to identify authors of anonymous articles from the local daily newspapers using n-gram based algorithm. The articles discuss similar topics, all are written in Serbian and published in the same period of time. The scheme of our algorithm is depicted in Figure 1 and represents a classical profile-based algorithm:  
Knowledge about how the world changes over time is a vital component of commonsense knowledge for Artiﬁcial Intelligence (AI) and natural language understanding. Actions and events are fundamental components to any knowledge about changes in the state of the world: the states before and after an event differ in regular and predictable ways. We describe a novel system that tackles the problem of extracting knowledge from text about how actions and events change the world over time. We leverage standard language-processing tools, like semantic role labelers and coreference resolvers, as well as large-corpus statistics like pointwise mutual information, to identify STRIPS representations of actions and events, a type of representation commonly used in AI planning systems. In experiments on Web text, our extractor’s Area under the Curve (AUC) improves by more than 31% over the closest system from the literature for identifying the preconditions and add effects of actions. In addition, we also extract signiﬁcant aspects of STRIPS representations that are missing from previous work, including delete effects and arguments. 
Event extraction is a particularly challenging type of information extraction (IE) that may require inferences from the whole article. However, most current event extraction systems rely on local information at the phrase or sentence level, and do not consider the article as a whole, thus limiting extraction performance. Moreover, most annotated corpora are artificially enriched to include enough positive samples of the events of interest; event identification on a more balanced collection, such as unfiltered newswire, may perform much worse. In this paper, we investigate the use of unsupervised topic models to extract topic features to improve event extraction both on test data similar to training data, and on more balanced collections. We compare this unsupervised approach to a supervised multi-label text classifier, and show that unsupervised topic modeling can get better results for both collections, and especially for a more balanced collection. We show that the unsupervised topic model can improve trigger, argument and role labeling by 3.5%, 6.9% and 6% respectively on a pre-selected corpus, and by 16.8%, 12.5% and 12.7% on a balanced corpus. 
This paper investigates the application of an existing seed-based minimally supervised learning algorithm to different social domains exhibiting different properties of the available data. A systematic analysis studies the respective data properties of the three domains including the distribution of the semantic arguments and their combinations. The experimental results conﬁrm that data properties have a strong inﬂuence on the performance of the learning system. The main results are insights about: (i) the effects of data properties such as redundancy and frequency of argument mentions on coverage and precision (ii) the positive effects of negative examples if used effectively (iii) the different effects of negative examples depending on the domain data properties and (iv) the potential of reusing rules from one domain for improving the relation extraction performance in another domain. 
Previous work on relation extraction has focussed on identifying relationships between entities that occur in the same sentence (intra-sentential relations) rather than between entities in different sentences (inter-sentential relations) despite previous research having shown that intersentential relations commonly occur in information extraction corpora. This paper describes a SVM-based approach to relation extraction that is applied to both types. Adapted features and techniques for counter-acting bias in SVM models are used to deal with speciﬁc issues that arise in the inter-sentential case. It was found that the structured features used for intrasentential relation extraction can be easily adapted for the inter-sentential case and provides comparable performance. 
Natural language processing tasks often rely on part-of-speech (POS) tagging as a preprocessing step. However it is not clear how the absence of any part-of-speech tagger should hamper the development of other natural language processing tools. In this paper we investigate the contribution of fully unsupervised part-of-speech induction to a common natural language processing task. We focus on the supervised English shallow parsing task and compare systems relying either on POS induction, on POS tagging, or on lexical features only as a baseline. Our experiments on the English CoNLL'2000 dataset show a significant benefit from POS induction over the baseline, with performances close to those obtained with a traditional POS tagger. Results demonstrate a great potential of POS induction for shallow parsing which could be applied to resource-scarce languages. 
Part of speech tagging accuracy deteriorates severely when a tagger is used out of domain. We investigate a fast method for domain adaptation, which provides additional in-domain training data from an unannotated data set by applying POS taggers with different biases to the unannotated data set and then choosing the set of sentences on which the taggers agree. We show that we improve the accuracy of a trigram tagger, TnT, from 85.77% to 86.10%. In order to improve performance on unknown words, we investigate using active learning for learning ambiguity classes of domain speciﬁc words, yielding an accuracy of 89.15% for TnT. 
We experiment with extending the dictionaries used by three open-source partof-speech taggers, by using data from a large Icelandic morphological database. We show that the accuracy of the taggers can be improved signiﬁcantly by using the database. The reason is that the unknown word ratio reduces dramatically when adding data from the database to the taggers’ dictionaries. For the best performing tagger, the overall tagging accuracy increases from the base tagging result of 92.73% to 93.32%, when the unknown word ratio decreases from 6.8% to 1.1%. When we add reliable frequency information to the tag proﬁles for some of the words originating from the database, we are able to increase the accuracy further to 93.48% – this is equivalent to 10.3% error reduction compared to the base tagger. 
The standard ParsEval metrics alone are often not sufﬁcient for evaluating parsers integrated in natural language understanding systems. We propose to augment intrinsic parser evaluations by extrinsic measures in the context of human-robot interaction using a corpus from a human cooperative search task. We compare a constituent with a dependency parser on both intrinsic and extrinsic measures and show that the conversion to semantics is feasible for different syntactic paradigms. 
This paper discusses two Hidden Markov Models (HMM) for linking linguistically motivated XTAG grammar and the automatically extracted LTAG used by MICA parser. The former grammar is a detailed LTAG enriched with feature structures. And the latter one is a huge size LTAG that due to its statistical nature is well suited to be used in statistical approaches. Lack of an eﬃcient parser and sparseness in the supertags set are the main obstacles in using XTAG and MICA grammars respectively. The models were trained by the standard HMM training algorithm, BaumWelch. To converge the training algorithm to a better local optimum, the initial state of the models also were estimated using two semi-supervised EM-based algorithms. The resulting accuracy of the model (about 91%) shows that the models can provide a satisfactory way for linking these grammars to share their capabilities together. 
This paper proposes a method for automatically generating summaries taking into account the information in which users may be interested. Our approach relies on existing model summaries from tourist sites and captures from them the type of information humans use to describe places around the world. Relational patterns are ﬁrst extracted and categorized by the type of information they encode. Then, we apply them to the collection of input documents to automatically extract the most relevant sentences and build the summaries. In order to evaluate the performance of our approach, we conduct two types of evaluation. On the one hand, we use ROUGE to assess the information contained in our summaries against existing human written summaries, whereas on the other hand, we carry out a human readability evaluation. Our results indicate that our approach achieves high performance both in ROUGE and manual evaluation. 
Aggregation is a sub-task of Natural Language Generation (NLG) that improves the conciseness and readability of the text outputted by NLG systems. Till date, approaches towards the aggregation task have been predominantly manual (manual analysis of domain speciﬁc corpus and development of rules). In this paper, a new algorithm for aggregation in NLG is proposed, that learns context sensitive aggregation rules from a parallel corpus of multi-sentential texts and their underlying semantic representations. Additionally, the algorithm accepts external constraints and interacts with the surface realizer to generate the best output. Experiments show that the proposed context sensitive probablistic aggregation algorithm performs better than the deterministic hand crafted aggregation rules. 
In this paper, we present a new hybridisation approach consisting of enriching the phrase table of a phrase-based statistical machine translation system with bilingual phrase pairs matching structural transfer rules and dictionary entries from a shallowtransfer rule-based machine translation system. We have tested this approach on different small parallel corpora scenarios, where pure statistical machine translation systems suffer from data sparseness. The results obtained show an improvement in translation quality, specially when translating out-of-domain texts that are well covered by the shallow-transfer rule-based machine translation system we have used. 
With the increasing demand for fast and accurate audiovisual translation, subtitlers are starting to consider the use of translation technologies to support their work. An important issue that arises from the use of such technologies is measuring how much effort needs to be put in by the subtitler in post-editing (semi-)automatic translations. In this paper we present an objective way of measuring post-editing effort in terms of time. In experiments with English-Portuguese subtitles, we measure the post-editing effort of texts translated using machine translation and translation memory systems. We also contrast this effort against that of translating the texts without any tools. Results show that post-editing is on average 40% faster than translating subtitles from scratch. With our best system, more than 69% of the translations require little or no postediting. 
This paper describes a new, freely available, highly multilingual named entity resource for person and organisation names that has been compiled over seven years of large-scale multilingual news analysis combined with Wikipedia mining, resulting in 205,000 person and organisation names plus about the same number of spelling variants written in over 20 different scripts and in many more languages. This resource, produced as part of the Europe Media Monitor activity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a number of purposes. These include improving name search in databases or on the internet, seeding machine learning systems to learn named entity recognition rules, improve machine translation results, and more. We describe here how this resource was created; we give statistics on its current size; we address the issue of morphological inflection; and we give details regarding its functionality. Updates to this resource will be made available daily. 
We introduce several models for alignment of etymological data, that is, for ﬁnding the best alignment, given a set of etymological data, at the sound or symbol level. This is intended to obtain a means of measuring the quality of the etymological data sets, in terms of their internal consistency. One of our main goals is to devise automatic methods for aligning the data that are as objective as possible, the models make no a priori assumptions—e.g., no preference for vowel-vowel or consonant-consonant alignments. We present a baseline model and several successive improvements, using data from the Uralic language family. 
As developers of a highly multilingual named entity recognition (NER) system, we face an evaluation resource bottleneck problem: we need evaluation data in many languages, the annotation should not be too time-consuming, and the evaluation results across languages should be comparable. We solve the problem by automatically annotating the English version of a multi-parallel corpus and by projecting the annotations into all the other language versions. For the translation of English entities, we use a phrase-based statistical machine translation system as well as a lookup of known names from a multilingual name database. For the projection, we incrementally apply different methods: perfect string matching, perfect consonant signature matching and edit distance similarity. The resulting annotated parallel corpus will be made available for reuse. 
In this paper we present a knowledge-light approach to extract a bilingual lexicon for closely related languages from comparable corpora. While in most related work an existing dictionary is used to translate context vectors, we take advantage of the similarities between languages instead and build a seed lexicon from words that are identical in both languages and then further extend it with context-based cognates and translations of the most frequent words. We also use cognates for reranking translation candidates obtained via context similarity and extract translation equivalents for all content words, not just nouns as in most related work. The results are very encouraging, suggesting that other similar languages could beneﬁt from the same approach. By enlarging the seed lexicon with cognates and translations of the most frequent words and by cognate-based reranking of translation candidates we were able to improve the average baseline precision from 0.592 to 0.797 on the mean reciprocal rank for the ten top-ranking translation candidates for nouns, verbs and adjectives with a 46% recall on the gold standard of 1000 random entries from a traditional dictionary. 
In this work, we analyze sentiments and opinions expressed in user-written Web messages. The messages discuss healthrelated topics: medications, treatment, illness and cure, etc. Recognition of sentiments and opinions is a challenging task for humans as well as an automated text analysis. In this work, we apply both the approaches. The paper presents the annotation model, discusses characteristics of subjectivity annotations in health-related messages, and reports the results of the annotation agreement. For external evaluation of the labeling results, we apply Machine Learning methods on the annotated data and present the obtained results. 
In this paper we consider whether the thematic document clustering approach of Contextual Document Clustering is able to capture the overall sentiment of a cluster of documents. We provide a novel mechanism to determine the sentiment of a cluster based on the latter approach and assess the approach on three data sets formed from the NY Times annotated corpus. We demonstrate that CDC does provide a strong tendency to capture the sentiment of a cluster. 
The fuzziness of Chinese sentence boundary makes discourse analysis more challenging. Moreover, many articles posted on the Internet are even lack of punctuation marks. In this paper, we collect documents written by masters as a reference corpus and propose a model to label the punctuation marks for the given text. Conditional random field (CRF) models trained with the corpus determine the correct delimiter (a comma or a full-stop) between each pair of successive clauses. Different tagging schemes and various features from different linguistic levels are explored. The results show that our segmenter achieves an accuracy of 77.48% for plain text, which is close to the human performance 81.18%. For the rich formatted text, our segmenter achieves an even better accuracy of 82.93%. 
Many annotation schemes for discourse relations allow combinations such as temporal+cause (for events that are temporally and causally related to each other) and temporal+contrast (for contrasts between subsequent time spans, or between events that are temporally coextensive). However, current approaches for the automatic classiﬁcation of discourse relations are limited to producing only one relation and disregard the others. We argue that the information contained in these ‘additional’ relations is indeed useful and present an approach to tag multiple ﬁne-grained discourse relations in ambiguous connectives from the German Tu¨Ba-D/Z corpus. Using a rich feature set, we show that good accuracy is possible even for inferred relations that are not part of the connective’s ‘core’ meaning. 
We investigate how the automatic identiﬁcation of noun compounds and named entities can contribute to keyphrase extraction and we also show how previously identiﬁed noun compounds affect named entity recognition and vice versa, how noun compound detection is supported by identiﬁed named entities. Our experiments demonstrate that already known noun compounds yield better performance in named entity recognition and already known named entities enhance noun compound detection. The integration of noun compound and named entity related features into a keyphrase extractor also proves to be more effective than the model not including them. Our results indicate that the above features tend to be beneﬁcial in several NLP-related tasks. 
We propose a Named Entity (NE) recognition method using rules acquired from unlabeled data. Rules are acquired from automatically labeled data with an NE recognizer. These rules are used to identify NEs, the beginning of NEs, or the end of NEs. The application results of rules are used as features for machine learning based NE recognizers. In addition, we use word information acquired from unlabeled data as in a previous work. The word information includes the candidate NE classes of each word, the candidate NE classes of co-occurring words of each word, and so on. We evaluate our method with IREX data set for Japanese NE recognition and unlabeled data consisting of more than one billion words. The experimental results show that our method using rules and word information achieves the best accuracy on the GENERAL and ARREST tasks of IREX. 
This article explores the portability of a coreference resolver across a variety of eight text genres. Besides newspaper text, we also include administrative texts, autocues, texts used for external communication, instructive texts, wikipedia texts, medical texts and unedited new media texts. Three sets of experiments were conducted. First, we investigated each text genre individually, and studied the effect of larger training set sizes and including genre-speciﬁc training material. Then, we explored the predictive power of each genre for the other genres conducting cross-domain experiments. In a ﬁnal step, we investigated whether excluding genres with less predictive power increases overall performance. For all experiments we use an existing Dutch mention-pair resolver and report on our experimental results using four metrics: MUC, B-cubed, CEAF and BLANC. We show that resolving out-of-domain genres works best when enough training data is included. This effect is further intensiﬁed by including a small amount of genre-speciﬁc text. As far as the cross-domain performance is concerned we see that especially genres of a very speciﬁc nature tend to have less generalization power. 
This paper addresses the problem of multilingual text summarisation. The goal is to analyse three approaches for generating summaries in four languages (English, Spanish, German and French), in order to determine the best one to adopt when tackling this issue. The proposed approaches rely on: i) language-independent techniques; ii) language-speciﬁc resources; and iii) machine translation resources applied to a mono-lingual summariser. The evaluation carried out employing the JRC corpus – a corpus speciﬁcally created for multi-lingual summarisation – shows that the approach which uses languagespeciﬁc resources is the most appropriate in our comparison framework, performing better than state-of-the-art multi-lingual summarisers. Moreover, the readability assessment conducted over the resulting summaries for this approach proves that they are also very competitive with respect to their quality. 
We present and evaluate the ﬁrst method known to us that can create rich nonextract-based opinion summaries from general text (e.g. newspaper articles). We ﬁrst describe two possible representations for opinion summaries and then present our system OASIS, which identiﬁes, and optionally aggregates, ﬁne-grained opinions from the same source on the same topic. We propose new evaluation measures for both types of opinion summary and employ the metrics in an evaluation of OASIS on a standard opinion corpus. Our results are encouraging — OASIS substantially outperforms a competitive baseline when creating document-level aggregate summaries that compute the average polarity value across the multiple opinions identiﬁed for each source about each topic. We further show that as state-ofthe-art performance on ﬁne-grained opinion extraction improves, we can expect to see opinion summaries of very high quality — with F-scores of 54-78% using our OSEM evaluation measure. 
Nowadays, many inﬂuential facts are reported multiple times by different sources and in different languages. This paper presents the results of an experiment on deploying cross-lingual information fusion techniques for reﬁning the results of a large-scale multilingual news event extraction system. An evaluation on a test corpus consisting of 618 event descriptions which refer to 523 real-world events revealed that the description of circa 10% of the events extracted by the mono-lingual systems could be reﬁned. In particular, an overall gain of 6,4% and 4,8% in recall and precision against the best monolingual system could be obtained respectively. 
The ability to accurately determine temporal relations between events is an important task for several natural language processing applications such as Question Answering, Summarization, and Information Extraction. Since current supervised methods require large corpora, which for many languages do not exist, we have focused our attention on approaches with less supervision as much as possible. This paper presents a fully generative model for temporal relation extraction based on the expectation maximization (EM) algorithm. Our experiments show that the performance of the proposed algorithm, regarding its little supervision, is considerable in temporal relation learning. 
We present an approach for Semantic Role Labeling (SRL) using Conditional Random Fields in a joint identiﬁcation/classiﬁcation step. The approach is based on shallow syntactic information (chunks) and a number of lexicalized features such as selectional preferences and automatically inferred similar words, extracted using lexical databases and distributional similarity metrics. We use semantic annotations from the Proposition Bank for training and evaluate the system using CoNLL-2005 test sets. The additional lexical information led to improvements of 15% (in-domain evaluation) and 12% (out-of-domain evaluation) on overall semantic role classiﬁcation in terms of F-measure. The gains come mostly from a better recall, which suggests that the addition of richer lexical information can improve the coverage of existing SRL models even when very little syntactic knowledge is available. 
We present an approach to perform external plagiarism analysis by applying several similarity detection techniques, such as lexical measures and a textual entailment recognition system developed by our research group. Some of the least expensive features of this system are applied to all corpus documents to detect those that are likely to be plagiarized. After this is done, the whole system is applied over this subset of documents to extract the exact n-grams that have been plagiarized, given that we now have less data to process and therefore can use a more complex and costly function. Apart from the application of strictly lexical measures, we also experiment with a textual entailment recognition system to detect plagiarisms with a high level of obfuscation. In addition, we experiment with the application of a spell corrector and a machine translation system to handle misspellings and plagiarisms translated into different languages, respectively. 
This paper describes a hybrid French - Romanian cognate identification module. This module is used by a lexical alignment system. Our cognate identification method uses lemmatized, tagged and sentence-aligned parallel corpora. This method combines statistical techniques, linguistic information (lemmas, POS tags) and orthographic adjustments. We evaluate our cognate identification module and we compare it to other methods using pure statistical techniques. Thus, we study the impact of the used linguistic information and the orthographic adjustments on the results of the cognate identification module and on cognate alignment. Our method obtains the best results in comparison with the other implemented statistical methods. 
In this paper we present an approach to large-scale coreference resolution for an ample set of human languages, with a particular emphasis on time performance and precision. One of the distinctive features of our approach is the use of a mature multilingual named entity repository (persons and organizations) gradually compiled over the past few years. Our experiments show promising results – an overall precision of 94% tested on seven different languages. We also present an extrinsic evaluation on seven languages in the context of summarization where we gauge the contribution of the coreference resolver towards the end summarization performance. 
 We introduce the problem of detecting Entity Instantiations, a type of entity relation in which a set of entities is introduced, and either a member or subset of this set is mentioned afterwards. We perform the ﬁrst, reliable, corpus study of Entity Instantiations, concentrating on intersentential annotation. We then develop the ﬁrst automatic instantiation detector, which incorporates lexical, contextual and world knowledge and shows signiﬁcant improvements over a strong baseline.  
Although several studies have developed models and type hierarchies for named entity annotation, no such resource is available for semantic relation annotation, despite its utility for various applications (e.g. question answering, information extraction). In this paper, we show that there are two issues in semantic relation description, one concerning knowledge engineering (what to annotate?) and the other concerning language engineering (how to deal with modality and modiﬁers?). We propose a new annotation scheme, making it possible to have both a precise and tractable annotation. A practical experiment shows that annotators using our scheme were able to quickly annotate a large number of sentences with very high inter-annotator agreement. 
In order to automatically extract opinion holders, we propose to harness the contexts of prototypical opinion holders, i.e. common nouns, such as experts or analysts, that describe particular groups of people whose profession or occupation is to form and express opinions towards speciﬁc items. We assess their effectiveness in supervised learning where these contexts are regarded as labeled training data and in rule-based classiﬁcation which uses predicates that frequently co-occur with mentions of the prototypical opinion holders. Finally, we also examine in how far knowledge gained from these contexts can compensate the lack of large amounts of labeled training data in supervised learning by considering various amounts of actually labeled training sets. 
Multiword expressions (MWEs) and named entities (NEs) exhibit unique and idiosyncratic features, thus, they often pose a problem to NLP systems. In order to facilitate their identiﬁcation we developed the ﬁrst corpus of Wikipedia articles in which several types of multiword expressions and named entities are manually annotated at the same time. The corpus can be used for training or testing MWE-detectors or NER systems, which we illustrate with experiments and it also makes it possible to investigate the co-occurrences of different types of MWEs and NEs within the same domain. 
Lexical Resources are a critical component for Natural Language Processing applications. However, the high cost of comparing and merging different resources has been a bottleneck to have richer resources with a broad range of potential uses for a significant number of languages. With the objective of reducing cost by eliminating human intervention, we present a new method for automating the merging of resources, with special emphasis in what we call the mapping step. This mapping step, which converts the resources into a common format that allows latter the merging, is usually performed with huge manual effort and thus makes the whole process very costly. Thus, we propose a method to perform this mapping fully automatically. To test our method, we have addressed the merging of two verb subcategorization frame lexica for Spanish, The results achieved, that almost replicate human work, demonstrate the feasibility of the approach. 
In this paper we introduce an unsupervised learning approach for WordNet construction. The whole construction method is an Expectation Maximization (EM) approach which uses Princeton WordNet 3.0 (PWN) and a corpus as the data source for unsupervised learning. The proposed method can be used to construct WordNet in any language. Links between PWN synsets and target language words are extracted using a bilingual dictionary. For each of these links a parameter is defined that shows probability of selecting PWN synset for target language word in corpus. Model parameters are adjusted in an iterative fashion. In our experiments on Persian language, by selecting 10% of highly probable links trained by the EM method, a Persian WordNet was obtained that covered 7,109 out of 11,076 distinct words and 9,427 distinct PWN synsets with a precision of more than 86%. 
Automatic authorship attribution, by its nature, is much more advantageous if it is domain (i.e., topic and/or genre) independent. That is, many real world problems that require authorship attribution may not have in-domain training data readily available. However, most previous work based on machine learning techniques focused only on in-domain text for authorship attribution. In this paper, we present comprehensive evaluation of various stylometric techniques for cross-domain authorship attribution. From the experiments based on the Project Gutenberg book archive, we discover that extremely simple techniques based on stopwords are surprisingly robust against domain change, essentially ridding the need for domain adaptation when supplied with a large amount of data. 
Among the motivations to write in Wikipedia given by the current literature there is often coincidence, but none of the studies presents the hypothesis of contributing for the visibility of the own national or language related content. Similar to topical coverage studies, we outline a method which allows collecting the articles of this content, to later analyse them in several dimensions. To prove its universality, the tests are repeated for up to twenty language editions of Wikipedia. Finally, through the best indicators from each dimension we obtain an index which represents the degree of autoreferentiality of the encyclopedia. Last, we point out the impact of this fact and the risk of not considering its existence in the design of applications based on user generated content. 
Semantic argument structures are often incomplete in that core arguments are not locally instantiated. However, many of these implicit arguments can be linked to referents in the wider context. In this paper we explore a number of linguistically motivated strategies for identifying and resolving such null instantiations (NIs). We show that a more sophisticated model for identifying deﬁnite NIs can lead to noticeable performance gains over the state-ofthe-art for NI resolution. 
This paper explores a new approach to help non-expert users with no background in linguistics to add new words to a monolingual dictionary in a rule-based machine translation system. Our method aims at choosing the correct paradigm which explains not only the particular surface form introduced by the user, but also the rest of inﬂected forms of the word. A large monolingual corpus is used to extract an initial set of potential paradigms, which are then interactively reﬁned by the user through active machine learning. We show the results of experiments performed on a Spanish monolingual dictionary. 
In the biomedical domain, many terms are neoclassical compounds (composed of several Greek or Latin roots). The study of their morphology is important for numerous applications since it makes it possible to structure, translate, retrieve them efﬁciently... In this paper, we propose an original yet fruitful approach to carry out this morphological analysis by relying on Japanese, more precisely on terms written in kanjis, as a pivot language. In order to do so, we have developed a specially crafted alignment algorithm relying on analogy learning. Aligning terms with their kanji-based counterparts provides at the same time a decomposition of the term into morphs, and a kanji label for each morph. Evaluated on a dataset of French terms, our approach yields a precision greater than 70% and shows its relevance compared with existing techniques. We also illustrate the interest of this approach through two direct applications of the produced alignments: translating unknown terms and discovering relationships between morphs for terminological structuring. 
In this paper, we demonstrate the portability of the lexical acquisition (LA) method proposed in Cholakov and van Noord (2010a). Here, LA refers to the acquisition of linguistic descriptions for words which are not listed in the lexicon of a given computational grammar, i.e., words which are unknown to this grammar. The method we discuss was originally developed for the Dutch Alpino system, and the paper shows that the method also applies to the GG (Crysmann, 2003), a computational HPSG grammar of German. The LA method obtains very similar results for German (84% F-measure on learning unknown words). Extending the GG with the lexical entries proposed by the LA method causes an important improvement in parsing accuracy for a test set of sentences containing unknown words. Furthermore, in a smaller experiment, we show that the linguistic knowledge the LA method provides can also be used for sentence generation. 
This article evaluates the integration of data extracted from a syntactic lexicon, namely the Lexicon-Grammar, into several probabilistic parsers for French. We show that by modifying the Part-ofSpeech tags of verbs and verbal nouns of a treebank, we obtain accurate performances with a parser based on Probabilistic Context-Free Grammars (Petrov et al., 2006) and a discriminative parser based on a reranking algorithm (Charniak and Johnson, 2005). 
Event extraction systems typically take advantage of language and domain-speciﬁc knowledge bases, including patterns that are used to identify speciﬁc facts in text; techniques to acquire these patterns can be considered one of the most challenging issues. In this work, we propose a languageindependent and weakly-supervised algorithm to automatically discover linear patterns from texts. Our approach is based on a phrase-based statistical machine translation system trained on monolingual data. A bootstrapping version of the algorithm is proposed. Our method was tested on patterns with different domain-speciﬁc semantic roles in three languages: English, Spanish and Russian. Performance shows the feasibility of our approach and its capability of working with texts in various languages. 
This paper demonstrates a web-based online system, called META-DARE1. META-DARE is built to assist researchers to obtain insights into seed-based minimally supervised machine learning for relation extraction. META-DARE allows researchers and students to conduct experiments with an existing machine learning system called DARE (Xu et al., 2007). Users can run their own learning experiments by constructing initial seed examples and can monitor the learning process in a very detailed way, namely, via interacting with each node in the learning graph and viewing its content. Furthermore, users can study the learned relation extraction rules and their applications. META-DARE is also an analysis tool which gives an overview of the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a very convenient user interface for visualization of the learning graph, the learned rules and the system performance proﬁle. 
Transliteration mining is aimed at building high quality multi-lingual named entity (NE) lexicons for improving performance in various Natural Language Processing (NLP) tasks including Machine Translation (MT) and Cross Language Information Retrieval (CLIR). In this paper, we apply two Dynamic Bayesian network (DBN)-based edit distance (ED) approaches in mining transliteration pairs from Wikipedia. Transliteration identiﬁcation results on standard corpora for seven language pairs suggest that the DBN-based edit distance approaches are suitable for modeling transliteration similarity. An evaluation on mining transliteration pairs from English-Hindi and English-Tamil Wikipedia topic pairs shows that they improve transliteration mining quality over state-of-the-art approaches. 
In this paper, we present an opinion detection system built on top of a robust syntactic parser. The goal of this system is to extract opinions associated with products but also with characteristics of these products, i.e. to perform feature-based opinion extraction. To carry out this task, and following a target corpus study, the robust syntactic parser is enriched by associating polarities to pertinent lexical elements and by developing generic rules to extract relations of opinions together with their polarity, i.e. positive or negative. These relations are used to feed an opinion representation model. A first evaluation shows very encouraging results, but numerous perspectives and developments remain to be investigated. 
Most NLP systems make predictions based solely on linguistic (textual or spoken) input. We show how to use visual information to make better linguistic predictions. We focus on selectional preference; speciﬁcally, determining the plausible noun arguments for particular verb predicates. For each argument noun, we extract visual features from corresponding images on the web. For each verb predicate, we train a classiﬁer to select the visual features that are indicative of its preferred arguments. We show that for certain verbs, using visual information can significantly improve performance over a baseline. For the successful cases, visual information is useful even in the presence of cooccurrence information derived from webscale text. We assess a variety of training conﬁgurations, which vary over classes of visual features, methods of image acquisition, and numbers of images. 
For the task of turning a natural language question into an explicit intermediate representation of the complexity in question answering systems, all published works so far use rulebased approach to the best of our knowledge. We believe it is because of the complexity of the representation and the variety of question types and also there are no publicly available corpus of a decent size. In these rule-based approaches, the process of creating rules is not discussed. It is clear that manually creating the rules in an ad-hoc manner is very expensive and error-prone. In this paper, we focus on the process of creating those rules manually, in a way that consistency between rules is maintained and the effort to create a new rule is independent of the size of the current rule set. Experimental results are promising where our system achieves better performance and requires much less time and cognitive load compared to previous work. 
In this paper we present the development process of NLP-QT, a question treebank that will be used for data-driven parsing in the context of a domain-speciﬁc QA system for querying NLP resource metadata. We motivate the need to build NLP-QT as a resource in its own right, by comparing the Penn Treebank-style annotation scheme used for QuestionBank (Judge et al., 2006) with the modiﬁed NP annotation for the Penn Treebank introduced by Vadas and Curran (2007). We argue that this modiﬁed annotation scheme provides a better interface representation for semantic interpretation and show how it can be incorporated into the NLP-QT resource, without signiﬁcant loss in parser performance. The parsing experiments reported in the paper conﬁrm the feasibility of an iterative, semi-automatic construction of the NLP-QT resource similar to the approach taken for QuestionBank. At the same time, we propose to improve the iterative reﬁnement technique used for QuestionBank by adopting Hwa (2001)’s heuristics for selecting additional material to be handcorrected and added to the data set at each iteration. 
In order to be able to systematically link compounds in GermaNet to their constituent parts, compound splitting needs to be applied recursively and has to identify the immediate constituents at each level of analysis. Existing tools for compound splitting for German only offer an analysis of all component parts of a compound at once without any grouping of subconstituents. Thus, existing tools for splitting compounds were adapted to overcome this issue. Algorithms combining three heterogeneous kinds of compound splitters are developed to achieve better results. The best overall result with an accuracy of 92.42% is achieved by a hybrid combined compound splitter that takes into account all knowledge provided by the individual compound splitters, and in addition some domain knowledge about German derivation morphology and compounding. 
The long term goal of this research is to develop a program able to produce an automatic segmentation and categorization of textual sequences into discourse types. In this preliminary contribution, we present the construction of an algorithm which takes a segmented text as input and attempts to produce a categorization of sequences, such as narrative, argumentative, descriptive and so on. Also, this work aims at investigating a possible convergence between the typological approach developed in particular in the ﬁeld of text and discourse analysis in French by Adam (2008) and Bronckart (1997) and unsupervised statistical learning. 
Quotations from ﬁnancial leaders can have signiﬁcant inﬂuence upon the immediate prospects of economic actors. Indiscreet or candid comments from senior business leaders have had detrimental effects upon their organizations. Established polarity classiﬁcation techniques perform poorly when classifying quotations because they display a number of complex linguistic features and lack of training data. The proposed strategy segments the quotations by inferred “opinion maker” role and then applies individual polarity classiﬁcation strategies to each group of the segmented quotations. This strategy demonstrates a clear advantage over applying classical classiﬁcation techniques to the whole corpus of quotations. While modelling contextual information with Random Forests based on a vector of unigrams plus the “opinion maker role” reaches a maximum F-measure of 52.85%, understanding the “bias” of the quotation maker previously based on its lexical usage allows 86.23% F-measure for “unbiased” quotations and 71.10% F-measure for “biased” quotations with the Naive Bayes classiﬁer. 
This study aims to assess the usefulness of multi-word expressions (MWEs) as features for a readability formula that predicts the difﬁculty of texts for French as a foreign language. Using a MWE extractor combining a statistical approach with a linguistic ﬁlter, we deﬁne 11 predictors. These take into account the density and the probability of MWEs, but also their internal structure. Our experiments show that the predictive power of these 11 variables is low and that a simple approach based on the average probability of n-grams is more effective. 
We present a treatment of Arabic morphology which allows us to deal with ‘weak’ verbs by paying attention to the underlying phonological process. This provides us with a very clean way of thinking about such verbs, and also makes maintenance of the lexicon very straightforward. 
This paper presents a novel method for recognizing textual entailment which derives the hypothesis from the text through a sequence of parse tree transformations. Unlike related approaches based on tree-edit-distance, we employ transformations which better capture linguistic structures of entailment. This is achieved by (a) extending an earlier deterministic knowledge-based algorithm with syntactically-motivated on-the-ﬂy transformations, and (b) by introducing an algorithm that uniformly learns costs for all types of transformations. Our evaluations and analysis support the validity of this approach. 
We introduce a system that learns the participants of arbitrary given scripts. This system processes data from web experiments, in which each participant can be realized with different expressions. It computes participants by encoding semantic similarity and global structural information into an Integer Linear Program. An evaluation against a gold standard shows that we signiﬁcantly outperform two informed baselines. 
The paper discusses the transferring rules of the output from a dependency parser for Bulgarian into RMRS analyses. This task is required by the machine translation compatibility between Bulgarian and English resources. Since the Bulgarian HPSG grammar is still being developed, a repairing mechanism has been envisaged by parsing the Bulgarian data with the Malt Dependency Parser, and then retrieving RMRS analyses by exploring the linguistic knowledge within BulTreeBank-DP. 
Discourse incoherence is an important and typical problem with multi-document extractive summaries. To address this issue, we have developed a schema-based summarization approach for query-based blog summaries that utilizes discourse structures. In our schema design, we tried to model discourse structures which are typically used by humans in their summary writing in response to a particular question type. In our approach, a sentence instantiates a speciﬁc slot of the schema based on its discourse structures. To validate our approach, we have built a system named BlogSum and have evaluated its performance through 4 human participants using a likert scale of 1 to 5. The evaluation results show that our approach has signiﬁcantly improved summary coherence compared to the summaries with no discourse structuring without compromising on content evaluation. 
The paper presents the results of an analysis of the merits and problems of using sufﬁx arrays as an index data structure for annotated natural-language corpora. It shows how multiple sufﬁx arrays can be combined to represent layers of annotation, and how this enables matches for complex linguistic patterns to be identiﬁed in the corpus quickly and, for a large subclass of patterns, with greater theoretical efﬁciency than alternative approaches. The results reported include construction times and retrieval times for an annotated corpus of 1.9 billion characters in length, and a range of example patterns of varying complexity. 
This paper presents a formal mechanism to properly constrain the scope of negation and of certain quantificational determiners to their minimal clause in continuation semantics framework introduced in Barker and Shan (2008) and which was subsequently extended from sentential level to discourse level in Dinu (2011). In these works, type shifting is employed to account for side effects such as pronominal anaphora binding or quantifier scope. However, allowing arbitrary type shifting will result in overgenerating interpretations impossible in natural language. To filter out some of these impossible interpretations, once the negation or the quantifiers reach their maximal scope limits (that is their minimal clause), one should force their scope closing by applying a standard type shifter Lower. But the actual mechanism that forces the scope closing was left underspecified in previous work on continuation semantics. We propose here such a mechanism, designed to ensure that no lexical entries having the scope bounded to their minimal clause (such as not, no, every, each, any, etc) will ever take scope outside. 
Organizing data into category hierarchies (taxonomies) is useful for content discovery, search, exploration and analysis. In industrial settings targeted taxonomies for speciﬁc domains are mostly created manually, typically by domain experts, which is time consuming and requires a high level of expertise. This paper presents an algorithm and an implemented interactive system for automatically generating target-domain taxonomies based on the Wikipedia Category Hierarchy. The system also enables human post-editing, facilitated by intelligent assistance. 
Approaches based on machine learning, such as Support Vector Machines, are often used to classify semantic relations between entities. In such framework, classiﬁcation accuracy strongly depends on the set of features which are used to represent the input to the classiﬁer. We are proposing here a new type of features, namely the barrier features, which can be used in addition to more usual features, such as ngrams of PoS, word sufﬁxes and preﬁxes, hypernyms from WordNet etc., and to the parse tree of the whole sentence. Barrier features aim at giving a compact representation of the context of each entity involved in the relation. The effectiveness of the new features is assessed on documents from the TREC data set annotated by Roth and Yih. The obtained results show not only that the performance of the proposed approach are state-of-the-art but also that such improvement is due to the introduction of the barrier features. 
While the concept of similarity is well grounded in psychology, text similarity is less well-deﬁned. Thus, we analyze text similarity with respect to its deﬁnition and the datasets used for evaluation. We formalize text similarity based on the geometric model of conceptual spaces along three dimensions inherent to texts: structure, style, and content. We empirically ground these dimensions in a set of annotation studies, and categorize applications according to these dimensions. Furthermore, we analyze the characteristics of the existing evaluation datasets, and use those datasets to assess the performance of common text similarity measures. 
Preliminary research demonstrated the EmotiBlog annotated corpus relevance as a Machine Learning resource to detect subjective data. In this paper we compare EmotiBlog with the JRC Quotes corpus in order to check the robustness of its annotation. We concentrate on its coarse-grained labels and carry out a deep Machine Learning experimentation also with the inclusion of lexical resources. The results obtained show a similarity with the ones obtained with the JRC Quotes corpus demonstrating the EmotiBlog validity as a resource for the SA task. 
The Internet boom in recent years has increased the interest in the field of plagiarism detection. A lot of documents are published on the Net everyday and anyone can access and plagiarize them. Of course, checking all cases of plagiarism manually is an unfeasible task. Therefore, it is necessary to create new systems that are able to automatically detect cases of plagiarism produced. In this paper, we introduce a new hybrid system for plagiarism detection which combines the advantages of the two main plagiarism detection techniques. This system consists of two analysis phases: the first phase uses an intrinsic detection technique which dismisses much of the text, and the second phase employs an external detection technique to identify the plagiarized text sections. With this combination we achieve a detection system which obtains accurate results and is also faster thanks to the prefiltering of the text. 
We present a data-driven approach for recognizing and classifying TimeML events in Italian. A high-performance stateof-the-art approach, TIPSem, is adopted and extended with Italian-speciﬁc semantic features from a lexical resource. The resulting approach has been evaluated over the ofﬁcial TempEval2 Italian test data. The analysis of the results shows a positive impact of the semantic features both for event recognition and classiﬁcation. Moreover, the presented data-driven approach has been compared with an existing rule-based prototype over the same data set. The results are directly comparable and show that the machine learning strategy better deals with the complexity of the tasks. 
In this paper we look at the conjugation of the Romanian verb, in particular, at its irregularities, from a machine learning point of view. Our attempt is to predict the presence or absence of any alternation in the stem (apophony), using n-gram representations of the inﬁnitive. We combine formal labelling mechanisms with learning methods in order to build a general conjugational model. 
Recognition and translation of named entities (NEs) are two current research topics with regard to the proliferation of electronic documents exchanged through the Internet. The need to assimilate these documents through NLP tools has become necessary and interesting. Moreover, the formal or semiformal modeling of these NEs may intervene in both processes of recognition and translation. Indeed, the modeling makes more reliable the constitution of linguistic resources, limits the impact of linguistic specificities and facilitates transformations from one representation to another. In this context, we propose an approach of recognition and translation based on a representation model of Arabic NEs and a set of transducers resolving morphological and syntactical phenomena. 
Current statistical machine translation (SMT) systems are stated to be dependent on the availability of a very large training data for producing the language and translation models. Unfortunately, large parallel corpora are available for a limited set of language pairs and for an even more limited set of domains. In this paper we investigate the behavior of an SMT system exposed to training data of different sizes and types. Our experimental results show that even parallel corpora of modest sizes can be used for training purposes without lowering too much the evaluation scores. We consider two language pairs in both translation directions for the experiments: English-Romanian and German-Romanian. 
This paper discusses linguistic annotation issues, essential to a corpus-based approach to modelling the language use of foreign language learners in various contexts. We focus on learners of English and describe the corpora we use as well as the linguistic approach underlying their development. We present a scheme for describing grammatical choices and meaning components expressed in texts produced by learners. Our goal is to model the associations of corpus-attested linguistic patterns with their contexts, at different levels of language proﬁciency. 
We present the results of a project of building a lexical-functional grammar of Aymara, an Amerindian language. There was almost no research on Aymara in computational linguistics to date. The goal of the project is two-fold: First, we want to provide a formal description of the language. Second, NLP resources (lexicon and grammar) are being developed that could be used in machine translation and other NLP tasks. The paper presents formal description of selected properties of Aymara which are uncommon in wellresearched Western languages. Furthermore, we present an experimental machine translation system into Spanish and English. 
We discuss a method for identifying semantic arguments of a verb from a sentence. It differs from existing methods by an unique feature that represents all semantic arguments of a verb in a syntactic parse tree. The feature is a path in which at least one of the children of a node is a root of a subtree that associates with a semantic argument. Experiments on WSJ data from Penn TreeBank and PropBank show that our method achieves an average of precision 92.3% and an average of recall 94.2% on identifying semantic arguments of over six hundred verbs. 
The paper proposes a treatment of relative sentences within the framework of Head-driven Phrase Structure Grammar (HPSG). Relative sentences are considered as a rather delicate linguistic phenomenon and not explored enough by Arabic researchers. In an attempt to deal with this phenomenon, we propose in this paper a study about different forms of relative clauses and the interaction of relatives with other linguistic phenomena such as ellipsis and coordination. In addition, in this paper we shed light on the recursion in Arabic relative sentences which makes this phenomenon more delicate in its treatment. This study will be used for the construction of an HPSG grammar that can process relative sentences. The HPSG formalism is based on two fundamental components: features and AVM (AttributeValue-Matrix). In fact, an adaptation of HPSG for the Arabic language is made here in order to integrate features and rules of the Arabic language. The established HPSG grammar is specified in TDL (Type Description Language). This specification is used by the LKB platform (Linguistic Knowledge Building) in order to generate the parser. 
Incorporating distant information via manually selected skip chain templates has been shown to be beneﬁcial for the performance of conditional random ﬁeld models in contrast to a simple linear chain based structure (Sutton and McCallum, 2007; Galley, 2006; Liu et al., 2010). The set of properties to be captured by a template is typically manually chosen with respect to the application domain. In this paper, a search strategy to ﬁnd meaningful skip chains independent from the application domain is proposed. From a huge set of potentially beneﬁcial templates, some can be shown to have a positive impact on the performance. The search for a meaningful graphical structure demonstrates the usefulness of the approach with an increase of nearly 2 % F1 measure on a publicly available data set (Klinger et al., 2008). 
We propose the negation naive Bayes (NNB): a new method to categorize product pages on the Web depending on their information. It is a modiﬁed version of the naive Bayes (NB) and we got the idea from the complement naive Bayes (CNB). We compared the NNB with the NB and the CNB. Our experiments show that the NNB outperformed the other methods signiﬁcantly when the product pages were distributed non-uniformly through categories. 
This paper, we propose an approach for event extraction and corresponding event actor identificationwithin the TimeML framework. Firstly, for event extraction, we develop SVM based hybrid approach and for event actor identification the baseline model is developed based on the subject information of the dependency-parsed event sentences. Then we develop an unsupervised syntax based model that is based on the relationship of the event verbs with their argument structure extracted from the head information of the chunks in the parsed sentences. Evaluation on a collection of TempEval-2 corpus shows the precision, recall and F-measure values for the baseline model as 64.31%, 67.74% and 65.98%, respectively and the syntax based model as 69.12%, 66.90% and 67.99%, respectively. 
Machine translation (MT) technology is becoming more and more pervasive, yet the quality of MT output is still not ideal. Thus, human corrections are used to edit the output for further studies. However, how to judge the human correction might be tricky when the annotators are not experts. We present a novel way that uses cross-validation to automatically judge the human corrections where each MT output is corrected by more than one annotator. Cross-validation among corrections for the same machine translation, and among corrections from the same annotator are both applied. We get a correlation around 40% in sentence quality for Chinese-English and Spanish-English. We also evaluate the user quality as well. At last, we rank the quality of human corrections from good to bad, which enables us to set a quality threshold to make a trade-off between the scope and the quality of the corrections. 
Information extraction in specialized texts raises different problems related to the kind of searched information. In this paper, we are interested in relation identiﬁcation between some concepts in medical reports, task that was evaluated in the i2b2 2010 challenge. As relations are expressed in natural language with a great variety of forms, we proceeded to sentence analysis by extracting features that enable all together to identify a relation and we modeled this task as a multi-class classiﬁcation based on an SVM, each type of relation representing a class. We will present the selection of the features used by our system and an error analysis. This approach allowed us to obtain an F-measure of 0.70, classifying the system among the best systems. 
 Breaking away from traditional attempts at coreference resolution from discourseonly inputs, we try to do the same by constructing rich verb semantics from perceptual data, viz. a 2-D video. Using a bottom-up dynamic attention model and relative-motion-features between agents in the video, transitive verbs, their argument ordering etc. are learned through association with co-occurring adult commentary. This leads to learning of synonymous NP phrases as well as anaphora such as “it”,“each other” etc. This preliminary demonstration argues for a new approach to developmental NLP, with multi-modal semantics as the basis for computational language learning. 
Based on the evidences of preverbal conceptual development in infants, we adopt semantics-ﬁrst approach for word-learning. We ﬁrst cluster several perceptual categories from a complex visual interaction. Using a surveillance traﬃc video, we a) identify the moving objects by separating these from a static background, and b) group the similar appearances into clusters. The resulting models are found to be noisy approximations of traﬃc object categories and motion actions. Next, we consider these models along with parallel commentaries that describe the scene in free, unconstrained language. A bottom-up model of dynamic attention is applied to identify objects in perceptual focus, which are mapped to words in co-temporaneous utterances. Using no language-speciﬁc knowledge such as syntax, we show the ability to learn words for the object classes and also for the motion actions. 
The identiﬁcation of different kinds of multiword expressions require different solutions, on the other hand, there might be domain-related differences in their frequency and typology. In this paper, we show how our methods developed for identifying noun compounds and light verb constructions can be adapted to different domains and different types of texts. Our results indicate that with little effort, existing solutions for detecting multiword expressions can be successfully applied to other domains as well. 
 We present a novel method for FrameNetbased semantic role labeling (SRL), focusing on limitations posed by the limited coverage of available annotated data. Our SRL model is based on Bayesian clustering and has the advantage of being very robust in the face of unseen and incomplete data. Frame labeling and role labeling are modeled in like fashions, allowing cascading classiﬁcation scenarios. The model is shown to perform especially well on unseen data. In addition, we show that for seen data, predicting semantic types for roles improves role labeling performance.  
This paper presents a set of preliminary experiments which show that identifying translationese is possible with machine learning methods that work at character level, more precisely methods that use string kernels. But caution is necessary because string kernels very easily can introduce confounding factors. 
We examine how the recently explored class of linear transductions relates to ﬁnite-state models. Linear transductions have been neglected historically, but gainined recent interest in statistical machine translation modeling, due to empirical studies demonstrating that their attractive balance of generative capacity and complexity characteristics lead to improved accuracy and speed in learning alignment and translation models. Such work has until now characterized the class of linear transductions in terms of either (a) linear inversion transduction grammars (LITGs) which are linearized restrictions of inversion transduction grammars or (b) linear transduction grammars (LTGs) which are bilingualized generalizations of linear grammars. In this paper, we offer a new alternative characterization of linear transductions, as relating four ﬁnite-state languages to each other. We introduce the devices of zipper ﬁnite-state automata (ZFSAs) and zipper ﬁnite-state transducers (ZFSTs) in order to construct the bridge between linear transductions and ﬁnite-state models. 
In internet advertising, negative key phrases are used in order to exclude the display of an advertisement to non-target audience. We describe a method for automatically identifying negative key phrases. We use Wikipedia as our sense inventory and as an annotated corpus from which we create context vectors and determine negative phrases, which correlate with negative senses of a positive key phrase. 
As clarity in the Crisis Management domain is crucial, and there exists an enormous amount of Crisis Management documents, a speciﬁc language resource (the Controlled Language for Crisis Management, CLCM) for editing Crisis Management instructions in English has been previously developed. Based on a specially designed controlled language evaluation experiment, we have determined that manual simpliﬁcation, far from being easy, is an extremely time-consuming process and thus automatization is essential in order to facilitate the writing of clear instructions. This article describes this experiment which also aims to determine which operations should be privileged and are more urgent to be implemented in order to address the most critical issues ﬁrst. 
In this paper we present an information service system that allows users to search for the key players of requested technology areas and for their collaboration networks. This system utilizes information extraction and wrapper technologies for detecting persons, organizations, publications and patents as well as relationships among them. Furthermore, it applies relation extraction to detect statements on the web that indicate innovation trends. Various visualization methods are provided to let users monitor key players, their networks and technology trends in a comfortable way. 
The area of Subjectivity and sentiment analysis (SSA) has been witnessing a ﬂurry of novel research. However, only few attempts have been made to build SSA systems for the health domain. In the current study, we report efforts to partially bridge this gap. We present a new labeled corpus of professional articles collected from major Websites focused on the Obama health reform plan (OHRP). We introduce a new annotation scheme that incorporates subjectivity as well as topics directly related to the OHRP and describe a highlysuccessful SSA system that exploits the annotation. In the process, we introduce a number of novel features and a wide-coverage polarity lexicon for the health domain. 
During a quarter of a century of existence and in spite of much criticism, wordnets have thoroughly proved their appropriateness as repositories of linguistic knowledge and their usefulness in various applications. In this paper we present the methodology of creating the Romanian wordnet (RoWN), with special emphasis on the strategies adopted during ten years of ceaseless implementation and which highlight the efforts invested, the way we dealt with the alignment of the RoWN (previously aligned to PWN 2.0) to the PWN 3.0, as well as the future work we envisage for enriching and extending this resource. 
This article describes the Romanian lexical resources containing morphological data and dictionaries: synonyms, Romanian-English, and RomanianRussian. The inﬂection process at the creation of morphological resources based on the functional grammar with scattered context is considered. An arbitrary word is inﬂected knowing only its part of speech, and the gender for nouns. New words were obtained also by using preﬁxing and sufﬁxing. The research in automated preﬁxing and sufﬁxing permitted us to determine some word classes for which this method is applicable, and to implement the corresponding algorithms. We describe the database structure, and the DB population programming tools. The article describes an approach to the checking of integrity and correctness of the morphological resources presented as a database mapping Romanian words to their morphological derivatives. 
Syntactic analysis is a fundamental phase in NLP (Natural Language Processing) domain. This phase occurs in several applications and at different levels. Moreover, it wasn’t spilled in domain research, especially for Arabic language. In fact, most of researchers working on Arabic language treated simple structures and neglected complicated ones such as relatives, coordination, ellipse and juxtaposition. In this context, the present work lies within the construction of a HPSG (Head-driven Phrase Structure Grammar) grammar treating Arabic coordination. The established grammar is specified on TDL (Type Description Language) and experimented with a parser generated by LKB (Linguistic Knowledge Building) system. 
The annotation of documents with linguistic information requires time-consuming and therefore expensive manual annotation. Especially, a complex task, like coreference resolution, needs large data sets for the training of supervised machine learning methods. We present a tool which combines visualization techniques and unsupervised machine learning to support the annotation of documents with coreference information. Self-organizing Maps are used to cluster similar data and visualize the feature space. For link visualization, precise annotation, and error correction a matrix-based coreference visualization is used which exploits the transitive property of the coreference relation. 
In this article, we present the on-line interface that we have developed for the RST Spanish Treebank, the first corpus including Spanish texts annotated with rhetorical relations. This interface allows users to consult or download the texts and their corresponding annotations. In addition, it allows carrying out several tasks over a selected subcorpus: searching statistics in terms of words, rhetorical relations and Elementary Discourse Units (EDUs), and extracting information, in terms of texts passages marked with rhetorical relations (ex. Result, Cause or Background), which users may select. 
Plagiarism has always been a concern in many sectors, particularly in education. With the sharp rise in the number of electronic resources available online, an increasing number of plagiarism cases has been observed in recent years. As the amount of source materials is vast, the use of plagiarism detection tools has become the norm to aid the investigation of possible plagiarism cases. This paper describes an approach to improve plagiarism detection by incorporating a lexical generalisation technique. The goal is to identify plagiarised texts even if they are paraphrased using different words. Experiments performed on a subset of the PAN‟10 corpus show that the matching approach involving lexical generalisation yields promising results, as compared to standard n-gram matching strategies. 
The paper describes the method of extraction of two-word domain terms combining their features. The features are computed from three sources: the occurrence statistics in a domain-specific text collection, the statistics of global search engines, and a domainspecific thesaurus. The evaluation of the approach is based on manually created thesauri. We show that the use of multiple features considerably improves the automatic extraction of domain-specific terms. We compare the quality of the proposed method in two different domains. 
Usually, in the Question Answering domain, for a question in natural language, precise answers to the question are extracted from documents according only to the context of the question. In this work, we complemented this approach by adding a ﬁltering process on top of the document retrieval. This way, the system reevaluates the documents it has originally selected during the information retrieval step before the answer extraction and scoring. Such re-evaluation aims at ﬁltering out documents considered unusable for the search. Based on statistical language modeling, the ﬁltering process ﬁrstly determines the intrinsic relevancy of a document and then decides whether this document is a priori relevant for ﬁnding answers. Evaluation on factoid questions and a collection of 500k web documents has shown our approach properly supports the Question Answering task. 
Machine learning approaches for Information Extraction use different types of features to acquire semantically related terms from free text. These features may contain several kinds of linguistic knowledge: from orthographic or lexical to more complex features, like PoStags or syntactic dependencies. In this paper we select four main types of linguistic features and evaluate their performance in a systematic way. Despite the combination of some types of features allows us to improve the fscore of the extraction, we observed that by adjusting the positive and negative ratio of the training examples, we can build high quality classiﬁers with just a single type of linguistic feature, based on generic lexico-syntactic patterns. Experiments were performed on the Portuguese version of Wikipedia. 
This paper describes a system facilitating information retrieval in a set of textual documents by tackling the automatic titling and subtitling issue. Automatic titling here consists in extracting relevant noun phrases from texts as candidate titles. An original approach combining statistical criteria and noun phrases positions in the text helps collecting relevant titles and subtitles. So, the user may beneﬁt from an outline of all the subjects evoked in a mass of documents, and easily ﬁnd the information he/she is looking for. An evaluation on real data shows that the solutions given by this automatic titling approach are relevant. 
In this paper an unsupervised approach to domain adaptation is presented, which exploits external knowledge sources in order to port a classification model into a new thematic domain. Our approach extracts a new feature set from documents of the target domain, and tries to align the new features to the original ones, by exploiting text relatedness from external knowledge sources, such as WordNet. The approach has been evaluated on the task of document classification, involving the classification of newsgroup postings into 20 news groups. 
Recently, Opinion Mining (OM) is receiving more attention due to the abundance of forums, blogs, ecommerce web sites, news reports and additional web sources where people tend to express their opinions. There are a number of works about Sentiment Analysis (SA) studying the task of identifying the polarity, whether the opinion expressed in a text is positive or negative about a given topic. However, most of research is focused on English texts and there are very few resources for other languages. In this work we present an Opinion Corpus for Arabic (OCA) composed of Arabic reviews extracted from specialized web pages related to movies and films using this language. Moreover, we have translated the OCA corpus into English, generating the EVOCA corpus (English Version of OCA). In the experiments carried out in this work we have used different machine learning algorithms to classify the polarity in these corpora showing that, although the experiments with EVOCA are worse than OCA, the results are comparable with other English experiments, since the loss of precision due to the translation is very slight. 1. Introduction Nowadays, the interest in Opinion Mining (OM) has grown significantly due to different factors. On the one hand, the rapid evolution of the World Wide Web has changed our view of the Internet. It has turned into a collaborative framework where technological and social trends come together, resulting in the over exploited term Web 2.0. On the other hand, the tremendous use of e-commerce services has been accompanied by an increase in freely available online reviews and opinions about products and services. A customer who wants to buy a product usually searches information on the Internet trying to find other consumer analyses. In fact, web sites such as Amazon1, Epinions2 or IMDb3, can affect the customer decision. 
In this paper we describe and compare three approaches for the automatic extraction of medical terms using noun phrases (NPs) previously recognized on medical text corpus in Spanish. In the ﬁrst approach, as baseline, we extracted all NPs, while for the second and third ones the extraction process is directed to “speciﬁc NPs” that are determined on the basis of the syntactic and positional criteria, among others. As contributions (i) we showed that it is possible to extract medical terms using “speciﬁc NPs”, (ii) new terms were added in the software dictionary, and (iii) terms that were not in the reference lists were extracted. For the third contribution, we used the SNOMED CT R terms lists, aiming at improving the IULA reference lists. 
This paper addresses issues related to generating feedback messages to errors related to Arabic verbs made by second language learners (SLLs). The proposed approach allows for individualization. When a SLL of Arabic writes a wrong verb, it performs analysis of the input and distinguishes between different lexical error types. The proposed system issues the intelligent feedback that conforms to the learner’s proficiency level for each class of error. The proposed system has been effectively evaluated using real test data and achieved satisfactory results. 
We introduce an ontology that is representative of health discussions and vocabulary used by the general public. The ontology structure is built upon general categories of information that patients use when describing their health in clinical encounters. The pilot study shows that the general structure makes the ontology useful in text mining of social networking web sites. 
This paper presents a system based on Finite State Technology that recognises and classifies numerical entities in texts written in Basque. The system deals with a wide range of entities, such as temporal expressions, numbers related to units of measurement, or those that refer to common nouns. The system obtains 86.96% F-measure score following MUC evaluation and 78.82% using IREX and CONLL simple scoring protocol. 
We propose the creation and use of a multilingual parallel news corpus annotated with opinion towards entities, produced by projecting sentiment annotation from one language to several others. The objective is to save annotation time for development and evaluation purposes, and to guarantee comparability of opinion mining evaluation results across languages. By creating this resource, we answered the question whether sentiment is consistently translated across languages so that projection can actually be an option. We describe our approach to multilingual sentiment analysis and show its performance in 7 languages of the parallel corpus. 
We extract new terminology from a text by term validation in a dictionary. Our approach is based on estimating probabilities for previously unseen terms, i.e. not present in a dictionary. To do this we apply several probabilistic models previously not used for term recognition and propose a new one. We apply restriction of domain similarity on terms used for probability estimation and vary the parameters of the models. Performance of our approach is demonstrated using Wikipedia titles vocabulary. 
Consensus is the desired result in many argumentative discourses such as negotiations, public debates, and goal-oriented forums. However, due to the fact that usually people are poor arguers, a support of argumentation is necessary. Web-2 provides means for the online discussions which have their characteristic features. In our paper we study the features of discourse which lead to agreement. We use an argumentative corpus of Wikipedia discussions in order to investigate the influence of discourse structure and language on the final agreement. The corpus had been annotated with rhetorical relations and rhetorical structures leading to successful and unsuccessful discussions were analyzed. We also investigated language patterns extracted from the corpus in order to discover which ones are indicators of the following agreement. The results of our study can be used in system designing, whose purpose is to assist on-line interlocutors in consensus building. 
 Instances, Concepts and Relations • A concept (class) is a placeholder for a set of instances (objects) that share similar properties – set of instances • {matrix, kill bill, ice age, pulp fiction, inception, cidade de deus,...} – class label • movies, films – definition • a series of pictures projected on a screen in rapid succession with objects shown in successive positions slightly changed so as to produce the optical effect of a continuous picture in which the objects move (Merriam Webster) • a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement (WordNet) Instances, Concepts and Relations • Relations are assertions linking two (binary relation) or more (nary relation) concepts – actors-act in-movies; cities-capital of-countries • Facts are instantiations of relations, linking two or more instances – leonardo dicaprio-act in-inception; cairo-capital of-egypt • Attributes correspond to facts capturing quantifiable properties of a class or an instance – actors --> awards, birth date, height – movies --> producer, release date, budget  Open-Domain Information  diseases yellow fever, influenza, bipolar disorder, rocky mountain spotted fever, anosmia, myxedema,...  treatment symptoms causes incidence diagnosis  size color taste allergies calories  foods fish, turkey, rice, milk, chicken, cheese, eggs, corn, beans, wheat, asparagus, grapes,...  mass symbol atomic number electron configuration lewis dot diagram  chemical elements potassium, magnesium, gold, sulfur, palladium, argon, carbon, borium, ruthenium, zinc, lead,...  denominations country symbol exchange rate  currencies euro, won, lire, pounds, rand, us dollars, yen, pesos, pesetas, kroner,  drugs paxil, lipitor, ibuprofen, prednisone, albuterol, effexor, azithromycin,  side effects dosage price generic equivalent  currency converter  escudos, shillings,...  flag climate geography  countries australia, south korea, kenya, greece, sudan,  fluconazole, advil,...  withdrawal symptoms  currency population density  portugal, argentina, mexico, cuba, kuwait,...  Open-Domain Information  diseases yellow fever, influenza, bipolar disorder, rocky mountain spotted fever, anosmia, myxedema,...  used in the treatment of  decay product of  can reduce risk of  is a form of  good sources of  chemical elements potassium, magnesium, gold, sulfur, palladium, argon, carbon, borium, ruthenium, zinc, lead,...  drugs paxil, lipitor, ibuprofen, prednisone, albuterol, effexor, azithromycin, fluconazole, advil,...  depletes the body of brand name of  foods fish, turkey, rice, milk, chicken, cheese, eggs, corn, beans, wheat, asparagus, grapes,...  worth millions of  currency of  currencies euro, won, lire, pounds, rand, us dollars, yen, pesos, pesetas, kroner, escudos, shillings,...  countries australia, south korea, kenya, greece, sudan, portugal, argentina, mexico, cuba, kuwait,...  Terminology and Scope • Terminology – concept vs. class: used interchangeably – instance vs. entity: used interchangeably • Scope – discussing methods using queries to extract open-domain information – not discussing methods using queries in other tasks such as Web search in general (e.g., query suggestion, spelling correction, improving search results) Sources of Open-Domain Information • Human-compiled knowledge resources – resources created by experts – resources created collaboratively by non-experts • Sources of textual data – text documents (unstructured or semi-structured text) – (Web) search queries  Expert Resources • WordNet – [Fel98]: C. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press 1998. – lexical database of English created by experts – wide-coverage of upper-level conceptual hierarchies – replicated or extended to other languages • Cyc – [Len95]: D. Lenat. CYC: A Large-Scale Investment in Knowledge Infrastructure. Communications of the ACM 1995. – knowledge base of common-sense knowledge created by experts over 100+ person-years – terms and assertions capturing ground assertions and (inference) rules Collaborative, Non-Expert Resources • Wikipedia – [Rem02]: M. Remy. Wikipedia: The Free Encyclopedia. Journal of Online Information Review 2002. – free online encyclopedia developed collaboratively by Web volunteers – among top 20 most popular Web sites (according to comScore: Top 50 US Web Properties, Aug 2009) • DBpedia – [BLK+09] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer et al. DBpedia – A Crystallization Point for the Web of Data. Journal of Web Semantics 2009. – community effort to convert Wikipedia articles into structured data – manually-created ontology, mappings from subset of Wikipedia infoboxes to ontology, mappings from Wikipedia articles to WordNet concepts • Freebase – [BEP+08]: K. Bollacker, C. Evans, P. Paritosh et al. Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge. SIGMOD-08. – repository for storing structured data from Wikipedia and other sources, as well as from user contributions – collaboratively created, structured and maintained • Open Mind – [SLM+02]: P. Singh, T. Lin, E. Mueller, G. Lim, T. Perkins and W. Zhu. Open Mind Common Sense: Knowledge Acquisition from the General Public. Lecture Notes In Computer Science 2002. – collect common-sense knowledge from non-expert Web users – unlike Cyc, collect and represent knowledge in natural language rather than through formal assertions  Wikipedia infobox  Wikipedia  Wikipedia article  DBpedia, Freebase  Wikipedia infobox  Wikipedia infobox source code  <Sears_Tower, previous_building, World_Trade_Center> <Sears_Tower, construction_period, 1970-1973> ... DBpedia entries  Quantitative Comparison of Human-Compiled Resources • Wikipedia – 3.5+ million articles in English – articles also available in 200+ other languages • DBpedia – 2.5+ million instances, 250+ million relations • Freebase – 20+ million instances, 300+ million relations • Cyc – ResearchCyc: 300,000+ concepts and 3+ million assertions – OpenCyc 2.0: add mappings from Cyc concepts to Wikipedia articles • Open Mind – 800,000+ facts in English – facts also available in other languages Sources of Open-Domain Information • Human-compiled knowledge resources – resources created by experts – resources created collaboratively by non-experts • Sources of textual data – text documents (unstructured or semi-structured text) – (Web) search queries  Documents  Unstructured text  Semi-structured text  Documents  Semi-structured text  Semi-structured text  Alternative to Documents • Conventionally: data for textual information extraction is available as (some sort of) a document collection – documents capture knowledge, or assertions about the world – assertions are often “hidden” in expository text – the goal is to derive some of that knowledge from text • Alternatively: textual information extraction may be pursued even without a document collection – to find new knowledge within a document collection, users formulate their search queries based on the knowledge that they already possess at the time of the search --> query logs collectively capture knowledge, through requests that may be answered by knowledge asserted in document collections Next Topic • Part One: Introduction • Part Two: Queries as a Corpus • Part Three: Extraction from Queries  Queries as a Corpus • Structure of queries • Comparison with other textual sources • Usage, demographics and privacy Structure of Queries • [SW07]: S. Bergsma and Q. Wang. Learning Noun Phrase Query Segmentation. EMNLP-07. – identify segments of contiguous query tokens corresponding to semantic concepts, using manually annotated queries as training data • [TP08]: B. Tan and F. Peng. Unsupervised Query Segmentation Using Generative Language Models and Wikipedia. WWW-08. – identify segments of contiguous query tokens corresponding to semantic concepts, using evidence from queries and from Wikipedia documents • [BJR08]: C. Barr, R. Jones and M. Regelson. The Linguistic Structure of English Web-Search Queries. EMNLP-08. – identify structural characteristics of queries in the task of part of speech tagging • [ML09]: M. Manshadi and X. Li. Semantic Tagging of Web Search Queries. ACLIJCNLP-09. – classify queries into domains, and identify query fragments corresponding to prespecified, per-domain schema of tags • [GXC+09]: J. Guo and G. Xu and X. Cheng and H. Li. Named Entity Recognition in Query. SIGIR-09. – detect instances within queries, and classify instances into coarse-grained classes • [Li10]: X. Li. Understanding the Semantic Structure of Noun Phrase Queries. ACL-10. – represent noun phrase queries as a combination of intent heads and intent modifiers, and identify those components automatically  Finding Structure in Queries • [BJR08]: C. Barr, R. Jones and M. Regelson. The Linguistic Structure of English Web-Search Queries. EMNLP-08. Part-of-Speech Tags of Query Tokens • Task – investigate the task of part-of-speech (POS) tagging when applied to queries • Input data – set of 3.2K (2.5K unique) Web search queries, after automatic spell checking and tokenization • Manual annotation of POS tags of query tokens is unreliable – inter-annotator agreement: 0.79 (token-level), 0.65 (query-level) – main cause of annotation errors (70% of cases): actual query ambiguity (e.g., download may be a noun or a verb) rather than human annotation mistakes • POS tags have a different distribution in queries than in documents – in documents (Brown corpus): ~90 distinct tags, of which 15 for determiners, and 35 for verbs – in queries: ~20 distinct tags are sufficient, of which 1 for determiners and 1 for verbs  Suggested Part-of-Speech Tags  Part-of-Speech Example Percentage of  Tag  Token Query Tokens  proper noun  texas  40.2%  common noun pictures  30.9%  adjective  big  7.1%  URI  ebay.com  5.9%  preposition  in  3.7%  unknown  y  2.5%  verb  get  2.4%  ...  ...  ...  (Courtesy R. Jones) • Nouns are predominant in queries  – most frequent tags in documents: 13% of tokens are common nouns  – most frequent tags in queries: 40% of tokens are proper nouns, 71% of tokens are common nouns or proper nouns  • Verbs are infrequent in queries  – in documents: at least one verb in most sentences  – in queries: less than 3% of tokens  Part-of-Speech Tagging Experiments  • Use of capitalization in queries is inconsistent – 17% queries contain capitalization, of which 4% are all-caps – when a query contains mixed capitalization, first-letter token capitalization is indicative of an actual proper noun for 73% of cases – other uses of capitalization in queries: acronyms, capitalization for first token of query, first-letter capitalization for all tokens --> cannot rely on capitalization to identify proper nouns in queries  Experimental Setting tagger that assigns most frequent tag (over separate training lexicon) of each token tagger trained on annotated documents tagger trained on annotated queries tagger trained and evaluated on queries with perfect capitalization tagger trained and evaluated on queries with automatically-induced capitalization  Per-Token Tagging Accuracy 65.4% 48.2% 69.7% 89.4% 70.9%  Comparison with Other Textual Sources • [CGC+09]: M. Carman, R. Gwadera, F. Crestani and M. Baillie. A Statistical Comparison of Tag and Query Logs. SIGIR-09. – investigate similarity between vocabularies of tokens from search queries vs. tags assigned by users to Web documents • [GNL+10]: J. Gao, P. Nguyen, X. Li, C. Thrasher, M. Li and K. Wang. A Comparative Study of Bing Web N-gram Language Models for Web Search and Natural Language Processing. SIGIR 2010, Web N-gram Workshop. – generate a repository of n-grams from Web data, including from queries, and evaluate it in various text processing tasks  Characteristics of Documents vs. Queries  Characteristic  Data Source  Document Sentences Queries  Type of medium text  text  Purpose  convey info.  request info.  Available context surrounding text  self-contained  Average quality high (varies)  low  Grammatical style natural language  bag of keywords  Average length 25 words or more  2-3 words  Queries vs. Other Textual Sources • [CGC+09]: M. Carman, R. Gwadera, F. Crestani and M. Baillie. A Statistical Comparison of Tag and Query Logs. SIGIR-09.  Queries vs. Tags  • Task – investigate the similarity between query logs and user-generated tags (entered by users to annotate documents) • Input data – from query logs containing click-through data, and from Delicious (social bookmark) tags, select queries and tags associated with a set of 4K Web documents – each document clicked at least 50 times, and associated with a tag at least 20 times – generate respective vocabularies (i.e., sets) of tokens for tags and queries, after removing stop words and stemming all tokens with the Porter stemmer  Metric Token Occurrences Vocabulary Size  Queries  Tags Queries  Tags  Mean  955.3 1105.8  17.6 139.6  Std deviation 6464.7 1533.4  12.8 137.7  Median  278.0 393.0  15.0  83.0  Query vs. Tag Vocabulary • Compute overlap between query tokens and tag tokens • Optionally, remove low frequency tokens or keep high frequency tokens • Over more than half of documents, overlap ≥ 0.5 --> query vocabulary is very similar to tag vocabulary (Courtesy M. Carman) Query vs. Tag vs. Document Vocabulary • Include vocabulary of Web documents in comparison of relative overlap • Similarity between query and document vocabulary is higher than between query and tag vocabulary – since documents are clicked search results, they are likely to contain query tokens • Similarity is lowest between tag and document vocabulary – users do not necessarily enter tags that appear in document content (Courtesy M. Carman)  Repositories of Distilled Query Data • [GNL+10]: J. Gao, P. Nguyen, X. Li, C. Thrasher, M. Li and K. Wang. A Comparative Study of Bing Web N-gram Language Models for Web Search and Natural Language Processing. SIGIR 2010, Web N-gram Workshop.  Web N-Gram Collection  • Language models of n-grams, from Web documents and search queries  N-gram Length 1-grams 2-grams 3-grams 4-grams 5-grams  Body 1.2B 11.7B 60.0B 148.5B 230.0B  Documents Anchor Text 60.3M 464.1M 1.4B 2.3B N/A  Title 150M 1.1B 3.1B 5.1B N/A  Queries 251.5M 1.3B 3.1B 4.6B N/A  • Language models found to be more similar between queries and document title (and queries and document anchor text) than between queries and document body  Queries as a Corpus • Structure of queries • Comparison with other textual sources • Usage, demographics and privacy Usage, Demographics and Privacy • [MC08]: Q. Mei and K. Church. Entropy of Search Logs: How Hard is Search? With Personalization? With Backoff? WSDM-08. – investigate Web search from the perspective of entropy in search logs, and assess the impact of aggregated data about users (e.g., from IP addresses) on the outcome of Web search • [JBS08]: B. Jansen and D. Booth and A. Spink. Determining the Informational, Navigational, and Transactional Intent of Web Queries. Journal of Information Processing and Management 2008. – investigate the distribution of queries from the point of view of intent type (and subtypes), and automatically classify queries accordingly • [JBS09]: B. Jansen, D. Booth and A. Spink. Patterns of Query Reformulation During Web Searching. Journal of the American Society for Information Science and Technology 2009. – develop models to classify various types of query reformulations and identify the most frequent ones among Web users • [WC10]: Ingmar Weber and Carlos Castillo. The Demographics of Web Search. Sigir-10. – study the impact of various user demographics factors on the users’ choice of queries • [JKP+07]: R. Jones, R. Kumar, B. Pang and A. Tomkins. “I Know What You did Last Summer”: Query Logs and User Privacy. CIKM-07. – study the possibility of uncovering user identity from query logs, despite attempts to remove basic personally identifiable information from queries • [GBG+10]: S. Goel, A. Broder, E. Gabrilovich and B. Pang. Anatomy of the Long Tail: Ordinary People with Extraordinary Tastes. WSDM-10. • [KKM+09]: A. Korolova, K. Kenthapadi, N. Mishra and A. Ntoulas. Releasing Search Queries and Clicks Privately. WWW-09. – investigate methods to generate modified query log data that preserves user privacy  Query Usage  • Search zeitgeist – capture “the general intellectual, moral, and cultural climate of an era” (Merriam Webster), as reflected in the aggregation of search queries submitted by Web users  Top Global Events (2010) world cup olympics haiti earthquake oil spill ash cloud  Top Rising Queries (2010)  Entertainment  Consumer Electronics  justin bieber  ipad  shakira  iphone 4  eminem  nokia 5530  netflix  htc evo 4g  youtube videos  nokia n900  (Google Zeitgeist)  Geographical Distribution • For: ash cloud  (Google Zeitgeist)  Temporal Distribution • For: circuit city More queries submitted later during the year(s) (shopping season) More queries submitted, due to unusual event with high news coverage (Google Trends) Query Demographics • [WC10]: Ingmar Weber and Carlos Castillo. The Demographics of Web Search. Sigir-10.  Query Demographics  • Task  – investigate impact of user demographics on Web search  • Input data  – user profile data (birth year, gender, zip code)  – set of pairs of (query, clicked URL) from query logs  – census demographic data for various zip codes  (Courtesy I. Weber)  Feature  20%  Query Log Data 40% 60% 80%  Avg.  US Avg.  Per-capita income ($k)  16.0  18.9 22.4 27.7 22.7  21.6  Below poverty (%)  4.5  7.2  10.9  16.5  11.1  12.4  BA degree (%)  12.8  18.1 25.6 37.6 25.5  24.4  White (%)  61.9  78.8  88.1 94.4 76.9  75.1  Afric. Amer. (%)  0.9  2.4  5.7  15.5  4.0  12.3  Asian (%)  0.4  1.1  2.3  5.1  4.0  3.6  Non-English (%)  4.5  7.9  14.0 27.3  17.3  17.9  Year of birth  1956  1966 1974 1982 1968  1974  Role of Demographics in Web Search • Highly-discriminant queries for various user demographics Feature Query Per-capita income ($k) chris jordan electric candle warmer www.popsugar.com ns4w.org Below poverty (%) www.unitnet.com slaker kipasa www.tokbox.com BA degree (%) spencer stuart executive search insight venture partners federal circuit four seasons jackson hole (Courtesy I. Weber)  Role of Demographics in Web Search • Highly-discriminant queries for various user demographics Feature Query White (%) pulloff.com central boiler wood furnace firewood processors midwest super cub Afric. Amer. (%) trey songz bio def jam records address s2s magazine madinaonline Asian (%) sina big bang lyrics tvb series jay chou lyrics Role of Demographics in Web Search • Highly-discriminant queries for various user demographics Feature Query Year of birth, old www.johnshopkinshealthalerts.com www.envisionreports.com/vz yahoo free bridge games bnymellon.mobular.net/bnymellon/frp Year of birth, young free teen chatrooms wet seal tottaly layouts photofiltre brushes  Queries and User Privacy • [JKP+07]: R. Jones, R. Kumar, B. Pang and A. Tomkins. “I Know What You did Last Summer”: Query Logs and User Privacy. CIKM-07. Queries and User Privacy • Task – investigate the vulnerability of narrowing down the identify (demographics) of users submitting search queries, even after removal of personally identifiable information (names, numbers) from query logs • Input data – from user profile data (anonymized id, birth year, gender, zip code), select 100M profiles – from query logs, select query sessions issued by users with available profile data, for 744K users • Assessment of vulnerability – arrange data into buckets by age, gender, zip code – arrange buckets into bins, by conjunctions of age, gender, zip code – smaller bin size makes it easier to identify a particular user from the bin (especially when additional information, e.g., hobbies, is available about the user) – e.g., if input data is arranged into bins that share gender bucket, age bucket, and first 3 of 5 zip code digits (e.g., males, age 25-29, living in zip code 950xx) --> almost 100K of the 744K users fit into a bin of 100 users or less  Deriving Demographics from Queries • Identifying user gender and age – classifiers using bag-of-words features – gender identification: accuracy of 83.8% • examples of discriminative features: {bridal, makeup, hair, women’s,..} for women; {nfl, poker, male, compusa,..} for men – age identification: absolute error of 7 years (predicted vs. actual), better than always guessing the middle age point • examples of discriminative features: {myspace, pregnancy, wikipedia, mall,..} for lower age; {aarp, lottery, amazon.com, senior, repair,..} for higher age – if personally identifiable information (names and numbers) are removed from queries, both gender and age classification remain about as accurate • Identifying location (zip code) – existing classifier for locations: given query as input, output list of locations – convert list of locations into zip code buckets of known first 3, 4 or 5 digits Known Digits of Zip Code First 5 First 4 First 3 Correct at top one 6.2% 13.7% 34.9% Correct among top three 13.1% 251.% 54.1% – if personally identifiable information (names and numbers) are removed from queries, location classification becomes much less accurate  Deriving Queries from Known Information  • Identifying query sessions submitted by a known user – use demographics, conversations with, lifestyle changes of user, in order to guess queries that may have been submitted by user – as an approximation, manually create a set of guessed queries  Category Cars Sports Food  Common volkswagen beetle (478) honda odyssey (1504) toyota prius (1070) skiing (9618) football (123802) pizza (104888) italian restaurant (4998) brie (39325)  Rare triumph tr23 (23) e-type jaguar (5) bassmaster (388) skulling (17) assam (747)  Knowing that a user submitted the query etype jaguar narrows down the identity of the user to a bin of 5 possible users  Books  harry potter (27838)  holly lisle (20)  danielle steele (238) elizabeth moon (27)  freakonomics (574)  – use combinations of guessed queries (Courtesy R. Jones)  Deriving Queries from Known Information  Query Combination harry potter, pizza football, skiing italian restaurant, pizza harry potter, volkswagen beetle ... pizza, triumph tr3 brie, holly lisle, pizza danielle steele, volkswagen beetle  Bin Size 4855 2430 1441 27 ... 2 1 1  --> even if individual bits of information are far from unique among users, putting them together can uniquely identify a user  Next Topic • Part One: Introduction • Part Two: Queries as a Corpus • Part Three: Extraction from Queries  Extraction Methods • Methods for extraction of: – instances and concepts – attributes and relations  Instances and Concepts  diseases yellow fever, influenza, bipolar disorder, rocky mountain spotted fever, anosmia, myxedema,...  chemical elements potassium, magnesium, gold, sulfur, palladium, argon, carbon, borium, ruthenium, zinc, lead,...  drugs paxil, lipitor, ibuprofen, prednisone, albuterol, effexor, azithromycin, fluconazole, advil,...  foods fish, turkey, rice, milk, chicken, cheese, eggs, corn, beans, wheat, asparagus, grapes,... currencies euro, won, lire, pounds, rand, us dollars, yen, pesos, pesetas, kroner, escudos, shillings,... countries australia, south korea, kenya, greece, sudan, portugal, argentina, mexico, cuba, kuwait,...  Instances and Concepts • [Pas07]: M. Paşca. Weakly-Supervised Discovery of Named Entities using Web Search Queries. CIKM-07. – expand sets of instances using Web search queries • [VP08]: B. Van Durme and M. Paşca. Finding Cars, Goddesses and Enzymes: Parametrizable Acquisition of Labeled Instances for Open-Domain Information Extraction. AAAI-08. – extract labeled sets of instances from Web documents, by merging clusters of distributionally similar phrases with IsA pairs extracted with lexico-syntactic patterns • [PP09]: M. Pennacchiotti and P. Pantel. Entity Extraction via Ensemble Semantics. EMNLP-09. – expand sets of instances using multiple sources of text including queries • [AHH09]: E. Alfonseca and K. Hall and S. Hartmann. Large-Scale Computation of Distributional Similarities for Queries. NAACL-HLT-2009. – apply vector-space model of distributional similarities to queries rather than documents • [JP10]: A. Jain and P. Pantel. Open Entity Extraction from Web Search Query Logs. COLING-10. – extract clusters of distributionally similar phrases from Web search queries and clickthrough data Instances and Concepts • [VP08]: B. Van Durme and M. Paşca. Finding Cars, Goddesses and Enzymes: Parametrizable Acquisition of Labeled Instances for OpenDomain Information Extraction. AAAI-08.  Extraction from Documents and Queries • Input – target relation, available as a small set of extraction patterns • e.g., <C [such as|including] I> • Data sources – collection of Web documents – collection of anonymized Web search queries • Output – sets of instances, each set associated with a class label • e.g., marine animals = {whales, seals, dolphins, turtles, sea lions, fishes, penguins, squids, pacific walrus, aquatic birds, comb jellies, starfish, florida manatees, walruses,...} – each set also associated with lists of attributes Acquisition of Open-Domain Classes • Define a closed vocabulary of potential class instances, as the set of most frequently-submitted Web search queries – textual data source: Web query logs – output: noisy set of potential class instances • Acquire class labels for potential class instances, via handwritten extraction patterns – textual data source: Web documents – <C [such as|including] I>, where C is a potential class label (e.g., zoonotic diseases) and I is a potential instance (e.g., brucellosis) – output: noisy pairs of an instance and a class label • Organize potential class instances into sets of distributionally similar phrases – output: noisy sets of distributionally similar instances Merge into labeled sets of instances  Extraction of Labeled Instances  Input: - pairs of an instance and a class label - unlabeled sets of distributionally similar instances Output: - sets of instances, each set associated with a class label  For each unlabeled set of distributionally-similar instances S  For each class label L assigned to some instance(s) of set S  tf  A=set of instances of S whose class label is L  idf  B=set of sets that contain some instance(s) whose label is L  If |A| > J×|S|:  If |B| < K:  Collect instances of A, associated with the class label L  • Note: J, K are weighting parameters controlling precision/recall – J in [0,1); higher J --> higher precision – K is non-negative integer; lower K --> higher precision  Patterns and Distributional Similarities (Courtesy B. Van Durme)  hillary clinton j. carter ronald reagan  bill clinton nixon  al sharpton  george w. bush  george washington  benjamin franklin  jefferson  abe lincoln paul revere  john adams  gm  schwinn  volvo  toyota  ford  Car Companies  lettuce broccoli corn Fruits  Presidents carrot mango apple banana orange rose  Instances and Concepts • [PP09]: M. Pennacchiotti and P. Pantel. Entity Extraction via Ensemble Semantics. EMNLP-09. Extraction from Multiple Sources • Input – target classes, available as small sets of seed instances • e.g., {jodie foster, humphrey bogart, anthony hopkins} for Actor – target classes, also available as small sets of seed relations with other classes • e.g., < leonardo dicaprio, inception>, <nicole kidman, eyes wide shut> for Actor (corresponding to relation Actor-act in-Movie) • Data sources – collection of Web documents – collection of Web search queries – HTML tables identified within the collection of Web documents – collection of articles from Wikipedia • Output – ranked lists of instances, one per class • e.g., [gordon tootoosis, rosalind chao, john hawkes, jeffrey dean morgan,...] for Actor  Ensemble Semantics  FEATURE GENERATORS  S1  FG1  FG2  FGm  AGGREGATOR  KNOWLEDGE EXTRACTORS  KEn  KE2  RANKER S2 MODELER KB DECODER SK  KE1  (Courtesy P. Pantel, M. Pennacchiotti)  Extraction Components • Sources (S1, S2,..., Sk) – data sources from which instances and their relevant features are extracted • Knowledge extractors (KE1, KE2,..., KEn) – extract candidate instances from sources, using various algorithms • Feature generators (FG1, FG2,..., FGm) – collect evidence/features relevant to deciding whether candidate instances are correct or not • Aggregator – combine evidence available from multiple sources for candidate instances • Ranker – rank candidate instances extracted by knowledge extractors, based on features available from feature generators  Ranking Features • Collected by feature generators – 4 feature families: from Web documents, queries, tables, Wikipedia – 5 feature types: frequency, co-occurrence, distributional, pattern, termness (i.e., checking whether extracted terms are well-formed) (Courtesy P. Pantel, M. Pennacchiotti) Extraction Results • Input data = collection of 600 million Web documents; tables identified within the documents; one year of queries; 2 million Wikipedia articles • Evaluate lists of instances extracted for 3 classes: Actor, Athlete and Musician – create gold standard from samples of 500 instances selected randomly for each class – compute precision of extracted lists of instances, relative to and over the gold standards • Average precision: 0.860 (Actor), 0.915 (Athlete), 0.788 (Musician) • Precision@100: 0.99 (Athlete) • Estimated precision@22000: 0.97 (Athlete)  Instances and Concepts • [JP10]: A. Jain and P. Pantel. Open Entity Extraction from Web Search Query Logs. COLING-10. Extraction from Queries • Data sources – anonymized search queries along with frequencies and click-through data (clicked search results) – Web documents • Output – clusters of similar instances • e.g., {basic algebra, numerical analysis, discrete math, lattice theory, nonlinear physics, ...}, {aaa insurance, roadside assistance, personal liability insurance, international driving permits, ...} • Steps – collect set of candidate instances from queries – cluster instances using context in queries or click-through data or both  Similarity in Documents vs. Queries  • Contextual space of Web documents – an instance is represented by the contexts in which it appears in text documents – instances are modeled “objectively”, according to descriptions of the world • Contextual space of Web search queries – an instance is represented by the contexts in which it appears in a search queries – instances are modeled “subjectively”, according to users’ perception of the world  britney spears  galapagos islands  bruce springsteen  tasmania  Other singers celine dion  Other regions guinea  Contextual space of Web documents  britney spears  galapagos islands  serena williams Other celebrities paris hilton  south america cruise Other island travel topics kauai snorkeling  Contextual space of Web search queries  Extraction of Instances • Identify candidate instances – intuition: in queries composed by copying fragments from Web documents and pasting them into queries, capitalization of instances is preserved – from queries containing capitalization, extract contiguous sequences of capitalized tokens as instances Queries Candidate Instances Britney Spears new song --> Britney Spears travel to Italy Roma --> Italy Roma restaurant Cascal in Mountain View --> Cascal, Mountain View • Retain set of best candidate instances – first criterion: promote candidate instances whose capitalization is frequent in Web documents – second criterion: promote candidate instances that occur as full-length queries – retain set of candidate instances that score highly (above some thresholds) according to both criteria (Courtesy A. Jain)  Clustering of Instances • Induce unlabeled classes of instances, by clustering instances using features collected from queries – as an alternative to collecting features from unstructured text in documents – for efficiency, no attempt to parse the queries • Context features – vector of elements corresponding to contexts, where a context is the prefix and postfix around the instance, from queries containing the instance • Click-through features – vector of elements corresponding to documents, where a document is one that is clicked by a user submitting the instance as a full-length query • Hybrid features – normalized combination of context and click-through vectors  Impact of Clustering Features  • Given an instance, manually judge each co-clustered instance: – “If you were interested in instance I, would you also be interested in instance Ic in any intent?” – also, annotate with type of relation between instance and co-clustered instance • Compute precision, over a set of evaluation instances – CL-CTX: context – CL-CLK: click-through – CL-HYB: hybrid – CL-Web: context collected from Web documents rather than queries  Method CL-Web CL-CTX CL-CLK  Precision 0.73 0.46 0.81  CL-HYB  0.85  Relation Type topic sibling parent child synonym  Method CL-Web CL-CTX CL-CLK 0.27 0.46 0.46  0.72 -  0.43 0.09  0.29 0.13  0.01  -  0.01  0.01  0.03  0.12  CL-HYB 0.40 0.32 0.09 0.02 0.16  Extraction Methods • Methods for extraction of: – instances and concepts – attributes and relations  Attributes and Relations  diseases yellow fever, influenza, bipolar disorder, rocky mountain spotted fever, anosmia, myxedema,...  treatment symptoms causes incidence diagnosis  size color taste allergies calories  foods fish, turkey, rice, milk, chicken, cheese, eggs, corn, beans, wheat, asparagus, grapes,...  mass symbol atomic number electron configuration lewis dot diagram  chemical elements potassium, magnesium, gold, sulfur, palladium, argon, carbon, borium, ruthenium, zinc, lead,...  denominations country symbol exchange rate  currencies euro, won, lire, pounds, rand, us dollars, yen, pesos, pesetas, kroner,  drugs paxil, lipitor, ibuprofen, prednisone, albuterol, effexor, azithromycin, fluconazole, advil,...  side effects dosage price generic equivalent withdrawal symptoms  currency converter  escudos, shillings,...  flag climate geography currency population density  countries australia, south korea, kenya, greece, sudan, portugal, argentina, mexico, cuba, kuwait,...  Attributes and Relations  diseases yellow fever, influenza, bipolar disorder, rocky mountain spotted fever, anosmia, myxedema,...  used in the treatment of  decay product of  can reduce risk of  is a form of  good sources of  chemical elements potassium, magnesium, gold, sulfur, palladium, argon, carbon, borium, ruthenium, zinc, lead,...  drugs paxil, lipitor, ibuprofen, prednisone, albuterol, effexor, azithromycin, fluconazole, advil,...  depletes the body of brand name of  foods fish, turkey, rice, milk, chicken, cheese, eggs, corn, beans, wheat, asparagus, grapes,...  worth millions of  currency of  currencies euro, won, lire, pounds, rand, us dollars, yen, pesos, pesetas, kroner, escudos, shillings,...  countries australia, south korea, kenya, greece, sudan, portugal, argentina, mexico, cuba, kuwait,...  Attributes and Relations • [PV07]: M. Paşca and B. Van Durme. What You Seek is What You Get: Extraction of Class Attributes from Query Logs. IJCAI-07. – apply small set of patterns to extract attributes from queries • [PVG07]: M. Paşca, B. Van Durme and N. Garera. The Role of Documents vs. Queries in Extracting Class Attributes from Text. CIKM-07. – apply patterns to extract attributes from unstructured text in documents vs. queries • [Pas07]: M. Paşca. Organizing and Searching the World Wide Web of Facts Step Two: Harnessing the Wisdom of the Crowds. WWW-07. – expand sets of seed attributes using queries • [LWA09]: X. Li, Y. Wang and A. Acero. Extracting Structured Information from User Queries with Semi-Supervised Conditional Random Fields. SIGIR-09. – detect relevant fields in product-search queries, using click data and document content • [PER+10]: M. Paşca, E. Alfonseca, E. Robledo-Arnuncio, R. Martin-Brualla and K. Hall. The Role of Query Sessions in Extracting Instance Attributes from Web Search Queries. ECIR-10. – extract attributes of instances, from sequences of queries within query sessions • [YTT10]: X. Yin, W. Tan and Y. Tu. Automatic Extraction of Clickable Structured Web Contents for Name Entity Queries. WWW-10. – given a query containing an instance, extract structured data from click data and contents of subsequently visited documents • [SJY11]: A. Das Sarma, A. Jain and C. Yu. Dynamic Relationship and Event Discovery. WSDM-11. – acquire temporally-anchored relations that apply within a given set of instances, using queries and (news) documents  Attributes and Relations • [Pas07]: M. Paşca. Organizing and Searching the World Wide Web of Facts - Step Two: Harnessing the Wisdom of the Crowds. WWW-07. Extraction from Queries • Input – target classes, available as sets of representative instances • e.g., {Delphi, Apple Computer, Honda, Oracle, Coca Cola, Toyota, Washington Mutual, Delta, Reuters, Target, ...} for Company – small sets of seed attributes, one per class • e.g., {headquarters, stock price, ceo, location, chairman} for Company • Data source – anonymized search queries along with frequencies • Output – ranked (longer) lists of attributes, one per class • e.g., {headquarters, mission statement, stock price, ceo, code of conduct, stock symbol, organizational structure, corporate address, cio, ...} for Company • Steps – select candidate attributes, from queries containing an instance – create internal representation of candidate attributes, from queries containing an instance and a candidate attribute – rank candidate attributes, from similarity between internal representation of a candidate attribute and combined internal representation of all seed attributes  Class Attribute Extraction  Target classes Company: {Delphi, Apple Computer, Honda, Oracle, Coca Cola, Toyota, Washington Mutual, Delta, Reuters, Target,...} Seed attributes Company: {headquarters, stock price, ceo, location, chairman}  Pool of candidate attributes Company: {installing, stock price, accord, headquarters, mission statement,...} Query logs icdhnwmnoehaissclnwestsadhrilheaoilcainonoiagsnrglctasdlotictohnaoancoreyotaraemsdomcwcmtluscae1poeto9arou8cnlr8ndcadr.t91keylhf-sonpso7eserernaiowdticedhnaeqehylustwheaooaiasrlraardtteiorqcsrirlsutieysonpa8ceucfrokstomerrprpdsroiecirlmepahtptioaianrccgoterpt oration  Search-signature vectors (one per candidate attribute)  Company: installing  [ ] [ ] [cressida water pump]  prefix infix  postfix  [ ] [ ] [8.1-7 on solaris 8]  prefix infix  postfix  Company: stock price  [ ] [company one year] [target]  prefix  infix  postfix  [ ] [air lines] [history]  prefix  infix  postfix  Company: accord  [ ] [ ] [1989 sei]  prefix infix  postfix  [new] [ ] [ ] prefix infix postfix  Company: headquarters  [where is the world] [for] [corporation]  prefix infix  postfix  [ ] [new] [impact]  prefix infix  postfix  Company: mission statement  [ ] [for the] [corporation]  prefix  infix  postfix  [ ] [for] [airlines]  prefix infix  postfix  Reference search-signature vectors (one per class) Company  Ranked list of extracted class attributes Company: {headquarters, mission statement, stock price, ceo, code of conduct, stock symbol, organizational structure, corporate address, cio,...}  Top Extracted Attributes  Class Top Extracted Attributes  
 linguist  annotators  Example: Document Classiﬁcation  Documents  Labels  --- --- ------ -- --- ------ --- --- ---- --------------- ---- --  --- --- ------ -- --- ------ --- --- ---- --------------- ---- --  --- --- ------ -- --- ------ --- --- ---- --------------- ---- --  --- --- ------ -- --- ------ --- --- ---- --------------- ---- --  --- --- ------ -- --- ------ --- --- ---- --------------- ---- --  --- --- ------ -- --- ------ --- --- ---- --------------- ---- --  • Prior Knowledge: • labeled features: information about the label distribution when word w is present  sentiment polarity  positive negative  memorable  terrible  perfect  boring  exciting  mess  newsgroups classiﬁcation  baseball  Mac politics ...  hit  Apple  senate ...  Braves Macintosh taxes ...  runs Powerbook liberal ...  Example: Information Extraction  extraction from research papers:  W. H. Enright. Improving the efﬁciency of matrix operations in the numerical solution of stiff ordinary differential equations. ACM Trans. Math. Softw., 4(2), 127-136, June 1978.  • Prior Knowledge: • labeled features: • the word ACM should be labeled either journal or booktitle most of the time • non-Markov (long-range) dependencies: • each reference has at most one segment of each type  Example: Part-of-speech Induction  Tags  Text  A career with the European  institutions must become more  attractive.Too many young, new...  • Prior Knowledge: • linguistic knowledge: each sentence should have a verb • posterior sparsity: the total number of different POS tags assigned to each word type should be small  Example: Dependency Grammar Induction • Prior Knowledge: • linguistic rules: nouns are usually dependents of verbs • noisy labeled data: target language parses should be similar to aligned parses in a resource-rich source language Example:Word Alignment A career with the European institutions must become more attractive. Uma carreira nas instituições europeias têm de se tornar mais atractiva. • Prior Knowledge: • Bijectivity: alignment should be mostly one-to-one • Symmetry: source→target and target→source alignments should agree  This Tutorial In general, how can we leverage such knowledge and an unannotated corpus during learning?  Notation & Models  input variables (documents, sentences): structured output variables (parses, sequences): unstructured output variables (labels): input / output variables for entire corpus: probabilistic model parameters: generative models: discriminative models: model feature function:  x y y XY θ pθ(x, y) pθ (y|x) f (x, y)  Learning Scenarios • Unsupervised: • unlabeled data + prior knowledge • Lightly Supervised: • unlabeled data + “informative” prior knowledge • i.e. provides speciﬁc information about labels • Semi-Supervised: • labeled data + unlabeled data + prior knowledge  Running Example #1: Document Classiﬁcation  • model: Maximum Entropy Classiﬁer (Logistic Regression)  pθ (y |x)  =  
goal: solve complicated optimization problem y ∗ = arg max f (y ) y method: decompose into subproblems, solve iteratively beneﬁt: can choose decomposition to provide “easy” subproblems aim for simple and eﬃcient combinatorial algorithms • dynamic programming • minimum spanning tree • shortest path • min-cut • bipartite match • etc. Related work there are related methods used NLP with similar motivation related methods: • belief propagation (particularly max-product) (Smith and Eisner, 2008) • factored A* search (Klein and Manning, 2003) • exact coarse-to-ﬁne (Raphael, 2001) aim to ﬁnd exact solutions without exploring the full search space  Tutorial outline focus: • developing dual decomposition algorithms for new NLP tasks • understanding formal guarantees of the algorithms • extensions to improve exactness and select solutions outline: 1. worked algorithm for combined parsing and tagging 2. important theorems and formal derivation 3. more examples from parsing, sequence labeling, MT 4. practical considerations for implementing dual decomposition 5. relationship to linear programming relaxations 6. further variations and advanced examples 1. Worked example aim: walk through a dual decomposition algorithm for combined parsing and part-of-speech tagging • introduce formal notation for parsing and tagging • give assumptions necessary for decoding • step through a run of the dual decomposition algorithm  Combined parsing and part-of-speech tagging  S  NP  VP  N  V  NP  United ﬂies  D  A  N  some large jet  goal: ﬁnd parse tree that optimizes score(S → NP VP) + score(VP → V NP) + ... + score(United1, N) + score(V, N) + ...  Constituency parsing  notation: • Y is set of constituency parses for input • y ∈ Y is a valid parse • f (y ) scores a parse tree  goal:  arg max f (y ) y ∈Y  example: a context-free grammar for constituency parsing S  NP  VP  NV  NP  United ﬂies D A N  some large jet  Part-of-speech tagging  notation:  • Z is set of tag sequences for input • z ∈ Z is a valid tag sequence • g (z) scores of a tag sequence  goal:  arg max g (z) z ∈Z  example: an HMM for part-of speech tagging  N  V  D  A  N  United1 ﬂies2 some3 large4 jet5  Identifying tags notation: identify the tag labels selected by each model • y (i, t) = 1 when parse y selects tag t at position i • z(i, t) = 1 when tag sequence z selects tag t at position i  example: a parse and tagging with y (4, A) = 1 and z(4, A) = 1  S  NP  VP  NV  NP  United ﬂies D A N  some large jet  N  V  D  A  N  United1 ﬂies2 some3 large4 jet5  z  y  Combined optimization  goal:  arg max f (y ) + g (z) y ∈Y,z∈Z  such that for all i = 1 . . . n, t ∈ T ,  y (i, t) = z(i, t)  i.e. ﬁnd the best parse and tagging pair that agree on tag labels equivalent formulation:  arg max f (y ) + g (l(y )) y ∈Y where l : Y → Z extracts the tag sequence from a parse tree  Dynamic programming intersection can solve by solving the product of the two models example: • parsing model is a context-free grammar • tagging model is a ﬁrst-order HMM • can solve as CFG and ﬁnite-state automata intersection  replace S → NP VP with SN,N → NPN,V VPV ,N  S  NP  VP  NV  NP  United ﬂies D A N  some large jet  Parsing assumption the structure of Y is open (could be CFG, TAG, etc.)  assumption: optimization with u can be solved eﬃciently  arg max f (y ) + u(i, t)y (i, t) y ∈Y i ,t generally benign since u can be incorporated into the structure of f  example: CFG with rule scoring function h  f (y ) =  h(X → Y Z ) +  h(X → wi )  X →Y Z ∈y  (i,X )∈y  where  arg maxy∈Y arg maxy∈Y  f (y ) + u(i, t)y (i, t) =  i ,t  h(X → Y Z ) +  (h(X → wi ) + u(i, X ))  X →Y Z ∈y  (i,X )∈y  Tagging assumption we make a similar assumption for the set Z assumption: optimization with u can be solved eﬃciently arg max g (z) − u(i, t)z(i, t) z ∈Z i ,t  example: HMM with scores for transitions T and observations O  where  g (z) =  T (t → t ) +  O(t → wi )  t→t ∈z  (i ,t )∈z  arg maxz∈Z arg maxz∈Z  g (z) − u(i, t)z(i, t) =  i ,t  T (t → t ) +  (O(t → wi ) − u(i, t))  t→t ∈z  (i ,t )∈z  Dual decomposition algorithm Set u(1)(i , t) = 0 for all i , t ∈ T For k = 1 to K y (k) ← arg max f (y ) + u(k)(i , t)y (i , t) [Parsing] y ∈Y i ,t z(k) ← arg max g (z) − u(k)(i , t)z(i , t) [Tagging] z ∈Z i ,t If y (k)(i , t) = z(k)(i , t) for all i , t Return (y (k), z(k)) Else u(k+1)(i , t) ← u(k)(i , t) − αk (y (k)(i , t) − z(k)(i , t)) Algorithm step-by-step [Animation]  Main theorem theorem: if at any iteration, for all i, t ∈ T y (k)(i , t) = z(k)(i , t) then (y (k), z(k)) is the global optimum proof: focus of the next section 2. Formal properties aim: formal derivation of the algorithm given in the previous section • derive Lagrangian dual • prove three properties upper bound convergence optimality • describe subgradient method  Lagrangian goal: arg max f (y ) + g (z) such that y (i, t) = z(i, t) y ∈Y,z∈Z  Lagrangian:  L(u, y , z) = f (y ) + g (z) + u(i, t) (y (i, t) − z(i, t)) i ,t redistribute terms        L(u, y , z) = f (y ) + u(i, t)y (i, t) + g (z) − u(i, t)z(i, t)  i ,t  i ,t  Lagrangian dual  Lagrangian:        L(u, y , z) = f (y ) + u(i, t)y (i, t) + g (z) − u(i, t)z(i, t)  i ,t  i ,t  Lagrangian dual:  L(u) = max L(u, y , z) y ∈Y,z∈Z      = max f (y ) + u(i, t)y (i, t) + y ∈Y i ,t      max g (z) − u(i, t)z(i, t) z ∈Z i ,t  Theorem 1. Upper bound deﬁne: • y ∗, z∗ is the optimal combined parsing and tagging solution with y ∗(i, t) = z∗(i, t) for all i, t theorem: for any value of u L(u) ≥ f (y ∗) + g (z∗) L(u) provides an upper bound on the score of the optimal solution note: upper bound may be useful as input to branch and bound or A* search  Theorem 1. Upper bound (proof)  theorem: for any value of u, L(u) ≥ f (y ∗) + g (z∗) proof:  L(u) = max L(u, y , z)  (1)  y ∈Y,z∈Z  ≥ max L(u, y , z)  (2)  y ∈Y,z∈Z:y =z  = max f (y ) + g (z)  (3)  y ∈Y,z∈Z:y =z  = f (y ∗) + g (z∗)  (4)  Formal algorithm (reminder) Set u(1)(i , t) = 0 for all i , t ∈ T For k = 1 to K y (k) ← arg max f (y ) + u(k)(i , t)y (i , t) [Parsing] y ∈Y i ,t z(k) ← arg max g (z) − u(k)(i , t)z(i , t) [Tagging] z ∈Z i ,t If y (k)(i , t) = z(k)(i , t) for all i , t Return (y (k), z(k)) Else u(k+1)(i , t) ← u(k)(i , t) − αk (y (k)(i , t) − z(k)(i , t))  Theorem 2. Convergence notation: • u(k+1)(i , t) ← u(k)(i , t) + αk (y (k)(i , t) − z(k)(i , t)) is update • u(k) is the penalty vector at iteration k • αk is the update rate at iteration k  theorem: for any sequence α1, α2, α3, . . . such that  ∞  lim αt = 0 and  αt = ∞,  t →∞  t =1  we have  lim L(ut) = min L(u)  t →∞  u  i.e. the algorithm converges to the tightest possible upper bound  proof: by subgradient convergence (next section)  Dual solutions  deﬁne: • for any value of u      yu = arg max f (y ) + u(i, t)y (i, t) y ∈Y i ,t  and      zu = arg max g (z) − u(i, t)z(i, t) z ∈Z i ,t  • yu and zu are the dual solutions for a given u  Theorem 3. Optimality theorem: if there exists u such that yu(i , t) = zu(i , t) for all i, t then f (yu) + g (zu) = f (y ∗) + g (z∗) i.e. if the dual solutions agree, we have an optimal solution (yu, zu)  Theorem 3. Optimality (proof) theorem: if u such that yu(i, t) = zu(i, t) for all i, t then f (yu) + g (zu) = f (y ∗) + g (z∗) proof: by the deﬁnitions of yu and zu L(u) = f (yu) + g (zu) + u(i, t)(yu(i, t) − zu(i, t)) i ,t = f (yu) + g (zu) since L(u) ≥ f (y ∗) + g (z∗) for all values of u f (yu) + g (zu) ≥ f (y ∗) + g (z∗) but y ∗ and z∗ are optimal f (yu) + g (zu) ≤ f (y ∗) + g (z∗)  Dual optimization  Lagrangian dual:  L(u) = max L(u, y , z) y ∈Y,z∈Z      = max f (y ) + u(i, t)y (i, t) + y ∈Y i ,t      max g (z) − u(i, t)z(i, t) z ∈Z i ,t  goal: dual problem is to ﬁnd the tightest upper bound  min L(u) u  Dual subgradient          L(u) = max f (y ) + u(i, t)y (i, t) + max g (z) − u(i, t)z(i, t)  y ∈Y  z ∈Z  i ,t  i ,t  properties: • L(u) is convex in u (no local minima) • L(u) is not diﬀerentiable (because of max operator)  handle non-diﬀerentiability by using subgradient descent  deﬁne: a subgradient of L(u) at u is a vector gu such that for all v L(v ) ≥ L(u) + gu · (v − u)  Subgradient algorithm          L(u) = max f (y ) + u(i, t)y (i, t) + max g (z) − u(i, t)z(i, t)  y ∈Y  z ∈Z  i ,t  i ,j  recall, yu and zu are the argmax’s of the two terms subgradient: gu(i , t) = yu(i , t) − zu(i , t)  subgradient descent: move along the subgradient  u (i, t) = u(i, t) − α (yu(i, t) − zu(i, t)) guaranteed to ﬁnd a minimum with conditions given earlier for α  3. More examples aim: demonstrate similar algorithms that can be applied to other decoding applications • context-free parsing combined with dependency parsing • corpus-level part-of-speech tagging • combined translation alignment Combined constituency and dependency parsing setup: assume separate models trained for constituency and dependency parsing problem: ﬁnd constituency parse that maximizes the sum of the two models example: • combine lexicalized CFG with second-order dependency parser  Lexicalized constituency parsing  notation: • Y is set of lexicalized constituency parses for input • y ∈ Y is a valid parse • f (y ) scores a parse tree  goal:  arg max f (y ) y ∈Y  example: a lexicalized context-free grammar S(ﬂies)  NP(United) VP(ﬂies)  N  V  NP(jet)  United ﬂies D A N  some large jet  Dependency parsing deﬁne: • Z is set of dependency parses for input • z ∈ Z is a valid dependency parse • g (z) scores a dependency parse example:  *0 United1 ﬂies2 some3 large4 jet5  Identifying dependencies notation: identify the dependencies selected by each model • y (i, j) = 1 when constituency parse y selects word i as a modiﬁer of word j • z(i, j) = 1 when dependency parse z selects word i as a modiﬁer of word j example: a constituency and dependency parse with y (3, 5) = 1 and z(3, 5) = 1  S(ﬂies)  NP(United) VP(ﬂies)  N  V  NP(jet)  United ﬂies D A N  some large jet  y  *0 United1 ﬂies2 some3 large4 jet5 z  Combined optimization  goal:  arg max f (y ) + g (z) y ∈Y,z∈Z  such that for all i = 1 . . . n, j = 0 . . . n,  y (i, j) = z(i, j)  Algorithm step-by-step [Animation] Corpus-level tagging setup: given a corpus of sentences and a trained sentence-level tagging model problem: ﬁnd best tagging for each sentence, while at the same time enforcing inter-sentence soft constraints example: • test-time decoding with a trigram tagger • constraint that each word type prefer a single POS tag  Corpus-level tagging full model for corpus-level tagging N  He  saw  an  American man  The  smart  man  stood outside  Man  is  the  best measure  Sentence-level decoding notation: • Yi is set of tag sequences for input sentence i • Y = Y1 × . . . × Ym is set of tag sequences for the input corpus • Y ∈ Y is a valid tag sequence for the corpus • F (Y ) = f (Yi ) is the score for tagging the whole corpus i goal: arg max F (Y ) Y ∈Y example: decode each sentence with a trigram tagger  P  V  D  A  N  He  saw  an  American man  D  A  N  V  R  The  smart  man  stood outside  Inter-sentence constraints notation: • Z is set of possible assignments of tags to word types • z ∈ Z is a valid tag assignment • g (z) is a scoring function for assignments to word types (e.g. a hard constraint - all word types only have one tag)  example: an MRF model that encourages words of the same type to choose the same tag  z1  z2  N  N  N  N  N  N  N  A  man  man  man  man  man  man  g (z1) > g (z2)  Identifying word tags notation: identify the tag labels selected by each model • Ys (i, t) = 1 when the tagger for sentence s at position i selects tag t • z(s, i, t) = 1 when the constraint assigns at sentence s position i the tag t example: a parse and tagging with Y1(5, N) = 1 and z(1, 5, N) = 1  He  saw  an  American man  The  smart  man  stood outside  Y  man  man  man  z  Combined optimization  goal:  arg max F (Y ) + g (z) Y ∈Y,z∈Z  such that for all s = 1 . . . m, i = 1 . . . n, t ∈ T ,  Ys (i, t) = z(s, i, t)  Algorithm step-by-step [Animation]  Combined alignment (DeNero and Macherey, 2011) setup: assume separate models trained for English-to-French and French-to-English alignment problem: ﬁnd an alignment that maximizes the score of both models with soft agreement example: • HMM models for both directional alignments (assume correct alignment is one-to-one for simplicity)  English-to-French alignment  deﬁne: • Y is set of all possible English-to-French alignments • y ∈ Y is a valid alignment • f (y ) scores of the alignment  example: HMM alignment  
Hindi-Punjabi being closely related language pair (Goyal V. and Lehal G.S., 2008) , Hybrid Machine Translation approach has been used for developing Hindi to Punjabi Machine Translation System. Non-availability of lexical resources, spelling variations in the source language text, source text ambiguous words, named entity recognition and collocations are the major challenges faced while developing this syetm. The key activities involved during translation process are preprocessing, translation engine and post processing. Lookup algorithms, pattern matching algorithms etc formed the basis for solving these issues. The system accuracy has been evaluated using intelligibility test, accuracy test and BLEU score. The hybrid syatem is found to perform better than the constituent systems. Keywords: Machine Translation, Computational Linguistics, Natural Language Processing, Hindi, Punjabi. Translate Hindi to Punjabi, Closely related languages. 
We describe a novel application for structured search in scientiﬁc digital libraries. The ACL Anthology Searchbench is meant to become a publicly available research tool to query the content of the ACL Anthology. The application provides search in both its bibliographic metadata and semantically analyzed full textual content. By combining these two features, very efﬁcient and focused queries are possible. At the same time, the application serves as a showcase for the recent progress in natural language processing (NLP) research and language technology. The system currently indexes the textual content of 7,500 anthology papers from 2002–2009 with predicateargument-like semantic structures. It also provides useful search ﬁlters based on bibliographic metadata. It will be extended to provide the full anthology content and enhanced functionality based on further NLP techniques. 
Large lexical resources, such as corpora and databases of Web ngrams, are a rich source of pre-fabricated phrases that can be reused in many different contexts. However, one must be careful in how these resources are used, and noted writers such as George Orwell have argued that the use of canned phrases encourages sloppy thinking and results in poor communication. Nonetheless, while Orwell prized home-made phrases over the readymade variety, there is a vibrant movement in modern art which shifts artistic creation from the production of novel artifacts to the clever reuse of readymades or objets trouvés. We describe here a system that makes creative reuse of the linguistic readymades in the Google ngrams. Our system, the Jigsaw Bard, thus owes more to Marcel Duchamp than to George Orwell. We demonstrate how textual readymades can be identified and harvested on a large scale, and used to drive a modest form of linguistic creativity. 
We present a mobile touchable application for online topic graph extraction and exploration of web content. The system has been implemented for operation on an iPad. The topic graph is constructed from N web snippets which are determined by a standard search engine. We consider the extraction of a topic graph as a speciﬁc empirical collocation extraction task where collocations are extracted between chunks. Our measure of association strength is based on the pointwise mutual information between chunk pairs which explicitly takes their distance into account. An initial user evaluation shows that this system is especially helpful for ﬁnding new interesting information on topics about which the user has only a vague idea or even no idea at all. 
We introduce a new method for learning to detect grammatical errors in learner’s writing and provide suggestions. The method involves parsing a reference corpus and inferring grammar patterns in the form of a sequence of content words, function words, and parts-of-speech (e.g., “play ~ role in Ving” and “look forward to Ving”). At runtime, the given passage submitted by the learner is matched using an extended Levenshtein algorithm against the set of pattern rules in order to detect errors and provide suggestions. We present a prototype implementation of the proposed method, EdIt, that can handle a broad range of errors. Promising results are illustrated with three common types of errors in nonnative writing. 
Micro-blogging services provide platforms for users to share their feelings and ideas on the move. In this paper, we present a search-based demonstration system, called MemeTube, to summarize the sentiments of microblog messages in an audiovisual manner. MemeTube provides three main functions: (1) recognizing the sentiments of messages (2) generating music melody automatically based on detected sentiments, and (3) produce an animation of real-time piano playing for audiovisual display. Our MemeTube system can be accessed via: http://mslab.csie.ntu.edu.tw/memetube/ . 
Event related potentials (ERP) corresponding to stimuli in electroencephalography (EEG) can be used to detect the intent of a person for brain computer interfaces (BCI). This paradigm is widely used to build letter-byletter text input systems using BCI. Nevertheless using a BCI-typewriter depending only on EEG responses will not be sufﬁciently accurate for single-trial operation in general, and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed. Hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed. In this demonstration we will present a BCI system for typing that integrates a stochastic language model with ERP classiﬁcation to achieve speedups, via the rapid serial visual presentation (RSVP) paradigm. 
This paper presents Engkoo 1, a system for exploring and learning language. It is built primarily by mining translation knowledge from billions of web pages - using the Internet to catch language in motion. Currently Engkoo is built for Chinese users who are learning English; however the technology itself is language independent and can be extended in the future. At a system level, Engkoo is an application platform that supports a multitude of NLP technologies such as cross language retrieval, alignment, sentence classiﬁcation, and statistical machine translation. The data set that supports this system is primarily built from mining a massive set of bilingual terms and sentences from across the web. Speciﬁcally, web pages that contain both Chinese and English are discovered and analyzed for parallelism, extracted and formulated into clear term deﬁnitions and sample sentences. This approach allows us to build perhaps the world’s largest lexicon linking both Chinese and English together - at the same time covering the most up-to-date terms as captured by the net. 
Sentiment analysis is one of the hot demanding research areas since last few decades. Although a formidable amount of research have been done, the existing reported solutions or available systems are still far from perfect or do not meet the satisfaction level of end users’. The main issue is the various conceptual rules that govern sentiment and there are even more clues (possibly unlimited) that can convey these concepts from realization to verbalization of a human being. Human psychology directly relates to the unrevealed clues and governs the sentiment realization of us. Human psychology relates many things like social psychology, culture, pragmatics and many more endless intelligent aspects of civilization. Proper incorporation of human psychology into computational sentiment knowledge representation may solve the problem. In the present paper we propose a template based online interactive gaming technology, called Dr Sentiment to automatically create the PsychoSentiWordNet involving internet population. The PsychoSentiWordNet is an extension of SentiWordNet that presently holds human psychological knowledge on a few aspects along with sentiment knowledge. 
We present BLAST, an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part of MT development, since it gives a qualitative view, which is not obtained by standard evaluation methods. BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be ﬂexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation. 
Virtual instructors can be used in several applications, ranging from trainers in simulated worlds to non player characters for virtual games. In this paper we present a novel algorithm for rapidly prototyping virtual instructors from human-human corpora without manual annotation. Automatically prototyping full-ﬂedged dialogue systems from corpora is far from being a reality nowadays. Our algorithm is restricted in that only the virtual instructor can perform speech acts while the user responses are limited to physical actions in the virtual world. We evaluate a virtual instructor, generated using this algorithm, with human users. We compare our results both with human instructors and rule-based virtual instructors hand-coded for the same task. 
State-of-the-art Machine Translation (MT) systems are still far from being perfect. An alternative is the so-called Interactive Machine Translation (IMT) framework, where the knowledge of a human translator is combined with the MT system. We present a statistical IMT system able to learn from user feedback by means of the application of online learning techniques. These techniques allow the MT system to update the parameters of the underlying models in real time. According to empirical results, our system outperforms the results of conventional IMT systems. To the best of our knowledge, this online learning capability has never been provided by previous IMT systems. Our IMT system is implemented in C++, JavaScript, and ActionScript; and is publicly available on the Web. 
We present Wikulu1, a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface. Wikulu is implemented as an extensible architecture which transparently integrates natural language processing (NLP) techniques with wikis. It is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of NLP algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. Additionally, we show how Wikulu can be applied for visually analyzing the results of NLP algorithms, educational purposes, and enabling semantic wikis. 
The Automatic Content Linking Device is a just-in-time document retrieval system which monitors an ongoing conversation or a monologue and enriches it with potentially related documents, including multimedia ones, from local repositories or from the Internet. The documents are found using keyword-based search or using a semantic similarity measure between documents and the words obtained from automatic speech recognition. Results are displayed in real time to meeting participants, or to users watching a recorded lecture or conversation. 
MACAON is a tool suite for standard NLP tasks developed for French. MACAON has been designed to process both human-produced text and highly ambiguous word-lattices produced by NLP tools. MACAON is made of several native modules for common tasks such as a tokenization, a part-of-speech tagging or syntactic parsing, all communicating with each other through XML ﬁles . In addition, exchange protocols with external tools are easily deﬁnable. MACAON is a fast, modular and open tool, distributed under GNU Public License. 
 2 In-vehicle dialogue systems  This paper describes Dico II+, an in-vehicle dialogue system demonstrating a novel combination of ﬂexible multimodal menu-based dialogueand a “speech cursor” which enables menu navigation as well as browsing long list using haptic input and spoken output. 
We present an open-source toolkit which allows (i) to reconstruct past states of Wikipedia, and (ii) to efﬁciently access the edit history of Wikipedia articles. Reconstructing past states of Wikipedia is a prerequisite for reproducing previous experimental work based on Wikipedia. Beyond that, the edit history of Wikipedia articles has been shown to be a valuable knowledge source for NLP, but access is severely impeded by the lack of efﬁcient tools for managing the huge amount of provided data. By using a dedicated storage format, our toolkit massively decreases the data volume to less than 2% of the original size, and at the same time provides an easy-to-use interface to access the revision data. The language-independent design allows to process any language represented in Wikipedia. We expect this work to consolidate NLP research using Wikipedia in general, and to foster research making use of the knowledge encoded in Wikipedia’s edit history. 
We introduce a new publicly available tool that implements efﬁcient indexing and retrieval of large N-gram datasets, such as the Web1T 5-gram corpus. Our tool indexes the entire Web1T dataset with an index size of only 100 MB and performs a retrieval of any N-gram with a single disk access. With an increased index size of 420 MB and duplicate data, it also allows users to issue wild card queries provided that the wild cards in the query are contiguous. Furthermore, we also implement some of the smoothing algorithms that are designed speciﬁcally for large datasets and are shown to yield better language models than the traditional ones on the Web1T 5gram corpus (Yuret, 2008). We demonstrate the effectiveness of our tool and the smoothing algorithms on the English Lexical Substitution task by a simple implementation that gives considerable improvement over a basic language model. 
Emerging text-intensive enterprise applications such as social analytics and semantic search pose new challenges of scalability and usability to Information Extraction (IE) systems. This paper presents SystemT, a declarative IE system that addresses these challenges and has been deployed in a wide range of enterprise applications. SystemT facilitates the development of high quality complex annotators by providing a highly expressive language and an advanced development environment. It also includes a cost-based optimizer and a high-performance, ﬂexible runtime with minimum memory footprint. We present SystemT as a useful resource that is freely available, and as an opportunity to promote research in building scalable and usable IE systems. 
In this demo, we present SciSumm, an interactive multi-document summarization system for scientiﬁc articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the generalizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD). 
In this paper we present Clairlib, an opensource toolkit for Natural Language Processing, Information Retrieval, and Network Analysis. Clairlib provides an integrated framework intended to simplify a number of generic tasks within and across those three areas. It has a command-line interface, a graphical interface, and a documented API. Clairlib is compatible with all the common platforms and operating systems. In addition to its own functionality, it provides interfaces to external software and corpora. Clairlib comes with a comprehensive documentation and a rich set of tutorials and visual demos. 
Social networking and micro-blogging sites are stores of opinion-bearing content created by human users. We describe C-Feel-It, a system which can tap opinion content in posts (called tweets) from the micro-blogging website, Twitter. This web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. We present a qualitative evaluation of this system based on a human-annotated tweet corpus.  weighted-majority voting principle is used to predict sentiment of a tweet. An overall sentiment score for the search string is assigned based on the results of predictions for the tweets fetched. This score which is represented as a percentage value gives a live snapshot of the sentiment of users about the topic. The rest of the paper is organized as follows: Section 2 gives background study of Twitter and related work in the context of sentiment analysis for Twitter. The system architecture is explained in section 3. A qualitative evaluation of our system based on annotated data is described in section 4. Section 5 summarizes the paper and points to future work.  
This paper presents a system to summarize a Microblog post and its responses with the goal to provide readers a more constructive and concise set of information for efficient digestion. We introduce a novel two-phase summarization scheme. In the first phase, the post plus its responses are classified into four categories based on the intention, interrogation, sharing, discussion and chat. For each type of post, in the second phase, we exploit different strategies, including opinion analysis, response pair identification, and response relevancy detection, to summarize and highlight critical information to display. This system provides an alternative thinking about machinesummarization: by utilizing AI approaches, computers are capable of constructing deeper and more user-friendly abstraction. 
This demonstration presents the Annotation Librarian, an application programming interface that supports rapid development of natural language processing (NLP) projects built in Apache Unstructured Information Management Architecture (UIMA). The flexibility of UIMA to support all types of unstructured data – images, audio, and text – increases the complexity of some of the most common NLP development tasks. The Annotation Librarian interface handles these common functions and allows the creation and management of annotations by mirroring Java methods used to manipulate Strings. The familiar syntax and NLP-centric design allows developers to adopt and rapidly develop NLP algorithms in UIMA. The general functionality of the interface is described in relation to the use cases that necessitated its creation. 
In this paper, we present a new word alignment combination approach on language pairs where one language has no explicit word boundaries. Instead of combining word alignments of different models (Xiang et al., 2010), we try to combine word alignments over multiple monolingually motivated word segmentation. Our approach is based on link confidence score defined over multiple segmentations, thus the combined alignment is more robust to inappropriate word segmentation. Our combination algorithm is simple, efficient, and easy to implement. In the Chinese-English experiment, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations. 
In summarization, sentence ordering is conducted to enhance summary readability by accommodating text coherence. We propose a grouping-based ordering framework that integrates local and global coherence concerns. Summary sentences are grouped before ordering is applied on two levels: group-level and sentence-level. Different algorithms for grouping and ordering are discussed. The preliminary results on single-document news datasets demonstrate the advantage of our method over a widely accepted method. 
In this thesis proposal I present my thesis work, about pre- and postprocessing for statistical machine translation, mainly into Germanic languages. I focus my work on four areas: compounding, deﬁnite noun phrases, reordering, and error correction. Initial results are positive within all four areas, and there are promising possibilities for extending these approaches. In addition I also focus on methods for performing thorough error analysis of machine translation output, which can both motivate and evaluate the studies performed. 
Named entity disambiguation is the task of linking an entity mention in a text to the correct real-world referent predeﬁned in a knowledge base, and is a crucial subtask in many areas like information retrieval or topic detection and tracking. Named entity disambiguation is challenging because entity mentions can be ambiguous and an entity can be referenced by different surface forms. We present an approach that exploits Wikipedia relations between entities co-occurring with the ambiguous form to derive a range of novel features for classifying candidate referents. We ﬁnd that our features improve disambiguation results signiﬁcantly over a strong popularity baseline, and are especially suitable for recognizing entities not contained in the knowledge base. Our system achieves state-of-the-art results on the TAC-KBP 2009 dataset. 
This paper describes a method for automatically extracting and classifying multiword expressions (MWEs) for Urdu on the basis of a relatively small unannotated corpus (around 8.12 million tokens). The MWEs are extracted by an unsupervised method and classiﬁed into two distinct classes, namely locations and person names. The classiﬁcation is based on simple heuristics that take the co-occurrence of MWEs with distinct postpositions into account. The resulting classes are evaluated against a hand-annotated gold standard and achieve an f-score of 0.5 and 0.746 for locations and persons, respectively. A target application is the Urdu ParGram grammar, where MWEs are needed to generate a more precise syntactic and semantic analysis. 
Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic. 
In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. I propose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. I also present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese. 
We present ConsentCanvas, a system which structures and “texturizes” End-User License Agreement (EULA) documents to be more readable. The system aims to help users better understand the terms under which they are providing their informed consent. ConsentCanvas receives unstructured text documents as input and uses unsupervised natural language processing methods to embellish the source document using a linked stylesheet. Unlike similar usable security projects which employ summarization techniques, our system preserves the contents of the source document, minimizing the cognitive and legal burden for both the end user and the licensor. Our system does not require a corpus for training. 
Temporal–contrastive discourse connectives (although, while, since, etc.) signal various types of relations between clauses such as temporal, contrast, concession and cause. They are often ambiguous and therefore difﬁcult to translate from one language to another. We discuss several new and translation-oriented experiments for the disambiguation of a speciﬁc subset of discourse connectives in order to correct some of the translation errors made by current statistical machine translation systems. 
Sentiment analysis is one of the hot demanding research areas since last few decades. Although a formidable amount of research has been done but still the existing reported solutions or available systems are far from perfect or to meet the satisfaction level of end user's. The main issue may be there are many conceptual rules that govern sentiment, and there are even more clues (possibly unlimited) that can convey these concepts from realization to verbalization of a human being. Human psychology directly relates to the unrevealed clues; govern the sentiment realization of us. Human psychology relates many things like social psychology, culture, pragmatics and many more endless intelligent aspects of civilization. Proper incorporation of human psychology into computational sentiment knowledge representation may solve the problem. PsychoSentiWordNet is an extension over SentiWordNet that holds human psychological knowledge and sentiment knowledge simultaneously. 
This paper describes a backtracking strategy for an incremental deterministic transitionbased parser for HPSG. The method could theoretically be implemented on any other transition-based parser with some adjustments. In this paper, the algorithm is evaluated on CuteForce, an efﬁcient deterministic shiftreduce HPSG parser. The backtracking strategy may serve to improve existing parsers, or to assess if a deterministic parser would beneﬁt from backtracking as a strategy to improve parsing. 
Relation extraction in documents allows the detection of how entities being discussed in a document are related to one another (e.g. partof). This paper presents an analysis of a relation extraction system based on prior work but applied to the J.D. Power and Associates Sentiment Corpus to examine how the system works on documents from a range of social media. The results are examined on three different subsets of the JDPA Corpus, showing that the system performs much worse on documents from certain sources. The proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs. 
Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure’s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation. 
We propose a framework for generating an abstractive summary from a semantic model of a multimodal document. We discuss the type of model required, the means by which it can be constructed, how the content of the model is rated and selected, and the method of realizing novel sentences for the summary. To this end, we introduce a metric called information density used for gauging the importance of content obtained from text and graphical sources. 
Sentiment analysis of citations in scientiﬁc papers and articles is a new and interesting problem due to the many linguistic differences between scientiﬁc texts and other genres. In this paper, we focus on the problem of automatic identiﬁcation of positive and negative sentiment polarity in citations to scientiﬁc papers. Using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, including n-grams, specialised science-speciﬁc lexical features, dependency relations, sentence splitting and negation features. Our results show that 3-grams and dependencies perform best in this task; they outperform the sentence splitting, science lexicon and negation based features. 
Allophonic rules are responsible for the great variety in phoneme realizations. Infants can not reliably infer abstract word representations without knowledge of their native allophonic grammar. We explore the hypothesis that some properties of infants’ input, referred to as indicators, are correlated with allophony. First, we provide an extensive evaluation of individual indicators that rely on distributional or lexical information. Then, we present a ﬁrst evaluation of the combination of indicators of different types, considering both logical and numerical combinations schemes. Though distributional and lexical indicators are not redundant, straightforward combinations do not outperform individual indicators. 
Most spoken dialogue systems are still lacking in their ability to accurately model the complex process that is human turntaking. This research analyzes a humanhuman tutoring corpus in order to identify prosodic turn-taking cues, with the hopes that they can be used by intelligent tutoring systems to predict student turn boundaries. Results show that while there was variation between subjects, three features were significant turn-yielding cues overall. In addition, a positive relationship between the number of cues present and the probability of a turn yield was demonstrated. 
We consider the problem of predicting which words a student will click in a vocabulary learning system. Often a language learner will ﬁnd value in the ability to look up the meaning of an unknown word while reading an electronic document by clicking the word. Highlighting words likely to be unknown to a reader is attractive due to drawing his or her attention to it and indicating that information is available. However, this option is usually done manually in vocabulary systems and online encyclopedias such as Wikipedia. Furthurmore, it is never on a per-user basis. This paper presents an automated way of highlighting words likely to be unknown to the speciﬁc user. We present related work in search engine ranking, a description of the study used to collect click data, the experiment we performed using the random forest machine learning algorithm and ﬁnish with a discussion of future work. 
Turkish is an agglutinative language with complex morphological structures, therefore using only word forms is not enough for many computational tasks. In this paper we analyze the effect of morphology in a Named Entity Recognition system for Turkish. We start with the standard word-level representation and incrementally explore the effect of capturing syntactic and contextual properties of tokens. Furthermore, we also explore a new representation in which roots and morphological features are represented as separate tokens instead of representing only words as tokens. Using syntactic and contextual properties with the new representation provide an 7.6% relative improvement over the baseline. 
 talking event. In the sentence John thinks Mary is  In my thesis, I propose to build a system that would enable extraction of social interactions from texts. To date I have deﬁned a comprehensive set of social events and built a preliminary system that extracts social events from news articles. I plan to improve the performance of my current system by incorporating  great, only John is aware of Mary and the event is the thinking event. My thesis will introduce a novel way of constructing networks by analyzing text to capture such interactions or events. Motivation: Typically researchers construct a social network from various forms of electronic interaction records like self-declared friendship links,  semantic information. Using domain adaptation techniques, I propose to apply my system to a wide range of genres. By extracting linguistic constructs relevant to social interactions, I will be able to empirically analyze different kinds of linguistic constructs that people use to express social interactions. Lastly, I will attempt to make convolution kernels more scalable and interpretable.  sender-receiver email links and phone logs etc. They ignore a vastly rich network present in the content of such sources. Secondly, many rich sources of social networks remain untouched simply because there is no meta-data associated with them (literary texts, new stories, historical texts). By providing a methodology for analyzing language to extract interaction links between people, my work will over-  come both these limitations. Moreover, by empiri-  
Arabic language is a morphologically complex language. Affixes and clitics are regularly attached to stems which make direct comparison between words not practical. In this paper we propose a new automatic headline generation technique that utilizes character cross-correlation to extract best headlines and to overcome the Arabic language complex morphology. The system that uses character cross-correlation achieves ROUGE-L score of 0.19384 while the exact word matching scores only 0.17252 for the same set of documents. 
 Recently, Weinberger et al. (2009) introduced fea-  One of the major problems of K-means is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional. We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our  ture hashing, a simple yet effective and analyzable dimension-reduction technique for large-scale multitask learning. The idea is to combine features which have the same hash value. For example, given a hash function h and a vector x, if h(1012) = h(41234) = 42, we make a new vector y by setting y42 = x1012 + x41234 (or equally possibly x1012−x41234, −x1012+x41234, or −x1012−x41234).  analysis gives theoretical motivation and justiﬁcation for applying feature hashing to Kmeans, by showing how much will the objective of K-means be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task.  This trick greatly reduces the size of dense vectors, since the maximum index value becomes equivalent to the maximum hash value of h. Furthermore, unlike random projection (Achlioptas, 2003; Boutsidis et al., 2010), feature hashing retains spar-  
In this paper we introduce a novel use of the lexicographic semiring and motivate its use for speech and language processing tasks. We prove that the semiring allows for exact encoding of backoff models with epsilon transitions. This allows for off-line optimization of exact models represented as large weighted ﬁnite-state transducers in contrast to implicit (on-line) failure transition representations. We present preliminary empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions, the use of the more powerful lexicographic semiring is competitive in terms of time of intersection. 
Active Learning (AL) is typically initialized with a small seed of examples selected randomly. However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. The evaluation is conducted in the context of word sense disambiguation. 
We propose a generative model based on Temporal Restricted Boltzmann Machines for transition based dependency parsing. The parse tree is built incrementally using a shiftreduce parse and an RBM is used to model each decision step. The RBM at the current time step induces latent features with the help of temporal connections to the relevant previous steps which provide context information. Our parser achieves labeled and unlabeled attachment scores of 88.72% and 91.65% respectively, which compare well with similar previous models and the state-of-the-art. 
We describe a novel mechanism called Reservoir Counting for application in online Locality Sensitive Hashing. This technique allows for signiﬁcant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint.  
We investigate the empirical behavior of ngram discounts within and across domains. When a language model is trained and evaluated on two corpora from exactly the same domain, discounts are roughly constant, matching the assumptions of modiﬁed Kneser-Ney LMs. However, when training and test corpora diverge, the empirical discount grows essentially as a linear function of the n-gram count. We adapt a Kneser-Ney language model to incorporate such growing discounts, resulting in perplexity improvements over modiﬁed Kneser-Ney and Jelinek-Mercer baselines.  cross-domain setting. We ﬁnd that, when training and testing on corpora that are as similar as possible, empirical discounts indeed do not grow with ngram count, which validates the parametric assumption of Kneser-Ney smoothing. However, when the train and evaluation corpora differ, even slightly, discounts generally exhibit linear growth in the count of the n-gram, with the amount of growth being closely correlated with the corpus divergence. Finally, we build a language model exploiting a parametric form of the growing discount and show perplexity gains of up to 5.4% over modiﬁed Kneser-Ney. 2 Discount Analysis  
In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method. 
The written form of Arabic, Modern Standard Arabic (MSA), differs quite a bit from the spoken dialects of Arabic, which are the true “native” languages of Arabic speakers used in daily life. However, due to MSA’s prevalence in written form, almost all Arabic datasets have predominantly MSA content. We present the Arabic Online Commentary Dataset, a 52M-word monolingual dataset rich in dialectal content, and we describe our long-term annotation effort to identify the dialect level (and dialect itself) in each sentence of the dataset. So far, we have labeled 108K sentences, 41% of which as having dialectal content. We also present experimental results on the task of automatic dialect identiﬁcation, using the collected labels for training and evaluation. 
We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets. 
This paper introduces a new training set condensation technique designed for mixtures of labeled and unlabeled data. It ﬁnds a condensed set of labeled and unlabeled data points, typically smaller than what is obtained using condensed nearest neighbor on the labeled data only, and improves classiﬁcation accuracy. We evaluate the algorithm on semisupervised part-of-speech tagging and present the best published result on the Wall Street Journal data set. 
Transliteration, a rich source of proper noun spelling variations, is usually recognized by phonetic- or spelling-based models. However, a single model cannot deal with different words from different language origins, e.g., “get” in “piaget” and “target.” Li et al. (2007) propose a method which explicitly models and classiﬁes the source language origins and switches transliteration models accordingly. This model, however, requires an explicitly tagged training set with language origins. We propose a novel method which models language origins as latent classes. The parameters are learned from a set of transliterated word pairs via the EM algorithm. The experimental results of the transliteration task of Western names to Japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes. 
Beginning with Goldsmith (1976), the phonological tier has a long history in phonological theory to describe non-local phenomena. This paper deﬁnes a class of formal languages, the Tier-based Strictly Local languages, which begin to describe such phenomena. Then this class is located within the Subregular Hierarchy (McNaughton and Papert, 1971). It is found that these languages contain the Strictly Local languages, are star-free, are incomparable with other known sub-star-free classes, and have other interesting properties. 
 1.1 Attribution of translated texts  We investigate authorship attribution using classiﬁers based on frame semantics. The purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, speciﬁcally to address the difﬁcult problem of authorship attribution of translated texts. Our results suggest (i) that frame-based classiﬁers are usable for author attribution of both translated and untranslated texts; (ii) that framebased classiﬁers generally perform worse than the baseline classiﬁers for untranslated texts, but (iii) perform as well as, or superior to the baseline classiﬁers on translated texts; (iv) that—contrary to current belief—naïve classiﬁers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classiﬁer. 
Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and speciﬁc models are designed to tackle each type. In this paper, we propose a uniﬁed letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level alignment for model training. Experiments on both Twitter and SMS messages show that our system signiﬁcantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18.16% over jazzy spell checker on the two test sets respectively). 
This paper describes an unsupervised, language-independent model for ﬁnding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation. 
Community-based knowledge forums, such as Wikipedia, are susceptible to vandalism, i.e., ill-intentioned contributions that are detrimental to the quality of collective intelligence. Most previous work to date relies on shallow lexico-syntactic patterns and metadata to automatically detect vandalism in Wikipedia. In this paper, we explore more linguistically motivated approaches to vandalism detection. In particular, we hypothesize that textual vandalism constitutes a unique genre where a group of people share a similar linguistic behavior. Experimental results suggest that (1) statistical models give evidence to unique language styles in vandalism, and that (2) deep syntactic patterns based on probabilistic context free grammars (PCFG) discriminate vandalism more effectively than shallow lexicosyntactic patterns based on n-grams. 
Humor identiﬁcation is a hard natural language understanding problem. We identify a subproblem — the “that’s what she said” problem — with two distinguishing characteristics: (1) use of nouns that are euphemisms for sexually explicit nouns and (2) structure common in the erotic domain. We address this problem in a classiﬁcation approach that includes features that model those two characteristics. Experiments on web data demonstrate that our approach improves precision by 12% over baseline techniques that use only word-based features. 
Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identiﬁcation of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in signiﬁcantly increased understanding of user intent, compared to two strong baselines. 
 text allowing us to infer that someone has a partic-  We investigate systems that identify opinion expressions and assigns polarities to the extracted expressions. In particular, we demonstrate the beneﬁt of integrating opinion extraction and polarity classiﬁcation into a joint model using features reﬂecting the global polarity structure. The model is trained using  ular feeling about some topic. Then, opinions can be assigned a polarity describing whether the feeling is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the  large-margin structured prediction methods.  global sentence structure into account in (Johansson  The system is evaluated on the MPQA opinion  and Moschitti, 2010b); later we also added holder  corpus, where we compare it to the only previously published end-to-end system for opinion expression extraction and polarity classiﬁcation. The results show an improvement of between 10 and 15 absolute points in F-measure.  extraction (Johansson and Moschitti, 2010a). For the task of classiﬁying the polarity of a given expression, there has been fairly extensive work on suitable classiﬁcation features (Wilson et al., 2009). While the tasks of expression detection and polar-  
This opinion paper discusses subjective natural language problems in terms of their motivations, applications, characterizations, and implications. It argues that such problems deserve increased attention because of their potential to challenge the status of theoretical understanding, problem-solving methods, and evaluation techniques in computational linguistics. The author supports a more holistic approach to such problems; a view that extends beyond opinion mining or sentiment analysis. 
In conversation, when speech is followed by a backchannel, evidence of continued engagement by one’s dialogue partner, that speech displays a combination of cues that appear to signal to one’s interlocutor that a backchannel is appropriate. We term these cues backchannel-preceding cues (BPC)s, and examine the Columbia Games Corpus for evidence of entrainment on such cues. Entrainment, the phenomenon of dialogue partners becoming more similar to each other, is widely believed to be crucial to conversation quality and success. Our results show that speaking partners entrain on BPCs; that is, they tend to use similar sets of BPCs; this similarity increases over the course of a dialogue; and this similarity is associated with measures of dialogue coordination and task success. 
 pages. We compare the performance of a ques-  tion detector trained on the text domain using lex-  We investigate the use of textual Internet conversations for detecting questions in spoken conversations. We compare the text-trained model with models trained on manuallylabeled, domain-matched spoken utterances with and without prosodic features. Overall, the text-trained model achieves over 90% of the performance (measured in Area Under the Curve) of the domain-matched model including prosodic features, but does especially poorly on declarative questions. We describe efforts to utilize unlabeled spoken utterances and prosodic features via domain adaptation.  ical features with one trained on MRDA using lexical features and/or prosodic features. In addition, we experiment with two unsupervised domain adaptation methods to incorporate unlabeled MRDA utterances into the text-based question detector. The goal is to use the unlabeled domain-matched data to bridge stylistic differences as well as to incorporate the prosodic features, which are unavailable in the labeled text data. 2 Related Work Question detection can be viewed as a subtask of  speech act or dialogue act tagging, which aims  
 iugene ghrnik Department of Computer Science Brown University, Providence, RI 02912 ec@cs.brown.edu  estrt e extend the populr entity grid representE tion for lol oherene modelingF he grid strts wy informtion out the entities it modelsY we dd disourse promineneD nmed entity type nd oreferene fetures to distinE guish etween importnt nd unimportnt enE titiesF e improve the est result for WSJ doE ument disrimintion y T7F I sntrodution e wellEwritten doument is oherent @rllidy nd rsnD IWUTA it strutures informtion so tht eh new piee of informtion is interpretle given the preeding ontextF wodels tht distinguish oherent from inoherent douments re widely used in genE ertionD summriztion nd text evlutionF emong the most populr models of oherene is the entity grid @frzily nd vptD PHHVAD  stE tistil model sed on gentering heory @qrosz et lFD IWWSAF he grid models the wy texts fous on importnt entitiesD ssigning them repetedly to prominent syntti rolesF hile the grid hs een suessful in  vriety of pplitionsD it is still  surprisingly unsophistited modelD nd there hve een few diret improvements to its simple feture setF e present n extension to the entity grid whih distinguishes etween different types of entityD reE sulting in signi¢nt gins in performne1F et its oreD the grid model works y prediting whether n entity will pper in the next sentene 1e puli implementtion is ville vi https:// bitbucket.org/melsner/browncoherenceF 
 2 ISO-TimeML  This article presents the main points in the creation of the French TimeBank (Bittar, 2010), a reference corpus annotated according to the ISO-TimeML standard for temporal annotation. A number of improvements were made to the markup language to deal with linguistic phenomena not yet covered by ISO-TimeML, including cross-language modiﬁcations and others speciﬁc to French. An automatic preannotation system was used to speed up the annotation process. A preliminary evaluation of the methodology adopted for this project yields positive results in terms of data quality and annotation time. 
Web search is an information-seeking activity. Often times, this amounts to a user seeking answers to a question. However, queries, which encode user’s information need, are typically not expressed as full-length natural language sentences — in particular, as questions. Rather, they consist of one or more text fragments. As humans become more searchengine-savvy, do natural-language questions still have a role to play in web search? Through a systematic, large-scale study, we ﬁnd to our surprise that as time goes by, web users are more likely to use questions to express their search intent. 
Previous work on quantifier scope annotation focuses on scoping sentences with only two quantified noun phrases (NPs), where the quantifiers are restricted to a predefined list. It also ignores negation, modal/logical operators, and other sentential adverbials. We present a comprehensive scope annotation scheme. We annotate the scope interaction between all scopal terms in the sentence from quantifiers to scopal adverbials, without putting any restriction on the number of scopal terms in a sentence. In addition, all NPs, explicitly quantified or not, with no restriction on the type of quantification, are investigated for possible scope interactions. 
Mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We ﬁrst use the bilingual dictionary to ﬁnd candidate document alignments and then use them to ﬁnd an interlingual representation. Since the candidate alignments are noisy, we develop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain. 
This work introduces AM-FM, a semantic framework for machine translation evaluation. Based upon this framework, a new evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated. The metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively. Comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks (overall quality assessment and comparative ranking) over a large collection of human evaluations involving five European languages. Finally, the main pros and cons of the proposed framework are discussed along with future research directions. 
Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with characterlevel metrics on the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding. 
Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1 point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 
 framework for learning word pair scores as a function  of arbitrary features of that pair. These approaches,  We cast the word alignment problem as maximizing a submodular function under matroid constraints. Our framework is able to express complex interactions between alignment components while remaining computationally efﬁcient, thanks to the power and generality of submodular functions. We show that submodularity naturally arises when modeling word  however, have two potentially substantial limitations: words have fertility of at most one, and interactions between alignment decisions are not representable. Lacoste-Julien et al. (2006) address this issue by formulating the alignment problem as a quadratic assignment problem, and off-the-shelf integer linear programming (ILP) solvers are used to solve to op-  fertility. Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches.  timization problem. While efﬁcient for some median scale problems, ILP-based approaches are limited since when modeling more sophisticated interactions, the number of variables (and/or constraints) required grows polynomially, or even exponentially, making  
In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately. 
In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in ﬁnding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. 
 greedy one-best search, and positive results were re-  Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signﬁcant improvement of the state of the art. An open source release of our parser is freely available.  ported. In the aspect of training, global structural learning has been used to replace local learning on each decision (Zhang and Clark, 2008; Huang et al., 2009), although the effect of global learning has not been separated out and studied alone. In this short paper, we study a third aspect in a statistical system: feature deﬁnition. Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing accuracy. Various recent attempts have been made to include non-local features into graph-based dependency parsing (Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Transition-  based parsing, by contrast, can easily accommodate  
An attractive property of attribute-value grammars is their reversibility. Attribute-value grammars are usually coupled with separate statistical components for parse selection and ﬂuency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and ﬂuency ranking. 
Graph-based dependency parsing can be sped up signiﬁcantly if implausible arcs are eliminated from the search-space before parsing begins. State-of-the-art methods for arc ﬁltering use separate classiﬁers to make pointwise decisions about the tree; they label tokens with roles such as root, leaf, or attaches-tothe-left, and then ﬁlter arcs accordingly. Because these classiﬁers overlap substantially in their ﬁltering consequences, we propose to train them jointly, so that each classiﬁer can focus on the gaps of the others. We integrate the various pointwise decisions as latent variables in a single arc-level SVM classiﬁer. This novel framework allows us to combine nine pointwise ﬁlters, and adjust their sensitivity using a shared threshold based on arc length. Our system ﬁlters 32% more arcs than the independently-trained classiﬁers, without reducing ﬁltering speed. This leads to faster parsing with no reduction in accuracy.  son, 2007), one for each ordered word-pair in the sentence. Potential arcs are scored using rich linear models that are discriminatively trained to maximize parsing accuracy (McDonald et al., 2005). The vast majority of these arcs are bad; in an n-word sentence, only n of the n2 potential arcs are correct. If many arcs can be ﬁltered before parsing begins, then the entire process can be sped up substantially. Previously, we proposed a cascade of ﬁlters to prune potential arcs (Bergsma and Cherry, 2010). One stage of this cascade operates one token at a time, labeling each token t according to various roles in the tree: • Not-a-head (NaH ): t is not the head of any arc • Head-to-left (HtL{1/5/*}): t’s head is to its left within 1, 5 or any number of words • Head-to-right (HtR{1/5/*}): as head-to-left • Root (Root): t is the root node, which eliminates arcs according to projectivity  
We propose a model that incorporates an insertion operator in Bayesian tree substitution grammars (BTSG). Tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than BTSG. The experimental parsing results show that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG. 
We present a simple, language-independent method for integrating recovery of empty elements into syntactic parsing. This method outperforms the best published method we are aware of on English and a recently published method on Chinese. 
 as for machine translation. As a result, the output  In this paper, we show that local features computed from the derivations of tree substitution grammars — such as the identify of particular fragments, and a count of large and small fragments — are useful in binary grammatical classiﬁcation tasks. Such features outperform n-gram features and various model scores by  of such text generation systems is often very poor grammatically, even if it is understandable. Since grammaticality judgments are a matter of the syntax of a language, the obvious approach for modeling grammaticality is to start with the extensive work produced over the past two decades in the ﬁeld of parsing. This paper demonstrates the  a wide margin. Although they fall short of the performance of the hand-crafted feature set of Charniak and Johnson (2005) developed for parse tree reranking, they do so with an order of magnitude fewer features. Furthermore, since the TSGs employed are learned in a Bayesian setting, the use of their derivations can be viewed as the automatic discov-  utility of local features derived from the fragments of tree substitution grammar derivations. Following Cherry and Quirk (2008), we conduct experiments in a classiﬁcation setting, where the task is to distinguish between real text and “pseudo-negative” text obtained by sampling from a trigram language model (Okanohara and Tsujii, 2007). Our primary  ery of tree patterns useful for classiﬁcation. On the BLLIP dataset, we achieve an accuracy of 89.9% in discriminating between grammatical text and samples from an n-gram language model.  points of comparison are the latent SVM training of Cherry and Quirk (2008), mentioned above, and the extensive set of local and nonlocal feature templates developed by Charniak and Johnson (2005) for parse tree reranking. In contrast to this latter set  
 graph to obtain words that augment the original  We propose a new method for query-oriented extractive multi-document summarization. To enrich the information need representation of a given query, we build a co-occurrence graph to obtain words that augment the original query terms. We then formulate the sum-  query terms. We call this method Query Snowball. Another challenge in sentence selection for query-oriented multi-document summarization is how to avoid redundancy so that diverse pieces of information (i.e. nuggets (Voorhees, 2003)) can be covered. For penalizing redundancy across sen-  marization problem as a Maximum Coverage Problem with Knapsack Constraints based on word pairs rather than single words. Our experiments with the NTCIR ACLIA question answering test collections show that our method achieves a pyramid F3-score of up to 0.313, a 36% improvement over a baseline using Maximal Marginal Relevance.  tences, using single words as the basic unit may not always be appropriate, because different nuggets for a given information need often have many words in common. Figure 1 shows an example of this word overlap problem from the NTCIR-8 ACLIA2 Japanese question answering test collection. Here, two gold-standard nuggets for the question “Sen to Chihiro no Kamikakushi (Spirited Away) is a full-  
 discrete and continuous scales. Results tend to dif-  Studies assessing rating scales are very common in psychology and related ﬁelds, but are rare in NLP. In this paper we assess discrete and continuous scales used for  fer for different types of data. E.g., results from pain measurement show a continuous scale to outperform a discrete scale (ten Klooster et al., 2006). Other results (Svensson, 2000) from measuring students’  measuring quality assessments of computergenerated language. We conducted six separate experiments designed to investigate the validity, reliability, stability, interchangeability and sensitivity of discrete vs. continuous scales. We show that continuous scales are viable for use in language evaluation, and offer  ease of following lectures show a discrete scale to outperform a continuous scale. When measuring dyspnea, Lansing et al. (2003) found a hybrid scale to perform on a par with a discrete scale. Another consideration is the types of data produced by discrete and continuous scales. Parametric  distinct advantages over discrete scales.  methods of statistical analysis, which are far more  
 processing (Shaw and Hatzivassiloglou, 1999; Mal-  In this paper, we argue that ordering prenominal modiﬁers – typically pursued as a supervised modeling task – is particularly wellsuited to semi-supervised approaches. By relying on automatic parses to extract noun phrases, we can scale up the training data by orders of magnitude. This minimizes the predominant issue of data sparsity that  ouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modiﬁer ordering is how to order modiﬁers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of  has informed most previous approaches. We  an educated adult (Aitchison, 2003). Up to a quar-  compare several recent approaches, and ﬁnd improvements from additional training data across the board; however, none outperform a simple n-gram model.  ter of these words may be adjectives, which poses a signiﬁcant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observa-  
This short paper introduces an implemented and evaluated monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. After brieﬂy motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of ﬂuency and accuracy.  quences of dialogue acts. The approach is dataoriented in that the mapping rules have been automatically derived from an annotated parallel monologue/dialogue corpus, rather than being handcrafted. The paper proceeds as follows. Section 2 reviews existing approaches to dialogue generation. Section 3 describes the current approach. We provide an evaluation in Section 4. Finally, Section 5 describes our conclusions and plans for further research.  
In this paper, we address the problem of optimizing the style of textual content to make it more suitable to being listened to by a user as opposed to being read. We study the differences between the written style and the audio style by consulting the linguistics and journalism literatures. Guided by this study, we suggest a number of linguistic features to distinguish between the two styles. We show the correctness of our features and the impact of style transformation on the user experience through statistical analysis, a style classiﬁcation task, and a user study. 
The task of aligning corresponding phrases across two related sentences is an important component of approaches for natural language problems such as textual inference, paraphrase detection and text-to-text generation. In this work, we examine a state-of-the-art structured prediction model for the alignment task which uses a phrase-based representation and is forced to decode alignments using an approximate search approach. We propose instead a straightforward exact decoding technique based on integer linear programming that yields order-of-magnitude improvements in decoding speed. This ILP-based decoding strategy permits us to consider syntacticallyinformed constraints on alignments which signiﬁcantly increase the precision of the model. 
Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference. 
 the negative categories flowers (e.g. Rose, Iris)  State-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories.  and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difﬁcult to design, introducing a substantial amount of human expertise into an otherwise unsupervised framework. McIntosh (2010) made some progress towards automatically learning useful negative categories during bootstrapping. In this work we identify an unsupervised source of semantic constraints inspired by the Coupled Pattern Learner (CPL, Carlson et al. (2010)). In CPL, relation bootstrapping is coupled with lexicon bootstrapping in order to control semantic drift in the  
We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substantial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia. 
 The drawbacks above would be alleviated if data  from several different domains and relationships  In this paper, we extend distant supervision (DS) based on Wikipedia for Relation Extraction (RE) by considering (i) relations deﬁned in external repositories, e.g. YAGO, and (ii) any subset of Wikipedia documents. We show that training data constituted by sentences containing pairs of named entities in target relations is enough to produce reliable supervision. Our experiments with state-of-the-art relation extraction models, trained on the above data, show a meaningful F1 of 74.29% on a manually annotated test set: this highly improves the state-of-art in RE using DS. Additionally, our end-to-end experiments demonstrated that our extractors can be applied to  were available. A form of weakly supervision, speciﬁcally named distant supervision (DS) when applied to Wikipedia, e.g. (Banko et al., 2007; Mintz et al., 2009; Hoffmann et al., 2010) has been recently developed to meet the requirement above. The main idea is to exploit (i) relation repositories, e.g. the Infobox, x, of Wikipedia to deﬁne a set of relation types RT (x) and (ii) the text in the page associated with x to produce the training sentences, which are supposed to express instances of RT (x). Previous work has shown that selecting the sentences containing the entities targeted by a given relation is enough accurate (Banko et al., 2007; Mintz  any general text document.  et al., 2009) to provide reliable training data. How-  ever, only (Hoffmann et al., 2010) used DS to de-  
Detecting the linguistic scope of negated and speculated information in text is an important Information Extraction task. This paper presents ScopeFinder, a linguistically motivated rule-based system for the detection of negation and speculation scopes. The system rule set consists of lexico-syntactic patterns automatically extracted from a corpus annotated with negation/speculation cues and their scopes (the BioScope corpus). The system performs on par with state-of-the-art machine learning systems. Additionally, the intuitive and linguistically motivated rules will allow for manual adaptation of the rule set to new domains and corpora. 
As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six. 
 good paraphrases (Callison-Burch, 2008), it is in-  sufﬁcient in our task because it cannot properly ﬁlter  We present an approach of expanding parallel corpora for machine translation. By utilizing Semantic role labeling (SRL) on one side of the language pair, we extract SRL substitution rules from existing parallel corpus. The rules are then used for generating new sentence pairs. An SVM classiﬁer is built to ﬁlter the generated sentence pairs. The ﬁltered corpus is used for training phrase-based translation models, which can be used directly in translation tasks or combined with baseline models. Experimental results on ChineseEnglish machine translation tasks show an average improvement of 0.45 BLEU and 1.22 TER points across 5 different NIST test sets.  the candidates for the replacement. If we allow all the NPs to be replaced with other NPs, each sentence pair can generate huge number of new sentences. Instead, we resort to Semantic Role Labeling (Palmer et al., 2005) to provide more lexicalized and semantic constraints to select the candidates. The method only requires running SRL labeling on either side of the language pair, and that enables applications on low resource languages. Even with the SRL constraints, the generated corpus may still be large and noisy. Hence, we apply an additional ﬁltering stage on the generated corpus. We used an SVM classiﬁer with features derived from standard phrase based translation models and bilingual lan-  
Broad-coverage semantic annotations for training statistical learners are only available for a handful of languages. Previous approaches to cross-lingual transfer of semantic annotations have addressed this problem with encouraging results on a small scale. In this paper, we scale up previous efforts by using an automatic approach to semantic annotation that does not rely on a semantic ontology for the target language. Moreover, we improve the quality of the transferred semantic annotations by using a joint syntacticsemantic parser that learns the correlations between syntax and semantics of the target language and smooths out the errors from automatic transfer. We reach a labelled F-measure for predicates and arguments of only 4% and 9% points, respectively, lower than the upper bound from manual annotations. 
This paper presents a new approach to detecting and tracking changes in word meaning by visually modeling and representing diachronic development in word contexts. Previous studies have shown that computational models are capable of clustering and disambiguating senses, a more recent trend investigates whether changes in word meaning can be tracked by automatic methods. The aim of our study is to offer a new instrument for investigating the diachronic development of word senses in a way that allows for a better understanding of the nature of semantic change in general. For this purpose we combine techniques from the ﬁeld of Visual Analytics with unsupervised methods from Natural Language Processing, allowing for an interactive visual exploration of semantic change. 
We present an NLP system that classifies the assertion type of medical problems in clinical notes used for the Fourth i2b2/VA Challenge. Our classifier uses a variety of linguistic features, including lexical, syntactic, lexicosyntactic, and contextual features. To overcome an extremely unbalanced distribution of assertion types in the data set, we focused our efforts on adding features specifically to improve the performance of minority classes. As a result, our system reached 94.17% micro-averaged and 79.76% macro-averaged F1-measures, and showed substantial recall gains on the minority classes. 
We present a preliminary study on unsupervised preposition sense disambiguation (PSD), comparing different models and training techniques (EM, MAP-EM with L0 norm, Bayesian inference using Gibbs sampling). To our knowledge, this is the ﬁrst attempt at unsupervised preposition sense disambiguation. Our best accuracy reaches 56%, a signiﬁcant improvement (at p <.001) of 16% over the most-frequent-sense baseline. 
Understanding language requires both linguistic knowledge and knowledge about how the world works, also known as common-sense knowledge. We attempt to characterize the kinds of common-sense knowledge most often involved in recognizing textual entailments. We identify 20 categories of common-sense knowledge that are prevalent in textual entailment, many of which have received scarce attention from researchers building collections of knowledge. 
 We validate our model on the challenging task of  In many computational linguistic scenarios, training labels are subjectives making it necessary to acquire the opinions of multiple annotators/experts, which is referred to as ”wis-  listener backchannel feedback prediction in dyadic conversations. Backchannel feedback includes the nods and paraverbals such as ”uh-huh” and ”mmhmm” that listeners produce as they are speaking.  dom of crowds”. In this paper, we propose a new approach for modeling wisdom of crowds based on the Latent Mixture of Discriminative Experts (LMDE) model that can automatically learn the prototypical patterns and hidden dynamic among different experts. Experiments show improvement over state-of-the-art approaches on the task of listener backchannel prediction in dyadic conversations.  Backchannels play a signiﬁcant role in determining the nature of a social exchange by showing rapport and engagement (Gratch et al., 2007). When these signals are positive, coordinated and reciprocated, they can lead to feelings of rapport and promote beneﬁcial outcomes in diverse areas such as negotiations and conﬂict resolution (Drolet and Morris, 2000), psychotherapeutic effectiveness (Tsui and  Schultz, 1985), improved test performance in class-  
For 20 years, information extraction has focused on facts expressed in text. In contrast, this paper is a snapshot of research in progress on inferring properties and relationships among participants in dialogs, even though these properties/relationships need not be expressed as facts. For instance, can a machine detect that someone is attempting to persuade another to action or to change beliefs or is asserting their credibility? We report results on both English and Arabic discussion forums. 
Annotated corpora are essential for almost all NLP applications. Whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. With this work we want to draw attention to this fact. Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. 
In this paper we propose a new method for evaluating systems that extract temporal information from text. It uses temporal closure1 to reward relations that are equivalent but distinct. Our metric measures the overall performance of systems with a single score, making comparison between different systems straightforward. Our approach is easy to implement, intuitive, accurate, scalable and computationally inexpensive. 
 2 Linguistic Facts  We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efﬁcient annotation, and present the ﬁrst quantitative analysis of Arabic morphosyntactic phenomena. 
Broad coverage lexicons for the English language have traditionally been handmade. This approach, while accurate, requires too much human labor. Furthermore, resources contain gaps in coverage, contain specific types of information, or are incompatible with other resources. We believe that the state of open-license technology is such that a comprehensive syntactic lexicon can be automatically compiled. This paper describes the creation of such a lexicon, NU-LEX, an open-license feature-based lexicon for general purpose parsing that combines WordNet, VerbNet, and Wiktionary and contains over 100,000 words. NU-LEX was integrated into a bottom up chart parser. We ran the parser through three sets of sentences, 50 sentences total, from the Simple English Wikipedia and compared its performance to the same parser using Comlex. Both parsers performed almost equally with NU-LEX finding all lex-items for 50% of the sentences and Comlex succeeding for 52%. Furthermore, NULEX’s shortcomings primarily fell into two categories, suggesting future research directions. 
Colour is a key component in the successful dissemination of information. Since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complemented with the use of appropriate colours in information visualization and product marketing. Yet, there is no comprehensive resource that captures concept–colour associations. We present a method to create a large word–colour association lexicon by crowdsourcing. A wordchoice question was used to obtain sense-level annotations and to ensure data quality. We focus especially on abstract concepts and emotions to show that even they tend to have strong colour associations. Thus, using the right colours can not only improve semantic coherence, but also inspire the desired emotional response. 
We present Conditional Random Fields based approaches for detecting agreement/disagreement between speakers in English broadcast conversation shows. We develop annotation approaches for a variety of linguistic phenomena. Various lexical, structural, durational, and prosodic features are explored. We compare the performance when using features extracted from automatically generated annotations against that when using human annotations. We investigate the efﬁcacy of adding prosodic features on top of lexical, structural, and durational features. Since the training data is highly imbalanced, we explore two sampling approaches, random downsampling and ensemble downsampling. Overall, our approach achieves 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on the English broadcast conversation data. 
 A → [AA] | AA | e/f | /f | e/  Word alignment has an exponentially large search space, which often makes exact infer-  Figure 1: BTG rules. [AA] denotes a monotone concatenation and AA denotes an inverted concatenation.  ence infeasible. Recent studies have shown  that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efﬁciently  (BTG, Figure 1), which has only one nonterminal symbol.  searched using synchronous parsing algorithms. However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efﬁciency and accuracy. In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which  Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those derivations will slow down the parsing speed signiﬁcantly. Furthermore, spurious derivations may ﬁll up the n-best list and supersede po-  eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments.  tentially good results, making it harder to ﬁnd the best alignment. Besides, over-counting those spurious derivations will also affect the likelihood es-  timation. In order to reduce spurious derivations,  
 2 Prior Work  There are a number of systems that use a syntax-based reordering step prior to phrasebased statistical MT. An early work proposing this idea showed improved translation performance, but subsequent work has had mixed results. Speculations as to cause have suggested the parser, the data, or other factors. We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT? 
In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. The human expert interacts with the IMT system by partially correcting the errors of the system’s output. Then, the system proposes a new solution. This process is repeated until the output meets the desired quality. In this scenario, the interaction is typically performed using the keyboard and the mouse. In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. An on-line handwritten text recognition (HTR) system has been speciﬁcally designed to operate with IMT systems. Our HTR system improves previous approaches in two main aspects. First, HTR decoding is tightly coupled with the IMT system. Second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of ngrams and word-based IBM models. The proposed system achieves an important boost in performance with respect to previous work.  system goal is not to produce “perfect” translations in a completely automatic way, but to help the user build the translation with the least effort possible. A typical approach to IMT is shown in Fig. 1. A source sentence f is given to the IMT system. First, the system outputs a translation hypothesis eˆs in the target language, which would correspond to the output of fully automated MT system. Next, the user analyses the source sentence and the decoded hypothesis, and validates the longest error-free preﬁx ep ﬁnding the ﬁrst error. The user, then, corrects the erroneous word by typing some keystrokes κ, and sends them along with ep to the system, as a new validated preﬁx ep, κ. With that information, the system is able to produce a new, hopefully improved, sufﬁx eˆs that continues the previous validated preﬁx. This process is repeated until the user agrees with the quality of the resulting translation.  f  system  e s  ep ,  
 from most resources. Second, as regards the pre-  In this paper, we present a novel way of tackling the monolingual alignment problem on pairs of sentential paraphrases by means of edit rate computation. In order to inform the edit rate, information in the form of subsentential paraphrases is provided by a range of techniques built for different purposes. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources.  cision of paraphrase acquisition techniques in particular, it is notable that most works on paraphrase acquisition are not based on direct observation of larger paraphrase pairs. Even monolingual corpora obtained by pairing very closely related texts such as news headlines on the same topic and from the same time frame (Dolan et al., 2004) often contain unrelated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual corpora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and  
We present an SCFG binarization algorithm that combines the strengths of early terminal matching on the source language side and early language model integration on the target language side. We also examine how different strategies of target-side terminal attachment during binarization can signiﬁcantly affect translation quality. 
We show that unseen words account for a large part of the translation error when moving to new domains. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able to ﬁnd translations for otherwise OOV terms. We show several approaches to integrating such translations into a phrasebased translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 Bleu points) on four domains and two language pairs. 
We discuss some of the practical issues that arise from decoding with general synchronous context-free grammars. We examine problems caused by unary rules and we also examine how virtual nonterminals resulting from binarization can best be handled. We also investigate adding more ﬂexibility to synchronous context-free grammars by adding glue rules and phrases. 
To address the parse error issue for tree-tostring translation, this paper proposes a similarity-based decoding generation (SDG) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding. Experiments on Chinese-English translation demonstrated that our approach can achieve a significant improvement over the standard method, and has little impact on decoding speed in practice. Our approach is very easy to implement, and can be applied to other paradigms such as tree-to-tree models.  
 et al., 2003) is a widely known one using small num-  ber of features in a maximum-entropy (log-linear)  In this paper we present a novel discriminative mixture model for statistical machine translation (SMT). We model the feature space with a log-linear combination of multiple mixture components. Each component contains a large set of features trained in a maximumentropy framework. All features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance. This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT. It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. The proposed approach improves the translation performance signiﬁcantly on a large-scale Arabic-to-English MT task.  model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efﬁciently on a forest of translations from a develop-  ment set. Even though signiﬁcant improvement has  
Recent advances in Machine Translation (MT) have brought forth a new paradigm for building NLP applications in low-resource scenarios. To build a sentiment classiﬁer for a language with no labeled resources, one can translate labeled data from another language, then train a classiﬁer on the translated text. This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch. Various prior work have achieved positive results using this approach. In this opinion piece, we take a step back and make some general statements about crosslingual adaptation problems. First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT. Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolingual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered. This paper will describe a series of carefullydesigned experiments that led us to these conclusions. 
One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the documentlevel context, we use noun phrases which signiﬁcantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a signiﬁcant improvement of 1.20% BLEU points in JapaneseEnglish translation and 1.41% BLEU points in English-Japanese translation. 
Language models based on word surface forms only are unable to beneﬁt from available linguistic knowledge, and tend to suffer from poor estimates for rare features. We propose an approach to overcome these two limitations. We use factored features that can ﬂexibly capture linguistic regularities, and we adopt conﬁdence-weighted learning, a form of discriminative online learning that can better take advantage of a heavy tail of rare features. Finally, we extend the conﬁdence-weighted learning to deal with label noise in training data, a common case with discriminative language modeling. 
The language model (LM) is a critical component in most statistical machine translation (SMT) systems, serving to establish a probability distribution over the hypothesis space. Most SMT systems use a static LM, independent of the source language input. While previous work has shown that adapting LMs based on the input improves SMT performance, none of the techniques has thus far been shown to be feasible for on-line systems. In this paper, we develop a novel measure of cross-lingual similarity for biasing the LM based on the test input. We also illustrate an efﬁcient on-line implementation that supports integration with on-line SMT systems by transferring much of the computational load off-line. Our approach yields signiﬁcant reductions in target perplexity compared to the static LM, as well as consistent improvements in SMT performance across language pairs (English-Dari and English-Pashto). 
 The fact that spurious word alignments might oc-  In most statistical machine translation systems, the phrase/rule extraction algorithm uses alignments in the 1-best form, which might contain spurious alignment points. The usage of weighted alignment matrices that encode all possible alignments has been shown to gener-  cur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best align-  ate better phrase tables for phrase-based systems. We propose two algorithms to generate the well known MSD reordering model using weighted alignment matrices. Experiments on the IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set.  ment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propose two methods for gener-  
 each i, let  We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word f to word e by simplifying English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems.  c(  fj,  ei)  ←  c(  f j,  ei)  +  
Contrary to popular belief, we show that the optimal parameters for IBM Model 1 are not unique. We demonstrate that, for a large class of words, IBM Model 1 is indifferent among a continuum of ways to allocate probability mass to their translations. We study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate. 
In contrast to many languages (like Russian or French), modern English does not distinguish formal and informal (“T/V”) address overtly, for example by pronoun choice. We describe an ongoing study which investigates to what degree the T/V distinction is recoverable in English text, and with what textual features it correlates. Our ﬁndings are: (a) human raters can label English utterances as T or V fairly well, given sufﬁcient context; (b), lexical cues can predict T/V almost at human level. 
We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and ﬁnally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches. 
A topic model outputs a set of multinomial distributions over words for each topic. In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for ﬁnding translations of terms in comparable corpora without using any linguistic resources. Experiments on a document-aligned English-Italian Wikipedia corpus conﬁrm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported. 
 Chinese Pinyin input method is very important for Chinese language information processing. Users may make errors when they are typing in Chinese words. In this paper, we are concerned with the reasons that cause the errors. Inspired by the observation that pressing backspace is one of the most common user behaviors to modify the errors, we collect 54, 309, 334 error-correction pairs from a realworld data set that contains 2, 277, 786 users via backspace operations. In addition, we present a comparative analysis of the data to achieve a better understanding of users’ input behaviors. Comparisons with English typos suggest that some language-speciﬁc properties result in a part of Chinese input errors. 
 tionally, the oﬃcer may be engaged in many de-  Common approaches to assessing document quality look at shallow aspects, such as grammar and vocabulary. For many real-world applications, deeper notions of quality are needed. This work represents a ﬁrst step in a project aimed at developing computational methods for deep assessment of quality in the domain of intelligence reports. We present an automated system for ranking intelligence reports with regard to coverage of relevant material. The system employs methodologies from the ﬁeld of automatic summarization, and achieves performance on a par with human judges, even in the absence of the underlying information sources.  cision processes within a small window of time. Given the nature of the task, it is vital that the limited time be used eﬀectively, i.e., that the highest-quality information be handled ﬁrst. Our project aims to provide a system that will assist intelligence oﬃcers in the decision making process by quickly and accurately ranking reports according to the most important criteria for the task. In this paper, as a ﬁrst step in the project, we focus on content-related criteria. In particular, we chose to start with the aspect of “coverage”. Coverage is perhaps the most important element in a time-sensitive scenario, where an intelligence oﬃcer may need to choose among several reports while ensuring no relevant and  
We present a method for lexical simpliﬁcation. Simpliﬁcation rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, it does not require any alignment or correspondence among the complex and simple corpora. We evaluate the simpliﬁcation according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simpliﬁcation. Results show that our method outperforms an established simpliﬁcation baseline for both meaning preservation and simpliﬁcation, while maintaining a high level of grammaticality. 
Identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. As a ﬁrst step towards enhancing existing peerreview systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. In addition, we investigate the utility of incorporating additional specialized features tailored to peer review. Our preliminary results show that the structural features, review unigrams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review speciﬁc auxiliary features can further improve helpfulness prediction. 
Despite the rising interest in developing grammatical error detection systems for non-native speakers of English, progress in the ﬁeld has been hampered by a lack of informative metrics and an inability to directly compare the performance of systems developed by different researchers. In this paper we address these problems by presenting two evaluation methodologies, both based on a novel use of crowdsourcing. 
 only an observed ﬁrst-name/last-name pair. This has  This paper presents an original approach to semi-supervised learning of personal name ethnicity from typed graphs of morphophonemic features and ﬁrst/last-name co-occurrence statistics. We frame this as a general solution to an inference problem over typed graphs where the edges represent labeled relations between features that are parameterized by the edge types. We propose a framework for parameter estimation on different constructions of typed graphs for this problem using a gradient-free optimization method based on grid search. Results on both in-domain and out-of-domain data show signiﬁcant gains over 30% accuracy improvement using the techniques presented in the paper.  important consequences in targeted advertising and personalization in social networks, and in gathering intelligence for business and government research. We propose a parametrized typed graph framework for this problem and perform the hidden attribute inference using random walks on typed graphs. We also propose a novel application of a gradient-free optimization technique based on grid search for parameter estimation in typed graphs. Although, we describe this in the context of person-attribute learning, the techniques are general enough to be applied to various typed graph based problems. 2 Data for Person-Ethnicity Learning  
 users’ “following” list starts to consist of Twitter ac-  counts for different purposes. Take an average user  The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of ﬁtness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.  “Bob” for example. Some people he follows are his “Colleagues”, some are “Technology Related People”, and others could be “TV show comedians”. When Bob wants to read the latest news from his “Colleagues”, because of lacking effective ways to group users, he has to scroll through all “Tweets” from other users. There have been suggestions from many Twitter users that a grouping feature could be very useful. Yet, the only way to create groups is to create “lists” of users in Twitter manually by selecting each individual user. This process is tedious and could be sometimes formidable when a user is following many people. In this paper, we propose an interactive group creating system for Twitter. A user creates a group by ﬁrst providing a small number of seeding users, then the system ranks the friend list according to how  likely a user belongs to the group indicated by the  
We present a class-based language model that clusters rare words of similar morphology together. The model improves the prediction of words after histories containing outof-vocabulary words. The morphological features used are obtained without the use of labeled data. The perplexity improvement compared to a state of the art Kneser-Ney model is 4% overall and 81% on unknown histories. 
We present a pointwise approach to Japanese morphological analysis (MA) that ignores structure information during learning and tagging. Despite the lack of structure, it is able to outperform the current state-of-the-art structured approach for Japanese MA, and achieves accuracy similar to that of structured predictors using the same feature set. We also ﬁnd that the method is both robust to outof-domain data, and can be easily adapted through the use of a combination of partial annotation and active learning. 
Machine transliteration is deﬁned as automatic phonetic translation of names across languages. In this paper, we propose synchronous adaptor grammar, a novel nonparametric Bayesian learning approach, for machine transliteration. This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 
Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.  the corpora to select the best one. Evidence from the word segmentation literature suggests that description length provides a good approximation to this segmentation quality function. We discuss the Minimum Description Length (MDL) principle in more detail in the next section. Unfortunately, evaluating all possible segmentations is intractable, since a corpus of length n has 2n−1 possible segmentations. As a result, MDL methods have to rely on an efﬁcient algorithm to generate a relatively small number of candidate segmentations to choose between. It is an empirical question which algorithm will generate the most effective set of candidate segmentations. In this work, we compare a variety of unsupervised word segmentation algorithms operating in conjunction with MDL for fully unsupervised segmentation, and ﬁnd that the Bootstrapped Voting Experts (BVE) algorithm generally achieves the best performance.  
Paraphrase generation is an important task that has received a great deal of interest recently. Proposed data-driven solutions to the problem have ranged from simple approaches that make minimal use of NLP tools to more complex approaches that rely on numerous language-dependent resources. Despite all of the attention, there have been very few direct empirical evaluations comparing the merits of the different approaches. This paper empirically examines the tradeoffs between simple and sophisticated paraphrase harvesting approaches to help shed light on their strengths and weaknesses. Our evaluation reveals that very simple approaches fare surprisingly well and have a number of distinct advantages, including strong precision, good coverage, and low redundancy. 
This paper focuses on domain-speciﬁc senses and presents a method for assigning category/domain label to each sense of words in a dictionary. The method ﬁrst identiﬁes each sense of a word in the dictionary to its corresponding category. We used a text classiﬁcation technique to select appropriate senses for each domain. Then, senses were scored by computing the rank scores. We used Markov Random Walk (MRW) model. The method was tested on English and Japanese resources, WordNet 3.0 and EDR Japanese dictionary. For evaluation of the method, we compared English results with the Subject Field Codes (SFC) resources. We also compared each English and Japanese results to the ﬁrst sense heuristics in the WSD task. These results suggest that identiﬁcation of domain-speciﬁc senses (IDSS) may actually be of beneﬁt. 
Recognizing entailment at the lexical level is an important and commonly-addressed component in textual inference. Yet, this task has been mostly approached by simpliﬁed heuristic methods. This paper proposes an initial probabilistic modeling framework for lexical entailment, with suitable EM-based parameter estimation. Our model considers prominent entailment factors, including differences in lexical-resources reliability and the impacts of transitivity and multiple evidence. Evaluations show that the proposed model outperforms most prior systems while pointing at required future improvements. 
We investigate the expression of opinions about human entities in user-generated content (UGC). A set of 2,800 online news comments (8,000 sentences) was manually annotated, following a rich annotation scheme designed for this purpose. We conclude that the challenge in performing opinion mining in such type of content is correctly identifying the positive opinions, because (i) they are much less frequent than negative opinions and (ii) they are particularly exposed to verbal irony. We also show that the recognition of human targets poses additional challenges on mining opinions from UGC, since they are frequently mentioned by pronouns, definite descriptions and nicknames. 
We derive two variants of a semi-supervised model for ﬁne-grained sentiment analysis. Both models leverage abundant natural supervision in the form of review ratings, as well as a small amount of manually crafted sentence labels, to learn sentence-level sentiment classiﬁers. The proposed model is a fusion of a fully supervised structured conditional model and its partially supervised counterpart. This allows for highly efﬁcient estimation and inference algorithms with rich feature deﬁnitions. We describe the two variants as well as their component models and verify experimentally that both variants give signiﬁcantly improved results for sentence-level sentiment analysis compared to all baselines. 
Identifying domain-dependent opinion words is a key problem in opinion mining and has been studied by several researchers. However, existing work has been focused on adjectives and to some extent verbs. Limited work has been done on nouns and noun phrases. In our work, we used the feature-based opinion mining model, and we found that in some domains nouns and noun phrases that indicate product features may also imply opinions. In many such cases, these nouns are not subjective but objective. Their involved sentences are also objective sentences and imply positive or negative opinions. Identifying such nouns and noun phrases and their polarities is very challenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 
Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author. We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm. We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task. Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well. 
Although Subjectivity and Sentiment Analysis (SSA) has been witnessing a ﬂurry of novel research, there are few attempts to build SSA systems for Morphologically-Rich Languages (MRL). In the current study, we report efforts to partially ﬁll this gap. We present a newly developed manually annotated corpus of Modern Standard Arabic (MSA) together with a new polarity lexicon.The corpus is a collection of newswire documents annotated on the sentence level. We also describe an automatic SSA tagging system that exploits the annotated data. We investigate the impact of different levels of preprocessing settings on the SSA classiﬁcation task. We show that by explicitly accounting for the rich morphology the system is able to achieve signiﬁcantly higher levels of performance. 
 important task given the huge amount of product re-  views written on the Web and the difﬁculty of man-  We present a method for identifying the positive or negative semantic orientation of for-  ually handling them. Another interesting application is mining attitude in discussions (Hassan et al.,  eign words. Identifying the semantic orientation of words has numerous applications in the areas of text classiﬁcation, analysis of product review, analysis of responses to surveys, and mining online discussions. Identifying the semantic orientation of English words has been extensively studied in literature. Most of  2010), where the attitude of participants in a discussion is inferred using the text they exchange. Due to its importance, several researchers have addressed the problem of identifying the semantic orientation of individual words. This work has almost exclusively focused on English. Most of this  this work assumes the existence of resources (e.g. Wordnet, seeds, etc) that do not exist in foreign languages. In this work, we describe a method based on constructing a multilingual network connecting English and foreign words. We use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connec-  work used several language dependent resources. For example Turney and Littman (2003) use the entire English Web corpus by submitting queries consisting of the given word and a set of seeds to a search engine. In addition, several other methods have used Wordnet (Miller, 1995) for connecting semantically related words (Kamps et al., 2004; Takamura et al., 2005; Hassan and Radev, 2010).  tions. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results.  When we try to apply those methods to other languages, we run into the problem of the lack of resources in other languages when compared to English. For example, the General Inquirer lexicon  (Stone et al., 1966) has thousands of English words  
 as hierarchical class structure in many application  Recently, hierarchical text classiﬁcation has become an active research topic. The essential idea is that the descendant classes can share the information of the ancestor classes in a predeﬁned taxonomy. In this paper, we claim that each class has several latent concepts and its subclasses share information with these different concepts respectively. Then, we propose a variant Passive-Aggressive (PA) algorithm for hierarchical text classiﬁcation with latent concepts. Experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classiﬁcation algorithms.  ﬁelds: web taxonomies (i.e. the Yahoo! Directory http://dir.yahoo.com/ and the Open Directory Project (ODP) http://dmoz.org/), email folders and product catalogs. The approaches of hierarchical text classiﬁcation can be divided in three ways: ﬂat, local and global approaches. The ﬂat approach is traditional multi-class classiﬁcation in ﬂat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy(Yang and Liu, 1999; Yang and Pedersen, 1997; Qiu et al., 2011). The local approach proceeds in a top-down fashion, which ﬁrstly picks the most relevant categories  
 In this study, a novel approach to robust dialogue act detection for error-prone speech recognition in a spoken dialogue system is proposed. First, partial sentence trees are proposed to represent a speech recognition output sentence. Semantic information and the derivation rules of the partial sentence trees are extracted and used to model the relationship between the dialogue acts and the derivation rules. The constructed model is then used to generate a semantic score for dialogue act detection given an input speech utterance. The proposed approach is implemented and evaluated in a Mandarin spoken dialogue system for tour-guiding service. Combined with scores derived from the ASR recognition probability and the dialogue history, the proposed approach achieves 84.3% detection accuracy, an absolute improvement of 34.7% over the baseline of the semantic slot-based method with 49.6% detection accuracy.  Figure 1: Details of the SLU and DM modules. guage generation in the textual form, which is passed to a text-to-speech synthesizer for speech waveform generation. The cycle repeats when the user responds with a new utterance. Clearly, one can see that the inference of the user’s overall intention via DA detection is an important task in SDS. Figure 1 depicts the training and test phases of  the SLU module and the DM module in our system.  
There are several theories regarding what inﬂuences prominence assignment in English noun-noun compounds. We have developed corpus-driven models for automatically predicting prominence assignment in noun-noun compounds using feature sets based on two such theories: the informativeness theory and the semantic composition theory. The evaluation of the prediction models indicate that though both of these theories are relevant, they account for different types of variability in prominence assignment. 
Verbal feedback is an important information source in establishing interactional rapport. However, predicting verbal feedback across languages is challenging due to languagespeciﬁc differences, inter-speaker variation, and the relative sparseness and optionality of verbal feedback. In this paper, we employ an approach combining classiﬁer weighting and SMOTE algorithm oversampling to improve verbal feedback prediction in Arabic, English, and Spanish dyadic conversations. This approach improves the prediction of verbal feedback, up to 6-fold, while maintaining a high overall accuracy. Analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration. 
In the face of sparsity, statistical models are often interpolated with lower order (backoff) models, particularly in Language Modeling. In this paper, we argue that there is a relation between the higher order and the backoff model that must be satisﬁed in order for the interpolation to be effective. We show that in n-gram models, the relation is trivially held, but in models that allow arbitrary clustering of context (such as decision tree models), this relation is generally not satisﬁed. Based on this insight, we also propose a generalization of linear interpolation which signiﬁcantly improves the performance of a decision tree language model.  Note the context space for this function, w1i−1 is arbitrarily long, necessitating some independence assumption, which usually consists of reducing the relevant context to n − 1 immediately preceding tokens: p(wi|w1i−1) ≈ p(wi|wii−−n1+1) These distributions are typically estimated from observed counts of n-grams wii−n+1 in the training data. The context space is still far too large; therefore, the models are recursively smoothed using lower order distributions. For instance, in a widely used n-gram LM, the probabilities are estimated as follows:  
We present a novel probabilistic classiﬁer, which scales well to problems that involve a large number of classes and require training on large datasets. A prominent example of such a problem is language modeling. Our classiﬁer is based on the assumption that each feature is associated with a predictive strength, which quantiﬁes how well the feature can predict the class by itself. The predictions of individual features can then be combined according to their predictive strength, resulting in a model, whose parameters can be reliably and efﬁciently estimated. We show that a generative language model based on our classiﬁer consistently matches modiﬁed Kneser-Ney smoothing and can outperform it if sufﬁciently rich features are incorporated. 
 tences that can only be plausibly translated into mul-  We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely  tiple English sentences. An example is given in (1),  where one Chinese sentence is plausibly translated  into three English sentences.  (1) 这 段 时间一直在 留意  这  coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classiﬁcation accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.  this period time AS AS pay attention to this 款 nano 3 ，[1] 还 专门 跑 了 CL Nano 3 , even in person visit AS 几 家 电脑 市场 , [2] 相比较 a few AS computer market , comparatively 而言 , [3] 卓越 的 价格 算  
This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. We use unsupervised data to generate informative ‘condensed feature representations’ from the original feature set used in supervised NLP systems. The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique. Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III. 
Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric. 
Comparative News Summarization aims to highlight the commonalities and differences between two comparable news topics. In this study, we propose a novel approach to generating comparative news summaries. We formulate the task as an optimization problem of selecting proper sentences to maximize the comparativeness within the summary and the representativeness to both news topics. We consider semantic-related cross-topic concept pairs as comparative evidences, and consider topic-related concepts as representative evidences. The optimization problem is addressed by using a linear programming model. The experimental results demonstrate the effectiveness of our proposed model. 
Surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection. We therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our uniﬁed approach performs better than greedy or random baselines. 
In this paper we investigate how much data is required to train an algorithm for attribute selection, a subtask of Referring Expressions Generation (REG). To enable comparison between different-sized training sets, a systematic training method was developed. The results show that depending on the complexity of the domain, training on 10 to 20 items may already lead to a good performance. 
In this paper we examine the task of sentence simpliﬁcation which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simpliﬁcation. The data contains the full range of simpliﬁcation operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based translation approach for simpliﬁcation. 
We investigate the relevance of hierarchical topic models to represent the content of Web gists. We focus our attention on DMOZ, a popular Web directory, and propose two algorithms to infer such a model from its manually-curated hierarchy of categories. Our ﬁrst approach, based on information-theoretic grounds, uses an algorithm similar to recursive feature selection. Our second approach is fully Bayesian and derived from the more general model, hierarchical LDA. We evaluate the performance of both models against a ﬂat 1-gram baseline and show improvements in terms of perplexity over held-out data. 
 Roark and Hollingshead (2008; 2009) have re-  cently shown that using a ﬁnite-state tagger to close  We present a novel pruning method for context-free parsing that increases efﬁciency by disallowing phrase-level unary productions in CKY chart cells spanning a single word. Our work is orthogonal to recent work on “closing” chart cells, which has focused on multi-word constituents, leaving span-1 chart cells unpruned. We show that a simple discriminative classiﬁer can learn with high accuracy which span-1 chart cells to close to phrase-level unary productions. Eliminating these unary productions from the search can have a large impact on downstream processing, depending on implementation details of the search. We apply our method to four parsing architectures and demonstrate how it is complementary to the cell-closing paradigm, as well as other pruning methods such as coarse-to-ﬁne, agenda, and beam-search pruning.  cells within the CKY chart can reduce the worst-case and average-case complexity of context-free parsing, without reducing accuracy. In their work, word positions are classiﬁed as beginning and/or ending multi-word constituents, and all chart cells not conforming to these constraints can be pruned. Zhang et al. (2010) and Bodenstab et al. (2011) both extend this approach by classifying chart cells with a ﬁner granularity. Pruning based on constituent span is straightforwardly applicable to all parsing architectures, yet the methods mentioned above only consider spans of length two or greater. Lexical and unary productions spanning a single word are never pruned, and these can, in many cases, contribute signiﬁcantly to the parsing effort. In this paper, we investigate complementary methods to prune chart cells with ﬁnite-state preprocessing. Informally, we use a tagger to re-  strict the number of unary productions with non-  
We consider a very simple, yet effective, approach to cross language adaptation of dependency parsers. We ﬁrst remove lexical items from the treebanks and map part-of-speech tags into a common tagset. We then train a language model on tag sequences in otherwise unlabeled target data and rank labeled source data by perplexity per word of tag sequences from less similar to most similar to the target. We then train our target language parser on the most similar data points in the source labeled data. The strategy achieves much better results than a non-adapted baseline and stateof-the-art unsupervised dependency parsing, and results are comparable to more complex projection-based cross language adaptation algorithms. 
 approach (Cer et al., 2010). The transition-based ap-  This paper suggests two ways of improving transition-based, non-projective dependency parsing. First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective  proach has a worst-case parsing complexity of O(n) for projective, and O(n2) for non-projective parsing (Nivre, 2008). The complexity is lower for projective parsing because it can deterministically drop certain tokens from the search space whereas that  parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a signiﬁcant improvement to parsing accuracy, showing near state-of-the-  is not advisable for non-projective parsing. Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). This is because the amount of non-projective dependencies is much smaller than the amount of projective dependencies, so a parser can perform projective parsing for most cases and perform non-projective  art performance with respect to other parsing approaches evaluated on the same data set.  parsing only when it is needed. One other advantage of the transition-based approach is that it can  
This work introduces a new approach to checking treebank consistency. Derivation trees based on a variant of Tree Adjoining Grammar are used to compare the annotation of word sequences based on their structural similarity. This overcomes the problems of earlier approaches based on using strings of words rather than tree structure to identify the appropriate contexts for comparison. We report on the result of applying this approach to the Penn Arabic Treebank and how this approach leads to high precision of error detection. 
This paper presents the introduction of WordNet semantic classes in a dependency parser, obtaining improvements on the full Penn Treebank for the first time. We tried different combinations of some basic semantic classes and word sense disambiguation algorithms. Our experiments show that selecting the adequate combination of semantic features on development data is key for success. Given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements. 
We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs. 
 verbs like improve. In this paper, we obtain syn-  We combine multiple word representations based on semantic clusters extracted from the (Brown et al., 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al., 2006) in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al., 2005). We also provide an ensemble method for combining diverse cluster-based models. The two contributions together signiﬁcantly improves unlabeled dependency accuracy from 90.82% to 92.13%.  tactic clusters from the Berkeley parser (Petrov et al., 2006). This paper makes two contributions: 1) We combine together multiple word representations based on semantic and syntactic clusters in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al., 2005), and 2) We provide an ensemble method for combining diverse clustering algorithms that is the discriminative parsing analog to the generative product of experts model for parsing described in (Petrov, 2010). These two contributions combined signiﬁcantly improves unlabeled dependency accuracy: 90.82% to  
For the task of automatic treebank conversion, this paper presents a feature-based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard. Experiments on two Chinese treebanks show that our approach improves conversion accuracy by 1.31% over a strong baseline. 
We investigate full-scale shortest-derivation parsing (SDP), wherein the parser selects an analysis built from the fewest number of training fragments. Shortest derivation parsing exhibits an unusual range of behaviors. At one extreme, in the fully unpruned case, it is neither fast nor accurate. At the other extreme, when pruned with a coarse unlexicalized PCFG, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation.  task. Moreover, basic tie-breaking variants and lexical augmentation are insufﬁcient to achieve competitive accuracies.1 On the other hand, SDP is dramatically improved in both speed and accuracy when a simple, unlexicalized PCFG is used for coarseto-ﬁne pruning (and tie-breaking). On the English WSJ, the coarse PCFG and the ﬁne SDP together achieve 87% F1 with basic treebank annotation (see Table 2) and up to 90% F1 with richer treebank annotation (see Table 4). The main contribution of this work is to analyze the behavior of shortest derivation parsing, showing both when it fails and when it succeeds. Our ﬁnal parser, which combines a simple PCFG coarse pass with an otherwise pure SPD ﬁne pass, can be quite accurate while being straightforward to implement. 2 Implicit Grammar for SDP  
This paper proposes three modules based on latent topics of documents for alleviating “semantic drift” in bootstrapping entity set expansion. These new modules are added to a discriminative bootstrapping algorithm to realize topic feature generation, negative example selection and entity candidate pruning. In this study, we model latent topics with LDA (Latent Dirichlet Allocation) in an unsupervised way. Experiments show that the accuracy of the extracted entities is improved by 6.7 to 28.2% depending on the domain. 
In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available. 
In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from nonparallel text.  
In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a signiﬁcant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 
This paper extends the training and tuning regime for phrase-based statistical machine translation to obtain ﬂuent translations into morphologically complex languages (we build an English to Finnish translation system). Our methods use unsupervised morphology induction. Unlike previous work we focus on morphologically productive phrase pairs – our decoder can combine morphemes across phrase boundaries. Morphemes in the target language may not have a corresponding morpheme or word in the source language. Therefore, we propose a novel combination of post-processing morphology prediction with morpheme-based translation. We show, using both automatic evaluation scores and linguistically motivated analyses of the output, that our methods outperform previously proposed ones and provide the best known results on the EnglishFinnish Europarl translation task. Our methods are mostly language independent, so they should improve translation into other target languages with complex morphology. 
 Active learning has been applied to several NLP  Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classiﬁcation. While various simulation studies for a number of NLP tasks have shown that  tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Schu¨tze, 2008; Tomanek and Hahn,  AL works well on goldstandard data, there is  2009), Word Sense Disambiguation (Chen et al.,  some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to ﬁlter out  2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classiﬁcation (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classiﬁer performance, sometimes by as much as half. Most of  inconsistent annotations during AL and show  these studies, however, have only simulated the ac-  that this makes AL far more robust when applied to noisy data.  tive learning process using goldstandard data. This setting is crucially different from a real world sce-  nario where we have to deal with erroneous data  
This paper proposes a new method for approximate string search, speciﬁcally candidate generation in spelling error correction, which is a task as follows. Given a misspelled word, the system ﬁnds words in a dictionary, which are most “similar” to the misspelled word. The paper proposes a probabilistic approach to the task, which is both accurate and efﬁcient. The approach includes the use of a log linear model, a method for training the model, and an algorithm for ﬁnding the top k candidates. The log linear model is deﬁned as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word. The learning method employs the criterion in candidate generation as loss function. The retrieval algorithm is efﬁcient and is guaranteed to ﬁnd the optimal k candidates. Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings. 
We consider a semi-supervised setting for domain adaptation where only unlabeled data is available for the target domain. One way to tackle this problem is to train a generative model with latent variables on the mixture of data from the source and target domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features speciﬁc to the source domain only and, consequently, a classiﬁer relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary signiﬁcantly across domains. We show that this constraint is effective on the sentiment classiﬁcation task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks. 
We describe an exact decoding algorithm for syntax-based statistical translation. The approach uses Lagrangian relaxation to decompose the decoding problem into tractable subproblems, thereby avoiding exhaustive dynamic programming. The method recovers exact solutions, with certiﬁcates of optimality, on over 97% of test examples; it has comparable speed to state-of-the-art decoders. 
We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows signiﬁcant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining queryproduct associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision. 
Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efﬁcient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-)supervised hashing method named Semi-Supervised SimHash (S3H) for high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance. 
 articles or web pages). As previous research shows,  Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classiﬁers on the independent annotations, and exploit the dependencies between them to fur-  these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chun-  ther improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.  kers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are grammatically diverse. Some queries are keyword concatenations, some are semi-  complete verbal phrases and some are wh-questions.  
We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. Query weighting is a key step in ranking model adaptation. As the learning object of ranking algorithms is divided by query instances, we argue that it’s more reasonable to conduct importance weighting at query level than document level. We present two query weighting schemes. The ﬁrst compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. This method can efﬁciently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these ﬁne-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting signiﬁcantly outperforms document instance weighting methods. 
Joint sentiment-topic (JST) model was previously proposed to detect sentiment and topic simultaneously from text. The only supervision required by JST model learning is domain-independent polarity word priors. In this paper, we modify the JST model by incorporating word polarity priors through modifying the topic-word Dirichlet priors. We study the polarity-bearing topics extracted by JST and show that by augmenting the original feature space with polarity-bearing topics, the in-domain supervised classiﬁers learned from augmented feature representation achieve the state-of-the-art performance of 95% on the movie review data and an average of 90% on the multi-domain sentiment dataset. Furthermore, using feature augmentation and selection according to the information gain criteria for cross-domain sentiment classiﬁcation, our proposed approach performs either better or comparably compared to previous approaches. Nevertheless, our approach is much simpler and does not require difﬁcult parameter tuning. 
 expressed in the review. Sentiment classiﬁcation  We describe a sentiment classiﬁcation method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automat-  has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004).  ically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to ﬁnd the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classiﬁer. Unlike previous cross-domain sentiment classiﬁcation methods, our method can efﬁciently learn from  Supervised learning algorithms that require labeled data have been successfully used to build sentiment classiﬁers for a speciﬁc domain (Pang et al., 2002). However, sentiment is expressed differently in different domains, and it is costly to annotate data for each new domain in which we would like to apply a sentiment classiﬁer. For example, in the domain of reviews about electronics products, the  multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classiﬁcation methods on a benchmark dataset containing Amazon user reviews for different types of products.  words “durable” and “light” are used to express positive sentiment, whereas “expensive” and “short battery life” often indicate negative sentiment. On the other hand, if we consider the books domain the words “exciting” and “thriller” express positive sentiment, whereas the words “boring” and “lengthy” usually express negative sentiment. A classiﬁer  
 recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We  Weston, 2008; Turian et al., 2010). In this paper, we present a model to capture both semantic and sentiment similarities among words. The semantic component of our model learns word  present a model that uses a mix of unsuper-  vectors via an unsupervised probabilistic model of  vised and supervised techniques to learn word vectors capturing semantic term–document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations  documents. However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we ﬁnd that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t cap-  present in many online documents (e.g. star  ture the fact that these are both very strong positive  ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and ﬁnd it out-performs several previously introduced methods for sentiment classiﬁcation. We also introduce a large dataset of movie reviews to serve as a more robust  sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wil-  benchmark for work in this area.  son et al., 2004; Alm et al., 2005; Andreevskaia  and Bergler, 2006; Pang and Lee, 2005; Goldberg  
It has been widely recognized that one of the most difficult and intriguing problems in natural language processing (NLP) is how to cope with idiosyncratic multiword expressions. This paper presents an overview of the comprehensive dictionary (JDMWE) of Japanese multiword expressions. The JDMWE is characterized by a large notational, syntactic, and semantic diversity of contained expressions as well as a detailed description of their syntactic functions, structures, and flexibilities. The dictionary contains about 104,000 expressions, potentially 750,000 expressions. This paper shows that the JDMWE’s validity can be supported by comparing the dictionary with a large-scale Japanese N-gram frequency dataset, namely the LDC2009T08, generated by Google Inc. (Kudo et al. 2009). 
We describe an annotation tool developed to assist in the creation of multimodal actioncommunication corpora from on-line massively multi-player games, or MMGs. MMGs typically involve groups of players (5-30) who control their avatars1, perform various activities (questing, competing, fighting, etc.) and communicate via chat or speech using assumed screen names. We collected a corpus of 48 group quests in Second Life that jointly involved 206 players who generated over 30,000 messages in quasisynchronous chat during approximately 140 hours of recorded action. Multiple levels of coordinated annotation of this corpus (dialogue, movements, touch, gaze, wear, etc) are required in order to support development of automated predictors of selected real-life social and demographic characteristics of the players. The annotation tool presented in this paper was developed to enable efficient and accurate annotation of all dimensions simultaneously. 
We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the ﬁrst publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as deﬁned by the agreement between human examiners on the same corpus. Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner. 
A lack of standard datasets and evaluation metrics has prevented the ﬁeld of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efﬁcient to compute, experiments show that these metrics correlate highly with human judgments. 
 are efﬁcient at encoding local word interactions, the  This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random ﬁeld paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves signiﬁcantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-theart parsing-based machine translation system.  n-gram model clearly ignores the rich syntactic and semantic structures that constrain natural languages. As the machine translation (MT) working groups stated on page 3 of their ﬁnal report (Lavie et al., 2006), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Wang et al. (2006) integrated n-gram, structured language model (SLM) (Chelba and Jelinek, 2000) and probabilistic latent semantic analysis (PLSA) (Hofmann, 2001) under the directed MRF framework (Wang et al., 2005) and studied the stochastic properties for the composite language model. They derived a generalized inside-outside algorithm to train the composite language model from a general EM (Dempster et al., 1977) by following Jelinek’s ingenious deﬁnition of the inside and outside  
 State-of-the-art statistical machine translation (MT) systems have made signiﬁcant progress towards producing user-acceptable translation output. However, there is still no efﬁcient way for MT systems to inform users which words are likely translated correctly and how conﬁdent it is about the whole sentence. We propose a novel framework to predict wordlevel and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed conﬁdence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed conﬁdence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.  
 We argue that BLEU (Papineni et al., 2002) and other  We introduce a novel semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers, producing scores that correlate with human judgment as well as HTER but at much lower labor cost. As machine translation systems improve in lexical choice and flu-  automatic n-gram based MT evaluation metrics do not adequately capture the similarity in meaning between the machine translation and the reference translation—which, ultimately, is essential for MT output to be useful. Ngram based metrics assume that “good” translations tend to share the same lexical choices as the reference trans-  ency, the shortcomings of widespread n-gram based,  lations. While BLEU score performs well in captur-  fluency-oriented MT evaluation metrics such as  ing the translation fluency, Callison-Burch et al. (2006)  BLEU, which fail to properly evaluate adequacy, become more apparent. But more accurate, nonautomatic adequacy-oriented MT evaluation metrics like HTER are highly labor-intensive, which bottlenecks the evaluation cycle. We first show that when using untrained monolingual readers to annotate semantic roles in MT output, the non-automatic version of the metric HMEANT achieves a 0.43 correlation coefficient with human adequacy judgments at the sentence level, far superior to BLEU at only 0.20,  and Koehn and Monz (2006) report cases where BLEU strongly disagree with human judgment on translation quality. The underlying reason is that lexical similarity does not adequately reflect the similarity in meaning. As MT systems improve, the shortcomings of the n-gram based evaluation metrics are becoming more apparent. State-of-the-art MT systems are often able to output fluent translations that are nearly grammatical and contain roughly the correct words, but still fail to express mean-  and equal to the far more expensive HTER. We then  ing that is close to the input.  replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric, and show that even the semiautomated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment, which is still about 80% as closely correlated as  At the same time, although HTER (Snover et al., 2006) is more adequacy-oriented, it is only employed in very large scale MT system evaluation instead of day-to-day research activities. The underlying reason is that it requires rigorously trained human experts to make difficult  HTER despite an even lower labor cost for the evalu-  combinatorial decisions on the minimal number of edits  ation procedure. The results show that our proposed  so as to make the MT output convey the same meaning as  metric is significantly better correlated with human  the reference translation—a highly labor-intensive, costly  judgment on adequacy than current widespread automatic evaluation metrics, while being much more cost effective than HTER.  process that bottlenecks the evaluation cycle. Instead, with MEANT, we adopt at the outset the principle that a good translation is one that is useful,  
This paper presents an exponential model for translation into highly inﬂected languages which can be scaled to very large datasets. As in other recent proposals, it predicts targetside phrases and can be conditioned on sourceside context. However, crucially for the task of modeling morphological generalizations, it estimates feature parameters from the entire training set rather than as a collection of separate classiﬁers. We apply it to English-Czech translation, using a variety of features capturing potential predictors for case, number, and gender, and one of the largest publicly available parallel data sets. We also describe generation and modeling of inﬂected forms unobserved in training data and decoding procedures for a model with non-local target-side feature dependencies.  in turn used to compute additional relative frequency (maximum likelihood) estimates predicting targetside inﬂections. This approach makes it difﬁcult to handle the complex interplay between different predictors for inﬂections. For example, the accusative case is usually preserved in translation, so that nouns appearing in the direct object position of English clauses tend to be translated to words with accusative case markings in languages with richer morphology, and vice versa. However, there are exceptions. For example, some verbs that place their object in the accusative case in Czech may be rendered as prepositional constructions in English (Naughton, 2005): David was looking for Jana David hledal Janu David searched Jana-ACC  
We introduce a novel Bayesian approach for deciphering complex substitution ciphers. Our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. Bayesian inference is performed on our model using an efﬁcient sampling technique. We evaluate the quality of the Bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100% accurate decipherments. The new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous Zodiac-408 cipher in a fully automated fashion, which has never been done before. 
Topic models have been used extensively as a tool for corpus exploration, and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data. However, creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software. In this work, we develop a framework for allowing users to iteratively reﬁne the topics discovered by models such as latent Dirichlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions.  Contrary to the impression given by the tables shown in topic modeling papers, topics discovered by topic modeling don’t always make sense to ostensible end users. Part of the problem is that the objective function of topic models doesn’t always correlate with human judgements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010).  
N -gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%. 
This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the ofﬁcial game manual as the text guide. Our results show that a linguistically-informed game-playing agent signiﬁcantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the builtin AI of Civilization II. 1 
Information retrieval (IR) and figurative language processing (FLP) could scarcely be more different in their treatment of language and meaning. IR views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved, focusing more on a text’s potential relevance than its potential meaning. In contrast, FLP views language as a system of unstable signs that can be used to talk about the world in creative new ways. There is another key difference: IR is practical, scalable and robust, and in daily use by millions of casual users. FLP is neither scalable nor robust, and not yet practical enough to migrate beyond the lab. This paper thus presents a mutually beneficial hybrid of IR and FLP, one that enriches IR with new operators to enable the non-literal retrieval of creative expressions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 
This paper proposes the use of local histograms (LH) over character n-grams for authorship attribution (AA). LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization and document visualization using word histograms. In this work we explore the suitability of LHs over n-grams at the character-level for AA. We show that LHs are particularly helpful for AA, because they provide useful information for uncovering, to some extent, the writing style of authors. We report experimental results in AA data sets that conﬁrm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship veriﬁcation and plagiarism detection. 
 2 Rethinking Word Difficulty  While computational estimation of difficulty of words in the lexicon is useful in many educational and assessment applications, the concept of scalar word difficulty and current corpus-based methods for its estimation are inadequate. We propose a new paradigm called word meaning maturity which tracks the degree of knowledge of each word at different stages of language learning. We present a computational algorithm for estimating word maturity, based on modeling language acquisition with Latent Semantic Analysis. We demonstrate that the resulting metric not only correlates well with external indicators, but captures deeper semantic effects in language. 
 Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identiﬁable instances of opinion spam, in this work we study deceptive opinion spam—ﬁctitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classiﬁer that is nearly 90% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing. 
Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 
This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker’s opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The ﬁrst one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods signiﬁcantly outperform the baseline approach that extracts the longest utterances. In particular, we ﬁnd that incorporating dialogue structure in the graph-based method contributes to the improved system performance. 
We present disputant relation-based method for classifying news articles on contentious issues. We observe that the disputants of a contention are an important feature for understanding the discourse. It performs unsupervised classification on news articles based on disputant relations, and helps readers intuitively view the articles through the opponent-based frame. The readers can attain balanced understanding on the contention, free from a specific biased view. We applied a modified version of HITS algorithm and an SVM classifier trained with pseudo-relevant data for article analysis. 
We present a probabilistic topic model for jointly identifying properties and attributes of social media review snippets. Our model simultaneously learns a set of properties of a product and captures aggregate user sentiments towards these properties. This approach directly enables discovery of highly rated or inconsistent properties of a product. Our model admits an efﬁcient variational meanﬁeld inference algorithm which can be parallelized and run on large snippet collections. We evaluate our model on a large corpus of snippets from Yelp reviews to assess property and attribute prediction. We demonstrate that it outperforms applicable baselines by a considerable margin. 
Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classiﬁer to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter. 
Summarizing and analyzing Twitter content is an important and challenging task. In this paper, we propose to extract topical keyphrases as one way to summarize Twitter. We propose a context-sensitive topical PageRank method for keyword ranking and a probabilistic scoring function that considers both relevance and interestingness of keyphrases for keyphrase ranking. We evaluate our proposed methods on a large Twitter data set. Experiments show that these methods are very effective for topical keyphrase extraction. 
We present a novel method for record extraction from social streams such as Twitter. Unlike typical extraction setups, these environments are characterized by short, one sentence messages with heavily colloquial speech. To further complicate matters, individual messages may not express the full relation to be uncovered, as is often assumed in extraction tasks. We develop a graphical model that addresses these problems by learning a latent set of records and a record-message alignment simultaneously; the output of our model is a set of canonical records, the values of which are consistent with aligned messages. We demonstrate that our approach is able to accurately induce event records from Twitter messages, evaluated against events from a local city guide. Our method achieves signiﬁcant error reduction over baseline methods.1 
Grapheme-to-phoneme conversion (G2P) of names is an important and challenging problem. The correct pronunciation of a name is often reﬂected in its transliterations, which are expressed within a different phonological inventory. We investigate the problem of using transliterations to correct errors produced by state-of-the-art G2P systems. We present a novel re-ranking approach that incorporates a variety of score and n-gram features, in order to leverage transliterations from multiple languages. Our experiments demonstrate signiﬁcant accuracy improvements when re-ranking is applied to n-best lists generated by three different G2P programs. 
 alignment model to predict word alignments in the  We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unanno-  parallel corpus. Discriminative models are attractive because they can incorporate arbitrary, overlapping features, meaning that errors observed in the predictions made by the model can be addressed by engineering new and better features. Unfortunately, both approaches are problematic, but in different ways. In the case of discriminative alignment models, manual alignment data is required for training, which is problematic for at least three reasons. Manual alignments are notoriously difﬁcult to create and are available only for a handful of language pairs. Second, manual alignments impose a commit-  tated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.  ment to a particular preprocessing regime; this can be problematic since the optimal segmentation for translation often depends on characteristics of the test set or size of the available training data (Habash and Sadat, 2006) or may be constrained by requirements of other processing components, such parsers.  
 well as heuristic interpolations between the union  and intersection like grow-diag-ﬁnal (Koehn et al.,  Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-ﬁnal. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efﬁcient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.  2003). This paper presents a model-based alternative to aligner combination. Inference in a probabilistic model resolves the conﬂicting predictions of two directional models, while taking into account each model’s uncertainty over its output. This result is achieved by embedding two directional HMM-based alignment models into a larger bidirectional graphical model. The full model structure and potentials allow the two embedded directional models to disagree to some extent, but reward agreement. Moreover, the bidirectional model enforces a one-to-one phrase alignment structure, similar to the output of phrase alignment models (Marcu and Wong, 2002; DeNero et al., 2008), unsupervised inversion transduction grammar (ITG) models (Blunsom et al., 2009), and supervised ITG models (Haghighi et al., 2009; DeNero and Klein, 2010).  Inference in our combined model is not tractable  
 |G| is the number of grammar productions, a non-  Efﬁcient decoding for syntactic parsing has become a necessary research area as statistical grammars grow in accuracy and size and as more NLP applications leverage syntactic analyses. We review prior methods for pruning and then present a new framework that uniﬁes their strengths into a single approach. Using a log linear model, we learn  negligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efﬁciency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent vari-  the optimal beam-search pruning parameters for each CYK chart cell, effectively predicting the most promising areas of the model space to explore. We demonstrate that our method is faster than coarse-to-ﬁne pruning, exempliﬁed in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and  able grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G| from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a sin-  under identical operating conditions.  gle sentence of 25 words, an unacceptable amount  of time for real-time applications or when process-  
We study the problem of ﬁnding the best headdriven parsing strategy for Linear ContextFree Rewriting System productions. A headdriven strategy must begin with a speciﬁed righthand-side nonterminal (the head) and add the remaining nonterminals one at a time in any order. We show that it is NP-hard to ﬁnd the best head-driven strategy in terms of either the time or space complexity of parsing. 
We present a method for the computation of preﬁx probabilities for synchronous contextfree grammars. Our framework is fairly general and relies on the combination of a simple, novel grammar transformation and standard techniques to bring grammars into normal forms. 
Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is signiﬁcantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the ﬁrst empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task. 
 presence of a heuristic that limits redundancy. As  another example, Yih et al. (2007) learn predictors  We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a marginbased objective whose loss captures end sum-  of individual words’ appearance in the references, but in isolation from the sentence selection procedure. Exceptions are Li et al. (2009) who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy. One main contribution of the current paper is the direct optimization of summary quality  mary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efﬁciently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also  in a single model; we ﬁnd that our learned systems substantially outperform unlearned counterparts on both automatic and manual metrics. While pure extraction is certainly simple and does guarantee some minimal readability, Lin (2003)  present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.  showed that sentence compression (Knight and Marcu, 2001; McDonald, 2006; Clarke and Lapata, 2008) has the potential to improve the resulting summaries. However, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by ap-  plying heuristic sentence compressions, but their  
 taking advantage of surface level features such as  Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.  word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-deﬁned structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume´-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than speciﬁc concepts. Nonetheless, all these  
In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the ﬂuency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and ﬂuency. 
 F (Sˆ) ≥ (1 − 1/e)F (Sopt) ≈ 0.632F (Sopt). This is particularly attractive since the quality of the so-  We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efﬁcient  lution does not depend on the size of the problem, so even very large size problems do well. It is also important to note that this is a worst case bound, and in most cases the quality of the solution obtained will be much better than this bound suggests. Of course, none of this is useful if the objective function F is inappropriate for the summarization  scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding  task. In this paper, we argue that monotone nondecreasing submodular functions F are an ideal class of functions to investigate for document summarization. We show, in fact, that many well-established methods for summarization (Carbonell and Goldstein, 1998; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010; Shen and Li, 2010) correspond to submodular function opti-  further evidence that submodular functions are a natural ﬁt for document summarization.  mization, a property not explicitly mentioned in these publications. We take this fact, however, as testament  to the value of submodular functions for summariza-  
We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system. 
We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-speciﬁed constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we ﬁnd that a small set of constraints is applicable across the domains, and that using domain-speciﬁc constraints can further improve performance. 1 
 use supervised learning of relation-speciﬁc exam-  ples, which can achieve high precision and recall.  Information extraction (IE) holds the promise  Unfortunately, however, fully supervised methods  of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.  are limited by the availability of training data and are unlikely to scale to the thousands of relations found on the Web. A more promising approach, often called “weak” or “distant” supervision, creates its own training data by heuristically matching the contents of a database to corresponding text (Craven and Kumlien, 1999). For example, suppose that r(e1, e2) = Founded(Jobs, Apple) is a ground tuple in the database and s =“Steve Jobs founded Apple, Inc.” is a sentence containing synonyms for both e1 = Jobs and e2 = Apple, then s may be a natural language expression of the fact that r(e1, e2) holds and could be a useful training example. While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles (Hoffmann et al., 2010)), Riedel et al. (2010) observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles). To ﬁx this problem they cast weak supervision as a form of  multi-instance learning, assuming only that at least  
In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difﬁcult to identify. We propose a novel algorithmic approach to RE that starts by ﬁrst identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identiﬁed, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides signiﬁcant improvement in RE performance. 
Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource deprived language (L1) can beneﬁt from the annotation work done in a resource rich language (L2) via parameter projection. However, this method assumes the presence of sufﬁcient annotated data in one resource rich language which may not always be possible. Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of L1 is used to annotate the untagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high conﬁdence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and signiﬁcantly reduces annotation cost. 
Resolving polysemy and synonymy is required for high-quality information extraction. We present ConceptResolver, a component for the Never-Ending Language Learner (NELL) (Carlson et al., 2010) that handles both phenomena by identifying the latent concepts that noun phrases refer to. ConceptResolver performs both word sense induction and synonym resolution on relations extracted from text using an ontology and a small amount of labeled data. Domain knowledge (the ontology) guides concept creation by deﬁning a set of possible semantic types for concepts. Word sense induction is performed by inferring a set of semantic types for each noun phrase. Synonym detection exploits redundant information to train several domain-speciﬁc synonym classiﬁers in a semi-supervised fashion. When ConceptResolver is run on NELL’s knowledge base, 87% of the word senses it creates correspond to real-world concepts, and 85% of noun phrases that it suggests refer to the same concept are indeed synonyms. 
Negation is present in all human languages and it is used to reverse the polarity of part of statements that are otherwise afﬁrmative by default. A negated statement often carries positive implicit meaning, but to pinpoint the positive part from the negative part is rather difﬁcult. This paper aims at thoroughly representing the semantics of negation by revealing implicit positive meaning. The proposed representation relies on focus of negation detection. For this, new annotation over PropBank and a learning algorithm are proposed. 
Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efﬁcient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.  (parameters)  (world)  θ  w  Semantic Parsing  Evaluation  x  z  y  (question) state with the largest area  (logical form)  (answer)  ∗∗  Alaska  x1  state 1 1 area c  z ∼ pθ(z | x) y= z w  argmax  Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.  
We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (BergKirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm. 
Extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference. In this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates. We model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs. We apply the algorithm over a large data set of extracted predicate instances, from which a resource of typed entailment rules has been recently released (Schoenmackers et al., 2010). Our results show that using global transitivity information substantially improves performance over this resource and several baselines, and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs. 
This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difﬁcult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporating syntax into phrase-based translation. We give a formal deﬁnition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a signiﬁcant BLEU score improvement and a large decrease in perplexity. 
We present an unsupervised model for joint phrase alignment and extraction using nonparametric Bayesian methods and inversion transduction grammars (ITGs). The key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size. 
 utilised an ITG-ﬂavour which focused on hierarchi-  While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel ﬂexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically signiﬁcant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.  cal phrase-pairs to capture context-driven translation and reordering patterns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exempliﬁed the difﬁculties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints  in the translation rules extracted, or from striving to  
Community-based question answer (Q&A) has become an important issue due to the popularity of Q&A archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in Q&A archives aims to ﬁnd historical questions that are semantically equivalent or relevant to the queried questions. In this paper, we propose a novel phrase-based translation model for question retrieval. Compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation of phrases as a whole, rather than translating single words in isolation. Experiments conducted on real Q&A data demonstrate that our proposed phrasebased translation model signiﬁcantly outperforms the state-of-the-art word-based translation model. 
Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modiﬁcation yields a signiﬁcant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 
We develop a general dynamic programming technique for the tabulation of transition-based dependency parsers, and apply it to obtain novel, polynomial-time algorithms for parsing with the arc-standard and arc-eager models. We also show how to reverse our technique to obtain new transition-based dependency parsers from existing tabular methods. Additionally, we provide a detailed discussion of the conditions under which the feature models commonly used in transition-based parsing can be integrated into our algorithms. 
 structure trees (Clark and Curran, 2009), and un-  bounded dependencies (Rimell et al., 2009).  CCGs are directly compatible with binarybranching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted gram-  The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. In this paper we ﬁll a gap in the CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that  mar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.  the scoring model can be deﬁned over actions, allowing highly efﬁcient parsing by using a greedy algorithm in which the highest scoring action (or a  small number of possible actions) is taken at each  
 VP  Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we ﬁrst present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical afﬁnities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker. 
 speciﬁcally targets the repair disﬂuencies. By com-  Unrehearsed spoken language often contains disﬂuencies. In order to correctly interpret a spoken utterance, any such disﬂuencies must be identiﬁed and removed or otherwise dealt with. Operating on transcripts of  bining language models and using an appropriate loss function in a log-linear reranker we are able to achieve f-scores which are higher than previously reported. Often in natural language processing algorithms,  speech which contain disﬂuencies, we study  more data is more important than better algorithms  the effect of language model and loss function on the performance of a linear reranker that rescores the 25-best output of a noisychannel model. We show that language models trained on large amounts of non-speech data improve performance more than a lan-  (Brill and Banko, 2001). It is this insight that drives the ﬁrst part of the work described in this paper. This paper investigates how we can use language models trained on large corpora to increase repair detection accuracy performance.  guage model trained on a more modest amount  There are three main innovations in this paper.  of speech data, and that optimising f-score rather than log loss improves disﬂuency detection performance. Our approach uses a log-linear reranker, operating on the top n analyses of a noisy channel model. We use large language models, introduce new features into this reranker and examine different optimisation strategies. We obtain a disﬂuency detection f-scores of 0.838 which improves upon the current state-of-the-  First, we investigate the use of a variety of language models trained from text or speech corpora of various genres and sizes. The largest available language models are based on written text: we investigate the effect of written text language models as opposed to language models based on speech transcripts. Second, we develop a new set of reranker features explicitly designed to capture important properties of speech repairs. Many of these features are lexically  art.  grounded and provide a large performance increase.  Third, we utilise a loss function, approximate ex-  
Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of subword units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively.  Hybrid word/sub-word recognizers can produce a sequence of sub-word units in place of OOV words. Ideally, the recognizer outputs a complete word for in-vocabulary (IV) utterances, and sub-word units for OOVs. Consider the word “Slobodan”, the given name of the former president of Serbia. As an uncommon English word, it is unlikely to be in the vocabulary of an English recognizer. While a LVCSR system would output the closest known words (e.x. “slow it dawn”), a hybrid system could output a sequence of multi-phoneme units: s l ow, b ax, d ae n. The latter is more useful for automatically recovering the word’s orthographic form, identifying that an OOV was spoken, or improving performance of a spoken term detection system with OOV queries. In fact, hybrid systems have improved OOV spoken term detection (Mamou et al., 2007; Parada et al., 2009), achieved better phone error rates, especially in OOV regions (Rastrow et al., 2009b), and obtained state-of-the-art performance for OOV detection (Parada et al., 2010).  
This paper focuses on identifying, extracting and evaluating features related to syntactic complexity of spontaneous spoken responses as part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspects considered important in the construct of communicative competence. Our goal is to find effective features, selected from a large set of features proposed previously and some new features designed in analogous ways from a syntactic complexity perspective that correlate well with human ratings of the same spoken responses, and to build automatic scoring models based on the most promising features by using machine learning methods. On human transcriptions with manually annotated clause and sentence boundaries, our best scoring model achieves an overall Pearson correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2. 
In this paper, we adopt an n-best rescoring scheme using pitch-accent patterns to improve automatic speech recognition (ASR) performance. The pitch-accent model is decoupled from the main ASR system, thus allowing us to develop it independently. N-best hypotheses from recognizers are rescored by additional scores that measure the correlation of the pitch-accent patterns between the acoustic signal and lexical cues. To test the robustness of our algorithm, we use two different data sets and recognition setups: the ﬁrst one is English radio news data that has pitch accent labels, but the recognizer is trained from a small amount of data and has high error rate; the second one is English broadcast news data using a state-of-the-art SRI recognizer. Our experimental results demonstrate that our approach is able to reduce word error rate relatively by about 3%. This gain is consistent across the two different tests, showing promising future directions of incorporating prosodic information to improve speech recognition. 
The automatic coding of clinical documents is an important task for today’s healthcare providers. Though it can be viewed as multi-label document classiﬁcation, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: ﬁrst, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By conﬁrming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84. 
 textual analysis. Research to date has concentrated  In this work we address the task of computerassisted assessment of short student answers. We combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation. We also present a ﬁrst attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers.  on two subtasks of CAA: grading essay responses, which includes checking the style, grammaticality, and coherence of the essay (Higgins et al., 2004), and the assessment of short student answers (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Mohler and Mihalcea, 2009), which is the focus of this work. An automatic short answer grading system is one that automatically assigns a grade to an answer provided by a student, usually by comparing it to one or more correct answers. Note that this is different  from the related tasks of paraphrase detection and  
We investigate whether wording, stylistic choices, and online behavior can be used to predict the age category of blog authors. Our hypothesis is that signiﬁcant changes in writing style distinguish pre-social media bloggers from post-social media bloggers. Through experimentation with a range of years, we found that the birth dates of students in college at the time when social media such as AIM, SMS text messaging, MySpace and Facebook ﬁrst became popular, enable accurate age prediction. We also show that internet writing characteristics are important features for age prediction, but that lexical content is also needed to produce signiﬁcantly more accurate results. Our best results allow for 81.57% accuracy. 
Sociolinguists have long argued that social context influences language use in all manner of ways, resulting in lects 1 . This paper explores a text classification problem we will call lect modeling, an example of what has been termed computational sociolinguistics. In particular, we use machine learning techniques to identify social power relationships between members of a social network, based purely on the content of their interpersonal communication. We rely on statistical methods, as opposed to language-specific engineering, to extract features which represent vocabulary and grammar usage indicative of social power lect. We then apply support vector machines to model the social power lects representing superior-subordinate communication in the Enron email corpus. Our results validate the treatment of lect modeling as a text classification problem – albeit a hard one – and constitute a case for future research in computational sociolinguistics. 
 commonly used features are described by Soon et al.  (2001).  In this paper, we present an unsupervised framework that bootstraps a complete coreference resolution (CoRe) system from word associations mined from a large unlabeled corpus. We show that word associations are useful for CoRe – e.g., the strong association between Obama and President is an indicator of likely coreference. Association information  Most existing systems are supervised systems, trained on human-labeled benchmark data sets for English. These systems use linguistic features based on number, gender, person etc. It is a challenge to adapt these systems to new domains, genres and languages because a signiﬁcant human labeling effort is usually necessary to get good performance.  has so far not been used in CoRe because it is sparse and difﬁcult to learn from small labeled corpora. Since unlabeled text is readily available, our unsupervised approach addresses the sparseness problem. In a self-training framework, we train a decision tree on a corpus that is automatically labeled using word associations. We show that this unsupervised system  To address this challenge, we pursue an unsupervised self-training approach. We train a classiﬁer on a corpus that is automatically labeled using association information. Self-training approaches usually include the use of some manually labeled data. In contrast, our self-trained system is not trained on any manually labeled data and is therefore a com-  has better CoRe performance than other learning approaches that do not use manually labeled data.  pletely unsupervised system. Although training on automatically labeled data can be viewed as a form of supervision, we reserve the term supervised sys-  tem for systems that are trained on manually labeled  
Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach. 
We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. We show that this new model outperforms several baselines and competing models, as well as a direct translation of the Denis / Baldridge model, for both Italian and Japanese zero anaphora. We incorporate our model in complete anaphoric resolvers for both Italian and Japanese, showing that our approach leads to improved performance also when not used in isolation, provided that separate classiﬁers are used for zeros and for explicitly realized anaphors. 
 2002), thereby obviating the need for world knowl-  While world knowledge has been shown to improve learning-based coreference resolvers, the improvements were typically obtained by incorporating world knowledge into a fairly weak baseline resolver. Hence, it is not clear whether these beneﬁts can carry over to a stronger baseline. Moreover, since there has been no attempt to apply different sources of world knowledge in combination to coreference resolution, it is not clear whether they offer complementary beneﬁts to a resolver. We systematically compare commonly-used and under-investigated sources of world knowledge for coreference resolution by applying them to two learning-based coreference models and evaluating them on documents annotated with two different annotation schemes.  edge. However, since these heuristics are not perfect, complementing them with world knowledge would be an important step towards bringing coreference systems to the next level of performance. Despite the usefulness of world knowledge for coreference resolution, early learning-based coreference resolvers have relied mostly on morphosyntactic features (e.g., Soon et al. (2001), Ng and Cardie (2002), Yang et al. (2003)). With recent advances in lexical semantics research and the development of large-scale knowledge bases, researchers have begun to employ world knowledge for coreference resolution. World knowledge is extracted primarily from three data sources, web-based encyclopedia (e.g., Ponzetto and Strube (2006), Uryupina et al. (2011)), unannotated data (e.g., Daume´ III  and Marcu (2005), Ng (2007)), and coreference-  
The local multi bottom-up tree transducer is introduced and related to the (non-contiguous) synchronous tree sequence substitution grammar. It is then shown how to obtain a weighted local multi bottom-up tree transducer from a bilingual and biparsed corpus. Finally, the problem of non-preservation of regularity is addressed. Three properties that ensure preservation are introduced, and it is discussed how to adjust the rule extraction process such that they are automatically fulﬁlled. 
Tree-to-string translation is syntax-aware and efﬁcient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the ﬁrst best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve signiﬁcant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and signiﬁcant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks. 
We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and ﬁnd the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically signiﬁcant. 
 rules. Although this approach does improve trans-  Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding ineﬃcient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using Bleu) as composed rules.  lation quality dramatically by weakening the independence assumptions in the translation model, they suﬀer from two main problems. First, composition can cause a combinatorial explosion in the number of rules. To avoid this, ad-hoc limits are placed during composition, like upper bounds on the number of nodes in the composed rule, or the height of the rule. Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar. Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down. Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules. For example, Galley et al. (2004) initially built a syntax-based system using only minimal rules, and subsequently  
 that of many existing semi-supervised systems,  In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an  despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justiﬁably so, as the most  elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.  discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Grifﬁths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types  (Brown et al., 1992) and the incorporation of  
 OCR/HR output is legal and/or probable. When an  Arabic handwriting recognition (HR) is a challenging problem due to Arabic’s connected letter forms, consonantal diacritics and rich morphology. In this paper we isolate the task of identiﬁcation of erroneous words in HR from the task of producing corrections for these words. We consider a variety of linguistic (morphological and syntactic) and non-linguistic features to automatically identify these errors. Our best approach achieves a roughly ∼15% absolute increase in F-score over a simple but reasonable baseline. A detailed error analysis shows that linguistic features, such as lemma (i.e., citation form) models, help improve HR-error detection precisely where we expect them to: semantically incoherent error words.  illegal word or phrase is discovered (error detection), these systems usually attempt to generate a legal alternative (error correction). In this paper, we present a HR error detection system that uses deep lexical and morphological feature models to locate possible "problem zones" – words or phrases that are likely incorrect – in Arabic HR output. We use an off-the-shelf HR system (Natarajan et al., 2008; Saleem et al., 2009) to generate an N-best list of hypotheses for each of several scanned segments of Arabic handwriting. Our problem zone detection (PZD) system then tags the potentially erroneous (problem) words. A subsequent HR post-processing system can then focus its effort on these words when generating additional alternative hypotheses. We only discuss the PZD system and not the task of  
Most previous studies of morphological disambiguation and dependency parsing have been pursued independently. Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the “pipeline” approach, assuming that morphological information has been separately obtained. However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. In evaluations on various highly-inﬂected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection. 
This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets. 
Spelling correction for keyword-search queries is challenging in restricted domains such as personal email (or desktop) search, due to the scarcity of query logs, and due to the specialized nature of the domain. For that task, this paper presents an algorithm that is based on statistics from the corpus data (rather than the query log). This algorithm, which employs a simple graph-based approach, can incorporate different types of data sources with different levels of reliability (e.g., email subject vs. email body), and can handle complex spelling errors like splitting and merging of words. An experimental study shows the superiority of the algorithm over existing alternatives in the email domain. 
We present a novel approach to grammatical error correction based on Alternating Structure Optimization. As part of our work, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using various feature sets. Our experiments show that our approach outperforms two baselines trained on non-learner text and learner text, respectively. Our approach also outperforms two commercial grammar checking software packages. 
We consider the problem of correcting errors made by English as a Second Language (ESL) writers and address two issues that are essential to making progress in ESL error correction - algorithm selection and model adaptation to the ﬁrst language of the ESL learner. A variety of learning algorithms have been applied to correct ESL mistakes, but often comparisons were made between incomparable data sets. We conduct an extensive, fair comparison of four popular learning methods for the task, reversing conclusions from earlier evaluations. Our results hold for different training sets, genres, and feature sets. A second key issue in ESL error correction is the adaptation of a model to the ﬁrst language of the writer. Errors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. We propose a novel way to adapt a learned algorithm to the ﬁrst language of the writer that is both cheaper to implement and performs better than other adaptation methods. 
 spell checkers such as Aspell do not take context  Automated grammar correction techniques have seen improvement over the years, but there is still much room for increased performance. Current correction techniques mainly focus on identifying and correcting a speciﬁc type of error, such as verb form misuse or preposition misuse, which restricts the correc-  into consideration, which prevents them from ﬁnding misspellings which have the same form as valid words. Also, current grammar correction systems are mostly rule-based, searching the text for deﬁned types of rule violations in the English grammar. While this approach has had some success in ﬁnding various grammatical errors, it is conﬁned to  tions to a limited scope. We introduce a novel  speciﬁcally deﬁned errors.  technique, based on a noisy channel model, which can utilize the whole sentence context to determine proper corrections. We show how to use the EM algorithm to learn the parameters of the noise model, using only a data set of erroneous sentences, given the proper language model. This frees us from the bur-  In this paper, we approach this problem by modeling various types of human errors using a noisy channel model (Shannon, 1948). Correct sentences are produced by a predeﬁned generative probabilistic model, and lesioned by the noise model. We learn the noise model parameters using an  den of acquiring a large corpora of corrected  expectation-maximization (EM) approach (Demp-  sentences. We also present a cheap and efﬁcient way to provide automated evaluation results for grammar corrections by using BLEU and METEOR, in contrast to the commonly used manual evaluations.  ster et al., 1977; Wu, 1983). Our model allows us to deduce the original intended sentence by looking for the the highest probability parses over the entire sentence, which leads to automated whole sentence spelling and grammar correction based on contex-  
Linking entities with knowledge base (entity linking) is a key issue in bridging the textual data with the structural knowledge base. Due to the name variation problem and the name ambiguity problem, the entity linking decisions are critically depending on the heterogenous knowledge of entities. In this paper, we propose a generative probabilistic model, called entitymention model, which can leverage heterogenous entity knowledge (including popularity knowledge, name knowledge and context knowledge) for the entity linking task. In our model, each name mention to be linked is modeled as a sample generated through a three-step generative story, and the entity knowledge is encoded in the distribution of entities in document P(e), the distribution of possible names of a specific entity P(s|e), and the distribution of possible contexts of a specific entity P(c|e). To find the referent entity of a name mention, our method combines the evidences from all the three distributions P(e), P(s|e) and P(c|e). Experimental results show that our method can significantly outperform the traditional methods. 
 content through a geo-centric interface. The Perseus  We investigate automatic geolocation (i.e. identiﬁcation of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document’s raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset.  project performs automatic toponym resolution on historical texts in order to display a map with each text showing the locations that are mentioned (Smith and Crane, 2001); Google Books also does this for some books, though the toponyms are identiﬁed and resolved quite crudely. Hao et al (2010) use a location-based topic model to summarize travelogues, enrich them with automatically chosen images, and provide travel recommendations. Eisenstein et al (2010) investigate questions of dialectal differences and variation in regional interests in Twitter users using a collection of geotagged tweets. An intuitive and effective strategy for summarizing geographically-based data is identiﬁcation of the location—a speciﬁc latitude and longitude—that forms the primary focus of each document. Determining a single location of a document is only  
We use search engine results to address a particularly difﬁcult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries. 
Standard algorithms for template-based information extraction (IE) require predeﬁned template schemas, and often labeled data, to learn to extract their slot ﬁllers (e.g., an embassy is the Target of a Bombing template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role ﬁllers from speciﬁc documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role ﬁllers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates. 
Argumentation schemes are structures or templates for various kinds of arguments. Given the text of an argument with premises and conclusion identiﬁed, we classify it as an instance of one of ﬁve common schemes, using features speciﬁc to each scheme. We achieve accuracies of 63–91% in one-against-others classiﬁcation and 80–94% in pairwise classiﬁcation (baseline = 50% in both cases). 
We present a novel model to represent and assess the discourse coherence of text. Our model assumes that coherent text implicitly favors certain types of discourse relation transitions. We implement this model and apply it towards the text ordering ranking task, which aims to discern an original text from a permuted ordering of its sentences. The experimental results demonstrate that our model is able to signiﬁcantly outperform the state-ofthe-art coherence model by Barzilay and Lapata (2005), reducing the error rate of the previous approach by an average of 29% over three data sets against human upper bounds. We further show that our model is synergistic with the previous approach, demonstrating an error reduction of 73% when the features from both models are combined for the task. 
This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspeciﬁcation. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspeciﬁed input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice. 
 tions and moves with which speakers display these  We present a novel computational formulation of speaker authority in discourse. This notion, which focuses on how speakers position themselves relative to each other in discourse, is ﬁrst developed into a reliable coding scheme (0.71 agreement between human annotators). We also provide a computational  types of positioning (Stolcke et al., 2000). To complement these bodies of work, we choose to focus on the question of how speakers position themselves as authoritative in a discourse. This means that we must describe the way speakers introduce new topics or discussions into the discourse; the way they position themselves relative to that  model for automatically annotating text using this coding scheme, using supervised learning enhanced by constraints implemented with Integer Linear Programming. We show that this constrained model’s analyses of speaker authority correlates very strongly with expert human judgments (r2 coefﬁcient of 0.947).  topic; and how these functions interact with each other. While all of the tasks mentioned above focus on speciﬁc problems in the larger rhetorical question of speaker positioning, none explicitly address this framing of authority. Each does have valuable ties to the work that we would like to do, and in section 2, we describe prior work in each of those areas, and  
One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores. 
This paper proposes a novel reordering model for statistical machine translation (SMT) by means of modeling the translation orders of the source language collocations. The model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. During decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 
 depend on preceding reordering decisions. This pro-  vides a natural reordering mechanism which is able  We present a novel machine translation model  to deal with local and long-distance reorderings in a  which models translation by a linear sequence of operations. In contrast to the “N-gram” model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance reorderings in a uniﬁed way, and (ii) a joint sequence model for the translation and reordering probabilities which is more ﬂexible than standard phrase-based MT. We observe statistically signiﬁcant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.  consistent way. Our approach can be viewed as an extension of the N-gram SMT approach (Marin˜o et al., 2006) but our model does reordering as an integral part of a generative model. The paper is organized as follows. Section 2 discusses the relation of our work to phrase-based and the N-gram SMT. Section 3 describes our generative story. Section 4 deﬁnes the probability model, which is ﬁrst presented as a generative model, and then shifted to a discriminative framework. Section 5 provides details on the search strategy. Section 6 explains the training process. Section 7 describes the experimental setup and results. Section 8 gives a few examples illustrating different aspects of our  model and Section 9 concludes the paper.  
A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difﬁculty incurred by their subsequent disconﬁrmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difﬁculty. We also present a behavioral experiment conﬁrming the key empirical predictions of the theory. 
When designing grammars of natural language, typically, more than one formal analysis can account for a given phenomenon. Moreover, because analyses interact, the choices made by the engineer inﬂuence the possibilities available in further grammar development. The order in which phenomena are treated may therefore have a major impact on the resulting grammar. This paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering. I argue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses. The idea is illustrated through a comparative study of auxiliary structures in HPSG-based grammars for German and Dutch. Auxiliaries form a central phenomenon of German and Dutch and are likely to inﬂuence many components of the grammar. This study shows that a special auxiliary+verb construction signiﬁcantly improves efﬁciency compared to the standard argument-composition analysis for both parsing and generation. 
We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing—the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic ﬁnite-state methods, produces better results than relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. These ﬁnite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL.  ing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL is a non-probabilistic algorithm based on a complex set of inter-relating heuristics and a non-standard (though interesting) representation of constituent trees. This makes it hard to extend. Note that although Reichart and Rappoport (2010) improve on Seginer’s results, they do so by selecting training sets to best match the particular test sentences—CCL itself is used without modiﬁcation. Ponvert et al. (2010) explore an alternative strategy of unsupervised partial parsing: directly predicting low-level constituents based solely on word co-occurrence frequencies. Essentially, this means segmenting raw text into multiword constituents. In that paper, we show—somewhat surprisingly—that  CCL’s performance is mostly dependent on its ef-  
We propose an automatic method of extracting paraphrases from deﬁnition sentences, which are also automatically acquired from the Web. We observe that a huge number of concepts are deﬁned in Web documents, and that the sentences that deﬁne the same concept tend to convey mostly the same information using different expressions and thus contain many paraphrases. We show that a large number of paraphrases can be automatically extracted with high precision by regarding the sentences that deﬁne the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 × 108 Web documents with a precision rate of about 94%.  concept. The method is based on our observation that two sentences deﬁning the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such deﬁnition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the deﬁnition sentences on the Web. For instance, the following two sentences, both of which deﬁne the same concept “osteoporosis”, include two pairs of phrasal paraphrases, which are indicated by underlines 1 and 2 , respectively. (1) a. Osteoporosis is a disease that 1 decreases the quantity of bone and 2 makes bones fragile. b. Osteoporosis is a disease that 1 reduces bone mass and 2 increases the risk of bone fracture.  
We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reﬂect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a network of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity. 
In this work, we present a novel approach to the generation task of ordering prenominal modiﬁers. We take a maximum entropy reranking approach to the problem which admits arbitrary features on a permutation of modiﬁers, exploiting hundreds of thousands of features in total. We compare our error rates to the state-of-the-art and to a strong Google ngram count baseline. We attain a maximum error reduction of 69.8% and average error reduction across all test sets of 59.1% compared to the state-of-the-art and a maximum error reduction of 68.4% and average error reduction across all test sets of 41.8% compared to our Google n-gram count baseline. 
 realized as a prepositional phrase in (1a) and as a  In this paper we describe an unsupervised  subject in (1b).  method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers. We present an algorithm that iteratively splits and merges clusters represent-  (1) a. [Joe]A0 broke the [window]A1 with a [rock]A2. b. The [rock]A2 broke the [window]A1. c. The [window]A1 broke.  ing semantic roles, thereby leading from an initial clustering to a ﬁnal clustering of better quality. The method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently. By com-  The semantic roles in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations. Under the  bining role induction with a rule-based com-  PropBank annotation framework (which we will as-  ponent for argument identiﬁcation we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin.  sume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are speciﬁc to that predicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admit-  
Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. The state-of-the-art research on the task is transductive inference (e.g. cross-event inference). In this paper, we propose a new method of event extraction by well using cross-entity inference. In contrast to previous inference methods, we regard entitytype consistency as key feature to predict event mentions. We adopt this inference method to improve the traditional sentence-level event extraction system. Experiments show that we can get 8.6% gain in trigger (event) identification, and more than 11.8% gain for argument (role) classification in ACE event extraction. 
 most event extraction systems primarily recognize  The goal of our research is to improve event extraction by learning to identify secondary role ﬁller contexts in the absence of event keywords. We propose a multilayered event extraction architecture that progressively “zooms in” on relevant information. Our extraction model includes a document genre classiﬁer to recognize event narratives, two types of sentence classiﬁers, and noun phrase classiﬁers to extract role ﬁllers. These modules are organized as a pipeline to gradually zero in on event-related information. We present results on the MUC-4 event extraction data set and show that this model performs better than previous systems.  contexts that explicitly refer to a relevant event. For example, a system that extracts information about murders will recognize expressions associated with murder (e.g., “killed”, “assassinated”, or “shot to death”) and extract role ﬁllers from the surrounding context. But many role ﬁllers occur in contexts that do not explicitly mention the event, and those ﬁllers are often overlooked. For example, the perpetrator of a murder may be mentioned in the context of an arrest, an eyewitness report, or speculation about possible suspects. Victims may be named in sentences that discuss the aftermath of the event, such as the identiﬁcation of bodies, transportation of the injured to a hospital, or conclusions drawn from an investigation. We will refer to these types of  
In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is done through two tasks, Entity Linking – linking names in context to entities in the KB – and Slot Filling – adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes (“slots”) derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges. 
This paper focuses on mining the hyponymy (or is-a) relation from large-scale, open-domain web documents. A nonlinear probabilistic model is exploited to model the correlation between sentences in the aggregation of pattern matching results. Based on the model, we design a set of evidence combination and propagation algorithms. These significantly improve the result quality of existing approaches. Experimental results conducted on 500 million web pages and hypernym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision. 
 and Wellner, 2003) decompose the task into a col-  This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can ﬁnd its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.  lection of pairwise or mention set coreference decisions. Decisions for each pair or each group of mentions are based on probabilities of features extracted by discriminative learning models. The aforementioned approaches have proven to be fruitful; however, there are some notable problems. Pairwise modeling may fail to produce coherent partitions. That is, if we link results of pairwise decisions to each other, there may be conﬂicting coreferences. Graph-partitioning methods attempt to reconcile pairwise scores into a ﬁnal coherent clustering, but they are combinatorially harder to work with in discriminative approaches.  
e evlute severl populr models of lol disourse oherene for domin nd tsk genE erlity y pplying them to ht disentngleE mentF sing experiments on syntheti multiE prty onverstionsD we show tht most modE els trnsfer well from text to dilogueF goE herene models improve results overll when good prses nd topi models re villeD nd on  onstrined tsk for rel ht dtF 
Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 
We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difﬁcult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents. 
The availability of learner corpora, especially those which have been manually error-tagged or shallow-parsed, is still limited. This means that researchers do not have a common development and test set for natural language processing of learner English such as for grammatical error detection. Given this background, we created a novel learner corpus that was manually error-tagged and shallowparsed. This corpus is available for research and educational purposes on the web. In this paper, we describe it in detail together with its data-collection method and annotation schemes. Another contribution of this paper is that we take the ﬁrst step toward evaluating the performance of existing POStagging/chunking techniques on learner corpora using the created corpus. These contributions will facilitate further research in related areas such as grammatical error detection and automated essay scoring. 
 Arabic–English, where there is ample data. How-  ever, large bilingual parallel corpora exist for rela-  Naively collecting translations by crowdsourcing the task to non-professional translators yields disﬂuent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Speciﬁcally, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-toEnglish evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation.  tively few languages pairs. There are various options for creating new train- ing resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars.  In this paper we examine the idea of creating low  
 Such problems can be abstracted as adding addi-  In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms signiﬁcantly published results.  tional annotations to an existing tree structure. For example, the English Penn treebank (Marcus et al., 1993) contains function tags and many carry semantic information. To add semantic information to the basic syntactic trees, a logical step is to predict these function tags after syntactic parsing. For the problem of predicting projectable syntactic constituent, one can use a sentence alignment tool and syntactic trees on source sentences to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the  
We present a discriminative learning method to improve the consistency of translations in phrase-based Statistical Machine Translation (SMT) systems. Our method is inspired by Translation Memory (TM) systems which are widely used by human translators in industrial settings. We constrain the translation of an input sentence using the most similar ‘translation example’ retrieved from the TM. Differently from previous research which used simple fuzzy match thresholds, these constraints are imposed using discriminative learning to optimise the translation performance. We observe that using this method can beneﬁt the SMT system by not only producing consistent translations, but also improved translation outputs. We report a 0.9 point improvement in terms of BLEU score on English–Chinese technical documents. 
This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes independently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 
We present minimum Bayes-risk system combination, a method that integrates consensus decoding and system combination into a uniﬁed multi-system minimum Bayes-risk (MBR) technique. Unlike other MBR methods that re-rank translations of a single SMT system, MBR system combination uses the MBR decision rule and a linear combination of the component systems’ probability distributions to search for the minimum risk translation among all the ﬁnite-length strings over the output vocabulary. We introduce expected BLEU, an approximation to the BLEU score that allows to efﬁciently apply MBR in these conditions. MBR system combination is a general method that is independent of speciﬁc SMT models, enabling us to combine systems with heterogeneous structure. Experiments show that our approach bring signiﬁcant improvements to single-system-based MBR decoding and achieves comparable results to different state-of-the-art system combination methods. 
 provably goes beyond the expressive power of syn-  We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.  chronous CFG and TSG. Therefore, it is necessary to ﬁnd ways to take advantage of more powerful synchronous grammars to improve machine translation. Synchronous tree adjoining grammars (TAG) (Shieber and Schabes, 1990) are a good candidate. As a formal tree rewriting system, TAG (Joshi et al., 1975; Joshi, 1985) provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution  
We propose a novel approach to translating from a morphologically complex language. Unlike previous research, which has targeted word inﬂections and concatenations, we focus on the pairwise relationship between morphologically related words, which we treat as potential paraphrases and handle using paraphrasing techniques at the word, phrase, and sentence level. An important advantage of this framework is that it can cope with derivational morphology, which has so far remained largely beyond the capabilities of statistical machine translation systems. Our experiments translating from Malay, whose morphology is mostly derivational, into English show significant improvements over rivaling approaches based on ﬁve automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens). 
 French ne voudrais pas voyager par chemin de fer  We propose a principled and efﬁcient phraseto-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semiMarkov model, word-to-phrase and phraseto-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overﬁtting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include “gappy phrases” (such as French ne pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime. 
While it is has often been observed that the product of translation is somehow different than non-translated text, scholars have emphasized two distinct bases for such differences. Some have noted interference from the source language spilling over into translation in a source-language-specific way, while others have noted general effects of the process of translation that are independent of source language. Using a series of text categorization experiments, we show that both these effects exist and that, moreover, there is a continuum between them. There are many effects of translation that are consistent among texts translated from a given source language, some of which are consistent even among texts translated from families of source languages. Significantly, we find that even for widely unrelated source languages and multiple genres, differences between translated texts and non-translated texts are sufficient for a learned classifier to accurately determine if a given text is translated or original. 
We present a ﬁrst known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classiﬁcation. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1 to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classiﬁcation of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data. 
 (e.g. WordNet, VerbOcean, FrameNet) and deep  This paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment. We claim that, in spite of the inherent difﬁculties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference. We experiment with a phrasal matching method in order to: i) build a system portable across languages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge.  processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognizers and normalizers) has to confront, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating languagespeciﬁc components into the same cross-lingual architecture. As a ﬁrst step to overcome these problems, (Mehdad et al., 2010) proposes a “basic solution”, that brings CLTE back to the monolingual scenario by translating H into the language of T. Despite the advantages in terms of modularity and portability of the architecture, and the promising experimental results, this approach suffers from one main limitation which motivates the investigation on alternative solutions. Decoupling machine translation (MT) and TE, in fact, ties CLTE performance to the availability of MT components, and to the quality of the translations. As a consequence, on one side trans-  
Resolving coordination ambiguity is a classic hard problem. This paper looks at coordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don’t do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classiﬁers with monolingual and bilingual features and iteratively improve them via co-training. The co-trained classiﬁer achieves close to 96% accuracy on Treebank data and makes 20% fewer errors than a supervised system trained with Treebank annotations. 
We propose a novel unsupervised method for separating out distinct authorial components of a document. In particular, we show that, given a book artificially “munged” from two thematically similar biblical books, we can separate out the two constituent books almost perfectly. This allows us to automatically recapitulate many conclusions reached by Bible scholars over centuries of research. One of the key elements of our method is exploitation of differences in synonym choice by different authors. 
 our method identiﬁes a small subset of lexical items  We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. Using aggregate demographic statistics about the authors’ geographic communities, we solve a multi-output  that are most inﬂuenced by demographics, and discovers conjunctions of demographic attributes that are especially salient for lexical variation. Sociolinguistic associations are difﬁcult to model, because the space of potentially relevant interactions  regression problem between demographics and lexical frequencies. By imposing a composite 1,∞ regularizer, we obtain structured sparsity, driving entire rows of coefﬁcients to zero. We perform two regression studies. First, we use term frequencies to predict demographic attributes; our method identiﬁes a compact set of words that are strongly associated with author demographics. Next, we  is large and complex. On the linguistic side there are thousands of possible variables, even if we limit ourselves to unigram lexical features. On the demographic side, the interaction between demographic attributes is often non-linear: for example, gender may negate or amplify class-based language differences (Zhang, 2005). Thus, additive models which assume that each demographic attribute makes a lin-  conjoin demographic attributes into features, which we use to predict term frequencies. The composite regularizer identiﬁes a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.  ear contribution are inadequate. In this paper, we explore the large space of potential sociolinguistic associations using structured sparsity. We treat the relationship between language and demographics as a set of multi-input, multi-  output regression problems. The regression coefﬁ-  
 Previous studies on Wikiﬁcation differ with re-  spect to the corpora they address and the subset  Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call “global” approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.  of expressions they attempt to link. For example, some studies focus on linking only named entities, whereas others attempt to link all “interesting” expressions, mimicking the link structure found in Wikipedia. Regardless, all Wikiﬁcation systems are faced with a key Disambiguation to Wikipedia (D2W) task. In the D2W task, we’re given a text along with explicitly identiﬁed substrings (called mentions) to disambiguate, and the goal is to output the corresponding Wikipedia page, if any, for each mention. For example, given the input sentence “I am visiting friends in <Chicago>,” we output http://en.wikipedia.org/wiki/Chicago – the Wikipedia page for the city of Chicago, Illinois, and not (for example) the page for the 2002 ﬁlm of the same name. Local D2W approaches disambiguate each mention in a document separately, utilizing clues such  as the textual similarity between the document and  
 space, which makes efﬁcient decoding and struc-  The large combined search space of joint word segmentation and Part-of-Speech (POS) tagging makes efﬁcient decoding very hard. As a result, effective high order features representing rich contexts are inconvenient to use. In this work, we propose a novel stacked subword model for this task, concerning both ef-  tured learning of parameters very hard. Moreover, the representation ability of models is limited since using rich contextual word features makes the search intractable. To overcome such efﬁciency and effectiveness limitations, the approximate inference and reranking techniques have been explored in previous work (Zhang and Clark, 2010; Jiang et al., 2008b).  ﬁciency and effectiveness. Our solution is a two step process. First, one word-based segmenter, one character-based segmenter and one local character classiﬁer are trained to produce coarse segmentation and POS information. Second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with  In this paper, we present an effective and efﬁcient solution for joint Chinese word segmentation and POS tagging. Our work is motivated by several characteristics of this problem. First of all, a majority of words are easy to identify in the segmentation problem. For example, a simple maximum matching segmenter can achieve an f-score of about 90.  POS tags by a ﬁne-grained sub-word tagger. The coarse-to-ﬁne search scheme is efﬁcient, while in the sub-word tagging step rich contextual features can be approximately derived. Evaluation on the Penn Chinese Treebank shows that our model yields improvements over the best system reported in the literature.  We will show that it is possible to improve the efﬁciency and accuracy by using different strategies for different words. Second, segmenters designed with different views have complementary strength. We argue that the agreements and disagreements of different solvers can be used to construct an intermediate sub-word structure for joint segmentation  and tagging. Since the sub-words are large enough  
Translating compounds is an important problem in machine translation. Since many compounds have not been observed during training, they pose a challenge for translation systems. Previous decompounding methods have often been restricted to a small set of languages as they cannot deal with more complex compound forming processes. We present a novel and unsupervised method to learn the compound parts and morphological operations needed to split compounds into their compound parts. The method uses a bilingual corpus to learn the morphological operations required to split a compound into its parts. Furthermore, monolingual corpora are used to learn and filter the set of compound part candidates. We evaluate our method within a machine translation task and show significant improvements for various languages to show the versatility of the approach. 
Lots of Chinese characters are very productive in that they can form many structured words either as preﬁxes or as sufﬁxes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a uniﬁed way. 
There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn’t a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct. 
 ty answers from the archives to be retrieved, we need  In this paper we address the problem of question recommendation from large archives of community question answering data by exploiting the users’ information needs. Our experimental results indicate that questions based on the same or similar information need can provide excellent question recommenda-  to search CQA archives of previous questions that are closely associated with answers. If a question is found to be interesting to the user, then a previous answer can be provided with very little delay. Question search and question recommendation are proposed to facilitate ﬁnding highly relevant or potentially interesting questions. Given a user’s ques-  tion. We show that translation model can be effectively utilized to predict the information need given only the user’s query question. Experiments show that the proposed information need prediction approach can improve the performance of question recommendation.  tion as the query, question search tries to return the most semantically similar questions from the question archives. As the complement of question search, we deﬁne question recommendation as recommending questions whose information need is the same or similar to the user’s original question. For  
We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identiﬁcation accuracy and over 13% absolute improvement in full frame-semantic parsing F1 score on a blind test set, over a state-of-the-art supervised baseline. 
We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modiﬁcation of the MetropolisHastings split-merge sampler, resulting in an efﬁcient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain. 
 S  This paper presents an unsupervised method for deriving inference axioms by composing semantic relations. The method is independent of any particular relation inventory. It relies on describing semantic relations using primitives and manipulating these primitives according to an algebra. The method was tested using a set of eight semantic relations yielding 78 inference axioms which were evaluated over PropBank. 
Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicateargument structures like “quarterbacks throw passes to receivers”. Using several statistical measures, we show that our model is able to generalize and explain the data statistically signiﬁcantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems. 
 Current approaches for this task take a data driven  approach (Zettlemoyer and Collins, 2007; Wong and  Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difﬁcult to obtain. This supervision bottleneck is one of the major difﬁculties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by conﬁdence estimation. Evaluated over Geoquery, a standard dataset for this  Mooney, 2007), in which the learning algorithm is given a set of NL sentences as input and their corresponding MR, and learns a statistical semantic parser — a set of parameterized rules mapping lexical items and syntactic patterns to their MR. Given a sentence, these rules are applied recursively to derive the most probable interpretation. Since semantic interpretation is limited to the syntactic patterns observed in the training data, in order to work well these approaches require considerable amounts of annotated data. Unfortunately an-  task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.  notating sentences with their MR is a time consuming task which requires specialized domain knowledge and therefore minimizing the supervision effort is one of the key challenges in scaling semantic  parsers.  
 In this paper, we dedicate to the topic of aspect ranking, which aims to automatically identify important product aspects from online consumer reviews. The important aspects are identiﬁed according to two observations: (a) the important aspects of a product are usually commented by a large number of consumers; and (b) consumers’ opinions on the important aspects greatly inﬂuence their overall opinions on the product. In particular, given consumer reviews of a product, we ﬁrst identify the product aspects by a shallow dependency parser and determine consumers’ opinions on these aspects via a sentiment classiﬁer. We then develop an aspect ranking algorithm to identify the important aspects by simultaneously considering the aspect frequency and the inﬂuence of consumers’ opinions given to each aspect on their overall opinions. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach. We further apply the aspect ranking results to the application of documentlevel sentiment classiﬁcation, and improve the performance signiﬁcantly.  Figure 1: Sample reviews on iPhone 3GS product huge collections of consumer reviews on the Web. These reviews have become an important resource for both consumers and ﬁrms. Consumers commonly seek quality information from online consumer reviews prior to purchasing a product, while many ﬁrms use online consumer reviews as an important resource in their product development, marketing, and consumer relationship management. As illustrated in Figure 1, most online reviews express consumers’ overall opinion ratings on the product, and their opinions on multiple aspects of the product. While a product may have hundreds of aspects, we argue that some aspects are more important than  the others and have greater inﬂuence on consumers’  
 publications are two examples of document domains  This paper explores approaches to sentiment classiﬁcation of U.S. Congressional ﬂoordebate transcripts. Collective classiﬁcation techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce  where network structures have been used to assist classiﬁcation (Gantner and Schmidt-Thieme, 2009; Cao and Gao, 2005). The contribution of this research is in four parts: (1) we introduce an approach that gives better than state of the art performance for collective classiﬁcation on the ConVote corpus of congressional debate  novel approaches for incorporating the outputs of machine learners into collective classiﬁcation algorithms. Our experimental evaluation shows that the mean-ﬁeld algorithm obtains the best results for the task, signiﬁcantly outperforming the benchmark technique.  transcripts (Thomas et al., 2006); (2) we provide a comparative overview of collective document classiﬁcation techniques to assist researchers in choosing an algorithm for collective document classiﬁcation tasks; (3) we demonstrate effective novel approaches for incorporating the outputs of SVM classiﬁers into  
Building on earlier work that integrates different factors in language modeling, we view (i) backing off to a shorter history and (ii) class-based generalization as two complementary mechanisms of using a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation. This view entails that the classes in a language model should be learned from rare events only and should be preferably applied to rare events. We construct such a model and show that both training on rare events and preferable application to rare events improve perplexity when compared to a simple direct interpolation of class-based with standard language models. 
Topic models have been successfully applied to many document analysis tasks to discover topics embedded in text. However, existing topic models generally cannot capture the latent topical structures in documents. Since languages are intrinsically cohesive and coherent, modeling and discovering latent topical transition structures within documents would be beneﬁcial for many text analysis tasks.  ics, is generally not captured due to the exchangeability assumption (Blei et al., 2003), i.e., the document generation probabilities are invariant to content permutation. In reality, natural language text rarely consists of isolated, unrelated sentences, but rather collocated, structured and coherent groups of sentences (Hovy, 1993). Ignoring such latent topical structures inside the documents means wasting valuable clues about topics and thus would lead to non-optimal topic modeling.  In this work, we propose a new topic model, Structural Topic Model, which simultaneously discovers topics and reveals the latent topical structures in text through explicitly modeling topical transitions with a latent ﬁrst-order Markov chain. Experiment results show that the proposed Structural Topic Model can effectively discover topical structures in text, and the identiﬁed structures signiﬁcantly improve the performance of tasks such as sentence annotation and sentence ordering.  Taking apartment rental advertisements as an example, when people write advertisements for their apartments, it’s natural to ﬁrst introduce “size” and “address” of the apartment, and then “rent” and “contact”. Few people would talk about “restriction” ﬁrst. If this kind of topical structures are captured by a topic model, it would not only improve the topic mining results, but, more importantly, also help many other document analysis tasks, such as sentence annotation and sentence ordering.  Nevertheless, very few existing topic models at-  
 which are clearly associated with the domain of  We propose a method for automatically labelling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia articles containing the top-ranking topic terms,  stock market trading. The aim of this research is to automatically generate topic labels which explicitly identify the semantics of the topic, i.e. which take us from a list of terms requiring interpretation to a single label, such as STOCK MARKET TRADING in the  and sub-phrases extracted from the Wikipedia  above case.  article titles. We rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, signiﬁcantly better than a benchmark method.  The approach proposed in this paper is to ﬁrst generate a topic label candidate set by: (1) sourcing topic label candidates from Wikipedia by querying with the top-N topic terms; (2) identifying the top-ranked document titles; and (3) further postprocessing the document titles to extract sub-strings. We translate each topic label into features extracted  
Cross-language document summarization is defined as the task of producing a summary in a target language (e.g. Chinese) for a set of documents in a source language (e.g. English). Existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language. In this study, we propose to use the bilingual information from both the source and translated documents for this task. Two summarization methods (SimFusion and CoRank) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction. Experimental results on the DUC2001 dataset with manually translated reference Chinese summaries show the effectiveness of the proposed methods. 
In this paper, we present a novel approach which incorporates the web-derived selectional preferences to improve statistical dependency parsing. Conventional selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This paper extends previous work to wordto-word selectional preferences by using webscale data. Experiments show that web-scale data improves statistical dependency parsing, particularly for long dependency relationships. There is no data like more data, performance improves log-linearly with the number of parameters (unique N-grams). More importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance. 
It is well known that parsing accuracy suffers when a model is applied to out-of-domain data. It is also known that the most beneﬁcial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001). Hence, an important task is to select appropriate domains. However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. As more and more data becomes available, automatic ways to select data that is beneﬁcial for a new (unknown) target domain are becoming attractive. This paper evaluates various ways to automatically acquire related training data for a given test set. The results show that an unsupervised technique based on topic models is effective – it outperforms random data selection on both languages examined, English and Dutch. Moreover, the technique works better than manually assigned labels gathered from meta-data that is available for English. 
We present a systematic comparison and combination of two orthogonal techniques for efﬁcient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the ﬁrst evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model. 
We explore the contribution of morphological features – both lexical and inﬂectional – to dependency parsing of Arabic, a morphologically rich language. Using controlled experiments, we ﬁnd that deﬁniteness, person, number, gender, and the undiacritzed lemma are most helpful for parsing on automatically tagged input. We further contrast the contribution of form-based and functional features, and show that functional gender and number (e.g., “broken plurals”) and the related rationality feature improve over form-based features. It is the ﬁrst time functional morphological features are used for Arabic NLP. 
Recent work has shown how a parallel corpus can be leveraged to build syntactic parser for a target language by projecting automatic source parse onto the target sentence using word alignments. The projected target dependency parses are not always fully connected to be useful for training traditional dependency parsers. In this paper, we present a greedy non-directional parsing algorithm which doesn’t need a fully connected parse and can learn from partial parses by utilizing available structural and syntactic information in them. Our parser achieved statistically signiﬁcant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a signiﬁcant improvement over previously reported results for Bulgarian and set a benchmark for Hindi. 
The role of search queries, as available within query sessions or in isolation from one another, in examined in the context of ranking the class labels (e.g., brazilian cities, business centers, hilly sites) extracted from Web documents for various instances (e.g., rio de janeiro). The co-occurrence of a class label and an instance, in the same query or within the same query session, is used to reinforce the estimated relevance of the class label for the instance. Experiments over evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using documentbased counts. 
Text mining and data harvesting algorithms have become popular in the computational linguistics community. They employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process (or both) in a recursive cycle, using data learned in one step to generate more seeds for the next. They therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 
The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. In this paper, we study how to build a Korean comparison mining system. Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. We perform various experiments to find relevant features and learning techniques. As a result, we achieve outstanding performance enough for practical use. 
With the evolution of human lives and the spread of information, new things emerge quickly and new terms are created every day. Therefore, it is important for natural language processing systems to extract new words in progression with time. Due to the broad areas of applications, however, there might exist the mismatch of statistical characteristics between the training domain and the testing domain, which inevitably degrades the performance of word extraction. This paper proposes a scheme of word extraction in which histogram equalization for feature normalization is used. Through this scheme, the mismatch of the feature distributions due to different corpus sizes or changes of domain can be compensated for appropriately such that unknown word extraction becomes more reliable and applicable to novice domains. The scheme was initially evaluated on the corpora announced in SIGHAN2. 68.43% and 71.40% F-measures for word identification, which correspond to 66.72%/32.94% and 75.99%/58.39% recall rates for IV/OOV, respectively, were achieved for the CKIP and the CUHK test sets, respectively, using four combined features with equalization. When applied to unknown word extraction for a novice domain, this scheme can identify such pronouns as “海角七號” (Cape No. 7, the name of a film), “蠟筆小新” (Crayon Shinchan, the name of a cartoon figure), “金 融海嘯” (Financial Tsunami) and so on, which cannot be extracted reliably with rule-based approaches, although the approach appears not so good at identifying such terms as the names of humans, places, or organizations, for which the semantic structure is prominent. This scheme is complementary with the outcomes of two word segmentation systems, and is promising if other rule-based approaches could be further integrated.  ∗ Department of Information Management, National Taiwan University of Science and Technology, Tel: (886)-2-2703-1225 Fax: (886)-2-2737-6777 E-mail: bslin@cs.ntust.edu.tw; m9709104@mail.ntust.edu.tw  42  Bor-Shen Lin and Yi-Cong Chen  Keywords: Unknown Word Extraction, Word Identification, Machine Learning, Multilayer Perceptrons, Histogram Equalization. 1. Introduction With the evolution of human lives and the accelerated spread of information, new words are created quickly as new things emerge every day. It is then necessary for natural language processing systems to identify and learn new words to progress with time. Chinese word segmentation systems, for example, typically utilize large dictionaries collected over a long period of time. No matter the size of the vocabulary for the dictionaries, it is hardly possible for them to include all of the words or phrases that have been invented so far in the extensive knowledge domains, not to mention to predict in advance new terms to appear in the future. Therefore, it is more practical for Chinese word segmentation systems to use dynamic dictionaries that can be updated quickly and frequently with the new words found in the corpora of the desired domains. Hence, unknown word extraction is actually essential for quite a few natural language processing systems. It is also useful for exploring hot or new terms for desired knowledge domains or internet communities. The approaches to unknown word extraction can be roughly divided into two categories, rule-based approaches and statistical approaches. For rule-based approaches, semantic rules for specific types of words, such as the names of humans, places, and organizations, normally are specially designed (Sun et al., 1994). For statistical approaches, statistical features in corpora typically have been computed and used for the decision in the threshold test. Occurrence frequency, for example, is a widely used feature (Lu et al., 2004). In such approaches, the threshold is often obtained heuristically and might depend highly on the corpus. In addition, statistical approaches and rule-based approaches can be combined. Some approaches have used statistical features obtained from the corpus and have designed rules for various types of unknown words based on these features, through which even the unknown words with low occurrence frequency can be extracted (Chen et al., 2002). For most of the approaches, the decision rules are obtained from the training corpus heuristically, and perhaps cannot be applied to the testing domain. Therefore, use of machine learning approaches with more general features is suggested in order to obtain the decision boundary by learning automatically. Liang, for example, proposed a tri-syllable filter for screening the word candidates and the artificial neural network with statistical features for the final decision (Liang et al., 2000). Nevertheless, the trained artificial neural network is not shown to be able to be applied to novice domains. Besides, Goh et al. made use of the character features (the POS and position) in support vector machine to extract new words (Goh et al., 2003). To reduce the dependency of the word extraction scheme on the training corpus so that use in diverse or novice domains becomes possible, this paper utilizes the machine learning  Histogram Equalization on Statistical Approaches for  43  Chinese Unknown Word Extraction  approaches to combine the statistical features. Histogram equalization for statistical features was further introduced to compensate for the mismatch between the training and testing corpora that might come from the difference in corpus size or the change of the domain. It is then unnecessary to retrain the model parameters, and the extraction approach becomes more general for new domains. This scheme was first evaluated on SIGHAN2 corpora for traditional Chinese provided by Chinese Knowledge Information Processing Group (CKIP) and City University of Hong Kong (CUHK). When combing four heterogeneous statistical features, DLG, AV, Link, and PreC, and applying histogram equalization for DLG, the F-measures of 68.43% and 71.40% for within-domain CKIP corpus and cross-domain CUHK corpus, respectively, can be achieved. This scheme was finally used to explore unknown words in a novice domain of a news event. When compared with the words extracted by two word segmentation systems provided by CKIP and Institute of Computing Technology Chinese Academy of Science (ICTCAS), it was found that this approach is complementary with the other two. Such terms as “海角七號＂(Cape No. 7, the name of a film), “蠟筆小 新 ＂ (Crayon Shinchan, the name of a figure in a cartoon), “ 金 融 海 嘯 ＂ (Financial Tsunami), and so on, with prominent statistical characteristics but less structure in semantics, can be extracted successfully by the proposed approach only. These terms are hard to identify using rule-based approaches because it is difficult to draw semantic rules from such terms. Without using semantic rules, however, this extraction approach seems less robust for extracting the names of humans, places, or organizations with prominent structure. This, however, could be overcome by integrating the proposed scheme with the rule-based approaches. 2. Statistical Features Every sentence in a Chinese corpus contains a sequence of characters. If every combination of adjacent characters in a sentence must be considered as a word candidate, there would be huge number of word candidates where a large portion would be redundant. Therefore, every combination of adjacent characters, denoted as “character group” in this paper, needs to be screened first so the total number of word candidates can be reduced to a manageable size and the statistics could be computed. The occurrence count for each character group, i.e. the character n-gram, is computed and used as one of the screening criteria. Those character groups with length less than eight and with occurrence count more than or equal to five are accepted as word candidates. For each word candidate, the statistical features are computed as below.  44  Bor-Shen Lin and Yi-Cong Chen  2.1 Logarithm of Character N-Gram (LogC)  LogC(Ti ) = log (C(Ti ))  (1)  Ti : the word candidate with index i. C(Ti) : the occurrence count for the word candidate Ti. Since words tend to appear repeatedly in the corpora, those word candidates with high occurrence count are more probable to be words. Nevertheless, there are often quite a few false alarms when occurrence count is the only decision feature.  2.2 Description Length Gain (DLG)  DLG(Ti ) = L( X ) − L( X [@ → Ti ])  (2)  L( X ) = − X ∑x∈V p(x)log2 p(x)  X : all sentences in the corpus.  X[ @ → Ti] : all sentences in the corpus with Ti replaced as "@" L(．) : the entropy of the corpus.  |X|: the total number of characters in the corpus.  V: the set consisting of all characters in the corpus.  Description length gain was proposed by Kit et al. to measure the amount of information for every word candidate according to the degree of data compression (Kit et al., 1999). In Equation 2, L(X) is the entropy of the corpus containing the word candidate Ti, while L(X[@→ Ti]) is the entropy of the corpus with Ti replaced by the token "@". Therefore, DLG(Ti) indicates the entropy reduction due to the elimination of the word candidate Ti in the corpus, or equivalently the information gain of the corpus contributed by including the word candidate Ti. The more information a word candidate contributes, the higher the probability that it is a word.  2.3 Accessor Variety (AV)  AV(Ti)= min{ LAV(Ti), RAV(Ti)}  (3)  LAV(Ti) : the number of different left-context characters for the candidate Ti RAV(Ti) : the number of different right-context characters for the candidate Ti Access variety was proposed by Feng et al. to estimate the degree to which a character group occurs independently in the corpus (Feng et al., 2004). The access variety for a character group is evaluated by counting the number of different characters in its left or right context. If the access variety is high, it implies the character group is often used independently in diverse contexts and tends to be a word. On the contrary, low access variety implies that the character  Histogram Equalization on Statistical Approaches for  45  Chinese Unknown Word Extraction  group is often used together with specific characters, and thus tends to be a part of a word instead of being a word itself. Hence, the larger the access variety is, the more probable the character group is a word.  2.4 Logarithm of Total Links (Link)  The feature LogC defined in Eq. 1 considers the occurrence count of a word candidate but  does not take its internal structure into account. Since the occurrence counts of partial  character sequences for a word candidate (denoted as links here) might also provide some  evidence in support of this candidate being a word, a novel feature for estimating such links is  proposed as follows.  Link(Ti ) = log (∑k≤1C(S(Ti ; k,l)))  (4)  S(Ti;k,l): a partial character sequence of the word candidate Ti from position k through position l.  The word candidate “行政院長” (meaning executive director), for example, has the partial character sequences “行政,” “行政院,” “行政院長,” “政院,” “政院長,” and “院長,” in which the first three and the last one are also known words. The occurrence counts of these internal links can be accumulated, and the logarithm of the summation can be taken to obtain this feature.  2.5 Independence of Prefix Character(PreC) In the Chinese language, some characters are frequently used and co-occur with other words as prefixes. The preposition “在” (meaning at), for example, might co-occur with the words “台北” (Taipei), “拍攝” (take a photo) or “學校” (school), and so on. Since such prefix characters are of high frequency, their combinations with other words (e.g. “在台北”, “在拍 攝” or “在學校”) might also be of high frequency. This induces quite a few false alarms when only occurrence count is used for word extraction. To alleviate such problems, a novel feature is proposed here to measure the independence of the prefix character for a word candidate, which is defined as the average of the occurrence counts for all the character groups with the same prefix character.  C(F ) = ∑ C(x1L )  (5)  x∈S (F )  PreC(Ti )  =  ⎧ ⎪ ⎨  
Detecting intent shift is fundamental for learning users’ behaviors and applying their experiences. In this paper, we propose a search-query-log based system to predict users’ intent shifts. We begin with selecting sessions in search query logs for training, extracting features from the selected sessions, and clustering sessions of similar intent. The resulting intent clusters are used to predict intent shift in testing data. The experimental results show that the proposed model achieves an accuracy of 0.5099, which is significantly better than the baselines. Moreover, the miss rate and spurious rate of the model are 0.0954 and 0.0867, respectively. Keywords: Intent Shift Detection, Intent Analysis, Search Query Logs Analysis. 1. Introduction Understanding behavior in users’ search sessions is important because of the multiple potential applications, such as query recommendation, web page re-ranking, and advertisement arrangement. Several approaches have been proposed to define a session in a sequence of actions between a user and a search engine. For example, the time-based approach employs a time threshold to partition the queries in a fixed time period into a session. Determining a suitable threshold is the major problem of this approach. Information will be lost when a large threshold is adopted. In contrast, noise will be introduced when a small threshold is adopted. The query-based approach postulates that an information need is satisfied with a fixed number of queries. This suffers from a problem similar to the time-based approach. A large or a small threshold will introduce too little or too much information. Clarifying a boundary in a sequence of queries to form an intent-coherent session is a fundamental task in mining users’ behaviors on the World Wide Web. One of the possible approaches to accomplish this task is determining intent shifts in a sequence of queries. An intent shift occurs when the intent of the current query is different from the original search intent during information access. Given a sequence of queries, q1, q2, ..., and qn, there is an intent shift between qi and qi+1 if the intent of qi+1 is different from that intent of q1, q2, ..., qi. ∗ Department of Computer Science and Information Engineering, National Taiwan University E-mail: cjwang@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw  62  Chieh-Jen Wang and Hsin-Hsi Chen  Take Figure 1 as an example, which is selected from a real session in a query log dataset. An intent shift occurs at q4 (the 4th query in the session) because the search intent from q1 to q3 is about dogs and the search intent of q4 is about a gymnasium, which is significantly different from the original search intent.  Intent Shift Figure 1. An intent shift example. On the web, users complete their information needs through searching and browsing. They submit queries and click URLs in a session to represent their intents. The interactions between users and search engines are kept in search query logs. As similar search behaviors demonstrate similar intents, such a log dataset is a good resource to investigate common users’ behaviors. In this paper, we will mine users’ behaviors from search query logs and use them for detecting whether there exist intent shifts or not. The challenging issue is that there may be more than one search intent in a session. Multiple-intent sessions may have negative effects on the clustering performance and impact later intent shift detection. Detecting intent shifts in a session will result in sessions of better quality and have positive effects on the clustering. This is a chicken and egg problem. In this paper, we propose some strategies to sample sessions in search query logs, disambiguate the ambiguous queries and clicked URLs, extract features from different intent representations, generate intent clusters by different cluster algorithms, and explore different intent shift detection models. The remainder of this paper is organized as follows. In Section 2, we compare our research with others. Section 3 gives an overview of the proposed system and the major resource used in this work. Section 4 describes the session sampling strategies and an algorithm for query and URL disambiguation. Section 5 describes how to assemble the sessions of similar intent in a cluster. In addition, we apply intent clusters generated by different clustering models to detect intent shifts. Section 6 analyzes the performance of different clustering models and discusses the findings along with their implications. Section 7 concludes the remarks.  Intent Shift Detection Using Search Query Logs  63  2. Related Work Given a sequence of queries, an intent boundary detection algorithm divides it into several sub-sequences. Each sub-sequence of queries, containing a single information need, forms a session. Jansen, Spink, Blakely, & Koshman (2007) specify that a session is a series of users’ interactions toward a single information need. A session is a basic unit for intent clustering because clustering models put together user behaviors of the same intent into a cluster. Generally speaking, a session is usually identified by hard or soft segmentation. In hard segmentation, a session is segmented by users’ actions, such as open/close a browser or login/logout of a search engine, or by some heuristic methods, such as time cutoffs (Silverstein, Henzinger, Marais, & Moricz, 1998; Montgomery & Faloutsos, 2001) or mean session lengths (Silverstein et al., 1998; Jansen, Spink, Blakely, & Koshman, 2007). On the contrary, soft segmentation identifies an intent boundary according to topic shift in query streams (He & Harper, 2002), some category of user intent (Ozmutlu & Cavdur, 2005) or dynamic comprehension time (Wang, Lin, & Chen, 2010). Several algorithms have been proposed for detecting the intent shifts or identifying intent boundaries (Cao et al., 2008). The task of query classification is to classify the queries into some predefined categories. Queries, however, are usually short and ambiguous. To realize the meanings of queries, researchers have introduced the concept of user intent behind queries (Broder, 2002). Queries were classified by the searcher’s intent, such as navigational query, whose immediate intent is to reach a particular web site, informational query, whose intent is to acquire some information, and transactional query, whose intent is to perform some web-mediated activity. Queries are characterized along four general facets, i.e., ambiguity, authority, temporal sensitivity, and spatial sensitivity (Nguyen & Kan, 2007). Manshadi and Li (2009) classify queries into finer categories. Shen et al. (2005) employ the Open Directory Project (ODP) taxonomy to represent clicked URLs and investigate the topic transition. Hence, queries have to be disambiguated if we want to know their exact meanings. 3. System Overview An intent shift detection system is outlined in Figure 2. The MSN Search Query Log excerpt (RFP 2006 dataset) (Craswell, Jones, Dupret, & Viegas, 2009) is the main resource of this work. This data set consists of 14.9 million queries and 12.2 million clicks during a one-month period in May 2006. The MSN Search Query Log excerpt is separated into two files, one named query and the other named click. Query file is described by a set of attributes, including Time, Query, QueryID, and ResultCount, and the click file contains attributes like QueryID, Query, Time, URL, and URL Position. Note that these two files are linked through QueryID. In total, there are 7.4 million sessions, which contain the activities of users from the time of  64  Chieh-Jen Wang and Hsin-Hsi Chen  the first query submission to the time of a timeout between their web browser and the search engine.  q1q2q3...qn−2qn−1qn Figure 2. A system overview of intent shift detection. Not all sessions are suitable for constructing the intent shift detection system because of the different backgrounds of users. Besides, a session may contain noise or insufficient information. For these reasons, the search query logs are purified. We take several strategies to get a reliable data set for this study. Queries and URLs are disambiguated based on the ODP (The Open Directory Project, 2002), which is the largest, the most widely distributed human-compiled, and the most comprehensive taxonomy of the websites.  Intent Shift Detection Using Search Query Logs  65  The ODP contains more than 4.5 million websites organized into more than 600 thousand paths. A path (Perugini, 2008) is defined as an ordered hierarchical structure of hyperlink labels from the root category of a directory to a leaf in the ODP. For example, the Academia Sinica website (http://www.sinica.edu.tw/) is assigned a path (Top/Regional/Asia/Taiwan/Education). The root category is “Top” and “Regional” is a sub-category of “Top”. The ODP not only contains website annotations edited by volunteers collaboratively, but also provides a natural language description of categories and websites. After preprocessing, the purified sessions are partitioned into intent clusters by two hierarchical cluster algorithms with queries, clicked URLs, and their corresponding ODP categories. We use the sets of the intent clusters generated by various clustering models to construct the intent shift detection system. To evaluate the performance of the intent shift detection methods on various intent representations, we manually prepare a ground truth. A total of 500 sessions are sampled and annotated for testing. Given a sequence of queries in a testing session, the intent shift detection system will identify whether intent shifts occur. 4. Preprocessing For creating a reliable dataset, we first clean the MSN Search Query Log excerpt by session cleaning and category disambiguation before intent clustering. Session cleaning filters out potential noise in the MSN Search Query Log excerpt in Section 4.1. The most preferable ODP categories for a clicked URL are selected by a category disambiguation in Section 4.2. 4.1 Session Cleaning In search query logs, longer sessions containing more queries and clicked URLs tend to contain noise because users have higher probability of changing intents. On the other hand, smaller sessions may not be complete enough to describe the whole information need. In this work, we aim to capture the user intent embedded in sessions as much as we can. Balancing the noiselessness and the completeness is a basic issue for session cleaning. At the session cleaning stage, we employ the following four filtering strategies. Filter Strategy #1: Sessions longer than one hour are removed, because a long session may have multiple intents in it. According to the statistics of the MSN Search Query Log excerpt, the longest duration time of a session is more than 99 hours and several intents are observed. Intuitively, it is unlikely for a user to interact constantly with a search engine and maintain only a single intent. As mentioned in related work, several time cutoffs are proposed to segment a session. In specific, Anick (2003) adopts 60 minutes as a time cutoff to segment a session. Therefore, we postulate that the original intent of a user will shift if s/he queries a search engine for more than 60 minutes.  66  Chieh-Jen Wang and Hsin-Hsi Chen  Filter Strategy #2: Users may accomplish their goals with few queries in a small session. For example, a user may submit a navigation query (e.g., Academia Sinica), click the official website, and stop the search. A small session does not provide enough information to know his or her exact intent. In this example, the intent may be finding a job or searching for a research institute in Academia Sinica. We consider total number of queries in a session as a filtering strategy. Sessions with less than n queries are regarded as small sessions and are removed. As mentioned in related work, a variety of length cutoffs have been proposed by different research projects. Additionally, Silverstein, Henzinger, Marais, & Moricz, (1998) reported the most probable range is between 2 to 3 queries. In our experiments, n is set to 3.  Filter Strategy #3: Users may click other search engines during searching and browsing. Nevertheless, how users interact with the search engines is not recorded, so that we have no information about the subsequent actions. As a result, we remove sessions which contain the clicked URLs to other search engines.  Filter Strategy #4: Since our system utilizes ODP categories of URLs for queries and URLs disambiguation, we keep only the sessions where all clicked URLs appear in the ODP.  Table 1 lists the number of remaining sessions after each combination of strategies. After all of the filtering processes, a total of 14,242 sessions remain. The number of the remaining sessions is a low percentage (e.g. 0.19%) of the original sessions, but that is not a problem since a huge collection of query logs is available in the real world. More sessions will be generated if more logs are available. Pure sessions are important to generate correct results. The basic idea of filtering strategies is avoiding the garbage in, garbage out problem.  Table 1. Results of session filtering.  Filtering Strategies Number of Sessions Remaining Rate (%)  Original  7,468,628  100.00  
This paper presents a corpus-driven linguistic approach to embodiment in modern patent language as a contribution to the growing needs in intellectual property rights. While there is work that appears to fill a niche in English for Specific Purposes (ESP), the present study suggests that a statistical retrieval approach is necessary for compiling a patent technical word list to expand learner vocabulary size. Since a significant percentage of technical vocabulary appears within the range of independent claim among claim lexis, this study examines the essential features to show how it was characterized with respect to the linguistic specificity of patent style. It is further demonstrated how the proposed approach to the term independent claim contained in the patent specification is reliable for patent application on an international level. For example, clausal types that specify how clauses are used in U.S. patent documents under co-occurrence relations are potential for patent writing, while verb-noun collocations allow learners to grip hidden semantic prosodic associations. In short, the research content and statistical investigations of our approach highlight the pedagogical value of Patent English for ESP teachers, applied linguists, and the development of interdisciplinary research. Keywords: Intellectual Property Rights, Patent Document Processing, Corpus, Systemic Functional Linguistics, Co-Ocurrence.  ∗ Graduate School of Decision Science and Technology, Tokyo Institute of Technology, Japan E-mail: darryanlin@gmail.com The author for correspondence is Darren Hsin-Hung Lin. +Department of Foreign Languages and Literature, National Cheng Kung University, Taiwan E-mail: shelley@mail.ncku.edu.tw  78  Darren Hsin-Hung Lin and Shelley Ching-Yu Hsieh  1. Introduction In the knowledge economy age, intellectual property rights (IPR) are important assets. Especially to the knowledge industry, IPR is the key measure of a company competing with others. As globalization has resulted in rapid greater economic growth, the challenges of interdisciplinary communication concerned with intellectual property and other significant sector encounters have increased. The recognition of this importance has brought intellectual property into the limelight. Resulting from such recognition, as well as the recent emphasis on using English as the lingua franca to apply patents on an international level, the application of technical vocabulary for the writing of professional patents has become an essential issue in applied linguistic research. 2. Literature Review Law is a system of rules, carried out by lawyers, attorneys, jury, paralegals, and related legal institutes. It is not just a tool that shapes politics, economy, and society, but also it is a socially prominent medium applied to maintain social order. A large number of recently specialized areas, such as international trade, economics, finance, accounting, and electronic commerce, recently have been recruiting interdisciplinary specialists with expertise in both law and English proficiency to engage in legal workplace practice. While the widespread use of law has naturally had impact on almost all fields of discipline, the increasing use of English has radically changed the way in which we perceive this language’s international function (Modiano, 2001). English for Legal Purposes (ELP), a growing trend in the field of English for Specific Purposes (ESP), therefore, has become a research topic (Dudley-Evans & St. John, 1998:7) and is used in either professional or legislative settings. As the Internet shortens the distance between countries, patent information is readily available via online access. To protect novel inventions, intellectual property law is a developing domain across legal professions. The area of intellectual property law includes patent law, copyright law, trademark law, and trade secret law, together with some aspects of other branches of the law, such as licensing and unfair competition (American Bar Association, 2010). Intellectual property lawyers are required to have command of interdisciplinary knowledge as new developments in law generate the need for lawyers with specific backgrounds-patent law, technology law, business law, and economy economic law. It is worth mentioning that the demand for intellectual property lawyers has remained unusually high even though the global markets were affected by economic recession in the end of 2007 (World Intellectual Property Organization, 2009). As long as novel inventions continue to be  Characteristics of Independent Claim:  79  A Corpus-Linguistic Approach to Contemporary English Patents  created, there is a need for intellectual property law to be enforced to protect human rights and their invisible property for specific purposes. Patent, known as interdisciplinary innovation, has drawn the attention of most lawyers. Tsai (2008) reported that patents are granted for innovations as they reflect economic growth of a country by illustrating creative activities and displaying the knowledge power of that particular country or region. The diversity of languages used in patent applications has boosted translation demand for patent right protection. Besides, many paralegals, such as patent attorneys, lawyer assistants, or translators, participate in legal circles for a living nowadays. It is important to equip them not only with background knowledge, but competency of professional writing for the job market. Accordingly, in the present study, the researchers look at the role of patent writing for research purposes. 2.1 Corpus-based Studies on Law Corpus linguistics is often concerned with the study of natural language, which explores real and authentic language use by means of a corpus (McEnery & Wilson, 2001). At the present day, a corpus represents a wide variety of language use, both spoken and written language, by a collection of texts stored in a computer (Mudraya, 2006). Biber, Conrad, & Reppen (1998) claimed a corpus-based analysis is characterized by four primary features. First, a corpus-based study is empirical, for it uncovers the natural patterns of real language use. A corpus-based study, however, relies heavily on computer-assisted tools. Computer-assisted tools, such as concordancers, enable researchers and practitioners to tag linguistic features, to code grammatical variants, and to carry out data capture and mark-up. Third, research data are analyzed either quantitatively or qualitatively in a corpus-based study. For example, the total frequency of the term independent claim is shown in a quantitative way. The concordancer can show the frequency of coded articles and average words per article. Analysis probing into observing linguistic phenomena of the term, such as polysemy or near-synonym, in turn, is qualitative. Finally, a corpus-based analysis is meaningful once research questions have been proposed. A corpus may be designed to characterize the use of an independent claim adopting a functional approach. Since the investigation is prompted to answer the research questions concerned with such design, the corpus-based analysis becomes meaningful. As corpus-based study is widely accepted and has become the norm in interdisciplinary social sciences (Ball, 1996; Chen, 2001; Lee & Swales, 2006), it further represents how language has been evaluated in prescriptive and descriptive ways in academic research (Dudley-Evans & St. John, 1998; Hyland & Tse, 2005; Nelson, 2006; Hyland, 2008). In sum, a corpus-based study is insightful in that it is not only representative in social science research, but also it contributes to characterizing the legal language people associate  80  Darren Hsin-Hung Lin and Shelley Ching-Yu Hsieh  with (Hsieh, 1998). Over the years, there has been corpus-based research on law in social science research (Feak, Reinhart, & Sinsheimer, 2000; Candlin, Bhatia, & Jensen, 2002; Badger, 2003; Chiu, 2008). Nevertheless, few works concerned with patents can be found. As corpus-based studies have been conducted widely in social science research, the application of corpus tools has been noticed in recent development. The present study is warranted by such trends for investigation into contemporary patents. 2.2 ESP Studies on Law ESP is now well established as an important and distinct part of English teaching (Cheng, Sin, & Li, 2008:16). As English has acquired the status of lingua franca in almost any field of research, the teaching of ESP generally has been seen as a separate activity within English language teaching and ESP research has been seen as an identifiable component of applied linguistic research (Dudley-Evans & St. John, 1998). The origins of ESP can be traced back to the 1960s, when there was a growing need for technological and business industries (Swales, 2000:59-61). ESP, the prime realization of applied discourse analysis, later evolved for every specialized area needing appropriate teaching materials. Recently, ESP has been utilized as an umbrella term with a multitude of acronyms denoting the various sub-fields (Dudley-Evans & St. John, 1998). Under the ESP framework, there are two major sub-fields, English for Academic Purposes (EAP) and English for Occupational Purposes (EOP), which are distinguished by their research nature and pedagogical tradition (Robinson, 1991; Dudley-Evans & St. John, 1998). EAP is concerned with students’ needs to learn academic language, which constitutes the majority of ESP, whereas EOP comprises professional purposes in administration, medicine, law and business, and vocational purposes for non-professionals in work or pre-work situations (Dudley-Evans & St. John, 1998:7). In the ESP domain, ELP is an important but comparatively uncultivated corner (Dudley-Evans & St. John, 1998:51). González and Vyushkina (2009) characterize English for Academic Legal Purposes (EALP) as being used in university degree programs, while English for Occupational Legal Purposes (EOLP) is used in training for practical skills in the workplace. Over the years, there has been continuing interest in the research of EALP (Bhatia, 1993; Bowles, 1995; Harris, 1997; Feak, Reinhart & Sinsheimer, 2000; Candlin, Bhatia, & Jensen, 2002; Badger, 2003; Du, 2009). Nevertheless, studies have been concerned mostly with material development, genre analysis, and curriculum design. Corpus-based studies on EOLP, in contrast, are relatively undeveloped. Badger (2003) once conducted a corpus-based study on law in the genre of newspaper law reports. He found that newspaper law reports serve the same function as law cases do, which facilitates law school students in identifying the reasoning of the legal decision of the case. His corpus-based study is innovative, but it is  Characteristics of Independent Claim:  81  A Corpus-Linguistic Approach to Contemporary English Patents  EALP and is solely for reading. To be specific, corpus-based applications on EOLP are comparatively unseen and the voice that professional writing gathers in the workplace entails the directions for future research. Accordingly, it is confirmed that while EALP is widely developed for law school students and academic purposes, there is an underlying need to build up EAOP, in particular, Patent English, for workplace needs. 2.3 Vocabulary Studies on Law Writing for specific purposes requires familiarity with not only the content but also the language. Unfamiliarity with vocabulary in writing is perceived to be a challenging task for language learners. As the importance of teaching vocabulary in ESP has gained recognition (Swales, 1983), Coxhead & Nation (2001) have categorized vocabulary in ESP into four groups: high-frequency words, academic vocabulary, technical vocabulary, and low-frequency vocabulary. Nation (2001) defines those words in the use of writing. High-frequency words refer to the most frequently used 2000 words of English used in all types of writing. Low-frequency words are the rarely used terms and cover only 5% of all words. Academic words, namely semi-technical or sub-technical vocabulary, are for academic purposes. This kind of vocabulary is common to a wide range of academic fields but is not what is known as high-frequency vocabulary and is not technical in that it is not typically associated with just one field (Chung & Nation, 2003:104). In contrast, technical words are the ones used in a specialized field that are considerably different from subject to subject. As Chung & Nation (2003:104) point out, technical vocabulary is largely of interest to and used by people working in a specialized field. In the genre of law, Mellinkoff (1963) suggests legal vocabulary highlighting those common words with uncommon meanings. For example, merger and acquisition bear the same literal meaning as ‘combination’ in general English. Nevertheless, in economic and financial law, merger depicts the acquisition of one company by another. This combination into a single legal entity will increase the benefits to each other and is semantically positive. As to acquisition, the combination often bears unequal treatment and is considered negative. Since there is very little research on technical vocabulary in legal disciplines, Harris (1997) analyzed procedural vocabulary extracted from the area of English contract law. His research shows that technical words enhance legal reading and also strengthen text analysis skills. Denton (2009:5) covered frequently used legal vocabulary in his teaching. Specific meaning of vocabulary, such as merger and acquisition in economic law, is viewed as concept for him to teach. His research concludes that the learning of terminology for Legal English is the priority for participants to foster when they are learning vocabulary conceptually. In other  82  Darren Hsin-Hung Lin and Shelley Ching-Yu Hsieh  words, learning legal vocabulary with concepts of the target context is essential in vocabulary development. Haberstroh (2009) developed the legal academic word list. His research enriches the well-established area of EALP at the present day; however, the rapidly growing trend of EOLP remains comparatively undeveloped. In brief, a general conclusion can be drawn in that there is a need to prepare inter-disciplinary patent writing, but exploring technical vocabulary with corpus-driven approaches into such development has the higher priority. 3. Methodology for Corpus Creation The present study adopted a corpus-based research approach to study patent technical words from the USPTO (United States Patent and Trademark Office) glossary1 in the field of intellectual property, with an emphasis on their frequency and word associations in contemporary patents. In assessing the proper coverage needed for a lexical study, the distribution of each IPR domain is taken into consideration beforehand. Figure 1 shows the results of the coverage of technical words of IPR from the USPTO glossary.  Number of Technical Words  250 200 150 100 50 0 Patent Trademark Infotech General  Coverage  Figure 1. The coverage of technical words in intellectual property The coverage was confined to the domains. As the USPTO glossary surveyed, four primary domains were outlined-patent, trademark, infotech (information technology), and general domain. Among the total 558 words of the glossary, 212 words are word items  
Anaphora is a rhetorical device commonly used in written texts. It denotes the use of terms referring to previously-mentioned entities, concepts, or events. In this paper, the definite anaphora in Chinese texts is addressed and empirical approaches to tackle abstract anaphors are presented. The resolution is built on the association between target anaphors and the corresponding referents in their multiple-type features extracted from different levels of discourse units. Experimental results show that features extracted from clauses are more useful than those extracted from sentences in referent identification. Besides, the presented salience-based model outperforms the SVM-based model no matter whether the best set of extracted features is employed or not. Keywords: Anaphora Resolution, Chinese Text, Definite Anaphora, Feature Extraction 1. Introduction 1.1 Motivation Anaphora is an instance of an expression referring to the preceding utterances. Effective anaphora resolution enhances understanding of a text and facilitates many applications of natural language processing. The resolution involves anaphor recognition and referent recognition. In Chinese texts, anaphors can be missing or be present as pronouns, demonstratives and definite descriptions. Common pronouns are like “他” (“he, him”), “她” (“she, her”), “它” (“it”), “我們” (“we, us”), “他們” (“they, them”); demonstratives are “這” (“this”), ”那” (“that”) and definite description are like the pattern “這+[quantifier ]+noun phrase.” Without concerning zero anaphora, about 54% of the explicit anaphors are pronouns, 40% are definite descriptions, and 6% are demonstratives in a corpus containing 20 news articles. Essentially, the challenges involved with Chinese anaphora resolution are attributed to the complexities of Chinese sentence structures. It is known that although a Chinese sentence ∗ College of Computer Science, National Chiao Tung University, Hsinchu, Taiwan, R.O.C. E-mail: tliang@cs.nctu.edu.tw; sunrise0406.iit97g@g2.nctu.edu.tw  16  Tyne Liang and Jyun-Hua Cheng  features the subject-verb-object order, the sentence may be formed by a series of verbs or by pronoun or subject dropping, thus making sentence parsing difficult. Moreover, there is no blank space between adjacent words in Chinese sentences, making word or noun phrase identification difficult. Unlike most previous research projects focusing on non-abstract anaphora resolution, this paper addresses the definite anaphora in Chinese written texts and presents empirical parser-free approaches to resolve abstract anaphors, like ”這項方案” (“this plan”). The resolution is based on the linking between anaphors and their referents in multiple aspects of contextual, semantic and surface features. Among them, semantic features are extracted with the help of three outer resources, namely, Tongyici Cilin1 (TYCC for short), CKIP Lexicon2, and Google search results3. Additionally, the features extracted from different discourse units are investigated and the best feature set is verified at referent identification. In the experiments, both SVM-based and salience models are implemented for model comparison. Experimental results show that the features extracted from clauses are more useful than those extracted from sentences for anaphora resolution. Besides, the presented salience-based model outperforms the SVM-based model regardless of whether the best set of extracted features is employed or not. 1.2 Abstract Definite Anaphora In Chinese texts, a definite anaphor contains a demonstrative (tagged as “Nep” by CKIP Chinese word segmentation system4 (CKIP tagger for short)) followed with an optional quantifier (tagged as “Nf”) and a noun phrase. Lexicons with Nep-tag are like ”這, 此, 其, 那, 什麼, 其中, 個中, 甚, 啥, 哪, 斯, 甚麼”. Such anaphora is similar to the definite description anaphora in English texts in which the anaphors are composed of the definite article “the” followed by a noun phrase. In fact, there is no definite article in Chinese, so we may treat the definite noun “the+noun phrase” and demonstrative noun “this or that+noun phrase” to be the same in Chinese texts. In this paper, we focus on the “這+[quantifier]+ abstract-type noun phrase” anaphor since it is frequently expressed in Chinese texts. The abstract noun phrases are defined and categorized according to CKIP Lexicon. Table 1 shows some target anaphor instances we identified from our corpus. Abstract definite anaphora can be expressed in two ways. One is direct anaphora in which both the referent and the anaphor contain the same head noun. For example, both anaphor ”這項方案” (this plan”) and its referent ”學生停車方案” (“student parking plan”) 
We report applications of language technology to analyzing historical documents in the Database for the Study of Modern Chinese Thoughts and Literature (DSMCTL). We studied two historical issues with the reported techniques: the conceptualization of “huaren” ( 華 人 , Chinese people) and the attempt to institute constitutional monarchy in the late Qing dynasty. We also discuss research challenges for supporting sophisticated issues using our experience with DSMCTL, the Database of Government Officials of the Republic of China, and the Dream of the Red Chamber. Advanced techniques and tools for lexical, syntactic, semantic, and pragmatic processing of language information, along with more thorough data collection, are needed to strengthen the collaboration between historians and computer scientists. Keywords: Temporal Analysis, Keyword Trends, Collocation, Chinese Historical Documents, Digital Humanities, Natural Language Processing, Chinese Text Analysis. 1. Introduction Natural language processing (NLP) is a well-known research area in computer science and has been successfully applied to handle and analyze modern textual material in the past decades. ∗ Department of Computer Science, National Chengchi University, Taiwan E-mail: chaolin@nccu.edu.tw + National Chengchi University, Taiwan E-mail: gtqf1908@gmail.com # Institute of Chinese Studies, Chinese University of Hong Kong, Hong Kong E-mail: gtqf1908@gmail.com ‡ Department of Chinese Literature, National Chengchi University, Taiwan § Department of History, National Chengchi University, Taiwan  28  Chao-Lin Liu et al.  Whether we can extend the applications of current NLP techniques to historical Chinese text and in the humanistic context (e.g., Xiang & Unsworth, 2006; Hsiang, 2011a; Hsiang, 2011b; Yu, 2012) is a challenge. Word senses and grammar have changed over time, and people have assigned different meanings to the same symbols, phrases, and word patterns. We explored the applications of NLP techniques to support the study of historical issues, based on the textual material from three data sources. These include the Database for the Study of Modern Chinese Thoughts and Literature (DSMCTL),1 the Database of Government Officials of the Republic of China (DGOROC),2 and the Dream of the Red Chamber (DRC).3 DSMCTL is a very large database that contains more than 120 million Chinese characters about Chinese history between 1830 and 1930. DGOROC includes government announcements starting from 1912 to the present. DRC is a famous Chinese novel that was composed in the Qing dynasty. These data sources offer great chances for researchers to study Chinese history and literature, and, due to the huge amount of content, computing technology is expected to provide meaningful help. In this paper, we report how we employed NLP techniques to support historical studies. Chinese text did not contain punctuation until modern days, so we had to face not only the well-known Chinese segmentation problem but also the problem of missing sentence boundaries. In recent attempts, we applied the PAT Tree method (Chien, 1999) to extract frequent Chinese strings from the corpora, and we discovered that the distribution over the frequencies of these strings conforms to Zipf’s law (Zipf, 1949). We investigated the issue of how the Qing government attempted to convert from an imperial monarchy to a constitutional monarchy between 1905 and 1911, using the emperor’s memorials (奏摺, zou4 zhe2) 4 about the preparation of constitutional monarchy.5 To this end, we selected the keywords from the frequent strings with human inspection, and we applied 1中國近現代思想及文學史專業數據庫 (zhong1 guo2 jin4 xian4 dai4 si1 xiang3 ji2 wen2 xue2 shi3 zhuan1 ye4 shu4 ju4 ku4): http://dsmctl.nccu.edu.tw/d_about_e.html, a joint research project between the National Chengchi University (Taiwan) and the Chinese University of Hong Kong (Hong Kong), led by Guantao Jin and Qingfeng Liu 2 中 華 民 國 政 府 官 職 資 料 庫 (zhong1 hua2 min2 guo2 zheng4 fu3 guan1 zhi2 zi1 liao4 ku4): http://gpost.ssic.nccu.edu.tw/. The development of this database was led by Jyi-Shane Liu of the National Chengchi University. 3 紅 樓 夢 (hong1 lou2 meng4): http://en.wikipedia.org/wiki/Dream_of_the_Red_Chamber, a very famous Chinese novel that was composed in the eighteenth century 4 Most Chinese words are followed by their Hanyu pinyin and tone the first time they appear in this paper. 5 清 末 籌 備 立 憲 檔 案 史 料 (qing1 mo4 chou2 bei4 li4 xian4 dang3 an4 shi3 liao4) : http://baike.baidu.com/view/3299810.htm  Some Chances and Challenges in  29  Applying Language Technologies to Historical Studies in Chinese  techniques of information retrieval to support the study. We also studied the attitude of the Qing government towards the Chinese workers who worked in other countries between 1875 and 1911. We analyzed the co-occurrences, i.e., collocations, of the keywords over the years of interest, using the documents recorded in the diplomatic documents of the late Qing dynasty.6 Detailed observations and discussions of this historical research are reported in two other papers (Jin et al., 2011; Jin et al., 2012) that will be presented in the Third Conference of Digital Archives and Digital Humanities. While we have applied NLP techniques to support historical studies, we have also experienced some challenging problems at the lexical, syntactic, semantic, and pragmatic levels. For instance, what are the most appropriate computational functions that support a certain research need? Are the current databases good enough? We elaborate on these challenges based on our experience with the three data sources, i.e., DSMCTL, DGOROC, and DRC. No one may expect that NLP techniques will replace the major role of historians in historical studies, but the techniques should be able to work with historians to make their studies more efficient and more effective. Empirical experience reported in this paper and the literature have demonstrated the potential of NLP techniques. With the help of computing technology, historians can delegate some search work and basic analysis to computers and spend more time on higher-level philosophical issues than before. 2. Zipf’s Law Applicability The Database for the Study of Modern Chinese Thoughts and Literature contains six genres of text material that were published between 1830 and 1930. Except for the first category, most of them were collected from the late Qing dynasty: modern periodicals, personal publications of the literati, diplomatic documents, newspapers, official documents, and translated works by western commissioners. Currently, the database contains more than 120 million simplified Chinese characters.7  6 清季外交史料 (qing1 ji4 wai4 jiao1 shi3 liao4): http://zh.wikisource.org/zh-hant/清季外交史料選輯 7 DSMCTL was first built in a project led by Guantao Jin and Qingfeng Liu while they were with the Chinese University of Hong Kong. Due to budget constraints, the historical documents were sent to China, where the simplified Chinese was used, to be scanned and entered into computers. Hence, the earliest version of DSMCTL was in simplified Chinese. A traditional Chinese version of DSMCTL is still under development.  30  Chao-Lin Liu et al.  Table 1. Statistics for five collections  Collection  Number of Different Total Number Number of Different Number of  Pseudowords of Characters  Characters  Documents  Constitution  3288  713131  4097  399  Diplomacy  29315  2875032  5225  5758  Min_Bow  7784  1450623  6230  325  Nations  2649  679410  4916  160  New_People  33378  5259590  6647  1524  For modern Chinese information processing with NLP techniques, researchers rely on good machine readable lexicons and good methods to segment Chinese strings into Chinese words. Both of these infrastructural facilities are missing for the processing of non-modern Chinese text. Hence, we bootstrapped our work by computing frequent Chinese strings with the PAT Tree technique in the documents, and we asked historians to select relevant words from the frequent strings. Table 1 shows the statistics about five collections in the DSMCTL database: Constitution (清末立憲檔案), Diplomacy (清季外交史料), Min_Bow (民報),8 Nations (海國圖志),9 and New_People (新民叢報).10 They contain about 11 million characters, about one tenth of the whole DSMCTL database. We refer to strings that occurred more than 10 times11 in a collection as pseudowords. Many of these pseudowords have specific meanings, but not all of them do. We ranked the pseudowords based on their frequencies, i.e., the most and the second most frequent pseudowords were ranked first and second, respectively. Then, we computed the logarithmic values of the ranks and frequencies, resulting in the curves in Figure 1. The curves in Figure 1 indicate that the pseudowords in the Chinese historical documents, like documents written in modern English and Chinese languages (Ha et al., 2003; Xiao, 2008), conform to Zipf’s law quite well (Zipf, 1949).  8 民報 (min2 bao4): http://zh.wikipedia.org/wiki/民報 9 海國圖志 (hai3 guo2 tu2 zhi4): http://zh.wikipedia.org/wiki/海國圖志 10 新民叢報 (sin1 min2 cong2 bao4): http://zh.wikipedia.org/wiki/新民叢報 11 The selection of 10 as the threshold was by the historians. The choice was heuristic but arbitrary.  Some Chances and Challenges in  31  Applying Language Technologies to Historical Studies in Chinese  log(frequency)  4.5 Constitution  4  Diplomacy  3.5  Min_Bow  Nations  3  New_People  2.5  2  1.5  
Users might use general terms to query the information in need, when the exact keyword is unknown. We treat these inexact query terms as general queries. In this paper, we consturct a test data set to evaluate the performance of online search engine on searching Wikipedia with general queries and exact queries. We also proposed a new query expansion method that performs better on general queries. The Wikipedia query expansion method is regarding the Wikipedia as a thesaurus to find candidates of query expansion. The expanded queries are then combined with the pseudo relevance feedback. The performance of this method is better than online search engine on the general queries. Keywords: Information Retrieval, General query, Test Collection, Wikipedia, Query Expansion. 1. 研究動機 在本篇論文中，我們將評估資訊檢索系統面對一個普遍發生的情況:「使用者不知道如何 使用精確的關鍵字描述其資訊需求」。此時使用者只能使用籠統的關鍵字描述其資訊需 求，並透過多次重新檢索之後，才找到精確的關鍵字，最後取得相關資訊。我們稱這些 不精確的查詢關鍵字為籠統查詢(general query)。本文將評估線上搜尋引擎與我們所研發 的檢索系統對籠統查詢的檢索效能。 現今的資訊檢索測試集(Test Collection)不適合評估使用籠統查詢與精確關鍵字的檢 索效能，因為目前的各種大型測試集提供的是主題式的查詢資訊。如 TREC(Text REtrieval Conference)、CLEF(Cross Language Evaluation Forum)、NTCIR(NII Test Collection for IR Systems)等。各測試集典型的查詢主題(Topic)，都是使用主題當關鍵字描述其查詢的內 容，而這些查詢主題是不區分為精確或籠統的。 在本研究主要可以分為兩個主題，第一部分是建置籠統查詢之資訊檢索測試集，主 要是依據國際資訊檢索測試集機構(TREC、NTCIR)，以標準流程建構在 Web 文件上使 用籠統查詢的資訊檢索測試集。第二部份是對查詢擴展作進一步的探討，其中我們提出  以籠統查詢評估查詢擴展方法與線上搜尋引擎之資訊檢索效能  49  新的方法作查詢擴展，亦為虛擬關聯回饋與同義辭典的結合。 在建置測試集的標準流程，如圖 1 所示。標準的測試集是由三個文件集合所構成， 即查詢主題(Query set)、文件集(Document set)、相關判斷集(Relevance Judgment set)，其 中查詢主題與文件集是事先蒐集建構的，而相關判斷集則由利用各個不同的檢索系統的 檢索結果中，透過 Pooling Method 建構出相關判斷的 Pool，最後再透過參與判斷之相關 判斷者，對 Pool 中的每篇文件判斷所建構而成的。我們利用其維基百科所釋出的資料 (Wikipedia Dump Data)，作為我們的測試集的內容。而查詢主題的建構，是由真實世界 使用者對於維基百科全書知識需求所建構而成，其中包含了籠統查詢以及精確查詢等資 訊。 Test Collection  Queries  Information Retrieval Systems  Documents  Pooling Method  Judges  Pool  Search Results  Relevance Judgment  Scoring Program  圖 1. 評估資訊檢索效能之流程圖 本研究的另一主題，探討查詢擴展檢索系統的效能。查詢擴展方法能夠有效提升資 訊檢索系統的召回率(Recall)，而我們認為在籠統查詢上，由於使用者缺乏相關的資訊詳 細描述其精確的資訊需求，我們可以透過查詢擴展的方法，幫助使用者找尋到更多更相 關的文件。在查詢擴展的研究，我們將維基百科視為同義辭典，並結合虛擬關聯回饋的 機制，自動的查詢擴展，以提升檢索的效能。維基百科擁有超鏈結的特性，每個條目(頁 面)中都包含了許多超鏈結的鏈結文字(anchor text)，而這些字詞都是與條目擁有高度相關 的字詞，我們將這些相關字詞視為查詢擴展的候選詞，並與虛擬關聯回饋中的字詞共同 競選，以挑出更多相關的詞彙作查詢擴展。  50  許志全、吳世弘  2. 文獻探討  2.1 資訊檢索測試集 (Information Retrieval Test Collection) 資訊檢索評估是透過在一致性的評比環境中進行測試，衡量不同的資訊檢索技術或各檢 索系統間的效能評比。此方法最早是由 Cleverdo 在 Cranfield II(Cleverdon, 1967)時提出， 主要以文件集(Document set)、查詢主題集(Query set)、相關判斷集(Relevance Judgment set) 建構測試集，作為評估各系統的基礎資料，並訂定一套效能評估準則，評估各資訊檢索 技術及檢索系統間的效能。在 1992 年美國 Defense Advanced Research Projects Agency (DARPA)與 National Institute of Standards and Technology(NIST)共同舉辦 Text REtrieval Conference (TREC) ( Harman, 1993)，TREC 提供了當時最龐大的測試集，使得資訊檢索 測試的環境更接近於真實。  繼 TREC 之後，各界對於提供資訊檢索一致性的評比環境，許多機構亦開始提供不 同語系，相似於 TREC 的大型測試集，例如：Cross Language Evaluation Forum (CLEF) (Braschler, 2001)、NACSIS Test Collection for IR Systems (NTCIR) (Manning , Raghavan & Schütze, 2008)，這些機構與 TREC 每屆都會舉行各種不同的資訊檢索任務(Harman, 2005) (Ferro & Peters, 2008) (Kando, 2007)，如：單語言資訊檢索(Single Language IR, SLIR)、跨 語言資訊檢索(Cross Language IR, CLIR)、跨多語言資訊檢索(Multi-Lingual IR, MLIR)。 在文件集中 TREC、CLEF、NTCIR 都會將收集的新聞文件加上各種不同的標記(Tag)， 詳細的區分文件的特性，以利於系統進行剖析各種不同資訊，並有效的將其應用，表 1 為 NTCIR 文件標記之說明(陳光華, 2001) (陳光華, 2004) (Chen & Chiang, 2000)，圖 2 為 NTCIR 所使用的中文文件範例，其資訊是在陳光華教授所收集的 CIRB040r 文件集，本 研究是透過參與 NTCIR-6 CLIR 與 NTCIR-7 IR4QA 之任務所取得。 表1. NTCIR使用文件標記說明  Tag  <DOC>  </DOC>  The tag for each document  <DOCNO>  </DOCNO> Document identifier  <LANG>  </LANG>  Language code: CH, EN, JA, K  <HEADLINE> </HEADLINE> Title of this news article  <DATE>  </DATE>  Issue date  <TEXT>  </TEXT>  Text of news article  近幾年 TREC、CLEF、NTCIR 等在不同的任務也使用了不同的文件集，如：TREC 的 Web Track(Craswell & Hawking, 2004)、Terabyte Track (Büttcher , Clarke & Soboroff 2006)、Blog Track (Ounis & Soboroff, 2008)，CLEF 的 WebCLEF (Jijkoun & Rijke, 2007)， NTCIR 的 WEB Task 等 (Eguchi , Oyama, Aizawa & Ishikawa, 2004)，其文件集的型態不 同於以往的新聞文件，而是網際網路上的網路文本，即每一篇文件皆為網頁。  以籠統查詢評估查詢擴展方法與線上搜尋引擎之資訊檢索效能  51  <DOC> <DOCNO>edn_xxx_20000101_0265056</DOCNO> <LANG>CH</LANG> <HEADLINE>總統府前升旗 喜迎千禧曙光</HEADLINE> <DATE>2000-01-01</DATE> <TEXT> <P>記者蕭君暉∕台北報導</P> <P>全台陷入迎接千禧的狂熱！台東太麻里的海邊，數以萬計的人潮共同迎接台灣第一 道千禧曙光；…。</P> </TEXT> </DOC>  圖 2. NTCIR 中文文件範例  在查詢問題集中使用者描述其資訊需求稱之為查詢問題(Query)，查詢問題中包含使 用者所需的查詢關鍵字。TREC 提出使用查詢主題集(Topic Set)取代查詢問題集(Query Set)，其中的不同，在於查詢主題集以多欄位的方式陳述各種不同層次的查詢需求，而 後 NTCIR、CLEF 等亦使用查詢主題集作為各種查詢需求的陳述。表 2 為 NTCIR 所使用 Topic 的標記及其意義(陳光華, 2001)(陳光華, 2004)( Chen & Chiang, 2000)。 TREC 在早期是使用模擬的方式建構查詢主題集，而 NTCIR 的查詢主題集則是來自 於真實使用者的需求(CIRB010)，之後透過人工以及全文檢索工具的輔助，濾除敘述不清、 不夠詳盡，或者主題涵蓋範圍太廣泛、主題不符合等(陳光華, 2001)。 表2. NTCIR 查詢主題標記及說明  <TOPOIC> </TOPOIC> The tag for each topic  <NUM> </NUM> Topic identifier  <SLANG> </SLANG> Source language code: CH, EN, JA, KR  <TLANG> </TLANG> Target language code: CH, EN, JA, KR  <TITLE>  </TITLE>  The concise representation of information request, which is composed of noun or noun phrase.  <DESC>  </DESC>  A short description of the topic. The brief description of information need, which is composed of one or two sentences.  <NARR>  </NARR>  A much longer description of topic. The <NARR> has to be detailed, like the further interpretation to the request and proper nouns, the list of relevant or irrelevant items, the specific requirements or limitations of relevant documents, and so on.  <CONC> </CONC> The keywords relevant to whole topic.  52  許志全、吳世弘  相關判斷即為由判斷者(人)判斷查詢主題與文件集中的每篇文件之相關程度。現今 測試集的規模都相當龐大，無法閱讀所有文件，因此發展出 Pooling Method (Kageura & others, 1997)，此方法是假設真正相關的文件，會被多數的資訊檢索系統所檢索出，將所 有資訊檢索系統檢索的結果，建構一個相關文件候選的 Pool，評估者只需要判斷相關候 選 Pool 的文件，以此可降低建構相關判斷集的時間與人力。 NTCIR 在進行相關判斷時(陳光華, 2001)，每位判斷者必須詳細閱讀並瞭解查詢主題， 並以查詢主題中<NARR>欄位作為主要的判斷依據，將文件分到判斷者認為最適當的相 關類別。NTCIR 的相關判斷集分為四個層級，如表 3.所示，然而 TREC 的相關判斷集是 採取二元分層的方式(Harman, Braschler, Hess, Kluck, Peters & Schäuble, 2001)，TREC 的 作法被視為資訊檢索評估的標準流程，所以 NTCIR 亦採取二元分層的方式產生兩組相關 判斷集，即嚴謹相關(Rigid Relevance)以及寬鬆相關(Relaxed Relevant)，嚴謹相關視”S” 與”A”為相關，寬鬆相關視”S”、”A”、”B”為相關。 表3. NTCIR相關判斷層級  Label of Relevance  Sign  Score  Highly Relevant  S  3  Relevant  A  2  Partially Relevant  B  
A morphological family in Chinese is the set of compound words embedding a common morpheme. Self-organizing maps (SOM) of Chinese morphological families are built. Computation of the unified-distance matrices for the SOMs allows us to perform a semantic clustering of the members of the morphological families. Such a semantic clustering shed light on the interplay between morphology and semantics in Chinese. Then, we studied how the word lists used in a lexical decision task (LDT) [1] are mapped onto the clusters of the SOMs. We showed that such a mapping is helpful to predict whether in a LDT repetitive processing of members of a morphological family would elicit a satiation - habituation - of both morphological and semantic units of the shared morpheme. In their LDT experiment, [1] found evidence for morphological satiation but not for semantic satiation. Conclusions drawn from our computational experimentations and calculations are concordant with [1] behavioral experimental results. We finally showed that our work could be helpful to linguists to prepare adequate word lists for the behavioral study of Chinese morphological families. Keywords: Self-Organizing Maps, Computational Morphology and Semantics 1. Introduction In this paper, we call a morphological family the set of compound words embedding a common morpheme. So, the compound words in Tab. 1 which have all the morpheme „明‟ as a first character belong to the morphological family of „明‟.  Table 1. A subset of words belonging to the morphological family of 明 [1].  明朝 Ming Dynasty  明天 tomorrow  明白  明確  to understand explicit clear  明星 明亮 star bright  In Chinese, the meaning of a morpheme can be either transparent or opaque to the meaning of the compound word embedding it. For example, the common morpheme in Tab.1 “明” can mean (clear) or (bright) and is transparent to the meaning of “明星” (star) but rather opaque  Updated: June 9, 2010 240  to the meaning of “明天” (tomorrow). If some members of a morphological family are semantically similar, one could advance as a reason for such a similarity that these members are transparent to a same meaning of the shared morpheme. Most of Chinese morphemes are polysemous [2]. Hence, in theory, transparent members of a morphological family could belong to different semantic clusters whose centers would be the different meanings of the shared polysemous morpheme. This paper aims primarily at using computational linguistics methods to perform a semantic clustering of the members of the morphological families. Such a clustering is thereafter used to predict the results of a behavioral Lexical Decision Task1 (LDT) designed by [1] to study the phenomenon of morphological satiation in Chinese. In visual word recognition, morphological satiation is an impairment of morphological processing induced by a repetitive exposure to a same morpheme embedded in different Chinese compound words [1][3]. [1] posited that morphological satiation is due to habituation of the morphological unit of the repeated morpheme. This is represented on Fig. 1 by diagram (a). As a morpheme is thought to be a meaningful unit, it is logical to consider whether a semantic satiation [4][5][6] - an impairment of semantic processing causing a temporary loss of the meaning of the common morpheme - would occur concomitantly with morphological satiation2. In other words, the satiation observed by [1] could have two loci: a morphological one and a semantic one as represented on Fig. 1 by diagram (d). A morphological satiation could also have its loci of satiation on the links between the morphological, lexical and semantic units as represented on Fig.1 by the diagrams (b) and (c). We can quickly rule out the possibility of a locus on the link between morphological and lexical units as represented by the diagram (b). The reason is that in a LDT, this link is changing at each presentation of a new two-character word. The morphological unit of the repeated morpheme constitutes one fixed endpoint of the morphological/lexical link but the over endpoint is always changing. The present work of semantic clustering focuses on clarifying by computational means whether morphological satiation would probably have a sole morphological locus - diagram (a) - or whether it would have both a morphological and semantic locus - diagram (d) -. [1] behavioral LDT experiment results pointed to the existence of a sole morphological locus.  
This study investigates whether prior knowledge affects the processing of vague discourse in Mandarin Chinese. Vague discourse refers to the texts using vague references and neutral descriptors (e.g. 東西 dōngxī "thing", 事情 shìqíng "item", and 物件 wùjiàn "object"), rather than naming the referred to items at the basic level. Three conditions of discourse were tested: one was vague texts preceded by congruent titles, another was texts preceded by incongruent titles and the third was texts preceded without titles. An on-line self-paced reading task was conducted. Participants were instructed to read the vague texts and rate the level of comprehensibility. The rating scores for the level of comprehensibility and the reading time of the whole texts were measured. The experimental results show that people read texts preceded by congruent titles significantly faster than those preceded by incongruent and no titles. However, the reading time of texts preceded by incongruent titles was also significantly shorter than those preceded without titles. We conclude that when people simply read vague idea at a discourse level, the appropriate information is useful for text integration. Inappropriate information, however, can be paid little attention during the text processing and do not increase too much processing load. Keywords: vague texts, congruency, self-paced reading task, background knowledge 1. Introduction There are two basic approaches to text understanding: the top-down approach, in which the reader starts with a preexistent structure like a schema and tries to fit the text proposition into it, and the bottom-up approach, in which the reader starts with the text propositions and tries to create a new structure for them. According to the top-down approach, when no schema is explicitly given and the reader needs to determine referents and inter-relate propositions, s/he uses whatever information s/he can guess from the text. The reader may try to guess a schema at the same level of detail as that of "washing clothes"; alternatively, he may use some abstract default schema to relate propositions. We will refer to these two instances of the top-down approach as the "guessing" and "default schema" strategies, respectively [1]. 
 This research was funded by the National Science Council (NSC) of the Republic of China under Contract No. NSC 98-2631-S415-005. Wu Jiun-Shiung is the primary investigator of the project. Jenny Yi-Chun Kuo and Chung, Shu-Chun are the co-primary investigators. We thank to Mandarin teachers Miao-Chin Chiu and Chen-Hsuan Huang in the Division of Chinese as a Second Language, Language Center, National Chiayi University, and Xiurong Gong in Chinese Language Center of Feng-Chia University for their suggestions and comments on our teaching materials. We also thank the anonymous reviewers of the ROCLING conference for their precious comments. All remaining errors are ours. a All correspondences should be directed to: Wu Jiun-Shiung, Institute of Linguistics of National Chung Cheng University, 168 University Rd., Min-Hsiung, Chia-Yi 62102, Taiwan, R.O.C. 265  The purpose of this study is to develop a multimedia program and examine its effects on learning Chinese aspect markers le, zai, and zhe. The materials in the program were based on linguistic studies of le, zai, and zhe (Li & Thompson, 2005; Lin, 2002; Liu, 1997; Pan, 1996; Smith, 1997; Wu & Kuo, 2003; Wu, 2003, 2005, 2007; Xiao & McEnery, 2004; Yeh, 1993). We predicted that this multimedia program with animation presenting the target sentences can significantly improve Chinese as a Foreign Language (for short, CFL) learners’ acquisition of these aspect markers. The participants were totally 35 CFL beginners. Nineteen of them in the experimental group received the interactive multimedia program and sixteen of them in the control group took the computer-based grammar program. The teaching experiment is a section of twenty minutes per day for 3 days. We conduct a pretest, immediate posttest, and one-month delayed posttest, and the performances between the two groups were compared using the independent T-test. Findings indicated that the experimental group showed a significant advantage over the control group both in the immediate posttest and the delayed posttest. 摘要 本研究基於語言學對於華語時貌標記之研究，製作多媒體動畫課程，透過動畫來呈現時 貌標記的語態，幫助華語為外語學習者對「了」、「在」、「著」的習得。三十五位華語初 級學習者參與實驗，其中十九位為實驗組，用多媒體動畫課程自學;十六位為控制組， 用電腦輔助文法翻譯課程學習時貌標記，經過連續三天，每天二十分鐘的學習，以T-test 來檢驗，結果顯示實驗組在後測以及延宕後測的表現，比控制組來得有顯著的進步。 Keywords: Chinese Aspect Markers, Interactive Multimedia Program, Chinese as a Foreign Language, Animation 關鍵詞：時貌標記，多媒體動畫課程，華語為外語學習 1. Introduction The Chinese aspect markers1 have considered difficult for Chinese as a Foreign Language learners (Chao, 2002; Kao, 2006). Kao (2006) analyze the errors of the usage of the perfective le and of the imperfective zhe based on the corpus consisting of inter-language of Chinese produced by Chinese as a Foreign Language (for short, CFL) students abroad. Based on his study, he suggests that the interaction between aspect markers and different types of events, the comparison of the similar aspect markers and their individual characteristics should be introduced and emphasized in CFL instruction. In this study, we develop a curriculum for three Chinese aspect markers: the perfective le, the progressive zai and the durative zhe. In order to eliminate the negative effect of grammar translation and possibly insufficiency of pedagogical grammar, we use the generalizations 
The present study aims to investigate genre influence on the use and misuse of conjunctive adverbials (hereafter CAs) by compiling a learner corpus annotated with discoursal information on CAs. To do so, an online interface is constructed to collect and annotate data, and an annotating system for identifying the use and misuse of CAs is developed. The results show that genre difference has no impact on the use and misuse of CAs, but that there does exist a norm distribution of textual relations performed by CAs, indicating a preference preset in human cognition. Statistic analysis also shows that the proposed misuse patterns do significantly differ from one another in terms of appropriateness and necessity, ratifying the need to differentiate these misuse patterns. The results in the present study have three possible applications. First, the annotate data can serve as training data for developing technology that automatically diagnoses learner writing on the discoursal level. Second, the founding that textual relations performed by CAs form a distribution norm can be used as a principle to evaluate discoursal organization in learner writing. Lastly, the misuse framework not only identifies the location of misuse of CAs but also indicates direction for correction. Keywords: conjunctive adverbial, textual relation, misuse pattern, learner corpus. 1. Introduction Due to much interest in learning English around the globe, many tools are developed, or wanted to be developed, to facilitate learners to learn English better. One of many wanted tools is probably a tool that can automatically diagnose a piece of learner writing and provide direction for improvement of the writing. The need results from the fact that only 310  by constantly revising process can learners keep polishing their writing skill but that there is just not enough manpower to help learners recognize the defects in their writing. Therefore, much software is developed to satisfy the need, such as the two famous online writing platforms, My Access! and Criterion, and the two popular writing software packages, StyleWriter and White Smoke. However, after evaluating the above mentioned tools aiming to automatically diagnose learner writing, it is found that the diagnosis is mainly a grammar check at the sentence level yet fails to generate revising suggestions on the discourse level. In other words, the existing tools may help learners compose a piece of writing free from grammatical mistakes, but poor organization of sentences and anomaly in coherence may still lead to failure in comprehension. Therefore, a writing-facilitating tool that can automatically diagnose learner writing on the discourse level is further wanted. To do so, a further investigation of existing learner corpora is made to seek if they fit as training data for developing such tools in question. The result shows that all the three corpora under investigation, Taiwanese Learner Corpus of English (TLCE) [1], Chinese Learner English Corpus (CLEC) [2], and International Corpus of Learner English (ICLE) [3], are only annotated with linguistic information at the sentence level, which limits further development on the discourse level. In light of the investigation, the first goal of the present study is to construct a learner corpus that provides annotated discoursal information as a basis for developing technology that can automatically diagnose learner writing in terms of discoursal organization. With the goal in mind, the correct use and misuse of conjunctive adverbials are selected as the discoursal information that is used to annotate the targeted learner corpus. In terms of correct use, many writing textbooks introduce conjunctive adverbials (hereafter CA) as explicit linguistic features that organize textual relation among sentences in a coherent order, and contend that CAs performing certain textual relation would be more prominent in certain genre [4] [5] [6] [7] [8]. For instance, the words or phrases, such as firstly, next, and in addition, are thought to appear more in the process genre, indicating progressive relations in the text. Yet, after reviewing literature [9] [10] [11] [12] [13] [14], it is found that the textual relations performed by CAs present a norm distribution no matter which genre the writing belongs to, which is contrary to what writing textbooks usually suggest. In terms of misuse of CAs, [15] regulates three common misuse patterns, non-equivalent exchange, connective overuse, and surface logicality, that often occur in learner writing. However, after trying applying the misuse framework of CAs to classify the mistakes found in learner writing, the framework is found insufficient in doing so. Based on the review of literature on CAs, the second goal of the present study aims to empirically examine if writing genres play a role in the use of CAs, and to propose a framework that can better describe the misuse patterns of CAs found in learner writing. In short, the present study is two-fold. One is to compile a learner corpus annotated 311  with discoursal information, to be specific, information on CAs, which can serve as training data of developing technology that automatically diagnoses learner writing on the discoursal level, while the other is to investigate genre influence on use and misuse of CAs and to construct a misuse framework for CAs. 
(1) a. Mary wants another cigarette. b. Bill wants a beer. 323  c. Mary wants a job.  To capture each use of want, we can explicitly refer to the manner of the wanting relation in different contexts and have the following correspondent word sense enumeration, rendering the word want a selectional polysemy [3].  (2) a. want1: to want to smoke; b. want2: to want to drink; c. want3: to want to have  However, enumeration is unable to exhaustively list all the senses that verbs assume in new contexts [1]; that is, it cannot characterize all the possible meanings of the lexical item in the lexicon. Instead of adopting this approach, Pustejovsky [3] proposes that the way how verbs are combined with arguments can fit into more finely grounded compositional operations, extended from the type theory and qualia structures he developed in Generative Lexicon [2][3], as shown in Table 1.  Table 1: Verb-Argument Composition  Argument is Natural Artifactual Complex  Natural Selection/Accommodation Accommodation Dot Introduction  Verb selects Artifactual Qualia Introduction Selection/Accommodation Dot Introduction  Complex Dot Introduction Dot Introduction Selection/ Accommodation  In view of this, we aim to capture a full description of the composition between Mandarin Chinese verb “da3” (hit) indicating hand motions and its arguments collected from authentic corpus data, including Academia Sinica Balanced Corpus of Modern Chinese1 and Plurk data as a complement in this paper. Since there are over one-hundred different senses of the word “da3” listed in Chinese Wordnet2, we will re-categorize the senses of “da3” by referring to Huang’s definition [5] in the following sections in order to process the later analysis of the compositional operations, and provides the characteristics of such operations under the frame of Generative Lexicon (GL). We further predict that, due to the complexity of the senses of “da3”, the verb-argument composition will yield different results from those suggested by Pustejovsky [3] in Table 1.  
This study is to examine if typological universals built upon primary languages are applicable to interlanguage data in SLA. Implicational universal is considered the classic example of a typological universal by Croft (2003). Thus, the Interlanguage Structural Conformity Hypothesis, which consists of two implicational universals proposed by Eckman (1991), were tested against data from an interlanguage. The interlanguage data reconfirms that syllable structure plays a key role in the Fricative-Stop Prinicple. However, the Fricative-Stop Principle is sensitive to the position which clusters occur in a syllable. This typological universal is only applicable to final consonant clusters only. The test results do not conform with the Resolvability Principle. The Resolvability Principle claims that if a language has a consonantal sequence of length m in either initial or final position, it also has at least one continuous subsequence of length m-1 in this same position. Taiwanese3 speakers‟ interlanguage data show that they can produce a consonantal sequence of 3 [spr-], but fail to produce a consonantal sequence of 2 [bl-], which violates the proposed typological universal. Thus, intrinsic universals are proposed to explain the interlanguage data in this study, i.e. the position that a consonant cluster occurs in a 
In this paper, we develop a series of algorithms to improve the noise robustness of speech features based on discrete cosine transform (DCT). The DCT-based modulation spectra of clean speech feature streams in the training set are employed to generate two sequences representing the reference magnitudes and magnitude weights, respectively. The two sequences are then used to update the magnitude spectrum of each feature stream in the training and testing sets. The resulting new feature streams have shown robustness against the noise distortion. The experiments conducted on the Aurora-2 digit string database reveal that the proposed DCT-based approaches can provide relative error reduction rates of over 25% as compared with the baseline system using MVN-processed MFCC features. Experimental results also show that these new algorithms are well additive to many noise robustness methods to produce even higher recognition accuracy rates. I. Introduction Most of the state-of-the-art automatic speech recognition (ASR) system developed in the laboratory, in which the speech is not obviously distorted, can achieve excellent recognition performance. But in the real-world application, the recognition accuracy is seriously degraded due to so many distortions or variations existing in the application environment. Particularly speaking, the environmental distortions can be roughly classiﬁed into two types: channel distortion and additive noise, both inﬂuencing the performance of an ASR system a lot. The channel distortion occurs when the speech signal is transmitted by electronic devices or transmission lines, such as the air, the telephone line or the microphone. The additive noise is like the “shadow” or “background” existing in the environment, such as car noise and babble noise. Noise robustness techniques have thus received much attention in recent years since they are so important in the applicability of ASR. One school of noise-robustness techniques is devoted to compensate the original speech fea- 
This paper aims at finding the relationships between intelligibility and comprehensibility in speech synthesizers, and tries to design an appropriate comprehension task for evaluating the speech synthesizers’ comprehensibility. It is predicted that speech synthesizer with higher intelligibility, will have greater performance in comprehension. Also, since the two most popular used speech synthesis methods are HMM-based and unit selection, this study tries to compare whether the HTS-2008 (HMM-based) or Multisyn (unit selection) speech synthesizer has better performance in application. Natural speech is applied in the experiment as a controlled group to the speech synthesizers. The results in the intelligibility test shows that natural speech is better than HTS-2008, and HTS-2008 is much better than Multisyn system. Whereas, in the comprehension task, all the three speech systems present not much differences in speech comprehending process. This is because that the two speech synthesizers have reached the threshold of enough intelligibility to provide high speech comprehension quality. Therefore, although with equal comprehensible speech quality between HTS-2008 and Multisyn systems, HTS-2008 speech synthesizer is more recommended and preferable due to its higher intelligibility. Keywords: speech synthesizers, intelligibility evaluation, comprehension evaluation, HTS-2008, Multisyn 1. Introduction Recently, text-to-speech (TTS) system synthesizers have been evaluated from different aspects, such as intelligibility, naturalness, and preference of the synthetic speech, as noted by [1]. Since the final purpose of applying the synthetic speech is to make it usable to applications, it is worth carrying out experiments measuring the synthesizers’ performance with human listeners. For measuring speech synthesizers, it was necessary to involve perception factors in synthetic speech evaluation, rather than merely evaluating the intelligibility, in order to better assess the speech synthesizers, as indicated by [2]. [3] also evaluated the aspect of the listener’s perception on a comprehension task to learn how well the synthetic speech was created by the synthesizers could be understood by the listeners. Moreover, [2] had demonstrated that there was a strong relationship between the intelligibility and comprehension. Also, they had specified the intelligibility was one of the important factors that would affect listening comprehension. It is then worth observing the relationships between intelligibility and comprehension for speech synthesizers. Although several studies have been successfully evaluating the intelligibility of speech synthesizers, very few researchers have examined the association with comprehension. However, it is hard 
+黃仲淇 *顏合淨 *黃士庭 *張俊盛 ++楊秉哲 *國立清華大學資訊工程學系 +國立清華大學資訊系統與應用研究所  ++谷圳  ++資訊工業策進會  {raconquer, u901571, fi26.tw, koromiko1104, jason.jschang}@gmail.com  ++{maciaclark, cujing}@iii.org.tw  摘要 近年來，機器翻譯技術蓬勃發展並越顯重要。然而，現存的機器翻譯系統對於﹙系統未 收錄﹚未知詞多採直接輸出到目標翻譯的方式。此忽略的舉動可能造成未知詞附近的選 字錯誤，或是其附近的翻譯字詞順序錯置，因而降低翻譯品質或降低閱讀者對翻譯文章 的理解。經過我們的初步分析，大約有 25%的系統未知詞可用重述﹙paraphrase﹚的方式 來作翻譯，另外的 25%可利用組合單字翻譯來翻譯。另外，現有的片語式﹙phrase-based﹚ 機器翻譯系統對於落單字﹙singleton﹚的翻譯效果也未加重視。所謂的落單字是指系統 在翻譯此字時必須單獨翻譯：此字沒法與前面或是後面的字組合成連續字詞片語或是文 法翻譯結構。本研究將建構於片語式機器翻譯處理技術，開發未知詞翻譯模組和落單字 翻譯模組。實驗結果顯示即使在不假額外的雙語資料，我們的未知詞翻譯模組仍勝出片 語式翻譯系統，尤其是在包含有未知詞的句子上。 關鍵詞：未知詞，重述，片語式機器翻譯系統，落單字，機器翻譯  一、緒論 近年來，機器翻譯技術蓬勃發展並越顯重要。然而，現今先進的片語式機器翻譯系統對 於﹙系統未收錄﹚未知詞與落單字﹙singleton﹚的處理仍有改進的空間。翻譯系統對於 來源語﹙source language﹚未知詞採直接輸出到目標﹙target language﹚翻譯的方式，也就 是說，系統並不處理未知詞。此忽略的舉動可能造成未知詞附近的選字錯誤，或是其附 近的翻譯字詞順序錯置，因而降低翻譯品質或降低閱讀者對翻譯文章的理解。片語式機 器翻譯系統之所以可以有令人滿意的翻譯效果在於其翻譯的過程常常是多個連續的來 源語字詞一起翻譯到目標語。多個字詞一起翻譯的過程幫助了這些字詞翻譯的解歧，也 就是所謂的字義解歧﹙Word Sense Disambiguation﹚亦或是字詞翻譯解歧﹙Word Translation Disambiguation﹚。以中文字「起」為例。「起」有相當多的字義如「起床」、「上升」、「動 身」、「發揮」等。不同字義的﹙英文﹚翻譯也都不盡相同。而片語式翻譯系統則會將「起」 跟其周遭連續的字「的」、「很」和「早」一起看作是一個片語並翻譯成“get up very early”。換言之，解歧成「起床」字義。很少文獻針對片語式機器翻譯系統中的落單字 翻譯效果進行分析。所謂的落單字是指系統在翻譯此字時必須單獨翻譯：此字無法與前 面或是後面的字組合成連續字詞片語或是文法翻譯結構。落單字必然是片語式翻譯系統 的自然天敵。目前系統多靠語言模型﹙Language Model﹚來選擇落單字的翻譯。但是語 79  言模型受限於字數限制，也不考慮像是字詞詞性等語言現象，大多數選擇最高頻的翻 譯。落單字的翻譯解歧效果直接影響了翻譯之品質。 首先，我們分析了 NIST MT-08 的測試句。美國 NIST﹙National Institute of Standards and Technology﹚幾乎每年都會舉辦 MT 的比賽來促進自動翻譯研究的發展。經過我們的初 步分析，大約有 25%的系統未知詞可用重述﹙paraphrase﹚的方式來作翻譯，另外的 25% 可利用組合單字翻譯來翻譯。重述就是將未知詞轉換成意思相近但現於現有雙語語料中 的字詞。重述的論文探討已經相當多且齊全。在這個計畫中，我們將著重在跟重述佔有 相同重要角色的組合單字翻譯上。我們利用組合單字的翻譯來翻譯未知詞。我們的處理 方法不假額外的雙語資料﹙文獻多直接藉由擴大雙語語料來減少未知詞﹚，只利用現存 的訓練資料來尋找可能的單字翻譯，也就是，系統已知字詞﹙in-vocabulary﹚翻譯。更 精確的來說，我們組合排列現有的雙語訓練資料中未知詞的構成字之翻譯，並加以排序 以得到較為可能的未知詞翻譯。例如：藉由雙語資料中「上」的翻譯 upper、above、rise 等，以及「肢」的翻譯 body、limbs 等可組合出 NIST MT-06 未知詞「上肢」的翻譯 upper limbs。類似的方法可以組合出形容詞-名詞複合字未知詞「韓戰」﹙Korean war﹚，名詞名詞複合字未知詞「邊貿」﹙border trade﹚，動詞-形容詞複合字未知詞「成名」﹙become famous﹚之翻譯。其中，「邊貿」也是目前最尖端的翻譯系統 Google Translate 之未知詞。 另外，在針對片語式機器翻譯中落單字的翻譯時，我們發現，隨機抽樣 NIST MT-08 的五十中文句中，落單字佔全文比例高於 6%，落單字又以名詞、動詞居多，各佔 72%、 21%。人工分析系統對於不同詞性字詞的翻譯品質差異很大，名詞可達五成正確率 ﹙precision﹚，但是動詞只到兩成。分析 NTCIR 2011 年專利翻譯比賽的發展中資料，也 顯示了類似比例—落單字佔全文比例約 5%。由上面幾組數據，我們知道落單字跟未知 詞一樣，都是片語式機器翻譯系統急須面對處理的課題。我們預計利用「動詞-名詞」 或是「動詞-副詞」搭配詞﹙collocation﹚來幫助落單字的解歧，以增加片語式機器翻譯 系統之翻譯品質。畢竟，落單字要解歧就需要看稍微遠一點的字詞﹙context﹚，而搭配 詞往往又是幫助解歧的有用資訊﹙一個搭配詞一個字義 one sense per collocation﹚。以「起」 和「打擊」這兩個多義字來作說明。它們的翻譯可能為 get up、rise、increasing、play、 have 等，和 fight、combat、batting、bat 等。但是當「起」的附近有搭配詞「早」時 get up 較有可能，當附近有名詞搭配詞「作用」時 play、have 較有可能﹙此時的「起」有「發 揮」的意思﹚。類似地，當「打擊」附近有搭配詞「犯罪」出現時 fight、combat 較有可 能，而當其附近出現「區」，「棒球」時，則是 batting、bat 較為可能。由上面的例子， 我們預期：不一定緊密相鄰的「動詞-名詞」或是「動詞-副詞」搭配詞，或稱為有彈性 的搭配詞﹙flexible collocation﹚，將可幫助片語式機器翻譯中落單字解歧。 本研究將建構於現有片語式機器翻譯處理技術，例如公開原始碼的 Moses 翻譯系 統，開發未知詞翻譯模組和落單字翻譯模組。未知詞翻譯模組將從現存的雙語訓練資料 中尋找未知詞構成字之翻譯，進而組合、排序未知詞的翻譯候選﹙利用雙語對應機率和 單語流暢度加以排序﹚。排序好的翻譯候選將利用 XML 標記方法輸入片語式機器翻譯系 統以作句子翻譯。落單字翻譯模組則會先利用大量的中文語料﹙如：Chinese Gigawords﹚ 抽取出數學統計上可能的搭配詞如「起…作用」、「打擊…犯罪」等。然後藉由這些搭配 詞來為落單字解歧。解歧完後的落單字翻譯也是利用 XML 標記方法將翻譯候選提供給 真正作句子翻譯的片語式機器翻譯系統。所以我們的方法除了使用雙語資料外，也會利 用中文語料與英文語料﹙如： English Gigawords﹚取得中文搭配詞和英文語言模型。 80  二、研究方法﹙The Method﹚ 本研究的範圍在於解決一般機器翻譯最常忽略的未知詞翻譯問題還有落單字的翻譯解 歧問題。目標是，在現有雙語訓練語料中，為未知詞找出翻譯並有效排序翻譯候選，另 外，正確替落單字解歧，提升機器翻譯品質亦或是幫助閱讀者閱讀。我們將在以下章節 詳述建構在現有片語式機器翻譯系統之上的未知詞翻譯模組和落單字解歧模組。  （一） 未知詞翻譯模組 未知詞翻譯模組針對未收錄於機器翻譯訓練語料的字詞產生並依照可能機率排列其翻 譯候選。此模組可分為兩個子模組—組成字模組和重述模組﹙目前我們較著重在文獻較 少提到的組成字模組﹚。  1. 組成字模組  未知詞是系統未收錄的字詞，也就是，利用完全無誤比對﹙exact-match﹚來查詢雙語語 料以得目標語翻譯必定是徒勞無功的。此模組將原本完全無誤比對﹙exact-match﹚的翻 譯查詢轉換成一連串的部分比對﹙partial-match﹚查詢以先求得未知詞構成字的翻譯。接 著從這些查詢回來的雙語配對﹙phrase pair﹚中，擷取出未知詞組成字的可能翻譯。最 後，藉由組合組成字翻譯來翻譯未知詞，並且參考雙語字層級﹙character-level﹚翻譯機 率和目標語的語言模型來排序未知詞翻譯候選。步驟大綱如下。  步驟一：  wildcard  block  *  *  , appeal for  , increasing of  , upper  , the body  , four limbs  , prosthesis  步驟二：上一個步驟得到的是來源語的字詞翻譯而不是未知詞組成字的翻譯，也就 是，不是字層級﹙character-level﹚的翻譯。所以此步驟首先擷取出組成字的翻譯可能。 我們是利用 N-gram 來擷取出組成字的可能翻譯。以翻譯配對﹙上段, upper block﹚和﹙四 肢, four limbs﹚為例。未知詞的組成字「上」和「肢」的可能翻譯分別是“upper”、 “block”、“upper block” 和 “four”、“limbs”、“four limbs”。值得注意的是， 產生 N-gram 時，我們會考慮其變化型。這些產生的 N-gram，其實詞﹙例如名詞、動詞 等﹚限定必須出現在一個大的字詞語料庫中﹙例如 WordNet﹚，如果沒被此大的語料庫 所包含將被剔除︰畢竟一個沒被字詞語料庫包含的實詞，其 N-gram 應該也不是怎樣好 的翻譯候選。最後，我們排除低頻的 N-gram。為了公平的比較，次數是變化型的累加 並共享。為了得到原形化資訊，我們實作時，利用 NLTK 中提供的原形化器﹙Bird 等 人, 2008﹚。表一呈現步驟一和步驟二的個別產物。  81  ： phrase the body extremities four limbs prosthesis  body  N-grams  extremity extremities  four limb limbs four limbs  prosthesis  ：所有組成方法和特色組成方法所產生的雙語關聯例子  source phrase  translation limb  All Constituent Salient Constituent  ( , limb)  ( , limb)  limb  ( , limb)  ( , limb)  ( , limb)  limb  ( , limb)  ( , limb)  ( , limb)  hind limb  ( , hind) ( , hind) ( , limb) ( , limb) ( , hind limb) ( , hind limb)  ( , hind) ( , limb) ( , hind limb)  步驟三：我們利用雙語對應關係來刪除較不可能的組成字翻譯。步驟二所產生的 N-gram 有時候跟組成字的關聯是相當相當少的。為了減少計算量和增加翻譯的準確度， 我們將去除比較不可能的組成字翻譯 N-gram。以針對部分比對查詢「*肢」所找出來的 翻譯配對﹙四肢, four limbs﹚為例。因為“four”和“limbs”皆是常見且高頻的實詞，步 驟二將會保留兩者，並視為組成字「肢」的可能翻譯。我們很明顯的知道雖然“limbs” 是其合理的翻譯，但是中文應該是「四」才對的“four”顯然不是。也因此需要此步驟 來檢驗存留下來的組成字翻譯和組成字的關係強弱。  首先，我們利用雙語字典如 bilingual WordNet 來建立雙語對應關係。建立關係的方  82  式可分為兩種方法—所有組成和特色組成方法。我們詳述如下。  所有組成﹙all-constituent﹚方法：針對每一個字典中的翻譯配對<source phrase, translation>，我們為 source phrase 中的所有組成字和 translation 中的所有 N-gram 建立 起對應關係。也就是說，一旦一個 source phrase 中的組成字和 translation 中的 N-gram 有共同出現過，他們之間就會有一個連結。以字典中的<“後肢”,“hind limb”> 為例。「後」和「肢」這兩個構成字將會和“hind limb”的 N-gram 有所連結。我們 會為此配對建立 6 個雙語關聯﹙請參見表二﹚。  特色組成﹙salient-constituent﹚方法：相較於上述方法，此方法只會為 source phrase 中的特色組成字和 translation 的 N-gram 建立關聯。一個 source phrase 中的組成字是 特色組成字如果此組成字和 translation 是最有相關的。嚴謹的來說，針對字典中配 對<source phrase, translation>，特色組成字 c*是利用下面的公式而得  arg max Dice(c,translation) = arg max 2 ⋅ Count(c,transaltion)  c  c Count(c) + Count(translation)  其中 c 代表 source phrase 中的組成字，而 Count (∙)代表字典內的頻率。以<“後 肢”,“hind limb”>為例。我們比較 Dice(“後”, “hind limb”) 和 Dice(“肢”, “hind limb”) 來 決 定 特 色 組 成 字 。 因 為 Count(“ 後 ” , “hind limb”) 和 Count( “肢”, “hind limb”)為 1 且「後」、「肢」、和“hind limb”發生次數個別為 1073、201、1，因此擁有較高 Dice 值的「肢」被選為「後肢」的特色組成字，進而 註冊雙語關聯(“肢”, “hind”)、(“肢”, “limb”)、和(“肢”, “hind limb”)﹙可 參見表二﹚。我們可以知道特色組成方法所產生的雙語關聯將是所有組成方法所產 生的子集合。  一旦建立起字典的雙語關聯，我們將可以刪除沒出現在關聯內的組成字和其上一步 驟產生的 N-gram 配對。舉例來說，針對組成字「肢」所找到的翻譯配對﹙四肢, four limbs﹚ 及步驟二所允許的 N-gram﹙四肢, four﹚將在步驟三中被去除。因為組成字和其 N-gram 配對﹙肢, four﹚沒在表二中出現。在實作上，我們首先利用所有組成方法來去除翻譯候 選且保存高召回率。如果存留下來的組成字翻譯候選仍是超過門檻值﹙threshold﹚，我們 再使用特色組成方法來更積極作刪除以達到高準度。另外，這些雙語關聯也用作軟限制 ﹙soft constraint﹚而其頻率則當成是下一步驟排序的特徵﹙feature﹚。  步驟四：我們利用圖一的演算法來組合出並排列未知詞的翻譯候選。首先我們為未 知詞 O 的每一個組成字 c 從雙語翻譯對應 TE 中抽取出其翻譯 SubTrans﹙利用上述步驟 一到三﹚。SubTrans 是一個 list 其元素像﹙source word, target N-gram﹚，其中 source word 包含了 O 的組成字。然後﹙圖一步驟 1b﹚，我們使用雙向條件機率﹙bidirectional conditional probabilities﹚來測量組成字和其翻譯的雙語關聯度，並將這樣的資訊紀錄在相對應的字 層級﹙character-level﹚位置上。CandList 內的元素將像(c﹙, source word, target N-gram﹚, P(target N-gram|c) ∙P (c|target N-gram))。其中雙向條件機率 P(target N-gram|c)和 P(c|target N-gram) 則是由字層級﹙character-level﹚對應的平行語料﹙parallel corpus﹚訓練而來。以未知詞 「上肢」為例。我們首先為組成字「上」和「肢」取得 SubTrans{( “上訴”, “appeal”), (“上策”,“policy”), …, (“上段”,“upper”)}和{(“四肢”, “limb”), (“四肢”, “limbs”), …, (“義肢”, “prosthesis”)}。然後我們計算組成字和其 N-gram 的對應強 度並將這些資訊紀錄在 CandList 中﹙可參考表三﹚。  83  procedure GenerateAndEvaluateCandidates(O, TE, C, CT)  for each constituent c in the OOV O  (1a) SubTrans = RetrieveSublexicalTranslations(c, O, TE)  (1b) CandList[position (c, O)] = BilingualInfo(SubTrans, c, C)  (2a) Straight = CandList[1]  (2b) Inverted = CandList[|O|]  // where |O| denotes the length of O  for each constituent position cp >1 in ascending constituent positions of O  (3a) Straight ⊗ = CandList[cp]  for each constituent position cp <|O| in descending constituent positions of O  (3b)  Inverted ⊗ = CandList[cp]  (4a) Straight = MonolingualInfo(Straight, CT)  (4b) Inverted = MonolingualInfo(Inverted, CT)  Candidates = Straight + Inverted  (5) RankedCandidates = Sort Candidates in decreasing order of probability P  (6) Return the top N RankedCandidates with probabilities P exceeding θ  圖一：組合並排列未知詞之翻譯候選  ：針對未知詞 上肢 CandList  CandList  source target  c  P(target N-gram|c) ∙ P (c|target N-gram)  word N-gram  appeal  5×10-5 ∙ 0.17  CandList[1]  policy  1.2×10-7 ∙ 1×10-9  upper  0.02  ∙ 0.56  CandList[2]  limb limbs prosthesis  0.05 0.05 0.004  ∙ 0.01 ∙ 0.01 ∙ 0.12  一旦我們有組成字的翻譯，我們便可產生未知詞翻譯候選。雖然未知詞的翻譯範圍 遠小於翻譯一整個句子。翻譯的重組﹙re-ordering﹚仍是有可能發生。例如，「調」和「氣」 的個別翻譯是“adjustment”和“air”， 「調氣」的翻譯則是倒置成“air adjustment”。 也因此，全順接﹙straight﹚和全反接﹙inverted﹚的情況都會被考慮。在圖一步驟 3 中， Straight 和 Inverted 會接續的涵蓋未知詞的組成字：邊收集組成字翻譯邊累乘翻譯的機 率。每一個組合而成的翻譯候選 TransCand 的字詞翻譯分數是由雙向條件機率的乘績來 推估。計算方式如下：  ∏ Ptrans = o p(ci | target N − gramij ) ⋅ P(target N − gramij | ci ) ci∈o  84  其中 ci 代表未知詞的組成字而 target N-gramij 代表 ci 其中一個組成 TransCand 的翻譯。以 未知詞「上肢」之組成字翻譯(“上”, (“上段”, “upper”), 0.02 ∙ 0.56)和(“肢”, (“四肢”, “limb”), 0.05 ∙ 0.01)為例。我們會產生一個全順接的翻譯候選(“上肢”, 
This work represents several unsupervised feature selections based on frequent strings that help improve conditional random fields (CRF) model for Chinese word segmentation (CWS). These features include character-based N-gram (CNG), Accessor Variety based string (AVS), and Term Contributed Frequency (TCF) with a specific manner of boundary overlapping. For the experiment, the baseline is the 6-tag, a state-of-the-art labeling scheme of CRF-based CWS; and the data set is acquired from SIGHAN CWS bakeoff 2005. The experiment results show that all of those features improve our system’s F1 measure (F) and Recall of Out-of-Vocabulary (ROOV). In particular, the feature collections which contain AVS feature outperform other types of features in terms of F, whereas the feature collections containing TCB/TCF information has better ROOV. Keywords: Word Segmentation, Unsupervised Feature Selection, Conditional Random Fields 1. Introduction Many intelligent text processing tasks such as information retrieval, text-to-speech and 109  machine translation assume the ready availability of a tokenization into words, which is relatively straightforward in languages with word delimiters (e.g. space), while a little difficult for Asian languages such as Chinese and Japanese. 1.1 Background Chinese word segmentation (CWS) is an essential pre-work for Chinese text processing applications and it has been an active area of research in computational linguistics for two decades. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, conducted five word segmentation bakeoffs (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006; Jin and Chen, 2007; Zhao and Liu, 2010). After years of intensive researches, CWS has achieved high precision, but the issue of out-of-vocabulary word handling still remains. 1.2 The State of the Art of CWS Traditional approaches for CWS adopted dictionary and rules to segment unlabeled texts (c.f. Ma and Chen, 2003). In recent years, the mainstream is to use statistical machine learning models, especially the Conditional Random Fields (CRF) (Lafferty et al, 2001), which shows a moderate performance for sequential labeling problem and achieves competitive results with character position based methods (Zhao et al., 2010). 1.3 Unsupervised CRF Feature Selections for CWS For incorporating unsupervised feature selections into character position based CRF for CWS, Zhao and Kit (2006; 2007) tried strings based on Accessor Variety (AV), which was developed by Feng et al. (2004), and co-occurrence strings (COS). Jiang et al. (2010) applied a feature similar to COS, called Term Contributed Boundary (TCB). Tsai (2010) employ statistical association measures non-parametrically through a natural but novel feature representation scheme. Those unsupervised feature selection are based on frequent strings extracted automatically from unlabeled corpora. They are suitable for closed training evaluation that any external resource or extra information is not allowed. Without proper knowledge, the closed training evaluation of word segmentation can be difficult with Out-of-Vocabulary (OOV) words, where frequent strings collected from the test data may help. According to Zhao and Kit (2008), AV-based string (AVS) is one of the most effective unsupervised feature selection for CWS by character position based CRF. This motivates us to seek for explanations for AVS’s success. We suspect that AVS is designed to keep overlapping strings but COS/TCB is usually selected with its longest-first nature before integrated into CRF. Hence, we conduct a series of experiments to examine this hypothesis. 110  The remainder of the article is organized as follows. Section 2 briefly introduces CRF. Common unsupervised feature selections based on the concept of frequent strings are explained in Section 3. Section 4 discusses related works. Section 5 describes the design of labeling scheme, feature templates and a framework that is able to encode those overlapping features in a unified way. Details about the experiment are reported in Section 6. Finally, the conclusion is in Section 7. 2. Conditional Random Fields Conditional random fields (CRF) are undirected graphical models trained to maximize a conditional probability of random variables X and Y, and the concept is well established for sequential labeling problem (Lafferty et al., 2001). Given an input sequence (or observation sequence) X = x1 xT and label sequence Y = y1 yT , a conditional probability of linear-chain { } CRF with parameters Λ = λ1,...,λn can be defined as:  ∑ ∑ Pλ (Y  |  X  )  =  
The semantic orientation of terms is fundamental for sentiment analysis in sentence and document levels. Although some Chinese sentiment dictionaries are available, how to predict the orientation of terms automatically is still important. In this paper, we predict the semantic orientation of terms of E-HowNet. We extract many useful features from different sources to represent a Chinese term in E-HowNet, and use a supervised machine learning algorithm to predict its orientation. Our experimental result showed that the proposed approach can achieve 92.33% accuracy, which is comparable to the accuracy of human taggers. 關鍵詞：廣義知網，情緒分析，情緒字典, 語義傾向, 向量支援機 Keywords: E-NowNet, Sentiment Analysis, Sentiment dictionary, Semantic orientation, SVM 151  一、緒論 情緒分析（Sentiment Analysis）在現今的網路世界中，有許多實際且重要的運用，例如 從網路的評論文章中分析消費者對產品的評價，或分析消費者對產品性能的關注焦點等 等。不管對句子或文件層次的情緒分析，意見詞詞典都是一個重要的資源。通常意見詞 詞典是用人工來收集詞彙，並用人工標記詞彙的各種情緒屬性，包括主客觀（subjective or objective）、極性（orientation/polarity)、）及極性的強度（strength）[1]。這些情緒屬 性對不同的應用有不同的重要性，標記難度也各不相同，通常詞彙的極性是最容易進行 標記的屬性。 標記情緒屬性時，研究者可以從零開始收集詞彙以建立意見詞詞典，如台大意見詞詞典 NTUSD[2]。在另一方面，也有研究者嘗詴為自然語言處理中的許多現存的資源，添加 情緒屬性，如 SentiWordNet[3]。但現有資源的語彙量通常很大，如 WordNet 3.0 就包 括 206,941 個不同的英文字義（word-sense pair），要全部用人工進行標記之成本太高。 因此，通常的作法是少量標記一些詞彙，再用機器學習方法，為剩下的詞彙進行自動標 記，雖然自動標記的準確率不如人工標記，但對一般應用有某種程度的幫助。 在中文自然語言處理，NTUSD 是一部重要的意見詞詞典，但此詞典只包括詞彙及極性 的資訊。另一方面，董振東先生和陳克健教授所建立的知網[4]和廣義知網[5]，是重要 的語意資源。對於每個詞彙，都用有限的義原給予精確的定義，但這些定義卻缺乏情緒 的語意標記。因此，如何自動為廣義知網加上情緒標記，成為一個重要的課題，也是本 研究的目的。 本研究提出為廣義知網加上情緒標記的方法，首先利用 NTUSD 跟廣義知網詞彙的交集 建立標準答案集，再由標準答案集訓練出分類器，為其他廣義知網詞彙進行標記。如何 有效的運用監督式機器學習演算法，如何為詞彙抽取出有用的特徵，是主要的挑戰議 題。在此研究中，我們有系統的嘗詴抽取各種不同的詞彙特徵，最後得到跟人工標記準 確率不相上下的分類器。 第二節介紹廣義知網、及英文和中文相關的情緒屬性標記研究，第三節介紹從 E-HowNet 及 Google Chinese Web 5-gram 抽取特徵的方法，第四節呈現各種實驗的結果及分析， 包括跟 NTUSD 人工標記的比較，最後總結論文的成果。 二、相關研究 董振東先生於 1998 年創建知網（HowNet），並在 2003 年，跟中央研究院資訊所詞庫小 組在 2003 年，將中研院詞庫小組詞典（CKIP Chinese Lexical Knowledge Base）的詞條 跟知網連結，並作了一些修改，最後形成廣義知網（Extended-HowNet, E-HowNet）。詞 庫小組修改並擴展知網原先的語義義原角色知識本體，建構出廣義知網知識本體 （Extended-HowNet Ontology），並用這些新的語義義原，以結構化的形式來定義詞條， 詞條定義式的例子如圖一。 有關情緒屬性標記的研究，我們分為英文及中文來討論。在英文方面，最早是由 Hatzivassiloglou & McKeown[6]在 1997 年針對形容詞所做的研究，他們所用的形容詞 分別有正面詞 657 個及負面詞 679 個，該論文依據不同的實驗設定，監督式機器學習的 152  準確率（Accuracy）由 82% 到 90%。之後陸續有不同的研究，所用多為半監督式機器 學習的演算法[7-9]，效能從 67%到 88%不等，但因為這些演算法所用的資料集並不相 同，實驗過程及評估標準也不一樣，（有用 Accuracy、Precision、或 F-Measure），所以 效能沒有辦法直接比較。  圖一、「汽油」的廣義知網定義式  在中文的情緒屬性標記相關研究，Yuen et al.[10]2004 年利用 Turney & Littman[7] 的半 監督式機器學習演算法，在正面詞 604 個及負面詞 645 個的資料集上做實驗，得到最高 的成績是 80.23%的精確度及 85.03%的召回率。之後從 2006 到 2010 年，陸續的研究使 用不同的資料集，用不同類型的機器學習演算法來處理這個問題[11-14]，所得到的效能 在不同的指標（Accuracy、Precision、或 F-Measure）下，從 89%到 96%不等。因為基 準不同，這些效能一樣沒有辦法直接比較，但相較於英文，成績則明顯提高。  三、特徵抽取及機器學習演算法  由於我們運用監督式機器學習演算法來訓練分類器，最重要的問題是為詞彙抽取出有用 的特徵。在此論文中，我們分別從 E-HowNet 及 Google Chinese Web 5-gram 這兩個來 源抽取兩大類的特徵，接著將這兩個來源的特徵組合訓練分類器。此外，我們也嘗詴使 用組合式的監督式機器學習演算法（ensemble approach），來更進一步得到更高的效能， 以下我們分別詳細介紹。 （一）、基礎義原特徵  從 E-HowNet 抽取的特徵稱之為基礎義原特徵，也就是對每一個 E-HowNet 的詞彙 i， 用一向量 Vi = (wi,j) = (wi,1, wi,2, …, wi,n) 表示，其中 n 為向量的維度。 由於每一詞彙的每一個語意（sense）都有一個結構化的定義式，而且定義式中都用義 原來進行定義，公式 (1) 定義 Vi 中每個特徵的權重：  ­1, 如果定義式 i 中出現義原 j  wi, j  ® ¯ 0,  不出現義原 j  (1)  以圖一「汽油」這個詞彙為例，其定義式中出現了義原 material，所以它的值 w 汽油, material  153  就會是 1，其他沒出現的義原，值就會是 0。我們共使用了 2567 個義原來當特徵。 廣義知網的詞彙有歧異性，也就是每個詞彙可能有許多語意。而詞彙的第一個語意，是 出現頻率最高的語意（除了四個詞彙例外），所以我們用詞彙的第一個語意來抽取特徵。 只從詞彙的一個語意抽取特徵，而不把該詞彙所有的語意放在一起，代表這種方法可為 不同的語意給出不同的極性預測。只是由於目前 NTUSD 極性標記只到詞彙的層級，所 以無法對語意的層級進行極性預測。但只要有語意層級的極性標記，我們這種做法可馬 上套用。 1、基礎義原特徵加權值 除了公式 (1) 的方式外，我們可以利用更多 E-HowNet 的特性，來抽取出有用的資訊。 一個可能的方式是定義式中的結構，如果把定義式展開，會得到如圖二的樹狀結構。在 這樹狀結構中，義原所在的深度是一個有用的資訊，因此我們仿照劉群&李素建[15]的 公式，將深度的資訊當作權重引入公式 (1)，得到公式 (2)。  圖二、「天倫之樂」定義式的樹狀表示  wi, j  °®­1 °¯   0  
Yw_L7@hotmail.com 謝舒凱 Shu-Kai Hsieh 國立臺灣大學語言學研究所 Graduate Institute of Linguistics National Taiwan University shukaihsieh@ntu.edu.tw 
The classic work is that of Saffran and Newport (Saffran, Aslin, and Newport 1996) (S&N) on eight-month-olds’ acquisition of lexical items. As is well known, speech is heard as a mostly unsegmented stream, thus raising the question of how children learn to segment it into words. What S&N show is that infants use statistical regularities in the input. When the stream is mid-word, there are fewer possible continuations than between words because of the uncertainty in the next word. More technically, the perphoneme entropy is higher between words than within them. S&N show that eightmonth-olds are capable of detecting such differences. To do this S&N create an artiﬁcial “language” in which each “word” consists of three arbitrary phonemes, for example, bidukapupadotigolabubidaku. . . . So biduka and pupado are words, but the second two syllables of the ﬁrst plus the ﬁrst syllable of the second (dukapu) is not. All the words are played with no emphasis on any syllable and ∗ With apologies to Stephen Colbert. † Department of Computer Science, Brown University, Box 1910, Providence, RI 02912. E-mail: ec@cs.brown.edu. 1 I have written this paper in the ﬁrst person to reﬂect its origins as the Lifetime Achievement Award talk at ACL-2011. It is not, however, based upon a transcript, since I can write better writing than I can speak. I also have included a few things I did not have time to say in the original version. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 4  no difference in the spacing between syllables. (It is pretty boring, but the child is only subjected to two minutes of it.) After that the child is tested to see if he or she can distinguish real words from non-words. To test, either a word or a non-word is played from one of two speakers. This is not done until the child is already looking at that speaker and the word is replayed until the child looks away. The children are expected to gaze longer at the speaker that is playing a novel (non-) word than for words that they have already heard. Thus there are two testing conditions. In the ﬁrst, the non-words are completely novel in the sense that the three-syllable combination did not occur in the two minutes of pretest training. On average, the children focus on the speaker 0.88 seconds longer for the novel words. The second condition is more interesting. Here the non-words are made up of sound combinations that have in fact occurred on the tape, but relatively infrequently because they consist of pieces of two different words. Here the question is not a categorical one (Have I heard this combination or not?) but a statistical one: Is this a frequent combination or is it rare? Now the focus differential is 0.83 seconds. The conclusion is that children are indeed sensitive to the statistical differences.  1.2 What You See Where You Are Not Looking Try the following test. Keep your gaze on the plus sign in the following example and try to identify the letters to its left and its right.  A  +  BRACE  The “A” on the left is not too hard. The letters on the right are much harder, a phenomenon called “crowding.” The work I am following here (Rosenholtz 2011) looks into this and related phenomena. Obviously once we move our gaze around we have no problems with the letters. The center of the eye, the fovea, sends the brain a very detailed description. But elsewhere the brain gets many fewer bits of information. These bits have to “summarize” that piece of the image. The question that Rosenholtz (2011) looks at is what information these bits encode. Suppose we want a 1,000-bit summary of the top left image in Figure 1. The other three images offer three possibilities. The top-right image simply down-samples the pixels. This is clearly not what the eye is doing. Going back to the crowding example, we cannot make out the letters on the right, but we can be pretty sure they are letters. Furthermore, we have little difﬁculty identifying the letter on the left, so it is not just down-sampled. The bottom-left image also down-samples, but on wavelets. It is little better, so we can ignore the fact that many of us don’t know what wavelets are. The bottom-right image is the most interesting. It assumes that the brain receives a basket of statistics found useful by the statistical vision community for summarizing images in general and textures in particular. The image shown here is a sample from the posterior of the statistics for the original (leftmost) image. Here it is reasonably clear that we are looking at letters, but we cannot be sure what they are, matching introspection in the crowding example. If these are the statistics available to our brain, then looking foveally at such a reconstruction ought to be similar (equally difﬁcult) to looking non-foveally at the original.  644  Charniak  The Brain as a Statistical Inference Engine Q2  Figure 1 Simulations of the information available on non-foveated portions of an image. The work in Rosenholtz (2011) shows that this seems to be true, again supporting the idea that brains manipulate statistical information. 2. Bayes’ Law Once one accepts that the brain is manipulating probabilities, it seems inevitable that the overriding equation governing this process is Bayes’ Law: P(M)P(E | M) P(M | E) = P(E) where M is the learned model of the world and E is the relevant evidence (our perceptions). We take Bayes’ Law as our guide in the rest of this talk because there is so much to be learned from it. More speciﬁcally, let us use it in the following form: P(M | E) ∝ P(M)P(E | M) This is the case because P(E) acts as a linear scaling factor and thus can be ignored. First, Bayes’ Law divides responsibility between a prior over possible models (P(M)) and a term that depends on the evidence, its posterior given the model. The war between rationalists and empiricists seems to have quieted of late, but it raged during much of my academic career, with the rationalists (typically followers of Chomsky) insisting that the brain is pre-wired for language (and presumably much of the rest) and the empiricists believing that our model of the world is determined primarily by the evidence of our senses. Bayes’ Law calmly says that both play large roles, the exact balance to be determined in the future. The next section looks at the role of informative priors in current models of learning from sensory input. Secondly, Bayes’ Law says that evidence is incorporated via a generative model of the world (P(E | M)). This is, of course, as opposed to a discriminative model (P(M | E)). There is a lot to be said in favor of the latter. When both are possibilities, the general 645  Computational Linguistics  Volume 37, Number 4  wisdom has it that discriminative models work better on average. I certainly have that impression. But children have only the evidence of their senses to go by. Nobody reads them the Penn Treebank or any other training data early in their career. Thus generative models seem to be the only game in town. Furthermore, this generative model has to be a very large one—one of the entire world insofar as the child knows it. For example, we can describe visual experiences in language, so it must include both vision and language, not to mention touch and smell. Thus it must be a joint model of all of these. Taking this to heart, I look in Section 4 at some work that uses joint modeling of multiple phenomena. Lastly, Bayes’ Law gives another clue, this time by what it omits—any notion of how any of this is done. This suggests, to me at least, that the inference mechanism itself is not learned, and if not, it must be innate. Or to put it another way, Darwinian selection has already done the learning for us. In the ﬁnal section I will address what we can say about this mechanism. One last point before moving on. What is the model that is to be learned? Bayes’ Law does not, in fact, tell us. It only says that different models will be supported to differing degrees given the evidence available. Various answers have been suggested. One group tells us that “true” Bayesians do not adopt any model, they integrate over them. That is, if you don’t know the correct model, you should plan accordingly and not commit. This corresponds to the equation P(E) = P(M)P(E | M) M Now integrating over all models makes a lot of sense, but to me it sounds a lot like telling people to change all their passwords every month—it is sound advice, but who will take the time? One possibility is to adopt the most likely one, for example, arg max P(M)P(E | M) M At ﬁrst glance this would seem to be a no-brainer, but some worry that we could have a probability distribution something like that in Figure 2. Here the most likely one, off on the right, has the very bad property that if we are only a little bit wrong we end up with a very bad model of the world, thus negatively impacting the probability that we will survive to have progeny. Better is to take the average model. This will put us somewhere in the middle of the large block on the left, relatively safe from catastrophic results from small errors. My personal opinion is that this will turn out to be a non-problem. Given the right prior, the probability space over models will have one huge peak and not much else. Furthermore, as I discuss in the last section, our options on inference are going to  Figure 2 Projection on a line of models vs. probability for a bad case. 646  Charniak  The Brain as a Statistical Inference Engine  constrain us quite a bit, with an integration over comparatively few models coming out on top. 3. Informative Priors Bayes’ Law shows us how priors on possible world models should be combined with the evidence of our senses to guide our search for the correct one. In this section I give three examples of such priors, two in language, one in vision. 3.1 Priors in Word Segmentation In section 1 we looked at the work in Saffran, Aslin, and Newport (1996) on infants’ ability to divide a speech stream into individual “words.” This problem has also been attacked using computational approaches, albeit with simplifying assumptions. We base our discussion on the work of Goldwater and Grifﬁths (2007), as it dramatically shows the need for some informative prior. This, like most other computational linguistic work on word segmentation, considers an abstract model of the problem. A corpus of child-directed speech is translated into a simpliﬁed phoneme string by ﬁrst transcribing the words, then for each word writing down its most common pronunciation. All spaces between sounds inside an utterance are removed, but the boundaries between utterances are kept. For example, you want to see the book would come out as yuwanttusiD6bUk. The output is to be the phoneme sequence with word boundaries restored. Although we are really dealing with phoneme sequences with spaces added, we will speak of dividing the speech stream into words. A simple generative model for this task goes as follows: For each utterance: Repeat until the “end of utterance” symbol is chosen Pick the next word according to P(w). It is assumed here that P(w) is over possible vocabulary items plus a special “end-ofutterance” symbol. If we have no prior on possible P(w)’s, then Bayes’ Law reduces to ﬁnding the M = P(w) that makes the data most likely, and this is easy to specify. It is simply a distribution with n “words,” each one a single utterance. That is, it is a distribution that memorizes the training data. It is easy to see why this is so. If the model generalized at all, it would assign probability to a sequence that is not in the input, and thus the probability of observed training data must be less than that assigned by the memorization model. In Goldwater and Grifﬁths (2007) the model was forced away from this solution by adopting a “sparse prior”—in this case a “Dirichlet” distribution. This says, in effect, prefer M’s with as few different words as possible. Thus if the number of words is signiﬁcantly smaller than the number of utterances, there is some pressure to prefer a word-based distribution over one for each utterance. Unfortunately, the Dirichlet is a very weak prior, that is, the “pressure” is not very great. So in one solution to the problem the child’s utterance comes out as youwant to see thebook. There is still a distinct tendency to merge words together. Why is Dirichlet so weak? We want to evaluate the probability of our prior multiplied by a generative posterior for a particular model M. In our case M is just a  647  Computational Linguistics  Volume 37, Number 4  probability distribution over possible words. We assume for the sake of argument that  we get this distribution from estimated integer counts over possible words. (In Section 5  we look at how Goldwater and Grifﬁths [2007] actually infers this distribution.) So our  current guess is that we have seen the “word” D6bUk two times, and so forth. A simple  maximum  likelihood  distribution  would  assign  a  probability  to  this  word  of  2 L  ,  where  L  is the total number of word tokens the model currently proposes in all of the utterances.  Of course, the maximum likelihood distribution is called the maximum likelihood  distribution because for a given set of counts it assigns probabilities to make the prob-  ability of the data (the “likelihood”) as high as possible. Therefore it will lead us to the  memorization result.  The Dirichlet does something slightly different. Imagine that before we do the  division  2 L  we  subtract  
© 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 4  1. Introduction The evaluation of machine translation output is an important and at the same time difﬁcult task for the progress of the ﬁeld. Because there is no unique reference translation for a text (as for example in speech recognition), automatic measures are hard to deﬁne. Human evaluation, although of course providing (at least in principle) the most reliable judgments, is costly and time consuming. A great deal of effort has been spent on ﬁnding measures that correlate well with human judgments when determining which one of a set of translation systems is the best (be it different versions of the same system in the development phase or a set of “competing” systems, as for example in a machine translation evaluation). However, most of the work has been focused just on best–worst decisions, namely, ﬁnding a ranking between different machine translation systems. Although this is useful information and helps in the continuous improvement of machine translation (MT) systems, MT researches often would ﬁnd it helpful to have additional information about their systems. What are the strengths of their systems? Where do they make errors? Does a particular modiﬁcation improve some aspect of the system, although perhaps it does not improve the overall score in terms of one of the standard measures? Does a worseranked system outperform a best-ranked one in any aspect? Hardly any systematic work has been done in this direction and developers must resort to looking at the translation outputs in order to obtain an insight of the actual problems of their systems. A framework for human error analysis and error classiﬁcation has been proposed by Vilar et al. (2006), but as every human evaluation, this is also a difﬁcult and timeconsuming task. This article presents a framework for automatic analysis and classiﬁcation of errors in a machine translation output which is just a very ﬁrst step in this direction. The basic idea is to extend the standard error rates using linguistic knowledge. The ﬁrst step is the identiﬁcation of the actual erroneous words using the algorithms for the calculation of Word Error Rate (WER) and Position-independent word Error Rate (PER). The extracted erroneous words can then be used in combination with different types of linguistic knowledge, such as base forms, Part-of-Speech (POS) tags, Name Entity (NE) tags, compound words, sufﬁxes, preﬁxes, and so on, in order to obtain various details about the nature of actual errors, for example, error categories (e.g., morphological errors, reordering errors, missing words), contribution of different word classes (e.g., POS, NE), and so forth. The focus of this work is the deﬁnition of the following error categories: r inﬂectional errors r reordering errors r missing words r extra words r incorrect lexical choices and the comparison of the results of automatic error analysis with those obtained by human error analysis for these categories. Each error category can be further classiﬁed according to POS tags (e.g., inﬂectional errors of verbs, missing pronouns). The translation outputs used for the comparison of human and automatic error analysis 658  Popovic´ and Ney  Towards Automatic Error Analysis of Machine Translation Output  were produced in the frameworks of the GALE1 project, the TC-STAR2 project, and the fourth Workshop on Statistical Machine Translation3 (WMT09). The comparison with human error analysis is done considering two possible applications: estimating the contribution of each error category in a particular translation output, and comparing different translation outputs using these categories. In addition, we show how the new error measures can be used to get more information about the differences between translation systems trained on different source and target languages, between different training set-ups for a same phrase-based translation system, as well as between different translation systems. 1.1 Related Work A number of automatic evaluation measures for machine translation output have been investigated in recent years. The BLEU metric (Papineni et al. 2002) and the closely related NIST metric (Doddington 2002), along with WER and PER, have been widely used by many machine translation researchers. The Translation Edit Rate (TER) (Snover et al. 2006) and the CDER measure (Leusch, Uefﬁng, and Ney 2006) are based on the edit distance (WER) but allow reordering of blocks. TER uses an edit distance with additional costs for shifts of word sequences. The CDER measure drops certain constraints for the hypothesis: Only the words in the reference have to be covered exactly once, whereas those in the hypothesis can be covered zero, one, or multiple times. Preprocessing and normalization methods for improving the evaluation using the standard measures WER, PER, BLEU, and NIST are investigated by Leusch et al. (2005). The same set of measures is examined by Matusov et al. (2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g., translation of speech recognition output). The METEOR metric (Banerjee and Lavie 2005) ﬁrst counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. A method that uses the concept of maximum matching string (MMS) is presented by Turian, Shen, and Melamed (2003). IQ (Gime´nez and Amigo´ 2006) is a framework for automatic evaluation in which evaluation metrics can be combined. Nevertheless, none of these measures or extensions takes into account any details about actual translation errors, for example, what the contribution of verbs is in the overall error rate, how many full forms are wrong although their base forms are correct, or how many words are missing. A framework for human error analysis and error classiﬁcation has been proposed by Vilar et al. (2006), where a classiﬁcation scheme (Llitjo´ s, Carbonell, and Lavie 2005) is presented together with a detailed analysis of the obtained results. Automatic error analysis is still a rather unexplored area. A method for automatic identiﬁcation of patterns in translation output using POS sequences is proposed by Lopez and Resnik (2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between WER and PER for three POS classes (nouns, adjectives, and verbs) is proposed by Popovic´ et al. (2006) for the estimation of inﬂectional and reordering errors. Semi-automatic error analysis (Kirchhoff et al. 2007) is carried out in order to identify problematic  
© 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 4  applied to this lexical cognate data. For example, we tested different scenarios of Paciﬁc settlement and found compelling support for an origin of the Austronesian language family in Taiwan around 5,200 years before present (Gray, Drummond, and Greenhill 2009). This combination of computational phylogenetic methods and linguistic data promises to be a powerful way of exploring human prehistory and linguistic and cultural evolution (Greenhill, Blust, and Gray 2008; Currie et al. 2010; Gray, Bryant, and Greenhill 2010). Using the comparative method to identify phylogeny requires a lot of lexical data, detailed knowledge about phonology (in general, and in the languages in question), and a lot of time (Durie and Ross 1996), however. It has recently been suggested that the Levenshtein distance can be used to subgroup languages without the intensive time requirement of the comparative method (Brown et al. 2007) or the potential subjectivity involved in identifying sound correspondences (Serva and Petroni 2008). The Levenshtein distance (Levenshtein 1966) is a string comparison metric that counts the number of edit operations (replacements, insertions, and deletions) required to transform one string into another (Kruskal 1983). For example, the Levenshtein distance between the Mussau and Tongan cognate words tolu is 0, and the difference between tolu and Javanese telu is 1 (a replacement of the /e/ with /o/). This is usually normalized by dividing by the length of the longest word (Brown et al. 2007; Serva and Petroni 2008), so the distance between tolu/telu is 0.25. This Levenshtein classiﬁcation has recently been applied to the Indo-European (Serva and Petroni 2008; Tria et al. 2010), Austronesian (Petroni and Serva 2008), Turkic (van der Ark et al. 2007), Indo-Iranian (van der Ark et al. 2007), Mayan, Mixe-Zoque, Otomanguean, Huitotoan-Ocaina, Tacanan, Chocoan, Muskogean, and Austro-Asiatic language families (Brown et al. 2007; Holman et al. 2008; Bakker et al. 2009). The results of Levenshtein classiﬁcation have even been used to explore broader questions such as the relationship between population size and the rates of language evolution (Wichmann and Holman 2009), the dates of human population expansions (Serva and Petroni 2008; Wichmann and Holman 2009; Wichmann et al. 2010), whether languages arise and go extinct at a constant rate (Holman 2010), and to triangulate the homelands of language families (Wichmann, Mu¨ ller, and Velupillai 2010). Proponents of Levenshtein classiﬁcation have claimed that the results are very similar to that of the comparative method. However, to date there has been no rigorous attempt to quantify the performance of the Levenshtein distance at classifying languages. For example, Petroni and Serva (2008) use the Levenshtein distance to classify 50 Austronesian languages, and claim that their obtained language phylogeny is “similar” to the results of the comparative method. However, close inspection of their classiﬁcation reveals some puzzling incongruities. Their tree correctly places the Atayalic subgroup of the Formosan languages at the base of the tree (Blust 1999), but the next subgrouping on the tree is the large Oceanic subgroup before the rest of the Formosan languages are encountered. According to the comparative method, the Austronesian language family has a highly rake-like structure, and the Oceanic subgroup should be nested within Central-Eastern Malayo-Polynesian and Eastern Malayo-Polynesian (Blust 1993, 2009; Gray, Drummond, and Greenhill 2009). Instead, the Levenshtein classiﬁcation looks more like the results from a lexicostatistical analysis (Swadesh 1952; Embleton 1985), which incorrectly inferred the base of the Austronesian phylogeny to be in Near Oceania (Dyen 1965). This alternative tree topology has been largely discounted as a methodological error due to an inability of the lexicostatistical methodology to handle differences in the rates of lexical change (Bergsland and Vogt 1962; Blust 2000; Greenhill and Gray 2009). 690  Greenhill  Accuracy of Levenshtein Classiﬁcation  The incongruities between the Levenshtein classiﬁcation and the comparative method demonstrates the need for a quantitative evaluation of the accuracy of the Levenshtein distance for genealogically subgrouping languages. The large Austronesian language family is a good test case for evaluating the accuracy of comparative methods (Greenhill and Gray 2009; Greenhill, Drummond, and Gray 2010). First, the major subgroups of the Austronesian language family are well established (Dempwolff 1934, 1937, 1938; Grace 1959; Pawley 1972; Dahl 1973; Blust 1978, 1991, 1993, 1999, 2009; Ross 1988; Ross and Næss 2007). Second, most subgroups in the Austronesian family can be identiﬁed from the basic vocabulary commonly used for Levenshtein classiﬁcation (Greenhill, Drummond, and Gray 2010). Third, there is a large-scale database of Austronesian vocabulary available for such a test: the Austronesian Basic Vocabulary Database (Greenhill, Blust, and Gray 2008). In this article, I attempt to evaluate the accuracy of the Levenshtein classiﬁcation method for identifying genealogical subgroups. 2. Method The Austronesian Basic Vocabulary Database (ABVD) (Greenhill, Blust, and Gray 2008) is a large collection of basic vocabulary word lists from over 650 Austronesian languages. Each word list comprises 210 items such as words for body parts, kinship terms, colors, numbers, verbs, nouns, and so forth. These items are thought to be highly stable over time and resistant to being borrowed between languages (Swadesh 1952). From this database I extracted the word lists for 473 languages that belonged to the Austronesian language family and were not reconstructed proto-languages. To compare the performance of the Levenshtein classiﬁcation to the traditional subgroupings I subsampled triplets of languages from these data 10,000 times. The correct classiﬁcation of triplets is the simplest possible subgrouping task, with only four possible subgroupings: language A is more similar to language B than C (A,B|C), A is closer to C (A,C|B), B is closer to C (B,C|A), and no language is closer to the other (A,B,C). Restricting the comparison to triplets has the advantage of not requiring a tree construction algorithm to infer the topology, and hence avoids adding uncertainty caused by the phylogenetic reconstruction process (Susko, Inagaki, and Roger 2004). For each language triplet I obtained the expected linguistic subgrouping from the Ethnologue database (Lewis 2009). The Ethnologue is the primary catalogue of language information about the world’s languages. Each language has a classiﬁcation string associated with it that categorizes the language into a nested set of subgroups that are derived from primary historical linguistic research. It could be argued that the Ethnologue classiﬁcation lags behind linguistic research (Campbell and Grondona 2008). However, the deeper structure of the Austronesian language family has been well established for a long time (Dempwolff 1934, 1937, 1938; Grace 1959; Pawley 1972; Dahl 1973; Blust 1978, 1991, 1993, 1999, 2009; Ross 1988), and the recent release of the Ethnologue has updated the classiﬁcation to match even quite newly identiﬁed language subgroups like Temotu (Ross and Næss 2007). The normalized Levenshtein distance between each language pair in the threelanguage sample was calculated as follows. In each of the 210 words in the ABVD word lists, one entry was selected for each language. Where the ABVD had multiple entries for a word in a language one of the entries was sampled at random. The Levenshtein distance was then calculated for each pair of words (word 1 in language A vs. word 1 in language B; word 2 in language A vs. word 2 in language B; etc.), and normalized by dividing by the length of the longer word (Brown et al. 2007; Holman et al. 2008; Serva and Petroni 2008). When one of the language pairs had no entries for a given word the 691  
∗∗ Chair of Psychology, Lange Gasse 20, 90403 Nuremberg, Germany. E-mail: paul.karsten@wiso.uni-erlangen.de. Submission received: 5 January 2010; revised submission received: 15 February 2011; accepted for publication: 17 April 2011. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 4  validity (Teufel, Carletta, and Moens 1999; Calhoune et al. 2005; Gruenstein, Niekransz, and Purver 2005; Purver, Ehlers, and Niekrasz 2006). The main reason for the analysis of annotation quality is to obtain a measure of the “trustworthiness” of annotations (Artstein and Poesio 2008). Only if we can trust that annotations are provided in a consistent and reproducible manner, can we be sure that conclusions drawn from such data are likewise reliable and that the subsequent usage of annotations is not negatively inﬂuenced by inconsistencies and errors in the data. Inter-annotator (or inter-coder) agreement has become the quasi-standard procedure for testing the accuracy of manual annotations. This process is based on the assumption that if multiple coders agree in their coding decisions of the same material we can be certain that—at least for this set of data and this set of coders—annotations are free of unsystematic and distorting variations. Our degree of certainty depends on the ability of annotators to reach sufﬁcient agreement in their coding decisions. Perfect agreement among coders is very rare. Slips of attention can lead to errors in the annotations of individual annotators, as can misinterpretations of coding instructions, personal biases, or genuinely disparate interpretations of material (Krishnamurthy and Nicholls 2000; Beigman Klebanov, Beigman, and Diermeier 2008). A common rule of thumb requires that annotators should agree in at least 80% of all cases or—if using a chance-corrected measure such as kappa— reach values of at least 0.67 (Carletta 1996; DiEugenio and Glass 2004). Some recent discussions demand even higher agreement values from 0.80 up to 0.90 (Neuendorf 2002; Artstein and Poesio 2008). Experience has shown that such values are often hard to achieve, even when the coding task appears to be easy. To name only two examples: For the ib2b smoking challenge, agreement between medical experts on the smoking status of patients only reached 60% (Uzuner, Szolovits, and Kohane 2006). In a recent review of the word-sense disambiguation domain, Navigli (2009) reported upper boundaries for coder agreement of 90% for coarse-grained/binary sense inventories, but only 67–80% for ﬁne-grained inventories. Comparing annotations of the same data by independent coder groups can even yield agreement rates as low as 22% (Andreevskala and Bergler 2006; Grefenstette et al. 2006). Disagreements in human annotators are thus a common occurrence, and computational linguists expend considerable efforts to improve agreement rates among coders. The question is whether processes that lead to disagreement among coders are unsystematic, or whether there are conditions and settings that impact annotator agreement in a predictable way. Computational linguists have started to tackle this question (Oppermann, Burger, and Weilhammer 2000; Syrdal et al. 2001; Pitt et al. 2005). Possible inﬂuences on annotation decisions identiﬁed in past studies are the complexity of annotation schemes (Popescu-Belis 2008), the degree of annotators’ expertise (Lewis, Watterson, and Houghton 2003), the difﬁculty of the material (Wiebe, Wilson, and Cardie 2005; Beigman Klebanov and Beigman 2009), or the usage properties of individual words (Passonneau, Salleb-Aouissi, and Ide 2009). These ﬁndings point to a combination of annotator characteristics, such as level of expertise, and external factors, such as characteristics of the material or annotation scheme. Of course, high levels of annotator agreement is not a goal in itself. Yet, as an indicator for the trustworthiness of manual annotations we must aim to create conditions under which annotator groups can achieve their optimal results. Knowing the factors that systematically impact annotator agreement gives us a tool to proactively design situations that eliminate or reduce inﬂuences that affect comparability in a negative way—or if elimination is impossible, it at least makes us more aware of possible 700  Bayerl and Paul  What Determines Inter-Coder Agreement?  problems and potential reasons for lower agreement rates. In the same regard, knowing which annotator characteristics are conducive to consistent and reliable work enables us to select annotators that meet these criteria. Unfortunately, despite a wealth of annotation studies and practical experiences, we still lack a clear picture on what inﬂuences inter-annotator agreement. Although individual studies have found potential factors, it is unclear whether these ﬁndings are applicable only to the speciﬁc project or task in which they were observed, or whether they represent systematic inﬂuences on annotator agreement over a wide range of (or even all) situations. The main question guiding our study in this article was therefore whether we are able to identify factors that impact coder agreement in systematic ways across multiple studies and multiple domains. Trying to propose a general framework of factors inﬂuencing annotator agreement is challenging, as manual annotations are required in a vast range of tasks from POStagging to prosodic transcriptions, word-sense disambiguation, the classiﬁcation of text genres, or the identiﬁcation of gestures or emotions. Moreover, each annotation project has its speciﬁc, idiosyncratic features. If factors such as annotator expertise or scheme granularity are important inﬂuences in one speciﬁc project, how can we be sure that this is also the case in other projects or other areas? The best approach to answer this question is to review and synthesize information from a wide variety of studies in the same ﬁeld. In this article we used a metaanalytic approach to synthesize existing information on inter-annotator agreement. Meta-analysis is a statistical method that combines and compares results of independent studies to obtain an overview of the respective research ﬁeld. It is further used to identify factors with a (statistically) signiﬁcant impact on the outcomes of individual studies (Cooper and Hedges 1994b; Lipsey and Wilson 2001). Our investigation followed two aims: (1) to test whether there are factors that consistently inﬂuence annotator agreement, while abstracting from the speciﬁcs of individual annotation projects and settings; and (2) to get a ﬁrst indication of how far these inﬂuencing factors are generalizable across different annotation domains. This second aspect is of considerable practical importance. If inﬂuencing factors are comparable across domains, ﬁndings from one area can be applied to other areas. If not, each domain has to consider factors that impact coder agreement individually. This study contributes to the existing literature in that it approaches the question of inter-annotator agreement at the stage when decisions about procedures and settings can still inﬂuence the end results. It thus adds to earlier discussions by emphasizing the importance of the earliest stages in the annotation process before and when annotations are made. It does so by taking an objective, statistical approach to reanalyze and synthesize ﬁndings of existing empirical studies. In the next section, we give a short introduction to the meta-analytic method and describe the speciﬁc procedure followed in the present study. This is followed by the presentation of our ﬁndings and a discussion of the practical implications for annotation projects. Based on our results, we formulate recommendations for the planning, execution, and reporting of annotations. Further, we brieﬂy outline the broader theoretical implications and open questions for the concept of manual annotation quality. 2. Method The approach chosen in our study was the meta-analysis of existing annotation studies. Meta-analysis is a method that allows us to statistically integrate results of disparate empirical studies. It was developed in the last quarter of the 20th century as an answer 701  Computational Linguistics  Volume 37, Number 4  to the problems that characterize traditional narrative reviews (Cooper and Hedges 1994a). The traditional method has long been criticized for being “unsystematic, haphazard, and impos[ing] an impossible information-processing burden on the reviewer” (Hunter and Schmidt 1990, p. 488). As a result, authors of traditional reviews typically fall back on personal strategies with the consequence that different reviewers often come to very different conclusions, even if they analyze the same set of studies (Van Den Noortgate and Onghena 2005). To prevent such personal biases, meta-analyses use a statistical approach to synthesize research results. Meta-analyses are “quantitative summaries of research domains that describe the typical strength of the effect or phenomenon, its variability, its statistical signiﬁcance, and the nature of the moderator variables from which one can predict the relative strength of the effect or phenomenon” (Rosenthal 1995, p. 183). Since the term “meta-analysis” was coined (Glass 1976) and the ﬁrst studies were published in the ﬁelds of education and psychology (Smith and Glass 1977; Glass and Smith 1979), such quantitative syntheses have seen a strong rise in popularity in the psychological, educational, and medical sciences. Other areas are following suit, leading to what has been called a “meta-analytic revolution” (Fiske 1983). A meta-analysis consists of six basic stages: (1) formulation of the research questions or hypotheses and deﬁnition of criteria for the selection of primary studies, (2) comprehensive search for studies on the topic of interest that ﬁt the selection criteria, (3) extraction and coding of relevant data, (4) conversion of study results to an index that makes results comparable across studies (the so-called effect size such as correlations, standardized mean differences, or proportions), (5) combination and/or comparison of results across studies including the investigation of possible moderator variables (i. e., variables that change the strength of an effect or relationship between two variables by either increasing/decreasing its strength or changing its direction), and (6) interpretation and reporting of results. The ﬁfth stage constitutes the “essence of the meta-analysis” (Van Den Noortgate and Onghena 2005, p. 3), in which the results of the primary studies are analyzed statistically. Although all meta-analyses follow the same basic procedure, decisions on each of these steps need to be tailored to the question and data at hand. In the following we detail the procedure followed in our study.1 2.1 Identiﬁcation and Selection of Studies and Domains Our main research question was very broad, and we therefore started our search for primary studies with few restrictions. The search for relevant studies used three main approaches. First, electronic searches in computerized databases (MLA, CiteSeer) were conducted using variations and combinations of the keywords manual (annotation), human (coders), agreement, reliability, ITA, inter-coder, inter-annotator, and inter-tagger. Additional Internet-based searches in Google using the same keywords were conducted to ﬁnd documents published in proceedings and as internal papers, working notes, or theses. Further, on-line reference lists of projects and authors concerned with manual annotations were searched. Additional publications were identiﬁed through the backtracing of references in key articles. In a few cases authors were contacted for papers unavailable through the Internet or libraries. The use of a broad range of search methods and the inclusion of unpublished studies strengthens the representativeness of the 
Consider the sentence from a news article: George W. Bush met with Vladimir Putin in Moscow. How long did the meeting last? Our ﬁrst inclination is to say we have no idea. But in fact we do have some idea. We know the meeting lasted more than ten seconds and less than one year. As we guess narrower and narrower bounds, our chances of being correct go down, but if we are correct, the utility of the information goes up. Just how accurately can we make duration judgments like this? How much agreement can we expect among people? Will it be possible to extract this kind of information from text automatically? ∗ Microsoft Corporation, 475 Brannan St., San Francisco, CA 94107, USA. E-mail: fengpan@microsoft.com. ∗∗ 4676 Admiralty Way, Marina del Rey, CA 90292, USA. E-mail: me@rutumulkar.com. † 4676 Admiralty Way, Marina del Rey, CA 90292, USA. E-mail: hobbs@isi.edu. Submission received: 2 November 2006; revised submission received: 26 January 2011; accepted for publication: 7 March 2011. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 4  Sometimes we are explicitly told the duration of events, as in “a ﬁve-day meeting” and “I have lived here for three years.” But more often, such phrases are missing, and present-day natural language applications simply have to proceed without them. There has been a great deal of work on formalizing temporal information (Allen 1984; Moens and Steedman 1988; Zhou and Fikes 2002; Han and Lavie 2004; Hobbs and Pan 2004) and on temporal anchoring and event ordering in text (Hitzeman, Moens, and Grover 1995; Mani and Wilson 2000; Filatova and Hovy 2001; Boguraev and Ando 2005; Mani et al. 2006; Lapata and Lascarides 2006). The uncertainty of temporal durations has been recognized as one of the most signiﬁcant issues for temporal reasoning (Allen and Ferguson 1994). Chittaro and Montanari (2000) point out by way of example that we have to know how long a battery remains charged to decide when to replace it or to predict the effects of actions which refer to the battery charge as a precondition. Yet to our knowledge, there has been no serious published empirical effort to model and learn the vague and implicit duration information in natural language, and to perform reasoning over this information. Cyc has some fuzzy duration information, although it is not generally available; Rieger (1974) discusses the issue for less than a page; there has been work in fuzzy logic on representing and reasoning with imprecise durations (Godo and Vila 1995; Fortemps 1997). But none of these efforts make an attempt to collect human judgments on such durations or to extract them automatically from text. Nevertheless, people have little trouble exploiting temporal information implicitly encoded in the descriptions of events, relying on their knowledge of the range of usual durations of types of events. This hitherto largely unexploited information is part of our commonsense knowledge. We can estimate roughly how long events of different types last and roughly how long situations of various sorts persist. We know that government policies typically last somewhere between one and ten years, and weather conditions fairly reliably persist between three hours and one day. We are often able to decide whether two events overlap or are in sequence by accessing this information. We know that if a war started yesterday, we can be pretty sure it is still going on today. If a hurricane started last year, we can be sure it is over by now. This article describes an exploration into how this information can be captured automatically. Our results can have a signiﬁcant impact on computational linguistics applications like event anchoring and ordering in text (Mani and Schiffman 2007), event coreference (Bejan and Harabagiu 2010), question answering (Tao et al. 2010; Harabagiu and Bejan 2005), and other intelligent systems that would beneﬁt from such temporal commonsense knowledge, for example, temporal reasoning (Zhou and Hripcsak 2007). Our goal is to be able to extract this implicit event duration information from text automatically, and to that end we ﬁrst annotated the events in news articles with bounds on their durations. The corpus that we have annotated currently contains all 48 non-Wall-Street-Journal (non-WSJ) news articles (2,132 event instances), as well as 10 WSJ articles (156 event instances), from the TimeBank corpus annotated in TimeML (Pustejovsky et al. 2003). The non-WSJ articles (mainly political and disaster news) include both print and broadcast news that are from a variety of news sources, such as ABC, AP, CNN, and VOA. All the annotated data have already been integrated into the TimeBank corpus.1 This article is organized as follows. In Section 2 we describe our annotation guidelines, including the annotation strategy and assumptions, and the representative event  
These experiments demonstrate the utility of the corpus, and show that many NLP applications can now make use of NP structure. 1. Introduction The parsing of noun phrases (NPs) involves the same difﬁculties as parsing in general. NPs contain structural ambiguities, just as other constituent types do, and resolving ∗ School of Information Technologies, University of Sydney, NSW 2006, Australia. E-mail: dvadas1@it.usyd.edu.au. ∗∗ School of Information Technologies, University of Sydney, NSW 2006, Australia. E-mail: james@it.usyd.edu.au. Submission received: 23 April 2010; revised submission received: 17 February 2011; accepted for publication: 25 March 2011 © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 4  these ambiguities is required for their proper interpretation. Despite this, statistical methods for parsing NPs have not achieved high performance until now. Many Natural Language Processing (NLP) systems speciﬁcally require the information carried within NPs. Question Answering (QA) systems need to supply an NP as the answer to many types of factoid questions, often using a parser to identify candidate NPs to return to the user. If the parser cannot recover NP structure then the correct candidate may never be found, even if the correct dominating noun phrase has been found. As an example, consider the following extract: . . . as crude oil prices rose by 50%, a result of the. . . and the question: The price of what commodity rose by 50%? The answer crude oil is internal to the NP crude oil prices. Most commonly used parsers will not identify this internal NP, and will never be able to get the answer correct. This problem also affects anaphora resolution and syntax-based machine translation. For example, Wang, Knight, and Marcu (2007) ﬁnd that the ﬂat tree structure of the Penn Treebank elongates the tail of rare tree fragments, diluting individual probabilities and reducing performance. They attempt to solve this problem by automatically binarizing the phrase structure trees. The additional NP annotation provides these SBSMT systems with more detailed structure, increasing performance. However, this SBSMT system, as well as others (Melamed, Satta, and Wellington 2004; Zhang et al. 2006), must still rely on a non-gold-standard binarization. Our experiments in Section 6.3 suggest that using supervised techniques trained on gold-standard NP data would be superior to these unsupervised methods. This problem of parsing NP structure is difﬁcult to solve, because of the absence of a large corpus of manually annotated, gold-standard NPs. The Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) is the standard training and evaluation corpus for many syntactic analysis tasks, ranging from POS tagging and chunking, to full parsing. However, it does not annotate internal NP structure. The NP mentioned earlier, crude oil prices, is left ﬂat in the Penn Treebank. Even worse, NPs with different structures (e.g., world oil prices) are given exactly the same annotation (see Figure 1). This means that any system trained on Penn Treebank data will be unable to model the syntactic and semantic structure inside base-NPs.  Figure 1 Parse trees for two NPs with different structures. The top row shows the identical Penn Treebank bracketings, and the bottom row includes the full internal structure. 754  Vadas and Curran  Parsing Noun Phrases in the Penn Treebank  Our ﬁrst major contribution is a gold-standard labeled bracketing for every ambiguous noun phrase in the Penn Treebank. We describe the annotation guidelines and process, including the use of named entity data to improve annotation quality. We check the correctness of the corpus by measuring inter-annotator agreement and by comparing against DepBank (King et al. 2003). We also analyze our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs. This new resource will allow any system or corpus developed from the Penn Treebank to represent noun phrase structure more accurately. Our next contribution is to conduct the ﬁrst large-scale experiments on NP parsing. We use the newly augmented Treebank with the Bikel (2004) implementation of the Collins (2003) model. Through a number of experiments, we determine what effect various aspects of Collins’s model, and the data itself, have on parsing performance. Finally, we perform a comprehensive error analysis which identiﬁes the many difﬁculties in parsing NPs. This shows that the primary difﬁculty in bracketing NP structure is a lack of lexical information in the training data. In order to increase the amount of information included in the NP parsing model, we turn to NP bracketing. This task has typically been approached with unsupervised methods, using statistics from unannotated corpora (Lauer 1995) or Web hit counts (Lapata and Keller 2004; Nakov and Hearst 2005). We incorporate these sources of data and use them to build large-scale supervised models trained on our Penn Treebank corpus of bracketed NPs. Using this data allows us to signiﬁcantly outperform previous approaches on the NP bracketing task. By incorporating a wide range of features into the model, performance is increased by 6.6% F-score over our best unsupervised system. Most of the NP bracketing literature has focused on NPs that are only three words long and contain only nouns. We remove these restrictions, reimplementing Barker’s (1998) bracketing algorithm for longer noun phrases and combine it with the supervised model we built previously. Our system achieves 89.14% F-score on matched brackets. Finally, we apply these supervised models to the output of the Bikel (2004) parser. This post-processor achieves an F-score of 79.05% on the internal NP structure, compared to the parser output baseline of 70.95%. This work contributes not only a new data set and results from numerous experiments, but also makes large-scale wide-coverage NP parsing a possibility for the ﬁrst time. Whereas before it was difﬁcult to even evaluate what NP information was being recovered, we have set a high benchmark for NP structure accuracy, and opened the ﬁeld for even greater improvement in the future. As a result, downstream applications can now take advantage of the crucial information present in NPs.  2. Background The internal structure of NPs can be interpreted in several ways, for example, the DP (determiner phrase) analysis argued by Abney (1987) and argued against by van Eynde (2006), treats the determiner as the head, rather than the noun. We will use a deﬁnition that is more informative for statistical modeling, where the noun—which is much more semantically indicative—acts as the head of the NP structure. A noun phrase is a constituent that has a noun as its head,1 and can also contain determiners, premodiﬁers, and postmodiﬁers. The head by itself is then an unsaturated  
1. Context length. Careful selection of the length of the history or context that is the basis for predicting the next word. 2. Interpolation. Models typically interpolate several predictions, for example, predictions that are based on several different context lengths. 3. Classes. In a class-based model, prediction is (partially) based on classes that the n-grams involved are members of. 4. Similarity. Similarity models smooth predictions with predictions for similar entities. ∗ Institute for Natural Language Processing. E-mail: hinrichcl11@ifnlp.org. ∗∗ Universita¨t Stuttgart, Institut fu¨ r Maschinelle Sprachverarbei-Azenbergstrasse 12, D-70174 Stuttgart, Germany. E-mail: michael.walsh@ims.uni-stuttgart.de. Submission received: 3 August 2010; revised submission received: 17 February 2011; accepted for publication: 30 March 2011 © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 4  The language models that are most commonly used today, in particular the modiﬁed Kneser-Ney (KN) model (Chen and Goodman 1998), are based on the ﬁrst two strategies, context length and interpolation—that is, they interpolate distributions of different history lengths. We will call such models history-length interpolated models. The set of contexts that history-length-interpolated models base their prediction on is limited to those whose history is identical for the history length considered. For example, the length-2 component of the model will compute the probability of P(w3|w1w2) based on contexts in the training corpus with identical history w1w2. Class-based and similarity-based models consider a wider range of contexts. Their estimates rely on contexts in the training data that are similar to (or in the same class as) the new sequence whose probability is to be estimated. Thus, for example, in attempting to estimate a probability for the bigram black cloud, unseen in training, the transition probability associated with the class to which black belongs being followed by the class to which cloud belongs can be used. The intuition is that although black cloud might not have been seen in training, the class sequence containing related bigrams like gray cloud, or black mist, or gray mist, that is, combinations of other members of the two classes seen in training, can offer a reasonable estimate. In principle, this type of generalization is more powerful than history-length interpolation and has been, and continues to be, used to good effect in a variety of domains. However, the model must be a good model of the distribution of sequences of strings; if its assumptions are too unrealistic or approximate, then class-based generalization will perform worse than history-length interpolation. Although there has been much work on class-based and similarity-based language models in recent years, no such model has been widely adopted as superior to historylength-interpolated models. We believe one reason for this is that the granularity of context that is optimal for generalization has not been investigated sufﬁciently. Consequently, in this article, we present the following contributions: r We demonstrate that class-based models can be made more effective. In particular, we put forward the half-context hypothesis as a general principle on the basis of which to construct class-based language models. r We argue for the novel, beneﬁcial use of a mixed n-gram class of both bigrams and unigrams instead of a class of unigrams alone. r We specify a discounting method which facilitates better treatment of rare events. r We deploy a new clustering algorithm for class-based language models that is more efﬁcient than the exchange algorithm. r We perform a systematic investigation, including signiﬁcance testing, of half-context versus whole-context class-based models which demonstrates the utility of a half-context approach. r We carry out a novel ﬁne-grained context-speciﬁc experimental validation of a half-context model that performs better than a traditional class-based model, and, when interpolated, improves on a modiﬁed KN trigram model. This new ﬁne-grained analysis distinguishes those contexts best suited to history-length interpolation and those most appropriate for class-based generalization. 844  Schu¨ tze and Walsh  Half-Context Language Models  These contributions, particularly our analyses, offer a richer understanding of the relative characteristics of history-length interpolation and class-based generalization and should lead to more powerful language models that combine class-based and historylength generalization mechanisms. The remainder of the article is organized as follows. Section 2 deﬁnes half-context representation and puts forward the half-context hypothesis. Section 3 develops a halfcontext language model in the context of a speciﬁc subset of related prior work on language modeling. Additional related work is discussed in a subsequent subsection. Parameter estimation is described in Section 4. A variety of models and interpolations are evaluated, and ﬁne-grained results, signiﬁcance tests, and context-speciﬁc analyses are discussed in Section 5. Conclusions and opportunities for future work are presented in Section 6.  2. Half Contexts The representation employed in this article builds on a speciﬁcation used in our earlier work (Schu¨ tze 1993, 1995; Schu¨ tze and Walsh 2008), motivated by Exemplar Theory (Hintzman 1986; Nosofsky 1986; Pierrehumbert 2001), where rich exemplar representations facilitated the acquisition of local grammatical knowledge and outperformed a categorical representation in the same task. Speciﬁcally, each word was represented in terms of its immediate left and right neighborhood context. These neighborhoods were treated separately for two reasons: (1) separate treatment of left-neighbor information and right-neighbor information resulted in reduced complexity in the model and better generalization, and (2) right and left context behavior can differ considerably, for example, him and her would have very similar left contexts but could have signiﬁcantly differing right contexts (e.g., compare the life in her garden vs. the life in him garden). These representations of left and right context distributions of a given word were known as half-words but can in fact be viewed as a word-level instantiation of a broader representational formalism which we term half-contextualization. According to this schema a given unit (word, n-gram, class, etc.) is represented in terms of half-context (HC) distributions over its immediate left and right neighborhoods. Hence, for example, at the bigram level, each bigram type is speciﬁed by two distributions, namely a left HC distribution Pl and right HC distribution Pr that capture the bigram’s behavior to the immediate left/right. For example, given walk home early twice, and drive home early once, then the left HC distribution of the bigram home early, denoted Plhome early, is Plhome early(walk) = 2/3 and Plhome early(drive) = 1/3, and 0 for all other words. These HC distributions underpin the HC language models presented in Section 3. In order to determine the extent of the particular merits of considering words as possessing two separate directional behaviors, in the experiments that follow we compare our HC language model against a whole-context (WC) model where a given word’s WC distribution is a single distribution which combines the word’s left and right HC distributions. For a clear statement of the contrast HC vs. WC, we deﬁne inward and outward distributions. For the estimation of P(wn+1|w1,n) based on a training set S, the inward distributions IWwn+1|w1,n consist of the set of right contexts of w1,n and left contexts of wn+1 in S; the outward distributions OWwn+1|w1,n consist of the set of left contexts of w1,n and right contexts of wn+1 in S.  845  Computational Linguistics  Volume 37, Number 4  We can then state our underlying hypothesis that HC-based classes are better for language modeling than WC-based classes as follows. Half-Context Hypothesis. A distributional language model should base its estimate of P(wn+1|w1,n ) on contexts v1,nvn+1 whose inward distributions IWvn+1|v1,n are similar to IWwn+1|w1,n . Similarity of the outward distributions OWwn+1|w1,n and OWvn+1|v1,n should not be employed as a criterion for using or not using training set contexts v1,nvn+1 for the estimation of P(wn+1|w1,n ). An example for the intuition behind the HC hypothesis is the him/her example given earlier. When estimating P(him|Mary helped), a context like Mary helped her should also be considered as evidence because even though the right contexts of him and her in the corpus are dissimilar, their left contexts are similar. The HC hypothesis states that we should only worry about similarity of the “relevant side” of the n-grams involved, that is we should only consider inward distributional information. Most clustering algorithms used for class-based language models, notably the exchange algorithm (Brown et al. 1992; Kneser and Ney 1993; Martin, Liermann, and Ney 1998), are “whole-context” clustering algorithms that violate the hypothesis. The HC hypothesis provides an alternative basis for designing class-based language models. In general, in designing a language model only information sources that are relevant for the task to be solved should be included. Adding additional complexity or nonrelevant additional features increases the variance of predictions without improving their accuracy. We can view this as a type of bias–variance tradeoff. Half-context models are simpler and have less variance because they only use one half of the available context information, the half that is actually useful for prediction. The experimental results that we report later in this article conﬁrm this by demonstrating that half-context models perform signiﬁcantly better than whole-context models. A consequence of only using inward distributions in accordance with the halfcontext hypothesis is that we need two different types of classes: one set of classes for the predictors and another set of classes for the predictees. The reason is that we use two distinct and unrelated representations, left-context distributions to induce classes of predictees and right-context distributions to induce classes of predictors. In other words, half-context models are inherently asymmetric, reﬂecting the fact that language models are inherently asymmetric: The role of the predictor and the predicted are different. This asymmetry shows up in word-based models to a limited extent: In most models the unit of prediction is a word; predictors include n-grams of any size in principle, not just words. However, in a class-based model the asymmetry between predictor and predicted is more important: There is no justiﬁcation for the premise (made, for example, in the Brown model) that the classes that are optimal for predictors are also the classes that are optimal for predictees. This observation has also been made by Gao et al. (2002), albeit without explicit reference to half contexts. We view our approach as better motivated since the asymmetry of our model is not posited, but follows from an analysis of the information sources needed for probabilistic inference in language modeling. Linguistic theory also provides evidence for half-context models. In many theories, there is a single formal concept that can be instantiated either by arguments of prepositions or by arguments of transitive verbs. For example, there are few if any syntactic differences between the arguments that can appear after a preposition like by and after a transitive verb like brought. Thus, the predicting histories by and brought should be treated alike in a class-based model. But that is not possible in a  846  Schu¨ tze and Walsh  Half-Context Language Models  whole-context model. We can interpret this as a syntactic justiﬁcation for half-context models. Referring back to our earlier allusion to the bias–variance tradeoff, if we had unlimited data, then estimating separate distributions for by and brought would be unproblematic; but because training sets are not unlimited, we can improve generalization by assigning the two linguistically identical contexts to the same right-context class. Finally, efﬁciency is also a strong argument for half-context models. Time of clustering and storage requirements are cut in half by omitting those parts of the context that are nonrelevant. The time complexity of many clustering algorithms depends on the number of different types of features that occur in a particular cluster as opposed to the number of tokens. The number of feature types occurring in a cluster is reduced substantially in half-context models. It is of course possible to ﬁnd cases where the outward distributions are helpful for accurate estimation. Consider estimating P(is|strilp), where strilp is a word that occurred once in the training set. Suppose for the sake of argument that is after nouns is more likely than is after adjectives (because phrases like yellow is the new black are infrequent). If strilp occurred in the context a very strilp car, then it is likely to be an adjective and P(is|strilp) is low. If strilp occurred in the context the strilp car, then it could also be a noun (as in the bakery car or the wedding ring) and P(is|strilp) should be estimated to be higher. In this case, it is the outward distribution of strilp that helps us to arrive at an accurate estimate. However, our hypothesis is not that there are no such cases; rather, we believe that as a generalization mechanism, only inward distributional information is useful in improving performance. This is borne out by the experiments reported herein. In the future, there may be non-distributional models that use more complex inferences for language modeling. Parsing-based language models (e.g., Hall and Johnson 2003) are a ﬁrst step in this direction. The hypothesis would probably not apply to such non-distributional models.  3. Half-Context Language Model  3.1 Half-Contextualization  Our starting point is the model by Brown et al. (1992). It models the probability of class c2 following class c1 where c2 emits (e in the diagram) the member word w2 and w1 belongs to (∈, a deterministic process) class c1:  w1  w2  ∈  e  c1  seq  c2  This model has been frequently investigated and discussed. Recent examples include its successful application in word co-occurrence and sentence retrieval investigations (Momtazi and Klakow 2009; Momtazi, Khudanpur, and Klakow 2010), and polarity classiﬁcation of movie reviews (Wiegand and Klakow 2008).  847  Computational Linguistics  Volume 37, Number 4  The key concept introduced in this article is that of a half-context class. Class-based language models can be half-contextualized by replacing classes that model right and left contexts simultaneously by right half-context classes cr1 and left half-context classes cl2. Words are assigned to HC classes and these HC classes then generate words:  w1  w2  ∈  e  cr1  seq  cl2  3.2 Mixed n-gram Classes A second modiﬁcation of the Brown model we propose is motivated by the fact that trigram models perform better than bigram models because a sequence of two words signiﬁcantly limits the possible ways of continuing. For this reason, we condition the sequential continuation on a mixed n-gram class of both bigrams and unigrams instead of on a class of unigrams alone. The resulting model, the HC model, is depicted in Figure 1. We show the bigram w1w2 as the member of the class cr12, but cr12 can also be the class of w2 if w1w2 was not frequent enough to be included in the clustering (criteria for inclusion are discussed in Section 4). To summarize, the generative process shown in Figure 1 is that the right-context class cr12 to which the bigram w1w2 belongs generates a unigram left-context class cl3 which generates w3. As will become apparent from the description of parameter estimation and the clustering algorithm in Section 4, the HC classes in the model are based  Figure 1 The half-context language model. 848  Schu¨ tze and Walsh  Half-Context Language Models  on inward (IW) distributions only. The corresponding outward (OW) distributions are not taken into account in accordance with the HC hypothesis. In addition to the novel use of half-contexts it is important to note that the right HC classes employed are mixed classes of unigrams and bigrams rather than of unigrams only. To our knowledge this represents the ﬁrst such usage and can be motivated by the fact that frequent bigrams in language often behave similarly whereas the constituent unigrams do not. For example, the right HCs of the bigrams University of and based in are similar because both are often followed by locations; but the right HCs of of and in are much more diffuse and there are many prepositional objects that occur more often with of than with in (e.g., names of people) and others that occur more often with in than with of (e.g., response).  3.3 Discounting  In initial experiments, we found that it was difﬁcult to achieve an improvement using class-based generalization because for many contexts history-length interpolation is the better strategy for estimation. For a high-frequency event, it can be best to base estimates on instances of this event with identical history only—instead of smoothing them with other contexts that are in the same class. Consider the unigram Hong. In 3,998 out of 4,045 cases in the training set part of our corpus of Wall Street Journal (WSJ) articles (consisting of 40 million words), it is followed by Kong. In this case, redistributing probability mass to other members of the class that Kong is a member of will decrease the estimate for P(Kong|Hong) (an estimate that should be close to 3,998/4,045) and decrease the model’s performance. On the other hand, we have H(P(w|Mr.)) ≈ 11.9 in our WSJ training set. Any of a large number of ﬁrst and last names can occur after Mr. and a language model should reallocate some probability mass from names that did occur in this environment in the training set to those names that did not. To treat these two different cases correctly, we use a variant of absolute discounting (Ney, Essen, and Kneser 1994). Following the notation of Chen and Goodman (1998), we ﬁrst deﬁne the number N1+(w1,n•) of distinct words that can occur after an n-gram in the training set:  N1+(w1,n•) = |{w|C(w1,nw) > 0}|  where C(w1,n) is the frequency of w1,n in the training set. We then deﬁne the exemplar-theoretic (ET) language model as follows:  PET(wn+1|w1,n )  =  D  N1+ (w1,n • ) C(w1,n )  PHC(wn+1|w1,n )  +  max(0,  C(w1,nwn+1 C(w1,n )  )  −  D)  (1)  The discount D is a parameter of the model that controls how much of each count is redistributed to the class-based model. In a way that is similar to other discounting methods, this formalization satisﬁes the two desiderata stated earlier: The estimates of high-frequency events are, in relative terms, much less affected than those of lowfrequency events.  849  Computational Linguistics  Volume 37, Number 4  PET is the exemplar-theoretic model we will evaluate in the experiments described below. We use an analogous model for WC distributions. In that case, PHC is simply replaced by PWC in Equation (1). To summarize, the innovations of our exemplar-theoretic model are (1) the use of HC classes instead of WC classes, (2) the use of mixed classes of unigrams and bigrams (instead of classes of unigrams), and (3) the use of absolute discounting to concentrate the effect of class-based generalization on rare hard-to-estimate events while leaving robust estimates based on frequent events largely unchanged. 3.4 Additional Related Work In addition to the motivating articles discussed earlier, other relevant work includes the randomization techniques applied by Emami and Jelinek (2005) to class-based n-gram language models. Half-context clusters are not at odds with a randomized approach as they could easily be implemented in such a fashion. Other related research includes the “mixed” model employed by Uszkoreit and Brants (2008), in which a word bigram (as opposed to a class of bigrams) probabilistically generates a class. They use, in our terminology, whole-context classes. The experiments reported subsequently suggest that HC classes are preferable to WC classes in the Brown-type set-up (classes generating classes); we plan to investigate whether this is also true in a mixed model in future work. Bassiou and Kotropoulos (2011) investigate two word-clustering techniques that operate on long-distance bigram probabilities (of varying distances) within a context and on interpolated long-distance bigram probabilities, both with a view to capturing long-distance dependencies. Evaluation of both clustering techniques—hierarchical clustering exploiting Mahalanobis distances to form compact clusters and Probabilistic Latent Semantic Analysis—demonstrates that the use of long distance bigrams or their interpolated varieties yield more compact and meaningful (in the case of interpolated long distance bigrams) word clusters than the use of the traditional bigram (and bigrams which employ trigger pairs over various histories). This research demonstrates an interesting avenue for contemporary models of word clustering and it would be no doubt interesting to see how such clustering strategies might contribute to half-context clustering, how their clusters would compare to those produced via bisecting k-means (though we cluster bigrams also), as proffered here, and indeed how long distance bigrams could be half-contextualized; these questions, however, are beyond the scope of the current article which seeks primarily to investigate the potential merits of halfcontextualization. Related work by Justo and Torres (2009) explores the use of language models that employ classes containing phrases. They describe their models as two-level because speciﬁc language models act within the classes. Their ﬁrst approach takes into account the probabilities between words which constitute the different phrases of a given class, that is, phrases are sequences of unconnected words and words are considered the basic lexical unit, and their second approach considers phrases to be indivisible lexical units. The ﬁrst model is also interpolated with a standard word-based language model, and the second model is interpolated with a standard phrase-based model. Worderror-rate analyses in an ASR system indicate that these models are better than their traditional counterparts. These results provide useful motivation for extending classbased language models from classes of isolated words to classes of longer sequences, such as the classes of bigrams employed in the half-context model. Their research 850  Schu¨ tze and Walsh  Half-Context Language Models  does not, however, consider the different directional behaviors of words or bigrams as we do. Zitouni and Zhou (2007, 2008) propose linearly interpolated hierarchical language models (and a back-off variety [Zitouni 2007]) where each vocabulary item constitutes a leaf node in a word-tree, words are clustered into classes, and, in a recursive process, classes are clustered into more general classes until the root is reached. The tree root is a class containing all vocabulary items. In attempting to estimate the likelihood of an n-gram event they linearly interpolate over different language models, each one of which is trained on one level of the tree. In this way they seek to strike a balance between speciﬁcity and generalization. In constructing the class hierarchy, words are represented by their probability given their left and right neighboring words over a vocabulary (equivalent to the whole-context representation discussed in this article) and similarity between words is established using the Kullback-Leibler distortion measure. Words occurring frequently in similar contexts should be clustered together with a view to ﬁnding a set of clusters that minimizes global discriminative information (see also Bai et al. 1998). The clustering algorithm is based on k-means. The use of a hierarchical tree, and interpolating over it, represents an interesting approach not at odds with our research (i.e., that is, half-context classes could form nodes in the tree), although our approach differs in the separate treatment of word contexts, the use of bigrams as class members, and in the clustering methodology. Additional related work includes research by Bahrani et al. (2008), who build class-based models using the k-means algorithm and words represented in terms of vectors where each vector element corresponds to the number of times the word had a particular part of speech tag given a tagged corpus. This approach would typically yield much shorter feature vectors than approaches (including our own) which have vectors matching the vocabulary size, thus leading to lower time complexity. They do not, however, avail of classes of bigrams as we do, nor look at directional behavior of words (though half-contextualization using part of speech tags would be an interesting extension of both our model and theirs). Abdoos and Naeini (2008) use a clustering ensemble approach to categorize words, although it is unclear from their evaluation how such an approach compares, in terms of performance, to others in the literature. Gao et al. (2002) propose an asymmetric clustering model (ACM) grounded upon the apt observation that different clusters for predicted and conditional words should be employed, a view shared here. Their research does not present an explicit treatment of half-contextualization, however, nor considers halfcontextualization and the signiﬁcance of inward distributional information as insights which meet language modeling needs. Furthermore, our evaluation also differs in that it involves comparison against a whole-context model and a modiﬁed KN trigram model, rather than a simple word trigram model. Our use of a mixed n-gram class of both bigrams and unigrams also represents a marked difference between approaches. With regard to context direction Essen and Steinbiss (1992) also look at left and right contexts similarly to our approach. However, they do not compare half-context with whole-context approaches and they pursue a less efﬁcient similarity-based approach in contrast to the class-based approach proposed here. Finally, Dagan, Lee, and Pereira’s (1999) similarity-based language model uses a similar word to the observed word as the conditioning context used to generate the next word in the sequence. Again, no comparison to whole-context approaches is made. A similarity-based approach is also difﬁcult to use for large corpora as it would necessitate the calculation of similarity of every word to every other word in the corpus. Similarities 851  Computational Linguistics  Volume 37, Number 4  can be computed more efﬁciently for a subset of words on a smaller corpus, but then many of the rare events that class and similarity based methods are most beneﬁcial for will not be covered. Our analyses in Section 5.2 and Section 5.3 demonstrate that half-context modeling is most beneﬁcial for rare events. Similar concerns apply to other similarity-based models, such as those proposed by Bengio et al. (2003) and Schwenk and Koehn (2008).  4. Parameter Estimation In this section we describe how the parameters of the model in Figure 1 are estimated. These parameters belong to two broad categories, namely, those which model the HC distributions (Pr and Pl) and are used in the construction of clusters, and those which capture emission probabilities Pe and sequence probabilities Pseq that are used when the model is applied. Estimates were calculated on the basis of the training set part of a corpus of WSJ articles, 1987–1989, consisting of almost 50 million words, which will be described in more detail subsequently.  4.1 Clustering of HC Distributions In the clustering, n-grams are represented as HC distributions. These distributions are estimated using maximum likelihood as follows:  Prw1w2 (w3 ) = Prw2 (w3 ) = PrUNK(w3 ) = Plw3 (w2 ) = PlUNK(w2 ) =  C(w1 w2 w3 ) w C(w1w2w) C(w2 w3 ) w C(w2w) C(w3 ) w C(w) C(w2 w3 ) w C(ww3) C(w2 ) w C(w)  Only a subset of items is clustered. When clustering unigrams we include all 54,243 unigrams that occur more than 10 times in the corpus as well as the unknown word UNK. For mixed clusterings of unigrams and bigrams we include all 378,109 unigrams and bigrams that occur more than 10 times and the unknown word UNK (thus, the unigram set is a subset of the mixed set). We call these sets Suni and Smixed and they are used for all HC and WC models herein, including the Brown model. We employ bisecting k-means (Steinbach, Karypis, and Kumar 2000) to cluster HC distributions. The distance measure employed is Euclidean distance because the formal properties of k-means, including convergence, only apply to Euclidean spaces. Bisecting k-means is applied to a small random sample of the set of items: k-means ﬁrst splits this random sample in two, then the largest existing cluster is split and so on until k = 512 (or k = 1,024, depending on the experiment) clusters have been found. The size of the random sample is then doubled, items in the enlarged sample are assigned to cluster  852  Schu¨ tze and Walsh  Half-Context Language Models  centroids, and centroids are recomputed. The size of the sample is doubled again and so on until all items have been assigned. Incremental doubling of the sample has the advantage that several iterations of reassignment and recomputation of centroids are performed (thus producing centroids that are good representatives of the overall distribution of items); and that at the same time the total number of assignments that needs to be computed is bounded by 2M where M is the number of items. Computing the assignments is responsible for almost all the computation time of k-means and more than 90% of the time needed to estimate the parameters of the exemplar-theoretic model. We do not investigate the effect of the number of clusters k on the performance of class models in this article. As the default we chose k = 1,024, similar to Brown et al.’s experiments. Note that we have 512 left HC clusters and 512 right HC clusters, a total of 1,024 in the experiments with k = 512. We also experiment with 2 × 1,024 clusters because one could also argue that this is the setting that is most comparable to Brown et al. We choose the powers of 2, k = 512 and k = 1,024 (instead of 500 and 1,000), for optimal compression and compact storage. Two examples of half-context clusters (one left HC cluster and one right HC cluster) and their sizes are given in Table 1. The three most frequent words in the left HC cluster have similar left contexts (dominated by forms of to be) and different right contexts (large variety of part of speech forms). The three most frequent bigrams in the right HC cluster have similar right contexts (dominated by gerunds) and dissimilar left contexts (again a large variety of possibilities). In traditional whole-context clusters one would need to split the two clusters at least in two. For example, the right HC cluster in Table 1 would have to be split into one subcluster containing without-ﬁrst/of-improperly and one subcluster containing pain-and. However, this presents two distinct disadvantages: (1) The extra clusters would require the estimation of more parameters, each based on fewer data points and hence less reliable, and (2) The left-context generalization, whereby of-improperly and pain-and have similar right contexts, would be lost. Once clusters and cluster memberships have been computed, we need to determine the relevant right HC cluster cr12 and left HC cluster cl3 when computing the probability P(w3|w1w2) according to the model in Figure 1. We do this as follows: 1. If w1w2 ∈ Smixed, we use the right HC cluster that Prw1w2 was assigned to. 2. Otherwise, if w2 ∈ Smixed, we use the right HC cluster that Prw2 was assigned to. 3. Otherwise, we use the right HC cluster that PrUNK was assigned to. 4. If w3 ∈ Suni, we use the left HC cluster that Plw3 was assigned to. 5. Otherwise, we use the left HC cluster that PlUNK was assigned to.  Table 1 Examples of half-context clusters.  most frequent n-grams in cluster size  left HC cluster unlikely, unclear, happening  753  right HC cluster pain-and, without-ﬁrst, of-improperly 248  853  Computational Linguistics  Volume 37, Number 4  4.2 Emission and Sequence Probabilities Emission probabilities need only be estimated for left HC clusters in the exemplartheoretic model. They are estimated by maximum likelihood:  Pe(w|c) =  C(w) w ∈c C(w )  Cluster sequence probabilities are additively smoothed:  Pseq(cl|cr )  =  C(crcl) + λ C(cr) + Bλ  where λ = 0.1, B ∈ {512, 1,024} is the number of HC clusters, C(crcl) is the number of trigrams w1w2w3 occurring in the training set, where w1w2 was assigned to cr and w3 to cl, and C(cr) is the number of bigrams w1w2 occurring in the training set, where w1w2 was assigned to cr. WC clusters are generated by representing an n-gram as the concatenation of two HC distributions, its left HC distribution and its right HC distribution. Clustering, membership assignment, and probability estimation are the same in all other respects.  5. Experiments and Analysis A corpus of WSJ articles, 1987–1989, consisting of almost 50 million words, was randomly split into training set (80%), validation set (10%), and test set (10%). Unigrams, bigrams, and trigrams and their counts were extracted from training, validation, and test sets. A modiﬁed KN model (Chen and Goodman 1998), termed P(KN), was estimated on the training set count ﬁles and applied to the test set using srilm, the SRI language modeling toolkit (Stolcke 2002). The same count ﬁles were the input to the HC and exemplar-theoretic model estimation and application procedure. Vocabulary size was the same for both KN and exemplar-theoretic models: 256,874 (the 256,873 words occurring in the training set and the unknown word). A total of 70.8% of tokens w3 in the test set occur in a context w1w2w3 occurring in the training set; for 22.2% of tokens only w2w3 occurs in the training set; and for 6.7% only w3 occurs in the training set. The out-of-vocabulary rate is 0.27%. All validation and test set words that do not occur in the training set are mapped to the special unknown token UNK. In all interpolation experiments, the weight of the P(KN) model is 1 − α and the weight of the model with which P(KN) is interpolated is α. The validation set was employed to determine the optimum interpolation weight α and discount D for each case. Total processing time for estimating the HC clusters for Suni and Smixed (lines 13 and 15 in Table 4, subsequently) was less than 3.5 hours on an Opteron 8214 processor. In evaluating our model it seems appropriate to compare its performance against other class-based models. Consequently, the SRI toolkit was also used to construct a class bigram language model, following the incremental version of the algorithm proposed by Brown et al. (1992), which we simply term the P(Brown) model. A total of 854  Schu¨ tze and Walsh  Half-Context Language Models  Table 2 Perplexity results for interpolation of P(Brown) with a bigram model P(KN). α = 0 corresponds to P(KN) alone, α = 1 corresponds to P(Brown) alone.  perplexity  α  validation test  0.000 0.025 0.050 0.075 0.100 0.200 1.000  164.52 164.08 164.03 164.15 164.40 166.25 245.14  164.80 164.33 245.45  Table 3 Models used in our experiments.  P(KN) P(ET-Brown) P(Half) P(Whole) P(KN-Brown) P(KN-Half) P(KN-Whole)  modiﬁed Kneser-Ney model exemplar-theoretic Brown model exemplar-theoretic Half-Context model (Equation 1) whole-context analogue of Equation 1 interpolation of P(KN) with P(ET-Brown) interpolation of P(KN) with P(Half) interpolation of P(KN) with P(Whole)  1,024 classes (the same number of classes as the combined left and right context clusters in the 2 × 512 HC model) were derived from the training data.1 Table 2 presents results for the interpolation of P(Brown) with a bigram model P(KN) when applied to the validation set over a number of interpolation weights, followed by results from the test data using the optimum weight for the P(Brown) model (α = 0.05) found during the validation phase. It is clear from Table 2 that although interpolating a traditional class-based model with a KN bigram model does offer some beneﬁt, this beneﬁt is slight (perplexity = 164.80 for P(KN) alone, versus 164.33 using the optimum interpolation weight on the test set). It is also clear that the traditional class-based model operating by itself (α = 1.0, perplexity = 245.45) performs poorly relative to P(KN). Of course the SRI class-based model employs whole-context classes, not half- context distributions which consider behavior to the left and right separately. The following models, detailed in Table 3, were used in our experiments: modiﬁed Kneser-Ney (P(KN)); exemplar-theoretic half-context (P(Half)); exemplar-theoretic wholecontext (P(Whole)); exemplar-theoretic Brown (P(ET-Brown)); and P(KN-Half), P(KN-Whole), and P(KN-Brown), the interpolations of Kneser-Ney with exemplar-theoretic half-context, whole-context, and Brown, respectively.2 Perplexity results, for each of these models, from the validation and test sets, are presented in Table 4. Order-2 in Table 4 implies  
Parsing algorithms based on 2-LCFGs can be very efﬁcient. In the general case, existing dynamic programming algorithms have running time of O(|w|4), where w is the input string. (In this article we disregard complexity factors that depend on the input grammar, which we will consider to be constants.) In cases in which, for each (lexical) head, the two processes of generating its left and right arguments are, to some ∗ School of Computer Science, University of St. Andrews, North Haugh, St. Andrews, Fife, KY16 9SX, Scotland. E-mail: markjan.nederhof@gmail.com. ∗∗ Department of Information Engineering, University of Padua, via Gradenigo 6/A, I-35131 Padova, Italy. E-mail: satta@dei.unipd.it. Submission received: 8 September 2010; revised submission received: 21 January 2011; accepted for publication: 17 April 2011. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 4  extent, independent one of the other, parsing based on 2-LCFGs can be asymptotically improved to O(|w|3). The reader is referred to Eisner and Satta (1999) for a detailed presentation of these computational results. In the literature, this condition on independence between left and right arguments of each head has been called splittability (Eisner 1997; Eisner and Satta 1999). Testing for splittability on an input 2-LCFG is therefore of central interest to parsing efﬁciency. The computability of this test has never been investigated, however. In this article splittability is deﬁned for 2-LCFGs in terms of equivalence to another grammar in which independence between left and right arguments of each head is ensured by a simple syntactic restriction. This restriction is called split form. Informally, a 2-LCFG is in split form if it can be factorized into individual subgrammars, one for each head, and each subgrammar produces the left and the right arguments of its head through two derivation processes, one happening strictly after the other. One may believe that a necessary and sufﬁcient condition for splittability is that a 2-LCFG does not allow recursive structures that alternately generate left (L) and right (R) arguments of some head a. These structures could well result in subderivations producing sequences of the form LnaRn, for any n ≥ 0, which would appear to preclude the application of the O(|w|3) algorithm. This situation, however, does not mean in general that the grammar is not splittable. Although a subset of the rules in a grammar may generate structures such as those just discussed, there may be additional rules that make the observed dependencies between left and right arguments irrelevant. In fact, splittability of a 2-LCFG is undecidable, as will be shown in this article. Our result is based on the fact that it is undecidable whether a linear context-free grammar with a center marker (to be deﬁned later) generates a regular language. This fact is originally proved in this article, and does not follow from the weaker result stating the undecidability of regularity of (general) linear context-free languages, which is well known in the formal language literature (Hopcroft and Ullman 1979, exercise 8.10a, page 214) and which is originally due to Hunt III and Rosenkrantz (1974). The remaining part of this article is organized as follows. In Section 2 we present some preliminary background, and in Section 3 we deﬁne 2-LCFGs and the notion of splittability. In Section 4 we prove the main result of this article, and draw some conclusions in Section 5.  2. Preliminaries In this article we assume the reader is familiar with the notions of context-free grammar, Turing machine, and undecidability (see, e.g., Hopcroft and Ullman 1979). We brieﬂy summarize the adopted notation now. A context-free grammar (CFG) is a 4-tuple G = (Σ, N, S, R), where Σ is a ﬁnite set of terminals, N is a ﬁnite set of nonterminals, disjoint from Σ and including the start symbol S, and R is a ﬁnite set of rules. Each rule has the form A → α, where A ∈ N and α ∈ (Σ ∪ N)∗. A CFG is called linear if each rule A → α has at most one nonterminal in the right-hand side α. We associate with G a binary relation called rewrite and denoted by the symbol ⇒G, deﬁned such that γAγ ⇒G γαγ if A → α is a rule in R and γ, γ ∈ (Σ ∪ N)∗. We drop subscript G from ⇒G whenever it is understood from the context. The reﬂexive and transitive closure of ⇒ is denoted as ⇒∗. The language generated by G is deﬁned as L(G) = {w ∈ Σ∗ | S ⇒∗ w}. 868  Nederhof and Satta  Splittability of 2-LCFGs  We now introduce a special class of CFGs that will play a central role in the proofs in this article. Assume a distinguished symbol # ∈ Σ, which we will refer to as a center marker. The class of linear CFGs with center marker #, or lin-CFG(#) for short, is the class of CFGs in which each rule either has the form A → # or the form A → uBv, where A, B ∈ N and u, v ∈ (Σ \ {#})∗, where symbol “\” denotes set difference. We say a grammar in lin-CFG(#) is in binary form if the total length of u and v in rules A → uBv is at most 1. It is not difﬁcult to see that there is a language-preserving transformation of grammars in lin-CFG(#) to binary form, as suggested by the following example. Example 1 Let Σ = {a, b, #}. One member of lin-CFG(#) is the CFG G deﬁned by the rules: S → a S a, S → a S b, S → b S a, S → b S b, S → #. For this grammar, strings in L(G) have the form u#v, with u ∈ (Σ \ {#})∗ and v ∈ (Σ \ {#})∗ having the same length. A rule such as S → aSb can be replaced by a pair of rules in binary form, for example S → aA and A → Sb, where A is a new nonterminal. 3. Bilexical CFGs and Splittability We start this section with the deﬁnition of 2-LCFG, which is based on Eisner and Satta (1999). Let ND be a ﬁnite alphabet whose symbols will be called delexicalized nonterminals. We combine ND with a set Σ of terminal symbols as follows: ND(Σ) = {A[a] | A ∈ ND, a ∈ Σ}. A bilexical context-free grammar is a CFG G = (Σ, ND(Σ) ∪ {S}, S, R). Every rule in R has one of the following ﬁve forms: r S → A[a]; r A[a] → B[b] C[a]; r A[a] → B[a] C[c]; r A[a] → B[a]; r A[a] → a. The nonterminal occurrences C[a] and B[a] in the second and third rules, respectively, and the nonterminal occurrence B[a] in the fourth rule will be referred to as head child occurrences. Notice that the head child of a rule is always associated with the same terminal as the left-hand-side nonterminal. The nonterminal occurrence A[a] in the ﬁrst rule, and the nonterminal occurrences B[b] and C[c] in the second and third rules, respectively, will be referred to as maximal projection occurrences. Observe how in a 869  Computational Linguistics  Volume 37, Number 4  parse tree generated by a 2-LCFG, each occurrence of a lexical element (represented as a terminal symbol) is also part of several head child occurrences of nonterminals above it, up to some unique maximal projection occurrence. We assume that the head child occurrence in a rule is always marked within the rule itself, in order to disambiguate cases in which the head child and the maximal projection share the same terminal (a = b in the second rule or a = c in the third).1 Let G = (Σ, N, S, R) be a 2-LCFG and choose some A[a] ∈ N that occurs as maximal projection in some rule in R. We now deﬁne a grammar G(A[a]) = (Σ(A[a]), N(A[a]), A[a], R(A[a])) in lin-CFG(a). The main idea here is that G(A[a]) captures all descending paths in parse trees of G from any maximal projection occurrence of A[a] down to a, treating the maximal projection occurrences of G’s nonterminals to the left and right of these paths as terminal symbols of G(A[a]). Each such maximal projection nonterminal B[b], when treated as a terminal, will be denoted as B[b]. The start symbol is A[a] and the rule set R(A[a]) is speciﬁed as follows: r D[a] → B[b] C[a] is in R(A[a]) for each rule D[a] → B[b] C[a] in R; r D[a] → B[a] C[c] is in R(A[a]) for each rule D[a] → B[a] C[c] in R; r D[a] → B[a] is in R(A[a]) for each rule D[a] → B[a] in R; r D[a] → a is in R(A[a]) for each rule D[a] → a in R. Grammar G(A[a]) might contain useless rules, that is, rules that never appear in a derivation of a string of the generated language, but this is irrelevant to the development of the results in this article. We now introduce an equivalence relation on 2-LCFGs, which will be used later in the deﬁnition of splittability. As we will see, our equivalence relation is stronger than the usual weak equivalence between grammars, where the latter only requires that the languages generated by two grammars be the same. In addition to weak equivalence, we demand that two 2-LCFGs establish the same predicate–argument dependencies between lexical elements in the generated sentences, as will be explained here. Deﬁnition 1 Two 2-LCFGs G1 and G2 are d-equivalent if the following conditions are all satisﬁed: 1. G1 and G2 have the same set of nonterminals that occur as maximal projections; 2. G1 and G2 have the same rules rewriting the start symbol, that is, the rules of the form S → A[a]; and 3. for each nonterminal A[a] that occurs as a maximal projection in G1 and G2, the grammars G1(A[a]) and G2(A[a]) generate the same languages. Lemma 1 If two 2-LCFGs G1 and G2 are d-equivalent, then L(G1) = L(G2). 
Deductive systems can provide a framework to prove correctness (soundness and completeness) of a parsing algorithm. Well-formed transformations of deductive systems would permit the addition of new capabilities such as weighted rules in the grammar. Deductive systems could also provide a means to compare different parsing algorithms. This book provides several examples of how such properties can be useful in parsing theory and parsing implementation, in particular for converting a parser into an error-correcting parser and explicitly showing the relationship between several dependency parsing algorithms. Sikkel’s deﬁnition of parsing schemas (Sikkel 1997) extends deductive systems by formally deﬁning the semantics of items and related concepts used in deductive systems. In particular, items are sets of partial constituency trees that are licensed by © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 4  rules of the grammar. As a result, parsing schemas allow compilation of schemas to executable parsers and also permit formal reasoning about properties of the parser directly. Unlike many deductive systems used to deﬁne parsers, there is no one-toone relationship between a parsing schema and algorithm. In later work, Alonso Pardo et al. (1999) showed parsing schemas can be used to deﬁne parsers for other grammar formalisms such as tree-adjoining grammars. This book, which is an extended version of Carlos Go´ mez Rodr´ıguez’s Ph.D. thesis work, extends the theory and practice of parsing schemas in several directions. There are three major parts to this book: Compiling and executing parsing schemas. The ﬁrst part of the book provides a language syntax that can be used to precisely specify parsing schemas and a compiler for this domain-speciﬁc language. The syntax is shown in the CYK example above. The full speciﬁcation also includes interpretations for indices such as i and i + 1 as word positions, A, B, C as grammar symbols, and how deduction steps and goals are converted into parser code. The implementation details, including static analysis of the schema and the Java code generation, are described well and in sufﬁcient detail. The source code of the compiler for parsing schemas is available for download at www.grupocole. org/software/COMPAS. (The code is typical research software—it takes some effort to use it, but once you do, you can play with compiling and running most of the schemas in the book.) This part of the book contains experiments on comparing many different parsing schemas for each of these formalisms. The comparison is done using handwritten grammars with feature structures (the parsers include feature uniﬁcation) and evaluated on test-suite data rather than on modern Treebank grammars and data from newswire and other “real-world” data. Another issue is that the comparison does not include the GHR parser (Graham, Harrison, and Ruzzo 1980), which may impact the comparison between CYK and Earley parsers. Also, interesting synthetic-data experiments are presented that compare tree-adjoining parsers with context-free parsers; it is not clear whether these results extend to natural language corpora. With regard to implementation of schemas, the focus is mainly on agenda-based implementation of deductive steps rather than, say, the use of (pushdown) transducers to produce parse trees. Error-repair parsers. The second part of the book focuses on error-repair in parsing (using parsing schemas, of course). Such an approach tries to deal with limited coverage of the grammar by performing insertions, deletions, or substitutions on the input string. This makes a lot of sense in programming language parsers, but for natural languages it makes little sense to transform the input because the grammar has poor coverage. It is trivial to add (weighted) glue rules that accept any input string, or a ﬁnite-state acceptor of strings can be used as a back-off grammar to improve coverage. Speech repair and other such cases are typically handled using appropriate augmentations of the underlying grammar combined with grammar-driven edits (Charniak and Johnson 2001). Despite this, error-repair is a good use-case for parsing schemas. Go´ mez Rodr´ıguez can show that some existing error-repair parsers are in fact provably correct, and also a generic recipe can be given that converts any given parser schema into an error-repair parser schema. This is an instructive use of parsing schema transformations, because it is easy to show that the changes preserve correctness. Parsing schemas for dependency parsers. This third part of the book has the potential to be the most popular. There is increased interest in multilingual dependency parsing, and there are a large number of different dependency parsing algorithms. Parsing schemas  882  Book Reviews  allow a concise description of many different parsing algorithms, and Go´ mez Rodr´ıguez provides many parsing schemas corresponding to popular dependency parsing algorithms; there are too many to list here, but he provides schemas for no less than ten dependency parsers, including some that recover non-projective dependencies. He also provides explicit relationships between schemas for these varied parsers, such as item reﬁnement (an item deduced in one parser is broken up as multiple items in another parser) or step reﬁnement (a deduction step in one parser can be emulated by a sequence of steps in another parser). It is also useful that these relationships are transitive and reﬂexive. However, it is in describing dependency parsing that the biggest weakness of parsing schemas is exposed and its potential role as an universal language for the parsing community runs into trouble. Non-constructive aspects of parsing cannot be represented with a schema, because that violates the semantics of deductive steps. For instance, in a parser that computes the dependency tree by using the minimum spanning tree (MST) algorithm (McDonald et al. 2005), there is a step that eliminates cycles in the graph. This step is not constructive and therefore the MST parser cannot be represented as a schema. Parsing schemas are generally grammar-driven and often parsers are written without any ﬁnite underlying grammar, which makes tree building harder to describe concisely. The discussion of related work touches on the use of Prolog for parsing schemas (Shieber, Schabes, and Pereira 1995), Datalog for specifying parsers (McAllester 2002; Liu and Stoller 2003), the DyALog system (Villemonte de la Clergerie 2005), and Dyna (Eisner, Goldlust, and Smith 2005). It is true that Dyna is quite powerful because it is a full general-purpose declarative programming language, but for that reason it offers an attractive alternative to parsing schemas. On the other hand, schemas do allow formal reasoning about parsers that may be more ﬁne-grained than is possible in Dyna. Surprisingly, work on semiring parsing (Goodman 1998, 1999) is not mentioned. The use of probabilities or weights is generally ignored in this book, even though it enables interesting methods for speeding up parsers such as coarse to ﬁne parsing (Goodman 1997) or generalized A∗ search for parsing (Pauls and Klein 2009). While there is more than enough content in this book, it does not cover the use of parsing schemas in machine translation. In particular, formal properties of schemas might make it easier to describe and implement the integration of language models into parsing algorithms for synchronous context-free grammars (Chiang 2007). Schemas might have much to offer with respect to proving correctness in machine translation decoders. The potential reader for this book is likely to be a parsing enthusiast curious about the power of schemas to represent parsing algorithms succinctly and to prove them correct. They might also be interested in showing relationships between their novel parsing schemas and other well-known parsers, or showing how extensions to existing parsers are well justiﬁed. Dependency parsing enthusiasts who want to wrap their head around the many different parsing algorithms out there might also be interested in the concise description of such parsers.  References Alonso Pardo, Miguel A., David Cabrero Souto, Eric de la Clergerie, and Manuel Vilares Ferro. 1999. Tabular algorithms for TAG parsing. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 150–157, Bergen.  Charniak, Eugene and Mark Johnson. 2001. Edit detection and parsing for transcribed speech. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, Pittsburgh, PA. Chiang, David. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.  883  Computational Linguistics  Volume 37, Number 4  Eisner, Jason, Eric Goldlust, and Noah A. Smith. 2005. Compiling comp ling: Weighted dynamic programming and the Dyna language. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 281–290, Vancouver, Canada. Goodman, Joshua. 1997. Global thresholding and multiple-pass parsing. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing: EMNLP-1997, pages 11–25, Providence, RI. Goodman, Joshua. 1998. Parsing inside-out. Ph.D. thesis, Harvard University, Cambridge, MA. Goodman, Joshua. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605. Graham, Susan L., Michael Harrison, and Walter L. Ruzzo. 1980. An improved context-free recognizer. ACM Transactions on Programming Languages and Systems, 2(3):415–462. Huang, Liang and Haitao Mi. 2010. Efﬁcient incremental decoding for tree-to-string translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, Cambridge, MA. Huang, Liang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077–1086, Uppsala. Kasami, Tadao. 1965. An efﬁcient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA. Liu, Yanhong A. and Scott D. Stoller. 2003. From Datalog rules to efﬁcient programs  with time and space guarantees. In Proceedings of the 5th ACM SIGPLAN International Conference on Principles and Practice of Declarative Programming, pages 172–183, Uppsala, Sweden. McAllester, David. 2002. On the complexity analysis of static analyses. Journal of the Association for Computing Machinery, 49(4):512–537. McDonald, Ryan T., Fernando Pereira, Kiril Ribarov, and Jan Hajicˇ. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the Human Language Technologies and Empirical Methods in Natural Language Processing Conference, pages 523–530, Vancouver, Canada. Pauls, Adam and Dan Klein. 2009. Hierarchical search for parsing. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 557–565, Boulder, CO. Shieber, Stuart M., Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1–2):3–36. Sikkel, Klaas. 1997. Parsing Schemata: A Framework for Speciﬁcation and Analysis of Parsing Algorithms. Springer, Berlin. Villemonte de la Clergerie, E´ ric. 2005. DyALog: a Tabular Logic Programming based environment for NLP. Proceedings of the 2nd International Workshop on Constraint Solving and Language Processing, Barcelona, Spain. Younger, Daniel H. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10:189–208.  Anoop Sarkar is an Associate Professor at Simon Fraser University, Burnaby, BC, Canada, where he co-directs the Natural Language Laboratory (natlang.cs.sfu.ca). His research is focused on statistical parsing and machine translation. His interests also include formal language theory and stochastic grammars, in particular tree automata and tree-adjoining grammars. His e-mail address is anoop@cs.sfu.ca.  
This article proposes ESA, a new unsupervised approach to word segmentation. ESA is an iterative process consisting of three phases: Evaluation, Selection, and Adjustment. In Evaluation, both the certainty and uncertainty of character sequence co-occurrence in corpora are considered as statistical evidence supporting goodness measurement. Additionally, the statistical data of character sequences with various lengths become comparable with each other by using a simple process called Balancing. In Selection, a local maximum strategy is adopted without thresholds, and the strategy can be implemented with dynamic programming. In Adjustment, a part of the statistical data is updated to improve successive results. In our experiment, ESA was evaluated on the SIGHAN Bakeoff-2 data set. The results suggest that ESA is effective on Chinese corpora. It is noteworthy that the F-measures of the results are basically monotone increasing and can rapidly converge to relatively high values. Furthermore, empirical formulae based on the results can be used to predict the parameter in ESA to avoid parameter estimation that is usually time-consuming. 1. Introduction Word segmentation is an important task in natural language processing (NLP) for languages without word delimiters (e.g., Chinese). To date, most existing approaches to Chinese word segmentation (CWS) are supervised. Although supervised approaches ∗ School of Computer Science and Technology, Beijing Institute of Technology, Beijing, 100081 China. E-mail: necrostone@gmail.com. ∗∗ School of Computer Science and Technology, Beijing Institute of Technology, Beijing, 100081 China. † School of Computer Science and Technology, Beijing Institute of Technology, Beijing, 100081 China. ‡ School of Computer Science and Technology, Beijing Institute of Technology, Beijing, 100081 China. E-mail: fxz@bit.edu.cn. Submission received: 8 December 2009; revised submission received: 14 October 2010; accepted for publication: 18 November 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 3  reach higher accuracy than unsupervised ones in many cases, they involve much more human effort. Furthermore, unsupervised approaches are more adaptive to relatively unfamiliar languages for which we do not have enough linguistic knowledge. In addition, unsupervised approaches can cooperate with supervised ones to overcome drawbacks of both. Since Sproat and Shih (1990) introduced mutual information (MI) to word segmentation, some researchers have conducted research on unsupervised approaches to word segmentation (Chang and Su 1997). Peng and Schuurmans (2001) proposed an unsupervised approach based on an improved expectation maximum (EM) learning algorithm and a pruning algorithm based on MI. Their approach outperforms softcounting (Ge, Pratt, and Smyth 1999) that is also based on EM and MI. Non-parametric Bayesian techniques—for example, the Pitman-Yor process (PYP, a generalization of the Dirichlet process, DP) (Pitman and Yor 1997), hierarchical DP (HDP) (Teh et al. 2006; Goldwater, Grifﬁths, and Johnson 2006), hierarchical PYP (HPYP) (Teh 2006a, 2006b), and hierarchical HPYP (HHPYP) (Wood and Teh 2008)—have been introduced to word segmentation. Mochihashi, Yamada, and Ueda (2009) proposed a novel unsupervised approach based on HPYP, and evaluated it on a part of SIGHAN Bakeoff-2 data set (Emerson 2005). Their evaluation results suggested that their approach outperformed the previous ones. Some approaches, such as TONGO (Ando and Lee 2000, 2003) and Voting Experts (Cohen, Heeringa, and Adams 2002; Cohen, Adams, and Heeringa 2007), are based on relatively simple ideas. In most cases, an unsupervised approach can be viewed as a kind of goodness measurement to ﬁnd boundaries between words or ﬁlter words from candidates or both. There are four goodness algorithms reviewed by Zhao and Kit (2008a). The algorithms, including Description Length Gain (DLG) (Kit and Wilks 1999), Accessor Variety (Feng et al. 2004a, 2004b), and Branching Entropy (Tanaka-Ishii 2005; Jin and Tanaka-Ishii 2006), were evaluated on SIGHAN Bakeoff-3 data set (Levow 2006). In this article, we propose ESA, a new unsupervised approach to word segmentation, and demonstrate its effectiveness on Chinese corpora. The approach was motivated by the following considerations: 1. In contrast to the semi-supervised or supervised approaches, we want to ﬁnd an approach which produces acceptable results under harsh conditions. The harsh conditions are lack of prior knowledge, namely, no lexicons, annotated corpora, or linguistic rules. The acceptability involves comparison with the gold standards, which usually means the manually segmented results. 2. In contrast to existing unsupervised approaches, we want to explore the potential of completely unsupervised approaches. Therefore, we try to avoid any manual interference. To avoid manual interference, we need to consider the following issues: 1. Unsupervised approaches usually rely on a maximization strategy or thresholds or both. Approaches adopting the maximization strategy alone can be easily adapted to various contexts with few manual adjustments, whereas approaches using thresholds may have a higher accuracy in some cases. 422  Wang et al.  A New Unsupervised Approach to Word Segmentation  2. Many approaches have constraints on maximum word length. Furthermore, some approaches adopt different strategies for processing words of different lengths. The constraints and the different strategies may improve results on speciﬁc languages, however they reduce the generality of the approaches. 3. Many approaches process characters with different strategies according to different character types. The character types are usually identiﬁed by encoding information. Because encoding information is prior knowledge of speciﬁc languages, a completely unsupervised approach should avoid it as much as possible in order to be applicable under any conditions. ESA is based on a new goodness algorithm that adopts a local maximum strategy and avoids thresholds. ESA has no constraints on maximum word length. In practice, this kind of constraint can have a negative impact on ESA’s segmentation. A simple process called Balancing is introduced to uniformly process words of different lengths. Moreover, ESA uses a self-revision mechanism to improve segmentation accuracy and guarantees convergence after a small number of iterations. In practice, ESA has only one parameter that needs to be conﬁgured, and the parameter can be predicted by empirical formulae proposed in this article. In Section 2, we describe ESA in detail. The SIGHAN Bakeoff-2 archives are available for research on the ofﬁcial Web site and therefore we can easily test ESA on that data. We provide our experimental results and discuss them in Section 3. In Section 4, we compare ESA with other approaches. Finally, we draw our conclusions in Section 5. 2. ESA ESA consists of Evaluation, Selection, and Adjustment as shown in Figure 1, and it is based on two simple ideas: 1. A better result can be produced by combining certainty and uncertainty. The key is how to combine them. 2. A better result can be produced by adopting the self-revised pattern based on an iterative process.  Figure 1 ESA and input/output data. 423  Computational Linguistics  Volume 37, Number 3  The input text can be viewed as a character sequence. A character sequence can be divided into two adjacent subsequences. The certainty mentioned previously means certainty of co-occurrence of adjacent subsequences. And the uncertainty means uncertainty of co-occurrence of adjacent subsequences. For example, suppose there are two character sequences, AB and AC. The occurrence of AB represents the certainty of co-occurrence of A and B, whereas the occurrence of AC represents the uncertainty of co-occurrence of A and B, and vice versa. The two kinds of information are combined to evaluate the segmentation. In other words, the decision of whether to segment a character sequence into two adjacent subsequences or not depends on both certainty and uncertainty. An iterative process can produce better results than a non-iterative scheme (Chang and Su 1997). In fact, the current result can be viewed as prior knowledge to adjust the next one. Devising an unsupervised approach is similar to clarifying how infants segment words without explicit instructions. In particular, infants are able to learn words from various kinds of information such as familiar names (Bortfeld et al. 2005), edges of utterances (Seidl and Johnson 2006), meaning maps (Estes et al. 2007), and auditory forms of words (Swingley 2008). There are a few notable issues: 1. Familiarity (Bortfeld et al. 2005) can be represented by high frequency. Frequent character sequences provide more credibility than infrequent ones. The appearance frequencies are the most important information for word segmentation. 2. The edges of utterances (Seidl and Johnson 2006) can be viewed as natural boundaries. In practice, the boundaries given by punctuation can improve the accuracy of segmentation. However, we think that punctuation should be ignored by completely unsupervised approaches in order to avoid relying on encoding information. 3. Both word lists and statistical data as prior knowledge enable human infants to segment words (Estes et al. 2007). For a completely unsupervised approach, the prior knowledge can be the approach itself and the previous results produced by the approach. 4. The early vocabularies of human infants are based on the sounds of words (Swingley 2008). Some research (Goldwater, Grifﬁths, and Johnson 2006) is based on phonemes, but the input data are still text. Before completely clarifying the mechanism of human learning, we tend to believe that machines can understand symbol sequences with simple logic. 2.1 Evaluation Evaluation is the phase that gives a character sequence or a pair of adjacent subsequences a goodness value according to statistical information. There are three issues to be settled: 1. What are the character sequence and the pair of adjacent subsequences that can be evaluated? 2. What is the necessary statistical information and how do we get it? 3. How do we calculate the goodness? 424  Wang et al.  A New Unsupervised Approach to Word Segmentation  2.1.1  The  Target  of  Evaluation.  A  character  sequence  contains  (N+1)×N 2  subsequences,  where N is the number of characters in the character sequence. These subsequences  are the targets to be evaluated.  A character sequence can be divided into various pairs of adjacent subsequences as  shown in Figure 2. A pair of adjacent subsequences contains a gap that is the potential  boundary between the two subsequences. Every character sequence has an individual  goodness value (IV). Every pair of adjacent subsequences has a combined goodness  value (CV) based on the IV of each subsequence and the goodness value of the gap  (LRV). IV and LRV indicate certainty and uncertainty of co-occurrence, respectively.  Therefore, CV is the combination of certainty and uncertainty. IV is the base of CV, and  LRV serves as the modiﬁer of the base.  2.1.2 The Information Needed. The information mentioned here is the statistical information that can be extracted from corpora. There are two basic quantities to be directly measured:  1. The frequency of a character sequence. For example, if the character sequence is ABAB: the frequencies of A, B, and AB are all 2; and the frequencies of ABAB, ABA, BAB, and BA are all 1. 2. The number of character sequences of the same length. For example, if the character sequence is ABC: the sequences of length 1 are A, B, and C; the sequences of length 2 are AB and BC; and the sequence of length 3 is ABC itself. Therefore, the number of the sequences of length 1, 2, and 3 are 3, 2, and 1, respectively.  Furthermore, there are three other quantities that can be calculated according to those just mentioned:  1. The average frequency of character sequences of the same length. For example, there are only two sequences of length 1: A and B. The frequencies of A and B are 2 and 8, respectively. Therefore, the arithmetic mean of frequencies of A and B is 5. In other words, the average frequency of character sequences of length 1 is 5. 2. The entropy of Sequence Plus One (SP1) of a character sequence. For example, consider the character sequence X. The SP1 of X is a set, and each member of the SP1 contains X and one character. In detail, the entropy  Figure 2 A character sequence and its subsequence pairs. 425  Computational Linguistics  Volume 37, Number 3  mentioned here is denoted by H(SP1L(X)) and H(SP1R(X)), where SP1L (SP1 Left) and SP1R (SP1 Right) are two subsets of SP1. In other words, SP1L(X) and SP1R(X) mean that the left side of X is attached with one character and the right side of X is attached with one character, respectively. For example, there are several character sequences: BC, ABC, BBC, BCD, and BCB. ABC and BBC are the members of SP1L(BC), whereas BCD and BCB are the members of SP1R(BC). Therefore, H(SP1L(BC)) is calculated according to the frequencies of ABC, BBC, and other members of SP1L(BC), whereas H(SP1R(BC)) is calculated according to the frequencies of BCD, BCB, and other members of SP1R(BC). The formal descriptions of SP1L and SP1R are  SP1L(x) = {s|s = c · x, s ∈ S, x ∈ S, c ∈ Σ}  (1)  and  SP1R(x) = {s|s = x · c, s ∈ S, x ∈ S, c ∈ Σ}  (2)  respectively. The symbol · denotes the attachment operator; s denotes a subsequence of the character sequence S; c denotes a character in the alphabet Σ. In fact, x is the largest proper subsequence of s. 3. The average entropies of SP1s of character sequences of the same length. The SP1s refer to both SP1Ls and SP1Rs. For example, there are only three character sequences of length 2: AB, BC, and CD. Therefore, the sum of H(SP1L(AB)), H(SP1L(BC)), and H(SP1L(CD)) is the numerator of the arithmetic mean, and the denominator is 3. The arithmetic mean is the average entropies of SP1Ls of AB, BC, and CD.  We directly use the preﬁx tree (trie) (Fredkin 1960) to record the information. Some other data structures can also be used (Morrison 1968; McCreight 1976; Manber and Myers 1990).  2.1.3 The Calculation of Goodness. The IV of a character sequence is formulated as  IV(x) = ( Fx )L  (3)  FML  The superscript L is the exponent; x is the character sequence to be evaluated; L is the length of x. F denotes the frequency of a character sequence, and therefore Fx is that of x; FM denotes the average frequency of character sequences of the same length, and therefore FML is that of length L. F can be viewed as a local variable, and FM brings global effects to the formula. By the division in IV, the character sequences of different lengths become comparable with each other. The division is based on a pattern called Balancing, which means keeping balance between the local and global effects. Furthermore, FM is formulated as  FML  =  
∗∗ Baskin School of Engineering, University of California, 1156 High Street, SOE-3, Santa Cruz, CA 95064, E-mail: maw@soe.ucsc.edu. Submission received: 20 January 2009; revised submission received: 18 October 2010; accepted for publication: 30 November 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 3  communicative intentions to try to satisfy multiple goals simultaneously (Pollack 1991; Stone and Webber 1998; Jordan 2000), such as projecting a speciﬁc image to the hearer while communicating information and minimizing communicative effort (Clark and Brennan 1991; Brennan and Clark 1996). The combination of these pragmatic effects results in the large range of linguistic variation observed between individual speakers (Biber 1988). Much of the research on generating utterances that manifest different linguistic styles has focused on text generation applications such as journalistic writing or instruction manuals (Hovy 1988; Scott and de Souza 1990; Paris and Scott 1994; Green and DiMarco 1996; Bouayad-Agha, Scott, and Power 2000; Power, Scott, and Bouayad-Agha 2003; Inkpen and Hirst 2004). Recent research in language generation for dialogue applications has also begun to take linguistic variation into account, developing algorithms to modify the system’s linguistic style based on either the user’s linguistic style, or other factors such as the user’s emotional state, her personality, or considerations of politeness strategies (Walker, Cahn, and Whittaker 1997; Lester, Towns, and Fitzgerald 1999; Lester, Stone, and Stelling 1999; Andre´ et al. 2000; Cassell and Bickmore 2003; Piwek 2003). There is growing evidence that dialogue systems such as intelligent tutoring systems are more effective if they can generate a range of different types of stylistic linguistic variation (Litman and Forbes-Riley 2004, 2006; Porayska-Pomsta and Mellish 2004; Wang et al. 2005; McQuiggan, Mott, and Lester 2008; Tapus and Mataric 2008). Most of this work uses either templates or handcrafted rules to generate utterances. This guarantees high quality, natural outputs, which is useful for demonstrating the utility of stylistic variation. Handcrafted approaches mean that utterances have to be constructed by hand for each new application, however, leading to problems of portability and scalability (Rambow, Rogati, and Walker 2001). Statistical natural language generation (SNLG) has the potential to address such scalability issues by relying on annotated data rather than manual parameter tuning. It also offers the promise of techniques for producing continuous stylistic variation over multiple stylistic factors by automatically learning a model of the relation between stylistic factors and properties (parameters) of generated utterances (Paiva and Evans 2004, 2005). It is difﬁcult to produce such continuous variation over multiple factors with a rule-based or template-based approach (but see Bouayad-Agha, Scott, and Power 2000). Moreover, to date, no one has shown that humans correctly perceive the generated variation as the system intended, nor has anyone shown that an SNLG approach can produce outputs that are natural enough to be used in dialogue applications such as intelligent tutoring systems, interactive drama systems, and conversational agents, where some types of stylistic variation have already been shown to be useful. In previous work, we argue that the Big Five model of personality provides a useful framework for modeling some types of stylistic linguistic variation. This model of human personality has become widely accepted in psychology over the last 50 years (Funder 1997). Table 1 tabulates each Big Five trait along with some of the important trait adjectives associated with the extremes of each trait. We believe that these trait adjectives provide an intuitive, meaningful deﬁnition of linguistic style. In previous work we describe a rule-based version of PERSONAGE, which here we will refer to as PERSONAGE-RB (Mairesse and Walker 2007; Mairesse 2008). In PERSONAGE-RB, generation parameters are implemented, and their values are set based on correlations between linguistic cues and the Big Five traits that have been systematically documented in the psychology literature (Scherer 1979; Furnham 1990; Pennebaker and King 1999; Mehl, Gosling, and Pennebaker 2006). For example, parameters for the extraversion 456  Mairesse and Walker  Trainable Generation of Personality Traits  Table 1 Example adjectives associated with the extremes of all Big Five traits.  High  Low  Extraversion  warm, gregarious, assertive, sociable, excitement seeking, active, spontaneous, optimistic, talkative  shy, quiet, reserved, passive, solitary, moody, joyless  Emotional stability  calm, even-tempered, reliable, peaceful, conﬁdent  neurotic, anxious, depressed, self-conscious, oversensitive, vulnerable  Agreeableness  trustworthy, friendly, considerate, unfriendly, selﬁsh, suspicious,  generous, helpful, altruistic  uncooperative, malicious  Conscientiousness  competent, disciplined, dutiful, achievement striving, deliberate, careful, orderly  disorganized, impulsive, unreliable, careless, forgetful  Openness to experience creative, intellectual, imaginative, narrow-minded, conservative,  curious, cultured, complex  ignorant, simple  trait include verbosity, sentence length, and the production of positive content. We showed experimentally that humans perceive utterances generated by PERSONAGE-RB as conveying the extremes of all Big Five traits (e.g., neuroticism (low) vs. emotionally stable (high), see Table 1). Our evaluation uses a validated perceptual questionnaire from the personality psychology literature (Gosling, Rentfrow, and Swann 2003). PERSONAGE-RB only generates 10 discrete personalities emphasizing either the high or the low end of one trait; however, psychologists measure personality traits on continuous scales (Norman 1963; Goldberg 1990; Marcus et al. 2006), and human language simultaneously manifests multiple personality traits. Some computational applications may require more than a small set of personality types, which suggests that systems adapting their linguistic style to the user would beneﬁt from ﬁne-grained personality models. We believe that the only way to robustly and efﬁciently learn such ﬁne-grained variation is to model personality as a continuous variable, rather than using arbitrary discrete personality classes. Personality generation models should thus learn to map continuous target personality scores to discrete utterances. In order to achieve this, the handcrafted rule-based approach would require the manual examination of psycholinguistic ﬁndings, followed by testing in the application domain, to determine the appropriate range for each parameter value. Extending this approach to continuous variation that can project multiple traits simultaneously does not appear to be tractable. The objective of this article is to present and evaluate a language generator that is trained with a novel method, and which learns to generate stylistic variation expressing multiple continuous stylistic dimensions (in this case multiple personality traits). Before presenting our method, let us review existing paradigms for statistical language generation. 1.1 Previous Statistical Language Generation Methods Previous work on SNLG has focused on three main approaches: (a) learning statistical language models (SLMs) from corpora in order to rerank a set of pre-generated  457  Computational Linguistics  Volume 37, Number 3  utterances; (b) learning utterance reranking models from user feedback rather than corpora; and (c) learning generation parameters directly from data. The ﬁrst approach has used SLMs to rerank a large set of candidate utterances, and focused on grammaticality and naturalness (Bangalore and Rambow 2000; LangkildeGeary 2002; Chambers and Allen 2004; Nakatsu and White 2006). The seminal work of Langkilde and Knight (1998) in this area showed that high quality paraphrases can be generated from an underspeciﬁed representation of meaning, by ﬁrst applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. The SLM scoring gives a low score (rank) to any ungrammatical output produced by the rule-based generator. We will refer to this as the overgenerate and scoring (OS) approach. In a novel twist, Isard, Brockmann, and Oberlander (2006) applied this method to the generation of dialogues in which conversational agents with different personalities discuss movies. The SLM ranking model blends SLMs from blogs annotated with Big Five personality traits with SLMs from Switchboard, a much larger conversational dialogue corpus. Their CRAG-2 generator discretizes the blog personality ratings into three groups (low, medium, and high), and models personality with three distinct SLM models for each trait. Each model estimates the likelihood of the utterance given the personality type. A cache model based on recently used linguistic forms can also be combined, in order to model recency effects and alignment (Pickering and Garrod 2004). This approach was integrated into a demonstrator, but it does not generate continuous variation (discretization of personality ratings), and to our knowledge it has never been evaluated to test whether the variation produced is perceivable by users. A second approach to SNLG is a variant of the OS technique that trains the scoring phase to replicate human judgments rather than relying on the probabilities or frequencies of a SLM. This approach typically uses higher-level syntactic, semantic, and discourse features rather than only n-grams, with typical results demonstrating that the performance of the scoring models approaches the gold-standard human ranking with a relatively small training set (Rambow, Rogati, and Walker 2001; Stent and Guo 2005; Nakatsu and White 2006). An advantage of this approach is that human judgments can be based on any aspect of the output, such as stylistic differences in politeness or personality. Walker et al. (2007) showed that this technique can be used to model individual preferences in rhetorical structure, syntactic form, and content ordering. In previous work, we also applied this method to scoring randomly produced outputs of PERSONAGE (Mairesse 2008). The resulting statistical generator is referred to as PERSONAGE-OS. We randomly varied PERSONAGE’s non-deterministic decisions points to generate a large number of paraphrases. We then computed post hoc features consisting of the actual generation decisions, surface word n-grams, and contentanalysis features from the Linguistic Inquiry and Word Count (LIWC) tool (Pennebaker, Francis, and Booth 2001) and the MRC psycholinguistic database (Coltheart 1981). Example content-analysis features include the ratio of words related to positive emotions (e.g., good), social interactions (e.g., pal), or the average frequency of use of each word. Scoring models trained on personality ratings of random utterances (in-domain data) outperformed the mean value baseline for all Big Five traits, with the best results for agreeableness, extraversion, and emotional stability. The models for those traits predict the ratings of unseen utterances with correlations of r = .52, r = .37, and r = .29, respectively. We also trained models on out-of-domain data, that is, 96 personality-annotated conversation extracts (without any generation decision features). Results show that the 458  Mairesse and Walker  Trainable Generation of Personality Traits  out-of-domain models perform worse for all traits, only outperforming the baseline for agreeableness and conscientiousness. We also explored several hybrid methods for training that mix and blend data from different sources. Inspired by recent work on domain adaptation, we tested whether the performance of the out-of-domain models can be improved when training includes a small amount of data from the target domain, by applying the method of Daume´ (2007). Whereas adding out-of-domain data improved performance for some traits, we ﬁnd that adding a single domain feature performs as well as Daume’s method. The results showed that mixing randomly generated in-domain utterances with rule-based in-domain utterances improves performance; the rule-based utterances provide a way to incorporate knowledge from the personality psychology literature into an SNLG approach. Thus, personality scoring models can be effective, although the computational cost of the OS approach remains a major drawback. The third SNLG approach estimates the generation parameters directly from data, without any overgeneration phase. If the language generator is constrained to be a generative SLM, the parameters can then be learned through standard maximum-likelihood estimation. Whereas n-gram SLMs can only model local linguistic phenomena, Belz showed that a context-free grammar (PCFG) can successfully model individual differences in the production of weather reports (Belz 2005, 2008). This method provides a principled way to produce utterances matching the linguistic style of a speciﬁc corpus (e.g., of an individual author) without any overgeneration phase. However, standard PCFG generation methods require a treebank-annotated corpus, and they cannot model context-dependent generation decisions, such as the control of sentence length or the generation of referring expressions. Paiva and Evans (2005) adopt a more general framework by learning a regression model mapping generation decisions to stylistic dimensions extracted from a corpus, independently of the language generation mechanism. Factors are identiﬁed by applying factor analysis to a corpus exhibiting stylistic variation, and expressed as a linear combination of linguistic features (Biber 1988). Textual outputs are generated with a rule-based generator in the target domain that is allowed to randomly vary the generation parameters, while logging the parameter settings corresponding to each output. Then the same factors found in the original corpus are measured in the random outputs, and linear regression is applied to learn which generation parameters predict the factor measurements. The generation parameters can then be manipulated to hit multiple stylistic targets on a continuous scale (because factors are measured continuously) by searching for the parameter setting yielding the target stylistic scores according to the linear models. The generator of Paiva and Evans, trained in this way, can reproduce intended factor levels across several factors, such as sentence length and type of referring expression, thus modeling the stylistic variation as measured in the original corpus. Again, it has not been shown that humans perceive the stylistic differences that this approach produces. 1.2 Parameter Estimation Models In the previous sections, we referred to two existing methods for controlling the parameters of PERSONAGE to produce stylistic variation: PERSONAGE-RB uses handcrafted generation parameter values for every target style of interest, and PERSONAGE-OS uses a statistical rescoring model to rerank a set of randomly generated utterances. The following sections develop and evaluate PERSONAGE-PE, a trainable generator 459  Computational Linguistics  Volume 37, Number 3  which uses a direct generation method inspired by Paiva and Evans’ approach (2005), to produce the stylistic variation found in personality traits, without any overgeneration phase. Whereas Paiva and Evans learn models predicting the target stylistic scores from the generation parameters, we train parameter estimation models (PE) to estimate the optimal generation parameters given target personality scores, which are then used by the base generator to produce the output utterance. As parameter estimation models learn the reverse relationship to Paiva and Evans’ regression models, there is no need to search for the optimal generation parameter values at generation time. We evaluate the PE approach using the PERSONAGE base generator, whose parameters, architecture, and capabilities are described in Section 2. Our experimental method is described in Section 3, together with an analysis of the data required to train our models. Section 4 analyzes some of the learned models, and evaluates the quality of the generated outputs using human judges, to compare our approach with the handcrafted PERSONAGE-RB generator of our previous work. Finally, Section 5 discusses the implications of our results and suggests many areas of future work. This article makes several contributions. First, we present a novel method for training an SNLG system that can produce multiple stylistic dimensions simultaneously, over continuous dimensions, without overgeneration or search. In order to evaluate our approach, we present the ﬁrst empirical results showing that humans correctly perceive the stylistic variations (of any kind based on any utterance dimensions) that a statistical language generator intended to produce. Our evaluation of PERSONAGE-RB is the only other result that we know of for non-statistical generators (Mairesse and Walker 2007). Our experiments show that PERSONAGE-PE produces utterances perceived by humans as portraying different personalities, while maintaining a reasonable naturalness level (4.0 on a scale of 1 to 7). We do not know of any other human evaluation of an SNLG system that produces stylistic variation. Additionally, we test a wide range of machine learning algorithms to determine the best model for each generation decision in Section 4.1. We are not aware of any other work on SNLG to test such a wide range of algorithms. 2. The PERSONAGE Base Generator The architecture of the PERSONAGE base generator is shown in Figure 1; it is discussed in detail in (Mairesse 2008) and (Mairesse and Walker 2010), and is only brieﬂy summarized here. The PERSONAGE architecture (Figure 1) builds on a standard natural language generation (NLG) pipeline architecture as described in Reiter and Dale (2000), Kittredge, Korelsky, and Rambow (1991), and Walker and Rambow (2002). We assume that the inputs to the generator are (1) a high-level communicative goal; (2) a content pool that can be used to achieve that goal, and (3) a set of generation parameter values. In a dialogue system, the communicative goal is provided by the dialogue manager. Two types of communicative goals are currently supported by PERSONAGE: recommendation and comparison of restaurants. PERSONAGE’s content pool is based on a database of restaurants in New York City, with associated scalar values representing evaluative ratings for six attributes: food quality, service, cuisine, location, price, and atmosphere. The ﬁrst component of the architecture shown in Figure 1 is the content planner, which speciﬁes the structure of the information to be conveyed. The resulting content plan tree is then processed by the sentence planner, which selects syntactic templates for expressing individual propositions, and aggregates them to produce the utterance’s 460  Mairesse and Walker  Trainable Generation of Personality Traits  Figure 1 The architecture of the PERSONAGE base generator. full syntactic structure. The pragmatic marker insertion component then modiﬁes the syntactic structure locally to produce various pragmatic effects, depending on the markers’ insertion constraints. The lexical choice component selects the most appropriate lexeme for each content word, given the lexical selection parameters. Finally, the RealPro surface realizer (Lavoie and Rambow 1997) converts the ﬁnal syntactic structure into a string by applying surface grammatical rules, such as morphological inﬂection and function word insertion. When integrated into a dialogue system, the output of the realizer is annotated for prosodic information by the prosody assigner before being sent to the text-to-speech engine to be converted into an acoustic signal. PERSONAGE does not currently express personality through prosody, although there are studies that could be used to develop such parameters (Scherer 1979; Furnham 1990). Figure 1 also indicates the modules in which PERSONAGE introduces parameters to produce and control personality-based linguistic variation. The generation parameters are shown in Table 2 and organized into blocks that correspond to the modules of the architecture in Figure 1; compare Figure 1 to Table 2. As mentioned previously, all of PERSONAGE’s parameters are motivated by ﬁndings in the personality psychology literature. The mapping from a ﬁnding to parameters represents a set of hypotheses about how the ﬁnding can be implemented, however, as discussed in more detail in Mairesse and Walker (2007) and Mairesse (2008). Table 2 includes a description for each parameter that explains what the parameter does and often includes an example. For example, there are 12 content planning parameters shown in the ﬁrst block of Table 2; these control aspects of utterances such as their verbosity, rhetorical structure, content selection parameters such as positive content, and the level of redundancy and restatement (Walker 1993). Table 2 also includes 13 pragmatic marker parameters, which we believe to be completely novel. These include the introduction of HEDGES and TAG QUESTIONS. We are not aware of any other generators that produce the range of pragmatic variation illustrated here. Note also that 461  Computational Linguistics  Volume 37, Number 3  Table 2 PERSONAGE’s generation parameters. The Type column indicates whether the stylistic effect is modeled as a continuous (C) or binary (B) parameter (i.e., resulting in continuous or binary parameter estimation models). Aggregation operation parameters are selection probabilities (C).  Parameter  Type  Description  Content planning:  VERBOSITY  C Control the number of propositions in the utterance  RESTATEMENTS  C Paraphrase an existing proposition, e.g., ‘Chanpen Thai has  great service, it has fantastic waiters’  REPETITIONS  C Repeat an existing proposition  CONTENT POLARITY  C Control the polarity of the propositions expressed,  i.e., referring to negative or positive attributes  REPETITIONS POLARITY  C Control the polarity of the restated propositions  CONCESSIONS  Emphasize one attribute over another, e.g., ‘even if  Chanpen Thai has great food, it has bad service’  CONCESSIONS POLARITY  C Determine whether positive or negative attributes are  emphasized  POLARIZATION  C Control whether the expressed polarity is neutral  or extreme  POSITIVE CONTENT FIRST  C Determine whether positive propositions—including  the claim—are uttered ﬁrst  REQUEST CONFIRMATION  B Begin the utterance with a conﬁrmation of the restaurant’s  name, e.g., ‘did you say Chanpen Thai?’  INITIAL REJECTION  B Begin the utterance with a mild rejection, e.g., ‘I’m not sure’  COMPETENCE MITIGATION B Express the speaker’s negative appraisal of the hearer’s  request, e.g., ‘everybody knows that . . . ’  Syntactic template selection:  SELF-REFERENCES  C  SYNTACTIC COMPLEXITY  C  TEMPLATE POLARITY  C  Control the number of ﬁrst person pronouns Control the syntactic complexity (syntactic embedding) Control the connotation of the claim, i.e., whether positive or negative affect is expressed  Aggregation operations: PERIOD RELATIVE CLAUSE WITH CUE WORD CONJUNCTION MERGE ALSO CUE WORD CONTRAST - CUE WORD JUSTIFY - CUE WORD CONCEDE - CUE WORD  C Leave two propositions in their own sentences, e.g., ‘Chanpen Thai has great service. It has nice decor.’ C Aggregate propositions with a relative clause, e.g., ‘Chanpen Thai, which has great service, has nice decor’ C Aggregate propositions using with, e.g., ‘Chanpen Thai has great service, with nice decor’ C Join two propositions using a conjunction, or a comma if more than two propositions C Merge the subject and verb of two propositions, e.g., ‘Chanpen Thai has great service and nice decor’ C Join two propositions using also, e.g.,’Chanpen Thai has great service, also it has nice decor’ C Contrast two propositions using while, but, however, on the other hand, e.g., ‘While Chanpen Thai has great service, it has bad decor’, ‘Chanpen Thai has great service, but it has bad decor’ C Justify a proposition using because, since, so, e.g., ‘Chanpen Thai is the best, because it has great service’ C Concede a proposition using although, even if, but/though, e.g., ‘Although Chanpen Thai has great service, it has bad decor’, ‘Chanpen Thai has great service, but it has bad decor though’  462  Mairesse and Walker  Trainable Generation of Personality Traits  Table 2 (continued) Parameter MERGE WITH COMMA OBJECT ELLIPSIS  Type  Description  C Restate a proposition by repeating only the object, e.g., ‘Chanpen Thai has great service, nice waiters’ C Restate a proposition after replacing its object by an ellipsis, e.g., ‘Chanpen Thai has . . . , it has great service’  Pragmatic markers: SUBJECT IMPLICITNESS STUTTERING PRONOMINALIZATION NEGATION SOFTENER HEDGES EMPHASIZER HEDGES ACKNOWLEDGMENTS FILLED PAUSES EXCLAMATION EXPLETIVES NEAR EXPLETIVES TAG QUESTION IN-GROUP MARKER  C Make the restaurant implicit by moving the attribute to the subject, e.g., ‘the service is great’ C Duplicate the ﬁrst letters of a restaurant’s name, e.g., ‘Ch-ch-anpen Thai is the best’ C Replace occurrences of the restaurant’s name by pronouns B Negate a verb by replacing its modiﬁer by its antonym, e.g., ‘Chanpen Thai doesn’t have bad service’ B Insert syntactic elements (sort of, kind of, somewhat, quite, around, rather, I think that, it seems that, it seems to me that) to mitigate the strength of a proposition, e.g., ‘Chanpen Thai has kind of great service’ or ‘It seems to me that Chanpen Thai has rather great service’ B Insert syntactic elements (really, basically, actually, just) to strengthen a proposition, e.g., ‘Chanpen Thai has really great service’ or ‘Basically, Chanpen Thai just has great service’ B Insert an initial back-channel (yeah, right, ok, I see, oh, well), e.g., ‘Well, Chanpen Thai has great service’ B Insert syntactic elements expressing hesitancy (like, I mean, err, mmhm, you know), e.g., ‘I mean, Chanpen Thai has great service, you know’ or ‘Err... Chanpen Thai has, like, great service’ B Insert an exclamation mark, e.g., ‘Chanpen Thai has great service!’ B Insert a swear word, e.g., ‘the service is damn great’ B Insert a near-swear word, e.g., ‘the service is darn great’ B Insert a tag question, e.g., ‘the service is great, isn’t it?’ B Refer to the hearer as a member of the same social group, e.g., pal, mate, and buddy  Lexical choice:  LEXICON FREQUENCY  C Control the average frequency of use of each content word,  according to BNC frequency counts  LEXICON WORD LENGTH C Control the average number of letters of each content word  VERB STRENGTH  C Control the strength of the verbs, e.g., ‘I would suggest’  vs. ‘I would recommend’  Figure 1 indicates that the lexical choice parameters in Table 2 make use of multiple online lexical resources such as WordNet and VERBOCEAN to support lexical variation. The LEXICAL FREQUENCY parameter is calculated with respect to a corpus. Furthermore, whereas some parameters primarily have a linear effect on an utterance (e.g., verbosity), other parameters are highly non-linear (e.g., the effect of inserting two expletives rather than one is not as strong as the effect of inserting one expletive rather than none). Parameters are therefore modeled as having either continuous (C) or binary (B) values, as illustrated in column Type of Table 2. The models for continuous 463  Computational Linguistics  Volume 37, Number 3  and binary parameters are trained using different algorithms. Section 3 will provide examples of learned models of both types. In addition, because generation decisions can be non-deterministic, some continuous parameter values are generation decision probabilities; for example, the input to aggregation parameters such as CONJUNCTION is the probability that the aggregation operation is selected to combine any pair of propositions in the utterance (e.g., CONJUNCTION aggregates two propositions with the conjunction and). If the propositions cannot be aggregated because of syntactic constraints, another aggregation operation is sampled until the aggregation is successful. Complete details on the implementation of individual parameters can be found in Mairesse (2008) and Mairesse and Walker (2010). To make PERSONAGE as domain-independent as possible, the input parameter values are normalized between 0 and 1 for continuous parameters, and to 0 or 1 for binary parameters. For example, a VERBOSITY parameter of 1 maximizes the utterance’s verbosity given the input, regardless of the actual number of propositions expressed. In order to ensure naturalness over the full parameter range, the maximum value of some continuous parameters is associated with an input-independent threshold (e.g., there cannot be more than two repeated propositions per utterance). Although the goal of the base generator is to satisfy its input parameters, it cannot guarantee that all input parameter values will be reﬂected in the utterance due to constraints on the input content plan and other parameters. A consequence is that non-deterministic decision points are introduced to satisfy these naturalness constraints (e.g., if too many pragmatic marker parameters are enabled, only a random subset will appear in the utterance). Therefore, the only assumption we make regarding the impact of parameter values on the generation process is that they affect the likelihood of observing their intended effect over a large set of utterances.  3. Generation of Personality through Data-Driven Parameter Estimation Whereas PERSONAGE-RB uses handcrafted parameter settings to convey different personality traits, PERSONAGE-PE relies on parameter estimation models to estimate the parameter values in Table 2 from target personality scores. At training time, our method requires the following steps: 1. Use a base generator to produce multiple utterances by randomly varying its parameters (see Section 3.1). 2. Ask human subjects to evaluate (rate) the personality/style of each utterance. 3. Train statistical models predicting the parameter values from the personality ratings (see Section 4.1). 4. Select the best model for each parameter via cross-validation (see Section 4.2). At generation time, the models are used to predict the optimal set of generation parameters given a set of target personality scores, and the base generator is called once with the predicted parameter values. The architecture for the PE method is shown in Figure 2. 464  Mairesse and Walker  Trainable Generation of Personality Traits  Figure 2 PERSONAGE-PE’s parameter estimation framework. In contrast with the overgenerate–score (OS) method discussed in Section 1.1, parameter estimation models predict generation decisions directly from input personality scores, in the spirit of the approach of Paiva and Evans (2005). However, whereas Paiva and Evans’ approach searches for the generation decisions that will yield the optimal target scores according to their model, our PE method does not involve any search, as generation decisions are assumed to be conditionally independent given the target personality, and treated as dependent variables in individual models. This section further details the steps required for training parameter estimation models. We ﬁrst explain in Section 3.1 how we collect the judge’s ratings for our training set. Then Section 3.2 analyzes the coverage and naturalness of the collected data. Finally, Section 3.3 describes how the models are trained. 3.1 Collecting Judgments of Random Sample In order to train the parameter estimation models, the ﬁrst step is to collect a data set mapping generation decisions to personality ratings. This involves the following substeps: 1. Generate a sample of random utterances that produces examples covering the full range of all of the 67 PERSONAGE parameters as shown in Table 2. 2. Log the generation decisions that were made to produce each utterance. 3. Judges rate the random sample with a standard personality test shown in Figure 3, based on Gosling, Rentfrow, and Swann (2003). This results in each utterance in the sample being labelled with ﬁve scalar values, one for each of the Big Five traits. To be the basis for training a high performing statistical generator, the random sample must satisfy two properties. First, it must cover the full range of scalar values for each Big Five trait or there will not be enough training data to learn how to produce utterances manifesting those values. Second, the randomly produced utterances must be natural enough to produce stable personality judgments. The only way to verify that the random sample satisﬁes these properties is by ﬁrst generating the random sample and then analyzing the judge’s ratings. We generated 160 random utterances 465  Computational Linguistics  Volume 37, Number 3  Figure 3 The Ten Item Personality Inventory used in our experiments to calculate values for the Big Five traits, as modiﬁed for our experimental setting. to constitute our random sample. Table 3 shows examples of random utterances and the scalar ratings for each trait that result from the judgment collection process. A major advantage of the Big Five framework is that it offers standard validated questionnaires (John, Donahue, and Kentle 1991; Costa and McCrae 1992; Gosling, Rentfrow, and Swann 2003). Figure 3 shows the Ten Item Personality Inventory (TIPI) that we used to collect the personality judgments (Gosling, Rentfrow, and Swann 2003), adapted to our domain and task. The TIPI produces a scalar rating for each of the Big Five traits ranging from 1 (e.g., highly neurotic) to 7 (e.g., very stable), and it was shown to correlate well with longer questionnaires such as the Big Five Inventory, with convergent correlations of .87, .70, .75, .81, and .65 for extraversion, emotional stability, agreeableness, conscientiousness, and openness to experience, respectively (Gosling, Rentfrow, and Swann 2003). Although the TIPI has mostly been used as a self-report measure of personality, it has also been used to assess personality perceptions of observers, for example, based on short social interactions (Srivastava, Guglielmo, and Beer 2010) or social networking Web sites (Gosling, Gaddis, and Vazire 2007). The judges in our experiment were researchers and postgraduate students in psychology, history, and anthropology who were familiar with the Big Five trait theory, but not with natural language generation. They were all native speakers of English. As illustrated in Figure 3, the judges were asked to rate each utterance in the random sample using the TIPI scale. They were instructed to rate the utterance as if it had been uttered by a friend responding in a dialogue to a request to recommend restaurants. Each judge rated the same sets of utterances corresponding to 20 communicative goals, 16 utterances per goal, one set at a time. The order of the sets and the order of the utterances within each set were both randomized. The judges were asked to read all the utterances in a set 466  Mairesse and Walker  Trainable Generation of Personality Traits  Table 3 Example outputs of PERSONAGE with random parameter settings (random utterances), and scalar personality trait values after collection of TIPI judgments. Extra = extraversion, ems = emotional stability, agree = agreeableness, consc = conscientiousness, and open = openness to experience.  # Communicative goal 1.a compare (Kin Khao, Tossed) 1.b compare (Kin Khao, Tossed) 2.a compare (Le Rivage, Pintaile’s Pizza) 2.b compare (Le Rivage, Pintaile’s Pizza) 3.a recommend (Chimi-churri Grill) 3.b recommend (Chimi-churri Grill) 4.a recommend (Cent anni) 4.b recommend (Cent anni)  PERSONAGE’s output  Ratings  Kin Khao and Tossed are bloody outstanding. Kin Khao just has rude staff. Tossed features sort of unmannered waiters, even if the food is somewhat quite adequate.  extra=4.75 ems=6.00 agree=3.75 consc=4.00 open=3.25  Err... I am not really sure. Tossed offers kind of decent food. Mmhm... however, Kin Khao, which has quite ad-ad-adequate food, is a thai place. You would probably enjoy these restaurants.  extra=4.00 ems=4.00 agree=5.50 consc=4.75 open=5.00  Ok, right, Pintaile’s Pizza is rather cheap. The atmosphere is bad, isn’t it? I guess Le Rivage is in Manhattan, also its price is... it’s expensive, you know.  extra=1.75 ems=3.50 agree=5.25 consc=4.00 open=2.75  Yeah, I thought everybody knew that Le Rivage’s price is around 40 dollars, it’s located in Manhattan with kind of poor atmosphere, Pintaile’s Pizza is located in the Upper East Side and its price is 14 dollars. They’re... I would advise these restaurants, alright?  extra=5.00 ems=4.25 agree=4.25 consc=5.75 open=3.00  Obviously, oh God Chimichurri Grill has like, quite adequate food, so it’s the only re-restaurant I would consider, and it’s a latin american place and located in Midtown West with ra-ra-rather friendly staff.  extra=4.50 ems=3.75 agree=5.25 consc=4.00 open=4.50  I see, oh Chimichurri Grill is a latin american place with sort of poor atmosphere. Although it doesn’t have rather nasty food, its price is 41 dollars. I suspect it’s kind of alright.  extra=2.50 ems=4.50 agree=3.50 consc=4.75 open=4.25  Did you say Ce-Cent’anni? I see, I mean, I would consider it because it has friendly staff and tasty food, you know buddy.  extra=4.75 ems=5.00 agree=6.25 consc=6.25 open=5.25  I am not sure. Cent’anni is... it’s located in Manhattan, also the atmosphere is somewhat bloody poor, but it features tasty food though. Actually, this eating house, which provides quite acceptable service, is an italian restaurant. It’s sort of the best eating place of its kind.  extra=4.25 ems=4.50 agree=4.25 consc=4.25 open=5.75  467  Computational Linguistics  Volume 37, Number 3  before rating them. Eight utterances out of sixteen were randomly generated for each communicative goal. The remaining utterances were generated using the handcrafted parameter settings of PERSONAGE-RB for each end of each Big Five trait (Mairesse and Walker 2007). The rule-based utterances are used as a comparison point, not for training the models. The same methodology was used to collect additional extraversion ratings for another set of 160 random and 80 rule-based utterances in a separate experiment, resulting in 320 random and 240 rule-based utterances for that trait, and 160 random utterances and 40 rule-based utterances for each of the other four traits. Examples of the resulting scalar ratings are shown in Table 3. The judges also evaluated the naturalness of each utterance on the same scale. 3.2 Generation Range and Naturalness Analysis of the collected ratings of the random utterances shows that 67.8% of the utterances were rated as natural (rating above or equal to 4), with an average naturalness rating of 4.38 out of 7. Figure 4 shows the distributions of openness to experience and naturalness ratings. Figure 4a illustrates that most randomly generated utterances are not perceived as projecting an extreme personality. Table 4 examines whether randomly generated utterances can hit the extreme ends of each trait scale by tabulating the most extreme ratings obtained from the 8 random utterances generated for each communicative goal with the ratings of the rule-based utterance generated from the same goal. This comparison provides useful information regarding (a) the potential of data-driven models to outperform handcrafted methods, and (b) whether our training corpus is large enough to capture the range of behavior we intend to convey. Paired t-tests over 20 communicative goals show that on average the most extreme random utterance is signiﬁcantly more extreme for the positive end of the extraversion, emotional stability, and agreeableness scales, and signiﬁcantly more extreme for both ends of the conscientiousness and openness to experience scales (p < .05, two-  Figure 4 The distribution of training data samples for openness personality judgments and naturalness. 468  Mairesse and Walker  Trainable Generation of Personality Traits  Table 4 For each communicative goal, the most extreme rating of the random utterances (Random) is compared with the ratings obtained for rule-based utterances (Rule-based). Ratings are averaged over 20 content plans and over all judges. • = the ratings of the random utterances are signiﬁcantly more extreme (◦ = more moderate) than the ratings of rule-based utterances (p < .05, two-tailed).  Method  Rule-based  Random  Trait  Low High Lowest Highest  Extraversion  2.96 5.98  Emotional stability  3.29 5.96  Agreeableness  3.41 5.66  Conscientiousness  3.71 5.53  Openness to experience 2.89 4.21  3.60 ◦ 3.05 3.26 3.11 • 2.28 •  6.23 • 6.25 • 6.01 • 5.93 • 5.48 •  Naturalness  4.59  4.38  tailed). However, random utterances are not perceived as introverted as those generated using the introvert parameter settings (see Rule-based/Low column for extraversion). Compare the distributions of judgments for the rule-based extraversion utterances with the judgments on the random sample shown in Figure 5. Nevertheless, these results suggest that randomizing PERSONAGE’s parameters produces a wide range of variation with an utterance sample of less than 10 utterances, for any communicative goal. The bottom row of Table 4 also compares the naturalness of the random utterances with the naturalness of the rule-based utterances produced by PERSONAGE-RB (Mairesse and Walker 2007; Mairesse 2008). Results suggest that the random utterances  Figure 5 The distribution of extraversion judgments for utterances generated using random parameters (Random) and handcrafted parameters derived from psychology studies (Rule-based). 469  Computational Linguistics  Volume 37, Number 3  Table 5 Average inter-rater correlation for the rule-based and random utterances. All correlations are signiﬁcant at the p < .05 level (two-tailed).  Parameter set  Rule-based Random  Extraversion  .73  .30  Emotional stability  .67  .33  Agreeableness  .54  .40  Conscientiousness  .42  .26  Openness to experience  .44  .28  are less natural than the rule-based utterances, and this difference is close to signiﬁcance (p = .075, two-tailed t-test). It is also important to quantify the quality of the annotations by evaluating the interrater agreement between the judges. Table 5 shows that the judges agree signiﬁcantly on the ratings of random utterances for all Big Five traits (p < .05, two-tailed), with correlations ranging from .26 (conscientiousness) to .40 (agreeableness), which are high correlations for human perceptual judgments. However the agreement is lower than on the rule-based utterances. A possible explanation of both the naturalness differences and rater agreement is that the random generation decisions sometimes produce utterances with inconsistent personality cues, which can be interpreted in different ways by the judges. For example, the utterance ‘Err... I am sure you would like Chanpen Thai!’ expresses markers of both introversion (ﬁlled pause) and extraversion (exclamation mark). 3.3 Training Parameter Estimation Models Parameter estimation requires a series of pre-processing steps, in order to ensure that the models’ output is re-usable by the PERSONAGE base generator. The initial data set includes the random sample annotated with the generation decision features shown in Table 2, together with the average judges’ ratings along each Big Five dimension, as described in Section 3.1. The following transformations are performed before the learning phase: r Reverse input and output: As parameter estimation models map from personality scores to generation parameters, the generation decisions are set as the data set’s output variables and the averaged personality ratings as the input features. r Predict parameters individually: A new data set is created for each output variable (i.e., generation parameter) as the statistical models we use only predict one output. We thus make the simplifying assumption that PERSONAGE’s generation parameters are independent.1 r Map output variables into PERSONAGE’s input space: The generation decisions made when generating each utterance in the random sample were recorded. In order to ensure that the parameter estimation models’  
To evaluate and challenge our approach, we conducted a series of experiments that test the effectiveness of the proposed strategy. Experimental results show that basing the content structuring and content selection process on a user model increases the efﬁciency and effectiveness of the user’s interaction. Users complete their tasks more successfully and more quickly. Furthermore, user surveys revealed that participants found that the user-model based system presents complex trade-offs understandably and increases overall user satisfaction. The experiments also indicate that presenting users with a brief overview of options that do not ﬁt their requirements signiﬁcantly improves the user’s overview of available options, also making them feel more conﬁdent in having been presented with all relevant options. ∗ Cluster of Excellence, Saarland University, Campus C7 4, 66041 Saarbru¨ cken, Germany. E-mail: vera@coli.uni-saarland.de. ∗∗ Intelligent Systems Lab Amsterdam, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, The Netherlands. E-mail: A.Winterboer@uva.nl. † School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK. E-mail: j.moore@ed.ac.uk. Submission received: 18 June 2009; revised submission received: 4 October 2010; accepted for publication: 1 December 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 3  1. Introduction A common goal of many spoken dialog systems (SDSs) is to offer efﬁcient and natural access to applications and services, such as e-mail, calendars, travel booking, navigation systems, and product recommendation, in situations where the user’s hands and/or eyes are busy with another task, for example driving a car (Pon-Barry, Weng, and Varges 2006) or operating equipment (Hieronymus and Dowding 2007). The naturalness and usability of a spoken dialog interface depends not only on its ability to recognize and interpret user utterances correctly, but also on its ability to present information in ways that users can understand and that help them to achieve their goals. One class of SDSs that has received considerable attention from both academic research and industry are information-seeking SDSs, which are designed to enable users to browse the space of available options (e.g., ﬂights, hotels, movies) and choose a suitable option from a potentially large set of choices. Dialogs with such systems typically consist of two main types of activity: information gathering, in which the system tries to establish users’ constraints and preferences, and information presentation, in which the system typically enumerates the set of options that match the user’s constraints. An example is given in Figure 1. In some systems, these activities take place in strictly sequential phases: All of the information necessary to form a database query is gathered, and then the returned options are presented, one at a time or in small groups. In other systems, the activities are interleaved, with users reﬁning their constraints after being presented with some options, or a summary of the option space. In either case, when the number of options to be presented is large, this process can be laborious, leading to reduced user satisfaction. Moreover, as Walker et al. (2004) observe, having to access the set of available options sequentially makes it difﬁcult for the user to remember the various aspects of multiple options and to compare them mentally. Although much research has been conducted on the information gathering phase of spoken dialog systems, relatively little attention has been devoted to information presentation. An analysis of the Communicator corpus consisting of approximately 2,000 dialogs with nine different spoken dialog systems found that information presentation is the main contributor to dialog duration1 (Moore 2006); see Table 1. Moreover, the DARPA Communicator evaluation showed that task duration is negatively correlated with user satisfaction (r = −0.31, p < 0.001, see Walker, Passonneau, and Boland [2001]). Thus, there is reason to believe that improvements in information presentation will lead to improvements in spoken dialog systems. Recently, two approaches to information presentation that present an alternative to sequential information presentation have been proposed. In the user-model (UM) based approach, the system identiﬁes a small number of options that best match the user’s preferences (Moore et al. 2004; Walker et al. 2004). In the summarize and reﬁne (SR) approach, the system structures the large number of options into a small number of clusters that share attributes. The system then summarizes the clusters based on their attributes, thus prompting the user to provide additional constraints (Polifroni, Chung, and Seneff 2003; Chung 2004). In this article, we propose an approach to information presentation which shortens dialog duration by combining the beneﬁts of these two approaches (UMSR). Our  
Dependency parsing involves ﬁnding the structure of a sentence as expressed by a set of directed links (called dependencies) between individual words. Dependency formalisms have attracted considerable interest in recent years, having been successfully applied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, and Weischedel 2008), textual entailment recognition (Herrera, Pen˜ as, and Verdejo 2005), relation extraction (Culotta and Sorensen 2004; Fundel, Ku¨ ffner, and Zimmer 2006), and question answering (Cui et al. 2005). Key characteristics of the dependency parsing approach are that dependency structures specify head–modiﬁer and head–complement relationships, which form the basis of predicate–argument structure, but are not represented explicitly in constituency trees; there is no need for dependency parsers to postulate the existence of non-lexical nodes; and some variants of dependency parsers ∗ Facultade de Informa´tica, Universidade da Corun˜ a Campus de Elvin˜ a, s/n, 15071 A Corun˜ a, Spain. E-mail: cgomezr@udc.es. ∗∗ School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: J.A.Carroll@sussex.ac.uk. † School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: D.J.Weir@sussex.ac.uk. Submission received: 21 October 2009; revised submission received: 23 December 2010; accepted for publication: 29 January 2011. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 3  are able to represent non-projective structures (McDonald et al. 2005), which is important when parsing free word order languages where discontinuous constituents are common. The formalism of parsing schemata, introduced by Sikkel (1997), is a useful tool for the study of constituency parsers, supporting precise, high-level descriptions of parsing algorithms. Potential applications of parsing schemata include devising correctness proofs, extending our understanding of relationships between different algorithms, deriving new variants of existing algorithms, and obtaining efﬁcient implementations automatically (Go´ mez-Rodr´ıguez, Vilares, and Alonso 2009). The formalism was originally deﬁned for context-free grammars (CFG) and since then has been applied to other constituency-based formalisms, such as tree-adjoining grammars (Alonso et al. 1999). This article considers the application of parsing schemata to the task of dependency parsing. The contributions of this article are as follows. r We introduce dependency parsing schemata, a novel adaptation of the original parsing schemata framework (see Section 2). r We use the dependency parsing schemata to deﬁne and compare a number of existing dependency parsers (projective parsers are presented in Section 3, and their formal properties discussed in Sections 4 and 5; a number of non-projective parsers are presented in Section 6). r We present parsing algorithms for several sets of mildly non-projective dependency structures, including a parser for a new class of structures we call mildly ill-nested, which encompasses all the structures in a number of existing dependency treebanks (see Section 7). r We adapt the dependency parsing schema framework to the formalism of Link Grammar (Sleator and Temperley 1991, 1993) (see Section 8). Although some of these contributions have been published previously, this article presents them in a thorough and consistent way. The deﬁnition of dependency parsing schemata was ﬁrst published by Go´ mez-Rodr´ıguez, Carroll, and Weir (2008), along with some of the projective schemata presented here and their associated proofs. The results concerning mildly non-projective parsing in Section 7 were ﬁrst published by Go´ mezRodr´ıguez, Weir, and Carroll (2008, 2009). On the other hand, the material on Nivre and Covington’s projective parsers, as well as all the non-projective parsers and the application of the formalism to Link Grammar, are entirely new contributions of this article. The notion of a parsing schema comes from considering parsing as a deduction process which generates intermediate results called items. In particular, items in parsing schemata are sets of partial constituency trees taken from the set of all partial parse trees that do not violate the constraints imposed by a grammar. A parsing schema can be used to obtain a working implementation of a parser by using deductive engines such as the ones described by Shieber et al. (1995) and Go´ mez-Rodr´ıguez, Vilares, and Alonso (2009), or the Dyna language (Eisner, Goldlust, and Smith 2005). 2. Dependency Parsing Schemata Although parsing schemata were originally deﬁned for CFG parsers, they have since been adapted to other constituency-based grammar formalisms. This involves ﬁnding 542  Go´ mez-Rodr´ıguez, Carroll, and Weir  Dependency Parsing Schemata  a suitable deﬁnition of the set of structures contained in items, and a way to deﬁne deduction steps that captures the formalism’s composition rules (Alonso et al. 1999). Although it is less clear how to adapt parsing schemata to dependency parsing, a number of dependency parsers have the key property of being constructive: They proceed by combining smaller structures to form larger ones, terminating when a complete parse for the input sentence is found. We show that this makes it possible to deﬁne a variant of the traditional parsing schemata framework, where the encodings of intermediate dependency structures are deﬁned as items, and the operations used to combine them are expressed as inference rules. We begin by addressing a number of preliminary issues. Traditional parsing schemata are used to deﬁne grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley PREDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classiﬁers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) → (b, j), which speciﬁes that a word b located at position j in the input string can have the word a in position i as a dependent. Deduction steps in data-driven parsers can be associated with the D-rules corresponding to the links they create, so that parsing schemata for such parsers are deﬁned using grammars of D-rules. In this way, we obtain a representation of some of the declarative aspects of these parsing strategies that is independent of the particular model used to make the decisions associated with each D-rule. Note that this representation is useful for designing control structures or probabilistic models for the parsers, because it makes explicit the choice points where the models will have to make probabilistic decisions, as well as the information available at each of those choice points. Additionally, D-rules allow us to use an uniform description that is valid for both data-driven and grammar-driven parsers, because D-rules can function like grammatical rules. The fundamental structures in dependency parsing are dependency trees. Therefore, just as items for constituency parsers encode sets of partial constituency trees, items for dependency parsers can be deﬁned using partial dependency trees. However, dependency trees cannot express the fact that a particular structure has been predicted, but not yet built; this is required for grammar-based algorithms such as those of Lombardo and Lesmo (1996) and Kahane, Nasr, and Rambow (1998). The formalism can be made general enough to include these parsers by using a novel way of representing intermediate states of dependency parsers based on a form of dependency trees that include nodes labelled with preterminals and terminals (Go´ mez-Rodr´ıguez, Carroll, and Weir 2008; Go´ mez-Rodr´ıguez 2009). For simplicity of presentation, we will only use this representation (called extended dependency trees) in the grammar-based algorithms that need it, and we will deﬁne the formalism and the rest of the algorithms with simple dependency trees. Some existing dependency parsing algorithms, for example, the algorithm of Eisner (1996), involve steps that connect spans which can represent disconnected dependency graphs. Such spans cannot be represented by 543  Computational Linguistics  Volume 37, Number 3  a single dependency tree. Therefore, our formalism allows items to be sets of forests of partial dependency trees, rather than sets of trees. We are now ready to deﬁne the concepts needed to specify item sets for dependency parsers. Deﬁnition 1 An interval (with endpoints i and j) is a set of natural numbers of the form [i..j] = {k | i ≤ k ≤ j}. We will use the notation i..j for the ordered list of the numbers in [i..j]. A dependency graph for a string w = w1 . . . wn is a graph G = (V, E), where V ⊆ [1..n] and E ⊆ V × V. The edge (i, j) is written i → j, and each such edge is called a dependency link, encoding the fact that the word wi is a syntactic dependent (or child) of wj or, conversely, that wj is the parent, governor, or head of wi. We write i → j to denote that there exists a (possibly empty) path from i to j. The projection of a node i, denoted i , is the set of reﬂexive-transitive dependents of i, that is, i = { j ∈ V | j → i}. In contexts where we refer to different dependency graphs, we use the notation i G to specify the projection of a node i in the graph G. Deﬁnition 2 A dependency graph T for a string w1 . . . wn is called a dependency tree for that string if it contains no cycles and all of its nodes have exactly one parent, except for one node that has none and is called the root or head of the tree T, denoted head(T). The yield of a dependency tree T, denoted yield(T), is the ordered list of its nodes. We will use the term dependency forest to refer to a set of dependency trees for the same string,1 and the generic term dependency structure to refer to a dependency tree or forest. A dependency tree is said to be a parse tree for a string w1 . . . wn if its yield is 1..n. Deﬁnition 3 We say that a dependency graph G = (V, E) for a string w1 . . . wn is projective if i is an interval for every i ∈ V. Deﬁnition 4 Let δ(G) be the set of dependency trees which are syntactically well-formed according to a given grammar G (which may be a grammar of D-rules or of CFG-like rules, as explained previously). We deﬁne an item set for dependency parsing as a set I ⊆ Π, where Π is a partition of the power set, (δ(G)), of the set δ(G). Each element of I, called an item, is a set of dependency forests for strings. For example, each member of the item [1, 5] in the item set of the parser by Yamada and Matsumoto (2003) that will be explained in Section 3.4 is a dependency forest with two projective trees, one with head 1 and the other with head 5, and such that the concatenation of their yields is 1..5. Figure 1 shows the three dependency forests that constitute the contents of this item under a speciﬁc grammar of D-rules. Following Sikkel (1997), items are sets of syntactic structures and tuples are a shorthand notation for such sets, as seen in the previous example. An alternative approach,  
© 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 3  Note that this problem is not only deﬁned for Chinese sentiment classiﬁcation, but also for various sentiment analysis tasks in other different languages. The proposed approach in this study can also be applied for generic cross-lingual text categorization tasks. Pilot studies have been performed to make use of English resources for subjectivity classiﬁcation in Romanian (Mihalcea, Banea, and Wiebe 2007; Banea et al. 2008), and the methods are very straightforward. First, they use machine translation for translating resources (such as a lexicon or corpus) between Romanian and English, and then they employ the lexicon-based or corpus-based method for subjectivity classiﬁcation in either Romanian or English. Similar experiments have been performed for subjectivity classiﬁcation in Spanish (Banea et al. 2008). However, our empirical study shows that sentiment classiﬁcation performance using these methods is far from satisfactory because the machine translation quality is not very good according to the recent NIST open machine translation evaluation results, and thus a language gap between the original language and the translated language still exists. In this study, we ﬁrst investigate several basic methods for cross-lingual sentiment classiﬁcation, and then propose a bilingual co-training approach to improve the accuracy of corpus-based polarity classiﬁcation of Chinese product reviews. Unlabeled Chinese reviews can be fully leveraged in the proposed approach. First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese test reviews and additional unlabeled reviews into English reviews. Then, we can view the classiﬁcation problem in two different ways: the Chinese view with only Chinese features and the English view with only English features. We then use the co-training approach to make full use of the two redundant views of features. The SVM classiﬁer (Joachims 2002) is adopted as the basic classiﬁer in the proposed approach. Three machine translation services (Google Translate, Yahoo Babel Fish, and Microsoft Bing Translate) are used for review translation in the experiments. The experimental results on two test sets show that the proposed approach based on any machine translation service can outperform a few popular baselines, including advanced transductive methods. We also ﬁnd that the balanced growth of the positive and negative instances at each iteration in the co-training algorithm is very important for the success of the algorithm. The rest of this article is organized as follows: Section 2 discusses related work. Section 3 introduces several basic methods. The proposed co-training approach is described in detail in Section 4. Sections 5 and 6 present the evaluation set-up and results, respectively. Lastly, we conclude this article and discuss future work in Section 7. 2. Related Work 2.1 Sentiment Classiﬁcation Sentiment classiﬁcation can be performed on words, sentences, or documents. In this article we focus on document-level sentiment classiﬁcation, and research in this area has followed a lexicon-based (i.e., rule-based) or a corpus-based (i.e., classiﬁcation-based) approach. Lexicon-based methods involve deriving a sentiment measure for text based on sentiment lexica. Turney (2002) predicts the sentiment orientation of a review as the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is known as the semantic orientation method. Kim and Hovy (2004) build 588  Wan  Bilingual Co-Training for Sentiment Classiﬁcation of Chinese Product Reviews  three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words. Kanayama, Nasukawa, and Watanabe (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensiﬁers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in ﬁnancial news text. Corpus-based methods consider the sentiment analysis task as a classiﬁcation task and they use a labeled corpus to train a sentiment classiﬁer. Since the work of Pang, Lee, and Vaithyanathan (2002), various classiﬁcation models and linguistic features have been proposed to improve classiﬁcation performance (Mullen and Collier 2004; Pang and Lee 2004; Read 2005; Wilson, Wiebe, and Hoffmann 2005). More recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of a text at varying levels of granularity. Blitzer, Dredze, and Pereira (2007) investigate domain adaptation for sentiment classiﬁers, focusing on on-line reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classiﬁer and a lexicon-based classiﬁer with precision-based vote weighting. A non-negative matrix tri-factorization approach has been proposed for sentiment classiﬁcation, which learns from lexical prior knowledge in the form of domain-independent sentiment-laden terms in conjunction with domain-dependent unlabeled data and a few labeled data (Li, Zhang, and Sindhwani 2009). Dasgupta and Ng (2009) propose a semi-supervised approach to sentiment classiﬁcation where they ﬁrst use spectral techniques to mine the unambiguous reviews and then exploit them to classify the ambiguous reviews by a novel combination of active learning, transductive learning, and ensemble learning. Chinese sentiment analysis has also been studied (Li and Sun 2007) and most such work uses similar lexicon-based or corpus-based methods for Chinese sentiment classiﬁcation. To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages. Standard naive Bayes and SVM classiﬁers have been applied for subjectivity classiﬁcation in Romanian and Spanish (Mihalcea, Banea, and Wiebe 2007; Banea et al. 2008), and the results show that automatic translation is a feasible alternative for the construction of resources and tools for subjectivity analysis in a new target language. Wan (2008) focuses on leveraging both Chinese and English lexica to improve Chinese sentiment analysis by using lexiconbased methods. Wei and Pal (2010) apply structural correspondence learning (SCL) to minimize the noise introduced by machine translations. In this study, we focus on developing novel approaches to improve the corpus-based method for cross-lingual sentiment classiﬁcation of Chinese product reviews. 2.2 Cross-Domain Text Classiﬁcation Cross-domain text classiﬁcation can be considered as a more general task than crosslingual sentiment classiﬁcation. In this task, the labeled and unlabeled data come from different domains and their underlying distributions are often different from each other, which violates the basic assumption of traditional supervised learning. To date, many semi-supervised learning algorithms have been developed for addressing the cross-domain text classiﬁcation problem by transferring knowledge across domains, and such algorithms include Transductive SVM (Joachims 1999), EM (Nigam et al. 2000), EM-based naive Bayes classiﬁer (Dai et al. 2007a), Topic-bridged PLSA (Xue 589  Computational Linguistics  Volume 37, Number 3  et al. 2008), Co-Clustering–based classiﬁcation (Dai et al. 2007b), and the two-stage approach (Jiang and Zhai 2007). Dai et al. (2007b) use co-clustering as a bridge to propagate the class structure and knowledge from the in-domain to the out-of-domain. Jiang and Zhai (2007) look for a set of features generalizable across domains at the ﬁrst generalization stage, and then pick up useful features speciﬁc to the target domain at the second adaptation stage. Daume´ III and Marcu (2006) introduce a statistical formulation of this problem in terms of a simple mixture model. In recent years, a few methods/algorithms have been proposed for cross-domain sentiment classiﬁcation, including structural correspondence learning (Blitzer, Dredze, and Pereira 2007), crossdomain graph ranking (Wu et al. 2009), and spectral feature alignment (Pan et al. 2010). Moreover, several previous studies focus on the problem of cross-lingual text classiﬁcation, which can be considered a special case of cross-domain text classiﬁcation. Bel, Koster, and Villegas (2003) empirically investigate three translation strategies for cross-lingual text categorization: document translation, terminology translation, and proﬁle-based translation. A few novel models have been proposed to address the problem—for example, the EM-based algorithm (Rigutini, Maggini, and Liu 2005), the information bottleneck approach (Ling et al. 2008), multilingual domain models (Gliozzo and Strapparava 2005), and the structural correspondence learning approach (Prettenhofer and Stein 2010; Wei and Pal 2010). Shi et al. (2010) introduce a method to transfer classiﬁcation knowledge across languages by translating the model features and using an EM algorithm. The most recent related work includes multilingual text categorization based on multi-view learning (Amini, Usunier, and Goutte 2009; Amini and Goutte 2010). To the best of our knowledge, co-training has not yet been investigated for cross-domain or cross-lingual text classiﬁcation. 3. The Basic Methods A straightforward method for cross-lingual sentiment classiﬁcation is to use machine translation for transferring lexica or corpora of reviews between English and Chinese, and then apply the lexicon-based or corpus-based method for sentiment classiﬁcation in either the English or Chinese language. Therefore, the basic methods consist of two main steps: resource translation and sentiment classiﬁcation. According to different translation directions and classiﬁcation methods, four basic methods are introduced as follows. 3.1 Lexicon-Based Method in English Language: LEX(EN) This method ﬁrst translates Chinese reviews into English reviews, and then identiﬁes the sentiment polarity of the translated English reviews based on English sentiment lexica, as illustrated in Figure 1. For any speciﬁc language, we employ the semantic-oriented approach used in Wan (2008) to compute the semantic orientation value of a review. The unsupervised approach is quite straightforward and it makes use of the following sentiment lexica: positive Lexicon (Positive Dic) containing terms expressing positive polarity, Negative Lexicon (Negative Dic) containing terms expressing negative polarity, Negation Lexicon (Negation Dic) containing terms that are used to reverse the semantic polarity of a particular term, and Intensiﬁer Lexicon (Intensiﬁer Dic) containing terms that are used to change the degree to which a term is positive or negative. The semantic orientation value for a review is computed by summing the polarity values of all terms in the review, making use of both the word polarity deﬁned in the positive and negative lexica and the contextual valence shifters deﬁned in the negation and intensiﬁer lexica. 590  Wan  Bilingual Co-Training for Sentiment Classiﬁcation of Chinese Product Reviews  Figure 1 Framework of LEX(EN). For example, given a review of the image quality is not good, although the term good is a positive term, the use of the negation term not reverses the polarity orientation value, and the overall polarity orientation value of the review is negative. Given a review of the image quality is very good, the use of the intensiﬁer term very intensiﬁes the polarity orientation value of good, and the overall polarity orientation value of the review is positive. In our study, the scope of a negation or intensiﬁer term is simply determined by using a distance window of two words. We do not use a parser to determine the scope because the parsing results for the translated reviews are not reliable. Finally, if the semantic orientation value of a review is less than 0, the review is labeled as negative; otherwise, the review is labeled as positive. 3.2 Lexicon-Based Method in Chinese Language: LEX(CN) This method ﬁrst translates English sentiment lexica into Chinese lexica, and then identiﬁes the sentiment polarity of Chinese reviews based on the translated Chinese lexica, as illustrated in Figure 2. After we retrieve the four translated Chinese lexica, we apply the algorithm for semantic orientation value computation used in Wan (2008) to predict the polarity orientation of the Chinese reviews. Each Chinese review is ﬁrst segmented into Chinese terms/words by using our in-house conditional random ﬁeld (CRF)–based Chinese word segmentation tool, and then the polarity orientation value for the Chinese review is computed by summing the polarity values of all terms in the review. The terms deﬁned in the negation lexicon are used to reverse the polarity values of the nearby Chinese terms, and the terms deﬁned in the intensiﬁer lexicon are used to intensify the polarity values of the nearby Chinese terms. The scope of a negation or intensiﬁer term is also simply determined by using a distance window of two words. 3.3 Corpus-Based Method in English Language: SVM(EN) As illustrated in Figure 3, we ﬁrst learn a classiﬁer based on labeled English reviews, and then translate test Chinese reviews into English reviews. Lastly, we use the classiﬁer to classify the translated English reviews. In this study, we use the widely used SVM 591  Computational Linguistics  Volume 37, Number 3  Figure 2 Framework of LEX(CN).  Figure 3 Framework of SVM(EN). classiﬁer for classiﬁcation. We also use a transductive variant of the SVM classiﬁer for making use of unlabeled Chinese reviews, which will be described in Section 5.5. All English unigrams and bigrams are used as features, and the feature weight is simply set to term frequency.1 Finally, the sign of the prediction value of the classiﬁer indicates the polarity orientation of the review. 3.4 Corpus-Based Method in Chinese Language: SVM(CN) As illustrated in Figure 4, we ﬁrst translate labeled English reviews into Chinese reviews, and then learn a classiﬁer based on the translated Chinese reviews with labels. 
Manfred Stede§ University of Potsdam We present a lexicon-based approach to extracting sentiment from text. The Semantic Orientation CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensiﬁcation and negation. SO-CAL is applied to the polarity classiﬁcation task, the process of assigning a positive or negative label to a text that captures the text’s opinion towards its main subject matter. We show that SO-CAL’s performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability. 1. Introduction Semantic orientation (SO) is a measure of subjectivity and opinion in text. It usually captures an evaluative factor (positive or negative) and potency or strength (degree to which the word, phrase, sentence, or document in question is positive or negative) ∗ Corresponding author. Department of Linguistics, Simon Fraser University, 8888 University Dr., Burnaby, B.C. V5A 1S6 Canada. E-mail: mtaboada@sfu.ca. ∗∗ Department of Computer Science, University of Toronto, 10 King’s College Road, Room 3302, Toronto, Ontario M5S 3G4 Canada. E-mail: jbrooke@cs.toronto.edu. † School of Computing Science, Simon Fraser University, 8888 University Dr., Burnaby, B.C. V5A 1S6 Canada. E-mail: mta45@sfu.ca. ‡ Department of Computer Science, University of British Columbia, 201-2366 Main Mall, Vancouver, B.C. V6T 1Z4 Canada. E-mail: kvoll@cs.ubc.ca. § Department of Linguistics, University of Potsdam, Karl-Liebknecht-Str. 24-25. D-14476 Golm, Germany. E-mail: stede@uni-potsdam.de. Submission received: 14 December 2009; revised submission received: 22 August 2010; accepted for publication: 28 September 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 2  towards a subject topic, person, or idea (Osgood, Suci, and Tannenbaum 1957). When used in the analysis of public opinion, such as the automated interpretation of on-line product reviews, semantic orientation can be extremely helpful in marketing, measures of popularity and success, and compiling reviews. The analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (Pang and Lee 2008), subjectivity (Lyons 1981; Langacker 1985), opinion mining (Pang and Lee 2008), analysis of stance (Biber and Finegan 1988; Conrad and Biber 2000), appraisal (Martin and White 2005), point of view (Wiebe 1994; Scheibman 2002), evidentiality (Chafe and Nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (Ketal 1975; Ortony, Clore, and Collins 1988) and affect (Batson, Shaw, and Oleson 1992). In this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. Our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal. There exist two main approaches to the problem of extracting sentiment automatically.1 The lexicon-based approach involves calculating orientation for a document from the semantic orientation of words or phrases in the document (Turney 2002). The text classiﬁcation approach involves building classiﬁers from labeled instances of texts or sentences (Pang, Lee, and Vaithyanathan 2002), essentially a supervised classiﬁcation task. The latter approach could also be described as a statistical or machine-learning approach. We follow the ﬁrst method, in which we use dictionaries of words annotated with the word’s semantic orientation, or polarity. Dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also Stone et al. 1966; Tong 2001), or automatically, using seed words to expand the list of words (Hatzivassiloglou and McKeown 1997; Turney 2002; Turney and Littman 2003). Much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (Hatzivassiloglou and McKeown 1997; Wiebe 2000; Hu and Liu 2004; Taboada, Anthony, and Voll 2006).2 First, a list of adjectives and corresponding SO values is compiled into a dictionary. Then, for any given text, all adjectives are extracted and annotated with their SO value, using the dictionary scores. The SO scores are in turn aggregated into a single score for the text. The majority of the statistical text classiﬁcation research builds Support Vector Machine classiﬁers, trained on a particular data set using features such as unigrams or bigrams, and with or without part-of-speech labels, although the most successful features seem to be basic unigrams (Pang, Lee, and Vaithyanathan 2002; Salvetti, Reichenbach, and Lewis 2006). Classiﬁers built using supervised methods reach quite a high accuracy in detecting the polarity of a text (Chaovalit and Zhou 2005; Kennedy and Inkpen 2006; Boiy et al. 2007; Bartlett and Albright 2008). However, although such classiﬁers perform very well in the domain that they are trained on, their performance drops precipitously (almost to chance) when the same classiﬁer is used in  
Generally, a distinction is made between inﬂectional morphology and word formation. Inﬂectional morphology deals with the various realizations of the “same” lexical word, depending on the particular syntactic context in which the word appears. Typical examples of inﬂection are verbs agreeing with one or more of their arguments in the clause, or nouns inﬂected in particular case forms in order to show their syntactic relation to other words in the phrase or clause, for example, showing which verb argument they express. Word formation deals with the creation of new lexical words from existing ∗ Centre for Language Studies, Radboud Universiteit, Postbus 9103, 6500 HD Nijmegen, The Netherlands/ Department of Linguistics, Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6, 04103 Leipzig, Germany. E-mail: h.hammarstrom@let.ru.nl. ∗∗ Språkbanken, Department of Swedish Language, University of Gothenburg, Box 200, SE-405 30 Göteborg, Sweden. E-mail: lars.borin@svenska.gu.se. Submission received: 2 March 2010; revised submission received: 19 August 2010; accepted for publication: 4 October 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 2  ones, for example, agent nouns from verbs. If the same kinds of mechanisms are used as in inﬂectional morphology (i.e., the resulting word is derived out of only one existing word), linguists talk about derivational morphology. If two or more existing lexical words are combined in order to make up a new word, the terms compounding or incorporation are used, depending on the categories of the words involved. There is a fairly wide array of formal means available cross-linguistically for expressing inﬂectional and derivational categories in languages. Most commonly, however, some form of afﬁxation is involved—that is, some phonological material is added to the end of the word (sufﬁxation), to the beginning of the word (preﬁxation), or (much more rarely) inside the stem of the word (inﬁxation). Sufﬁxes and preﬁxes (but rarely inﬁxes) can form long chains, where the different positions, or “slots,” express different kinds of inﬂectional or derivational categories. If a language has sufﬁxing and/or preﬁxing—sometimes called concatenative morphology—it obviously follows that text words in that language can be segmented into a sequence of morphological elements: a stem and a number of sufﬁxes after the stem and/or preﬁxes before the stem.1 Morphology is one of the oldest linguistic subdisciplines, and this brief presentation by necessity omits many intricacies and greatly simpliﬁes a vast scholarship. (For standard, in-depth, introductions to this fascinating ﬁeld, see, e.g., Nida [1949], Jensen [1990], Spencer and Zwicky [1998], or Haspelmath [2002].) In language technology applications, a morphological component forms a bridge between texts and structured information about the vocabulary of a language. Some kind of morphological analysis and/or generation thus forms a basic component in many natural language processing applications. Many languages have quite complex morphological systems, with the number of potential inﬂected forms of a single lexical word running into the thousands, requiring a substantial amount of work if the linguistic knowledge of the morphological component is to be deﬁned manually. For this reason, researchers often turn to machine learning approaches. This survey article is concerned with unsupervised approaches to morphology learning. For the purposes of the present survey, we use the following deﬁnition of Unsupervised Learning of Morphology (ULM). Input: Raw (unannotated, non-selective2) natural language text data Output: A description of the morphological structure (there are various levels to be distinguished; see subsequent discussion) of the language of the input text With: As little supervision (parameters, thresholds, human intervention, model selection during development, etc.) as possible Some approaches have explicit or implicit biases towards certain kinds of languages; they are nevertheless considered to be ULM for this survey. Morphology may be narrowly taken as to include only derivational and inﬂectional afﬁxation, where the number of afﬁxes a root may take is ﬁnite3 and the order of the afﬁxes may not be  
The problem of Question Answering (QA) has received considerable attention in the past few years. Nevertheless, most of the work has focused on the task of factoid QA, where questions match short answers, usually in the form of named or numerical entities. Thanks to international evaluations organized by conferences such as the Text REtrieval Conference (TREC) and the Cross Language Evaluation Forum (CLEF) Workshop, annotated corpora of questions and answers have become available for several languages, which has facilitated the development of robust machine learning models for the task.1 ∗ Stanford University, 353 Serra Mall, Stanford, CA 94305–9010. E-mail: mihais@stanford.edu. ∗∗ Google Inc., Brandschenkestrasse 110, CH–8002 Zu¨ rich, Switzerland. E-mail: massi@google.com. † Yahoo! Research, Avinguda Diagonal 177, 8th Floor, 08018 Barcelona, Spain. E-mail: hugoz@yahoo-inc.com. 
AGC can directly beneﬁt Information Retrieval (Freund, Clarke, and Toms 2006), where users may want documents that serve a particular communicative purpose (instructions, reviews, user guides, etc.). AGC can also beneﬁt Language Technology indirectly, where differences in the low-level properties that correlate with genre may impact system performance. For example, if a part–of–speech (PoS) tagger or Statistical Machine Translation system trained on a corpus of editorials was then used for PoS tagging or translating a corpus of letters to the editor, it would beneﬁt from the knowledge that inter alia the likelihood of the word “states” being a verb is considerably higher in letters (∼20%) than in editorials (∼2%).1 ∗ University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK. E-mail: p.petrenz@sms.ed.ac.uk. ∗∗ University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK. E-mail: bonnie.webber@ed.ac.uk. 
Pustejovsky and Rumshisky consider extended senses of verbs within the framework of the generative lexicon (Pustejovsky 2006) in Chapter 4. They argue that some extended senses can in fact be viewed as non-metaphorical usages, and offer a further classiﬁcation of metaphorical usages into strong and weak metaphors depending on whether the core meaning of the predicate is generalized. 2. Part II: Computing Lexical Relations Church kicks off Part II by responding to an earlier position piece by Kilgarriff (2007). Church argues that we should not abandon the use of large, but noisy and unbalanced, corpora, and discusses tasks to which such corpora are better suited than cleaner and more balanced, but smaller, corpora. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 2  In Chapter 8 Grefenstette builds on two ideas previously seen in this collection— the use of massive amounts of data, and the importance of multiword expressions— to estimate the number of concepts. Grefenstette uses the number of frequently occurring noun–noun and adjective–noun sequences on the Web to arrive at an estimate of roughly two hundred million concepts. He acknowledges some of the limitations of his estimate, but nevertheless, this estimate gives insight into the potential number of entries in future lexical resources. Patrick Hanks has developed corpus pattern analysis, a manual technique for identifying the typical patterns in which a verb is used.1 Patterns are often described in terms of the classes (i.e., coarse semantic categories) of nouns occurring with a verb. In Chapter 9 Guthrie and Guthrie present an unsupervised statistical method for ﬁnding adjectives that are predictive of these noun classes, and present experimental results for the task of automatically determining an ambiguous noun’s class from only its modifying adjective. In Chapter 10 Geyken considers the impact of corpus size on the ability of pointwise mutual information to identify German verb-nominalization constructions. Geyken ﬁnds that when using a one-billion-word opportunistic corpus, most verbnominalizations in a German dictionary are found to have positive pointwise mutual information, but that this is not the case when using a one-hundred-million word balanced corpus. Word Sketches (Kilgarriff and Tugwell 2002) are automatically derived statistical summaries of the grammatical and collocational behavior of words that have proven to be a useful lexicographic tool. In Chapter 11 Pala and Rychly´ examine some of the errors found in a word sketch for the Czech verb videˇt (‘see’), and conclude that the quality of the word sketch is relatively low, but could be improved through, primarily, better part-of-speech tagging and lemmatization. We return to corpus pattern analysis in Chapter 12. Cinkova´ et al. conduct a study to determine whether humans can reliably reproduce corpus pattern analysis, and further examine the relationship between nouns and semantic types in the existing Pattern Dictionary of English Verbs.2 In Chapter 13 Jezek and Frontini discuss an extension of corpus pattern analysis to produce a pattern bank—a resource of corpus instances annotated more richly than in corpus pattern analysis that could potentially beneﬁt many natural language processing tasks—for Italian. 3. Part III: Lexical Analysis and Dictionary Writing In Chapter 15 Atkins presents DANTE (a new English lexical database manually constructed by analyzing a large corpus of English) and contrasts it with FrameNet. Atkins discusses the possibility of (semi-automatically) linking the lexical units in DANTE and FrameNet. This is an interesting research problem, and moreover, the results would be a very rich lexical resource which would have many potential applications in computational linguistics. In Chapter 16 Kilgarriff and Rychly´ describe a system for semi-automatically deriving a draft of a dictionary entry, in particular, determining a word’s senses. Beginning with an automatically produced clustering of a word’s collocates, their method uses  
Computational Linguistics  Volume 37, Number 2  Part II begins with an identiﬁcation of some linguistic shortcomings of Lambek’s original grammar and introduces a number of extensions. In contrast to the broad introduction to Lambek categorial grammar given in Part I, Part II delves into more advanced material that is less well established. Section 5 introduces bracket operators for Lambek categorial grammar, motivated by examples of medial and parasitic extraction from English. This section, like the others in Part II, is short and has room only for the deﬁnitions and some linguistic examples. Section 6 introduces discontinuity operators, motivated by English examples of discontinuous idioms, quantiﬁcation, VP ellipsis, medial extraction, and pied-piping, among other constructions. Section 7 introduces additive operators to Lambek categorial grammar to handle lexical ambiguity of prepositions more elegantly. Finally, section 8 gives a very brief introduction to modal Lambek categorial grammar and its application to English modals. Part III, titled “Further Processing Issues,” is much more loosely connected than the two preceding sections. Section 9 introduces the results of Caplan and Hildebrandt (1988) on the ability of aphasic speakers of English to understand certain sentences. The complexity of these sentences based on their analyses within Lambek categorial grammar is then analyzed and found to correspond to the experimental results. The primary purpose of this section appears to be a motivation for type-logical grammar from a psycho-linguistic standpoint. Section 10 introduces methods for pre-evaluating the syntax of a sentence using only the lexicon. Section 11 discusses a chart parsing algorithm based on the proof nets of section 4. This section lacks any analysis of running time, however, which is relevant due to the NP-completeness results for Lambek categorial grammar (Pentus 2006). Section 12 partially addresses these concerns through some philosophical arguments and offers some directions for future research. The primary weakness of this book is that it remains quite ﬁrmly theoretical despite the fact that categorial grammar, in general, does not. Throughout the book, type-logical grammar is motivated via English examples and those English examples are not English utterances drawn from books or newspapers but rather carefully constructed examples such as those commonly found in mainstream linguistics research. As a result, the arguments made often have little relevance to mainstream computational linguistics. Furthermore, the relationship between type-logical grammar and more practically oriented categorial grammars such as combinatory categorial grammar (Steedman 2000) is completely overlooked. The end result is that much of the content of this book, especially Parts II and III, will remain inaccessible to a large portion of computational linguists.  References Ajdukiewicz, K. 1935. Die Syntaktische Konnexita¨t. Studia Philosophica, 1:1–27. Caplan, D. and N. Hildebrandt. 1988. Disorders of Syntactic Comprehension. MIT Press, Cambridge, MA. de Groote, P. 2001. Towards abstract categorial grammars. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 252–259, Toulouse. Lambek, J. 1958. The mathematics of sentence structure. American Mathematical Monthly, 65(3):154–170. Lambek, J. 1999. Type grammar revisited. In A. Lecomte, F. Lamarche, and  G. Perrier, editors, Logical Aspects of Computational Linguistics: Selected Papers from the Second International Conference, LACL ’97, Lecture Notes in Artiﬁcial Intelligence, vol. 1582. Springer, New York, pages 1–27. Montague, R. and R. H. Thomason. 1974. Formal Philosophy: Selected Papers of Richard Montague. Yale University Press, New Haven, CT. Pentus, M. 2006. Lambek calculus is NP-complete. Theoretical Computer Science, 357(1–3):186–201. Steedman, M. 2000. The Syntactic Process. MIT Press, Cambridge, MA.  408  
language problems in IR with European and East Asian languages, CLIR problems and approaches, needs for CLIR and MLIR, and a brief history of CLIR. r Chapter 2, “Using manually constructed translation systems and resources for CLIR,” covers an introduction to MT, basic use of MT in CLIR, and dictionary-based translation for CLIR. r Chapter 3, “Translation based on parallel and comparable corpora,” covers methods for automatic paragraph and sentence alignment, use © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 2  of translation models for CLIR, alternative approaches using parallel corpora, discussion of CLIR methods and resources, and mining for translation resources and relations. r Chapter 4, “Other methods to improve CLIR,” covers pre- and post-translation expansion, fuzzy matching, combination of translations, transitive translation, and integration of monolingual and translingual relations. r Chapter 5, “A look into the future: Towards a uniﬁed view of monolingual IR and CLIR?” summarizes the state-of-the-art in CLIR and proposals for improvements.  CLIR approaches are in general presented together with their statistical models, whose understanding does not require more than elementary calculus and probability theory. However, the book does not present algorithms or data structures to implement the models, so it might not be a sufﬁcient resource to build an effective CLIR system. The ﬁrst two chapters are rather introductory and lead to the conventional CLIR approach, in which MT or dictionary-based translation is simply cascaded with monolingual IR. A discussion on the limitations of using such general translation tools convinces the reader of the need for translation techniques that are more speciﬁc to and better integrated with IR. In Chapters 3 and 4, the core of the book, several advanced CLIR models from the recent literature are discussed. In particular, Chapter 3 focuses on the collection and processing of parallel texts and on statistical translation models for query terms. Chapter 4 discusses cross-lingual counterparts of well-established IR techniques (i.e., pre- and post-translation query expansion) as well as CLIR-speciﬁc methods to further improve retrieval performance (e.g., fuzzy matching and translation combination). Finally, in Chapter 5 the author, starting from a parallel between query expansion in IR and query translation in CLEF, proposes new directions for future work. 
Manually annotating corpora or manually developing any other linguistic resource, such as a set of judgments about system outputs, represents such a high cost that many researchers are looking for alternative solutions to the standard approach. MTurk is becoming a popular one. However, as in any scientiﬁc endeavor involving humans, there is an unspoken ethical dimension involved in resource construction and system evaluation, and this is especially true of MTurk. We would like here to raise some questions about the use of MTurk. To do so, we will deﬁne precisely what MTurk is and what it is not, highlighting the issues raised by the system. We hope that this will point out opportunities for our community to deliberately value ethics above cost savings. What Is MTurk? What Is It Not? MTurk is an on-line crowdsourcing, microworking1 system which enables elementary tasks to be performed by a huge number of people (typically called “Turkers”) on-line. Ideally, these tasks are meant to be solved by computers, but they still remain out of computational reach (for instance, the translation of an English sentence into Urdu). ∗ INIST-CNRS/LIPN, 2 alle´e de Brabois, F-54500 Vandoeuvre-le`s-Nancy, France. E-mail: karen.fort@inist.fr. ∗∗ LIMSI/CNRS, Rue John von Neumann, Universite´ Paris-Sud F-91403 ORSAY, France. E-mail: gilles.adda@limsi.fr. † Center for Computational Pharmacology, University of Colorado School of Medicine, University of Colorado at Boulder. E-mail: kevin.cohen@gmail.com. 
We present a metric called Nouveau-ROUGE that improves correlation with manual evaluation metrics and can be used to predict both the pyramid score and overall responsiveness for update summaries. Nouveau-ROUGE can serve as a less expensive surrogate for manual evaluations when comparing existing systems and when developing new ones. 1. Introduction Update summaries focus on what is new relative to a previous body of information. They pose new challenges both to algorithm developers and to evaluation of summaries. In 2007, DUC (Document Understanding Conference) introduced an update summarization task, repeated in 2008 for TAC (Text Analysis Conference).1 This task consisted of producing a multi-document summary for a set of articles on a single topic, followed by one (2008) or two (2007) multi-document summaries for sets of articles on ∗ Institute for Defense Analyses, Center for Computing Sciences, 17100 Science Drive, Bowie, MD 20715 USA. E-mail: {judith,conroy}@super.org. ∗∗ Computer Science Department, Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742 USA. E-mail: oleary@cs.umd.edu. 
Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identiﬁed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods signiﬁcantly. 1. Introduction Opinion mining (or sentiment analysis) has attracted a great deal of attention from researchers of natural language processing and data mining in the past few years due ∗ College of Computer Science, Zhejiang University, 38 Zheda Rd., Hangzhou 310027, Zhejiang, China. E-mail: qiuguang@zju.edu.cn. ∗∗ Department of Computer Science, University of Illinois, 851 South Morgan Street Chicago, IL 60607-7053. E-mail: liub@cs.uic.edu. † College of Computer Science, Zhejiang University, 38 Zheda Rd., Hangzhou 310027, Zhejiang, China. E-mail: bjj@zju.edu.cn. ‡ College of Computer Science, Zhejiang University, Corresponding author, 38 Zheda Rd., Hangzhou 310027, Zhejiang, China. E-mail: chenc@zju.edu.cn. Submission received: 2 September 2009; revised submission received: 20 January 2010; accepted for publication: 20 July 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 1  to many challenging research problems and practical applications. Two fundamental problems in opinion mining are opinion lexicon expansion and opinion target extraction (Liu 2006; Pang and Lee 2008). An opinion lexicon is a list of opinion words such as good, excellent, poor, and bad which are used to indicate positive or negative sentiments. It forms the foundation of many opinion mining tasks, for example, sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) sentiment classiﬁcation, and feature-based opinion summarization (Hu and Liu 2004). Although there are several opinion lexicons publicly available, it is hard, if not impossible, to maintain a universal opinion lexicon to cover all domains as opinion expressions vary signiﬁcantly from domain to domain. A word can be positive in one domain but has no opinion or even negative opinion in another domain. Therefore, it is necessary to expand a known opinion lexicon for applications in different domains using text corpora from the corresponding domains. Opinion targets are topics on which opinions are expressed. They are important because without knowing the targets, the opinions expressed in a sentence or document are of limited use. For example, in the opinion sentence I am not happy with the battery life of this phone, battery life is the target of the opinion. If we do not know that, this opinion is of little value. Although several researchers have studied the opinion lexicon expansion and opinion target extraction (also known as topic, feature, or aspect extraction) problems, their algorithms either need additional and external resources or impose strong constraints and are of limited success. Detailed discussions of existing works will be given in Section 2. In this article, we propose a novel propagation based method to solve the opinion lexicon expansion and target extraction problems simultaneously. Our approach differs from existing approaches in that it requires no additional resources except an initial seed opinion lexicon, which is readily available. Thus, it can be seen as a semi-supervised method due to the use of the seeds. It is based on the observation that there are natural relations between opinion words and targets due to the fact that opinion words are used to modify targets. Furthermore, we ﬁnd that opinion words and targets themselves have relations in opinionated expressions too. These relations can be identiﬁed via a dependency parser based on the dependency grammar (Tesniere 1959), and then exploited to perform the extraction tasks. The basic idea of our approach is to extract opinion words (or targets) iteratively using known and extracted (in previous iterations) opinion words and targets through the identiﬁcation of syntactic relations. The identiﬁcation of the relations is the key to the extractions. As our approach propagates information back and forth between opinion words and targets, we call it double propagation. Opinion word sentiment or polarity assignment (positive, negative, or neutral) and noisy target pruning methods are also designed to reﬁne the initially extracted results. In evaluation, we compare our approach with several state-of-the-art existing approaches in opinion lexicon expansion (or opinion word extraction) and target (or feature/topic) extraction. The results show that our approach outperforms these existing approaches signiﬁcantly.  2. Related Work Our work is related to opinion word extraction and target (or topic) extraction in opinion mining. 10  Qiu et al.  Opinion Word Expansion and Target Extraction through Double Propagation  2.1 Opinion Word Extraction Extensive work has been done on sentiment analysis at word, expression (Breck, Choi, and Cardie 2007; Takamura, Inui, and Okumura 2007), sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) levels. We only describe work at word level as it is most relevant to our work. In general, the existing work can be categorized as corporabased (Hatzivassiloglou and McKeown 1997; Wiebe 2000; Wiebe et al. 2004; Turney and Littman 2003; Kanayama and Nasukawa 2006; Kaji and Kitsuregawa 2007) and dictionary-based (Hu and Liu 2004; Kim and Hovy 2004; Kamps et al. 2004; Esuli and Sebastiani 2005; Takamura, Inui, and Okumura 2005) approaches. Our work falls into the corpora-based category. Hatzivassiloglou and McKeown (1997) proposed the ﬁrst method for determining adjective polarities or orientations (positive, negative, and neutral). The method predicts orientations of adjectives by detecting pairs of such words conjoined by conjunctions such as and and or in a large document set. The underlying intuition is that the orientations of conjoined adjectives are subject to some linguistic constraints. For example, in the sentence This car is beautiful and spacious, if we know that beautiful is positive, we can infer that spacious is positive too. The weakness of this method is that as it relies on the conjunction relations it is unable to extract adjectives that are not conjoined. Wiebe (2000) and Wiebe et al. (2004) proposed an approach to ﬁnding subjective adjectives using the results of word clustering according to their distributional similarity. However, they did not tackle the prediction of sentiment polarities of the found subjective adjectives. Turney and Littman (2003) compute the point wise mutual information (PMI) of the target term with each seed positive and negative term as a measure of their semantic association. Their work requires additional access to the Web (or any other corpus similar to the Web to ensure sufﬁcient coverage), which is time consuming. Another recent corpora-based approach is proposed by Kanayama and Nasukawa (2006). Their work ﬁrst uses clause level context coherency to ﬁnd candidates, then uses a statistical estimation method to determine whether the candidates are appropriate opinion words. Their method for ﬁnding candidates would have low recall if the occurrences of seed words in the data are infrequent or an unknown opinion word has no known opinion words in its context, however. Besides, the statistical estimation can be unreliable if the corpus is small, which is a common problem for statistical approaches. We will compare our approach with this approach in our experiments. In dictionary-based approaches, Kamps et al. (2004) take advantage of WordNet to construct a synonymy network by connecting pairs of synonymous words. The semantic orientation of a word is decided by its shortest paths to two seed words good and bad which are chosen as representatives of positive and negative orientations. Esuli and Sebastiani (2005) use text classiﬁcation techniques to classify orientations. Their method is based on the glosses (textual deﬁnitions) in an on-line “glossary” or dictionary. The work of Takamura, Inui, and Okumura (2005) also exploits the gloss information from dictionaries. The method constructs a lexical network by linking two words if one appears in the gloss of the other. The weights of links reﬂect if these two connected words are of the same orientation. The works of Hu and Liu (2004) and Kim and Hovy (2004) are simpler as they simply used synonyms and antonyms. However, all dictionary-based methods are unable to ﬁnd domain dependent sentiment words because most entries in dictionaries are domain-independent. For example, unpredictable is often a positive opinion word in movie reviews, as in unpredictable plot, but in car reviews unpredictable is likely to be negative, as in unpredictable steering. Our approach 11  Computational Linguistics  Volume 37, Number 1  extracts opinion words using domain dependent corpora; thus we are able to ﬁnd domain-dependent opinion words. 2.2 Opinion Target Extraction Opinion target (or topic) extraction is a difﬁcult task in opinion mining. Several methods have been proposed, mainly in the context of product review mining (Hu and Liu 2004; Popescu and Etzioni 2005; Kobayashi, Inui, and Matsumoto 2007; Mei et al. 2007; Scafﬁdi et al. 2007; Wong, Lam, and Wong 2008; Stoyanov and Cardie 2008). In this mining task, opinion targets usually refer to product features, which are deﬁned as product components or attributes, as in Liu (2006). In the work of Hu and Liu (2004), frequent nouns and noun phrases are treated as product feature candidates. In our work, we also extract only noun targets. Different pruning methods are proposed to remove the noise. To cover infrequent features that are missed, they regard the nearest nouns/noun phrases of the opinion words identiﬁed by frequent features as infrequent features. In Popescu and Etzioni (2005), the authors investigated the same problem. Their extraction method, however, requires that the product class is known in advance. The algorithm determines whether a noun/noun phrase is a feature by computing the PMI score between the phrase and class-speciﬁc discriminators through a Web search. Querying the Web is a problem, as discussed earlier. We will compare these two representative methods with our approach in the experiments. In Scafﬁdi et al. (2007), the authors proposed a language model approach to product feature extraction with the assumption that product features are mentioned more often in a product review than they are mentioned in general English text. However, statistics may not be reliable when the corpus is small, as pointed out earlier. The recent work by Kobayashi, Inui, and Matsumoto (2007) focused on the aspect-evaluation (aspect and evaluation mean the opinion target and opinion word respectively in our context) and aspect-of extraction problems in blogs. Their aspectevaluation extraction uses syntactic patterns learned via pattern mining to extract aspect, evaluation pairs. Our work differs from theirs in that we make use of syntactic relations from dependency trees. Additionally, we consider not only the relations of opinion targets and opinion words, but also many other types of relations, as we will see in Section 3. In Stoyanov and Cardie (2008), the authors treated target extraction as a topic coreference resolution problem. The key to their approach is to cluster opinions sharing the same target together. They proposed to train a classiﬁer to judge if two opinions are on the same target, which indicates that their approach is supervised. Our work differs from theirs in that our approach is semi-supervised. Other related work on target extraction mainly uses the idea of topic modeling to capture targets in reviews (Mei et al. 2007). Topic modeling is to model the generation of a document set and mine the implied topics in the documents. However, our experiments with topic modeling show that it is only able to ﬁnd some general or coarse topics in texts and represent them as clusters of words. Their aim is thus different from our ﬁne-grained opinion target extraction task. 3. Relation Identiﬁcation As stated previously, identiﬁcation of the relations between opinion words/targets and other opinion words/targets is the key to our opinion lexicon expansion and target 12  Qiu et al.  Opinion Word Expansion and Target Extraction through Double Propagation  extraction methods. In this section, we will describe the relation identiﬁcation in detail. Hereafter, for convenience, we refer to the relations between opinion words and targets as OT-Rel, between opinion words themselves as OO-Rel, and between targets as TT-Rel. In this work, we employ a dependency grammar to describe the relations syntactically. In the dependency grammar, a syntactic relation between two words A and B can be described as A (or B) depends on B (or A). We deﬁne two categories to summarize all possible dependencies between two words in sentences. Deﬁnition 1 (Direct Dependency (DD)) A direct dependency indicates that one word depends on the other word without any additional words in their dependency path (i.e., directly) or they both depend on a third word directly. Some examples are given in Figures 1 (a) and (b). In (a), A depends on B directly and they both depend on H directly in (b). Deﬁnition 2 (Indirect Dependency (IDD)) An indirect dependency indicates that one word depends on the other word through some additional words (i.e., indirectly) or they both depend on a third word through additional words. Some examples are shown in Figures 1 (c) and (d). In (c), A depends on B through H1; in (d), A depends on H through H1 and B depends on H through H2. Actually, IDDs denote all possible relations apart from DDs. Note that DDs and IDDs describe only the topology of all possible dependencies. We then impose some constraints of the Part-of-speech (POS) tags on the opinion words  Figure 1 Different dependencies between words A and B. 13  Computational Linguistics  Volume 37, Number 1  and targets, and also the potential syntactic relations on the dependency path. In this work, we employ the Stanford POS tagging tool1 to do the POS tagging and Minipar2 as the sentence parser. We consider opinion words to be adjectives and targets to be nouns/noun phrases, which has been widely adopted in previous work (Hu and Liu 2004; Popescu and Etzioni 2005; Mei et al. 2007). Thus the potential POS tags for opinion words are JJ (adjectives), JJR (comparative adjectives), and JJS (superlative adjectives), whereas those for targets are NN (singular nouns) and NNS (plural nouns). The dependency relations describing relations between opinion words and targets include mod, pnmod, subj, s, obj, obj2 and desc; and the relations for opinion words and targets themselves contain only the conjunction relation conj. Therefore, we formulate OT-Rel, OO-Rel, or TT-Rel as a quadruple POS(wi), DT, R, POS(wj) , in which POS(wi) is the POS tag of word wi, DT is the dependency type (i.e., DD or IDD), and R is the syntactic relation. The values of POS(wi) and R are listed as described here.  4. Opinion Lexicon Expansion and Target Extraction We perform the opinion lexicon expansion and target extraction tasks iteratively based on propagation using the relations deﬁned herein. To bootstrap the propagation, we only require a seed opinion lexicon. Currently, we focus on one major type of opinionated content, namely, product reviews, in which targets refer to product features. Hereafter, we use target and product feature (or feature for short) interchangeably for convenience. Our extraction approach adopts the rule-based strategy which is quite natural given the well-deﬁned relations. For example, in an opinion sentence Canon G3 takes great pictures, the adjective great is parsed as directly depending on the noun pictures through mod, formulated as a OT-Rel quadruple JJ, DD, mod, NN . If we know great is an opinion word and are given a rule like “a noun on which an opinion word directly depends through mod is taken as the target,” we can easily extract pictures as the target. Similarly, if we know pictures is a target, we could extract the adjective great as an opinion word using a similar rule. Based on such observations, the idea of the whole propagation approach is ﬁrst to extract opinion words and targets using the seed opinion lexicon and then use the newly extracted opinion words and targets for further target and opinion word extraction. The propagation ends until no more new opinion words or targets can be identiﬁed. In this way, even if the seed opinion lexicon is small, targets can still be extracted with high recall (as we will see in the experiments) and at the same time the opinion lexicon is also expanded. In the following sections, we ﬁrst describe the extraction rules in detail and then demonstrate the whole propagation algorithm with a walk-through example to show how the propagation works. For the opinion lexicon expansion, one important issue is to assign sentiment polarities to the newly found opinion words. We propose a novel polarity assignment method to perform this task. In target extraction, we also propose several pruning methods to remove different types of noise introduced during the propagation process. We will describe these methods in Sections 4.3 and 4.4.  
∗ Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: yael.sygal@gmail.com. ∗∗ Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il. Submission received: 3 June 2009; revised submission received: 21 June 2010; accepted for publication: 14 September 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 1  Group 2001; Oepen et al. 2002; Hinrichs, Meurers, and Wintner 2004; Bender et al. 2005; King et al. 2005; Mu¨ ller 2007) in several theoretical frameworks, including TAG (Joshi, Levy, and Takahashi 1975), LFG (Dalrymple 2001), HPSG (Pollard and Sag 1994), and XDG (Debusmann, Duchier, and Rossberg 2005). Grammar development is a complex enterprise: It is not unusual for a single grammar to be developed by a team including several linguists, computational linguists, and computer scientists. The scale of grammars is overwhelming—large-scale grammars can be made up by tens of thousands of line of code (Oepen et al. 2000) and may include thousands of types (Copestake and Flickinger 2000). Modern grammars are written in grammatical formalisms that are often reminiscent of very-high-level, declarative (mostly logical) programming languages, and are thus very similar to computer programs. This raises problems similar to those encountered in large-scale software development (Erbach and Uszkoreit 1990). Although whereas software engineering provides adequate solutions for the programmer, grammar engineering is still in its infancy. In this work we focus on typed uniﬁcation grammars (TUG), and their implementation in grammar-development platforms such as LKB (Copestake 2002), ALE (Carpenter and Penn 2001), TRALE (Meurers, Penn, and Richter 2002), or Grammix (Mu¨ ller 2007). Such platforms conceptually view the grammar as a single entity (even when it is distributed over several ﬁles), and provide few provisions for modular grammar development, such as mechanisms for deﬁning modules that can interact with each other through well-deﬁned interfaces, combination of sub-grammars, separate compilation and automatic linkage of grammars, information encapsulation, and so forth. This is the main issue that we address in this work.1 We provide a preliminary yet thorough and well-founded solution to the problem of grammar modularization. We ﬁrst specify a set of desiderata for a beneﬁcial solution in Section 1.1, and then survey related work, emphasizing the shortcomings of existing approaches with respect to these desiderata. Much of the information in typed uniﬁcation grammars is encoded in the signature, and hence the key is facilitating a modularized development of type signatures. In Section 2 we introduce a deﬁnition of signature modules, and show how two signature modules combine and how the resulting signature module can be extended to a stand-alone type signature. We lift our deﬁnitions from signatures to full grammar modules in Section 3. In Section 4 we use signature modules and their combination operators to work out a modular design of the HPSG grammar of Pollard and Sag (1994), demonstrating the utility of signature modules for the development of linguistically motivated grammars. We then outline MODALE, an implementation of our solutions which supports modular development of type signatures in the context of both ALE and TRALE (Section 5). We show in Section 6 how our solution complies with the desiderata of Section 1.1, and conclude with directions for future research. 1.1 Motivation The motivation for modular grammar development is straightforward. Like software development, large-scale grammar development is much simpler when the task can be cleanly distributed among different developers, provided that well-deﬁned interfaces govern the interaction among modules. From a theoretical point of view, modularity  
Existing speech interfaces have mostly been used to perform a single task, where the user ﬁnishes with one task before moving on to the next. We envision that ∗ Nuance Communications, Inc., 505 First Ave. South, Suite 700, Seattle, WA 98104. E-mail: fan.yang@nuance.com. Submission received: 26 July 2009; revised submission received: 22 July 2010; accepted for publication: 13 October 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 1  next-generation speech interfaces will be able to work with the user on multiple tasks at the same time, which is especially useful for real-time tasks. For instance, a driver in a car might use a speech interface to catch up on e-mails, while occasionally checking upcoming trafﬁc conditions, and receiving navigation instructions; or a police ofﬁcer might need to be alerted to a nearby accident while accessing a database during a routine trafﬁc stop. Several speech interfaces that allow multi-tasking dialogues have been built (e.g., Traum and Rickel 2002; Kun, Miller, and Lenharth 2004; Lemon and Gruenstein 2004; Larsson 2003). However, it is unclear that the mechanisms of managing multiple verbal tasks in these systems resemble human conventions or do the best to help users with task switching. For complex domains, the user might be confused about which task the interface is talking about, or might be confused about where they left off in a task. In order to build a speech interface that supports multi-tasking dialogue, we need to determine a set of conventions that the user and interface can follow in task switching. We propose to start with conventions that are actually used in human–human speech conversations, which are natural for users to follow and probably efﬁcient in problemsolving. Once we understand the human conventions, we can try to implement them in a dialogue manager and run user studies to verify the effectiveness of such conventions in human–computer dialogue. In this article we focus on understanding the human conventions of managing multiple tasks. Multi-tasking dialogues, where multiple independent topics overlap with each other in time, regularly arise in human–human conversation: For example, a driver and a passenger in a car might be talking about their summer plans, while occasionally interjecting road directions or conversation about what music to listen to. However, little is known about how people manage multi-tasking dialogues. Given the scenario where a real-time task with a time constraint arises during the course of an ongoing task, we are specially interested in two switching behaviors: task interruption, which is to suspend the ongoing task and switch to a waiting real-time task, and task resumption, which is to return to the interrupted ongoing task after completing a realtime task. The ﬁrst question we ask is how quickly conversants respond to a real-time task. Intuitively if the real-time task is very urgent (e.g., the driver is about to miss a turn), the passenger might want to immediately cut off the ongoing conversation, and notify the driver of the turn. However, if the real-time task is less urgent, for example, the driver does not like the music and wants the passenger to load another CD, do conversants still immediately interrupt the ongoing conversation? If conversants do vary how quickly they interrupt, are there any regularities of where conversants switch from the ongoing task to the real-time task? We hypothesize that, given the choice, conversants interrupt the ongoing task where the interruption is less disruptive to the ongoing task. The second question we ask is how conversants signal task interruptions. Previous research showed that conversants signal the start of a new topic in single-tasking speech (monologue and dialogue) with discourse markers and prosodic cues. We thus hypothesize that conversants also use these cues to signal task interruptions. We also investigate whether conversants vary the intensity of the cues, and under what circumstances. The third question we ask is what conversants do immediately upon resuming the ongoing task. Switching to a real-time task causes the ongoing task to be temporarily suspended. On completing the real-time task and returning to the ongoing task, do conversants simply continue on from where they were interrupted? We hypothesize that 76  Yang, Heeman, and Kun  Multi-Tasking Dialogues  conversants might sometimes perform certain actions to recover from the interruption. For example, it is imaginable that conversants might ask where were we at for summer plans, and then review what was discussed before the interruption. To answer these questions, we collect the MTD corpus, which consists of a set of human–human dialogues where pairs of conversants have multiple overlapping verbal tasks to perform. In our research, we keep things relatively simple by having conversants talk to each other to play two games on computers. The ﬁrst game, the ongoing task, is a poker game in which conversants need to assemble a poker hand, which usually takes a relatively long time to complete. The second game, the real-time task, is a picture game in which conversants need to ﬁnd out whether they have a certain picture on their displays, which can be done in a couple of turns but has a time constraint. In Section 3, we describe the task setup and corpus collection. In Section 4, we examine when and where conversants suspend the ongoing task and switch to the real-time task. In Section 5, we examine how conversants signal task interruptions. In Section 6, we examine the behavior of context restoration in task resumptions. In addition to the three questions we have asked, in Section 7, we use machine learning to automatically recognize task interruptions. Recognizing task interruptions is an important component in building speech interfaces that support multi-tasking dialogue. For example, the speech interface can accordingly switch the language model when it detects that the user has switched to another task, which should improve speech recognition performance (Iyer and Ostendorf 1999) and utterance understanding, leading to higher user satisfaction (Walker, Passonneau, and Boland 2001). We run machine learning experiments to determine how well we can automatically recognize task interruptions and to understand the utility of the features that we found in our corpus studies. Finally, we conclude the paper in Section 8. This paper includes and extends Heeman et al. (2005), Yang, Heeman, and Kun (2008), and Yang and Heeman (2009) with more corpus data, more robust statistical analysis, more machine learning experiments, and more comprehensive discussions.  2. Related Research 2.1 Existing Systems for Multi-Tasking Dialogues There is some initial research effort in building speech interfaces to support multitasking dialogue. Kun, Miller, and Lenharth (2004) developed a system called Project54, which allowed a user to interact with multiple devices in a police cruiser using speech. The architecture of Project54 allowed for handling multiple tasks overlapped in time. For example, when pulling over a vehicle, an ofﬁcer could ﬁrst issue a spoken command to turn on the lights and siren, then issue spoken commands to initiate a data query, go back to interacting with the lights and siren (perhaps to change the pattern after the vehicle has been pulled over), and ﬁnally receive the spoken results of the data query. This example shows that system responses related to different tasks could be interleaved: The system responded to the data query after the user had already switched back to interacting with the lights and siren. Lemon and Gruenstein (2004) also explored multi-tasking in a speech interface. They built a speech interface for a human operator to direct a robotic helicopter on executing multiple tasks, such as searching for a car and ﬂying to a tower. The interface kept an ordered set of active dialogue tasks, and interpreted the user utterance in terms of the most active task for which the utterance made sense. Conversely, during the 77  Computational Linguistics  Volume 37, Number 1  interface’s turn of speaking, it could produce an utterance for any of the dialogue tasks and thus intermixed utterances from different tasks. In Kun, Miller, and Lenharth (2004) or Lemon and Gruenstein (2004), the systems did not explicitly signal tasks switching, either for task interruptions or for task resumptions, but instead relied on semantic interpretation to determine which task an utterance belonged to. Larsson (2003) built the GoDis system which hard-coded two types of signals when resuming an interrupted conversation. The ﬁrst type of signal was to use the discourse marker so to implicitly signal a topic resumption. The second type of signal was to use the phrase returning to the issue of to explicitly resume an interrupted topic. For example, when searching for the price of an air ticket with GoDis, the user could suspend the system’s question when do you want to travel by interjecting a question do I need a visa. The system, after a short dialogue answering the user’s question about a visa, would resume the ticket booking by returning to the issue of price. Traum and his colleagues (Rickel et al. 2002; Traum and Rickel 2002) developed the Mission Rehearsal Exercise system in which the user and virtual humans collaborated on multiple tasks that could interrupt each other. They created a scenario in which a lieutenant (the user) was sent to a village for an Army peacekeeping task. However, on his way, he encountered an auto accident in which his platoon’s vehicle crashed into a civilian vehicle, injuring a local boy. The boy’s mother and an Army medic were hunched over him, and a sergeant approached the lieutenant to brief him on the situation. These multiple virtual humans could interrupt or be involved in conversations with the lieutenant. The authors proposed and partially implemented a multi-level dialogue manager, with levels for turn-taking, initiative, grounding, topic management, negotiation, and rhetorical structure. In their view, topic management included where one topic is started before an old one is completed. They described how topic shifts in general can be signaled with cue phrases, such as now and anyways, and with nonverbal cues. These research works show the usefulness of a spoken dialogue system being able to handle multiple tasks, and promote a thorough examination of multi-tasking dialogue. In this article we examine the conventions of task switching in human–human dialogue as the ﬁrst step towards understanding the practice of managing tasking switching in a computer dialogue system. 2.2 Insights from Non-Verbal Task Switching Research in cognitive science suggests that task interruptions and resumptions are complicated behavior and warrant investigation. There is extensive research on the disruptiveness of interruptions, in which individuals switch between multiple manualvisual tasks. For example, Gillie and Broadbent (1989) found that the length (in time) of an interruption is not an important factor, but that the real-time task’s complexity and similarity to the ongoing task contribute to the disruptiveness. On the other hand, in their study of checklists, Linde and Goguen (1987) found that it is not the number of interruptions but the length of interruptions that affects the disruptiveness. Cutrell, Czerwinski, and Hovitz (2001) examined the inﬂuence of instant messaging on users performing ongoing computing tasks, and found that interruptions unrelated to the ongoing task resulted in longer task resumptions. Although these results do not appear to always converge on the same conclusions, they suggest that task switching can be disruptive to users. Researchers have been trying to minimize the disruptive effect of task switching in human–computer interaction. McFarlane (1999) explored four alternatives for when 78  Yang, Heeman, and Kun  Multi-Tasking Dialogues  to suspend the ongoing task and switch to the interruption, namely, immediate, negotiated, mediated, and scheduled, and found mixed results. Renaud (2000) argued for, and built, a prototype of a visualization tool to help users restore the context of the ongoing task when returning from an interruption. Hess and Detweiler (1994) and Gopher, Greenshpan, and Armony (1996) found that the disruptive effects are reduced as people gain more experience with interruptions. These studies suggest that it is worthwhile to investigate how a computer dialogue system should manage task switching. 2.3 Insights from Discourse Structure Research Research in discourse structure also sheds light on task switching. It is important to understand the conventions that people use to manage discourse structure as these might also be used for managing multiple tasks. According to Grosz and Sidner (1986), the structure of a discourse is a combination of linguistic structure, intentional structure, and attentional state. The linguistic structure is a hierarchical segmentation of the dialogue. Each segment has a purpose, which is established by the conversant who initiates the segment. The purposes come together to form the intentional structure. The attentional state contains the objects, properties, and relations that are most salient at any point in the dialogue. The attentional state is claimed to work like a stack. When a new segment is started, a new focus space is created on top of the attentional stack. When the segment completes, the focus space is popped off.1 Signaling discourse structure in single-tasking speech is about signaling the boundary of related discourse segments that contribute to the achievement of a discourse purpose. Two types of cues have been identiﬁed. The ﬁrst type is discourse markers (Grosz and Sidner 1986; Schiffrin 1987; Moser and Moore 1995; Passonneau and Litman 1997; Bangerter and Clark 2003). Discourse markers can be used to signal the start of a new discourse segment and its relation to other discourse segments. For example, now might signal moving on to the next topic, and well might signal a negative or unexpected response. The second type of cue is prosody. In read speech, Grosz and Hirschberg (1992) studied broadcast news and found that pause length is the most important factor that indicates a new discourse segment. Ayers (1992) found that pitch range appears to correlate more closely with hierarchical topic structure in read speech than in spontaneous speech. In spontaneous monologue, Butterworth (1972) found that the beginning of a discourse segment exhibits slower speaking rate; Swerts (1995) and Passonneau and Litman (1997) found that pause length correlates with discourse segment boundaries; Hirschberg and Nakatani (1996) found that the beginning of a discourse segment correlates with higher pitch. In human–human dialogue, similar behavior has been observed: The pitch value tends to be higher for starting a new discourse segment (Nakajima and Allen 1993). In human–computer dialogue, Swerts and Ostendorf (1995) found that the ﬁrst utterance of a discourse segment correlates with slower speaking rate and longer preceding pause. Thus, we are interested in whether discourse markers and prosodic cues are also used in signaling task interruptions in multi-tasking dialogue.  
∗ University of Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue, Cambridge, UK. E-mail: yue.zhang@cl.cam.ac.uk. ∗∗ University of Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue, Cambridge, UK. E-mail: stephen.clark@cl.cam.ac.uk. Submission received: 10 November 2009; revised submission received: 12 August 2010; accepted for publication: 20 September 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 1  beam-search. We show that the framework, which is conceptually and computationally simple, is practically effective for structural prediction problems that can be turned into an incremental process, allowing accuracies competitive with the state-of-the-art to be achieved for all the problems we consider. The framework is extremely ﬂexible and easily adapted to each task. One advantage of beam-search is that it does not impose any requirements on the structure of the problem, for example, the optimal sub-problem property required for dynamicprogramming, and can easily accommodate non-local features. The generalized perceptron is equally ﬂexible, relying only on a decoder for each problem and using a trivial online update procedure for each training example. An advantage of the linear perceptron models we use is that they are global models, assigning a score to a complete hypothesis for each problem rather than assigning scores to parts which are then combined under statistical independence assumptions. Here we are following a recent line of work applying global discriminative models to tagging and wide-coverage parsing problems (Lafferty, McCallum, and Pereira 2001; Collins 2002; Collins and Roark 2004; McDonald, Crammer, and Pereira 2005; Clark and Curran 2007; Carreras, Collins, and Koo 2008; Finkel, Kleeman, and Manning 2008). The ﬂexibility of our framework leads to competitive accuracies for each of the tasks we consider. For word segmentation, we show how the framework can accommodate a word-based approach, rather than the standard and more restrictive character-based tagging approaches. For POS-tagging, we consider joint segmentation and POS-tagging, showing that a single beam-search decoder can be used to achieve a signiﬁcant accuracy boost over the pipeline baseline. For Chinese and English dependency parsing, we show how both graph-based and transition-based algorithms can be implemented as beam-search, and then combine the two approaches into a single model which outperforms both in isolation. Finally, for Chinese phrase-structure parsing, we describe a global model for a shift-reduce parsing algorithm, in contrast to current deterministic approaches which use only local models at each step of the parsing process. For all these tasks we present results competitive with the best results in the literature. In Section 2 we describe our general framework of the generic beam-search algorithm and the generalized perceptron. Then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, 2009, 2010), presented in our single coherent framework. We give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework. For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is signiﬁcantly faster with comparable accuracy. For the joint segmentation and POS-tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work (Zhang and Clark 2008a), while being more than an order of magnitude faster. In Section 7 we provide further discussion of the framework based on the studies of the individual tasks. We present the main advantages of the framework, and give an analysis of the main reasons for the high speeds and accuracies achieved. We also discuss how this framework can be applied to a potential new task, and show that the comparability of candidates in the incremental process is an important factor to consider. In summary, we study a general framework for incremental structural prediction, showing how the framework can be tailored to a range of syntactic processing problems to produce results competitive with the state-of-the-art. The conceptual and computational simplicity of the framework, together with its language-independent nature, 106  Zhang and Clark  Syntactic Processing  make it a competitive choice that should be considered for comparison by developers of alternative approaches.  2. The Decoding and Training Framework The framework we study in this article addresses the general structural prediction problem of mapping an input structure x ∈ X onto an output structure y ∈ Y, where X is the set of possible inputs, and Y is the set of possible outputs. For example, for the problem of Chinese word segmentation, X is the set of raw Chinese sentences and Y is the set of all possible segmented Chinese sentences. For the problem of English dependency parsing, X is the set of all English sentences and Y is the set of all possible English dependency trees. Given an input sentence x, the output F(x) is deﬁned as the highest scored among the possible output structures for x:  F(x) = arg max Score(y)  (1)  y∈GEN(x)  where GEN(x) denotes the set of possible outputs for an input sentence x, and Score(y) is some real-valued function on Y. To compute Score(y), the output structure y is mapped into a global feature vector Φ(y) ∈ N d. Here a feature is a count of the occurrences of a certain pattern in an output structure, extracted according to a set of feature templates, and d is the total number of features. The term global feature vector is used by Collins (2002) to distinguish between feature counts for whole sequences and the local feature vectors in maximum entropy tagging models, which are boolean-valued vectors containing the indicator features for one element in the sequence (Ratnaparkhi 1998). Having deﬁned the feature vector, Score(y) is computed using a linear model:  Score(y) = Φ(y) · w  (2)  where w ∈ Rd is the parameter vector of the model, the value of which is deﬁned by  supervised learning using the generalized perceptron.  For the general framework we study in this article, the output y is required to be  built through an incremental process. Suppose that K incremental steps are taken in  total to build y. The incremental change at the ith step (0 < i ≤ K) can be written as  δ(y, i). For word segmentation, δ(y, i) can be an additional character added to the output;  for shift-reduce parsing, δ(y, i) can be an additional shift-reduce action. Denoting the  change to the global feature vector at the incremental step as Φ(δ(y, i)), the global feature  vector Φ(y) can be written as Φ(y) =  K i=1  Φ(δ(y,  i)).  Hence,  Score(y)  can  be  computed  incrementally by  K  Score(y) = Φ(δ(y, i)) · w  (3)  i=1  In the following sections, we describe the two major components of the general framework, that is, the general beam-search algorithm for ﬁnding F(x) = arg maxy∈GEN(x) Score(y) for a given x, and the generalized perceptron for training w.  107  Computational Linguistics  Volume 37, Number 1  2.1 Beam-Search Decoding Given an input x, the output structure y is built incrementally. At each step, an incremental sub-structure is added to the partially built output. Due to structural ambiguity, different sub-structures can be built. Taking POS-tagging for example, the incremental sub-structure for each processing step can be a POS-tag assigned to the next input word. Due to structural ambiguity, different POS-tags can be assigned to a word, and the decoding algorithm searches for the particular path of incremental steps which builds the highest scored output. We present a generic beam-search algorithm for our decoding framework, which uses an agenda to keep the B-best partial outputs at each incremental step. The partially built structures, together with useful additional information, are represented as a set of state items. Additional information in a state item is used by the decoder to organize the current structures or keep a record of the incremental process. For POS-tagging it includes the remaining input words yet to be assigned POS-tags; for a shift-reduce parser, it includes the stack structure for the shift-reduce process and the incoming queue of unanalyzed words. The agenda is initialized as empty, and the state item that corresponds to the initial structure is put onto it before decoding starts. At each step during decoding, each state item from the agenda is extended with one incremental step. When there are multiple choices to extend one state item, multiple new state items are generated. The new state items generated at a particular step are ranked by their scores, and the B-best are put back onto the agenda. The process iterates until a stopping criterion is met, and the current best item from the agenda is taken as the output. Pseudo code for the generic beam-search algorithm is given in Figure 1, where the variable problem represents a particular task, such as word segmentation or dependency parsing, and the variable candidate represents a state item, which has a different deﬁnition for each task. For example, for the segmentation task, a candidate is a pair, consisting of the partially segmented sentence and the remaining character sequence yet to be segmented. The agenda is an ordered list, used to keep all the state items generated at each stage, ordered by score. The variable candidates is the set of state items that can be used to generate new state items, that is, the B-best state items from the previous stage. B is the number of state items retained at each stage.  function BEAM-SEARCH(problem, agenda, candidates, B) candidates ← {STARTITEM(problem)} agenda ← CLEAR(agenda) loop do for each candidate in candidates agenda ← INSERT(EXPAND(candidate, problem), agenda) best ← TOP(agenda) if GOALTEST(problem, best) then return best candidates ← TOP-B(agenda, B) agenda ← CLEAR(agenda) Figure 1 The generic beam-search algorithm. 108  Zhang and Clark  Syntactic Processing  STARTITEM initializes the start state item according to the problem; for example, for the segmentation task, the start state item is a pair consisting of an empty segmented sentence and the complete sequence of characters waiting to be segmented. CLEAR removes all items from the agenda. INSERT puts one or more state items onto the agenda. EXPAND represents an incremental processing step, which takes a state item and generates new state items from it in all possible ways; for example, for the segmentation task, EXPAND takes the partially segmented sentence in a state item, and extends it in all possible ways using the ﬁrst character in the remaining character sequence in the state item. TOP returns the highest scoring state item on the agenda. GOALTEST checks whether the incremental decoding process is completed; for example, for the segmentation task, the process is completed if the state item consists of a fully segmented sentence and an empty remaining character sequence. TOP-B returns the Bhighest scoring state items on the agenda, which are used for the next incremental step. State items in the agenda are ranked by their scores. Suppose that K incremental steps are taken in total to build an output y. At the ith step (0 < i ≤ K), a state item in the agenda can be written as candidatei, and we have i Score(candidatei) = Φ(δ(candidatei, n)) · w n=1 Features for a state item can be based on both the partially built structure and the additional information we mentioned earlier. The score of a state item can be computed incrementally as the item is built. The score of the start item is 0. At the ith step (0 < i ≤ K), a state item candidatei is generated by extending an existing state item candidatei−1 on the agenda with δ(candidatei, i). In this case, we have Score(candidatei) = Score(candidatei−1) + Φ(δ(candidatei, i)) · w Therefore, when a state item is extended, its score can be updated by adding the incremental score of the step Φ(δ(candidatei, i)) · w. The nature of the scoring function means that, given appropriately deﬁned features, it can be computed efﬁciently for both the incremental decoding and training processes. Because the correct item can fall out of the agenda during the decoding process, the general beam-search framework is an approximate decoding algorithm. Nevertheless, empirically this algorithm gives competitive results on all the problems in this article. 2.2 The Generalized Perceptron The perceptron learning algorithm is a supervised training algorithm. It initializes the parameter vector as all zeros, and updates the vector by decoding the training examples. For each example, the output structure produced by the decoder is compared with the correct structure. If the output is correct, no update is performed. If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output. Intuitively, the training process is effectively coercing the decoder to produce the correct output for each training example. The algorithm can perform multiple passes over the same training sentences. In all experiments, we decide the number of training iterations using a set of development test data, by choosing the number that gives the highest 109  Computational Linguistics  Volume 37, Number 1  Inputs: training examples (xi, yi) Initialization: set w = 0 Algorithm: for r = 1..P, i = 1..N calculate zi = decode(xi) if zi = yi w = w + Φ(yi) − Φ(zi) Outputs: w Figure 2 The generalized perceptron algorithm, adapted from Collins (2002).  development test accuracy as the ﬁnal number in testing. Figure 2 gives the algorithm, where N is the number of training sentences and P is the number of passes over the data. The averaged perceptron algorithm (Collins 2002) is a standard way of reducing overﬁtting on the training data. It was motivated by the voted-perceptron algorithm (Freund and Schapire 1999) and has been shown to give improved accuracy over the non-averaged perceptron on a number of tasks. Let N be the number of training sentences, P the number of training iterations, and wi,r the parameter vector immediately after the ith sentence in the rth iteration. The averaged parameter vector γ ∈ Rd is deﬁned as  γ  =  
∗∗ The Interaction Lab, School of Mathematical and Computer Sciences (MACS), Heriot-Watt University, Edinburgh EH14 4AS, UK. E-mail: o.lemon@hw.ac.uk. Submission received: 23 January 2009; revised submission received: 13 August 2010; accepted for publication: 13 September 2010. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 1  provably optimal action policies, a precise mathematical model for action selection, possibilities for generalization to unseen states, and automatic optimization of competing trade-offs in the objective function. See Young (2000), Lemon and Pietquin (2007), and Frampton and Lemon (2009) for an introduction to dialogue strategy learning. One of the major limitations of this approach is that it relies on a large quantity of data being available. In cases when a ﬁxed data set is used for learning (e.g., Walker 2000; Singh et al. 2002; Henderson, Lemon, and Georgila 2008), the optimal policy can only be discovered when it is present in the data set. (Note, by a policy being “present in a data set” we mean that the set of state-action mappings which deﬁne the policy is contained in that data set. When a policy is not present in a data set, either some states covered by the policy are not seen at all in that data, or the actions chosen by the policy in some states are different to those seen in the data.) To overcome this problem, simulated learning environments are being used to explore optimal policies which were previously unseen in the data (e.g., Eckert, Levin, and Pieraccini 1997; Ai, Tetreault, and Litman 2007; Young et al. 2009). However, several aspects of the components of this simulated environment are usually hand-crafted, and thus limit the scope of policy learning. In particular, the optimization (or reward) function is often manually set (Paek 2006). In order to build simulation components from real data, annotated in-domain dialogue corpora have to be available which explore a range of dialogue management decisions. Collecting dialogue data without a working prototype is problematic, leaving the developer with a classic “chicken-or-egg” problem. We therefore propose to learn dialogue strategies using simulation-based RL, where the simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data. In a WOZ experiment, a hidden human operator, the so-called “wizard,” simulates (partly or completely) the behavior of the application, while subjects are left in the belief that they are interacting with a real system (Fraser and Gilbert 1991). In contrast to preceding work, our approach enables strategy learning in domains where no prior system is available. Optimized learned strategies are then available from the ﬁrst moment of on-line operation, and handcrafting of dialogue strategies is avoided. This independence from large amounts of in-domain dialogue data allows researchers to apply RL to new application areas beyond the scope of existing dialogue systems. We call this method “bootstrapping.” In addition, our work is the ﬁrst using a data-driven simulated environment. Previous approaches to simulation-based dialogue strategy learning usually handcraft some of their components. Of course, some human effort is needed in developing the WOZ environment and annotating the collected data, although automatic dialogue annotation could be applied (Georgila et al. 2009). The alternative—collecting data using hand-coded dialogue strategies—would still require annotation of the user actions, and has the disadvantage of constraining the system policies explored in the collected data. Therefore, WOZ data allows exploration of a range of possible strategies, as intuitively generated by the wizards, in contrast to using an initial system which can only explore a pre-deﬁned range of options. However, WOZ experiments usually only produce a limited amount of data, and the optimal policy is not likely to be present in the original small data set. Our method shows how to use these data to build a simulated environment in which optimal policies can be discovered. We show this advantage by comparing RL-based strategy against a supervised strategy which captures average human wizard performance on the dialogue task. This comparison allows us to measure relative improvement over the training data. 154  Rieser and Lemon  Learning and Evaluation of Dialogue Strategies for New Applications  The use of WOZ data has earlier been proposed in the context of RL. Williams and Young (2004) use WOZ data to discover the state and action space for the design of a Markov Decision Process (MDP). Prommer, Holzapfel, and Waibel (2006) use WOZ data to build a simulated user and noise model for simulation-based RL. Although both studies show promising ﬁrst results, their simulated environments still contain many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. Schatzmann et al. (2007) propose to “bootstrap” with a simulated user which is entirely hand-crafted. In the following we propose what is currently the most strongly data-driven approach to these problems. We also show that the resulting policy performs well for real users. In particular we propose a ﬁve-step procedure (see Figure 1): 1. We start by collecting data in a WOZ experiment, as described in Section 2. 2. From these data we train and test different components of our simulated environment using Supervised Learning techniques (Section 3). In  Figure 1 Data-driven methodology for simulation-based dialogue strategy learning for new applications. 155  Computational Linguistics  Volume 37, Number 1  particular, we extract a supervised policy, reﬂecting human (wizard) performance on this task (see Section 3.3). We build a noise simulation (Section 3.4), and two different user simulations (Section 3.5), as well as a data-driven reward function (Section 3.6). 3. We then train and evaluate dialogue policies by interacting with the simulated environment (Section 4). 4. Once the learned policies are “good enough” in simulation, we test them with real users (Section 5). 5. In addition, we introduce a ﬁnal phase where we meta-evaluate the whole framework (Section 6). This ﬁnal step is necessary because WOZ experiments only simulate human–computer interaction (HCI). We therefore need to show that a strategy bootstrapped from WOZ data indeed transfers to real HCI. We ﬁrst show that the results between simulated and real interaction are compatible (Section 6.1). We also meta-evaluate the reward function, showing that it is a stable, accurate estimate for real user’s preferences (Section 6.2). Note that RL is fundamentally different to Supervised Learning (SL): RL is a statistical planning approach which allows us to ﬁnd an optimal policy (sequences of actions) with respect to an overall goal (Sutton and Barto 1998); SL, in contrast, is concerned with deducing a function from training data for predicting/classifying events. This article is not concerned with showing differences between SL and RL on a small amount of data, but we use SL methods to capture the average human wizard strategy in the original data, and show that simulation-based RL is able to ﬁnd new policies that were previously unseen. We apply this framework to optimize multimodal information-seeking dialogue strategies for an in-car digital music player. Dialogue Management and multimodal output generation are two closely interrelated problems for information seeking dialogues: the decision of when to present information depends on how many pieces of information to present and the available options for how to present them, and vice versa. We therefore formulate the problem as a hierarchy of joint learning decisions which are optimized together. We see this as a ﬁrst step towards an integrated statistical model of Dialogue Management and more advanced output planning/Natural Language Generation (Lemon 2008; Rieser and Lemon 2009b; Lemon 2011; Rieser, Lemon, and Liu 2010; Janarthanam and Lemon 2010). In the following, Section 2 describes the Wizard-of-Oz data collection (i.e., how to collect appropriate data when no initial data or system exists), Section 3 explains the construction of the simulated learning environment (including how to determine a data-driven reward function), Section 4 presents training and evaluation of the learned policies in simulation (i.e., how to learn effective dialogue strategies), Section 5 presents the results of the tests with real users, and Section 6 presents a meta-evaluation of the framework, including transfer results.  2. Wizard-of-Oz Data Collection The corpus used for learning was collected in a multimodal study of German taskoriented dialogues for an in-car music player application. The corpus was created 156  Rieser and Lemon  Learning and Evaluation of Dialogue Strategies for New Applications  in the larger context of the TALK project1 and is also known as the SAMMIE corpus (Kruijff-Korbayova´ et al. 2006). In contrast to conventional WOZ trials we were not only interested in the users’ behavior, but also in the behavior of our human wizards. This study provides insights into natural strategies of information presentation as performed by human wizards.  2.1 Experimental Setup Six people played the role of an intelligent interface (the “wizards”). The wizards were able to speak freely and display search results on the screen by clicking on precomputed templates. Wizards’ outputs were not restricted, in order to explore the different ways they intuitively chose to present search results. Wizard’s utterances were immediately transcribed and played back to the user with Text-To-Speech. Twenty-one subjects (11 women, 10 men) were given a set of predeﬁned tasks to perform, as well as a primary driving task, using a driving simulator (Mattes 2003). The users were able to speak, as well as make selections on the screen. The experiment proceeded as follows. First the wizards were trained to use the database interface and they were also given general instructions about how to interact with the user. Training took 45 minutes, including ﬁve example tasks. After the user arrived s/he was introduced to the driving simulator and had to perform a short test drive. The users solved two sets of tasks with two tasks in each. After each task the user ﬁlled out a task-speciﬁc questionnaire, in which they indicated perceived task success and satisfaction on a ﬁve-point Likert scale. Finally, the user was interviewed by the experiment leader following a questionnaire containing questions similar to the PARADISE study (Walker, Kamm, and Litman 2000), including questions on task ease, timing, multimodal and verbal presentation, as well as future use of such systems. All subjects reported that they were convinced that they were interacting with a real system. To approximate speech recognition errors we used a tool that randomly deletes parts of the transcribed utterances. Due to the fact that humans are able to make sense of even heavily corrupted input, this method not only covers non-understandings, but wizards also built up their own hypotheses about what the user really said, which can lead to misunderstandings. The word deletion rate varied: 20% of the utterances were weakly corrupted (= deletion rate of 20%), and 20% were strongly corrupted (= deletion rate of 50%). In 60% of the cases the wizard saw the transcribed speech uncorrupted. Example (1) illustrates the kind of corrupted utterances the wizard had to deal with. (1) uncorrupted: “Zu dieser Liste bitte Track ‘Tonight’ hinzufu¨ gen.” [ “Add track ‘Tonight’ to this list.”] weakly corrupted: “Zu dieser Liste bitte Track ‘Tonight’ . . . . ” [“. . . track ‘Tonight’ to this list.”] strongly corrupted: “Zu . . . Track ‘Tonight’ . . . . [“. . . track ‘Tonight’ to . . . ”] There are some shortcomings of this technique, which we discuss in Rieser and Lemon (2009a). However, the data are useful for our purposes because our main interest  
© 2011 Association for Computational Linguistics  Computational Linguistics  w1: w2: w0w1w2:  [A, x0, x1] [B, x1, x2] [S, x0, x2]  Figure 1 CFG parsing in weighted deduction notation.  Volume 37, Number 1  variables range over positions in the input string. In order to determine complexity in the length n of the input string, it is sufﬁcient to count the number of unique position variables in each rule. If all rules have at most k position variables, M = O(nk), and parsing takes time O(nk) in the length of the input string. In the remainder of this article, we will explore methods for minimizing k, the largest number of position variables in any rule, among equivalent deduction systems. These methods directly minimize the parsing complexity of the resulting deduction system. Although we will assume no cyclic dependencies among rule instantiations for the majority of the article, we will discuss the cyclic case in Section 2.2. It is often possible to improve the computational complexity of a deduction rule by decomposing the computation into two or more new rules, each having a smaller number of variables than the original rule. We refer to this process as factorization. One straightforward example of rule factorization is the binarization of a CFG, as shown in Figure 2. Given a deduction rule for a CFG rule with r nonterminals on the righthand side, and a total of r + 1 variables, an equivalent set of rules can be produced, each with three variables, storing intermediate results that indicate that a substring of the original rule’s righthand side has been recognized. This type of rule factorization produces an O(n3) parser for any input CFG. Another well-known instance of rule factorization is the hook trick of Eisner and Satta (1999), which reduces the complexity of parsing for bilexicalized CFGs from O(n5) to O(n4). The basic rule for bilexicalized parsing combines two CFG constituents marked with lexical heads as shown in Figure 3a. Here items with type C indicate constituents, with [C, x0, h, x1] indicating a constituent extending from position x0 to position x1, headed by the word at position h. The item [D, m → h] is used to indicate the weight assigned by the grammar to a bilexical dependency headed by the word at  a) w1: w2: w3: w4: w0w1w2w3w4:  [A, x0, x1] [B, x1, x2] [C, x2, x3] [D, x3, x4] [S, x0, x4]  b)  w1: w2: w1w2:  [A, x0, x1] [B, x1, x2] [X, x0, x2]  w5: w3: w3w5:  [X, x0, x2] [C, x2, x3] [Y, x0, x3]  w6: w3: w0w3w6:  [Y, x0, x3] [D, x3, x4] [S, x0, x3]  Figure 2 Binarization of the CFG rule S → A B C D as rule factorization: The deduction rule above can be factored into the three equivalent rule below.  232  Gildea  Grammar Factorization by Tree Decomposition  a) w: w1: w2: w w1w2:  [D, m → h] [C, x0, h, x1] [C, x1, m, x2] [C, x0, h, x2]  b) w: w2: w w2:  [D, m → h] [C, x1, m, x2] [H, h, x1, x2]  wh: w1: whw1:  [H, h, x1, x2] [C, x0, h, x1] [C, x0, h, x2]  Figure 3 Rule factorization for bilexicalized parsing.  position h with the word at position m as a modiﬁer. The deduction rule is broken into two steps, one which includes the weight for the bilexical grammar rule, and another which identiﬁes the boundaries of the new constituent, as shown in Figure 3b. The hook trick has also been applied to Tree Adjoining Grammar (TAG; Eisner and Satta 2000), and has been generalized to improve the complexity of machine translation decoding under synchronous context-free grammars (SCFGs) with an n-gram language model (Huang, Zhang, and Gildea 2005). Rule factorization has also been studied in the context of parsing for SCFGs. Unlike monolingual CFGs, SCFGs cannot always be binarized; depending on the permutation between nonterminals in the two languages, it may or may not be possible to reduce the rank, or number of nonterminals on the righthand side, of a rule. Algorithms for ﬁnding the optimal rank reduction of a speciﬁc rule are given by Zhang and Gildea (2007). The complexity of synchronous parsing for a rule of rank r is O(n2r+2), so reducing rank improves parsing complexity. Rule factorization has also been applied to Linear Context-Free Rewriting Systems (LCFRS), which generalize CFG, TAG, and SCFG to deﬁne a rewriting system where nonterminals may have arbitrary fan-out, which indicates the number of continuous spans that a nonterminal accounts for in the string (Vijay-Shankar, Weir, and Joshi 1987). Recent work has examined the problem of factorization of LCFRS rules in order to reduce rank without increasing grammar fan-out (Go´ mez-Rodr´ıguez et al. 2009), as well as factorization with the goal of directly minimizing the parsing complexity of the new grammar (Gildea 2010). We deﬁne factorization as a process which applies to rules of the input grammar independently. Individual rules are replaced with an equivalent set of new rules, which must derive the same set of consequent items as the original rule given the same antecedent items. While new intermediate items of distinct types may be produced, the set of items and weights derived by the original weighted deduction system is unchanged. This deﬁnition of factorization is broad enough to include all of the previous examples, but does not include, for example, the fold/unfold operation applied to grammars by Johnson (2007) and Eisner and Blatz (2007). Rule factorization corresponds to the unfold operation of fold/unfold. If we allow unrestricted transformations of the input deduction system, ﬁnding the most efﬁcient equivalent system is undecidable; this follows from the fact that it is undecidable whether a CFG generates the set of all strings (Bar-Hillel, Perles, and Shamir 1961), and would therefore be recognizable in constant time. Whereas the fold/unfold operation of Johnson (2007) and Eisner and Blatz (2007) speciﬁes a narrower class of  233  Computational Linguistics  Volume 37, Number 1  grammar transformations, no general algorithms are known for identifying an optimal series of transformations in this setting. Considering input rules independently allows us to provide algorithms for optimal factorization. In this article, we wish to provide a general framework for factorization of deductive parsing systems in order to minimize computational complexity. We show how to apply the graph-theoretic property of treewidth to the factorization problem, and examine the question of whether efﬁcient algorithms exist for optimizing the parsing complexity of general parsing systems in this framework. In particular, we show that the existence of a polynomial time algorithm for optimizing the parsing complexity of general LCFRS rules would imply an improved approximation algorithm for the wellstudied problem of treewidth of general graphs. 
Chapter 6 gives a brief history of mechanized speech and language technology. It begins with a detailed description of early attempts at speech synthesis and typewriters, and then discusses telegraphy, computer encoding of text, and Braille. Chapter 7 then moves to an overview of modern speech technology including both recognition and  Computational Linguistics  Volume 37, Number 1  synthesis. This section presents the state of the art without losing the reader in technical details. It is very clearly written, well referenced, and authoritative. Chapter 8 is mainly about machine translation. It combines a balanced historical discussion of research and development in machine translation with a very accessible introduction to statistical machine translation. It is a pity that the same level of detail couldn’t have been extended to other ﬁelds of natural language processing, such as information retrieval and question answering. The book ﬁnishes with a discussion of the future of language technology. The ﬁrst section of the closing chapter predicts how language technology, especially speech technology, will improve in the near future. This section includes a good discussion of the importance of investment in developing technology—not everything that could be done will be; the greatest improvement will be seen in those areas in which people are prepared to invest time and money. It was nice to see this often neglected topic addressed. The ﬁnal section talks about the social implications of language technology, warning that they are not all positive. Overall the book is a pleasure to read,1 with well-chosen illustrations, nice examples, illuminating illustrations, and numerous examples of different writing systems. The author is clearly knowledgeable about writing systems, especially the decipherment of scripts with little extant text, as well as speech processing, his main ﬁeld of research. These sections are very clearly written, well referenced, and authoritative. The sections on the social implications are more speculative. The book’s weakness is that the coverage is very idiosyncratic. For example, there is almost no discussion of printing, which is surely one of the great language technologies. In contrast, several fairly minor topics, such as Blissymbolics and the Phaistos disc, are treated very thoroughly. In particular, there is no discussion on the bandwidth and compressibility of various scripts and almost nothing on computer-mediated communication and its effect on language. For a book on language and technology published in 2010, this is a disappointment. The structure is also a little inconsistent: Chapters 3 and 6 have summaries, Chapter 7 has a synopsis, and the rest just ﬁnish. In summary, Sproat’s book is an enjoyable collection of essays on language, technology, and society, but still leaves room for a more deﬁnitive tome. Francis Bond is an Associate Professor in the Division of Linguistics and Multilingual Studies, Nanyang Technological University, Singapore. His main research interests are in natural language understanding, machine translation, HPSG, and WordNets. Previously he was a researcher at Nippon Telegraph and Telephone (NTT) and the National Institute of Information and Communications Technology, Japan (NICT). E-mail: bond@ieee.org.  
In an earlier Last Words piece, Ken Church (Church 2006) pointed out how the ACL conference reviewing process can be derailed by the lack of positive endorsement by reviewers who are not well qualiﬁed to review a given paper. He went on to suggest that papers rejected by NAACL are “often strong contenders for the best-paper award at ACL.” An instance of this phenomenon was observed in 2009, when a paper rejected from NAACL 2009 with an average acceptance score of 2.3 out of 5 was given a best paper award at ACL 2009 (Branavan et al. 2009).1 It is especially hard to ﬁnd qualiﬁed reviewers these days partly because computational linguistics has become increasingly specialized. Papers in ﬁelds like parsing and machine translation involve very technical modiﬁcations to a few current models. Reviewers for such areas need to be ‘insiders’, well-versed in the latest developments in the sub-area. This need is likely to become more pronounced as the specialization trend continues. Reviewers are currently selected based on informal social networks. Unfortunately, most researchers do not have an extensive set of names of reviewers at hand, and relying ∗ The MITRE Corporation, 202 Burlington Road, Bedford, MA 01730, USA. E-mail: imani@mitre.org. 1 Such discrepancies in judgments across conferences are not conﬁned to computational linguistics; for example, the classic Page Rank paper from WWW 1998 (Brin and Page 1998) had previously been rejected by SIGIR 1998. © 2011 Association for Computational Linguistics  Computational Linguistics  Volume 37, Number 1  on personal connections (not to mention memories of reviewers’ prior performance) is limiting and could bias the selection of reviewers to those who share a particular point-of-view. This lack of information as to whom to contact can result in woefully inappropriate selections of reviewers. 2.2 The Lack of Quality Control Even when qualiﬁed reviewers can be found, reviews are often hurried. At the 2009 ACL Business Meeting, Ido Dagan pointed to the growing dissatisfaction with the quality of conference reviewing, adding that the problems seemed to be exacerbated by increasing the number of reviewers. The lack of quality is in part due to the large number of conferences that compete for the reviewer’s time. As Fortnow (2009) observes, in the case of computer science conferences the intensive time commitment required for reviewing makes it less likely that more experienced researchers will sign on as reviewers. Such hurried reviewing and decision-making can result in a preference for safe, more incremental papers rather than those that develop new models and research directions (Fortnow 2009). 
WikiNetTK is a Java-based open-source toolkit for facilitating the interaction with and the embedding of world knowledge in NLP applications. For user interaction we provide a visualization component, consisting of graphical and textual browsing tools. This allows the user to inspect the knowledge base to which WikiNetTK is applied. The application-oriented part of the toolkit provides various functionalities: access to various types of information in the knowledge base as well as methods for computing association paths and relatedness measures. The system is applied to a large-scale multilingual concept network obtained by extracting and combining various sources of information from Wikipedia. 
The Linguist’s Assistant (LA) is a practical computational paradigm for describing languages. LA seeks to specify in semantic representations a large subset of possible written communication. These semantic representations then become the starting point and organizing principle from which a linguist describes the linguistic surface forms of a language using LA's visual lexicon and grammatical rule development interface. The resulting computational description can then be used in our document authoring and translation applications. 
This paper aims at presenting TTC TermSuite: a tool suite for multilingual terminology extraction from comparable corpora. This tool suite offers a userfriendly graphical interface for designing UIMA-based tool chains whose components (i) form a functional architecture, (ii) manage 7 languages of 5 different families, (iii) support standardized ﬁle formats, (iv) extract single- and multi- word terms languages by languages (v) and align them by pairs of languages. 
We present a method for characterizing a research work in terms of its focus, domain of application, and techniques used. We show how tracing these aspects over time provides a novel measure of the inﬂuence of research communities on each other. We extract these characteristics by matching semantic extraction patterns, learned using bootstrapping, to the dependency trees of sentences in an article’s abstract. We combine this information with pre-calculated article-to-community assignments to study the inﬂuence of a community on others in terms of techniques borrowed and the ‘maturing’ of some communities to solve other problems. As a case study, we show how the computational linguistics community and its sub-ﬁelds have changed over the years with respect to their foci, methods used, and domain problems. For instance, we show that part-of-speech tagging and parsing have increasingly been adopted as tools for solving problems in other domains. We also observe that speech recognition and probability theory have had the most seminal inﬂuence. 
This paper presents a new task, learning logical structures of paragraphs in legal articles, which is studied in research on Legal Engineering (Katayama, 2007). The goals of this task are recognizing logical parts of law sentences in a paragraph, and then grouping related logical parts into some logical structures of formulas, which describe logical relations between logical parts. We present a two-phase framework to learn logical structures of paragraphs in legal articles. In the ﬁrst phase, we model the problem of recognizing logical parts in law sentences as a multi-layer sequence learning problem, and present a CRF-based model to recognize them. In the second phase, we propose a graph-based method to group logical parts into logical structures. We consider the problem of ﬁnding a subset of complete sub-graphs in a weighted-edge complete graph, where each node corresponds to a logical part, and a complete sub-graph corresponds to a logical structure. We also present an integer linear programming formulation for this optimization problem. Our models achieve 74.37% in recognizing logical parts, 79.59% in recognizing logical structures, and 55.73% in the whole task on the Japanese National Pension Law corpus. 
Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to extract the pre-ordering rules from word-aligned HPSG-tree-tostring pairs and 2) a bottom-up algorithm to apply the extracted rules to HPSG trees to yield target language style source sentences. Experimental results are reported for large-scale English-to-Japanese translation, showing signiﬁcant improvements of BLEU score compared with the baseline SMT systems. 
How well can a phrase translation model perform if we permute the source words to ﬁt target word order as perfectly as word alignment might allow? And how well would it perform if we limit the allowed permutations to ITGlike tree-transduction operations on the source parse tree? First we contribute oracle results showing great potential for performance improvement by source-reordering, ranging from 1.5 to 4 BLEU points depending on language pair. Although less outspoken, the potential of tree-based source-reordering is also significant. Our second contribution is a source reordering model that works with two kinds of tree transductions: the one permutes the order of sibling subtrees under a node, and the other ﬁrst deletes layers in the parse tree in order to exploit sibling permutation at the remaining levels.The statistical parameters of the model we introduce concern individual tree transductions conditioned on contextual features of the tree resulting from all preceding transductions. Experiments in translating from English to Spanish/Dutch/Chinese show signiﬁcant improvements of respectively 0.6/1.2/2.0 BLEU points. 
Lexicalized reordering models play a central role in phrase-based statistical machine translation systems. Starting from the distance-based reordering model, improvements have been made by considering adjacent words in word-based models, adjacent phrases pairs in phrasebased models, and ﬁnally, all phrases pairs in a sentence pair in the reordering graphs. However, reordering graphs treat all phrase pairs equally and fail to weight the relationships between phrase pairs. In this work, we propose an extension to the reordering models, named weighted reordering models, that allows discriminative behavior to be deﬁned in the estimation of the reordering model orientations. We apply our extension using the weighted alignment matrices to weight phrase pairs, based on the consistency of their alignments, and deﬁne a distance metric to weight relationships between phrase pairs, based on their distance in the sentence. Experiments on the IWSLT 2010 evaluation dataset for for the Chinese-English language pair yields an improvement of 0.38 (2%) and 0.94 (3.7%) BLEU points over the state-of-the-art work’s results using weighted alignment matrices. 
Temporal relations between events is a valuable source of information which can be used in a large number of natural language processing applications such as question answering, summarization, and information extraction. Supervised temporal relation classiﬁcation requires large corpora which are difﬁcult, time consuming, and expensive to produce. Active learning strategies are well-suited to reduce this effort by efﬁciently selecting the most informative samples for labeling. This paper presents novel active learning strategies based on support vector machines (SVM) for temporal relation classiﬁcation. A large number of empirical comparisons of different active learning algorithms and various kernel functions in SVM shows that proposed active learning strategies are effective for the given task. 
Sparse learning framework, which is very popular in the ﬁeld of nature language processing recently due to the advantages of efﬁciency and generalizability, can be applied to Conditional Random Fields (CRFs) with L1 regularization method. Stochastic gradient descent (SGD) method has been used in training L1-regularized CRFs, because it often requires much less training time than the batch training algorithm like quasi-Newton method in practice. Nevertheless, SGD method sometimes fails to converge to the optimum, and it can be very sensitive to the learning rate parameter settings. We present a two-stage training algorithm which guarantees the convergence, and use heuristic line search strategy to make the ﬁrst stage of SGD training process more robust and stable. Experimental evaluations on Chinese word segmentation and name entity recognition tasks demonstrate that our method can produce more accurate and compact model with less training time for L1 regularization. 
We present a novel topic modeling approach to sentiment analysis for documents organized into hierarchical categories. In our approach, positive, negative, and subject matter topics are learned and used to infer document labels. A Markov chain Monte Carlo model procedure adapts the number and structure of topics based on a minimum description length objective function. We apply our approach to Yelp.com business reviews and Amazon.com book reviews and demonstrate that 1) the model adaptation procedure selects a high quality model from the space of alternatives, and 2) the resulting model performs well relative to state of the art regression and topic modeling approaches. 
This paper focuses on examining the effect of extra-linguistic information, such as eye gaze, integrated with linguistic information on multi-modal reference resolution. In our evaluation, we employ eye gaze information together with other linguistic factors in machine learning, while in prior work such as Kelleher (2006) and Prasov and Chai (2008) the incorporation of eye gaze and linguistic clues was heuristically realised. Conducting our empirical evaluation using a data set extended the REX-J corpus (Spanger et al., 2010) including eye gaze information, we examine which types of clues are useful on these three data sets, which consist largely of pronouns, nonpronouns and both respectively. Our results demonstrate that a dynamically moving visible indicator within the computer display (e.g. a mouse cursor) contributes to reference resolution for pronouns, while eye gaze information is more useful for the resolution of non-pronouns. 
Event coreference is an important and complicated task in cascaded event template extraction and other natural language processing tasks. Despite its importance, it was merely discussed in previous studies. In this paper, we present a globally optimized coreference resolution system dedicated to various sophisticated event coreference phenomena. Seven resolvers for both event and object coreference cases are utilized, which include three new resolvers for event coreference resolution. Three enhancements are further proposed at both mention pair detection and chain formation levels. First, the object coreference resolvers are used to effectively reduce the false positive cases for event coreference. Second, A revised instance selection scheme is proposed to improve link level mention-pair model performances. Last but not least, an efficient and globally optimized graph partitioning model is employed for coreference chain formation using spectral partitioning which allows the incorporation of pronoun coreference information. The three techniques contribute to a significant improvement of 8.54% in B3 F-score for event coreference resolution on OntoNotes 2.0 corpus. 
The phrase based systems for machine translation are limited by the phrases that they see during the training. For highly inﬂected languages, it is uncommon to see all the forms of a word in the parallel corpora used during training. This problem is ampliﬁed for verbs in highly inﬂected languages where the correct form of the word depends on factors like gender, number and tense aspect. We propose a solution to augment the phrase table with all possible forms of a verb for improving the overall accuracy of the MT system. Our system makes use of simple stemmers and easily available monolingual data to generate new phrase table entries that cover the different variations seen for a verb. We report signiﬁcant gains in BLEU for English to Hindi translation. 
This paper addresses the problem of predicting the pronunciation of Japanese text. The difﬁculty of this task lies in the high degree of ambiguity in the pronunciation of Japanese characters and words. Previous approaches have either considered the task as a word-level classiﬁcation problem based on a dictionary, which does not fare well in handling out-of-vocabulary (OOV) words; or solely focused on the pronunciation prediction of OOV words without considering the contextual disambiguation of word pronunciations in text. In this paper, we propose a uniﬁed approach within the framework of phrasal statistical machine translation (SMT) that combines the strengths of the dictionary-based and substring-based approaches. Our approach is novel in that we combine wordand character-based pronunciations from a dictionary within an SMT framework: the former captures the idiosyncratic properties of word pronunciation, while the latter provides the ﬂexibility to predict the pronunciation of OOV words. We show that based on an extensive evaluation on various test sets, our model signiﬁcantly outperforms the previous state-of-the-art systems, achieving around 90% accuracy in most domains. 
We compare the use of an unsupervised transliteration mining method and a rulebased method to automatically extract lists of transliteration word pairs from a parallel corpus of Hindi/Urdu. We build joint source channel models on the automatically aligned orthographic transliteration units of the automatically extracted lists of transliteration pairs resulting in two transliteration systems. We compare our systems with three transliteration systems available on the web, and show that our systems have better performance. We perform an extensive analysis of the results of using both methods and show evidence that the unsupervised transliteration mining method is superior for applications requiring high recall transliteration lists, while the rule-based method is useful for obtaining high precision lists. 
We observe that (1) it is difficult to combine transliteration and meaning translation when transforming named entities (NE); and (2) there are different translation variations in NE translation, due to different semantic information. From this basis, we propose a novel semantic-specific NE translation model, which automatically incorporates the global context from corpus in order to capture substantial semantic information. The presented approach is inspired by example-based translation and realized by log-linear models, integrating monolingual context similarity model, bilingual context similarity model, and mixed language model. The experiments show that the semantic-specific model has substantially and consistently outperformed the baselines and related NE translation systems. 
We present an attempt to extract a largescale Japanese learners’ corpus from the revision log of a language learning SNS. This corpus is easy to obtain in largescale, covers a wide variety of topics and styles, and can be a great source of knowledge for both language learners and instructors. We also demonstrate that the extracted learners’ corpus of Japanese as a second language can be used as training data for learners’ error correction using an SMT approach. We evaluate different granularities of tokenization to alleviate the problem of word segmentation errors caused by erroneous input from language learners. Experimental results show that the character-wise model outperforms the word-wise model. 
In recent years, keyphrase extraction has received great attention, and been successfully employed by various applications. Keyphrases extracted from news articles can be used to concisely represent main contents of news events. Keyphrases can help users to speed up browsing and ﬁnd the desired contents more quickly. In this paper, we ﬁrst present several criteria of high-quality news keyphrases. After that, in order to integrate those criteria into the keyphrase extraction task, we propose a novel formulation which converts the task to a binary integer programming problem. The formulation cannot only encode the prior knowledge as constraints, but also learn constraints from data. We evaluate the proposed approach on a manually labeled corpus. Experimental results demonstrate that our approach achieves better performances compared with the state-of-the-art methods. 
This paper describes experiments on the TREC entity track that studies retrieval of homepages representing entities relevant to a query. Many studies have focused on extracting entities that match the given coarse-grained types such as organizations, persons, locations by using a named entity recognizer, and employing language model techniques to calculate similarities between query and supporting snippets of entities from which entities are extracted to rank the entities. This paper proposes three improvements over baseline, i.e., 1) incorporating homepages of entities to supplement supporting snippets, 2) recognizing ﬁne-grained named entities to ﬁlter out or negatively reward extracted entities that do not match the speciﬁed ﬁne-grained types of entities such as a university, airline, author, and 3) adopting a dependency tree-based similarity method to improve language model techniques. Our experiments demonstrate that the proposed approaches can signiﬁcantly improve performance, for instance, the absolute improvements of nDCG@R and P@1 scores are 8.4%, and 27.5%. 
This paper explores new approaches to active learning (AL) for semantic role labeling (SRL), focusing in particular on combining typical informativity-based sampling strategies with a novel measure of representativeness based on compressed dependency trees (CDTs). In essence, the compressed representation encodes the target predicate and the key dependents of the verb complex in the sentence. We ﬁrst present our method for producing CDTs from the output of an existing dependency parser. The compressed trees are used as features for training a supervised SRL system. Second, we present a study of AL for SRL. We investigate a number of different sample selection strategies, and the best results are achieved by incorporating CDTs for example selection based on both informativity and representativeness. We show that our approach can reduce by up to 50% the amount of training data needed to attain a given level of performance. 
We describe a method for training a semantic role labeler for CCG in the absence of gold-standard syntax derivations. Traditionally, semantic role labeling is performed by placing human-annotated semantic roles on gold-standard syntactic parses, identifying patterns in the syntaxsemantics relationship, and then predicting roles on novel syntactic analyses. The gold standard syntactic training data can be eliminated from the process by extracting training instances from semantic roles projected onto a packed parse chart. This process can be used to rapidly develop NLP tools for resource-poor languages of interest. 
We propose an approach to Japanese predicate argument structure analysis exploiting argument position and type. In particular, we propose the following two methods. First, in order to use information in the sentences in preceding context of the predicate more effectively, we propose an improved similarity measure between argument positions which is more robust than a previous co-reference-based measure. Second, we propose a ﬂexible selection-and-classiﬁcation approach which accounts for the minor types of arguments. Experimental results show that our proposed method achieves state-ofthe-art accuracy for Japanese predicate argument structure analysis. 
A multiword is compositional if its meaning can be expressed in terms of the meaning of its constituents. In this paper, we collect and analyse the compositionality judgments for a range of compound nouns using Mechanical Turk. Unlike existing compositionality datasets, our dataset has judgments on the contribution of constituent words as well as judgments for the phrase as a whole. We use this dataset to study the relation between the judgments at constituent level to that for the whole phrase. We then evaluate two different types of distributional models for compositionality detection – constituent based models and composition function based models. Both the models show competitive performance though the composition function based models perform slightly better. In both types, additive models perform better than their multiplicative counterparts. 
Context-free grammars with latent annotations (PCFG-LA) have been found to be effective for parsing many languages; however, currently their lexical model may be subject to over-ﬁtting and requires language engineering to handle out-ofvocabulary (OOV) words. Inspired by previous studies that have incorporated rich features into generative models, we propose to use a feature-rich log-linear lexical model to train PCFG-LA grammars that are more robust to rare and OOV words. The proposed lexical model has three advantages: over-ﬁtting is alleviated via regularization, OOV words are modeled using rich features, and lexical features are exploited for grammar induction. Our approach results in signiﬁcantly more accurate PCFG-LA grammars that are ﬂexible to train for different languages (with test F scores of 90.5, 85.0, and 81.9 on WSJ, CTB6, and ATB, respectively). 
In this paper, we present a simple and effective ﬁne-grained feature generation scheme for dependency parsing. We focus on the problem of grammar representation, introducing ﬁne-grained features by splitting various POS tags to different degrees using HowNet hierarchical semantic knowledge. To prevent the oversplitting, we adopt a threshold-constrained bottomup strategy to merge the derived subcategories. We conduct the experiments on the Penn Chinese Treebank. The results show that, with the ﬁne-grained features, we can improve the dependency parsing accuracies by 0.52% (absolute) for the unlabeled ﬁrst-order parser, and in the case of second-order parser, we can improve the dependency parsing accuracies by 0.61% (absolute). 
This paper presents a system for Natural Language Programming using Class Sequential Rules (CSR). The system recognizes a number of procedural primitives and operators. The domain of the system is presently limited to the world of numbers and operations on numbers. We evaluate the effectiveness of CSRs at the task of Natural Language Programming using an annotated corpus of programming instructions in natural language, achieving a precision and recall of 85% and 64% respectively. We also compare the performance of a system trained on annotated data with that of a system using handcrafted rules. 
We describe “treeblazing”, a method of using annotations from the GENIA treebank to constrain a parse forest from an HPSG parser. Combining this with self-training, we show signiﬁcant dependency score improvements in a task of adaptation to the biomedical domain, reducing error rate by 9% compared to out-of-domain gold data and 6% compared to self-training. We also demonstrate improvements in treebanking efﬁciency, requiring 25% fewer decisions, and 17% less annotation time. 
There has been substantial recent interest in aligning mentions of named entities in unstructured texts to knowledge base descriptors, a task commonly called entity linking. This technology is crucial for applications in knowledge discovery and text data mining. This paper presents experiments in the new problem of crosslanguage entity linking, where documents and named entities are in a different language than that used for the content of the reference knowledge base. We have created a new test collection to evaluate cross-language entity linking performance in twenty-one languages. We present experiments that examine issues such as: the importance of transliteration; the utility of cross-language information retrieval; and, the potential beneﬁt of multilingual named entity recognition. Our best model achieves performance which is 94% of a strong monolingual baseline. 
Annotating Named Entity Recognition (NER) training corpora is a costly process but necessary for supervised NER systems. This paper presents an approach to generate large-scale Chinese NER training data from an EnglishChinese discourse level aligned parallel corpus. Difficulty of NER is different among languages due to their unique features. For example, the performance of English NER systems is usually higher than the Chinese ones on average. In our method, we first employ a high performance NER system on one side of a bilingual corpus. And then, we project the NE labels to the other side according to the word level alignment. At last, we select high-quality labeled sentences using different strategies and generate an NER training corpus. In our experiments, we generate a Chinese NER corpus with 167,100 sentences through an EnglishChinese parallel corpus. The system trained on the automatically generated corpus attains a comparable result with the one trained on the manuallyannotated corpus. Further experiments show that the NER performance is significantly improved on two different evaluation sets by using the generated training data as an additional corpus to the manually-labeled data. 
Community-based Question Answering (cQA) is a popular online service where users can ask and answer questions on any topics. This paper is concerned with the problem of question retrieval. Question retrieval in cQA aims to ﬁnd historical questions that are semantically equivalent or relevant to the queried questions. Although the translation-based language model (Xue et al., 2008) has gained the state-of-the-art performance for question retrieval, they ignore the latent topic information in calculating the semantic similarity between questions. In this paper, we propose a topic model incorporated with the category information into the process of discovering the latent topics in the content of questions. Then we combine the semantic similarity based latent topics with the translation-based language model into a uniﬁed framework for question retrieval. Experiments are carried out on a real world cQA data set from Yahoo! Answers. The results show that our proposed method can signiﬁcantly improve the question retrieval performance of translation-based language model. 
Systems that distill information about events from large corpora generally extract sentences that are relevant to a short event query. We present a novel co-training strategy for this task that employs a multidocument news summary corpus featuring 2.5 million unlabeled sentences, thus obviating the need for extensive manual annotation. Our experiments indicate that this technique signiﬁcantly outperforms standard classiﬁcation approaches with linear feature combination on this task. An analysis of our approach under various settings reveals how classiﬁer and parameter choice can be used to control runtime overhead while contributing to an absolute increase of 22% in recall. 
Since the present-day Japanese use of voiced consonant mark had established in the Meiji Era, modern Japanese literary text written in the Meiji Era often lacks compulsory voiced consonant marks. This deteriorates the performance of morphological analyzers using ordinary dictionary. In this paper, we propose an approach for automatic labeling of voiced consonant marks for modern literary Japanese. We formulate the task into a binary classiﬁcation problem. Our pointwise prediction method uses as its feature set only surface information about the surrounding character strings. As a consequence, training corpus is easy to obtain and maintain because we can exploit a partially annotated corpus for learning. We compared our proposed method as a preprocessing step for morphological analysis with a dictionary-based approach, and conﬁrmed that pointwise prediction outperforms dictionary-based approach by a large margin. 
 The problem of Sam. dhi-Splitting is central to computational processing of Sanskrit texts. Currently the best-known algorithm for this task, given a chunk, generates all possible splits and chooses the Maximum-a-Posteriori estimate as the ﬁnal answer. Our contributions to the task of Sam. dhi-Splitting are two-fold. Firstly, we improve upon the current algorithm by proposing a principled modiﬁcation of the posterior probability function to achieve better results. Secondly, we propose an algorithm based on Bayesian Word-Segmentation methods. We ﬁnd that the unsupervised version of our algorithm achieves a better precision than the current algorithm with the original probabilistic model. We then present a supervised version of our algorithm that outperforms all previous methods/models.  
This paper presents a simple yet effective semi-supervised method to improve Chinese word segmentation and POS tagging. We introduce novel features derived from large auto-analyzed data to enhance a simple pipelined system. The auto-analyzed data are generated from unlabeled data by using a baseline system. We evaluate the usefulness of our approach in a series of experiments on Penn Chinese Treebanks and show that the new features provide substantial performance gains in all experiments. Furthermore, the results of our proposed method are superior to the best reported results in the literature. 
Dialectal Arabic (DA) is the spoken vernacular for over 300M people worldwide. DA is emerging as the form of Arabic written in online communication: chats, emails, blogs, etc. However, most existing NLP tools for Arabic are designed for processing Modern Standard Arabic, a variety that is more formal and scripted. Apart from the genre variation that is a hindrance for any language processing, even in English, DA has no orthographic standard, compared to MSA that has a standard orthography and script. Accordingly, a word may be written in many possible inconsistent spellings rendering the processing of DA very challenging. To solve this problem, such inconsistencies have to be normalized. This work is the ﬁrst step towards addressing this problem, as we attempt to identify spelling variants in a given textual document. We present an unsupervised clustering approach that addresses the problem of identifying orthographic variants in DA. We employ different similarity measures that exploit string similarity and contextual semantic similarity. To our knowledge this is the ﬁrst attempt at solving the problem for DA. Our approaches are tested on data in two dialects of Arabic - Egyptian and Levantine. Our system achieves the highest Entropy of 0.19 for Egyptian (corresponding to 68% cluster precision) and Levantine (corresponding to 64% cluster precision) respectively. This constitutes a signiﬁcant reduction in entropy (from 0.47 for Egyptian and 0.51 for Levantine) and improvement in cluster precision (from 29% for both) from the baseline.  
In this paper, we propose a Localized Feature Selection (LFS) framework tailored to the HL-SOT approach to sentiment analysis. Within the proposed LFS framework, each node classiﬁer of the HL-SOT approach is able to perform classiﬁcation on target texts in a locally customized index term space. Extensive empirical analysis against a human-labeled data set demonstrates that with the proposed LFS framework the classiﬁcation performance of the HL-SOT approach is enhanced with computational efﬁciency being greatly gained. To ﬁnd the best feature selection algorithm that caters to the proposed LFS framework, ﬁve classic feature selection algorithms are comparatively studied, which indicates that the TS, DF, and MI algorithms achieve generally better performances than the CHI and IG algorithms. Among the ﬁve studied algorithms, the TS algorithm is best to be employed by the proposed LFS framework. 
Sentiment analysis is the problem of determining the polarity of a text with respect to a particular topic. For most applications, however, it is not only necessary to derive the polarity of a text as a whole but also to extract negative and positive utterances on a more ﬁnegrained level. Sentiment analysis systems working on the (sub-)sentence level, however, are difﬁcult to develop since shorter textual segments rarely carry enough information to determine their polarity out of context. In this paper, therefore, we present a fully automatic framework for ﬁne-grained sentiment analysis on the subsentence level combining multiple sentiment lexicons and neighborhood as well as discourse relations to overcome this problem. We use Markov logic to integrate polarity scores from different sentiment lexicons with information about relations between neighboring segments, and evaluate the approach on product reviews. The experiments show that the use of structural features improves the accuracy of polarity predictions achieving accuracy scores of up to 69%. 
Syntactic structures have been good features for opinion analysis, but it is not easy to use them. To find these features by supervised learning methods, correct syntactic labels are indispensible. Two possible sources to acquire syntactic structures are parsing trees and dependency trees. For the annotation processing, parsing trees are more readable for annotators, while dependency trees are easier to use by programs. To use syntactic structures as features, this paper tried to annotate on human friendly materials and transform these annotations to the corresponding machine friendly materials. We annotated the gold answers of opinion syntactic structures on the parsing tree from Chinese Treebank, and then proposed methods to find their corresponding dependency relations on the dependency trees generated from the same sentence. With these relations, we could train a model to annotate opinion dependency relations automatically to provide an opinion dependency parser, which is language independent if language resources are incorporated. Experiment results show that the annotated syntactic structures and their corresponding dependency relations improve at least 8% of the performance of opinion analysis. 
Sentiment detection of a given expression involves interaction with its component constituents through rules such as polarity propagation, reversal or neutralization. Such compositionality-based sentiment detection usually performs better than a vote-based bag-ofwords approach. However, in some contexts, the polarity of the adjectival modifier may not always be correctly determined by such rules, especially when the adjectival modifier characterizes the noun so that its denotation becomes a particular concept or an object in customer reviews. In this paper, we examine adjectival modifiers in customer review sentences whose polarity should either be propagated (SHIFT) or not (UNSHIFT). We refine polarity propagation rules in the literature by considering both syntactic and semantic clues of the modified nouns and the verbs that take such nouns as arguments. The resulting rules are shown to work particularly well in detecting cases of ‘UNSHIFT’ above, improving the performance of overall sentiment detection at the clause level, especially in ‘neutral’ sentences. We also show that even such polarity that is not propagated is still necessary for identifying implicit sentiment of the adjacent clauses. 
Word classes automatically induced from distributional evidence have proved useful many NLP tasks including Named Entity Recognition, parsing and sentence retrieval. The Brown hard clustering algorithm is commonly used in this scenario. Here we propose to use Latent Dirichlet Allocation in order to induce soft, probabilistic word classes. We compare our approach against Brown in terms of efﬁciency. We also compare the usefulness of the induced Brown and LDA word classes for the semi-supervised learning of three NLP tasks: ﬁne-grained Named Entity Recognition, Morphological Analysis and semantic Relation Classiﬁcation. We show that using LDA for word class induction scales better with the number of classes than the Brown algorithm and the resulting classes outperform Brown on the three tasks. 
The abundance of user-generated content comes at a price: the quality of content may range from very high to very low. We propose a regression approach that incorporates various features to recommend short-text documents from Twitter, with a bias toward quality perspective. The approach is built on top of a linear regression model which includes a regularization factor inspired from the content conformity hypothesis - documents similar in content may have similar quality. We test the system on the Edinburgh Twitter corpus. Experimental results show that the regularization factor inspired from the hypothesis can improve the ranking performance and that using unlabeled data can make ranking performance better. Comparative results show that our method outperforms several baseline systems. We also make systematic feature analysis and ﬁnd that content quality features are dominant in short-text ranking. 
The effort required to build a classiﬁer for a task in a target language can be signiﬁcantly reduced by utilizing the knowledge gained during an earlier effort of model building in a source language for a similar task. In this paper, we investigate whether unlabeled data in the target language can be labeled given the availability of labeled data for a similar domain in the source language. We view the problem of labeling unlabeled documents in the target language as that of clustering them such that the resulting partitioning has the best alignment with the classes provided in the source language. We develop a cross language guided clustering (CLGC) method to achieve this. We also propose a method to discover concept mapping between languages which is utilized by CLGC to transfer supervision across languages. Our experimental results show signiﬁcant gains in the accuracy of labeling documents over the baseline methods. 
In this paper we study a novel relation extraction problem where a general relation type is deﬁned but relation extraction involves extracting speciﬁc relation descriptors from text. This new task can be treated as a sequence labeling problem. Although linear-chain conditional random ﬁelds (CRFs) can be used to solve this problem, we modify this baseline solution in order to better ﬁt our task. We propose two modiﬁcations to linear-chain CRFs, namely, reducing the space of possible label sequences and introducing long-range features. Both modiﬁcations are based on some special properties of our task. Using two data sets we have annotated, we evaluate our methods and ﬁnd that both modiﬁcations to linear-chain CRFs can significantly improve the performance for our task. 
The accuracy and coverage of existing methods for extracting attributes of instances from text in general, and Web search queries in particular, are limited by two main factors: availability of input textual data to which the methods can be applied, and inherent limitations of the underlying assumptions and algorithms being used. This paper proposes a weakly-supervised approach for the acquisition of attributes of instances from input data available in the form of synthetic queries automatically generated from submitted queries. The generated queries allow for the acquisition of additional attributes, leading to extracted lists of attributes of higher quality than with comparable previous methods. 
A novel reranking method has been developed to reﬁne web search queries. A label propagation algorithm was applied on a clickthrough graph, and the candidates were reranked using a query language model. Our method ﬁrst enumerates query candidates with common landing pages with regard to the given query to create a clickthrough graph. Second, it calculates the likelihood of the candidates, using a language model generated from web search query logs. Finally, the candidates are sorted by the score calculated from the likelihood and label propagation. As a result, high precision and coverage were achieved in the task of Japanese abbreviation expansion, without using handcrafted training data. 
We propose a content-based approach to mine parallel resources from the entire web using cross lingual information retrieval (CLIR) with search query relevance score (SQRS). Our method improves mining recall by going beyond URL matching to find parallel documents from non-parallel sites. We introduce SQRS to improve the precision of mining. Our method makes use of search engines to query for target document given each source document and therefore does not require downloading target language documents in batch mode, reducing computational cost on the local machines and bandwidth consumption. We obtained a very high mining precision (88%) on the parallel documents by the pure CLIR approach. After extracting parallel sentences from the mined documents and using them to train an SMT system, we found that the SMT performance, with 29.88 BLEU score, is comparable to that obtained with high quality manually translated parallel sentences with 29.54 BLEU score, illustrating the excellent quality of the mined parallel material. 
This paper presents a novel crawling strategy to locate bilingual sites. It does so by focusing on the Web graph neighborhood of these sites and exploring the patterns of the links in this region to guide its visitation policy. A sub-task in the problem of bilingual site discovery is the job of detecting bilingual sites, i.e., given a Web site, verify whether it is bilingual or not. We perform this task by combining supervised learning and language identiﬁcation. Experimental results demonstrate that our crawler outperforms previous crawling approaches and produces a high-quality collection of bilingual sites, which we evaluate in the context of machine translation in the tourism and hospitality domain. The parallel text obtained using our novel crawling strategy results in a relative improvement of 22% in BLEU score (English-to-Spanish) over an out-ofdomain seed translation model trained on the European parliamentary proceedings. 
We present an efﬁcient technique to incorporate a small number of cross-linguistic parameter settings deﬁning default word orders to otherwise unsupervised grammar induction. A syntactic prototype, represented by the integrated model between Categorial Grammar and dependency structure, generated from the language parameters, is used to prune the search space. We also propose heuristics which prefer less complex syntactic categories to more complex ones in parse decoding. The system reduces errors generated by the state-of-the-art baselines for WSJ10 (1% error reduction of F1 score for the model trained on Sections 2–22 and tested on Section 23), Chinese10 (26% error reduction of F1), German10 (9% error reduction of F1), and Japanese10 (8% error reduction of F1), and is not signiﬁcantly different from the baseline for Czech10. 
Various works have used word alignments in parallel corpora to transfer information like POS tags, syntactic trees and word senses from source to target sentences. In this paper, we work on the problem of projecting syntactic relations from English to morphologically rich Hindi parallel text. We show the effectiveness of Local Word Groups (LWGs) in simplifying alignments as well as in transferring syntactic dependencies by building an alignment model with LWGs as base units and training a dependency parser on the relations projected using these LWGs. The LWG alignment model using GIZA++ scores decreases the Alignment Error Rate by 1.16 points when compared to the best GIZA++ model trained on lemmas. We also show that a dependency parser trained on the syntactic relations projected using LWGs obtained statistical signiﬁcant improvements over the relations projected using lemmas by a margin of 3.49%. 
We present a uniﬁed generative model of coordination that considers parallelism of conjuncts and selectional preferences. Parallelism of conjuncts, which frequently characterizes coordinate structures, is modeled as a synchronized generation process in the generative parser. Selectional preferences learned from a large web corpus provide an important clue for resolving the ambiguities of coordinate structures. Our experiments of Japanese dependency parsing indicate the effectiveness of our approach, particularly in the domains of newspapers and patents. 
Recent research efforts have led to the development of a state-of-the-art supervised coreference model, the cluster-ranking model. However, it is not clear whether the features that have been shown to be useful when employed in traditional coreference models will fare similarly when used in combination with this new model. Rather than merely re-evaluate them using the cluster-ranking model, we examine two interesting types of features derived from syntactic parses, tree-based features and path-based features, and discuss the challenges involved in employing them in the cluster-ranking model. Results on a set of Switchboard dialogues show their effectiveness in improving the cluster-ranking model: using them to augment a baseline coreference feature set yields a 8.6–11.7% reduction in relative error. 
We propose a statistical sentence simpliﬁcation system with log-linear models. In contrast to state-of-the-art methods that drive sentence simpliﬁcation process by hand-written linguistic rules, our method used a margin-based discriminative learning algorithm operates on a feature set. The feature set is deﬁned on statistics of surface form as well as syntactic and dependency structures of the sentences. A stack decoding algorithm is used which allows us to efﬁciently generate and search simpliﬁcation hypotheses. Experimental results show that the simpliﬁed text produced by the proposed system reduces 1.7 Flesch-Kincaid grade level when compared with the original text. We will show that a comparison of a state-ofthe-art rule-based system (Heilman and Smith, 2010) to the proposed system demonstrates an improvement of 0.2, 0.6, and 4.5 points in ROUGE-2, ROUGE-4, and AveF10, respectively. 
Heavy research has been done in recent years on tasks of traditional summarization. However, social context, which is critical in building high-quality social summarizer for web documents, is usually neglected. To address this issue, we propose a novel summarization approach based on social context. In this approach, social summarization is implemented by first employing the tripartite clustering algorithm to simultaneously discover document context and user context for a specified document. Then sentence relationships intra and inter documents plus intended user communities are taken into account to evaluate the significance of each sentence in different context views. Finally, a few sentences with highest overall scores are selected to form the summary. Experimental results demonstrate the effectiveness of the proposed approach and show the superior performance over several baselines. 
Multi-document summarization aims to produce a concise summary that contains salient information from a set of source documents. Since documents often cover a number of topical themes with each theme represented by a cluster of highly related sentences, sentence clustering plays a pivotal role in theme-based summarization. Moreover, noting that realworld datasets always contain noises which inevitably degrade the clustering performance, we incorporate noise detection with spectral clustering to generate ordinary sentence clusters and one noise sentence cluster. We are also interested in making the theme-based summaries biased towards a user’s query. The effectiveness of the proposed approaches is demonstrated by both the cluster quality analysis and the summarization evaluation conducted on the DUC generic and queryoriented summarization datasets. 
This paper proposes a novel extractive summarization method for speech dialogues between agents and customers in contact centers. The proposed method does not require any extra cost for applying the method such as preparing rules or creating training data. Conventional methods such as the tf*idf method, which gives importance to characteristic words in an input text, can miss the essential points for contact center work. Our proposed method evaluates the importance of each utterance from the standpoint of call agents who report calls for managing or analyzing calls. Specifically, the proposed method includes information frequently reported by call agents in summaries using past call logs commonly recorded in the contact center. Evaluation using real data (call dialogues and call logs) shows that the proposed method can extract essential points in terms of contact center work and outperforms the conventional method. 
This paper addresses a semantic tree-tostring alignment problem: indexing spoken documents with known hierarchical semantic structures, with the goal to help index and access such archives. We propose and study a number of alignment models of different modeling capabilities and time complexities to provide a comprehensive understanding of these unsupervised models and hence the problem itself. 
The evaluation of named entity recognition (NER) methods is an active ﬁeld of research. This includes the recognition of named entities in speech transcripts. Evaluating NER systems on automatic speech recognition (ASR) output whereas human reference annotation was prepared on clean manual transcripts raises difﬁcult alignment issues. These issues are emphasized when named entities are structured, as is the case in the Quaero NER challenge organized in 2010. This paper describes the structured named entity deﬁnition used in this challenge and presents a method to transfer reference annotations to ASR output. This method was used in the Quaero 2010 evaluation of extended named entity annotation on speech transcripts, whose results are given in the paper. 
The task of documenting the world’s languages is a mainstream activity in linguistics which is yet to spill over into computational linguistics. We propose a new task of transcription normalisation as an algorithmic method for speeding up the process of transcribing audio sources, leading to text collections of usable quality. We report on the application of sentence and word alignment algorithms to this task, before describing a new algorithm. All of the algorithms are evaluated over synthetic datasets. Although the results are nuanced, the transcription normalisation task is suggested as an NLP contribution to the grand challenge of documenting the world’s languages. 
This paper describes a novel method of constructing a language model for speech recognition of inputs with a particular style, using a large-scale Web archive. Our target is an open domain voice-activated QA system and our speech recognition module must recognize relatively short, domain independent questions. The central issue is how to prepare a large scale training corpus with low cost, and we tackled this problem by combining an existing domain adaptation method and distributional word similarity. From 500 seed sentences and 600 million Web pages we constructed a language model covering 413,000 words. We achieved an average improvement of 3.25 points in word error rate over a baseline model constructed from randomly sampled Web sentences. 
Phylogenetic methods are used to build evolutionary trees of languages given character data that may include lexical, phonological, and morphological information. Such data rarely admits a perfect phylogeny. We explore the use of the more permissive conservative Dollo phylogeny as an alternative or complementary approach. We propose a heuristic search algorithm based on the notion of chordal graphs. We test this approach by generating phylogenetic trees from three datasets, and comparing them to those produced by other researchers. 
We show that transductive (cross-domain) learning is an important consideration in building a general-purpose language identiﬁcation system, and develop a feature selection method that generalizes across domains. Our results demonstrate that our method provides improvements in transductive transfer learning for language identiﬁcation. We provide an implementation of the method and show that our system is faster than popular standalone language identiﬁcation systems, while maintaining competitive accuracy. 
Entity linking maps name mentions in context to entries in a knowledge base through resolving the name variations and ambiguities. In this paper, we propose two advancements for entity linking. First, a Wikipedia-LDA method is proposed to model the contexts as the probability distributions over Wikipedia categories, which allows the context similarity being measured in a semantic space instead of literal term space used by other studies for the disambiguation. Furthermore, to automate the training instance annotation without compromising the accuracy, an instance selection strategy is proposed to select an informative, representative and diverse subset from an auto-generated dataset. During the iterative selection process, the batch sizes at each iteration change according to the variance of classifier’s confidence or accuracy between batches in sequence, which not only makes the selection insensitive to the initial batch size, but also leads to a better performance. The above two advancements give significant improvements to entity linking individually. Collectively they lead the highest performance on KBP-10 task. Being a generic approach, the batch size changing method can also benefit active learning for other tasks.  its context, we should be able to disambiguate which president it is referring to). Compared with Cross-Document Coreference (Bagga and Baldwin, 1998) which clusters the articles according to the entity mentioned, entity linking has a given entity list (i.e. the reference KB) to which we disambiguate the entity mentions. Moreover, in the articles, there are new entities not present in KB. For name disambiguation in entity linking, there has been much previous work which demonstrates modeling context is an important part of measuring document similarity. However, the traditional approach for entity linking treats the context as a bag of words, n-grams, noun phrases or/and co-occurring named entities, and measures context similarity by the comparison of the weighted literal term vectors (Varma et al., 2009; Li et al., 2009; McNamee et al., 2009; Zhang et al., 2010; Zheng et al., 2010; Dredze et al., 2010). Such literal matching suffers from sparseness issue. For example, consider the following four observations of Michael Jordan without term match: 1) Michael Jordan is a leading researcher in machine learning and artificial intelligence. 2) Michael Jordan is currently a full professor at the University of California, Berkeley. 3) Michael Jordan (born February, 1963) is a  
Named entities and WordNet words are important in defining the content of a text in which they occur. Named entities have ontological features, namely, their aliases, classes, and identifiers. WordNet words also have ontological features, namely, their synonyms, hypernyms, hyponyms, and senses. Those features of concepts may be hidden from their textual appearance. Besides, there are related concepts that do not appear in a query, but can bring out the meaning of the query if they are added. The traditional constrained spreading activation algorithms use all relations of a node in the network that will add unsuitable information into the query. Meanwhile, we only use relations represented in the query. We propose an ontology-based generalized Vector Space Model to semantic text search. It discovers relevant latent concepts in a query by relation constrained spreading activation. Besides, to represent a word having more than one possible direct sense, it combines the most specific common hypernym of the remaining undisambiguated multi-senses with the form of the word. Experiments on a benchmark dataset in terms of the MAP measure for the retrieval performance show that our model is 41.9% and 29.3% better than the purely keyword-based model and the traditional constrained spreading activation model, respectively. 1. Introduction With rapid development of the World Wide Web and e-societies, Information Retrieval (IR) has many challenges in discovering and exploiting those rich and huge information resources. Semantic search improves search precision and recall by understanding user's intent and the contextual meaning of concepts in documents and queries (Huston and Croft, 2010; Losada, et al, 2010; Egozi, et al, 2011). Concepts are named entities or WordNet words (unnamed entities). Named entities are  those that are referred to by names such as people, organizations, and locations (Sekine, 2004) and could be described in ontologies. Each fully recognized named entity (NE) has three features, namely, name, class, and identifier. WordNet words are words in a lexical database (e.g. WordNet database). Each fully recognized WordNet word (WW) has three features, namely, form, direct hypernym, and sense. Lexical search is not adequate to represent the semantics of queries referring to NEs or WWs. Some examples of NE-based queries are: (1) Search for documents about “football clubs”; (2) Search for documents about “Barcelona”; (3) Search for documents about “Paris City”; (4) Search for documents about “Paris City, Texas, USA”. In fact, the first query searches for documents containing NEs of the class Football Club, e.g. Chelsea or Barcelona, rather than those containing the keywords “football club”. For the second query, target documents may mention Football Club Barcelona under other names, i.e., the football club’s aliases, such as Football Club Barca. Besides, documents containing Barcelona City or Barcelona University are also suitable. In the third query, users do not expect to receive answer documents about entities that are also named “Paris”, e.g. the actress Paris Hilton or University of Paris but are not cities. Meanwhile, the fourth query requests documents about a precisely identified named entity, i.e., the Paris City in Texas, USA, not the one in France. That are, entity aliases, classes, and identifiers have to be taken into account. Some examples about WW-based queries are: (1) Search for documents about “movement”; (2) Search for documents about “movement belonging to change”; and (3) Search for documents about “movement belonging to the act of changing location from one place to another”. That is because the word movement has many different senses. In fact, the first query searches for documents containing not only the word movement but also its synonyms, e.g. motion, front, cam-  571 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 571–579, Chiang Mai, Thailand, November 8 – 13, 2011. c 2011 AFNLP  paign, and trend, or its hypernyms, e.g. change, occurrence, social group, venture, and disposition. For the second query, users do not expect to receive answer documents about words that are also labelled “movement”, e.g. movement belonging to a natural event and movement belonging to a venture, but do not express changes. Meanwhile, the third query requests documents about a precisely identified word sense. The word movement means not only the action of changing something but also the act of changing location from one place to another, e.g. the movement of people from the farms to the cities. Moreover, queries may contain both named entities and WordNet words. Some examples of NE-WW based queries are “temblor in USA” or “natural calamity in USA”, for which documents about “earthquake in United States of America” are truly relevant answers. Besides, there are latent concepts that do not appear in queries but present user’s intent. Intuitively, adding correct related concepts to a query will increase the recall and the precision of searching. In contrast, adding incorrect related concepts will decrease performance of IR system. For examples, consider the following queries: (1) Search for documents about “cities that are tourist destinations of Thailand”; (2) Search for documents about “tsunami in Southeast Asia”; (3) Search for documents about “settlements are built in west of Jerusalem”; and (4) Search for documents about “Barack Obama uses high-tech defences”. For the first query, Chiang Mai and Phuket should be added into the query, because they belong to class City and are tourist destinations of Thailand. For the second query, countries having relation “is part of” with Southeast Asia in the exploited ontology should be added into the query, e.g. Indonesia or Philippine. However, added countries should be those that were actually hit by at least one tsunami, according to the given ontology. So, Laos should not be added into the query. For the third query, if there are facts that settlements are built in the locations in the west of Jerusalem, e.g. Givat Zeev and Pisgat Zeev, then those locations should add into the query. For the fourth query, bullet-resistant suit should be added into the query; because it is hyponym of high-tech defences and the President Barack Obama have used a bullet-resistant suit. In this paper, we propose a new ontologybased text search model with two key ideas as our two contributions. First, it exploits different ontological features of NE and WW existing in  documents and queries. Until now, there is no other text search model that formally exploits and presents in documents and queries all abovementioned NE features or all above-mentioned WW features. Specifically, in a context, after a disambiguation process, if a WordNet word has more than one sense with the equally highest rank, then the most specific common hypernym (msc_hypernym) of those senses will be chosen and the word will be represented by the pair of that hypernym and the form of the word. Meanwhile, other WordNet-based text search models choose one of those senses randomly or all of the senses (Vooheres, 1994; Liu, et al., 2004; Zaihrayeu, et al., 2007; Hsu, et al., 2008; Giunchiglia, et al., 2009). Second, our model expands a query by latent concepts relating to concepts and relations in the original query as asserted in employed ontologies. Our proposal is more general than Fu, et al. (2005), which considered only spatial relations. In the next section, we discuss related works. Section 3 describes the proposed system architecture and detailed model. Section 4 presents evaluation of the proposed model and discussion on experiment results in comparison to other models. Finally, section 5 gives some concluding remarks. 2 Related Works 2.1 Exploiting Named Entities There are works exploiting NEs but not for document search. The Falcons system described in Cheng, et al. (2008) is assisted by users to determine clearly the meaning of queries. In Cheng, et al. (2007), the authors use classes of NEs associated with keywords in a query. However, they are for entity search. Vallet and Zaragoza (2008), Santos, et al. (2010), Demartini, et al. (2010), and Kaptein, et al. (2010) use only names and classes of NEs, and they are for entity ranking (Balog, et al. 2009). Gupta and Ratinov (2008), Chang, et al. (2008), Wang, et al. (2009), and Jing, et al. (2010) use only labels of concepts (NE names or WW forms) to represent documents and queries. Moreover, they are for document classification, not document search. There are some papers using named entity for document search. Bast et al. (2007) considers only entity classes in combination with keywords. In Ahn, et al. (2010), the NameSieve system uses only names and classes of NEs, and limits in four entity class: who, where, when and  572  what. Beside, the system is helped by users to determine clearly the meaning of queries. In Egozi, et al. (2011), the authors use only names of concepts to present documents and queries. 2.2 Exploting WordNet Voorhees (1994), Liu, et al. (2004) and Hsu, et al. (2008) use all forms of a sense and all forms of every hyponym of a sense in a query. Meanwhile, Zaihrayeu, et al. (2007) uses all forms of a sense to expand a document, and Wang, et. al. (2004) and Giunchiglia, et al. (2009) additionally use all forms of every hypernym of a sense in a document. Mihalcea and Moldovan (2000) use senses in both queries and documents, and all forms of every hypernym of a sense in a document. Moreover, since the above-surveyed papers, except for Mihalcea and Moldovan (2000), use word forms to represent word senses, it may reduce the precision of system. Indeed, a query containing a word having form f and sense x could also match to documents containing a word having the same form f but different sense y. The drawback is similar with using only word forms of hypernyms and hyponyms of senses. Especially, in case a word has more than one sense determined by a Word Sense Diambiguation (WSD) algorithm, the above works choose randomly one sense from those senses, which may decrease the retrieval performance if that is a wrong choice. In contrast, in our system, such a word is represented by the combination of its form and the most specific common hypernym of the senses. 2.3 Exploiting Latent Concepts Some systems improve document retrieval performance by expanding queries with user’s participation, such as Sanderson (2004), Balog, et al. (2008), Castellani, et al. (2009), Meij, et al. (2009) and Ahn, et al. (2010). Whereas, Bendersky and Croft (2008), and Huston and Croft (2010) identify key concepts in queries to remove unimportant words. In Wang and Zhai (2008), the authors exploit synonyms or co-occurring relations in search engine logs for repairing or expanding queries. In Losada, et al. (2010), the system uses pseudorelevance feedback to expand queries. However, the two systems do not take account relations in a query. In Tran, et al. (2007), the authors map concepts of a query to an ontology to find suitable related concepts. In Cheng, et al. (2007), the target problem is to search for named entities of  specified classes associated with keywords in a query. Different from our model, the two systems do not take account relations in queries and they are for question-and-answering but not document search. In Castells, et al. (2007), the system finds identified named entities belonging to a class of NE in a query, after the query’s vector is constructed by the NEs. This step is unnecessarily time consuming. In our proposed models, the query and document vectors having the entity class can be constructed and matched right away. Beside, its queries must be specified by RDQL. Similarity, in Kasneci, et al. (2008), queries must be written by SPARQL. Concepts and relations must be clearly specified by users. Whereas, this need not in our system. Moreover, the work is for question-and-answering, not document retrieval. Spreading Activation (SA) is a popular algorithm for query expansion. But pure-SA would return most results irrelevant to queries (Berthold, et al., 2009). So, SA algorithms have been constrained by some methods to improve retrieval performance. In Rocha, et al. (2004), the authors propose a hybrid spread activation algorithm that combined SA algorithm together with ontology based information retrieval. In Aswath, et al. (2005), the system uses a two-level SA network to activate strongly positive and strongly negative matches based on keyword search results. In Schumacher, et al. (2008), the system finds answers of given query and added into the query before using an SA algorithm. Besides, Hsu, et al (2008) expands query by using SA on all relations in WordNet and only selecting kernel words that are activated and represent the content of a query by some rules. In Jiang and Tan (2009), the authors map the original query to a keyword set and searches for documents relating to the keyword set. After that, the documents are pre-annotated with information of an ontology and the initial concepts are extracted from the retrieved documents. An SA algorithm is used to find concepts semantically relating to the concepts in the ontology. Finally, the activated concepts are used to rerank the documents to present for user. In Lee, et al. (2010), the system sets up an associative network with nodes being web pages and links between the nodes being relations between the web pages. Initial nodes of SA algorithm are web pages that are strongly associated to given query.  573  Next, other nodes (web pages) of their network are activated. However, the above Constrained-SA (CSA) models do not use relations in a given query to constrain spreading. Meanwhile, our relationCSA method activates concepts relating to concepts and relations in queries. In Fu, et al. (2005), the authors use the relations in a query to expand the query. However, the work only exploits spatial relations (e.g. near, inside, north of). In contrast, in this paper, we propose more general rules for query expansion. 3 Ontology-based Text Search 3.1 System Architecture Our proposed system architecture of semantic text search is shown in Figure 1. It has two main parts. Part 1 presents document and query annotation and expansion. Part 2 presents the query expansion module using a relation-CSA (RCSA) method. Our proposed model needs an ontology having: (1) a comprehensive class catalog with a large concept population for expressing clearly information of documents and queries; and (2) a comprehensive set of relations between concepts and facts for expanding queries with latently related concepts. Since no single ontology is rich enough for every domain and application, merging or combining multiple ontologies are reasonable solutions (Choi, et al. 2006). So we have combined 3 ontologies, namely, KIM, WordNet, and YAGO to have a rich ontology for our model. In this work we employ KIM (Kiryakov, et al. 2005) for automatic NE recognition and semantic annotation of documents and queries. The KIM PROTON ontology contains about 300 classes and 100 attributes and relations. KIM World Knowledge Base (KB) contains about 77,500 entities with more than 110,000 aliases. NE descriptions are stored in an RDF(S) repository. Each NE has information about its specific class, aliases, and attributes (i.e., its own properties or relations with other NEs). The average precision and recall of the NE recognition engine are about 90% and 86%, respectively1. WordNet (Fellbaum, 1998) is a lexical database for English organized in synonym sets (synsets). There are various semantic relations between these synonym sets, such as hypernym, hyponym, holonym, meronym, and similarity. 
Cross-lingual document clustering (CLDC) is the task to automatically organize a large collection of cross-lingual documents into groups considering content or topic. Different from the traditional hard matching strategy, this paper extends traditional generalized vector space model (GVSM) to handle cross-lingual cases, referred to as CLGVSM, by incorporating cross-lingual word similarity measures. With this model, we further compare different word similarity measures in cross-lingual document clustering. To select cross-lingual features effectively, we also propose a softmatching based feature selection method in CLGVSM. Experimental results on benchmarking data set show that (1) the proposed CLGVSM is very effective for cross-document clustering, outperforming the two strong baselines vector space model (VSM) and latent semantic analysis (LSA) significantly; and (2) the new feature selection method can further improve CLGVSM. 
As a classic natural language processing technology, topic detection recently attracts more research interests due largely to the rapid development of microblog. The most challenging issue in microblog topic detection is sparse data problem. In this paper, the temporal-author-topic (TAT) model is designed to accomplish microblog topic detection in two phases. In the first phase, the TAT model is applied to clean the thread, namely, to filter noisy microblog texts out of each thread. In the second phase, microblog texts within each thread are merged to form the thread text so that the TAT model is applied to find global topics. The new approach differs from the Hierarchical Agglomerative Clustering (HAC) algorithm by making use of microblog threads to overcome the sparse data problem. Experimental results justify our claims. 
The design of a Spoken Dialogue System (SDS) is a long, iterative and costly process. Especially, it requires test phases on actual users either for assessment of performance or optimization. The number of test phases should be minimized, yet without degrading the ﬁnal performance of the system. For these reasons, there has been an increasing interest for dialogue simulation during the last decade. Dialogue simulation requires simulating the behavior of users and therefore requires user modeling. User simulation is often done by statistical systems that have to be tuned or trained on data. Yet data are generally incomplete with regard to the necessary information for simulating the user decision making process. For example, the internal knowledge the user builds along the conversation about the information exchanged while interacting is difﬁcult to annotate. In this contribution, we propose the use of a previously developed user simulation system based on Bayesian Networks (BN) and the training of this model using algorithms dealing with missing data. Experiments show that this training method increases the simulation performance in terms of similarity with real dialogues. 
In this paper, we introduce the task of identifying general and speciﬁc sentences in news articles. Given the novelty of the task, we explore the feasibility of using existing annotations of discourse relations as training data for a general/speciﬁc classiﬁer. The classiﬁer relies on several classes of features that capture lexical and syntactic information, as well as word speciﬁcity and polarity. We also validate our results on sentences that were directly judged by multiple annotators to be general or speciﬁc. We analyze the annotator agreement on speciﬁcity judgements and study the strengths and robustness of features. We also provide a task-based evaluation of our classiﬁer on general and speciﬁc summaries written by people. Here we show that the speciﬁcity levels predicted by our classiﬁer correlates with the intuitive judgement of speciﬁcity employed by people for creating these summaries. 
In this paper, we focus on the tasks of cross-domain sentiment classification. We find across different domains, features with some types of part-of-speech (POS) tags are domain-dependent, while some others are domain-free. Based on this finding, we proposed a POS-based ensemble model to efficiently integrate features with different types of POS tags to improve the classification performance. Weights are trained by stochastic gradient descent (SGD) to optimize the perceptron and minimal classification error (MCE) criteria. Experimental results show that the proposed ensemble model is quite effective for the task of cross-domain sentiment classification. 
Classiﬁcation of citations into categories such as use, refutation, comparison etc. may have several relevant applications for digital libraries such as paper browsing aids, reading recommendations, qualiﬁed citation indexing, or ﬁne-grained impact factor calculation. Most citation classiﬁcation approaches described so far heavily rely on rule systems and patterns tailored to speciﬁc science domains. We focus on a less manual approach by learning domaininsensitive features from textual, physical, and syntactic aspects. Our experiments show the effectiveness of this feature set with various machine learning algorithms on datasets of different sizes. Furthermore, we build an ensemble-style selftraining classiﬁcation model and get better classiﬁcation performance using only few training data, which largely reduces the manual annotation work in this task. 
The term “genre” covers different aspects of both texts and documents, and it has led to many classiﬁcation schemes. This makes different approaches to genre identiﬁcation incomparable and the task itself unclear. We introduce the linguistically motivated text classiﬁcation task language function analysis, LFA, which focuses on one well-deﬁned aspect of genres. The aim of LFA is to determine whether a text is predominantly expressive, appellative, or informative. LFA can be used in search and mining applications to efﬁciently ﬁlter documents of interest. Our approach to LFA relies on fast machine learning classiﬁers with features from different research areas. We evaluate this approach on a new corpus with 4,806 product texts from two domains. Within one domain, we correctly classify up to 82% of the texts, but differences in feature distribution limit accuracy on out-of-domain data. 
This paper investigates parameter adaptation in Statistical Machine Translation(SMT). To overcome the parameter bias-estimation problem with Minimum Error Rate Training(MERT), we extend it under a transductive learning framework, by iteratively re-estimating the parameters using both development and test data, in which the translation hypotheses of the test data are used as pseudo references. Furthermore, in order to overcome the over-training and unstableness problems respectively in employing such pseudo references, a termination criterion using a hyper-parameter and a Minimum Bayes Risk(MBR)-based hypothesis selection method are proposed in our work. Experimental results show that the transductive MERT method could yield significant performance improvements over a strong baseline on a large-scale Chineseto-English translation task. 
The direct optimization of a translation metric is an integral part of building stateof-the-art SMT systems. Unfortunately, widely used translation metrics such as BLEU-score are non-smooth, non-convex, and non-trivial to optimize. Thus, standard optimizers such as minimum error rate training (MERT) can be extremely time-consuming, leading to a slow turnaround rate for SMT research and experimentation. We propose an alternative approach based on particle swarm optimization (PSO), which can easily exploit the fast growth of distributed computing to obtain solutions quickly. For example in our experiments on NIST 2008 Chineseto-English data with 512 cores, we demonstrate a speed increase of up to 15x and reduce the parameter tuning time from 10 hours to 40 minutes with no degradation in BLEU-score. 
Phrase-based statistical machine translation (PBSMT) decoders translate source sentences one phrase at a time using strong independence assumptions over the source phrases. Translation table scores are typically independent of context, language model scores depend on a few words surrounding the target phrase and distortion models do not inﬂuence directly the choice of target phrases. In this work, we propose to condition the selection of each target word on the whole source sentence using a multilayer perceptron (MLP). Our interest in MLP lies in their hidden layer which encodes source sentences in a representation that is not directly tied to the notion of word. We evaluated our approach on an English to French translation task. Our MLP model was able to improve BLEU scores over a standard PBSMT system. 
Several bilingual WSD algorithms which exploit translation correspondences between parallel corpora have been proposed. However, the availability of such parallel corpora itself is a tall task for some of the resource constrained languages of the world. We propose an unsupervised bilingual EM based algorithm which relies on the counts of translations to estimate sense distributions. No parallel or sense annotated corpora are needed. The algorithm relies on a synset-aligned bilingual dictionary and in-domain corpora from the two languages. A symmetric generalized Expectation Maximization formulation is used wherein the sense distributions of words in one language are estimated based on the raw counts of the words in the aligned synset in the target language. The overall performance of our algorithm when tested on 4 language-domain pairs is better than current state-of-the-art knowledge based and bilingual unsupervised approaches. 
Compositional Distributional Semantic methods model the distributional behavior of a compound word by exploiting the distributional behavior of its constituent words. In this setting, a constituent word is typically represented by a feature vector conﬂating all the senses of that word. However, not all the senses of a constituent word are relevant when composing the semantics of the compound. In this paper, we present two different methods for selecting the relevant senses of constituent words. The ﬁrst one is based on Word Sense Induction and creates a static multi prototype vectors representing the senses of a constituent word. The second creates a single dynamic prototype vector for each constituent word based on the distributional properties of the other constituents in the compound. We use these prototype vectors for composing the semantics of noun-noun compounds and evaluate on a compositionality-based similarity task. Our results show that: (1) selecting relevant senses of the constituent words leads to a better semantic composition of the compound, and (2) dynamic prototypes perform better than static prototypes. 
Event extraction involves the identification of instances of a type of event, along with their attributes and participants. Developing a training corpus by annotating events in text is very labor intensive, and so selecting informative instances to annotate can save a great deal of manual work. We present an active learning (AL) strategy, pseudo co-testing, based on one view from a classifier aiming to solve the original problem of event extraction, and another view from a classifier aiming to solve a coarser granularity task. As the second classifier can provide more graded matching from a wider scope, we can build a set of pseudocontention-points which are very informative, and can speed up the AL process. Moreover, we incorporate multiple selection criteria into the pseudo cotesting, seeking training examples that are informative, representative, and varied. Experiments show that pseudo co-testing can reduce annotation labor by 81%; incorporating multiple selection criteria reduces the labor by a further 7%. 
In event-based Information Extraction systems, a major task is the automated ﬁlling from unstructured texts of a template gathering information related to a particular event. Such template ﬁlling may be a hard task when the information is scattered throughout the text and mixed with similar pieces of information relative to a different event. We propose in this paper a twostep approach for template ﬁlling: ﬁrst, an event-based segmentation is performed to select the parts of the text related to the target event; then, a graph-based method is applied to choose the most relevant entities in these parts for characterizing the event. An evaluation of this model based on an annotated corpus for earthquake events shows that we achieve a 77% F1-measure for the template-ﬁlling task. 
Supervised approaches to Relation Extraction (RE) are characterized by higher accuracy than unsupervised models. Unfortunately, their applicability is limited by the need of training data for each relation type. Automatic creation of such data using Distant Supervision (DS) provides a promising solution to the problem. In this paper, we study DS for designing endto-end systems of sentence-level RE. In particular, we propose a joint model between Web data derived with DS and manually annotated data from ACE. The results show (i) an improvement on the previous state-of-the-art in ACE, which provides important evidence of the beneﬁt of DS; and (ii) a rather good accuracy on extracting 52 types of relations from Web data, which suggests the applicability of DS for general RE. 
Open information extraction (IE) is a weakly supervised IE paradigm that aims to extract relation-independent information from large-scale natural language documents without signiﬁcant annotation efforts. A key challenge for Open IE is to achieve self-supervision, in which the training examples are automatically obtained. Although the feasibility of Open IE systems has been demonstrated for English, utilizing such techniques to build the systems for other languages is problematic because previous self-supervision approaches require language-speciﬁc knowledge. To improve the cross-language portability of Open IE systems, this paper presents a self-supervision approach that exploits parallel corpora to obtain training examples for the target language by projecting the annotations onto the source language. The merit of our method is demonstrated using a Korean Open IE system developed without any language-speciﬁc knowledge. 
This paper analyzes the effect of the structural variation of sentences on parsing performance. We examine the performance of both shallow and deep parsers for two sentence constructions: imperatives and questions. We ﬁrst prepare an annotated corpus for each of these sentence constructions by extracting sentences from a ﬁction domain that cover various types of imperatives and questions. The target parsers are then adapted to each of the obtained corpora as well as the existing query-focused corpus. Analysis of the experimental results reveals that the current mainstream parsing technologies and adaptation techniques cannot cope with different sentence constructions even with much in-domain data. 
We present a discriminative model for Japanese zero anaphora resolution that simultaneously determines an appropriate case frame for a given predicate and its predicate-argument structure. Our model is based on a log linear framework, and exploits lexical features obtained from a large raw corpus, as well as non-lexical features obtained from a relatively small annotated corpus. We report the results of zero anaphora resolution on Web text and demonstrate the effectiveness of our approach. In addition, we also investigate the relative importance of each feature for resolving zero anaphora in Web text. 
We compare two types of methods which deal with unknown words in the context of computational grammars. Methods of the ﬁrst type are based on the idea of supertagging and use a tagger to predict lexical descriptions for unknown tokens in a given input. The second type of methods perform lexical acquisition (LA) which, in the context of this paper, refers to the automatic acquisition of new lexical entries for the lexicon of a given grammar. The methods are compared based on the effect their application has on the parsing coverage and accuracy of the GG grammar of German (Crysmann, 2003). In particular, we adapt the LA method of Cholakov and van Noord (2010) which was originally developed for the Dutch Alpino system to be used with the GG. Its impact on coverage and accuracy on a test corpus of German newspaper texts is compared to the results reported previously on the same corpus for methods which employed a tagger. Furthermore, in a smaller experiment, we show that the linguistic knowledge this LA method provides can also be used for sentence realisation. 
We introduce a maximum spanning tree (MST) dependency parser that can be trained from partially annotated corpora, allowing for effective use of available linguistic resources and reduction of the costs of preparing new training data. This is especially important for domain adaptation in a real-world situation. We use a pointwise approach where each edge in the dependency tree for a sentence is estimated independently. Experiments on Japanese dependency parsing show that this approach allows for rapid training and achieves accuracy comparable to state-ofthe-art dependency parsers trained on fully annotated data. 
Efﬁcient data structures are necessary for searching large translation rule dictionaries in forest-based machine translation. We propose a breadth-ﬁrst representation of tree structures that allows trees to be stored and accessed efﬁciently. We describe an algorithm that allows incremental search for trees in a forest and show that its performance is orders of magnitude faster than iterative search. A B-tree index is used to store the rule dictionaries. Preﬁx-compressed indexes with a large page size are found to provide a balance of fast search and disk space utilisation. 
Word sequential alignment models work well for similar language pairs, but they are quite inadequate for distant language pairs. It is difﬁcult to align words or phrases of distant languages with high accuracy without structural information of the sentences. In this paper, we pro- pose a Bayesian subtree alignment model that incorporates dependency relations be- tween subtrees in dependency tree struc- tures on both sides. The dependency re- lation model is a kind of tree-based re- ordering model, and can handle non-local reorderings, which sequential word-based models often cannot handle properly. The model is also capable of handling multilevel structures, making it possible to ﬁnd many-to-many correspondences automatically without any heuristic rules. The size of the structures is controlled by nonparametric Bayesian priors. Experimental alignment results show that our model achieves 3.5 points better alignment error rate for English-Japanese than the word sequential alignment model, thereby verifying that the use of dependency information is effective for structurally different language pairs. 
Recent research on multilingual statistical machine translation focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs. This paper provides new insights into what factors make a good pivot language and investigates the impact of these factors on the overall pivot translation performance. Pivot-based SMT experiments translating between 22 IndoEuropean and Asian languages were used to analyze the impact of eight factors (language family, vocabulary, sentence length, language perplexity, translation model entropy, reordering, monotonicity, engine performance) on pivot translation performance. The results showed that 81% of system performance variations can be explained by these factors. 
The unavailability of parallel training corpora in resource-poor languages is a major bottleneck in cost-effective and rapid deployment of statistical machine translation (SMT) technology. This has spurred signiﬁcant interest in active learning for SMT to select the most informative samples from a large candidate pool. This is especially challenging when irrelevant outliers dominate the pool. We propose two supervised sample selection methods, viz. greedy selection and integer linear programming (ILP), based on a novel measure of beneﬁt derived from error analysis. These methods support the selection of diverse and high-impact, yet relevant batches of source sentences. Comparative experiments on multiple test sets across two resource-poor language pairs (English-Pashto and English-Dari) reveal that the proposed approaches achieve BLEU scores comparable to the full system using a very small fraction of all available training data (ca. 6% for E-P and 13% for E-D). We further demonstrate that the ILP method supports global constraints of signiﬁcant practical value. 
We propose a Named Entity (NE) recognition method in which word chunks are repeatedly decomposed and concatenated. We can obtain features from word chunks, such as the ﬁrst word of a word chunk and the last word of a word chunk, which cannot be obtained in word-sequence-based recognition methods. However, each word chunk may include a part of an NE or multiple NEs. To solve this problem, we use the following operators: SHIFT for separating the ﬁrst word from a word chunk, POP for separating the last word from a word chunk, JOIN for concatenating two word chunks, and REDUCE for assigning an NE label to a word chunk. We evaluate our method on a Japanese NE recognition data set that includes about 200,000 annotations of 191 types of NEs from over 8,500 news articles. The experimental results show that the training and processing speeds of our method are faster than those of a linear-chain structured perceptron and a semi-Markov perceptron while high accuracy is maintained. 
The representative method of using morphological evidence for Chinese unknown word (UW) extraction is Chinese word segmentation (CWS) model, and the method of using distributional evidence for UW extraction is accessor variety (AV) criterion. However, neither of these methods has been veriﬁed on large-scale corpus. In this paper, we propose extensions to remedy the drawbacks of these two methods to handle large-scale corpus: (1) for CWS, we propose a generalized deﬁnition of word to improve the recall; and (2) for AV, we propose a restricted version to decrease noise. We carry out experiments on a Chinese Web corpus with approximate 200 billion Chinese characters. Experimental results show that our methods outperform the baselines, and the combination of the two evidences can further improve the performance. Moreover, our methods can also efﬁciently segment the corpus on the ﬂy, which is especially valuable for processing large-scale corpus. 
 1Department of Computer Science, National Tsing Hua University,  300 No. 101, Section 2, Kuang-Fu Road, Hsinchu, Taiwan, R.O.C. 2Intelligent Agent Systems Lab., Institute of Information Science, Academia Sinica,  128 Academia Road, Sec.2, Nankang, Taipei, Taiwan, R.O.C. 3Department of Computer Science & Engineering, Yuan Ze University  135 Yuan-Tung Road, Chungli, Taoyuan, Taiwan, R.O.C.  hongjie@iis.sinica.edu.tw  thtsai@saturn.yzu.edu.tw  hsu@iis.sinica.edu.tw  Abstract* Entity linking (EL) is the task of linking a textual named entity mention to a knowledge base entry. It is a difficult task involving many challenges, but the most crucial problem is entity ambiguity. Traditional EL approaches usually employ different constraints and filtering techniques to improve performance. However, these constraints are executed in several different stages and cannot be used interactively. In this paper, we propose several disambiguation formulae/features and employ a Markov logic network to model interweaved constraints found in one type of EL, gene mention linking. To assess our systems effectiveness in different applications, we adopt two evaluation schemes: article-wide and instance-based precision/recall/F-measure. Experimental results show that our system outperforms the baseline systems and state-of-the-art systems under both evaluation schemes. 
News comment is a new text genre in the Web 2.0 era. Many people often write comments to express their opinions about recent news events or topics after they read news articles. Because news comments are freely written without checking, they are very different from formal news texts. In particular, named entities in news comments are usually composed of some wrongly written words, informal abbreviations or aliases, which brings great difficulties for machine detection and understanding. This paper addresses the task of named entity recognition in Chinese news comments on the Web. We propose to leverage the entity information in the referred news article to improve named entity recognition in the news comments. Three different schemes are investigated to find useful entities in the news article for new feature generation in the CRFs model. Finally, a dictionary-based correction step is employed to further improve the results. We manually labelled a benchmark dataset with 60 pieces of news and 6000 comments downloaded from a popular Chinese news portal – www.sina.com.cn. The experimental results on the dataset show that our method is effective for this special task. 
Word lists have become available for most of the world’s languages, but only a small fraction of such lists contain cognate information. We present a machine-learning approach that automatically clusters words in multilingual word lists into cognate sets. Our method incorporates a number of diverse word similarity measures and features that encode the degree of afﬁnity between pairs of languages. The output of the classiﬁcation algorithm is then used to generate cognate groups. The results of the experiments on word lists representing several language families demonstrate the utility of the proposed approach. 
This paper proposes a method for extending WordNet with terms in Wikipedia. Our method identiﬁes a WordNet synset by integrating evidence derived from the structure of an article in Wikipedia and distributional similarity of terms. Unlike previous methods, utilizing the hypernym and siblings of the target term acquired from Wikipedia, the proposed method can deal with terms other than Wikipedia article titles and can work well even when reliable distributional similarity of a target term is unavailable. Experiments show that the proposed method can identify synsets for 2,039,417 inputs at precision rate of 84%. Furthermore, it is estimated from the experimental results that there should be 328,572 terms among all the inputs whose synset our method can correctly identify, while previous methods relying only on distributional similarity and lexico-syntactic patterns cannot. 
By today, no lexical resource can claim to be fully comprehensive or perform best for every NLP task. This caused a steep increase of resource alignment research. An important challenge is thereby the alignment of differently represented word senses, which we address in this paper. In particular, we propose a new automatically aligned resource of Wiktionary and WordNet that has (i) a very high domain coverage of word senses and (ii) an enriched sense representation, including pronunciations, etymologies, translations, etc. We evaluate our alignment both quantitatively and qualitatively, and explore how it can contribute to practical tasks. 
We investigate the problem of parsing the noisy language of social media. We evaluate four Wall-Street-Journal-trained statistical parsers (Berkeley, Brown, Malt and MST) on a new dataset containing 1,000 phrase structure trees for sentences from microblogs (tweets) and discussion forum posts. We compare the four parsers on their ability to produce Stanford dependencies for these Web 2.0 sentences. We ﬁnd that the parsers have a particular problem with tweets and that a substantial part of this problem is related to POS tagging accuracy. We attempt three retraining experiments involving Malt, Brown and an in-house Berkeley-style parser and obtain a statistically signiﬁcant improvement for all three parsers. 
Recent advances in automatic knowledge acquisition methods have enabled us to construct massive knowledge bases of semantic relations. Most previous work has focused on semantic relations explicitly expressed in single sentences. Our goal in this work is to obtain valid non-single sentence relation instances, which are not written in any single sentence and may not be even written in a large corpus. We develop a method to infer new semantic relation instances by applying auto-discovered inference rules, and show that our method inferred a considerable number of valid instances that were not written in single sentences even in 600 million Web pages. 
Although some multiword expressions (MWEs) like How do you do? have exclusively idiomatic meaning, other MWEtypes like the phrase kick the bucket may be idiomatic or literal depending on context. The recently developed OpenMWE corpus provides the largest freely available collection of annotated MWE-tokens suitable for supervised classiﬁcation, but so far its potential has only been superﬁcially investigated and only for classiﬁcation of MWE-types in the corpus. Instead, we train and evaluate classiﬁers for crosstype classiﬁcation and introduce novel features specialised to this task. Our best crosstype classiﬁers performed as well on non-trained MWE-types as a majority class baseline which has knowledge of the MWE-type. 
Community-based Question Answering services contain many threads consisting of a question and its answers. When there are many answers for a question, it is hard for a user to understand them all. To address this problem, we focus on logical relations between answers in a thread and present a model for identifying the relations between the answers. We consider that there are constraints among the relations, such as a transitive law, and that it might be useful to take these constraints into account. To consider these constraints, we propose the model based on a Markov logic network. We also introduce super-relations to give additional information for logical relation identiﬁcation into our model. Through the experiment, we show that global constraints and super-relations make it easier to identify the relations. 
This paper proposes a method that automatically generates questions from queries for community-based question answering (cQA) services. Our query-to-question generation model is built upon templates induced from search engine query logs. In detail, we ﬁrst extract pairs of queries and user-clicked questions from query logs, with which we induce question generation templates. Then, when a new query is submitted, we select proper templates for the query and generate questions through template instantiation. We evaluated the method with a set of short queries randomly selected from query logs, and the generated questions were judged by human annotators. Experimental results show that, the precision of 1-best and 5best generated questions is 67% and 61%, respectively, which outperforms a baseline method that directly retrieves questions for queries in a cQA site search engine. In addition, the results also suggest that the proposed method can improve the search of cQA archives. 
Question classification is a crucial preprocessing for question answering system; it can help to make sure the user’s intention. Most of previous researches focus on the feature driven methods that represent a question with a bag of features, which ignore the important information contained in the words order and distance. To take such information into account, this paper proposes to describe the questions via the ExCSR (Extended Class Sequential Rule) model. To mine ExCSR rules, a method based on PrefixSpan, called DS-SRM (Distance Sensitive Sequential Rule Miner), is presented as well. Due to the existence of redundancy in the mined rules, a rule selection algorithm MCRSelection (Most Cover Rule Selection) is also proposed to find the most interesting rules. Experiments results on UIUC question set1 show that the proposed method can achieve the accuracy of 90.6%, which outperforms the previously reported results. 
Garbage in and garbage out. A Q&A system must receive a well formulated question that matches the user’s intent or she has no chance to receive satisfactory answers. In this paper, we propose a keywords to questions (K2Q) system to assist a user to articulate and reﬁne questions. K2Q generates candidate questions and reﬁnement words from a set of input keywords. After specifying some initial keywords, a user receives a list of candidate questions as well as a list of reﬁnement words. The user can then select a satisfactory question, or select a reﬁnement word to generate a new list of candidate questions and reﬁnement words. We propose a User Inquiry Intent (UII) model to describe the joint generation process of keywords and questions for ranking questions, suggesting reﬁnement words, and generating questions that may not have previously appeared. Empirical study shows UII to be useful and effective for the K2Q task. 
This paper regards social Q&A collections, such as Yahoo! Answer as a knowledge repository and investigates techniques to mine knowledge from them for improving a sentence-based complex question answering (QA) system. In particular, we present a question-type-speciﬁc method (QTSM) that studies at extracting question-type-dependent cue expressions from the social Q&A pairs in which question types are the same as the submitted question. The QTSM is also compared with question-speciﬁc and monolingual translation-based methods presented in previous work. Thereinto, the question-speciﬁc method (QSM) aims at extracting question-dependent answer words from social Q&A pairs in which questions are similar to the submitted question. The monolingual translationbased method (MTM) learns word-toword translation probabilities from all social Q&A pairs without consideration of question and question type. Experiments on extension of the NTCIR 2008 Chinese test data set verify the performance ranking of these methods as: QTSM > {QSM, MTM}. The largest F3 improvements of the proposed QTSM over the QSM and MTM reach 6.0% and 5.8%, respectively. 
This paper describes efforts of NLP researchers to create a system to aid the relief efforts during the 2011 East Japan Earthquake. Speciﬁcally, we created a system to mine information regarding the safety of people in the disaster-stricken area from Twitter, a massive yet highly unorganized information source. We describe the large scale collaborative effort to rapidly create robust and effective systems for word segmentation, named entity recognition, and tweet classiﬁcation. As a result of our efforts, we were able to effectively deliver new information about the safety of over 100 people in the disasterstricken area to a central repository for safety information. 
This paper describes a two-phase method for expanding abbreviations found in informal text (e.g., email, text messages, chat room conversations) using a machine translation system trained at the character level during the ﬁrst phase. In this way, the system learns mappings between character-level “phrases” and is much more robust to new abbreviations than a word-level system. We generate translation models that are independent of the way in which the abbreviations are formed and show that the results show little degradation compared to when type-dependent models are trained. Our experiments on a large data set show our proposed system performs well when tested both on isolated abbreviations and, with the incorporation of a second phase utilizing an in-domain language model, in the context of neighboring words. 
In this paper we address the problem of obtaining structured information about products in the form of attribute-value pairs by leveraging a combination of enterprise internal product descriptions and external data. Product descriptions are short text strings used internally within enterprises to describe a product. These strings usually comprise of the Brand name, name of the product, and its attributes like size, color, etc. Existing product data quality solutions provide us the capability to standardize and segment these descriptions into their composing attributes using domain speciﬁc rulesets. We provide techniques that can leverage the supervision provided by these existing rulesets for extracting missing values from other external text data sources accurately. We use a large real life data collection to demonstrate the effectiveness of our approach. 
Recent years have seen an exponential growth in the amount of multilingual text available on the web. This situation raises the need for novel applications for organizing and accessing multilingual content. Common examples of such applications include Multilingual Topic Tracking, Cross-Language Information retrieval systems etc. Most of these applications rely on the availability of multilingual lexical resources which require signiﬁcant effort to create. In this paper we present an unsupervised method for building bilingual topic hierarchies. In a bilingual topic hierarchy, topics (where a topic is a distribution over words) are arranged in a hierarchical fashion with abstract topics appearing near the root of the hierarchy and more concrete topics near the leaves. Such bilingual topic hierarchies can be useful for organizing bilingual corpus based on common topics, cross-lingual information retrieval and cross-lingual text classiﬁcation. Our method builds upon the prior work done on Bayesian non-parametric inferencing of topic hierarchies and multilingual topic modeling to extract bilingual topic hierarchies from unaligned text. We demonstrate the effectiveness of our algorithm in extracting such topic hierarchies from a collection of bilingual text passages and FAQs. 
This paper addresses the issue of redundant data in large-scale collections of Q&A forums. We propose and evaluate a novel algorithm for automatically detecting the near-duplicate Q&A threads. The main idea is to use the distributed index and MapReduce framework to calculate pairwise similarity and identify redundant data fast and scalably. The proposed method was evaluated on a real-world data collection crawled from a popular Q&A forum. Experimental results show that our proposed method can eﬀectively and eﬃciently detect nearduplicate content in large web collections. 
In this paper, we formalize the task of ﬁnding a knowledge base entry that a given named entity mention refers to, namely entity linking, by identifying the most “important” node among the graph nodes representing the candidate entries. With the aim of ranking these entities by their “importance”, we introduce three degree-based measures of graph connectivity. Experimental results on the TACKBP benchmark data sets show that our graph-based method performs comparably with the state-of-the-art methods. We also show that using the name phrase feature outperforms the commonly used bagof-word feature for entity linking. 
This paper addresses the problem of related entity extraction and focuses on extracting related persons as a case study. The proposed method builds on a search engine. Specifically, we mine candidate related persons for a query person q using q’s search results and the query logs containing q. The acquired candidates are then automatically rated and ranked using a SVM regression model that investigates multiple features. Experimental results on a set of 200 randomly sampled query persons show that the precision of the extracted top-1, 5, and 10 related persons exceeds 91%, 90%, and 84%, respectively, which significantly outperforms a state-ofthe-art baseline. 
This paper proposes a method for automatically acquiring strongly-related events from a large corpus using predicateargument co-occurring statistics and case frames. The strongly-related events are acquired in the form of strongly-related two predicates with their relevant arguments. First, strongly-related events are acquired from predicate-argument cooccurring statistics. Then, the remaining argument alignment is performed by using case frames. We conducted experiments using a Web corpus consisting of 1.6G sentences. The accuracy for the extracted event pairs was 96%, and the accuracy of the argument alignment was 79%. The number of acquired event pairs was about 20 thousands. 
We present a novel relevance feedback (RF) method that uses not only the surface information in texts, but also the latent information contained therein. In the proposed method, we infer the latent topic distribution in user feedback and in each document in the search results using latent Dirichlet allocation, and then we modify the search results so that documents with a similar topic distribution to that of the feedback are re-ranked higher. Evaluation results show that our method is effective for both explicit and pseudo RF, and that it has the advantage of performing well even when only a small amount of user feedback is available. 
In this paper, we propose a keyword-based passage retrieval algorithm for information extraction, trained by distant supervision. Our goal is to be able to extract attributes of people and organizations more quickly and accurately by first ranking all the potentially relevant passages according to their likelihood of containing the answer and then performing a traditional deeper, slower analysis of individual passages. Using Freebase as our source of known relation instances and Wikipedia as our text source, we collected a weighted set of keywords indicative of each relation and then use it to re-rank the passages retrieved by the Lemur search engine. Experiments show that our algorithm significantly outperforms stateof-the-art passage retrieval techniques in evaluations of both individual passage retrieval and end-to-end information extraction. 
We present results from improving vector space based extraction summarizers. The summarizer uses Random Indexing and Page Rank to extract those sentences whose importance are ranked highest for a document, based on vector similarity. Originally the summarizer used only word vectors based on the words in the document to be summarized. By using a larger word space model the performance of the summarizer was improved. Along with the performance, robustness was improved as random seeds did not affect the summarizer as much as before, making for more predictable results from the summarizer. 
Parsing discourse is a challenging natural language processing task. In this paper we take a data driven approach to identify arguments of explicit discourse connectives. In contrast to previous work we do not make any assumptions on the span of arguments and consider parsing as a token-level sequence labeling task. We design the argument segmentation task as a cascade of decisions based on conditional random ﬁelds (CRFs). We train the CRFs on lexical, syntactic and semantic features extracted from the Penn Discourse Treebank and evaluate feature combinations on the commonly used test split. We show that the best combination of features includes syntactic and semantic features. The comparative error analysis investigates the performance variability over connective types and argument positions. 
Relational lasso is a method that incorporates feature relations within machine learning. By using automatically obtained noisy relations among features, relational lasso learns an additional penalty parameter per feature, which is then incorporated in terms of a regularizer within the target optimization function. Relational lasso has been tested on three different tasks: text categorization, polarity estimation, and parsing, where it was compared with conventional lasso and adaptive lasso (Zou, 2006) when using a multi-class logistic regression optimization method. Relational lasso outperformed these other lasso methods in the tests. 
Recent large-scale hierarchical classiﬁcation tasks typically have tens of thousands of classes as well as a large number of samples, for which the dominant solution is the top-down method due to computational complexity. However, the top-down method suffers from accuracy deﬁciency, that is, its accuracy is generally lower than that of the ﬂat approach of 1-vs-Rest. In this paper, we employ meta-classiﬁcation technique to enhance the classifying procedure of the top-down method. We analyze the proposed method on the aspect of accuracy, and then test it with two realworld large-scale data sets. Our method both maintains the efﬁciency of the conventional top-down method and provides competitive classiﬁcation accuracies. 
Multi-modality manifold-ranking is recently used successfully in topic-focused multi-document summarization. This approach is based on Bag-Of-Words (BOW) assumption where the pair-wise similarity values between sentences are computed using the standard cosine similarity measure (TF*IDF). However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and disregards the syntactic and semantic information. In this paper, we propose the use of syntactic and shallow semantic kernels for computing the relevance between the sentences. We argue that the addition of syntactic and semantic information can improve the performance of the multimodality manifold-ranking algorithm. Extensive experiments on the DUC benchmark datasets prove the effectiveness of our approach. 
Domain adaptation (DA), which involves adapting a classiﬁer developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper describes how the optimal method for DA was determined depending on these properties using decision tree learning, given a triple of the target word type of WSD, the source data, and the target data, and discusses what properties affected the determination of the best method when Japanese WSD was performed. 
Tense prediction can be useful for many language processing tasks, such as temporal inference and machine translation. In this paper, we investigate using diverse contextual features for Chinese tense prediction under a statistical learning framework. Because of lack of annotated training data, we propose to leverage ChineseEnglish parallel corpora to automatically generate reference tense for model training. We also propose to use an iterative learning framework to deal with the noisy reference data to improve learning. Evaluation is performed using both automatically generated reference data and a manually annotated set with verb tense. Our results demonstrate the effectiveness of our proposed learning framework that maps annotation from one language to another using parallel data. Furthermore, we show better performance using our proposed iterative bootstrapping learning method compared to using the original automatically created training data. 
 This paper describes a new Markov Logic approach for Japanese Predicate-Argument (PA) relation extraction. Most previous work built separated classiﬁers corresponding to each case role and independently identiﬁed the PA relations, neglecting dependencies (constraints) between two or more PA relations. We propose a method which collectively extracts PA relations by optimizing all argument candidates in a sentence. Our method can jointly consider dependency between multiple PA relations and ﬁnd the most probable combination of predicates and their arguments in a sentence. In addition, our model involves new constraints to avoid considering inappropriate candidates for arguments and identify correct PA relations eﬀectively. Compared to the state-of-the-art, our method achieves competitive results without largescale data. 
SciPo is a system whose ultimate goal is to support novice writers in producing academic texts in Brazilian Portuguese through presentation of critiques and suggestions. Currently, it focuses on the rhetorical structure of texts, being capable of automatically detecting and criticizing the rhetorical structure of Abstract sections. We describe a system that enhances SciPo’s functionality by evaluating aspects of semantic coherence in academic abstracts. This system identiﬁes features of sentences based on semantic similarity measures and rhetorical structure. Different machine learning algorithms were trained and evaluated with these features, resulting in three classiﬁers capable of detecting speciﬁc coherence issues on sentences with regard to a rhetorical structure model for abstracts. Results indicate that the system yields higher performance than the baseline for all classiﬁers. 
This paper presents a hierarchical Bayesian model based on latent Dirichlet allocation (LDA), called subjLDA, for sentence-level subjectivity detection, which automatically identiﬁes whether a given sentence expresses opinion or states facts. In contrast to most of the existing methods relying on either labelled corpora for classiﬁer training or linguistic pattern extraction for subjectivity classiﬁcation, we view the problem as weakly-supervised generative model learning, where the only input to the model is a small set of domain independent subjectivity lexical clues. A mechanism is introduced to incorporate the prior information about the subjectivity lexical clues into model learning by modifying the Dirichlet priors of topic-word distributions. The subjLDA model has been evaluated on the Multi-Perspective Question Answering (MPQA) dataset and promising results have been observed in the preliminary experiments. We have also explored adding neutral words as prior information for model learning. It was found that while incorporating subjectivity clues bearing positive or negative polarity can achieve a signiﬁcant performance gain, the prior lexical information from neutral words is less effective. 
In this paper, we shall introduce a system for extracting the keyphrases for the reason of authors’ opinion from product reviews. The datasets for two fairly different product review domains related to movies and mobile phones were constructed semiautomatically based on the pros and cons entered by the authors. The system illustrates that the classic supervised keyphrase extraction approach – mostly used for scientiﬁc genre previously – could be adapted for opinion-related keyphrases. Besides adapting the original framework to this special task through deﬁning novel, taskspeciﬁc features, an efﬁcient way of representing keyphrase candidates will be demonstrated as well. The paper also provides a comparison of the effectiveness of the standard keyphrase extraction features and that of the system designed for the special task of opinion expression mining. 
Existing research on sentiment analysis mainly uses sentiment words and phrases to determine sentiments expressed in documents and sentences. Techniques have also been developed to find such words and phrases using dictionaries and domain corpora. However, there are still other types of words and phrases that do not bear sentiments on their own, but when they appear in some particular contexts, they imply positive or negative opinions. One class of such words or phrases is those that express resources such as water, electricity, gas, etc. For example, “this washer uses a lot of electricity” is negative but “this washer uses little water” is positive. Extracting such resource words and phrases are important for sentiment analysis. This paper formulates the problem based on a bipartite graph and proposes a novel iterative algorithm to solve the problem. Experimental results using diverse real-life sentiment corpora show good results. 
We propose a new subjectivity classiﬁcation at the segment level that is more appropriate for discourse-based sentiment analysis. Our approach automatically distinguish between subjective nonevaluative and objective segments and between implicit and explicit opinions, by using local and global context features. 
There is serious data sparseness problem in Chinese dictionary stored in a doublearray trie. This paper proposes six compression methods by code mapping and code dividing to make it more compact, and a metric called Resource Consumption Ratio is proposed to evaluate these methods. Under the proposed criteria, ﬁve of the six methods are better than the baseline. The best method maps the character code into its frequency order, and then divides it into two jump codes. It achieves a space usage reduction of 39.88% and takes only 0.20% time of the baseline on the construction while it takes 13.21% more time on the retrieval. As preprocessing methods, these methods can be used to reduce more space by combining to other compression method which improves the double-array structure itself. 
We propose a bootstrapping algorithm which successfully resolves two fundamental tasks: morphology acquisition and the acquisition of a subset of functional words. Given the outputs of these fundamental tasks, we build a nearly state-of-art morphology analyzer performing with a F1-score of 80.94%; also, we can improve the baseline model for acquiring functional words by an absolute error reduction of 26%. Furthermore, with these acquisition outputs, a minimally supervised tagging system proposed before can be turned into a totally unsupervised one, achieving a tagging accuracy of 85.26% for openclass words. 
In this paper, we first carry out an investigation on two existing pivot strategies for statistical machine transliteration, namely system-based and model-based strategies, to figure out the reason why the previous model-based strategy performs much worse than the system-based one. We then propose a joint alignment algorithm to optimize transliteration alignments jointly across source, pivot and target languages to improve the performance of the modelbased strategy. In addition, we further propose a novel synthetic data-based strategy, which artificially generates source-target data using pivot language. Experimental results on benchmarking data show that the proposed joint alignment optimization algorithm significantly improves the accuracy of model-based strategy and the proposed synthetic data-based strategy is very effective for pivot-based machine transliteration. 
We address the problem of joint part-of-speech (POS) tagging and dependency parsing in Chinese. In Chinese, some POS tags are often hard to disambiguate without considering longrange syntactic information. Also, the traditional pipeline approach to POS tagging and dependency parsing may suffer from the problem of error propagation. In this paper, we propose the ﬁrst incremental approach to the task of joint POS tagging and dependency parsing, which is built upon a shift-reduce parsing framework with dynamic programming. Although the incremental approach encounters difﬁculties with underspeciﬁed POS tags of look-ahead words, we overcome this issue by introducing so-called delayed features. Our joint approach achieved substantial improvements over the pipeline and baseline systems in both POS tagging and dependency parsing task, achieving the new state-of-the-art performance on this joint task. 
This paper presents work on extending the adverbial entries of LGLex, a NLP oriented syntactic resource for French. Adverbs were extracted from the Lexicon-Grammar tables of both simple adverbs ending in –ment „–ly‟ (Molinier and Levrier, 2000) and compound adverbs (Gross, 1986; 1990). This work relies on the exploitation of fine-grained linguistic information provided in existing resources. Various features are encoded in both LG tables and they haven‟t been exploited yet. They describe the relations of deleting, permuting, intensifying and paraphrasing that associate, on the one hand, the simple and compound adverbs and, on the other hand, different types of compound adverbs. The resulting syntactic resource is manually evaluated and freely available under the LGPL-LR license. 
Treebanks are valuable resources for natural language processing (NLP). There is much work in NLP which converts treebanks from one representation (e.g., phrase structure) to another (e.g., dependency) before applying machine learning. This paper provides a framework in which to think about the question of when such a conversion is possible. 
A method for deriving an approximately labeled dependency treebank from the Thai Categorial Grammar Treebank has been implemented. The method involves a lexical dictionary for assigning dependency directions to the CG types associated with the grammatical entities in the CG bank, falling back on a generic mapping of CG types in case of unknown words. Currently, all but a handful of the trees in the Thai CG bank can unambiguously be transformed into directed dependency trees. Dependency labels can optionally be assigned with a learned classiﬁer, which in a preliminary evaluation with a very small training set achieves 76.5% label accuracy. In the process, a number of annotation errors in the CG bank were identiﬁed and corrected. Although rather limited in its coverage, excluding e.g. long-distance dependencies, topicalisations and longer sentences, the resulting treebank is believed to be sound in terms of structural annotational consistency and a valuable complement to the scarce Thai language resources in existence. 
Existing work shows that lexical dependencies are helpful for constituent tree parsing. However, only first-order lexical dependencies have been employed and investigated in previous work. In this paper, we propose a method to employing higher-order1 lexical dependencies for constituent tree evaluation. Our method is based on a parse reranking framework, which provides a constrained search space (via N-best lists or parse forests) and enables our parser to employ relatively complicated dependency features. We evaluate our models on the Penn Chinese Treebank. The highest F1 score reaches 85.74%, thus outperforming all previously reported state-of-the-art systems. The dependency accuracy of constituent trees generated by our parser has been significantly improved as well. 
In this paper, we propose a factored parsing model consisting of a lexical and a constituent model. The discriminative lexical model allows the parser to utilize rich contextual features beyond those encoded in the context-free grammar (CFG) in use. Experiment results reveal that our parser achieves statistically signiﬁcant improvement in both parsing and tagging accuracy on both English and Chinese. 
Named Entity Recognition (NER) is a well-known Natural Language Processing (NLP) task, used as a preliminary processing to provide a semantic level to more complex tasks. In this paper we describe a new set of named entities having a multilevel tree structure, where base entities are combined to deﬁne more complex ones. This deﬁnition makes the NER task more complex than previous tasks, even more due to the use of noisy data for the annotation: transcriptions of French broadcast data. We propose an original and effective system to tackle this new task, putting together the strengths of solutions for sequence labeling approaches and syntactic parsing via cascading of different models. Our system was evaluated in the 2011 Quaero named entity detection evaluation campaign and ranked ﬁrst, with results far better than those of the other participating systems. 
This paper investigates clausal data-driven dependency parsing. We first motivate a clause as the minimal parsing unit by correlating inter- and intra-clausal relations with relation type, depth, arc length and non-projectivity. This insight leads to a two-stage formulation of parsing where intra-clausal relations are identified in the 1st stage and inter-clausal relations are identified in the 2nd stage. We compare two ways of implementing this idea, one based on hard constraints (similar to the one used in constraint-based parsing) and one based on soft constraints (using a kind of parser stacking). Our results show that the approach using hard constraints seems most promising and performs significantly better than single-stage parsing. Our best result gives significant increase in LAS and UAS, respectively, over the previous best result using single-stage parsing. 
In this paper we study the word-reordering problem in the decoding part of statistical machine translation, but independently from the target language generating process. In this model, a permuted sentence is given and the goal is to recover the correct order. We introduce a greedy algorithm called Local-(k, l)-Step, and show that it performs better than the DP-based algorithm. Our word-reordering algorithm can be used in the statistical machine translation process for improving the quality of the translation. Furthermore, motivated by the rank evaluation method, we introduce a novel way for evaluating the results of word-reordering by calculating the inversion pair cardinality. 
Word alignment is a fundamental step in machine translation. Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. To alleviate this problem, we extract hierarchical rules from weighted alignment matrix (Liu et al., 2009). Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules. To achieve a balance between rule table size and performance, we construct a scoring measure that incorporates both frequency and lexical weight to select the best target phrase for each source phrase. Experiments show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.5 points for tree-to-string model. 
The language specific Multiword expressions (MWEs) play important roles in many natural language processing (NLP) tasks. Integrating reduplicated multiword expressions (RMWEs) into the Phrase Based Statistical Machine Translation (PBSMT) to improve translation quality is reported in the present work between Manipuri, a highly agglutinative Tibeto-Burman language and English. In addition, Multiword Named Entities (MNEs) coupled with Transliterated non-named entities (non-NE) between Manipuri and English phrase based SMT system are also integrated. The tighter integration of RMWEs and NEs into the PBSMT is carried out after automatic extraction using SVM based machine learning technique followed by automatic bilingual RMWE and MNE extraction using GIZA++ alignment. Our experimental results show improvement in the PBSMT system BLEU and NIST scores over the baseline system. Subjective evaluation indicates the improvement in the adequacy. 
Conditional probabilistic models for word alignment are popular due to the elegant way of handling them in the training stage. However, they have weaknesses such as garbage collection and scale poorly beyond single word based models (DeNero et al., 2006): not all parameters should actually be used. To alleviate the problem, in this paper we explore regularity terms that penalize the used parameters. They share the advantages of the standard training in that iterative schemes decompose over the sentence pairs. We explore the models IBM-1 and HMM, then generalize to models we term Bi-word models, where each target word can be aligned to up to two source words. We give two optimization strategies for the arising tasks, using EM and projected gradient descent. While both are well-known, to our knowledge they have never been compared experimentally for the task of word alignment. As a side-effect, we show that, against common belief, for parametric HMMs the M-step is not solved by renormalizing expectations. We demonstrate that the regularity terms improve on the f-measures of the standard HMMs and that they improve translation quality. 
During the last years there is increasing interest in methods that perform some kind of weighting of heterogeneous parallel training data when building a statistical machine translation system. It was for instance observed that training data that is close to the period of the test data is more valuable than older data (Hardt and Elming, 2010; Levenberg et al., 2010). In this paper we obtain such a weighting by resampling alignments using weights that decrease with the temporal distance of bitexts to the test set. By these means, we can use all the available bitexts and still put an emphasis on the most recent one. The main idea of our approach is to use a parametric form or meta-weights for the weighting of the different parts of the bitexts. This ensures that our approach has only few parameters to optimize. We report experimental results on the Europarl corpus, translating from French to English and further veriﬁed it on the ofﬁcial WMT’11 task, translating from English to French. Our method achieves improvements of about 0.6 points BLEU on the test set with respect to a system trained on data without any weighting. 
We present and evaluate a set of architectures for conversational dialogue systems, exploring rule-based and statistical classiﬁcation approaches. In a case study, we show that while a rule-based dialogue policy is capable of high performance if perfect natural language understanding is assumed, a direct classiﬁcation approach that combines the dialogue policy with NLU has practical advantages. 
This paper presents a novel method to remove asymmetry between the source and the target languages thereby improving alignment and machine translation (MT) quality. Some words in the source language are redundant for MT tasks but necessary for the source sentence to be grammatical. This paper proposes a method to automatically detect such words. In addition, constraints under which these words should or should not be removed are extracted automatically from the target language. A lattice scheme is used for test sentences to provide alternate paths with and without removal of these words. Such a constraint-based removal technique gives a signiﬁcant improvement (p < 0.001) of 5.29 BLEU points over the baseline Phrase-based MT system for the English-Hindi language-pair. 
We demonstrate that statistical machine translation (SMT) can be improved substantially by imposing clause-based reordering constraints during decoding. Our analysis of clause-wise translation of different types of clauses shows that it is beneﬁcial to apply these constraints for ﬁnite clauses, but not for non-ﬁnite clauses. In our experiments in English-Hindi translation with an SMT system (DTM2), on a test corpus containing around 850 sentences with manually annotated clause boundaries, BLEU improves to 20.4 from the baseline score of 19.4. This statistically signiﬁcant improvement is also conﬁrmed by subjective (human) evaluation. We also report preliminary work on automatically identifying the kind of clause boundaries appropriate for enforcing reordering constraints. 
Minimum Bayes Risk (MBR) has been used as a decision rule for both singlesystem decoding and system combination in machine translation. For system combination, we argue that common MBR implementations are actually not correct, since probabilities in the hypothesis space cannot be reliably estimated. These implementations achieve the effect of consensus decoding (which may be beneﬁcial in its own right), but does not reduce Bayes Risk in the true Bayesian sense. We introduce Generalized MBR, which parameterizes the loss function in MBR and allows it to be optimized in the given hypothesis space of multiple systems. This extension better approximates the true Bayes Risk decision rule and empirically improves over MBR, even in cases where the combined systems are of mixed quality. 
Carlos Henr´ıquez TALP-UPC Jordi Girona, s/n 08034 Barcelona  marta.ruiz@barcelonamedia.org carlos.henriquez@upc.edu  Rafael E. Banchs Institute for Infocomm Research 
Near-synonym sets represent groups of words with similar meaning, which are useful knowledge resources for many natural language applications such as query expansion for information retrieval (IR) and computer-assisted language learning. However, near-synonyms are not necessarily interchangeable in contexts due to their specific usage and syntactic constraints. Previous studies have developed various methods for near-synonym choice in English sentences. To our best knowledge, there is no such evaluation on Chinese sentences. Therefore, this paper implements two baseline systems: pointwise mutual information (PMI) and a 5-gram language model that are widely used in previous work for Chinese nearsynonym choice evaluation. Experimental results show that the 5-gram language model achieves higher accuracy than PMI. 
This paper addresses the issue of cluster labeling and presents a method for assigning labels by using concepts in a machinereadable dictionary. We assume that salient terms in the cluster content have the same hypernym because hypernymic semantic relation represents a generalization that goes from speciﬁc to generic. Our experimental results reveal that hypernymic semantic relations can be exploited to increase labeling accuracy, as the results of 0.441 F-score improves over the two baselines. 
This paper proposes a weakly-supervised approach for extracting instances of semantic classes. This method constructs simple wrappers automatically based on speciﬁed seed instances and uses a compression model to assess the contextual evidence of its extraction. By adopting this compression model, our approach can better avoid erroneous extractions in a noisy corpus such as the Web. The empirical results show that our system performs quite consistently even when operating on a noisy text with a lot of possibly irrelevant documents. 
We propose a new method for word sense disambiguation for verbs. In our method, sense-dependent selectional preference of verbs is obtained through the probabilistic model on the lexical network. The meanﬁeld approximation is employed to compute the state of the lexical network. The outcome of the computation is used as features for discriminative classiﬁers. The method is evaluated on the dataset of the Japanese word sense disambiguation. 
Word Sense Induction (WSI) is the task of automatically inducing the different senses of a target word from unannotated text. Traditional approaches based on the vector space model (VSM) represent each context of a target word as a vector of selected features (e.g. the words occurring in the context). These approaches assume that the words occurring in the context are independent and do not exploit semantic relevance between words. In this paper we propose a WSI method which can exploit semantic relevance between words by incorporating a word graph into the framework of clustering of context vectors. The method is evaluated on the testing data of the Chinese Word Sense Induction task of the first CIPSSIGHAN Joint Conference on Chinese Language Processing (CLP2010). Experimental results show that our method significantly outperforms the baseline methods. 
In this paper, we compare a resourcedriven approach with a task-speciﬁc classiﬁcation model for a new near-synonym word choice sub-task, predicting whether a full or a clipped form of a word will be used (e.g. doctor or doc) in a given context. Our results indicate that the resourcedriven approach, the use of a formality lexicon, can provide competitive performance, with the parameters of the taskspeciﬁc model mirroring the parameters under which the lexicon was built. 
We describe a new semantic relatedness measure combining the Wikipedia-based Explicit Semantic Analysis measure, the WordNet path measure and the mixed collocation index. Our measure achieves the currently highest results on the WS-353 test: a Spearman ρ coefﬁcient of 0.79 (vs. 0.75 in (Gabrilovich and Markovitch, 2007)) when applying the measure directly, and a value of 0.87 (vs. 0.78 in (Agirre et al., 2009)) when using the prediction of a polynomial SVM classiﬁer trained on our measure. In the appendix we discuss the adaptation of ESA to 2011 Wikipedia data, as well as various unsuccessful attempts to enhance ESA by ﬁltering at word, sentence, and section level. 
Traditional approaches to semantic relatedness are often restricted to text-based methods, which typically disregard other multimodal knowledge sources. In this paper, we propose a novel image-based metric to estimate the relatedness of words, and demonstrate the promise of this method through comparative evaluations on three standard datasets. We also show that a hybrid image-text approach can lead to improvements in word relatedness, conﬁrming the applicability of visual cues as a possible orthogonal information source. 
The world of E-commerce is expanding, posing a large arena of products, their descriptions, customer and professional reviews that are pertinent to them. Most of the product attribute extraction techniques in literature work on structured descriptions using several text analysis tools. However, attributes in these descriptions are limited compared to those in customer reviews of a product, where users discuss deeper and more speciﬁc attributes. In this paper, we propose a novel supervised domain independent model for product attribute extraction from user reviews. The user generated content contains unstructured and semi-structured text where conventional language grammar dependent tools like parts-of-speech taggers, named entity recognizers, parsers do not perform at expected levels. We used Wikipedia and Web to identify product attributes from customer reviews and achieved F1score of 0.73. 
Online forum is an important source that people use to ﬁnd answers to questions. Most search engines simply retrieve “relevant” threads, but those are not necessarily good threads in terms of providing quality answers. In this paper we propose a two-step approach to classify online forum threads according to their informativeness in terms of question answering. We use statistical models to ﬁrst categorize posts inside a thread. Then, a variety of features including post level information and other meta-data information are used to classify the thread. We show promising results using the online support forum data we have collected. 
A learner’s language data of speaking, writing, listening, and reading have been compiled for a learner corpus in this study. The language data consist of linguistic output and language processing. Linguistic output refers to data of pronunciation, sentences, listening comprehension rate, and reading comprehension rate. Language processing refers to processing time and learners’ self-judgment of their difficulty of processing in speaking, listening, and reading and the fluency of their writing. This learner corpus will contribute to making the language learning process more clearly visible. 
We propose an unsupervised bootstrapping method to generate a new type of affect knowledge base: the sentiment expectation of nouns (e.g., “high salary” is desirable while “high price” is usually undesirable, because people have opposite sentiment expectation towards “salary” and “price”). A bootstrapping framework is designed to retrieve patterns that might be used to express complaints from the Web. The sentiment expectation of a noun could be automatically predicted with the output patterns. We evaluate the retrieved patterns and show that our method yields good results. Also, they are applied to improve both sentence and document level sentiment analysis results. 
In this study, we analyzed how answerers indicated unclear points in questions, and how questioners modiﬁed and resubmitted their questions based on indications of unclear points. 
 Abstract Information need is an important factor in question retrieval. This paper proposes a method to diversify the results of question retrieval in term of types of information needs. CogQTaxo, a question hierarchy is leveraged to represent users’ information needs cognitively from three linguistic levels. Based on a prediction model of question types, three factors, i.e., scores of IR model, question type similarity and question type novelty are linearly combined to re-rank the retrieved questions. Preliminary experimental results show that the proposed method enhances the question retrieval performance in information coverage and diversity. 
Non-standard spellings in text messages often convey extra pragmatic information not found in the standard word form. However, text message normalization systems that transform non-standard text message spellings to standard form tend to ignore this information. To address this problem, this paper examines the types of extra pragmatic information that are conveyed by non-standard word forms. Empirical analysis of our data shows that 40% of non-standard word forms contain emotional information not found in the standard form, and 38% contain additional emphasis. This extra information can be important to downstream applications such as text-to-speech synthesis. We further investigated the automatic detection of non-standard forms that display additional information. Our empirical results show that character level features can provide important cues for such detection. 
The challenging issues of discourse relation recognition in Chinese are addressed. Due to the lack of Chinese discourse corpora, we construct a moderate corpus with humanannotated discourse relations. Based on the corpus, a statistical classifier is proposed, and various features are explored in the experiments. The experimental results show that our method achieves an accuracy of 88.28% and an F-Score of 63.69% in four-class classification and achieves an F-Score of 93.57% in the best case. 
 Recent research usually models POS tagging as a sequential labeling problem, in which only local context features can be used. Due to the lack of morphological inﬂections, many tagging ambiguities in Chinese are difﬁcult to handle unless consulting larger contexts. In this paper, we try to improve Chinese POS tagging by using long-distance dependencies produced by a statistical dependency parser. Experimental results show that, despite error propagation, the syntactic features can signiﬁcantly improve the tagging accuracy from 93.88% to 94.41% (p < 10−5). Detailed analysis shows that these features are helpful for ambiguous pairs like {NN,VV} and {DEC,DEG}.1  
In this paper we explore the effect of selftraining on Hindi dependency parsing. We consider a state-of-the-art Hindi dependency parser and apply self-training by using a large raw corpus. We consider two types of raw corpus, one from same domain as of training and testing data and the other from different domain. We also do an experiment, where we add small gold-standard data to the training set. Comparing these experiments, we show the impact of adding small, but gold-standard data to training data versus large, but automatically parsed data on Hindi parser. 
Monolingual corpora which are aligned with similar text segments (paragraphs, sentences, etc.) are used to build and test a wide range of natural language processing applications. The drawback wanting to use them is the lack of publicly available annotated corpora which obligates people to make one themselves. The annotation process is a time consuming and costly task. This paper describes a new corpus-based measure to signiﬁcantly reduce the search space for a faster and easier manual annotation process for monolingual corpora. This measure can be used in making alignments on different types of text segments. The performance of this measure is evaluated on a manually annotated paragraph corpus, whose alignments are freely available, with promising results. 
We introduce a parallel corpus of spoken Cantonese and written Chinese. This sentencealigned corpus consists of transcriptions of Cantonese spoken in television programs in Hong Kong, and their corresponding Chinese (Mandarin) subtitles. Preliminary evaluation shows that the corpus reflects known syntactic differences between Cantonese and Mandarin, facilitates quantitative analyses on these differences, and already reveals some phenomena not yet discussed in the literature. 
The limitations of keyword-only approaches to information retrieval were recognized since the early days, specially in cases where different but closely-related words are used in the query and the relevant document. Query expansion techniques like pseudo-relevance feedback rely on the target document set in order to bridge the gap between those words, but they might suffer from topic drift. This paper explores the use of knowledge-based semantic relatedness in order to bridge the gap between query and documents. We performed query expansion, with positive effects over some language modeling baselines. 
Word Sense Disambiguation (WSD) is one of the fundamental natural language processing tasks. However, lack of training corpora is a bottleneck to construct a high accurate all-words WSD system. Annotating a large-scale corpus by experts costs enormous time and ﬁnancial resources. Human Computation is a novel idea for integrating human resources behind the Web, which has been wasted, to solve practical problems that are difﬁcult for computers. Based on human computation, we design a conﬁrmation code system, which can not only distinguish between human beings and computers (the function of normal conﬁrmation code system), but also annotate WSD corpora. The preliminary experimental result shows that the proposed method can annotate large-scale and high-quality WSD corpora within a short time. To the best of our knowledge, this is the ﬁrst attempt to use conﬁrmation code in natural language processing for corpora acquisition. 
Online discussion forums are a valuable means for users to resolve speciﬁc information needs, both interactively for the participants and statically for users who search/browse over historical thread data. However, the complex structure of forum threads can make it difﬁcult for users to extract relevant information. The discourse structure of web forum threads, in the form of labelled dependency relationships between posts, has the potential to greatly improve information access over web forum archives. In this paper, we present the task of parsing user forum threads to determine the labelled dependencies between posts. Three methods, including a dependency parsing approach, are proposed to jointly classify the links (relationships) between posts and the dialogue act (type) of each link. The proposed methods signiﬁcantly surpass an informed baseline. We also experiment with “in situ” classiﬁcation of evolving threads, and establish that our best methods are able to perform equivalently well over partial threads as complete threads. 
This paper describes an algorithm for exact decoding of phrase-based translation models, based on Lagrangian relaxation. The method recovers exact solutions, with certiﬁcates of optimality, on over 99% of test examples. The method is much more efﬁcient than approaches based on linear programming (LP) or integer linear programming (ILP) solvers: these methods are not feasible for anything other than short sentences. We compare our method to MOSES (Koehn et al., 2007), and give precise estimates of the number and magnitude of search errors that MOSES makes. 
Minimum error rate training is a crucial component to many state-of-the-art NLP applications, such as machine translation and speech recognition. However, common evaluation functions such as BLEU or word error rate are generally highly non-convex and thus prone to search errors. In this paper, we present LP-MERT, an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming. Given a set of N -best lists produced from S input sentences, this algorithm ﬁnds a linear model that is globally optimal with respect to this set. We ﬁnd that this algorithm is polynomial in N and in the size of the model, but exponential in S. We present extensions of this work that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 
We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 
We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We ﬁrst demonstrate that delexicalized parsers can be directly transferred between languages, producing signiﬁcantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the ﬁnal parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can signiﬁcantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 
We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the veriﬁed results. The experimental results show that our new parsers signiﬁcantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 
We present a novel approach to Data-Oriented Parsing (DOP). Like other DOP models, our parser utilizes syntactic fragments of arbitrary size from a treebank to analyze new sentences, but, crucially, it uses only those which are encountered at least twice. This criterion allows us to work with a relatively small but representative set of fragments, which can be employed as the symbolic backbone of several probabilistic generative models. For parsing we deﬁne a transform-backtransform approach that allows us to use standard PCFG technology, making our results easily replicable. According to standard Parseval metrics, our best model is on par with many state-ofthe-art parsers, while offering some complementary beneﬁts: a simple generative probability model, and an explicit representation of the larger units of grammar. 
We present a method that paraphrases a given sentence by ﬁrst generating candidate paraphrases and then ranking (or classifying) them. The candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. The ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. We experimented with both a Maximum Entropy classiﬁer and an SVR ranker. Experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. We also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. The paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind. 
We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora. Our key assumption is that collocation errors are often caused by semantic similarity in the ﬁrst language (L1language) of the writer. An analysis of a large corpus of annotated learner English conﬁrms this assumption. We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms. 
Class-instance label propagation algorithms have been successfully used to fuse information from multiple sources in order to enrich a set of unlabeled instances with class labels. Yet, nobody has explored the relationships between the instances themselves to enhance an initial set of class-instance pairs. We propose two graph-theoretic methods (centrality and regularization), which start with a small set of labeled class-instance pairs and use the instance-instance network to extend the class labels to all instances in the network. We carry out a comparative study with state-of-the-art knowledge harvesting algorithm and show that our approach can learn additional class labels while maintaining high accuracy. We conduct a comparative study between class-instance and instance-instance graphs used to propagate the class labels and show that the latter one achieves higher accuracy. 
This paper presents a model that extends semantic role labeling. Existing approaches independently analyze relations expressed by verb predicates or those expressed as nominalizations. However, sentences express relations via other linguistic phenomena as well. Furthermore, these phenomena interact with each other, thus restricting the structures they articulate. In this paper, we use this intuition to deﬁne a joint inference model that captures the inter-dependencies between verb semantic role labeling and relations expressed using prepositions. The scarcity of jointly labeled data presents a crucial technical challenge for learning a joint model. The key strength of our model is that we use existing structure predictors as black boxes. By enforcing consistency constraints between their predictions, we show improvements in the performance of both tasks without retraining the individual models. 
 to grasp the overview of consumer reviews and opin-  This paper presents a domain-assisted approach to organize various aspects of a product into a hierarchy by integrating domain knowledge (e.g., the product speciﬁcations), as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical  ions on various aspects of a product from such enormous reviews. Among hundreds of product aspects, it is also inefﬁcient for user to browse consumer reviews and opinions on a speciﬁc aspect. Thus, there is a compelling need to organize consumer reviews, so as to transform the reviews into a useful knowl-  organization of consumer reviews on various product aspects and aggregate consumer opinions on these aspects. With such organization, user can easily grasp the overview of consumer reviews. Furthermore, we apply the hierarchy to the task of implicit aspect identi-  edge structure. Since the hierarchy can improve information representation and accessibility (Cimiano, 2006), we propose to organize the aspects of a product into a hierarchy and generate a hierarchical organization of consumer reviews accordingly.  ﬁcation which aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach.  Towards automatically deriving an aspect hierarchy from the reviews, we could refer to traditional hierarchy generation methods in ontology learning, which ﬁrst identify concepts from the text, then determine the parent-child relations between these concepts using either pattern-based or clustering-  
We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-deﬁned sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines. 
Polarity classification of opinionated sentences with both positive and negative sentiments1 is a key challenge in sentiment analysis. This paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities. Firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on Rhetorical Structure Theory (RST). Then, a small set of cuephrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations (SSRs). Finally, an unsupervised method was adopted to generate, weigh and filter new SSRs without cue phrases for recognizing discourse relations. Experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification. 
We present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature. Thus, we can model the compositional effects required for accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., “very”) with a positive polar adjective (e.g., “good”) produces a phrase (“very good”) with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the ﬁrst such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically signiﬁcant improvements in performance over a bagof-words model. 
We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute signiﬁcant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 
 pre-ordering methods require a supervised syntac-  When translating among languages that differ substantially in word order, machine translation (MT) systems beneﬁt from syntactic preordering—an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees au-  tic parser to provide structural information about each sentence. We propose a method that learns both a parsing model and a reordering model directly from a word-aligned parallel corpus. Our approach, which we call Structure Induction for Reordering (STIR), requires no syntactic annotations to train, but approaches the performance of a re-  tomatically from a parallel corpus, instead of  cent syntactic pre-ordering method in a large-scale  using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent pre-  English-Japanese MT system. STIR predicts a pre-ordering via two pipelined models: (1) parsing and (2) tree reordering. The ﬁrst model induces a binary parse, which deﬁnes the space of possible reorderings. In particular, only trees that properly separate verbs from their object  ordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.  noun phrases will license an SVO to SOV transformation. The second model locally permutes this tree. Our approach resembles work with binary synchronous grammars (Wu, 1997), but is distinct in its emphasis on monolingual parsing as a ﬁrst phase, and in selecting reorderings without the aid of a  
Due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. However, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. In this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. We then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. We further explore three algorithms in rule matching: 0-1 matching, likelihood matching, and deep similarity matching. Our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 
Dependency structure, as a ﬁrst step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difﬁcult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the stateof-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the ﬁrst time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models. 
 about its documents (Mimno and McCallum, 2007;  Real document collections do not ﬁt the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model ﬁts a  Chang et al., 2009). In such exploratory data analysis, however, we are not concerned with the ﬁt to held out data. In this paper, we develop and study new methods for evaluating topic models. Our methods are based  corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-deﬁned ways. Our method can identify where a topic model ﬁts the data, where it falls short, and in which directions it might be improved.  on posterior predictive checking, which is a model diagnosis technique from Bayesian statistics (Rubin, 1984; Gelman et al., 1996). The goal of a posterior predictive check (PPC) is to assess the validity of a Bayesian model without requiring a speciﬁc alternative model. Given data, we ﬁrst compute a posterior  
 success is strongly tied to the ability of ﬁnding a  Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. However, in cases where lightweight decompositions are not readily available (e.g., due to the presence of rich features or logical con-  “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient algorithm exhibits extremely slow convergence (cf. Fig. 2). Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural  straints), the original subgradient algorithm  way, or because one would like to incorporate fea-  is inefﬁcient. We sidestep that difﬁculty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how ﬁrst-order logical constraints can be handled efﬁciently, even though the corresponding subproblems are no longer combinatorial,  tures that cannot be easily absorbed in few tractable components. Examples include features generated by statements in ﬁrst-order logic, features that violate Markov assumptions, or history features such as the ones employed in transition-based parsers. To tackle the kind of problems above, we adopt  and report experiments in dependency pars-  DD-ADMM (Alg. 1), a recently proposed algorithm  ing, with state-of-the-art results.  that accelerates dual decomposition (Martins et al.,  2011). DD-ADMM retains the modularity of the  
We exploit sketch techniques, especially the Count-Min sketch, a memory, and time efﬁcient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself. These methods use hashing to deal with massive amounts of streaming text. We apply CountMin sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory). 
 is a strong relationship between the size of topics  and the probability of topics being nonsensical as  Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional sub-  judged by domain experts: as the number of topics increases, the smallest topics (number of word tokens assigned to each topic) are almost always poor quality. The common practice of displaying only a small number of example topics hides the fact that as many as 10% of topics may be so bad that they cannot be shown without reducing users’ conﬁdence.  spaces (topics) that are obviously ﬂawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be ﬂawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that signiﬁcantly improves topic  The evaluation of statistical topic models has traditionally been dominated by either extrinsic methods (i.e., using the inferred topics to perform some external task such as information retrieval (Wei and Croft, 2006)) or quantitative intrinsic methods, such as computing the probability of held-out documents (Wallach et al., 2009). Recent work has focused on evaluation of topics as semantically-  quality in a large-scale document collection from the National Institutes of Health (NIH).  coherent concepts. For example, Chang et al. (2009) found that the probability of held-out documents is  not always a good predictor of human judgments.  
 tiﬁc literature according to categories of information  Argumentative Zoning (AZ) – analysis of the argumentative structure of a scientiﬁc paper – has proved useful for a number of information access tasks. Current approaches to AZ rely on supervised machine learning (ML). Requiring large amounts of annotated data, these approaches are expensive to develop and  structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientiﬁc documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel  port to different domains and tasks. A poten-  et al., 2009), qualitative dimensions (Shatkay et al.,  tial solution to this problem is to use weaklysupervised ML instead. We investigate the performance of four weakly-supervised classiﬁers on scientiﬁc abstract data annotated for multiple AZ classes. Our best classiﬁer based on the combination of active learning and selftraining outperforms our best supervised clas-  2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006;  siﬁer, yielding a high accuracy of 81% when using just 10% of the labeled data. This result suggests that weakly-supervised learning could be employed to improve the practical applicability and portability of AZ across different information access tasks.  Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientiﬁc domains and tasks. A potential solution to this bottleneck is to de-  
This paper presents a new algorithm for linear text segmentation. It is an adaptation of Afﬁnity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Afﬁnity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres – data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highquality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Afﬁnity Propagation formulation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines. 
This paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context. While it has been shown that distributional similarity can help identifying causality, we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events. We show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality. 
 prediction has been demonstrated by studies that  This paper introduces a psycholinguistic model of sentence processing which combines a Hidden Markov Model noun phrase chunker with a co-reference classiﬁer. Both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. This allows  show anticipation based on selectional restrictions: listeners are able to launch eye-movements to the predicted argument of a verb before having encountered it, e.g., they will ﬁxate an edible object as soon as they hear the word eat (Altmann and Kamide, 1999). Semantic prediction has also been shown in the context of semantic priming: a word that is pre-  us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. We evaluate our surprisal predictions on the Dundee corpus of eye-movement data show that our model achieve a better ﬁt with human reading times than a syntax-only model which does not have access to co-reference information.  ceded by a semantically related prime or by a semantically congruous sentence fragment is processed faster (Stanovich and West, 1981; Clifton et al., 2007). An example for syntactic prediction can be found in coordinate structures: readers predict that the second conjunct in a coordination will have the same syntactic structure as the ﬁrst conjunct (Frazier et al., 2000). In a similar vein, having encoun-  
We present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identiﬁcation problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efﬁciently ﬁnds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information. 
 lack of annotated textual data, and (ii) the fact that  In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We deﬁne a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to ﬁnd the morphological analysis for each unlabeled language which lies as close as possible in the feature space to a training language. We apply our model to eight inﬂecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.  they exhibit linguistic structure not found in English, and are thus not immediately susceptible to many traditional NLP techniques. Consider the example of nominal part-of-speech analysis. The Penn Treebank deﬁnes only four English noun tags (Marcus et al., 1994), and as a result, it is easy to treat the words bearing these tags as completely distinct word classes, with no internal morphological structure. In contrast, a comparable tagset for Hungarian includes 154 distinct noun tags (Erjavec, 2004), reﬂecting Hungarian’s rich inﬂectional morphology. When dealing with such languages, treating words as atoms leads to severe data sparsity problems. Because annotated resources do not exist for most morphologically rich languages, prior research has focused on unsupervised methods, with a focus on developing appropriate inductive biases. However, inductive biases and declarative knowledge are notoriously difﬁcult to encode in well-founded models. Even putting aside this practical matter, a universally correct inductive bias, if there is one, is unlikely to  be be discovered by a priori reasoning alone.  
Log-linear parsing models are often trained by optimizing likelihood, but we would prefer to optimise for a task-speciﬁc metric like Fmeasure. Softmax-margin is a convex objective for such models that minimises a bound on expected risk for a given loss function, but its na¨ıve application requires the loss to decompose over the predicted structure, which is not true of F-measure. We use softmaxmargin to optimise a log-linear CCG parser for a variety of loss functions, and demonstrate a novel dynamic programming algorithm that enables us to use it with F-measure, leading to substantial gains in accuracy on CCGBank. When we embed our loss-trained parser into a larger model that includes supertagging features incorporated via belief propagation, we obtain further improvements and achieve a labelled/unlabelled dependency F-measure of 89.3%/94.0% on gold part-of-speech tags, and 87.2%/92.8% on automatic part-of-speech tags, the best reported results for this task. 
We present a system for the large scale induction of cognate groups. Our model explains the evolution of cognates as a sequence of mutations and innovations along a phylogeny. On the task of identifying cognates from over 21,000 words in 218 different languages from the Oceanic language family, our model achieves a cluster purity score over 91%, while maintaining pairwise recall over 62%. 
We explore efﬁcient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora – 1% the size of the original – can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. 
 French parallel corpus is better than one trained  We investigate the diﬀerences between language models compiled from original target-language texts and those compiled from texts manually translated to the target language. Corroborating established observations of Translation Studies, we demonstrate that the latter are signiﬁcantly better predictors of translated sentences than the former, and hence ﬁt the reference set better. Furthermore, translated texts yield better language models for statistical machine translation than original texts.  on French-translated-to-English texts. Our research question is whether a language model compiled from translated texts may similarly improve the results of machine translation. We test this hypothesis on several translation tasks, where the target language is always English. For each language pair we build two English language models from two types of corpora: texts originally written in English, and human translations from the source language into English. We show that for each language pair, the latter language model better ﬁts a set of reference translations in terms of perplexity. We also  
Many machine translation evaluation metrics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment. It has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation. However, to date there has been no unambiguous report that these new metrics can improve a state-of-theart machine translation system over its BLEUtuned baseline. In this paper, we demonstrate that tuning Joshua, a hierarchical phrase-based statistical machine translation system, with the TESLA metrics results in signiﬁcantly better humanjudged translation quality than the BLEUtuned baseline. TESLA-M in particular is simple and performs well in practice on large datasets. We release all our implementation under an open source license. It is our hope that this work will encourage the machine translation community to ﬁnally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems. 
 Different annotation schemes often make differ-  ent assumptions with respect to how linguistic con-  Methods for evaluating dependency parsing using attachment scores are highly sensitive to representational variation between dependency treebanks, making cross-experimental evaluation opaque. This paper develops a robust procedure for cross-experimental evaluation, based on deterministic uniﬁcationbased operations for harmonizing different representations and a reﬁned notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground.  tent is represented in a treebank (Rambow, 2010). The consequence of such annotation discrepancies is that when we compare parsing results across different experiments, even ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reﬂect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll et al., 1998; Cer et al., 2010), evaluating parsers by comparing their performance in an embedding task (Miyao et al.,  2008; Buyko and Hahn, 2010), or neutralizing the  
 applications of parsing technology is the recovery of  In order to obtain a ﬁne-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a correspond-  structural relations, i.e. dependencies at the level of interpretation. We suggest a selection of ten linguistic phenomena that we believe (a) occur with reasonably high frequency in running text and (b) have the potential to shed some light on the depths of linguistic analysis. We quantify the frequency of these con-  ing set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.  structions in the English Wikipedia, then annotate 100 example sentences for each phenomenon with gold-standard dependencies reﬂecting core properties of the phenomena of interest. This gold standard is then used to estimate the recall of these dependencies by seven commonly used parsers, providing the  
Text simpliﬁcation aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simpliﬁes sentences using handcrafted rules aimed at splitting long sentences, or substitutes difﬁcult words using a predeﬁned dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simpliﬁcation from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simpliﬁcations that signiﬁcantly reduce the reading difﬁculty of the input, while maintaining grammaticality and preserving its meaning. 
Conversations provide rich opportunities for interactive, continuous learning. When something goes wrong, a system can ask for clariﬁcation, rewording, or otherwise redirect the interaction to achieve its goals. In this paper, we present an approach for using conversational interactions of this type to induce semantic parsers. We demonstrate learning without any explicit annotation of the meanings of user utterances. Instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation. This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system. Experiments on DARPA Communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations. 
We investigate an important and challenging problem in summary generation, i.e., Evolutionary Trans-Temporal Summarization (ETTS), which generates news timelines from massive data on the Internet. ETTS greatly facilitates fast news browsing and knowledge comprehension, and hence is a necessity. Given the collection of time-stamped web documents related to the evolving news, ETTS aims to return news evolution along the timeline, consisting of individual but correlated summaries on each date. Existing summarization algorithms fail to utilize trans-temporal characteristics among these component summaries. We propose to model trans-temporal correlations among component summaries for timelines, using inter-date and intra-date sentence dependencies, and present a novel combination. We develop experimental systems to compare 5 rival algorithms on 6 instinctively different datasets which amount to 10251 documents. Evaluation results in ROUGE metrics indicate the effectiveness of the proposed approach based on trans-temporal information. 
We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone. 
 proposed. Recent work suggests exploiting exter-  Automatically produced texts (e.g. translations or summaries) are usually evaluated with n-gram based measures such as BLEU or ROUGE, while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes. In this paper we ﬁrst present an in-  nal knowledge sources and/or deep linguistic annotation, and measure combination (see Section 2). However, original measures based on lexical matching, such as BLEU (Papineni et al., 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively. There are, in our opinion, two main reasons behind this fact.  depth analysis of the state of the art in order  First, the use of a common measure certainly allows  to clarify this issue. After this, we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human-produced references satisﬁes. These properties imply that corroborating system improvements with additional measures  researchers to carry out objective comparisons between their work and other published results. Second, the advantages of novel measures are not easy to demonstrate in terms of correlation with human judgements.  always increases the overall reliability of the evaluation process. In addition, the greater the heterogeneity of the measures (which is measurable) the higher their combined reliability. These results support the use of heterogeneous measures in order to consolidate text evaluation results.  Our goal is not to answer which is the most reliable metric or to propose yet another novel measure. Rather than this, we ﬁrst analyze in depth the state of the art, concluding that it is not easy to determine the reliability of a measure. In absence of a clear proof of the advantages of novel measures, sys-  tem developers naturally tend to prefer well-known  
The Text Analysis Conference (TAC) ranks summarization systems by their average score over a collection of document sets. We investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between human and machine evaluation systems.  
We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-ﬁne approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 
Preordering of source side sentences has proved to be useful in improving statistical machine translation. Most work has used a parser in the source language along with rules to map the source language word order into the target language word order. The requirement to have a source language parser is a major drawback, which we seek to overcome in this paper. Instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with highquality word alignments. Our model learns pairwise costs of a word immediately preceding another word. We use the Lin-Kernighan heuristic to ﬁnd the best source reordering efﬁciently during training and testing and show that it sufﬁces to provide good quality reordering. We show gains in translation performance based on our reordering model for translating from Hindi to English, Urdu to English (with a public dataset), and English to Hindi. For English to Hindi we show that our technique achieves better performance than a method that uses rules applied to the source side English parse. 
We propose an algorithm allowing to efﬁciently retrieve example treelets in a parsed tree database in order to allow on-the-ﬂy extraction of syntactic translation rules. We also propose improvements of this algorithm allowing several kinds of ﬂexible matchings. 
 descriptions written in a patient’s record encode im-  This paper presents a generative model for the automatic discovery of relations between entities in electronic medical records. The model discovers relation instances and their types by determining which context tokens express the relation. Additionally, the valid semantic classes for each type of relation are determined. We show that the model produces clusters of relation trigger words which better correspond with manually annotated relations than several existing clustering techniques. The discovered relations reveal some of the implicit semantic structure present in patient records.  portant information about the relationships between the problems a patients has, the treatments taken for the problems, and the tests which reveal and investigate the problems. The ability to accurately detect semantic relations in EMRs, such as Treatment-Administered-forProblem, can aid in querying medical records. After a preprocessing phase in which the relations are detected in all records they can be indexed and retrieved later as needed. A doctor could search for all the times that a certain treatment has been used on a particular problem, or determine all the treatments used for a speciﬁc problem. An additional application is the use of the relational information  
We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More speciﬁcally, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves signiﬁcantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks.  inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect knowledge. The KB we consider is a large triple store, which can be represented as a labeled, directed graph in which each entity a is a node, each binary relation R(a, b) is an edge labeled R between a and b, and unary concepts C(a) are represented as an edge labeled “isa” between the node for the entity a and a node for the concept C. We present a trainable inference method that learns to infer relations by combining the results of different random walks through this graph, and show that the method achieves good scaling properties and robust inference in a KB containing over 500,000 triples extracted from the web by the NELL system (Carlson et al., 2010). 1.1 The NELL Case Study  
This paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjective-noun phrases, such as e.g. hot in hot summer denoting the attribute TEMPERATURE, rather than TASTE. We formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space deﬁned over possible attributes. The vectors incorporate latent semantic information obtained from two variants of LDA topic models. Our LDA models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of LDA models. For the ﬁrst time, we extend the attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline.  data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ Se´aghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE, TASTE or EMOTIONALITY. These adjective meanings can be combined with nouns such as tea, soup or debate, which can be characterized in terms of attributes as well. The goal of the task is to determine the hidden attribute meaning predicated over the noun in a given adjective-noun phrase, as illustrated in (1).  
In this paper, we propose a novel topic model based on incorporating dictionary deﬁnitions. Traditional topic models treat words as surface strings without assuming predeﬁned knowledge about word meaning. They infer topics only by observing surface word co-occurrence. However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. Exploiting dictionary deﬁnitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense deﬁnitions. We show that explicitly modeling word deﬁnitions helps improve performance signiﬁcantly over the baseline for a text categorization task. 
We present an automatic method which leverages word lengthening to adapt a sentiment lexicon speciﬁcally for Twitter and similar social messaging networks. The contributions of the paper are as follows. First, we call attention to lengthening as a widespread phenomenon in microblogs and social messaging, and demonstrate the importance of handling it correctly. We then show that lengthening is strongly associated with subjectivity and sentiment. Finally, we present an automatic method which leverages this association to detect domain-speciﬁc sentiment- and emotionbearing words. We evaluate our method by comparison to human judgments, and analyze its strengths and weaknesses. Our results are of interest to anyone analyzing sentiment in microblogs and social networks, whether for research or commercial purposes. 
 comments on a popular online news site like Yahoo!  In recent years, the amount of user-generated opinionated texts (e.g., reviews, user comments) continues to grow at a rapid speed: featured news stories on a major event easily attract thousands of user comments on a popular  News. One question becomes immediate: how can we help people consume such gigantic amount of opinionated information? One possibility is to take the summarization route. Brieﬂy speaking (see Section 2 for a more detailed  online News service. How to consume subjective information of this volume becomes an interesting and important research question. In contrast to previous work on review analysis that tried to ﬁlter or summarize information for a generic average user, we explore a different direction of enabling personalized recommendation of such information. For each user, our task is to rank the comments associated with a given article according to personalized user preference (i.e., whether the  discussion), previous work has largely formulated review summarization as automatically or manually identify ratable aspects, and present overall sentiment polarity for each aspect (Hu and Liu, 2004; Popescu and Etzioni, 2005; Snyder and Barzilay, 2007; Titov and McDonald, 2008). A related line of research looked into predicting helpfulness of reviews in the hope of promoting those with better quality, where helpfulness is usually deﬁned as some function over the percentage of users who found the  user is likely to like or dislike the comment). To this end, we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way. Our full model signiﬁcantly outperforms strong baselines as well as related models that have been considered in previous work.  review to be helpful (Kim et al., 2006; Liu et al., 2007; Danescu-Niculescu-Mizil et al., 2009). In short, the focus of previous work has been on distilling subjective information for an average user. Whether opinion consumers are looking for quality information or just wondering what other people  think, each may have different purposes or prefer-  
 large corpus of status-response pairs found on Twit-  ter to create a system that responds to Twitter status  We present a data-driven approach to generat-  posts. Note that we make no mention of context, in-  ing responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We ﬁnd that mapping conversational stimuli onto responses is more difﬁcult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be  tent or dialogue state; our goal is to generate any response that ﬁts the provided stimulus; however, we do so without employing rules or templates, with the hope of creating a system that is both ﬂexible and extensible when operating in an open domain. Success in open domain response generation could be immediately useful to social media plat-  further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases. As far as we are aware, this is the ﬁrst work to investigate the use of phrase-based SMT to di-  forms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneﬁcial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or compan-  rectly translate a linguistic stimulus into an appropriate response.  ionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response  generation when used inside larger dialogue sys-  
 marization, we consider supervised models of the  We consider the problem of predicting measurable responses to scientiﬁc articles based primarily on their text content. Specifically, we consider papers in two ﬁelds (economics and computational linguistics) and make predictions about downloads and within-community citations. Our approach is based on generalized linear models, allowing interpretability; a novel extension that captures ﬁrst-order temporal effects is also presented. We demonstrate that text features signiﬁcantly improve accuracy of predictions over metadata features like authors, topical categories, and publication venues.  collective response of a scientiﬁc community to a published article. There are many measures of impact of a scientiﬁc paper; ours come from direct measurements of the number of downloads (from an established website where prominent economists post papers before formal publication) and citations (within a ﬁxed scientiﬁc community). We adopt a discriminative approach based on generalized linear models that can make use of any text or metadata features, and show that simple lexical features offer substantial power in modeling out-ofsample response and in forecasting response for future articles. Realistic forecasting evaluations require methodological care beyond the usual best  
A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efﬁciently corrects the initial segmentation given by a morphological analyzer. 
We present an inference algorithm that organizes observed words (tokens) into structured inﬂectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inﬂectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inﬂections, paradigms, and locally conditioned string edits. It assumes that inﬂected word tokens are generated from an inﬁnite mixture of inﬂectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted ﬁnitestate transducers with language-speciﬁc parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming. Given 50–100 seed paradigms, adding a 10million-word corpus reduces prediction error for morphological inﬂections by up to 10%. 
In this paper, we describe a novel approach to cascaded learning and inference on sequences. We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. In this model, inference on sequences is modeled as cascaded decision. However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. We also propose two types of efﬁcient dynamic programming which are required in the gradient-based optimization of an objective function. One of the dynamic programming algorithms resembles back propagation algorithm for multilayer feed-forward neural networks. The other is a generalized version of the forwardbackward algorithm. We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method. 
In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added beneﬁt, and our ﬁnal system outperforms the best published results on most of the 25 corpora tested. 
Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable ﬁne-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun ﬁxed yields both a higher number of semantically interpreted NCs and improved accuracy due to stronger semantic restrictions. 
In the last few years, the interest of the research community in micro-blogs and social media services, such as Twitter, is growing exponentially. Yet, so far not much attention has been paid on a key characteristic of microblogs: the high level of information redundancy. The aim of this paper is to systematically approach this problem by providing an operational deﬁnition of redundancy. We cast redundancy in the framework of Textual Entailment Recognition. We also provide quantitative evidence on the pervasiveness of redundancy in Twitter, and describe a dataset of redundancy-annotated tweets. Finally, we present a general purpose system for identifying redundant tweets. An extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical signiﬁcance. 
We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to deﬁne a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacriﬁcing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German. 
 a classical word sense disambiguation (WSD) task  Metaphor is ubiquitous in text, even in highly technical text. Correct inference about textual entailment requires computers to distinguish the literal and metaphorical senses of a word. Past work has treated this problem as a classical word sense disambiguation task. In this paper, we take a new approach, based on research in cognitive linguistics that views metaphor as a method for transferring knowledge from a familiar, well-understood, or concrete domain to an unfamiliar, less understood,  (Birke and Sarkar, 2006). Here, we take a different approach to the problem. Lakoff and Johnson (1980) argue that metaphor is a method for transferring knowledge from a concrete domain to an abstract domain. Therefore we hypothesize that the degree of abstractness in a word’s context is correlated with the likelihood that the word is used metaphorically. This hypothesis is the basis for our algorithm for distinguishing literal and metaphorical senses. Consider the following sentences:  or more abstract domain. This view leads to the hypothesis that metaphorical word usage is correlated with the degree of abstractness of the word’s context. We introduce an algorithm that uses this hypothesis to classify a word sense in a given context as either literal (denotative) or metaphorical (connotative). We evaluate this algorithm with a set of adjectivenoun phrases (e.g., in dark comedy, the adjec-  L: He shot down my plane. → C1: He ﬁred at my plane. A1: He refuted my plane. M : He shot down my argument. C2: He ﬁred at my argument. → A2: He refuted my argument. The literal sense of shot down in L invokes knowl-  tive dark is used metaphorically; in dark hair, it is used literally) and with the TroFi (Trope Finder) Example Base of literal and nonliteral usage for ﬁfty verbs. We achieve state-of-theart performance on both datasets.  edge from the domain of war. The metaphorical usage of shot down in M transfers knowledge from the concrete domain of war to the abstract domain of debate (Lakoff and Johnson, 1980). The entailments of L and M depend on the in-  
Decision trees have been applied to a variety of NLP tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. Moreover, forests (collections of decision trees) have been shown to substantially outperform individual decision trees. In this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. We show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8% relative reduction in Word Error Rate over an n-gram baseline. 
Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reﬂects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of ﬁctional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–11%. 
The last decade has seen many interesting applications of Question Answering (QA) technology. The Jeopardy! quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classiﬁcation of Jeopardy! deﬁnition questions. Our extensive empirical analysis shows that our classiﬁcation models largely improve on classiﬁers based on word-language models. Such classiﬁers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.  Jeopardy! is a popular quiz show in the US which has been on the air for 27 years. In each game, three contestants compete for the opportunity to answer 60 questions in 12 categories of 5 questions each. Jeopardy! questions cover an incredibly broad domain, from science, literature, history, to popular culture. We are drawn to Jeopardy! as a test bed for open-domain QA technology due to its broad domain, complex language, as well as the emphasis on accuracy, conﬁdence, and speed during game play. While the vast majority of Jeopardy! questions are factoid questions, we ﬁnd several other types of questions in the Jeopardy! data, which can beneﬁt from specialized processing in the QA system. The additional processing in these questions complements that of the factoid questions to achieve improved overall QA performance. Among the various types of questions handled by the system are deﬁnition questions shown in the examples below:  
 French  English  Multiword expressions (MWE), a known nuisance for both linguistics and NLP, blur the lines between syntax and semantics. Previous work on MWE identiﬁcation has relied primarily on surface statistics, which perform poorly for longer MWEs and cannot model discontinuous expressions. To address these problems, we show that even the simplest parsing models can eﬀectively identify MWEs of arbitrary length, and that Tree Substitution Grammars achieve the best results. Our experiments show a 36.4% F1 absolute improvement for French over an n-gram surface statistics baseline, currently the predominant method for MWE identiﬁcation. Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy. 
We present the ﬁrst algorithms to automatically identify explicit discourse connectives and the relations they signal for Arabic text. First we show that, for Arabic news, most adjacent sentences are connected via explicit connectives in contrast to English, making the treatment of explicit discourse connectives for Arabic highly important. We also show that explicit Arabic discourse connectives are far more ambiguous than English ones, making their treatment challenging. In the second part of the paper, we present supervised algorithms to address automatic discourse connective identiﬁcation and discourse relation recognition. Our connective identiﬁer based on gold standard syntactic features achieves almost human performance. In addition, an identiﬁer based solely on simple lexical and automatically derived morphological and POS features performs with high reliability, essential for languages that do not have high-quality parsers yet. Our algorithm for recognizing discourse relations performs signiﬁcantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation. 
This research studies the text genre of message board forums, which contain a mixture of expository sentences that present factual information and conversational sentences that include communicative acts between the writer and readers. Our goal is to create sentence classiﬁers that can identify whether a sentence contains a speech act, and can recognize sentences containing four different speech act classes: Commissives, Directives, Expressives, and Representatives. We conduct experiments using a wide variety of features, including lexical and syntactic features, speech act word lists from external resources, and domain-speciﬁc semantic class features. We evaluate our results on a collection of message board posts in the domain of veterinary medicine. 
 smoker, non-smoker – but at the end an applica-  Information-oriented document labeling is a special document multi-labeling task where the target labels refer to a speciﬁc information instead of the topic of the whole document. These kind of tasks are usually solved by looking up indicator phrases and analyzing their local context to ﬁlter false positive matches. Here, we introduce an approach for machine learning local content shifters which detects irrelevant local contexts using just the original document-level training labels. We handle content shifters in general, instead of learning a particular language phenomenon detector (e.g. negation or hedging) and form a single system for document labeling and content  tion has to assign labels to the documents(patients). Similarly, the soccer club names where a sportsman played for are document(sportman)-level labels in Wikipedia articles expressed by the Wikipedia categories. The target information in these tasks is usually just mentioned in the document and much of the document is irrelevant for this information request in contrast to standard document classiﬁcation tasks where the goal is to identify the topics of the whole document. On the other hand, they are not a standard information extraction task as the task is to assign class labels to documents, and the training dataset contains labels just at this level. These special tasks lie somewhere between  shift detection. Our empirical results achieved 24% error reduction – compared to supervised baseline methods – on three document labeling tasks.  information extraction and document classiﬁcation and require special approaches to solve them. We will call them Information-oriented document labeling throughout this paper. There are several appli-  
 is not a learning algorithm that can work best on  In this paper, we present a new ranking scheme, collaborative ranking (CR). In contrast to traditional non-collaborative ranking scheme which solely relies on the strengths of isolated queries and one stand-alone rank-  all types of data. In such a situation, it would be desirable to build a “collaborative” model by integrating multiple models. Such an idea forms the basis of ensemble methodology and it is wellknown that ensemble methods (e.g., bagging, boost-  ing algorithm, the new scheme integrates the  ing) can improve the performance of many prob-  strengths from multiple collaborators of a query and the strengths from multiple ranking algorithms. We elaborate three speciﬁc forms of collaborative ranking, namely, micro collaborative ranking (MiCR), macro collaborative ranking (MaCR) and micro-macro collaborative ranking (MiMaCR). Experiments on entity linking task show that our proposed  lems, in which classiﬁcation is the most intensively studied (Rokach, 2009). The other situation is related with isolated queries handled by learning algorithms. The single query may not be formulated with the best terms or the query itself may not contain comprehensive information required for a highperformance ranking algorithm. Therefore, tech-  scheme is indeed effective and promising.  niques of query expansion or query reformulation  can be introduced and previous research has shown  
 The correct recognition of MWUs is an important  Most NLP systems use tokenization as part of preprocessing. Generally, tokenizers are based on simple heuristics and do not recognize multi-word units (MWUs) like hot dog  building block of many NLP tasks. For example, in information retrieval (IR) the query hot dog should not retrieve documents that only contain the words hot and dog individually, outside of the phrase hot  or black hole unless a precompiled list of MWUs is available. In this paper, we propose a new cascaded model for detecting MWUs of arbitrary length for tokenization, focusing on noun phrases in the physics domain. We adopt a classiﬁcation approach because – unlike other work on MWUs – tokenization requires a completely automatic approach. We achieve an accuracy of 68% for recognizing  dog. In this study, we focus on noun phrases in the physics domain. For specialized domains such as physics, adaptable and reliable MWU recognition is of particular importance because comprehensive and up-to-date lists of MWUs are not available and would have to be created by hand. We chose noun phrases because domain-speciﬁc terminology  non-compositional MWUs and show that our MWU recognizer improves retrieval performance when used as part of an information retrieval system.  is commonly encoded in noun phrase MWUs; other types of phrases – e.g., verb constructions – rarely give rise to ﬁxed domain-speciﬁc multi-word sequences that should be treated as a unit.  
In this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion. We use both local contexts and global world knowledge to expand query language models. We place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query. Our experiments on the TAC-KBP 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance. Compared with the ofﬁcial results from KBP 2010 participants, our system shows competitive performance. 
We address the task of automatic discovery of information extraction template from a given text collection. Our approach clusters candidate slot ﬁllers to identify meaningful template slots. We propose a generative model that incorporates distributional prior knowledge to help distribute candidates in a document into appropriate slots. Empirical results suggest that the proposed prior can bring substantial improvements to our task as compared to a K-means baseline and a Gaussian mixture model baseline. Speciﬁcally, the proposed prior has shown to be effective when coupled with discriminative features of the candidates. 
This paper proposes a semi-supervised relation acquisition method that does not rely on extraction patterns (e.g. “X causes Y” for causal relations) but instead learns a combination of indirect evidence for the target relation — semantic word classes and partial patterns. This method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large Japanese Web corpus — in extreme cases, patterns that occur only once in the entire corpus. Such patterns are beyond the reach of current pattern based methods. We show that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance. 
 they would not undergo in isolation. Syntactically,  We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. The architecture combines various linguistically-motivated classiﬁcation features in a Bayesian Network.  some MWEs behave like words while other are phrases; some occur in one rigid pattern (and a ﬁxed order), while others permit various syntactic transformations. Semantically, the compositionality of MWEs is gradual, ranging from fully compositional to idiomatic (Bannard et al., 2003).  We introduce novel ways for computing many of these features, and manually deﬁne linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost entirely unsupervised and completely languageindependent; it relies on few language resources and is thus suitable for a large num-  Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing applications. Correct handling of MWEs has been proven beneﬁcial for various applications, including information retrieval, building ontologies, text alignment, and machine translation. We propose a novel architecture for identifying  ber of languages. Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic constructions. We demonstrate a signiﬁcant improvement in identiﬁcation accuracy, compared with less sophisticated baselines.  MWEs of various types and syntactic categories in monolingual corpora. Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of all types by focusing on the general idiosyncratic properties of MWEs rather than on speciﬁc properties of each sub-class  
An A-C bilingual dictionary can be inferred by merging A-B and B-C dictionaries using B as pivot. However, polysemous pivot words often produce wrong translation candidates. This paper analyzes two methods for pruning wrong candidates: one based on exploiting the structure of the source dictionaries, and the other based on distributional similarity computed from comparable corpora. As both methods depend exclusively on easily available resources, they are well suited to less resourced languages. We studied whether these two techniques complement each other given that they are based on different paradigms. We also researched combining them by looking for the best adequacy depending on various application scenarios. 
 terms of medium-range reordering, it does equally  poorly in long-distance reordering due to constraints  Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. Our approach signiﬁcantly improves Chinese–English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the  to guarantee efﬁciency. Syntax-based models that use phrase structure constituent labels as non-terminals in their transfer rules, exempliﬁed by that of Galley et al. (2004), produce smarter and syntactically motivated reordering. However, when working with off-the-shelf tools for parsing and alignment, this approach may impose harsh limits on rule extraction and requires  tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially ef-  serious efforts of optimization (Wang et al., 2010). An alternative approach is to augment the general hierarchical phrase-based model with soft syntactic constraints. Here, we derive three word-based, complementary constraints from the source dependency parsing, including:  fective for long-distance reodering.  • A dependency orientation feature, trained with  maximum entropy on the word-aligned par-  
 each doubling using general web data leads to im-  Part-of-speech language modeling is commonly used as a component in statistical machine translation systems, but there is mixed evidence that its usage leads to signiﬁcant improvements. We argue that its limited effectiveness is due to the lack of lexicalization. We introduce a new approach that builds a  provements of approximately 0.15 BLEU points. While large n-gram language models do lead to improved translation quality, they still lack any generalization beyond the surface forms (Schwenk, 2007). Consider example (1), which is a short sentence fragment from the MT09 Arabic-English test set, with the corresponding machine translation out-  separate local language model for each word and part-of-speech pair. The resulting models lead to more context-sensitive probability distributions and we also exploit the fact  put (1.b), from a phrase-based statistical machine translation system, and reference translation (1.c).  (1) a.  ...  that different local models are used to estimate the language model probability of each word during decoding. Our approach is evaluated for Arabic- and Chinese-to-English translation. We show that it leads to statistically  ... b. ... the background of press statements of controversial and accused him ... c. ... the background of controversial press  signiﬁcant improvements for multiple test sets and also across different genres, when compared against a competitive baseline and a system using a part-of-speech model.  statements and accused him ... Clearly, the adjective “controversial” should precede the nouns “press statement”, but since the AFP  and Xinhua portions of the Gigaword corpus, used  
Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves signiﬁcant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets. 
Models of word alignment built as sequences of links have limited expressive power, but are easy to decode. Word aligners that model the alignment matrix can express arbitrary alignments, but are difﬁcult to decode. We propose an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian. 
State of the art Tree Structures Prediction techniques rely on bottom-up decoding. These approaches allow the use of context-free features and bottom-up features. We discuss the limitations of mainstream techniques in solving common Natural Language Processing tasks. Then we devise a new framework that goes beyond Bottom-up Decoding, and that allows a better integration of contextual features. Furthermore we design a system that addresses these issues and we test it on Hierarchical Machine Translation, a well known tree structure prediction problem. The structure of the proposed system allows the incorporation of non-bottom-up features and relies on a more sophisticated decoding approach. We show that the proposed approach can ﬁnd better translations using a smaller portion of the search space.  input sequence is often a sentence for NLP applications. Tree structures generating the input sequence can be composed using rules, r, from the weighted grammar, G. TSP techniques return as output a tree structure or a set of trees (forest) that generate the input string or lattice. The output forest can be represented compactly as a weighted hypergraph (Klein and Manning, 2001). TSP tasks require ﬁnding the tree, t, with the highest score, or the best-k such trees. Mainstream TSP relies on Bottom-up Decoding (BD) techniques. With this paper we propose a new framework as a generalization of the CKY-like Bottom-up approach. We also design and test an instantiation of this framework, empirically showing that wider contextual information leads to higher accuracy for TSP tasks that rely on non-local features, like HMT. 2 Beyond Bottom-up Decoding  
Discriminative training for machine translation has been well studied in the recent past. A limitation of the work to date is that it relies on the availability of high-quality in-domain bilingual text for supervised training. We present an unsupervised discriminative training framework to incorporate the usually plentiful target-language monolingual data by using a rough “reverse” translation system. Intuitively, our method strives to ensure that probabilistic “round-trip” translation from a targetlanguage sentence to the source-language and back will have low expected loss. Theoretically, this may be justiﬁed as (discriminatively) minimizing an imputed empirical risk. Empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks. 
Mapping documents into an interlingual representation can help bridge the language barrier of cross-lingual corpora. Many existing approaches are based on word co-occurrences extracted from aligned training data, represented as a covariance matrix. In theory, such a covariance matrix should represent semantic equivalence, and should be highly sparse. Unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. In this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways. First, we explore word association measures and bilingual dictionaries to weigh the word pairs. Later, we explore different selection strategies to remove the noisy pairs based on the association scores. Our experimental results on the task of aligning comparable documents shows the efﬁcacy of sparse covariance matrices on two data sets from two different language pairs. 
 generalizations of the maximum entropy principle  We discuss and analyze the problem of ﬁnding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. We then use the reformulated problem to describe a space and time efﬁcient algorithm for tracking the entire relaxation path. Our derivations are based on a compact geometric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters. We demonstrate the usability of our approach by applying the problem to Zipﬁan distributions over a large alphabet.  to language processing is the work of Berger, Della Pietra×2, and Lafferty (Berger et al., 1996, Della Pietra et al., 1997). The original formulation of max-ent cast the problem as the task of ﬁnding the distribution attaining the highest entropy subject to equality constraints. While this formalism is aesthetic and paves the way to a simple dual in the form of a unique Gibbs distribution (Della Pietra et al., 1997), it does not provide sufﬁcient tools to deal with input noise and sparse representation of the target Gibbs distribution. To mitigate these issues, numerous relaxation schemes of the equality constraints have been proposed. A notable recent work by Dudik, Phillips, and Schapire (2007) provided a general constraint-relaxation framework. See also the references therein for an in depth overview of other approaches and generalizations of max-ent. The constraint relaxation surfaces a natural parameter, namely, a relaxation value. The dual form of this free parameter is the regularization value of penalized logistic regression problems. Typically this parameter is set by experimentation using cross validation technique. The relaxed maximum-entropy  
 also label the words that elicit positive sentiment  (such as “sensational” and “electrifying”) as posi-  In active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classiﬁer with low cost. However, how to measure the informativeness for both examples and feature on the same scale has not been well solved. In this paper, we propose a non-negative matrix factorization based approach to address this issue. We ﬁrst extend the matrix factorization  tive and words that evoke negative sentiment (such as “depressed” and “unfulﬁlling”) as negative. Recent work has demonstrated that labeled words (or feature supervision) can greatly reduce the number of labeled samples for building high-quality classiﬁers (Druck et al., 2008; Zaidan and Eisner, 2008). In fact, different kinds of supervision generally have different acquisition costs, different degrees of util-  framework to explicitly model the corresponding relationships between feature classes and examples classes. Then by making use of the reconstruction error, we propose a uniﬁed scheme to determine which feature or exam-  ity and are not mutually redundant (Sindhwani et al., 2009). Ideally, effective active learning schemes should be able to utilize different forms of supervision.  ple a classiﬁer is most likely to beneﬁt from having labeled. Empirical results demonstrate the effectiveness of our proposed methods.  To incorporate the supervision on words and documents at same time into the active learning scheme, recently an active dual supervision (or dual active  learning) has been proposed (Melville and Sind-  
Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneﬁcial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difﬁcult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difﬁculty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models. 
This paper investigates improving supervised word segmentation accuracy with unlabeled data. Both large-scale in-domain data and small-scale document text are considered. We present a uniﬁed solution to include features derived from unlabeled data to a discriminative learning model. For the large-scale data, we derive string statistics from Gigaword to assist a character-based segmenter. In addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-ofvocabulary (OOV) words which appear more than once inside a document. Novel features1 result in relative error reductions of 13.8% and 15.4% in terms of F-score and the recall of OOV words respectively. 
Metonymic language is a pervasive phenomenon. Metonymic type shifting, or argument type coercion, results in a selectional restriction violation where the argument’s semantic class differs from the class the predicate expects. In this paper we present an unsupervised method that learns the selectional restriction of arguments and enables the detection of argument coercion. This method also generates an enhanced probabilistic resolution of logical metonymies. The experimental results indicate substantial improvements the detection of coercions and the ranking of metonymic interpretations. 
Measuring semantic relatedness between words or concepts is a crucial process to many Natural Language Processing tasks. Exiting methods exploit semantic evidence from a single knowledge source, and are predominantly evaluated only in the general domain. This paper introduces a method of harnessing different knowledge sources under a uniform model for measuring semantic relatedness between words or concepts. Using Wikipedia and WordNet as examples, and evaluated in both the general and biomedical domains, it successfully combines strengths from both knowledge sources and outperforms stateof-the-art on many datasets. 
 Another important resource in the latter stream is  We re-investigate the rationale for and the effectiveness of adopting the notions of depth and density in WordNet-based semantic similarity measures. We show that the intuition for including these notions in WordNet-based similarity measures does not always stand up to empirical examination. In particular, the  semantic taxonomies such as WordNet (Fellbaum, 1998). Despite their high cost of compilation and limited availability across languages, semantic taxonomies have been widely used in similarity measures, and one of the main reasons behind this is that the often complex notion of lexical semantic similarity can be approximated with ease by the distance  traditional deﬁnitions of depth and density as ordinal integer values in the hierarchical structure of WordNet does not always correlate with human judgment of lexical semantic similarity, which imposes strong limitations on their contribution to an accurate similarity measure. We thus propose several novel deﬁnitions of depth and density, which yield sig-  between words (represented as nodes) in their hierarchical structures, and this approximation appeals much to our intuition. Even methods as simple as “hop counts” between nodes (e.g., that of Rada et al. 1989 on the English WordNet) can take us a long way. Meanwhile, taxonomy-based methods have been constantly reﬁned by incorporating various  niﬁcant improvement in degree of correlation with similarity. When used in WordNet-based semantic similarity measures, the new deﬁnitions consistently improve performance on a task of correlating with human judgment.  structural features such as depth (Sussna, 1993; Wu and Palmer, 1994), density (Sussna, 1993), type of connection (Hirst and St-Onge, 1998; Sussna, 1993), word class (sense) frequency estimates (Resnik, 1999), or a combination these features (Jiang and  
This paper presents a novel method for the computation of word meaning in context. We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. The evaluation on a lexical substitution task – carried out for both English and French – indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations. 
 and semantic properties of verbs. Levin’s taxon-  omy or its extended version in VerbNet (Kipper,  Most previous research on verb clustering has focussed on acquiring ﬂat classiﬁcations from corpus data, although many manually built classiﬁcations are taxonomic in nature. Also Natural Language Processing (NLP) applications beneﬁt from taxonomic classiﬁcations because they vary in terms of the granularity they require from a classiﬁcation. We introduce a new clustering method called Hierarchical Graph Factorization Clustering (HGFC) and extend it so that it is optimal for the task. Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a ﬂat test set. We demonstrate how the method can be used to acquire novel classiﬁcations as well as to extend existing ones on the basis of some prior knowledge about the classiﬁcation.  2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classiﬁcations as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ Se´aghdha and Copestake, 2008; Vlachos et al., 2009). The best of such approaches have yielded promising results. However, they have  mostly focussed on acquiring and evaluating ﬂat  
 over semantic networks, e.g. (Cowie et al., 1992; Wu  and Palmer, 1994; Resnik, 1995; Jiang and Conrath,  A central topic in natural language processing is the design of lexical and syntactic features suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We deﬁne efﬁcient and powerful kernels for measuring the similarity between de-  1997; Schtze, 1998; Leacock and Chodorow, 1998; Pedersen et al., 2004a; Budanitsky and Hirst, 2006). More recent research also focuses on mechanisms to deﬁne if two structures, e.g. graphs, are enough similar, as explored in (Mihalcea, 2005; Zhao et al., 2009; Fu¨rstenau and Lapata, 2009; Navigli and Lapata, 2010).  pendency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classiﬁcation show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classiﬁcation conﬁrms the beneﬁt of semantic smoothing for dependency kernels.  On one hand, previous work shows that there is a substantial lack of automatic methods for engineering lexical/syntactic features (or more in general syntactic/semantic similarity). On the other hand, automatic feature engineering of syntactic or shallow semantic structures has been carried out by means of structural kernels, e.g. (Collins and  Duffy, 2002; Kudo and Matsumoto, 2003; Cumby  
 phrase the body/corpse deliberated the motion. . . ?”  This paper investigates novel methods for incorporating syntactic information in probabilistic latent variable models of lexical choice and contextual similarity. The resulting models capture the effects of context on the interpretation of a word and in particular its effect  and “how similar are the phrases the body deliberated the motion and the corpse rotted?”. In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word. The work described in this paper uses probabilis-  on the appropriateness of replacing that word  tic latent variable models to describe patterns of syn-  with a potentially related one. Evaluating our techniques on two datasets, we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes.  tactic interaction, building on the selectional preference models of O´ Se´aghdha (2010) and Ritter et al. (2010) and the lexical substitution models of Dinu and Lapata (2010). We propose novel methods for incorporating information about syntactic context in  
Lexical co-occurrence is an important cue for detecting word associations. We propose a new measure of word association based on a new notion of statistical signiﬁcance for lexical co-occurrences. Existing measures typically rely on global unigram frequencies to determine expected co-occurrence counts. Instead, we focus only on documents that contain both terms (of a candidate word-pair) and ask if the distribution of the observed spans of the word-pair resembles that under a random null model. This would imply that the words in the pair are not related strongly enough for one word to inﬂuence placement of the other. However, if the words are found to occur closer together than explainable by the null model, then we hypothesize a more direct association between the words. Through extensive empirical evaluation on most of the publicly available benchmark data sets, we show the advantages of our measure over existing co-occurrence measures. 
 a previously-mentioned entity. Information status  An entity in a dialogue may be old, new, or mediated/inferrable with respect to the hearer’s beliefs. Knowing the information status of the entities participating in a dialogue can therefore facilitate its interpretation. We address the under-investigated problem of automatically determining the information status of discourse entities. Speciﬁcally, we extend Nissim’s (2006) machine learning approach to information-status determination with lexical and structured features, and exploit learned knowledge of the information status of each discourse entity for coreference resolution. Experimental results on a set of Switchboard dialogues reveal that (1) incorporating our proposed features into Nissim’s feature set enables our system to achieve stateof-the-art performance on information-status classiﬁcation, and (2) the resulting information can be used to improve the performance of learning-based coreference resolvers.  is a subject that has received a lot of attention in theoretical linguistics (Halliday, 1976; Prince, 1981; Hajicˇova´, 1984; Vallduv´ı, 1992; Steedman, 2000). Knowing the information status of discourse entities can potentially beneﬁt many NLP applications. One such task is anaphora resolution. While there is general belief that deﬁnite descriptions are mostly anaphoric, Vieira and Poesio (2000) empirically show that only 30% of these NPs are anaphoric. Without being able to determine whether an NP is anaphoric, an anaphora resolver will attempt to resolve every NP, potentially damaging its precision. Since new entities are by deﬁnition new to the hearer and therefore cannot refer to a previously-introduced NP, knowledge of information status could be used to improve anaphora resolution. Despite the potential usefulness of information status in NLP tasks, there has been little work on learning the information status of discourse entities. To investigate the plausibility of learning informa-  
Traditional approaches to sentiment classiﬁcation rely on lexical features, syntax-based features or a combination of the two. We propose semantic features using word senses for a supervised document-level sentiment classiﬁer. To highlight the beneﬁt of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features. In addition, we highlight the beneﬁt of senses by presenting a part-ofspeech-wise effect on sentiment classiﬁcation. Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classiﬁer still performs better than what it does in absence of sense annotation. Since word senses used as features show promise, we also examine the possibility of using similarity metrics deﬁned on WordNet to address the problem of not ﬁnding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline.  classiﬁcation task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semantics in a supervised sentiment classiﬁer. We use the synsets in Wordnet as the feature space to represent word senses. Thus, a document consisting of words gets mapped to a document consisting of corresponding word senses. Harnessing WordNet senses as features helps us address two issues: 1. Impact of WordNet sense-based features on the performance of supervised SA 2. Use of WordNet similarity metrics to solve the problem of features unseen in the training corpus  
 tiani (2006), Wilson et al. (2005a)): the latter con-  cerns words that express sentiment either explicitly  In this paper, we introduce a connotation lexicon, a new type of lexicon that lists words with connotative polarity, i.e., words with positive connotation (e.g., award, promotion) and words with negative connotation (e.g., cancer, war). Connotation lexicons differ from much studied sentiment lexicons: the latter concerns words that express sentiment, while the former concerns words that evoke or associate with a speciﬁc polarity of sentiment. Understanding the connotation of words would seem to require common sense and world knowledge. However, we demonstrate that much of the connotative polarity of words can be inferred from natural language text in a nearly unsu-  or implicitly, while the former concerns words that evoke or even simply associate with a speciﬁc polarity of sentiment. To our knowledge, there has been no previous research that investigates polarized connotation lexicons. Understanding the connotation of words would seem to require common sense and world knowledge at ﬁrst glance, which in turn might seem to require human encoding of knowledge base. However, we demonstrate that much of the connotative polarity of words can be inferred from natural language text in a nearly unsupervised manner. The key linguistic insight behind our approach is  pervised manner. The key linguistic insight behind our approach is selectional preference of connotative predicates. We present graphbased algorithms using PageRank and HITS that collectively learn connotation lexicon together with connotative predicates. Our empirical study demonstrates that the resulting connotation lexicon is of great value for sen-  selectional preference of connotative predicates. We deﬁne a connotative predicate as a predicate that has selectional preference on the connotative polarity of some of its semantic arguments. For instance, in the case of the connotative predicate “prevent”, there is strong selectional preference on negative connotation with respect to the thematic role (se-  timent analysis complementing existing sentiment lexicons.  mantic role) “THEME”. That is, statistically speaking, people tend to associate negative connotation  with the THEME of “prevent”, e.g., “prevent can-  
 re-ranking models.  Reranking models have been successfully applied to many tasks of Natural Language Processing. However, there are two aspects of this approach that need a deeper investigation: (i) Assessment of hypotheses generated for reranking at classiﬁcation phase: baseline models generate a list of hypotheses and these are used for reranking without any assessment; (ii) Detection of cases where reranking models provide a worst result: the best hypothesis provided by the reranking model is assumed to be always the best result. In some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. In this paper we propose solutions for these two aspects: (i) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an SLU basiline model. The selected hypotheses are reranked applying a state-of-the-art model based on Partial Tree Kernels, which encode SLU hypotheses in Support Vector Machines with complex structured features; (ii) ﬁnally, we apply a decision strategy, based on conﬁdence values, to select the ﬁnal hypothesis between the ﬁrst ranked hypothesis provided by the baseline SLU model and the ﬁrst ranked hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show signiﬁcant improvements with respect to current state-of-the-art and previous  
A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The ﬁrst pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists. The stronger language model is intended to capture more longdistance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to rescore word lattices in the second pass. A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice. An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup. 
 models have also been proposed–MELM (Rosen-  We propose an efﬁcient way to train maximum entropy language models (MELM) and neural network language models (NNLM). The advantage of the proposed method comes from a more robust and efﬁcient subsampling tech-  feld, 1996) and NNLM (Bengio et al., 2003) are two examples. Modeling P (w|h) can be seen as a multi-class classiﬁcation problem. Given the history, we have to choose a word in the vocabulary, which can eas-  nique. The original multi-class language mod-  ily be a few hundred thousand words in size. For  eling problem is transformed into a set of binary problems where each binary classiﬁer predicts whether or not a particular word will occur. We show that the binarized model is as powerful as the standard model and allows us to aggressively subsample negative training examples without sacriﬁcing predictive performance. Empirical results show that we can  complex models such as MELM and NNLM, this poses a computational challenge for learning, because the resulting objective functions are expensive to normalize. In contrast, n-gram LMs do not suffer from this computational challenge. In the web era, language modelers have access to virtually unlimited amounts of data, while the computing power  train MELM and NNLM at 1% ∼ 5% of the standard complexity with no loss in performance.  available to process this data is limited. Therefore, despite the demonstrated effectiveness of complex LMs, the n-gram is still the predominant approach  for most real world applications.  
In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. We ﬁrst develop an event-aspect LDA model to cluster sentences into aspects. We then use extended LexRank algorithm to rank the sentences in each cluster. We use Integer Linear Programming for sentence selection. Key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model. Also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. We compare our method with four baseline methods. Quantitative evaluation based on Rouge metric demonstrates the effectiveness and advantages of our method. 
 order of certain words can be fed to the grammati-  Machine-produced text often lacks grammaticality and ﬂuency. This paper studies grammaticality improvement using a syntax-based algorithm based on CCG. The goal of the search problem is to ﬁnd an optimal parse tree among all that can be constructed through selection and ordering of the input words. The search problem, which is signiﬁcantly harder than parsing, is solved by guided learning for best-ﬁrst search. In a standard word ordering task, our system gives a BLEU score of 40.1, higher than the previous result of 33.7 achieved by a dependency-based system.  cality improvement system. The input may also include words beyond the output of the base system, e.g. extra words from the SMT lattice, so that content word insertion and deletion can be performed implicity via word selection. We study the above task using CCG (Steedman, 2000). The main challenge is the search problem, which is to ﬁnd an optimal parse tree among all that can be constructed with any word choice and order from the set of input words. We use an approximate best-ﬁrst algorithm, guided by learning, to tackle the more-than-factorial complexity. Beam-search is used to control the volume of accepted hypotheses,  
Traditional computational approaches to referring expression generation operate in a deliberate manner, choosing the attributes to be included on the basis of their ability to distinguish the intended referent from its distractors. However, work in psycholinguistics suggests that speakers align their referring expressions with those used previously in the discourse, implying less deliberate choice and more subconscious reuse. This raises the question as to which is a more accurate characterisation of what people do. Using a corpus of dialogues containing 16,358 referring expressions, we explore this question via the generation of subsequent references in shared visual scenes. We use a machine learning approach to referring expression generation and demonstrate that incorporating features that correspond to the computational tradition does not match human referring behaviour as well as using features corresponding to the process of alignment. The results support the view that the traditional model of referring expression generation that is widely assumed in work on natural language generation may not in fact be correct; our analysis may also help explain the oft-observed redundancy found in humanproduced referring expressions. 
 Broadly, we can distinguish two forms of para-  Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not  phrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal  clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to  the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints:  syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text gener-  the NP 1’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting  ation tasks by augmenting its feature set, de-  paraphrastic transformations.  velopment data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.  A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual  
Part-of-speech (POS) is an indispensable feature in dependency parsing. Current research usually models POS tagging and dependency parsing independently. This may suffer from error propagation problem. Our experiments show that parsing accuracy drops by about 6% when using automatic POS tags instead of gold ones. To solve this issue, this paper proposes a solution by jointly optimizing POS tagging and dependency parsing in a unique model. We design several joint models and their corresponding decoding algorithms to incorporate different feature sets. We further present an effective pruning strategy to reduce the search space of candidate POS tags, leading to signiﬁcant improvement of parsing speed. Experimental results on Chinese Penn Treebank 5 show that our joint models signiﬁcantly improve the state-of-the-art parsing accuracy by about 1.5%. Detailed analysis shows that the joint method is able to choose such POS tags that are more helpful and discriminative from parsing viewpoint. This is the fundamental reason of parsing accuracy improvement. 
We propose a relaxed correspondence assumption for cross-lingual projection of constituent syntax, which allows a supposed constituent of the target sentence to correspond to an unrestricted treelet in the source parse. Such a relaxed assumption fundamentally tolerates the syntactic non-isomorphism between languages, and enables us to learn the target-language-speciﬁc syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 
 on predicting the preferred scopal ordering of sen-  tences with two quantifying determiners, for exam-  The computation of logical form has been proposed as an intermediate step in the translation of sentences to logic. Logical form encodes the resolution of scope ambiguities. In this paper, we describe experiments on a modestsized corpus of regulation annotated with a novel variant of logical form, called abstract syntax trees (ASTs). The main step in com-  ple, in the sentence “every kid climbed a tree”. In the related problem of translating database queries to logic, Zettlemoyer and Collins (2009) and Wong and Mooney (2007) consider the scope of adjectives in addition to determiners, for example the scope of “cheapest” in the noun phrase “the cheapest ﬂights from Boston to New York”. To our knowledge, em-  puting ASTs is to order scope-taking operators. A learning model for ranking is adapted for this ordering. We design features by studying the problem of comparing the scope of one operator to another. The scope comparisons are used to compute ASTs, with an F-score of 90.6% on the set of ordering decisons.  pirical studies of scope have been restricted to phenomenon between and within noun phrases. In this paper, we describe experiments on a novel annotation of scope phenomenon in regulatory texts – Section 610 of the Food and Drug Administration’s Code of Federal Regulations1 (FDA CFR). Determiners, modals, negation, and verb phrase  
 or part-of-speech, when a preﬁx of the input has al-  The notion of inﬁx probability has been introduced in the literature as a generalization of the notion of preﬁx (or initial substring) probability, motivated by applications in speech recognition and word error correction. For the case where a probabilistic context-free grammar is used as language model, methods for  ready been processed, as discussed by Jelinek and Lafferty (1991). Such distributions are useful for speech recognition, where the result of the acoustic processor is represented as a lattice, and local choices must be made for a next transition. In addition, distributions for the next word are also useful for applications of word error correction, when one  the computation of inﬁx probabilities have been presented in the literature, based on various simplifying assumptions. Here we present a solution that applies to the problem in its full generality.  is processing ‘noisy’ text and the parser recognizes an error that must be recovered by operations of insertion, replacement or deletion. Motivated by the above applications, the problem of the computation of inﬁx probabilities for PCFGs  
This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difﬁcult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difﬁcult attachment types, can successfully improve the quality of predicted parse trees output by several representative state-of-the-art dependency parsers for French.  into account sufﬁcient context when making attachment decisions, due to computational complexity. Assuming nonetheless that a predicted parse tree is mostly accurate, parse correction can revise difﬁcult attachments by using the predicted tree’s syntactic structure to restrict the set of candidate governors and extract a rich set of features to help select among them. Parse correction is also appealing because it is parser-agnostic: it can be trained to correct the output of any dependency parser. In Section 2 we discuss work related to parse correction, pp-attachment and coordination resolution. In Section 3 we discuss dependency structure and various statistical dependency parsing approaches. In Section 4 we introduce the parse correction framework, and Section 5 describes the features and learning model used in our implementation. In Section 6 we present experiments in which parse correction revises the predicted parse trees of four state-of-the-art dependency parsers for French. We provide concluding remarks in Section 7.  
We describe a generative model for nonprojective dependency parsing based on a simpliﬁed version of a transition system that has recently appeared in the literature. We then develop a dynamic programming parsing algorithm for our model, and derive an insideoutside algorithm that can be used for unsupervised learning of non-projective dependency trees.  programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for ma-  chine learning, such as best parses and feature ex-  
This paper introduces Chart Inference (CI), an algorithm for deriving a CCG category for an unknown word from a partial parse chart. It is shown to be faster and more precise than a baseline brute-force method, and to achieve wider coverage than a rule-based system. In addition, we show the application of CI to a domain adaptation task for question words, which are largely missing in the Penn Treebank. When used in combination with self-training, CI increases the precision of the baseline StatCCG parser over subjectextraction questions by 50%. An error analysis shows that CI contributes to the increase by expanding the number of category types available to the parser, while self-training adjusts the counts. 
 additional semantic interpretation, such as interpre-  Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. Furthermore, no com-  tations for prepositions, possessives, or noun compounds. In this paper, we describe 1) a new dependency conversion (Section 3) of the Penn Treebank (Marcus, et al., 1993) along with the associated dependency label scheme, which is based upon the Stanford parser’s popular scheme (de Marneffe and  monly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated ﬁne-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-ﬁrst parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the ﬁrst dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.  Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-ﬁrst framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, MALTPARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; Mc-  Donald and Pereira, 2006), the Charniak (2000)  
 Another attractive feature that helped make EM  We present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives. In its simplest form, lateen EM alternates between the two objectives of ordinary “soft” and “hard” expectation maximization (EM) algorithms. Switching objectives when stuck can help escape local optima. We ﬁnd that applying a single such alternation already yields state-of-the-art results for English dependency grammar induction. More elaborate lateen strategies track both objectives, with each validating the moves proposed by the other. Disagreements can signal earlier opportunities to switch or terminate, saving iterations. De-emphasizing ﬁxed points in these ways eliminates some guesswork from tuning EM. An evaluation against a suite of unsupervised dependency parsing tasks, for a variety of languages, showed that lateen strategies signiﬁcantly speed up training of both EM algorithms, and improve accuracy for hard EM.  instrumental (Meng, 2007) is its initial efﬁciency: Training tends to begin with large steps in a parameter space, sometimes bypassing many local optima at once. After a modest number of such iterations, however, EM lands close to an attractor. Next, its convergence rate necessarily suffers: Disproportionately many (and ever-smaller) steps are needed to ﬁnally approach this ﬁxed point, which is almost invariably a local optimum. Deciding when to terminate EM often involves guesswork; and ﬁnding ways out of local optima requires trial and error. We propose several strategies that address both limitations. Unsupervised objectives are, at best, loosely correlated with extrinsic performance (Pereira and Schabes, 1992; Merialdo, 1994; Liang and Klein, 2008, inter alia). This fact justiﬁes (occasionally) deviating from a prescribed training course. For example, since multiple equi-plausible objectives are usually available, a learner could cycle through them, optimizing alternatives when the primary objective function gets stuck; or, instead of trying to escape, it  
 lems of syntax discovery. Our methods are applica-  ble to vast quantities of unlabeled monolingual text.  We show that categories induced by unsuper-  Not all research on these problems has been fully  vised word clustering can surpass the performance of gold part-of-speech tags in dependency grammar induction. Unlike classic clustering algorithms, our method allows a word to have different tags in different contexts. In an ablative analysis, we ﬁrst demonstrate that this context-dependence is crucial to the superior performance of gold tags — requiring a word to always have the same part-ofspeech signiﬁcantly degrades the performance of manual tags in grammar induction, eliminating the advantage that human annotation has over unsupervised tags. We then introduce a sequence modeling technique that combines the output of a word clustering algorithm with context-colored noise, to allow words to be tagged differently in different contexts. With  unsupervised. For example, to the best of our knowledge, every new state-of-the-art dependency grammar inducer since Klein and Manning (2004) relied on gold part-of-speech tags. For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that “good enough” partsof-speech induction methods exist, justifying the focus on grammar induction with supervised part-ofspeech tags (Bod, 2006), pace (Cramer, 2007). One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al., 2011). In turn, not all unsupervised taggers actually in-  these new induced tags as input, our state-ofthe-art dependency grammar inducer achieves 59.1% directed accuracy on Section 23 (all sentences) of the Wall Street Journal (WSJ) corpus — 0.7% higher than using gold tags.  duce word categories: Many systems — known as part-of-speech disambiguators (Merialdo, 1994) — rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers — word clustering algorithms of Clark (2000) and  Brown et al. (1992) — that were recently shown to  
We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale ngram model for spelling correction. The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models signiﬁcantly reduces errors by up to 12.4% over the current state-of-the-art. The word co-occurrence information shows potential but only improves overall accuracy slightly. 
Accurate prediction of demographic attributes from social media and other informal online content is valuable for marketing, personalization, and legal investigation. This paper describes the construction of a large, multilingual dataset labeled with gender, and investigates statistical models for determining the gender of uncharacterized Twitter users. We explore several different classiﬁer types on this dataset. We show the degree to which classiﬁer accuracy varies based on tweet volumes as well as when various kinds of proﬁle metadata are included in the models. We also perform a large-scale human assessment using Amazon Mechanical Turk. Our methods signiﬁcantly out-perform both baseline models and almost all humans on the same task. 
Information published in online stock investment message boards, and more recently in stock microblogs, is considered highly valuable by many investors. Previous work focused on aggregation of sentiment from all users. However, in this work we show that it is beneﬁcial to distinguish expert users from non-experts. We propose a general framework for identifying expert investors, and use it as a basis for several models that predict stock rise from stock microblogging messages (stock tweets). In particular, we present two methods that combine expert identiﬁcation and per-user unsupervised learning. These methods were shown to achieve relatively high precision in predicting stock rise, and signiﬁcantly outperform our baseline. In addition, our work provides an in-depth analysis of the content and potential usefulness of stock tweets. 
 ical object affected by the breaking event. Analo-  gously, ball is the instrument of break both when  In this paper we present a method for unsuper-  realized as a prepositional phrase in (1a) and as a  vised semantic role induction which we formalize as a graph partitioning problem. Ar-  subject in (1b).  gument instances of a verb are represented as vertices in a graph whose edge weights quantify their role-semantic similarity. Graph partitioning is realized with an algorithm that iteratively assigns vertices to clusters based on  (1) a. [Jim]A0 broke the [window]A1 with a [ball]A2. b. The [ball]A2 broke the [window]A1. c. The [window]A1 broke [last night]TMP.  the cluster assignments of neighboring vertices. Our method is algorithmically and conceptually simple, especially with respect to how problem-speciﬁc knowledge is incorporated into the model. Experimental results on the CoNLL 2008 benchmark dataset demon-  The semantic roles in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broadcoverage human-annotated corpus of semantic roles and their syntactic realizations. Under the PropBank annotation framework (which we will assume  strate that our model is competitive with other  throughout this paper) each predicate is associated  unsupervised approaches in terms of F1 whilst attaining signiﬁcantly higher cluster purity.  with a set of core roles (named A0, A1, A2, and so on) whose interpretations are speciﬁc to that predi-  cate1 and a set of adjunct roles such as location or  
Based on analysis of on-line review corpus we observe that most sentences have complicated opinion structures and they cannot be well represented by existing methods, such as frame-based and feature-based ones. In this work, we propose a novel graph-based representation for sentence level sentiment. An integer linear programming-based structural learning method is then introduced to produce the graph representations of input sentences. Experimental evaluations on a manually labeled Chinese corpus demonstrate the effectiveness of the proposed approach. 
 livering the majority of information content from a  Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users  document set, and hence is a necessity. Traditional summarization methods play an important role with the exponential document growth on the Web. However, for the readers, the impact of human interests has seldom been considered. Tra-  have individual preferences on a particular  ditional summarization utilizes the same methodol-  source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates sum-  ogy to generate the same summary no matter who is reading. However, users may have bias on what they prefer to read due to their potential interests: they need personalization. Therefore, traditional summarization methods are to some extent insufﬁcient.  maries in an interactive and personalized man-  Topic biased summarization tries for personaliza-  ner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the  tion by pre-deﬁning human interests as several general categories, such as health or science. Readers are required to select their possible interests before summary generation so that the chosen topic has priority during summarization. Unfortunately, such topic biased summarization is not sufﬁcient for two reasons: (1) interests cannot usually be accurately  comparable performance between IPS and the  pre-deﬁned by ambiguous topic categories and (2)  best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.  user interests cannot always be foreknown. Often users do not really know what general ideas or detail information they are interested in until they read the summaries. Therefore, more ﬂexible interactions are required to establish personalization.  Due to all the insufﬁciencies of existed sum-  
We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classiﬁer software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios. 
We propose a general method to watermark and probabilistically identify the structured outputs of machine learning algorithms. Our method is robust to local editing operations and provides well deﬁned trade-oﬀs between the ability to identify algorithm outputs and the quality of the watermarked output. Unlike previous work in the ﬁeld, our approach does not rely on controlling the inputs to the algorithm and provides probabilistic guarantees on the ability to identify collections of results from one’s own algorithm. We present an application in statistical machine translation, where machine translated output is watermarked at minimal loss in translation quality and detected with high recall.  and human generated translations are currently both found extensively on the web, with no automatic way of distinguishing between them. Algorithms that mine data from the web (Uszkoreit et al., 2010), with the goal of learning to simulate human behavior, will now learn models from this contaminated and potentially selfgenerated data, reinforcing the errors committed by earlier versions of the algorithm. It is beneﬁcial to be able to identify a set of encountered structured results as having been generated by one’s own algorithm, with the purpose of ﬁltering such results when building new models. Problem Statement: We deﬁne a structured result of a query q as r = {z1 · · · zL} where the order and identity of elements zi are important to the quality of the result r. The structural  aspect of the result implies the existence of alter-  
This paper compares several translation representations for a synchronous context-free grammar parse including CFGs/hypergraphs, ﬁnite-state automata (FSA), and pushdown automata (PDA). The representation choice is shown to determine the form and complexity of target LM intersection and shortest-path algorithms that follow. Intersection, shortest path, FSA expansion and RTN replacement algorithms are presented for PDAs. Chinese-toEnglish translation experiments using HiFST and HiPDT, FSA and PDA-based decoders, are presented using admissible (or exact) search, possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs. For large rulesets with large LMs, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance. 
Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the ﬁrst experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model. 
 giving; Shafto et al. (2006)). Human knowledge-  bases such as Wikipedia also exhibit such multiple  Context-dependent word similarity can be measured over multiple cross-cutting dimensions. For example, lung and breath are similar thematically, while authoritative and superﬁcial occur in similar syntactic contexts, but share little semantic similarity. Both of these notions of similarity play a role in determining word meaning, and hence lexical semantic models must take them both into ac-  clustering structure (e.g. people are organized by occupation or by nationality). The effects of these overlapping categorization systems manifest themselves at the lexical semantic level (Murphy, 2002), implying that lexicographical word senses and traditional computational models of word-sense based on clustering or exemplar activation are too impoverished to capture the rich dynamics of word usage.  count. Towards this end, we develop a novel model, Multi-View Mixture (MVM), that represents words as multiple overlapping clusterings. MVM ﬁnds multiple data partitions based on different subsets of features, subject to the marginal constraint that feature subsets are distributed according to Latent Dirichlet Allocation. Intuitively, this constraint fa-  In this work, we introduce a novel probabilistic clustering method, Multi-View Mixture (MVM), based on cross-cutting categorization (Shafto et al., 2006) that generalizes traditional vector-space or distributional models of lexical semantics (Curran, 2004; Pado´ and Lapata, 2007; Schu¨tze, 1998; Turney, 2006). Cross-cutting categorization ﬁnds multi-  vors feature partitions that have coherent topical semantics. Furthermore, MVM uses soft feature assignment, hence the contribution of each data point to each clustering view is variable, isolating the impact of data only to views where they assign the most features. Through a series of experiments, we demonstrate the utility of MVM as an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation.  ple feature subsets (categorization systems) that produce high quality clusterings of the data. For example words might be clustered based on their part of speech, or based on their thematic usage. Contextdependent variation in word usage can be accounted for by leveraging multiple latent categorization systems. In particular, cross-cutting models can be used to capture both syntagmatic and paradigmatic notions of word relatedness, breaking up word features into multiple categorization systems and then com-  puting similarity separately for each system.  
It is often assumed that ‘grounded’ learning tasks are beyond the scope of grammatical inference techniques. In this paper, we show that the grounded task of learning a semantic parser from ambiguous training data as discussed in Kim and Mooney (2010) can be reduced to a Probabilistic Context-Free Grammar learning task in a way that gives state of the art results. We further show that additionally letting our model learn the language’s canonical word order improves its performance and leads to the highest semantic parsing f-scores previously reported in the literature.1 
This paper describes a novel approach to the semantic relation detection problem. Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. Speciﬁcally, we detect a new semantic relation by projecting the new relation’s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. First, we construct a large relation repository of more than 7,000 relations from Wikipedia. Second, we construct a set of non-redundant relation topics deﬁned at multiple scales from the relation repository to characterize the existing relations. Similar to the topics deﬁned over words, each relation topic is an interpretable multinomial distribution over the existing relations. Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations. The experimental results on Wikipedia and ACE data have conﬁrmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can signiﬁcantly improve the performance over the state-of-theart relation detection approaches. 
We report on empirical results in extreme extraction. It is extreme in that (1) from receipt of the ontology specifying the target concepts and relations, development is limited to one week and that (2) relatively little training data is assumed. We are able to surpass human recall and achieve an F1 of 0.51 on a question-answering task with less than 50 hours of effort using a hybrid approach that mixes active learning, bootstrapping, and limited (5 hours) manual rule writing. We compare the performance of three systems: extraction with handwritten rules, bootstrapped extraction, and a combination. We show that while the recall of the handwritten rules surpasses that of the learned system, the learned system is able to improve the overall recall and F1. 
 Traditional approaches to Relation Extraction from text require manually defining the relations to be extracted. We propose here an approach to automatically discovering relevant relations, given a large text corpus plus an initial ontology defining hundreds of noun categories (e.g., Athlete, Musician, Instrument). Our approach discovers frequently stated relations between pairs of these categories, using a two step process. For each pair of categories (e.g., Musician and Instrument) it first coclusters the text contexts that connect known instances of the two categories, generating a candidate relation for each resulting cluster. It then applies a trained classifier to determine which of these candidate relations is semantically valid. Our experiments apply this to a text corpus containing approximately 200 million web pages and an ontology containing 122 categories from the NELL system [Carlson et al., 2010b], producing a set of 781 proposed candidate relations, approximately half of which are semantically valid. We conclude this is a useful approach to semi-automatic extension of the ontology for large-scale information extraction systems such as NELL.  
 al., 2010). We propose a series of generative prob-  abilistic models, broadly similar to standard topic  We explore unsupervised approaches to rela-  models, which generate a corpus of observed triples  tion extraction between two named entities;  of entity mention pairs and the surface syntactic de-  for instance, the semantic bornIn relation between a person and location entity. Concretely, we propose a series of generative probabilistic models, broadly similar to topic models, each which generates a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. The output of each model is a cluster-  pendency path between them. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. The output of our approach is a clustering over observed relation paths (e.g. “X was born in Y” and “X is from Y”) such that expressions in the same cluster bear the same semantic relation  ing of observed relation tuples and their associated textual expressions to underlying semantic relation types. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path be-  type between entities. Past work has shown that standard supervised techniques can yield high-performance relation detection when abundant labeled data exists for a  tween entity mentions. We examine effectiveness of our approach via multiple evaluations and demonstrate 12% error reduction in precision over a state-of-the-art weakly supervised baseline.  ﬁxed inventory of individual relation types (e.g. leaderOf ) (Kambhatla, 2004; Culotta and Sorensen, 2004; Roth and tau Yih, 2002). However, less explored are open-domain approaches where the set of possible relation types are not ﬁxed and little to  no labeled is given for each relation type (Banko et  
This paper describes DUALIST, an active learning annotation paradigm which solicits and learns from labels on both features (e.g., words) and instances (e.g., documents). We present a novel semi-supervised training algorithm developed for this setting, which is (1) fast enough to support real-time interactive speeds, and (2) at least as accurate as preexisting methods for learning with mixed feature and instance labels. Human annotators in user studies were able to produce near-stateof-the-art classiﬁers—on several corpora in a variety of application domains—with only a few minutes of effort. 
 Eisner, 1996a) for reranking and extend it to capture  We propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of Eisner’s generative model. In our framework, we deﬁne two kinds of generative model for reranking. One is learned from  higher-order information than Eisner’s second-order generative model. Their reranking model showed large improvements in dependency parsing accuracy. They reported that the discriminative model is very effective at ﬁltering out bad candidates, while  training data ofﬂine and the other from a for-  the generative model is able to further reﬁne the se-  est generated by a baseline parser on the ﬂy. The ﬁnal prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efﬁciently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and  lection among the few best candidates. In this paper, we propose a forest generative reranking algorithm, opposed to Sangati et al. (2009)’s approach which reranks only k-best candidates. Forests usually encode better candidates more compactly than k-best lists (Huang, 2008). More-  Viterbi algorithms. Experimental results show  over, our reranking uses not only a generative model  that our proposed forest reranking algorithm achieves signiﬁcant improvement when compared with conventional approaches.  obtained from training data, but also a sentence speciﬁc generative model learned from a forest. In the reranking stage, we use linearly combined model  
 that we are not properly modeling the downstream  We present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions. The primary example is the extension of a standard supervised parsing objective function with additional loss-functions, either based on intrinsic parsing quality or task-speciﬁc extrinsic measures of quality. Our empirical results show  task in the parsers, it also means that there is some information from small task or domain-speciﬁc data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting speciﬁc extrinsic metrics of quality, which can include task speciﬁc  how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation.  metrics or external weak constraints on the parse structure. One obvious approach to this problem is to employ parser reranking (Collins, 2000). In such a setting, an auxiliary reranker is added in a pipeline following the parser. The standard setting involves  
 reasons (compactness, interpretability, good gener-  Linear models have enjoyed great success in structured prediction in NLP. While a lot of progress has been made on efﬁcient training with several loss functions, the problem of endowing learners with a mechanism for feature selection is still unsolved. Common approaches employ ad hoc ﬁltering or L1regularization; both ignore the structure of the  alization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003). Our focus is on methods which embed this selection into the learning problem via the regularization term. We depart from previous approaches in that we seek to make decisions jointly about all candidate features, and we want to promote sparsity  feature space, preventing practicioners from encoding structural prior knowledge. We ﬁll this gap by adopting regularizers that promote structured sparsity, along with efﬁcient algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability.  patterns that go beyond the mere cardinality of the set of features. For example, we want to be able to select entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity.  Such regularizers are able to encode prior knowl-  
 and the goal is to learn a grammar that can map new,  unseen, sentences onto their corresponding mean-  We consider the problem of learning fac-  ings, or logical forms.  tored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefﬁcient when words appear repeatedly with closely related lexical content. In this  One approach to this problem has developed algorithms for leaning probabilistic CCG grammars (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010). These grammars are well-suited to the task of semantic parsing, as they closely link syntax and semantics. They can be used to model a wide range of complex linguistic phenomena and are  paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a  strongly lexicalized, storing all language-speciﬁc grammatical information directly with the words in the lexicon. For example, a typical learned lexicon might include entries such as:  probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance beneﬁts greatly from the lexical factoring.  (1) f light N : λ x. f light(x) (2) f light N/(S|NP) : λ f λ x. f light(x) ∧ f (x) (3) f light N\N : λ f λ x. f light(x) ∧ f (x) (4) f are N : λ x.cost(x) (5) f are N/(S|NP) : λ f λ x.cost(x) ∧ f (x)  
People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-NER system doubles F1 score compared with the Stanford NER system. T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms cotraining, increasing F1 by 25% over ten common entity types. Our NLP tools are available at: http:// github.com/aritter/twitter_nlp  the size of the Library of Congress (Hachman, 2011) and is growing far more rapidly. Due to the volume of tweets, it is natural to consider named-entity recognition, information extraction, and text mining over tweets. Not surprisingly, the performance of “off the shelf” NLP tools, which were trained on news corpora, is weak on tweet corpora. In response, we report on a re-trained “NLP pipeline” that leverages previously-tagged out-ofdomain text, 2 tagged tweets, and unlabeled tweets to achieve more effective part-of-speech tagging, chunking, and named-entity recognition. 
 tion phrases enables the extraction of arbitrary re-  lations from sentences, obviating the restriction to a  Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-speciﬁed vocabulary. This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on bi-  pre-speciﬁed vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et  nary relations expressed by verbs. We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOEpos. More than 30% of REVERB’s extractions are at precision 0.8 or higher— compared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB’s errors, suggesting directions for future work.1  al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as TEXTRUNNER and WOE: incoherent extractions and uninformative extractions. Incoherent extractions are cases where the ex-  tracted relation phrase has no meaningful interpre-  
 learning (AL). AL reduces annotation effort by set-  ting up an annotation loop where, starting from a  Supervised classiﬁcation needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.  small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classiﬁer is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling. AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classiﬁcation (Tong and Koller, 2002), senti-  ment detection (Brew et al., 2010), and named entity  
 Short listings are typical in classiﬁed ads where  We present a named entity recognition (NER) system for extracting product attributes and values from listing titles. Information extraction from short listing titles present a unique challenge, with the lack of informative con-  each seller is given limited space (in terms of words) to describe the product. On eBay, product listing titles cannot exceed 55 characters in length. Similarly, on Craigslist and newspaper ads, the length of a listing title is restricted. Extracting product attributes  text and grammatical structure. In this work,  from such short titles faces the following challenges:  we combine supervised NER with bootstrapping to expand the seed list, and output nor-  • Loss of grammatical structure in short listings  malized results. Focusing on listings from  where many nouns are piled together.  eBay’s clothing and shoes categories, our bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identifying novel brands. Among the top 300 new brands predicted, our system  • Typographical errors, abbreviations, and acronyms that must be normalized to the standardized values. • Lack of contextual information to infer product attribute value.  achieves 90.33% precision. To output normalized attribute values, we explore several string comparison algorithms and found n-gram substring matching to work well in practice.  It can be argued that the use of short listings simpliﬁes the problem of attribute extraction, since short listings can be easily annotated and one can apply supervised learning approach to extract product at-  
With the recent rise in popularity and scale of social media, a growing need exists for systems that can extract useful information from huge amounts of data. We address the issue of detecting influenza epidemics. First, the proposed system extracts influenza related tweets using Twitter API. Then, only tweets that mention actual influenza patients are extracted by the support vector machine (SVM) based classifier. The experiment results demonstrate the feasibility of the proposed approach (0.89 correlation to the gold standard). Especially at the outbreak and early spread (early epidemic stage), the proposed method shows high correlation (0.97 correlation), which outperforms the state-of-the-art methods. This paper describes that Twitter texts reflect the real world, and that NLP techniques can be applied to extract only tweets that contain useful information.  posts more than 5.5 million messages (tweets) every day (reported by Twitter.com in March 2011). Twitter can potentially serve as a valuable information resource for various applications. Huberman et al. (2009) analyzed the relations among friends. Boyd et al. (2010) investigated commutation activity. Sakaki et al. (2010) addressed the detection of earthquakes. Among the numerous potential applications, this study addresses the issue of detecting influenza epidemics, which presents two outstanding advantages over current methods.  Large Scale: More than a thousand messages include the word “influenza” each day (Nov. 2008 – Oct. 2009). Such a huge data volume dwarfs traditional surveillance resources.  Real-time: Twitter enables real-time and direct surveillance. This characteristic is extremely suitable for influenza epidemic detection because early stage detection is important for influenza warnings.  
It is popular for users in Web 2.0 era to freely annotate online resources with tags. To ease the annotation process, it has been great interest in automatic tag suggestion. We propose a method to suggest tags according to the text description of a resource. By considering both the description and tags of a given resource as summaries to the resource written in two languages, we adopt word alignment models in statistical machine translation to bridge their vocabulary gap. Based on the translation probabilities between the words in descriptions and the tags estimated on a large set of description-tags pairs, we build a word trigger method (WTM) to suggest tags according to the words in a resource description. Experiments on real world datasets show that WTM is effective and robust compared with other methods. Moreover, WTM is relatively simple and efﬁcient, which is practical for Web applications. 
A rumor is commonly deﬁned as a statement whose true value is unveriﬁable. Rumors may spread misinformation (false information) or disinformation (deliberately false information) on a network of people. Identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unveriﬁed authority. In this paper, we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features: content-based, network-based, and microblog-speciﬁc memes for correctly identifying rumors. Moreover, we show how these features are also effective in identifying disinformers, users who endorse a rumor and further help it to spread. We perform our experiments on more than 10,000 manually annotated tweets collected from Twitter and show how our retrieval model achieves more than 0.95 in Mean Average Precision (MAP). Finally, we believe that our dataset is the ﬁrst large-scale dataset on rumor detection. It can open new dimensions in analyzing online misinformation and other aspects of microblog conversations. 
Attempts to proﬁle authors according to their characteristics extracted from textual data, including native language, have drawn attention in recent years, via various machine learning approaches utilising mostly lexical features. Drawing on the idea of contrastive analysis, which postulates that syntactic errors in a text are to some extent inﬂuenced by the native language of an author, this paper explores the usefulness of syntactic features for native language identiﬁcation. We take two types of parse substructure as features— horizontal slices of trees, and the more general feature schemas from discriminative parse reranking—and show that using this kind of syntactic feature results in an accuracy score in classiﬁcation of seven native languages of around 80%, an error reduction of more than 30%. 
This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield signiﬁcant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation. 
Although a vast amount of contents and knowledge has been made available in electronic format and on the web in recent years, translators still do not have friendly and targeted tools at their disposal for the various aspects of a translation process, i.e., the analysis phase, automatic creation and management of the linguistic resources needed and automatic updating with the relevant information generated by the computer translation tools used in the process (Machine Translation, Translation Memories, and so on). Text mining and information retrieval are not typically connected with the translation process and no existing online translation workspace integrates text mining or information retrieval facilities that are specifically aimed at improving the documentary competence of translators in order to process unstructured (textual) information, and make the information on the web or in texts accessible to translators. This paper explores a new approach to helping translators look for different types of information (glossaries, corpora, Wikipedia, and so on) related to the specific translation work they have to perform which can then be used to update the lexical base needed for the translation workflow (both human or machine-aided). This new approach is based on CATALOGA, a text mining tool, which can be combined with an IR application and/or an MT/TM system and used for different purposes. 
Background Statistical machine translation (Koehn 2010) is increasingly applied in the field of patent translation: Machine Translation Task at NTCIR-91, the EU-funded project Pluto (Tinsley et al. 2010) and the collaboration between the European Patent Office and Google translate2 (Täger 2011), etc. WIPO has experimented with the use of open-source software Moses (Koehn et al. 2007) and now provides three tools: 1) cross-lingual search for users: CLIR 2) computer aided translation tool for translators: TAPTA-js, and its Web version TAPTA-Web 3) web-based gist-translator (targeting PATENTSCOPE users): TAPTA-Web-Lite These three tools make use of the data-driven approach and use the classification of the applications (International Patent Classification - IPC) to take into account the domain when accessing translation proposals (the term ‘automatic translation’ will be generally translated into French as ‘traduction automatique’ but will be translated as ‘translation automatique’ in a domain like mechanical engineering). PATENTSCOPE and cross-lingual search: CLIR PATENTSCOPE is WIPO’s patent application search engine. As it contains patent applications in various languages, we offer users the possibility to search in multiple languages. A SMT model for various language pairs was trained using mainly patent application titles, we then automatically extract bilingual terminology from this model. CLIR allows users to search a term or a phrase and its variants in English, French, German, Japanese, Spanish, Chinese, and Korean (and to some extend Russian and Portuguese too) just by entering the term(s) in one of those languages in the search box. The system will suggest variants and translate the term(s), allowing the user to search 
The [US] National Virtual Translation Center (NVTC) has developed a model for rapid yet incremental insertion of translation technologies into its operational architecture, benefitting from cutting edge technologies, a strong assessment laboratory, key partnerships among international leaders, and day‐one collaboration with the NVTC Operations Directorate (its translation customer). This presentation will cover the components of the model for effective technology integration into US government translation environments. The NVTC Translation Technology Insertion Model The National Virtual Translation Center (NVTC) has a charter to provide expert, high quality translation for the needs of the United States intelligence, defense, and other federal initiatives. Its role has come to be the translation nexus for tens of thousands of documents, in over 100 languages and more than 20 genres, into and from English. The high quality translations can be used as standalone reports, or as input for follow‐on intelligence analysis. The variety of translation requirements at NVTC and throughout the US government challenges its translation technologists to create and implement an architecture ready for the rapid insertion of translation technologies, and a methodology for discerning the best technologies for specific translation requirements. Such an envisioned translation architecture embodies the orderly introduction of language technology components, human work roles, and the insertion points in the architecture where language technology will be introduced. The methodology for the Insertion Model takes into account the fact that many types of translation requests will quickly come to be routinely processed through the technologies. The limits on tool availability and maturity for certain languages, however, mean that some translation requests may be accomplished with fewer technology aids. Meanwhile, the capability must grow along several dimensions at once, even as the translation organizations take on larger and larger translation workloads. 
With the LiveDocs® concept, translation tools have taken yet another step beyond the conventional idea of corpora and translation memories by designing features to complement existing technologies, enabling users to choose a workflow that best suits their translation projects. LiveAlign™ - Alignment, traditionally performed separately prior to translation, can now be shifted into the translation workflow. Segments needed can be aligned on the fly in the translation grid, with the bulk of the work done automatically but still allowing intervention to fine-tune the results. ActiveTm™ - With corpora, users can simplify the management of previous translations by creating collections of translated documents, or corpora, which provide fuzzy and exact matches like TMs, but retain context for translations. Library™ - Integrated search technology allows users to find any expression in reference documents and to view it in context without leaving the translation environment. Even a monolingual document can be added into translation projects for reference. ezAttach™ - Translation projects can now contain images, executable files, sounds, even video files, as resources. Innovation is alive and well in the industry. How can you benefit? Based on previous experience, there is a great deal of knowledge and expertise amongst the session attendees. To set the stage for discussion, a short demonstration of these innovations will be given using memoQ. This will give attendees an opportunity to see these new concepts firsthand, with the purpose of providing food for thought and topics for group discussions. Structure of the presentation New-old resources – how to bring innovation into the corpora? 
Outline About the W3C Standards support for the multilingual Web The changing social context Best practices for the multilingual Web Getting involved  Outline Why is it that in 2011, it is still difficult for users and developers around the world to use the Web for their own language and culture? Which issues are more or less solved on the web (and how)? What are we doing to address the remaining problems, and how can you influence the outcomes?  About the W3C Internationalization Activity  About the Consortium 1994:World Wide Web Consortium created and still led by: Sir Tim Berners-Lee, inventor of the WWW. Mission: Lead the technical evolution of the Web and ensure its interoperability Keywords: consensus and vendor neutrality  About the Consortium Internationalization Activity  • Help W3C Working Groups understand issues and build in requirements relating to worldwide support for Web technologies • Liaise with other standards organizations to develop support for the international Web • Help users of Web technology understand what's available to them and how to use it by developing best practices and other resources  ‫عا��� ی��و یبب�و � ق� ی� ق�ی��ر پ�� عا��� ی�� ببنبا نبا‬ ᑖᑦᓱᒪ ᐃᑭᐊᖅᑭᕕᒃ ᓯᓚᕐᔪᐊᓕᒫᒥᒃ ᓈᕆᑎᑉᐹ. "Дүниежүзілік торды" нағыз дүниежүзілік етеміз! वल्ड वाईड वेबलाई यथाथ्र म �व�व्याप बनाउने ! የዓለም አቀፉን ድር በእውነት አለም አቀፍ ማድረግ! Κάνοντας τον Παγκόσμιο Ιστό πραγματικά Παγκόσμιο ਵਰਡ ਵਾਈਡ ਵੈਬ ਨੂੰ ਵਾਕਈ ਿਵ-ਿਵਆਪੀ ਬਨਾਉਣਾ ! 缔造真正全球通行的万维网 ワールド・ワイド・ウェッブを世界中に広げましょう េធឲ� ្យេវ លវ ៉យេវ៉បមានូទ ទំងពិភពេលកពិ្រ! 전세계의 월드 와이드 웹으로 만들기! Gwneud y we fyd-eang yn wirioneddol fyd-eang! !‫ליצור מהרשת רשת כלל עולמית באמת‬ การทําให World Wide Web แพรหลายไปทั่วโลกอยางแทจริง འཛམ་ིང་ཡོངས་འེལ་འདི་ ང་ོ མ་འབད་རང་ འཛམ་ིང་ཡོངས་�་བ་ �གསཔ་བཟོ་བ།  Standards support for the multilingual Web  Standards support Unicode  !!‫جعل شبكةو الوبي�ﮫﺎ�بﯽالراعا�ل�ميّ��ةﯽ�عاﮫﺎل�مﯽيّةﺳﺎز�ح �قﻢا‬  ‫ﮐﻮ" ﺣﻘﻴﻘﯽ ﻃﻮﺭ ﭘﺮ ﻋﺎﻟﻤﮕﻴﺮ ﺑﻨﺎﻧﺎ‬T‫ﺐ‬h‫ﻭﻳ‬e‫ﻴﺮ‬P‫ﻤﮕ‬a‫ﻋﺎﻟ‬th W3C follows to making text on the Web Համաշխարհային ցանցն իրոք համաշխարհային դարձնելը ᑖᑦᓱᒪ ᐃᑭᐊᖅᑭᕕᒃ ᓯᓚᕐᔪᐊᓕᒫᒥᒃ ᓈᕆᑎᑉᐹ. truly global is Unicode." "Дүниежүзілік торды" нағыз дүниежүзілік етеміз! वल्ड वाईड वेबलाई यथाथ्र म �व�व्याप बनाउने ! የዓለም አቀፉን ድር በእውነት አለም አቀፍ ማድረግ! Κάνοντας τον Παγκόσμιο Ιστό πραγματικά Παγκόσμιο  ਵਰਡ ਵਾਈਡ ਵੈਬ ਨੂੰ ਵਾਕਈ ਿਵ-ਿਵਆਪੀ ਬਨਾਉਣਾ ! 缔造真正全球通行的万维网  Tim Berners-Lee  !‫ליצור מהרשת רשת כלל עולמית באמת‬  ˈmeɪkɪŋ ðə wɜːld waɪd wɛb ˈtruːlɪ ˈwɜːldˈwaɪd  ワールド・ワイド・ウェッブを世界中に広げましょう េធ�ឲ្យេវ លវ ៉យេវ៉បមានូទ ទំងពិភពេលកពិ្រ!  전세계의 월드 와이드 웹으로 만들기!  Gwneud y we fyd-eang yn wirioneddol fyd-eang!  การทําให World Wide Web แพรหลายไปทั่วโลกอยางแทจริง འཛམ་ིང་ཡོངས་འེལ་འདི་ ང་ོ མ་འབད་རང་ འཛམ་ིང་ཡོངས་�་བ་�གསཔ་བཟོ་བ།  Standards support Unicode  !!‫جعل شبكةو الوبي�ﮫﺎ�بﯽالراعا�ل�ميّ��ةﯽ�عاﮫﺎل�مﯽيّةﺳﺎز�ح �قﻢا‬  Other  ‫ﻋﺎﻟﻤﮕﻴﺮ ﻭﻳﺐ ﮐﻮ ﺣﻘﻴﻘﯽ ﻃﻮﺭ ﭘﺮ ﻋﺎﻟﻤﮕﻴﺮ ﺑﻨﺎﻧﺎ‬  Համաշխարհային ցանցն իրոք համաշխարհային դարձնելը  ᑖᑦᓱᒪ ᐃᑭᐊᖅᑭᕕᒃ ᓯᓚᕐᔪᐊᓕᒫᒥᒃ ᓈᕆᑎᑉᐹ.  ASCII  "Дүниежүзілік торды" нағыз дүниежүзілік етеміз!  वल्ड वाईड वेबलाई यथाथ्र म �व�व्याप बनाउने !  የዓለም አቀፉን ድር በእውነት አለም አቀፍ ማድረግ!  UTF-8  Κάνοντας τον Παγκόσμιο Ιστό πραγματικά Παγκόσμιο  ਵਰਡ ਵਾਈਡ ਵੈਬ ਨੂੰ ਵਾਕਈ ਿਵ-ਿਵਆਪੀ ਬਨਾਉਣਾ ! 缔造真正全球通行的万维网 !‫ליצור מהרשת רשת כלל עולמית באמת‬ ˈmeɪkɪŋ ðə wɜːld waɪd wɛb ˈtruːlɪ ˈwɜːldˈwaɪd ワールド・ワイド・ウェッブを世界中に広げましょう េធ�ឲ្យេវ លវ ៉យេវ៉បមានូទ ទំងពិភពេលកពិ្រ! 전세계의 월드 와이드 웹으로 만들기!  Gwneud y we fyd-eang yn wirioneddol fyd-eang! การทําให World Wide Web แพรหลายไปทั่วโลกอยางแทจริง  Unicode on the Web  འཛམ་ིང་ཡོངས་འེལ་འདི་ ང་ོ མ་འབད་རང་ འཛམ་ིང་ཡོངས་�་བ་�གསཔ་བཟོ་བ།  Standards support Unicode <h2><a id="რჩეული">რჩეული ფოტოსურათი</a></h1> <p><a href="/wiki/ჭიამაია" title="ჭიამაია" class="mw-redirect">ჭიამაია</a> (Coccinellidae), ხოჭოების ოჯახს ეკუთვნის. აქვს ამობურცული, მომრგვალო ან ოვალური სხეული. ზურგზე ღია ფონზე შავი ლაქები აყრია, იშვიათად ...  Standards support Unicode normalization NFD I◌́zeli◌́to◌̋u◌̈l NFC Ízelítőül Ha a világ beszélni akarna, Unicode-ul szólalna meg. Regisztráljon már most a Tizedik Nemzetközi Unicode Konferenciára, melyet 1997. március 10-12-én rendeznek Meinz-ban, Németországban. Ezen a konferencián az iparág több neves szakértője is résztvesz. Ízelítőül a témákból: a világháló és a Unicode nemzetközisítése és lokalizálása, a Unicode alkalmazása működő rendszerekben és alkalmazásokban, szövegelrendezésnél, és többnyelvű számítógépeken.  Standards support Web resource identifiers  http://JP納豆.例.jp/dir1/引き割り.html  Scheme Domain name  Path  IDN  xn--jp-cd2fp15c.xn--fsq.jp  Standards support Top level domain names  ‫ﺍﻟﺴﻌﻮﺩﻳﺔ‬ ‫ﺍﻣﺎﺭﺍﺕ‬ ‫مصر‬  Al-Saudiah Emarat Misr  http://‫ﻣﺼﺮ‬.‫ﺍﻷﺗﺼﺎﻻﺕ‬-‫ﻭﺯﺍﺭﺓ‬  IDN  Standards support Web resource identifiers  http://JP納豆.例.jp/dir1/引き割り.html  Scheme IRI  Domain name  Path  /dir1/%E5%BC%95%E3%81%8D%E5%89%B2%E3%82%8A.html  Standards support Language tags language – region ♦ ISO 639 language codes ♦ ISO 3166 country codes  Before (RFC 3066) en en-GB en-scouse  Standards support Language tags: BCP 47  Now BCP 47  language – script – region – variant – extension – private_use (extlang) hi  ♦ nearly 8,000 subtags available ♦ subtags available only from new IANA registry (based on ISO and UN codes)  az-Cyrl zh-Hans es-419  ♦ only language subtag required  sl-IT-rozaj-njiva-1994  Standards support Key Events  Developing requirements Speech Synthesis Markup Language 這一晚會如常舉行 這一|晚會|如常|舉行 這一|晚會|如|常|舉行 這一晚|會|如常|舉行  Standards support CSS3  hanging alphabetic ideographic  A  ক 国  Implementers of user agents need to be prodded by the public to support the developing marketplace !  Standards support Hyphenation Zusätzlich erleichtert PLS die Eingrenzung von Anwendungen, indem es Aussprachebelang e von anderen Teilen der Anwendung abtrennt.  Zusätzlich erleichtert PLS die Eingrenzung von Anwendungen, indem es Aussprachebelange von anderen Teilen der Anwendung abtrennt. * { hyphens: auto; }  Standards support OpenType feature support by language  Standards support Vertical text  Standards support Vertical text  Standards support Vertical text  Standards support Ruby annotation  Standards support Ruby annotation <ruby<rb>凝</rb><rt>ぎょう</rt></ruby> <ruby><rb>視</rb><rt>し</rt></ruby> <ruby>凝<rt>ぎょう</rt>視<rt>し</rt></ruby> <ruby> <rbc><rb>凝</rb><rb>視</rb></rbc> <rtc><rt>ぎょう</rt><rt>し</rt></rtc> </ruby>  Developing requirements Requirements for Japanese Layout  Standards support Web fonts  െതാ�ര്  នរ� ជ‫ببنبا ن‌با‬តូ��‫� ی‬ប��ិ ន‫پ�� عا‬យ‫�ر‬ក� ‫ی‬ម �‫ب �و � ق� ی� ق‬  ལ་ིར་བ�ར་བའི་ལས་དོན།  Standards support Web fonts @font-face { font-family: 'battambang-woff'; font-style: normal; font-weight: normal; src: url(fonts/khmerosbbang.woff); }  :lang(kh) { font-family: 'battambang-woff'; font-size: 100%; }  Issues • Rendering detail for complex fonts. • Subsetting capability may be needed. • Can only be used for fonts with an appropriate licence.  Standards support Language declarations in HTML5 ✔ <html lang="de"> <head> ✘ … <meta http-equiv="Content-Language" content="de"> … </head> …  Standards support Date and time <time 8 datetime="2004-08-08"> สิงหาคม ๒๕๔๗</time> <form> <input type="date"> </form>  Standards support Bidirectional text support  ،‫نشاط التدويل‬W✘3C ✔W3C ،‫نشاط التدويل‬  ،‫ نشاط التدويل‬W3C <description dir="rtl">  </description>  Developing requirements Augmenting bidi support in HTML5 & CSS  Developing requirements Arabic mathematics  Standards support Internationalization Tag Set <para> Press the <uitext translate="no">START</uitext> button to sound the horn. The <uitext translate="no">MAKE-READY/RUN</uitext> indicator flashes. </para>  <para> Press the <uitext>START</uitext> button to sound the horn. The <uitext>MAKE-READY/RUN</uitext> indicator flashes. </para>  • supported by some translation tools – linked with XLIFF • being applied by specifications at W3C  <its:rules ... its:version="1.0"> <its:translateRule selector="//uitext" translate="no"/> </its:rules>  The changing social context  Social context The rise of the Mobile Web • "In China … over 73m people, or 29% of all internet users in the country, use mobile phones to get online." • "The number of pages viewed in June by 14m users of [Opera] software was over 3 billion, a 300% increase on a year earlier. The fastest growth was in developing countries including Russia, Indonesia, India and South Africa." Economist.com, Sept. 2008  Social context Mobile Web for Developing Society (MW4D) Track the social impact of the mobile web in the developing world, to ensure that the web's technical standards evolve to serve this rapidly emerging constituency.  Best practices for the multilingual Web  Best practices Capturing guidance for spec developers markup for bidirectional text Normalization working with case sensitivity more information about date & time  Best practices Tests  Best practices I18n resources Articles Tutorials httpT:/e/cwhnwicawl .nwot3es.org/International/ Tests Talks Tools Reviews  Best practices I18n resources Articles Tutorials Technical notes Tests Talks Tools Reviews  Best practices I18n resources Creating HTML & CSS  Best practices I18n resources Language  Best practices I18n resources Choosing language values  Best practices I18n resources  Best practices I18n resources  Best practices Text expansion  Korean  0.8  English views  
a. NATO, the North Atlantic Treaty Organization, was founded in 1949 by a group of ten Western European states together with the USA and Canada to counter the perceived threat from the Soviet Union and, in its initial stages, to prevent any resurgence of German ambitions. Whereas at first NATO was little more than a political association, the Korean War (1950-1953) brought the realization that a more robust organization, known as the integrated military structure, was required. b. In such a structure, the military forces of all services: Army, Navy, Air Force and Marines, belonging to twelve countries (others joined later and the Alliance now has 28 members) needed to cooperate with each other in order to provide effective collective defence and to achieve what we now call “interoperability”. Each of those nations and services had different doctrines, procedures, organizations, equipment and languages, although English and French had been adopted as the Alliance’s official languages at the very first meeting of the North Atlantic Council (NAC) in 1949. To deal with this problem, in 1951 NATO set up the Military Agency for Standardization (MAS) for the purpose of fostering the standardization of operational and administrative practices and war material. A number of groups within NATO soon recognized that it was not possible to plan and conduct effective Allied operations without a common understanding of the language, i.e. the terminology, to be used. They therefore began to compile glossaries, the most well-known in military circles being Allied Administrative Publication (AAP) 6 - NATO Glossary of Terms and Definitions (English and French) “listing terms of military significance and their definitions for use in NATO”, first published by the MAS in 1956. In all, more than 20 official NATO Glossaries covering various domains have been issued. You can find the latest versions of some of these glossaries by following this link: http://nsa.nato.int/nsa/ and clicking on the “Terminology” box to the left of the home page. c. You will note that from the very beginning, terminology was placed within the sphere of standardization and terms and definitions were adopted for use by NATO as a whole. In other words, terminology was not being recorded simply for the benefit of linguists. 
As a Translator 2.5 you have to: - Keep a perfect track of active or planned projects, quotes and tasks Translations tend to come in smaller chunks. In the past, a technical manual 300 pages long might be translated by one to three different translators (depending on timing), so it was pretty easy to keep track of all of it. Now, the same manual is going to be 80% post-edited and you can be hired to just take care of headlines, captions and small updates, perhaps for three manuals at e same time. You need to keep track of all of it, of which POs you have got and which ones you don't, any invoices ending payment, etc. You can run all this info through your customized mighty excel sheet, or you can  
{btsou99, lubin2010}@gmail.com Abstract This paper proposes to familiarize the MT users with two major areas of development: (1) To improve translation quality between uncommon language pairs, the use of a third language as the pivot. Various techniques have been shown to be promising when parallel corpora for the uncommon language pairs are not readily available. They require the use of two other language pairs involving a common third language pairing with each member of the initial target pair. (2) The surging demands in the field of patent translation and for efforts to bootstrap machine translation in uncommon language pairs (e.g., Japanese and Chinese) via more common language pairs (e.g., Chinese-English and English-Japanese), and the application of the pivot approach to expedite processing. 1. Introduction Recent success in the application of machine translation (MT) in many multilingual contexts, such as textual translation and cross-lingual information retrieval, has been dependent on the building up of critical bilingual resources such as Translation Memory (TM) to develop and continuously fine tune the translation or search engines. The basis for TM 
We will explain and demonstrate iMAGs (interactive Multilingual Access Gateways), in particular on a scientific laboratory web site and on the Greater Grenoble (La Métro) web site. This bilingual presentation has been obtained using an iMAG. Keywords 
The workflow of a translation servicemay be defined as the model (static view) or processes (dynamic view) according to which documents to be translated reach the translation service,are affected to a translator and then possibly a reviser, the translation is produced, and the translated document is pushed out towards the customer, the editing service, the web/paper publishing service, etc. The benefit of a well-designed and implemented translation workflow can be shown at several levels: 
1. Lack of people: – many minority languages suffer from a lack of people to assist with the creation of language technologies. There could be a lack of actual speakers of the language, a lack of people knowledgeable about the language, a lack of people willing to use modern technologies, a lack of people willing to create and take responsibility for the technologies, a  lack of people with knowledge on how to create language technologies and even a lack of people that believe there is merit in placing language resources in an electronic environment. 2. Lack of content: – many minority languages lack a significant amount of digital content that is necessary to build language technologies. Some languages do not have a written form at all, while some languages have written forms that are not stored in an electronic format and thus may require significant effort to digitise the language resources. 3. Lack of trust: – many minority languages treat their language resources as sacred treasures that need special care and protection. There is a concern that placing language resources and consequently cultural knowledge in electronic environments can open them up to abuse and degradation. There are also concerns about privacy and intellectual property rights. 4. Lack of finance: – creating language technologies cost money and time. Minority language communities are invariably communities operating in the lower ends of the economic scales and subsequently are the ones least likely to have money to spend on creating language resources. They are also the communities who struggle the most with health, education, housing and other social issues, so are the communities least likely to have the time to investigate, strategize and implement programmes that utilise language technologies. 5. Lack of tribal unity: – some minority language communities, do not agree on a single form of language, or a single written form of language, or a specific authority to make decisions about how a language should be represented electronically. Furthermore, minority language communities under duress due to inter tribal politics, or even inter tribal warfare face extra hardships and are unlikely to be in a position to consider language technologies. 6. Lack of government support: – some minority languages receive little in the way of support or recognition from government authorities. Some government authorities actively seek to repress minority languages e.g. the Kurds in Turkey (Nettle & Romaine, 2000, pp145-146), and the Siraya people in Taiwan [1]. Consequently many minority languages may not be in an environment where language technologies can serve a meaningful purpose. However the authors believe that this is not the case with regard to te reo Māori, and that this language can in fact benefit greatly by the uptake of language technologies. 
0. Abstract The Language Product Evaluation Tool (LPET) offers a framework for evaluating a range of language products, including full translations and various types of summaries. The result of a collaborative effort between researchers and practitioners working in a US government setting, the LPET serves as a practical tool aimed at establishing translation standards; facilitating the quality review process and the delivery of consistent, systematic feedback; providing information for planning professional development activities for translators; and providing aggregated data describing individual and organizational capability and performance. We begin this paper by describing the development of the LPET and its major features. We then highlight findings from operational testing with quality reviewers and describe validity testing that is in progress. We end by presenting a sample view of aggregated LPET data of the type that might be useful at the managerial or organizational level. 1. Introduction to the Language Product Evaluation Tool (LPET) 1.1. Motivation for the LPET. A critical task for language analysts in the United States (US) government is translation of foreign language materials (both written and spoken) into written English. Language analysts produce a range of language products, including full translations, complete summaries, targeted summaries, gists, and hybrids that combine features of multiple product types. Although it is common for products to be reviewed by a second, typically more experienced, language analyst, there is no government-wide standard for the precise nature of the review. As a result, practices for reviewing and providing feedback vary across and sometimes within organizations (Michael et al., 2008). Standardized assessments are vital to ensuring high standards of quality review, giving language analysts meaningful feedback for improvement, and providing managers with a set of metrics to help them understand and improve the capability and performance of their workforce. 1.2. Precursor to the LPET. In response to this need to establish uniform translation standards within the US government context, a team of researchers at the University of Maryland Center for Advanced Study of Language (CASL) first developed an analytic rubric aimed at explicating the components required for one particular type of language product referred to in government contexts as targeted summary translation. Targeted summaries are typically written 2  in response to customer requirements (akin to “translation briefs” or “commissions” in nongovernment contexts). For example, a targeted summary of a speech by a Chinese Communist Party official would be very different if it were written in response to a request for information about plans to float the Chinese yuan than if it were written in response to a request for information about Chinese relations with North Korea. In developing an analytic rubric for targeted summaries, we consulted the scientific literature; conducted interviews with language analysts, area specialists, quality reviewers, and instructors; and evaluated the strengths and weaknesses of an earlier holistic rubric (Michael, Bailey, Gannon-Kurowski, & Pinckney, 2007). Following standards in educational and psychological measurement (e.g., Crocker & Algina, 2006), we strove to produce a rubric characterized by the following qualities:  All elements worthy of evaluation are included.  Each element is unidimensional in that it does not overlap with other elements.  Each element communicates clearly to the user.  The rating on each element covers the range of performance, typically in the range of 3 to 7 levels. The resulting analytic rubric, known as the Summary Translation Evaluation Tool (STET), encapsulates six analytical dimensions that cover a summary’s content, structure, and style: Significance, Completeness, Accuracy, Omission of Irrelevant Information, Organization, and Writing. (For a more detailed description of the STET and its six dimensions, see Michael, Massaro, & Perlman, 2009.) To conduct a rigorous evaluation of the STET, we created a set of summaries established by experts to represent specified levels of performance along the six dimensions; a group of 38 language analysts then used the STET to evaluate each summary (Michael et al., 2010). Quantitative modeling of the results suggested that the tool was valid and sensitive; that is, variations in summary quality were appropriately reflected in ratings for the corresponding dimension and the STET permitted a reasonable range of performance ratings along each dimension. Reliability testing demonstrated some consistency across users and conditions but suggested a need for more extensive training, a finding that was instrumental in establishing the training protocol for the LPET. 3  1.3. Development of the LPET. Targeted summary translations are increasingly common in government contexts because of the overwhelming amount of material to be processed, making it impractical and unnecessary to write full translations of all source items. However, full translations and other types of language products are still required in many situations, and the LPET was developed because government practitioners who encountered the STET asked for a similar tool that extended to all product types. Like the STET, the LPET is a practical tool aimed at (1) establishing translation standards; (2) facilitating the quality review process and the delivery of consistent, systematic feedback; (3) providing information for planning professional development activities for translators; and (4) providing aggregated data describing individual and organizational capability and performance. Although academe and industry have already developed a variety of models for translation assessment, many of them fail to achieve the combination of practical applicability and theoretical soundness that is embodied in the STET and the LPET. Colina (2008, 2009) succinctly captured the issues on both sides: On the one hand, practical translation evaluation schemes, such as the Society of Automotive Engineers translation quality metric for the automobile industry (SAEJ2450) or the American Translators Association certification test grading criteria,1 have been developed ad hoc based on common patterns of errors, so they consist largely of lists of such errors that can be checked off. Because these schemes are based on neither an explicit theory of translation nor conceptually-based categories, they do not provide orienting principles for translation assessment that might be adapted for use in other contexts. On the other hand, many theoretical models of translation have been developed without reference to practical applicability. Such theories pose foundational questions about the type or degree of equivalence between source and target texts and the relative privileging of author, translator, or audience, but they fail to specify on-the-ground categories or metrics for deciding whether a given translation satisfies the abstract criteria of a given theory. The basic features of the LPET, presented in the next section, capture conceptual categories that underpin language product quality while providing a concise and systematic way for reviewers to document product quality and task difficulty. 
Translation Memory (TM) is a key technology in the translation industry, its success based on several assumptions: that it saves time, provides cost savings, and enhances consistency. Our research tests the assumption of consistency in TMs. We examine English/German and English/Japanese TM data from two commercial companies with a view to measuring the levels of consistency (or inconsistency) between TMs across product releases. To meet our aim we first had to develop a method for interrogating TMs for consistency. Following a pilot study, we chose to categorise translation units based on whether the TM translation process had introduced consistency or inconsistency. Additionally, we examined source text segments that contained minor inconsistencies where the corresponding target text introduced further inconsistency. By identifying categories of inconsistency along with investigating the accompanying metadata, this case study hopes to highlight the types of inconsistency that can occur in TM data and to suggest how TM tools might overcome this problem. Introduction Use of TM tools has become widespread since their introduction in the early 1990s based on the assumptions that they save time, reduce costs, and enhance consistency. These assumptions are repeated in promotional literature from software producers and translation vendors, as well as in reviews, and articles (Elimam, 2007). As the popularity of these tools have grown, a small number of researchers have noted that consistency may be reduced by translation units (TUs) that have “become inaccurate over time” (Bowker, 2002, p116). Following a pilot study in 2005, Bowker  found that student translators were not critical enough of proposed translation from a TM that had been seeded with errors and wrote that “although it is frequently claimed that TMs improve consistency, this is not always the case” (2005, p18). In 2007, Ribas López opined that dissemination of errors throughout a project from errors in a TM was an underreported phenomenon (2008, p52). This paper aims to provide empirical evidence of inconsistency in TMs, then to categorise these inconsistencies, and to compare results from TMs originating from different sources and in different language pairs. Typology of inconsistencies In order for consistency to be identified and measured in TM data, following a pilot study, we developed a typology of consistency for TUs. Within this typology the following four categories are possible: 1. inconsistent source segments are translated as inconsistent target segments 2. inconsistent source segments are translated as consistent target segments 3. consistent source segments are translated as inconsistent target segments 4. consistent source segments are translated as consistent target segments In category 1 we use the term ‘inconsistent source segments’ to refer to cases where there are very minor formal differences between two source segments and such differences do not reflect any semantic differences between the segments in question. Such minor formal differences include differences in: capitalisation, tags, punctuation, spaces, and spelling (where a segment may be inconsistent with another segment simply because of a misspelling or a typographical error in one of the segments). These minor source segment inconsistencies were prevalent in our pilot and main study data and, aside from the typographical errors, would present a 100% match or a 99% fuzzy match to a  translator using the TM. Thereafter, segment-level inconsistency is observed where two segments that one could reasonably expect to be formally identical differ from each other in some way. In the case of target segments, it appears reasonable to expect segments that are translations of ‘the same’ source segment (i.e. segments that are translations of different tokens of the same source type) to be formally identical, especially in a translation memory scenario where the goal is to reuse existing translations for already encountered source segments. Where there are two different translations (and thus two different target segments) for a single source segment type, we speak of target segment inconsistency. The differences between the target segments in question can be very minor formal differences (as defined above), but they can also be more substantial, in extreme cases even leading to semantic differences between the two segments. Category 2 TUs contain target segment consistency that has been introduced in the translation process. Category 3 was our initial focus for this study – introduced inconsistency in the target segments. Category 4 may be seen as the ideal in localisation, whereby the TM has provided the best possible leverage and thus saved the maximum possible amount of time and money. Inconsistent segments are counted by identifying the number of types n. The number of segmentlevel inconsistencies is the type count minus one (n-1). Thus in the case of a single source segment (type) that has 4 tokens, if there are 3 separate translations (3 types; one of which appears twice), then the number of target segment inconsistencies is 2 (or 3-1). Thus we give a special status (of ‘master’ or ‘reference’ segment) to one of the target segments, and treat the other two segments as inconsistent with that reference segment. The reference segment is the one which appears first chronologically, and which a translator could have, but did not reuse in unchanged form. For example, the following four translations for 'Click an empty part of the drawing area.' appear in the TM data: a. Klicken Sie auf der freien Zeichenfläche. b. Klicken Sie auf einen freien Bereich der Zeichenfläche. c. Klicken Sie auf einen freien Bereich der Zeichenfläche. d. Klicken Sie auf einen beliebigen freien Bereich auf der Zeichenfläche. Although there are four tokens, there are only three types: a, b, and d. If we assign the status of  reference segment to segment a, the segments that are inconsistent with the reference segment are b (repeated for c) and d: thus we count two inconsistencies. When we have three types n=3, and since our count is of type (n - 1), we count two inconsistencies. At segment-level, source or target segments are either consistent or formally differ and are thus inconsistent. However, there may be more than one inconsistency within these segments. For this reason we also count and categorise inconsistencies found within inconsistent target text segments (those found in categories 1 and 3 above). These inconsistencies are categorised mostly per part of speech aside from those with inconsistent punctuation or where word order has been changed. If there are more than three inconsistencies within a target segment, we consider that segment to have been wholly retranslated. These categorised inconsistencies may be further subcategorised; for example nominal inconsistencies that differ lexically, or in number (singular/plural). These subsegment-level inconsistencies are counted in the same way as segment-level inconsistencies: we identify the number of types n, assign one the status of master or reference segment, then count the types that are inconsistent with the part-of-speech or word order in the reference segment. Thus the count is n minus the reference segment (n-1). Again, the reference segment is the one which appears first chronologically, and which a translator could have, but did not reuse in unchanged form. TM data used in this study In order to maximise the validity of this research, it was important to obtain real-world TM data from several sources (Susam-Sarajeva, 2009, p7). However, obtaining a company's “translation family jewels” (Smith, 2008, p23) proved to be difficult, and these were only received following protracted negotiations and, in one case, after a non-disclosure agreement had been signed. We were further limited to English, German and Japanese by researcher's own language competence. The TM data was presented in the TMX format, and this was parsed using a Python script before the TUs were categorised. The aligned segments were loaded into a LibreOffice spreadsheet for categorisation. While it was possible to automatically highlight inconsistencies of categories 2 or 3, the minor inconsistencies in the source segments for category 1 could only be identified manually.  We obtained four sets of TM data from two companies. TM A is an English to German TM containing 22,691 TUs of aligned segments of which 188 contain only numbers, dates, or punctuation. The remaining 22,503 TUs were categorised as specified above. TM B is an English to Japanese TM from the same project as TM A, containing 18,799 TUs. After removing those that contain only numbers, dates, or punctuation, 18,650 TUs remained to be categorised. TM C is an English to German TM containing 301,583 TUs. After removing those that contain only numbers, dates, or punctuation, 293,924 TUs remained. Due to time constraints, we chose a sample of the first 50,061 TUs to analyse, having confirmed that the incidence of category 2 and 3 TUs in the remainder of the TM was consistent with the sample. This was also the case for TM D, containing 298,700 TUs in English and Japanese from the same project as TM C. After removing the TUs that contain only numbers, dates, or punctuation, we were left with 292,258 TUs of which 50,000 were subject to analysis.  Results by category Category 1: Inconsistent source text segments  Letter case Punctuation Tags Typo Space Word Order  TM A 55 47 42 4 10 0  TM B 13 2 2 1 2 0  TM C 370 50 46 13 30 1  Total Segments 353  47  947  (tokens)  Total Subsegment 158  25  510  Inconsistencies  Table 1: Inconsistencies found in category one source text segments  TM D 753 73 137 11 68 0 1980 1042  Category 1 TUs contain minor inconsistencies as specified in our typology in the source segment and other kinds of inconsistencies in the aligned target segment. The number of category 1 TUs found in our four sets of TM data differed, but in all four TMs the most prevalent category of source text (ST) inconsistency was in letter case or capitalisation of words. None of the TT segments aligned with ST segments that contain inconsistencies in letter case themselves contain instances of inconsistent letter case; rather the TT segments in question evince other kinds of inconsistencies, as  in the following example from TM C:  1.1s (SHIFT+right-click the drawing area.) 1.2s (Shift+right-click the drawing area.)  1.1t (Klicken Sie bei gedrückter UMSCHALTTASTE mit der rechten Maustaste in den Zeichenbereich.) 1.2t (Klicken Sie bei gedrückter UMSCHALTTASTE mit der rechten Maustaste in den Zeichnungsbereich.)  In both TT segments the ST word 'shift' has been translated as 'Umschalttaste' and capitalised. This would suggest that a TM match was used despite the change of case in the second ST segment. However, the German translation of 'drawing area' was changed from 'Zeichenbereich' to 'Zeichnungsbereich'. According to the metadata, segment 1.1t was created on December 22nd 2006 and last changed two years later on December 7th 2008. Segment 1.2t was created by a different translator on January 15th 2009 and last changed one year later on January 18th 2010. We found a higher incidence of inconsistent placing of the space character in TM D. These spaces were initially noticed by automatically comparing the ST segment and the following, seemingly identical, ST segment, as 54 of the 68 space inconsistencies were at the end of the segment following a full stop. Again, the aligned target text (TT) segments contain other kinds of inconsistencies, as in the following example:  2.1s {1}lsp{2} file. 2.2s {1}lsp{2} file. [Contains extra space]  2.1t {1}lsp{2} フ ァ イルから自動的にロードされます。 2.2t {1}lsp{2} フ ァ イルは変更しないでく ださい。  The ST segment contains a space inconsistency, while the aligned TT segments differ by particle (は 'wa' and から 'kara'), 2.1t has the additional adjective automatic or 自動的'jidouteki', and the verbs differ semantically and in form. These TT segments also provide examples of explicitation in Japanese TT, a phenomenon that appears throughout TMs B and D. The translation of a ST noun has become a sentence in the Japanese TT, containing detail not present in the ST. Thus we have 'You may load automatically from the lsp file.' in segment 2.1t and 'Please do not change the lsp file.' in segment 2.2t.  Category 1: Inconsistent TT segments  TM A  TM B  TM C  TM D  Noun  80  11  330  616  Punctuation  3  4  46  129  Tags  5  4  2  147  Verb  42  2  51  75  Space  6  0  29  141  Word Order  39  
As the number of manually acquired rules in a patent translation system increases, conflicts between rules inevitably exist. Meanwhile, lacking the matched cases of manually acquired rules in real bilingual corpus, people may conceive rules which cause implausible impact on machine translation quality. In this paper, we propose a feedback selecting algorithm for manually acquired rules in patent translation using automatic evaluation, which picks out manually acquired rules that benefit machine translation quality. Experiments show that we achieve significant improvement in terms of BLEU (+5.23 points over baseline). 
In the automatic translation of complicated patent sentences, one of the issues to improve translation quality is to translate verbs in the source language with various meanings to corresponding different words in the target language correctly. This paper proposes the disambiguation method using the word grouping. Verbs with various meanings usually co-occur with their corresponding nouns, and show the different meanings. Valence and frame structures of verbs were used to resolve such problems. However, the meanings should be dealt with more deeply and appropriately. This paper describes the trial of word grouping based on a thesaurus.  However, the quality of the translation is not so good. One of the reasons is the existence of verbs with various meanings. Such verbs should be differently translated into words in the target language. Especially in Japanese, verbs originated in traditional Japanese have many meanings. For example, a Japanese verb “ateru” has many meanings such that a sentence “battoni bo-ruwo ateru” is translated into English sentence “hit a ball with a bat”, “kabeni tewo ateru” into “put hands onto the wall”, and “kuziwo ateru” into “win a lot.” This paper proposes the grouping of words which co-occur with a verb. In this paper, we investigate how useful the word grouping is for the translation disambiguation of verbs. 2 Related Works  
This paper introduces a new approach of patent translation. The basic technology is functional language model proposed by the same authors. Using a long claim sentence that applied to America and China of Japanese patent originally, the translation process is demonstrated. Through this process, special linguistic issues in each language are clarified in translation of patents. Keywords: Natural Language Proceeding, Machine translation, Patent translation, Functional Model of sentences 
MOLTO is an FP7 European project whose goal is to translate texts between multiple languages in real time with high quality. Patents translation is a case of study where research is focused on simultaneously obtaining a large coverage without loosing quality in the translation. This is achieved by hybridising between a grammar-based multilingual translation system, GF, and a specialised statistical machine translation system. Moreover, both individual systems by themselves already represent a step forward in the translation of patents in the biomedical domain, for which the systems have been trained. 
This paper describes the work we conducted for building a statistical machine translation (SMT) system for the Chinese-English subtask of the NTCIR-9 patent MT evaluation. Our results show that most of the generic techniques we had developed for improving SMT performance work on patent data as well, and the changes we made to our SMT system training procedure in order to address special characteristics of patent documents produced additional improvements. 
This paper studies issues on machine translation of Japanese functional expressions into English. Unlike our previous works, in order to address the issue of resolving various ambiguities of a compound expression, this paper takes the approach of example-based machine translation. In this approach, a patent translation example database is developed given the phrase translation tables trained with parallel patent sentences as well as the training parallel patent sentences themselves. When identifying the most similar translation examples, we integrate semantic equivalence classes of Japanese functional expressions as well as more ﬁne-grained similarity measure of translation examples. In the evaluation, we compare the translation accuracy of the proposed framework with that of Moses, and show that the proposed framework somehow outperforms Moses. 
Over the past twenty years, we have attacked the historical methodological barriers between statistical machine translation and traditional models of syntax, semantics, and structure. In this tutorial, we will survey some of the central issues and techniques from each of these aspects, with an emphasis on `deeply theoretically integrated' models, rather than hybrid approaches such as superficial statistical aggregation or system combination of outputs produced by traditional symbolic components. On syntactic SMT, we will explore the trade-offs for SMT between learnability and representational expressiveness. After establishing a foundation in the theory and practice of stochastic transduction grammars, we will examine very recent new approaches to automatic unsupervised induction of various classes of transduction grammars. We will show why stochastic linear transduction grammars (LTGs and LITGs) and their preterminalized variants (PLITGs) are proving to be particularly intriguing models for the bootstrapping of inducing full-fledged stochastic inversion transduction grammars (ITGs). On semantic SMT, we will explore the trade-offs for SMT involved in applying various lexical semantics models. We will first examine word sense disambiguation, and discuss why traditional WSD models that are not deeply integrated within the SMT model tend, surprisingly, to fail. In contrast, we will show how a deeply embedded phrase sense disambiguation (PSD) approach succeeds where traditional WSD does not. We will then turn to semantic role labeling, and discuss the challenges of early approaches of applying SRL models to SMT. Finally, on semantic MT evaluation, we will explore some very new human and semi-automatic metrics based on semantic frame agreement. We show that by keeping the metrics deeply grounded within the theoretical framework of semantic frames, the new HMEANT and MEANT metrics can significantly outperform even the state-of-the-art expensive HTER and TER metrics, while at the same time maintaining the desirable characteristics of simplicity, inexpensiveness, and representational transparency.
In this tutorial, we cover techniques that facilitate the integration of Machine Translation (MT) and Translation Memory (TM), which can help the adoption of MT technology in localisation industry. The tutorial covers four parts: i) brief introduction of MT and TM systems, ii) MT confidence estimation measures tailored for the TM environment, iii) segment-level MT and MT integration, iv) sub-segment level MT and TM integration, and v) human evaluation of MT and TM integration. We will first briefly describe and compare how translations are generated in MT and TM systems, and suggest possible avenues to combines these two systems. We will also cover current quality / cost estimation measures applied in MT and TM systems, such as the fuzzy-match score in the TM, and the evaluation/confidence metrics used to judge MT outputs. We then move on to introduce the recent developments in the field of MT confidence estimation tailored towards predicting post-editing efforts. We will especially focus on the confidence metrics proposed by Specia et al., which is shown to have high correlation with human preference, as well as post-editing time. For segment-level MT and TM integration, we present translation recommendation and translation re-ranking models, where the integration happens at the 1-best or the N-best level, respectively. Given an input to be translated, MT-TM recommendation compares the output from the MT and the TM systems, and presents the better one to the post-editor. MT-TM re-ranking, on the other hand, combines k-best lists from both systems, and generates a new list according to estimated post-editing effort. We observe high precision of these models in automatic and human evaluations, indicating that they can be integrated into TM environments without the risk of deteriorating the quality of the post-editing candidate. For sub-segment level MT and TM integration, we try to reuse high quality TM chunks to improve the quality of MT systems. We can also predict whether phrase pairs derived from fuzzy matches should be used to constrain the translation of an input segment. Using a series of linguistically- motivated features, our constraints lead both to more consistent translation output, and to improved translation quality, as is measured by automatic evaluation scores. Finally, we present several methodologies that can be used to track post-editing effort, perform human evaluation of MT-TM integration, or help translators to access MT outputs in a TM environment.
This half-day tutorial provides a broad overview of how to evaluate translations that are produced by machine translation systems. The range of issues covered includes a broad survey of both human evaluation measures and commonly-used automated metrics, and a review of how these are used for various types of evaluation tasks, such as assessing the translation quality of MT-translated sentences, comparing the performance of alternative MT systems, or measuring the productivity gains of incorporating MT into translation workflows.
Localization is a term mainly used in the software industry to designate the adaptation of products to meet local market needs. At the center of this process lies the translation of the most visible part of the product {--} the user interface {--} and the product documentation. Not surprisingly, the localization industry has therefore long been an extensive consumer of translation technology and a key contributor to its progress. Software products are typically released in recurrent cycles, with large amounts of content remaining unchanged or undergoing only minor modifications from one release to the next. In addition, software development cycles are short, forcing translation to start while the product is still undergoing changes, so that localized products can reach global markets in a timely fashion. These two aspects result in a heavy dependency on the efficient handling of translation updates. It is only natural that the software industry turned to software-based productivity tools to automate the recycling of translations (through translation memories) and to support the management of the translation workflow (through translation management systems). Machine translation is a relatively recent addition to the localization technology mix, and not yet as widely adopted as one would expect. Its initial use in the software industry was for more accessory content which is otherwise often left untranslated, e.g. product support articles and antivirus alerts with their short lifecycle. The expectation had however always been that MT could one day be deployed on the bulk of user interface and product documentation, due to the expected process efficiencies and cost savings. While MT is generally still not considered {``}good{''} enough to be used raw on this type of content, it has now become an integral part of translation productivity environments, thereby transforming translators into post-editors. The tutorial will provide an overview of current localization practices and challenges, with a special focus on the role of translation memory and translation management technologies. As a use case of the integration of MT in such an environment, we will then present the approach taken by Autodesk with its large set of Moses engines trained on custom data. Finally, we will explore typical scenarios in which machine translation is employed in the localization industry, using practical examples and data gathered in different productivity and usability tests.
In Linux system environment, it is very common and convenient to use the word alignment generated from GIZA++ for most statistical machine translation (SMT) systems. While it is not the story for many researchers used to conducting their research under the Windows platform. Although either Cygwin or Virtual Machine (VM) can be used to emulate Linux environment in Windows environment, it is not always a good method. In this paper, we mainly want to share our experiences on how to use GIZA++ on Windows. 
Movies and TV shows are probably the most attractive media of language learning, and the associated subtitle is an important resource in the learning process. Despite its significance, subtitle has never been exploited effectively as it could be. In this paper we present ENGtube, which is a video service for ESL (English as Second Language) learners. The key component of this service is an integrated environment for displaying the video clips, the source subtitle and the translated subtitle with rich information at users’ disposal. The rich information of subtitle is produced by various speech and language technologies. 
We present S2TT, an integrated speech-totext translation system based on POCKETSPHINX and MOSES. It is compared to different baselines based on ANTS — the broadcast news transcription system developed at LORIA’s Speech group, MOSES and Google’s translation tools. A small corpus of reference transcriptions of broadcast news from the evaluation campaign ESTER2 was translated by human experts for evaluation. The Word Error Rate (WER) of the recognition stage of both systems are evaluated, and BLEU is used to score the translations. Furthermore, the reference transcriptions are automatically translated using MOSES and GOOGLE in order to evaluate the impact of recognition errors on translation quality. Index Terms: speech-to-text translation, speech recognition 
The Moses open source machine translation system is a powerful research tool. However, for doing machine translation at a production level Moses remains a work in progress. Adobe Systems Inc. has been working on completing a set of lightweight AIR1 based tools to address these limitations. We have created a set of tools to improve corpus production, to speed up automatic training, and to carry out automatic scoring of Moses driven MT projects. In the future these three tools will be the building blocks for producing a fully integrated and automatic Moses based machine translation system. 
 2010 and should achieve its goals till September 2012.  To fully exploit the huge potential of existing open SMT technologies and user-provided content, we have created an innovative online platform for data sharing and MT building. This platform is being developed in the EU collaboration project LetsMT!. This paper presents motivation in developing this platform, its architecture and main features. 
shows clients like Apple, eBay, HP, and translation companies how to develop global content and to optimize translation processes. As Director of Linguistics at Spoken Translation, he also leads the development of an interactive speech-to-speech MT system for healthcare. He is Past President current Vice President of AMTA and on the advisory board of several startups. Dr. Dillinger started in MT by studying with early pioneers Paul Garvin (chief linguist on the Georgetown MT project) and David Hays (director of the MT project at the Rand Corporation and a member of the ALPAC Commission), then worked with Hiroshi Uchida (creator of the Atlas System at Fujitsu) and Bud Scott (founder of Logos Corporation). He worked on interlingual MT in the multinational UNL Project and on commercial rule-based MT as Director of Linguistics at Logos Corporation and GlobalWords Technologies. He wrote the widely circulated LISA Best Practices Guide: Implementing Machine Translation, published a wide range of articles about linguistics, semantics, corpus linguistics, and machine translation, contributed to the emerging standards OLIF and UNL, and was awarded two patents for translation technology. Dr. Dillinger has taught at more than a dozen universities in several countries and has been a visiting researcher on four continents.  Title: MT everywhere: Next Steps 
In SMT, the instability of MERT, the commonly used optimizer, is an acknowledged problem. This paper presents two methods for smoothing the MERT instability. Both exploit a set of different realizations of the same system obtained by running the optimization stage multiple times. One method averages the sets of different optimal weights; the other combines the translations generated by the various realizations. Experiments conducted on two different sized tasks involving four different language pairs show that both methods are effective in smoothing instability, but also that the average system well competes with the more expensive system combination. 
The Cunei Machine Translation Platform is an open-source MT system designed to model instances of translation. One of the challenges to this approach is effective training. We describe two techniques that improve the training procedure and allow us to leverage the strengths of instance-based modeling. First, during training we approximate our model with a second-order Taylor series. Second, we discount models based on the magnitude of their approximation. By reducing error in training, our model now consistently outperforms the standard SMT model with gains ranging from 0.51 to 3.77 BLEU on GermanEnglish and Czech-English test sets. 
We propose Maximum Ranking Correlation (MRC) as an objective function in discriminative tuning of parameters in a linear model of Statistical Machine Translation (SMT). We try to maximize the ranking correlation between sentence level BLEU (SBLEU) scores and model scores of the N-best list, while the MERT paradigm focuses on the potential 1best candidates of the N-best list. After we optimize the MER and the MRC objectives using an multiple objective optimization algorithm at the same time, we interpolate them to obtain parameters which outperform both. Experimental results on WMT French–English data set conﬁrm that our method signiﬁcantly outperforms MERT on out-of-domain data sets, and performs marginally better than MERT on in-domain data sets, which validates the usefulness of MRC on both domain speciﬁc and general domain data. 
Part-of-speech tagging is a crucial preprocessing step for machine translation. Ambiguity in the natural language processing has made POS tagging hard. And particles are a major cause of ambiguity. But current studies have limited particles in narrow sense. Therefore, this study presents an English POS tagger basically addressing the tagging of particles in broad sense. A definition of particles in broad sense is given, a small size of 998k English annotated corpus in business domain is built, the maximum entropy model is adopted and rulebased approach is used in post-processing. Experiments show that our tagger achieves an F-score of 90.87% in closed test and 87.24% in open test, which is a quite satisfactory result. 
This paper presents a novel method for multistage dependency parsing based on dependency direction. In the method, dependency parsing processes are divided into multiple sub-stages, and each stage is in a sequential pattern, which makes it easier to take applicable solutions for different issues in dependency parsing. Meanwhile, dependency parsing in the previous stage provides a clearer context for next stage. Furthermore, due to the dependency direction, the proposed method has lower search complexity than that of classic graph-based methods. Experimental results show that compared with common methods, the proposed method in this paper offers comparable accuracy and higher efficiency. 
Spurious words usually have no counterpart in other languages, and are therefore a headache in machine translation. In this paper, we propose a novel framework, skeleton-enhanced translation, in which a conventional SMT decoder can boost itself by considering the skeleton of the source input and the translation of such skeleton. By the skeleton of a sentence it is meant the sentence with its spurious words removed. We will introduce two models for identifying spurious words: one is a context-insensitive model, which removes all tokens of certain words; another is a context-sensitive model, which makes separate decision for each word token. We will also elaborate two methods to improve a translation decoder using skeleton translation: one is skeleton-enhanced re-ranking, which re-ranks the n-best output of a conventional SMT decoder with respect to a translated skeleton; another is skeleton-enhanced decoding, which re-ranks the translation hypotheses of not only the entire sentence but any span of the sentence. Our experiments show significant improvement (1.6 BLEU) over the state-of-the-art SMT performance. 
This paper explores a tight coupling of Automatic Speech Recognition (ASR) and Machine Translation (MT) for speech translation with information sharing on the phonelevel. Our novel approach allows MT to access ﬁne-grained phonetic information from ASR, as a methodology for facilitating speech translation. Speciﬁcally, Phrase-based Statistical MT (PBSMT) models are adapted to work on source language phones, and with a conﬁguration of a source-language phoneme-to-grapheme component, sourcelanguage phones are translated into targetlanguage words. Furthermore, to take advantage of source-side phonetic confusion information from the speech recogniser, phone confusion networks are constructed from the phonetic confusion matrix and are used as SMT inputs to boost translation quality. Experiments are carried out on IWSLT English– Chinese translation task, and signiﬁcant improvements (1.27 absolute and 4.29% relatively BLEU points) are obtained by using phone confusion networks over the baseline PBSMT system. 
In this work, we investigate methods to automatically adapt our simultaneous lecture translation systems to the diverse topics that occur in educational lectures. Utilizing materials that are available before the lecture begins, such as lecture slides, our proposed framework iteratively searches for related documents on the World Wide Web and generates lecture-speciﬁc models and vocabularies based on the resulting documents. In this paper, we propose a novel method for vocabulary selection, a critical aspect of simultaneous translation systems where the occurrence of out-of-vocabulary words signiﬁcantly degrades intelligibility. We propose a novel approach based on feature-based ranking and evaluate the effectiveness of 21 different features and their combinations for this task. On the interACT German-English simultaneous lecture translation system our proposed approach signiﬁcantly improved vocabulary coverage, reducing out-of-vocabulary rate, on average by 60% and up to 84%, compared to a lecture-independent baseline. Furthermore, a 40k vocabulary selected using our method obtained better coverage than a lecture-independent 300k vocabulary, improving intelligibility and reducing the latency of the end-to-end system. 
Context plays a critical role in the understanding of language, especially conversational speech. However, few approaches exist to utilize the external contextual knowledge which is readily available to practical speech translation systems deployed in the ﬁeld. In this work, we propose a novel framework to integrate context in the language models used for conversational speech translation. The proposed approach takes into account the contextual distance between a test utterance and the training corpus, a measure obtained from the external context in which the utterances were spoken. Language model probabilities are adjusted through a sentence-level weighting scheme based on this context-distance measure. When incorporated into our English-Iraqi Arabic speech-to-speech translation system, the proposed approach obtains improvements in both speech recognition accuracy and translation quality compared to the baseline system. 
We investigate two problems in word alignment for machine translation. First, we compare methods for incremental word alignment to save time for large-scale machine translation systems. Various methods of using existing word alignment models trained on a larger, general corpus for incrementally aligning smaller new corpora are compared. In addition, by training separate translation tables, we eliminate the need for any re-processing of the baseline data. Experimental results are comparable or even superior to the baseline batch-mode training. Based on this success, we explore the possibility of sharpening alignment model via incremental training scheme. By first training a general word alignment model on the whole corpus and then dividing the same corpus into domainspecific partitions, followed by applying incremental training to each partition, we can improve machine translation quality as measured by BLEU. 
We propose a ﬂexible and efﬁcient domain adaptation method that yields consistent improvements in machine translation (for 11 language pairs). The idea is to decompose the word alignment process into two steps, model training and alignment inference, and perform Bayesian adaptation on the latter. This modularity allows one to incorporate out-of-domain data without the need to modify existing training algorithms. We show how ideas in sequential Bayesian methods can be naturally applied to the word alignment problem and demonstrate various positive results on EMEA and NIST datasets.  parliament bitext, and our target in-domain application is the medical domain. The in-domain bitext may be sufﬁcient to build a reasonable medical translation system; however, our goal is to further improve upon this in-domain baseline using the large out-of-domain bitext. We will focus our attention on translation model adaptation, and use standard methods for language model adaptation.1 To begin addressing this problem, we divide the training pipeline for building a translation model into the following four steps: • Step 1: Word alignment model training: Given bitext aligned at the sentence level, train a word alignment model.  
This paper investigates active learning to improve statistical machine translation (SMT) for low-resource language pairs, i.e., when there is very little pre-existing parallel text. Since generating additional parallel text to train SMT may be costly, active sampling selects the sentences from a monolingual corpus which if translated would have maximal positive impact in training SMT models. We investigate different strategies such as density and diversity preferences as well as multistrategy methods such as modiﬁed version of DUAL and our new ensemble approach GraDUAL. These result in signiﬁcant BLEU-score improvements over strong baselines when parallel training data is scarce. 
Translation consistency is an important issue in document-level translation. However, the consistency in Machine Translation (MT) output is generally overlooked in most MT systems due to the lack of the use of document contexts. To address this issue, we present a simple and effective approach that incorporates document contexts into an existing Statistical Machine Translation (SMT) system for document-level translation. Experimental results show that our approach effectively reduces the errors caused by inconsistent translations (25% error reduction). More interestingly, it is observed that as a “bonus” our approach is able to improve the BLEU score of the SMT system. 
Function words play an important role in sentence structures and express grammatical relationships with other words. Most statistical machine translation (SMT) systems do not pay enough attention to translations of function words which are noisy due to data sparseness and word alignment errors. In this paper, a novel method is designed to separate the generation of target function words from target content words in SMT decoding. With this method, the target function words are deleted before the translation modeling while in SMT decoding they are inserted back into the translations. To guide the target function words insertion, a new statistical model is proposed and integrated into the log-linear model for SMT, which can lead to better reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 
This paper explores a new approach to help non-expert users with no background in linguistics to add new words to a monolingual dictionary in a rule-based machine translation system. Our method aims at obtaining the correct paradigm which explains not only the particular surface form introduced by the user, but also the rest of inﬂected forms of the word. An initial set of potential paradigms is automatically obtained and then interactively reﬁned by the user with a novel graphical interface through active machine learning. We show the promising results of experiments performed with a Spanish monolingual dictionary. 
In the computing assisted translation process with machine translation (MT), postediting costs time and efforts on the part of human. To solve this problem, some have attempted to automate post editing. Post-editing isn’t always necessary, however, when MT outputs are of adequate quality for human. This means that we need to be able to estimate the translation quality of each translated sentence to determine whether post-editing should be performed. While conventional automatic metrics such as BLEU, NIST and METEOR, require the golden standards (references), for wider applications we need to establish methods that can estimate the quality of translations without references. This paper presents a sentence-level automatic quality evaluator, composed of an SMT phrase-based automatic post-editing (APE) module and a confidence estimator characterized by PLS (Partial Least Squares) regression analysis. It is known that this model is a better model for predicting output variable than a normal multiple regression analysis when the multicollinearity exists between the input variables. Experiments with Japanese to English patent translations show the validity of the proposed methods. 
In the context of massive adoption of Machine Translation (MT) by human localization services in Post-Editing (PE) workﬂows, we analyze the activity of post-editing high quality translations through a novel PE analysis methodology. We deﬁne and introduce a new unit for evaluating post-editing effort based on Post-Editing Action (PEA) - for which we provide human evaluation guidelines and propose a process to automatically evaluate these PEAs. We applied this methodology on data sets from two technologically different MT systems. In that context, we could show that more than 35% of the remaining effort can be saved by introducing of global PEA and edit propagation. 
This paper explores the use of machine translation (MT) to help users of computer-aided translation systems based on translation memory to identify the target words in the translation proposals that need to be changed or kept unedited. MT is used as a black box to obtain a set of features for each target word in the translation proposals and then used by a binary classiﬁer to determine the target words to change or keep unedited. Experiments conducted in the translation of Spanish texts into English with different corpora shows an accuracy above 96% for fuzzy-match scores above 70%. 
Translation sub-model is one of the most important components in statistical machine translation, but the conventional approach suffers from two major problems. Firstly, translation sub-model is not optimized with respect to any of automatic evaluation metrics of SMT (such as BLEU). The second problem is over-fitting to training data. This paper presents a new unified framework, by adding a scalable translation sub-model into the conventional framework. The sub-model is optimized with the same criterion as the translation output is evaluated (BLEU), and trained using margin infused relaxed algorithm (MIRA) to handle over-fitting. Under our new framework, MIRA and minimum error rate training (MERT) are unified into an interactive training process. Our approach has not only shown to improve performance over a state-of-the-art baseline, but also generalize well in-domain training data to out-of-domain test data. 
 ≒ᕩ  ≒ᕩ  One of the major reasons for translation errors in phrase-based SMT systems is the incorrect phrases induced from inaccuracy word-aligned parallel data. In this paper, we propose a novel approach that uses the minimum Bayes-risk (MBR) principle to improve the accuracy of phrase extraction. Our approach performs as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the link-pruned parallel data and used in SMT decoding. We evaluate our approach on the NIST Chinese-to-English MT tasks, and show significant improvements on parallel data sets of different scales. 
In this paper, we propose a phrase segmentation model for the phrase-based statistical machine translation. We observed that good translation candidates generated by a conventional phrase-based SMT decoder have lexical cohesion and show more uniform translation for each phrase segment. Based on the observation, we propose a novel phrase segmentation model using collocation between two adjacent words and translation entropy of phrase segments. Experimental results show that the proposed model signiﬁcantly improves the translation quality in both English-to-Korean and English-to-Chinese translation tasks. 
We explore a novel approach to automatically predict noun number in Chinese by using a word-aligned Chinese-English parallel corpus. We ﬁrst map number information from English onto Chinese to create a dataset labeled with a POS tagset enhanced with number information, and then train a model to automatically predict noun number using a combination of lexical and syntactic features. We evaluate the quality of the automatically mapped data and show the mapping is largely adequate despite a small percentage of errors. Trained on a relatively small data set, our model achieves a 4% improvement in absolute accuracy over a majority baseline that considers all nouns to be singular. 
Preprocessing of the parallel corpus plays an important role in improving the performance of a phrase-based statistical machine translation (PB-SMT). In this paper, we propose a frame work in which predefined information of Multiword Expressions (MWEs) can boost the performance of PB-SMT. We preprocess the parallel corpus to identify Noun-noun MWEs, reduplicated phrases, complex predicates and phrasal prepositions. Singletokenization of Noun-noun MWEs, phrasal preposition (source side only) and reduplicated phrases (target side only) provide significant gains over our previous best PBSMT model. Automatic alignment of complex predicates substantially improves the overall MT performance and the word alignment quality as well. For establishing NE alignments, we transliterate source NEs into the target language and then compare them with the target NEs. Target language NEs are first converted into a canonical form before the comparison takes place. The proposed system achieves significant improvements (6.38 BLEU points absolute, 73% relative improvement) over the baseline system on an English- Bengali translation task. 
This paper presents AMEANA, an opensource tool for error analysis for natural language processing tasks targeting morphologically rich languages. Unlike standard evaluation metrics such as BLEU or WER, AMEANA automatically provides a detailed error analysis that can help researchers and developers better understand the strengths and weaknesses of their systems. AMEANA is easily adaptable to any language provided the existence of a morphological analyzer. In this paper, we focus on usability in the context of Machine Translation (MT) and demonstrate it speciﬁcally for English-to-Arabic MT. 
Adobe Systems has employed Machine Translation as part of the document localization process for over two years. In order to encourage the wider adoption of the technology within the company, we have recently created a unified API across our available MT technologies. This unified MT service enables simpler integration of MT within products and processes, allows sharing of license and server costs across the company, creates a platform for mixing technologies into a best-of-breed solution, and provides greater sharing of expertise and best practices. 
This user report demonstrates in detail the work involved in deploying MT into a typical enterprise localisation workflow. It analyzes the challenges that MT brings, taking the workflow of Symantec (a software company) as an example. This paper reports that the traditional workflow (i.e. a localisation process from English into Chinese) can be changed dramatically. Although localisation cost can be reduced, extra effort and skills are required from both internal content localisers and localisation vendors. When planning the deployment of an MT system into a workflow, both challenges and benefits of MT should be examined and be taken into consideration by a company. 
This paper presents the evaluation results of a study conducted to determine the ability of various Machine Translation systems in translating User-Generated Content, particularly online forum content. Four systems are compared in this paper, focusing on the English>German and English>French language pairs, including a system called VICTOR, which is based on the Moses and IRSTLM toolkits. After describing some of the characteristics of these systems, the methodological framework used during a medium scale evaluation campaign is described. A careful analysis of both human and automated scores show that one system is overall significantly better than the other three systems for the English>German language pair, but that very little difference exists for specific post types (such as questions and solutions). The results are also much more balanced for the English>French language pair, suggesting that all systems could be useful in a multi-system deployment scenario. Our results also show that human scores and automated scores do not consistently correlate, penalizing certain systems more than others. Finally, we also show that the quality and coverage of the source posts impacts systems and language pairs differently. 
In the last decade, there have been a countless number of researches in soft syntactic features many of which have led to the improved performance for Hiero. However, it seems that all the syntactic constituent features cannot efficiently work together in the Hiero optimized by MERT. In this paper, we propose a more general soft syntactic constraint model based on discriminative classifiers for each constituent type and integrate all of them into the translation model with a unified form. The experimental results show that our method significantly improves the performance on the NIST05 Chinese-toEnglish translation task. 
Tree-to-tree translation model is widely studied in statistical machine translation (SMT) and is believed to be much potential to achieve promising translation quality. However, the existing models still suffer from the unsatisfactory performance due to the limitations both in rule extraction and decoding procedure. According to our analysis and experiments, we have found that tree-to-tree model is severely hampered by several rigid syntactic constraints: the both-side subtree constraint in rule extraction, the node constraint and the exact matching constraint in decoding. In this paper we propose two simple but effective approaches to overcome the constraints: utilizing fuzzy matching and category translating to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 
 State of the art phrase-based statistical  machine translation systems typically  contain two features which estimate  the “forward” and “backward”  conditional translation probabilities for  a given pair of source and target  phrase. These two  “relative  frequency” (RF) features are derived  from three counts: the joint count of  the source and target phrase and their  marginal counts. We propose to “un-  pack” these three statistics, making  them independent “3-count” features  instead of two RF features. In our ex-  periments, the 3-count features per-  form better than the RF ones in three  of four systems we tested. By trans-  forming and generalizing these 3-count  features slightly, further improvements  are obtained. Furthermore, under sev-  eral different experimental conditions,  we compare 3-count and generalized  3-count features to new features de-  rived from Kneser-Ney smoothing, to  a new low-frequency penalty feature,  and to several known smooth-  ing/discounting schemes. Generalized  3-count performs similarly to or better  than all of the smoothing methods ex-  cept modified Kneser-Ney. In our ex-  periments, the best phrase table (not  language model) smoothing yields  +0.6-1.4 BLEU.  
Adaptation of statistical machine translation (SMT) systems from generic to specific domains is challenging due to the lack of training data. In this paper we propose a framework for domain adaptation by exploiting a large monolingual in-domain corpus. We identify the significant patterns to capture the domain specific writing styles. The patterns are then translated with the involvements of domain experts. The major issue of our framework is to reduce the cost of the experts and better allocate their efforts. The experimental results show the proposed methods are effective, in terms of the significance and diversity of the patterns. The approaches to integrate the mined patterns into background SMT are also discussed. 
This paper reports experiments on adapting components of a Statistical Machine Translation (SMT) system for the task of translating online user-generated forum data from Symantec. Such data is monolingual, and differs from available bitext MT training resources in a number of important respects. For this reason, adaptation techniques are important to achieve optimal results. We investigate the use of mixture modelling to adapt our models for this speciﬁc task. Individual models, created from different in-domain and out-of-domain data sources, are combined using linear and log-linear weighting methods for the different components of an SMT system. The results show a more profound effect of language model adaptation over translation model adaptation with respect to translation quality. Surprisingly, linear combination outperforms log-linear combination of the models. The best adapted systems provide a statistically signiﬁcant improvement of 1.78 absolute BLEU points (6.85% relative) and 2.73 absolute BLEU points (8.05% relative) over the baseline system for English–German and English–French, respectively. 
Domain adaptation plays an important role in multi-domain SMT. Conventional approaches usually resort to statistical classifiers, but they require annotated monolingual data in different domains, which may not be available in some cases. We instead propose a simple but effective bagging-based approach without using any annotated data. Large-scale experiments show that our new method improves translation quality significantly over a hierarchical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 
The processing of many natural languages suffers from scarce linguistic resources. We introduce the idea of compatibility to extend training data for machine translation: If translation hypotheses by multiple systems are measured as compatible, they are considered as reliable predictions. By this way, we generate virtual parallel data per bridge language, and re-compiling on this corpus improves our machine translation quality by more than 30% relatively. 
Statistical post-editing (SPE) techniques have been successfully applied to the output of Rule Based MT (RBMT) systems. In this paper we investigate the impact of SPE on a standard Phrase-Based Statistical Machine Translation (PB-SMT) system, using PB-SMT both for the ﬁrst-stage MT and the second stage SPE system. Our results show that, while a naive approach to using SPE in a PB-SMT pipeline produces no or only modest improvements, a novel combination of source context modelling and thresholding can produce statistically signiﬁcant improvements of 2 BLEU points over baseline using technical translation data for French to English. 
In the ﬁeld of staistical machine translation (SMT), pre-ordering is a recently attractive approach that reorders source language words into the target language order prior to SMT decoding. It is effective for long-distance reordering in SMT, especially between languages with distant word ordering like English and Japanese. Its key idea is to decompose the SMT problem into two subproblems of translation and reordering and to solve them independently. However, most preordering approaches employ syntactic parsing in the source language and reordering rules depending on a certain target language. This paper focuses on the translation in the opposite direction and proposes post-ordering; foreign sentences are ﬁrst translated into foreign-ordered English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 
Translation memories (TMs) are very useful tools for translating texts in narrow domains. We propose the use of paraphrases for searching TMs. By using paraphrases, we can retrieve sentences that have the same meaning as the input sentences even if they do not match exactly. The paraphrase pairs used in our system are obtained from parallel corpora and are used to retrieve sentences in a statistical framework. 
This poster explores a way to qualitatively identify linguistic properties of a particular MT system, so that we can identify its strengths and weaknesses more readily. The paper provides preliminary results for two English-to-Japanese SMT systems. We demonstrate that comparison of n-gram frequencies between human translations and machine-translated outputs can lead us to linguistically meaningful information about a given MT system. We argue that our method has the potential to guide the research and development process in a way that numeric scores alone cannot and that it can shed new lights on how we assess MT quality. 
An example-based machine translation (EBMT) system based on analogies requires numerous analogies between linguistic units to work properly. Consequently, long sentences cannot be handled directly in such a framework. In this paper, we inspect the quality of translation of chunks obtained by marker-based chunking in English and French in both directions. Our results show that more than three quarters of the chunks can be translated by the one-step analogy-based translation method, and that a little bit less than half of the chunks has at least one translation that matches exactly with one of the references. 
Automatic bilingual term extraction is essential for providing a consistent bilingual term list for human translators engaged in translating a set of documents. We compare three statistical measures for extracting bilingual terms from a phrase-table built from a parallel corpus. We show that these measures extract different bilingual term candidates and a combination of these measures ranks valid bilingual terms highly. 
In this paper we present a non-languagespecific strategy that uses large amounts of monolingual data to improve statistical machine translation (SMT) when only a small parallel training corpus is available. This strategy uses word classes derived from monolingual text data to improve the word alignment quality, which generally deteriorates significantly because of insufficient training. We present a novel semantic word clustering algorithm to generate the word classes motivated by the word similarity metric presented in (Lin, 1998). Our clustering results showed this novel word clustering outperforms a state-of-the-art hierarchical clustering. We then designed a new procedure for using the derived word classes to improve word alignment quality. Our experiments showed that the use of the word classes can recover over 90% of the loss resulting from the alignment quality that is lost due to the limited parallel training. 
Lexical sparsity problem is much more serious for agglutinative language translation due to the multitude of inﬂected variants of lexicons. In this paper, we propose a novel optimization strategy to ease spareness by multi-granularity word alignment and translation for agglutinative language. Multiple alignment results are combined to catch the complementary information for alignments, and rules of different granularities can cooperate effectively to translate more unknown words. Experimental results on Uyghur-Chinese show that our proposed method signiﬁcantly improves the quality of word alignment and translation, by relative 10.25% of alignment error rate reduction and +2.46% BLEU increment, respectively. 
The classic Hierarchical Phrase-Based Translation Model suffers from 3 defects: 1) an overly large model with a lot of useless or even wrong translation rules; 2) a large search space leading to combinatorial explosion; 3) since there is only one variable, decoder can only choose hierarchical rules through lexical information. This paper presents strategies to mitigate these problems, including rule extraction filtering, decoding optimization and variable refinement. The experiments show that the proposed methods can not only speed the decoding process but also improve the translation quality. 
Reordering is a critical step for statistical machine translation. The hierarchical phrasebased model (HPB) based on a synchronous context-free grammar (SCFG) is prominent in solving global reorderings. However, the model is inadequate to supervise the reordering process so that sometimes phrases or words are reordered due to the wrong choice of translation rule. In order to solve this problem, we propose a novel lexical-based reordering model to evaluate the correctness of word order for each translation rule. Our approach employs the word alignment and translation information during the decoding process without causing too much extra computational consumption. Experimental results on the Chinese-to-English task showed that our method outperformed the baseline system in BLEU score signiﬁcantly. Moreover, the translation results further proved the effectiveness of our approach. 
Hierarchical phrase-based (HPB) models have shown strong capability in generalization and reordering. However, they are heavily dependent on continuous phrases and are difficult for modeling natural linguistic discontinuities directly. In this paper, we propose a novel approach for integrating discontinuous phrases into the Chinese-to-English HPB system. We focus on the extraction method of discontinuous phrases which retrieves various linguistic information missed in the HPB model, such as set phrases and long-distance reordering of adverbials, etc. After being transformed into the similar form to HPB rules, the translation rules with discontinuities can be seamlessly integrated into the CKY decoder. Experimental results show that the proposed approach for incorporating the linguistic discontinuities achieves statistically significant improvements over the traditional HPB system. 1. Introduction Hierarchical phrase-based (HPB) translation has emerged as one of the dominant current approaches to statistical machine translation (SMT), which combines the ideas of syntax-based translation and phrase-based translation. Based on the binary synchronous context-free grammar (2-SCFG), the HPB system (Chiang, 2005) has better generalization capability and can capture long distance reordering. However, due to the limitation of phrasal continuity, HPB model cannot extract the frequent patterns in Chinese-to-English translation, such as “а…ቡėas soon as”(Figure 1(a)), where words in the source(Chinese) phrases may be separated by gaps. Inaddition,  Bo Xu Digital Media Content Technology Research Center, Institute of Automation, Chinese Academic of Sciences. xubo@hitic.ia.ac.cn recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and Wu, 2009) has questioned the empirical adequacy of 2-SCFG systems, which are unable to generate translation units with certain types of alignments independently. Galley and Manning (2010) pointed out that discontinuous phrases can account for these missed patterns and proposed a generalization of conventional phrase-based decoding to handle discontinuities in both source and target phrases, which yields significant improvements over Joshua(Li et al., 2009)1, a state-of-the-art HPB system. Some other attempts to exploit phrasal discontinuities are also based on phrase-based SMT. Simard et al. (2005) presented an extension to Moses that allows one-word gaps in source and target phrases. This makes decoding simpler, but fixed-size discontinuous phrases are less general and will increase data sparseness. Cancedda et al. (2007) extended Simard’s work by using flexible phrases that may contain gaps of variable lengths. It should be noted that these attempts with discontinuous phrases were mainly carried out using the left-right SMT decoder which is ineffective in allowing phrasal discontinuities. Although they tried to extend the linear decoding to support phrases with gaps, the linguistic patterns within the hierarchical structures of discontinuous phrases are difficult to be utilized fully. Additionally, He and Zong (2008) proposed a generalized reordering model for phrase-based SMT which developed a CKY style decoder to combine continuous and discontinuous phrases. However, phrasal discontinuities are only used to improve the generalization capability in their phrase-based SMT while other benefits such as long-distance reordering are not fully exploited. In this paper, we propose a novel method to improve 2-SCFG through integrating 
The processing of many natural languages suffers from scarce linguistic resources. We introduce the idea of compatibility to extend training data for machine translation: If translation hypotheses by multiple systems are measured as compatible, they are considered as reliable predictions. By this way, we generate virtual parallel data per bridge language, and re-compiling on this corpus improves our machine translation quality by more than 30% relatively. 
Filtering noisy parallel corpora or removing mistranslations out of training sets can improve the quality of a statistical machine translation. Discriminative methods for ﬁltering the corpora such as a maximum entropy model, need properly labeled training data, which are usually unavailable. Generating all possible sentence pairs (the Cartesian product) to generate labeled data, produces an imbalanced training set, containing a few correct translations and thus inappropriate for training a classiﬁer. In order to treat this problem effectively, unsupervised methods are utilized and the problem is modeled as an outlier detection procedure. The experiments show that a ﬁltered corpus, results in an improved translation quality, even with some sentence pairs removed. 
The Web is an invaluable source of parallel data, but in recent years it has become polluted with increasing amounts of machine-translated content. Using such data to train an MT system can introduce error and decrease the resulting quality of the system. In this paper, we present an algorithm for filtering machine-translated content from Webscraped parallel corpora, and discuss its application in cleaning such corpora for use in training statistical machine translation systems. We demonstrate that our algorithm is capable of identifying machine-translated content in parallel corpora for a variety of language pairs, and that in some cases it can be very effective in improving the quality of an MT system. Trained on our filtered corpus, our most successful MT system outperformed one trained on the full, unfiltered corpus, thus challenging the conventional wisdom in natural language processing that “more data is better data”1. 
We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars, linear inversion transduction grammars and preterminalized linear inversion transduction grammars. While empirical results such as those in previous work are of course an ultimate test of modeling adequacy for machine translation applications, it is equally important to understand the formal theoretical properties of any such new representation. An important part of the expressivity of a transduction is the possibility to align tokens between the two languages generated. We refer to the number of different alignments that are allowed under a transduction as its weak alignment capacity. This aspect of expressivity is quantiﬁed for linear transductions using preterminalized linear inversion transduction grammars, and compared to the expressivity of ﬁnite-state transductions, inversion transductions and syntax-directed transductions. 
The advancements of mobile technology permit handheld devices to be smaller, versatile, and have more processing power. On the other hand, the development of complex applications which require more processing capabilities are being developed rapidly nowadays. The implementation of Machine Translation (MT) systems with high translation quality is always considered difficult in desktop devices. In order to understand the languages deeply, large amounts of knowledge and processing capabilities are always required to guarantee the translation quality. This turns out that it is even more challenging for handheld devices. As a result, this paper introduces the application of MT based on Constraint Synchronous Grammar (CSG) in devices with limited resources. Since CSG describes syntactic structures of two languages simultaneously based on feature constraints, the analysis and the generation of the translation can be done at one stage to lower the process complexity. 
Machine translation of patent documents is very important from a practical point of view. One of the key technologies for improving machine translation quality is the utilization of syntax. It is difﬁcult to select the appropriate parser for patent translation because the effects of each parser on patent translation are not clear. This paper provides comparative evaluation of several state-of-the-art parsers for English, focusing on the effects for patent machine translation from English to Japanese. We measured how much each parser contributed to improve translation quality when the parser was used to obtain the syntax of input sentences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 
We improve translation memory (TM)inspired consistent phrase-based statistical machine translation (PB-SMT) using rich linguistic information including lexical, part-of-speech, dependency, and semantic role features to predict whether a TM-derived sub-segment should constrain PB-SMT translation. Besides better translation consistency, for English-to-Chinese Symantec TMs we report a 1.01 BLEU point improvement over a regular state-of-the-art PB-SMT system, and a 0.45 BLEU point improvement over a TM-constrained PB-SMT system without access to rich linguistic information, both statistically signiﬁcant (p < 0.01). We analyze the system output and summarize the beneﬁts of using linguistic annotations to characterise the nature of translation consistency. 
We describe a method to detect common Chinese characters between Japanese and Chinese automatically by means of freely available resources and verify the effectiveness of the detecting method. We use a joint phrase alignment model on dependency trees and report results of experiments aimed at improving the alignment quality between Japanese and Chinese by incorporating the common Chinese characters information detected by proposed detecting method into the alignment model. Experimental results of JapaneseChinese phrase alignment show that our approach could achieve 0.73 points lower AER than the baseline system. 
This paper describes the evaluation campaign of the MEDAR project for English-to-Arabic (EnAr) MT systems. The campaign aimed at establishing some basic facts about the state of the art for MT on EnAr, collecting enough data to better train and tune systems and assessing the improvements made. The paper details the data used and their formats, the evaluation methodology and the results obtained by the systems. We conclude by giving some recommendations on MT evaluation for EnAr direction in terms of technology and resources. 
Example-Based Machine Translation (EBMT) for low resource language, like Bengali, has low-coverage issues, due to the lack of parallel corpus. In this paper, we propose an EBMT for low resource language, using chunk-string templates (CSTs) and translating unknown words. CSTs consist of a chunk in source-language, a string in target-language, and word alignment information. CSTs are prepared automatically from aligned parallel corpus and WordNet. To translate unknown words, we used WordNet hypernym tree and English-Bengali dictionary. If no translation candidate found, system transliterates the word. Proposed EBMT improved wide- coverage by 41 points and quality by 48.81 points in human evaluation. 
Topic modeling is a popular framework to analyze large text collections. In the previous work, employing topic modeling into statistic machine translation mainly depends on one major topic of the test document. Different from the previous work, the proposed approaches will coverage not only major topic but also sub-topics. The basic idea of this paper is assumed that better translation quality, closer similarity of “topic-document” distributions between the target-side and the sourceside documents. We first give some initial experimental results to support this assumption. Then we transfer generating such a target document into selecting target-side sentences by an effective algorithm. A preliminary study showed that enforcing “topic-document” distributions to be consistent between target-side and source-side in SMT can potentially improve translation quality. 
As Machine Translation (MT) becomes more popular among end-users, an increasingly relevant issue is that of estimating the quality of automatic translations for a particular task. The main application for such quality estimates has been selecting good enough translations for human post-editing. The endusers, in this case, are ﬂuent speakers of both source and target languages and the quality estimates reﬂect post-editing effort, for example, the time to post-edit a sentence. This paper focuses on quality estimation to address the challenging problem of making MT more reliable to a different type of end-user: those who cannot read the source language. We propose a number of indicators contrasting the source and translation texts to predict the adequacy of such translations at the sentence-level. Experiments with Arabic-English MT show that these indicators can yield improvements over previous work using general quality indicators based on source complexity and target ﬂuency. 
This paper addresses the manual evaluation of Machine Translation (MT) quality by means of crowdsourcing. To this purpose, we replicated the ranking evaluation of the ArabicEnglish BTEC task proposed at the IWSLT 2010 Workshop by hiring non-experts through the CrowdFlower interface to Amazon’s Mechanical Turk. In particular, we investigated the effectiveness of “gold units” offered by CrowdFlower as the main quality control mechanism. The analysis of the collected data shows that agreement rates for non-experts are comparable to those obtained for experts, and that the crowd-based system ranking has a very strong correlation with expert-based ranking. Our results conﬁrm that crowdsourcing is an effective way to reduce the costs of MT evaluation without sacriﬁcing quality, and demonstrate that just exploiting the CrowdFlower control mechanism is enough to approximate expert-level data quality. 
This paper describes an approach to the diagnostic evaluation of machine translation (MT) based on linguistic checkpoints, which can provide valuable information both to the developers and to the end-users of MT systems. We present a flexible framework and a new tool, DELiC4MT, for fine-grained diagnostic MT evaluation which can be extended to any language pair and applied to any evaluation target, once the phenomena of interest are covered by the linguistic analysis. As a case study, we evaluate the CoSyne1 MT software against four leading web-based MT systems across a set of linguistic phenomena for three language pairs (from German, Italian and Dutch into English). 
Most metrics in use for automatic evaluation of machine translation in use equally weigh different matched words in candidate and reference translations, ignoring the fact that each word contributes a different amount of information to the meaning of a sentence. We experiment with ten measures of term informativeness for the purpose of examining their performance in rating the information loads of words in machine translated texts. The information theoretic measure information gain is found to bring about a nearly 12% improvement in correlation with human judgments of translation quality under an optimal setting. We also assess how various parameters may affect the performance of these measures, among which data size turns out to be the most influential factor. A dataset of around 80 documents, 700 segments, with 4 versions of reference translation, is found to offer the most desirable performance. 
In this paper, we introduce a novel translation system combination framework using a textto-text generation technique. We are motivated by the observation that for many translation sentences, some of their constituents may be poorly translated, while others may be well translated. Moreover, it is often the case that other systems can provide good alternatives to problematic constituents. In our approach, the system constructs paraphrase lattices representing all possible hypotheses for the same source sentence. We then use a text-to-text generator operating on those lattices to generate those hypotheses. We filter ungrammatical combinations using a feature-based lexicalized tree adjoining grammars (FB-LTAG) and then use a TER-based metric to compute a consensus score function to select the best translation among grammatical hypotheses. The system combination gains 1.38 BLEU points over the best individual system. 
This paper presents a machine translation architecture which hybridizes Matxin, a rulebased system, with regular phrase-based Statistical Machine Translation. In short, the hybrid translation process is guided by the rulebased engine and, before transference, a set of partial candidate translations provided by SMT subsystems is used to enrich the treebased representation. The ﬁnal hybrid translation is created by choosing the most probable combination among the available fragments with a statistical decoder in a monotonic way. We have applied the hybrid model to a pair of distant languages, Spanish and Basque, and according to our evaluation (both automatic and manual) the hybrid approach signiﬁcantly outperforms the best SMT system on out-ofdomain data. Keywords: hybrid MT models, RBMT, phrase-based SMT, Spanish-Basque MT 
In this paper, we extensively evaluate a new hybridisation approach consisting of enriching the phrase table of a phrase-based statistical machine translation system with bilingual phrase pairs matching transfer rules and dictionary entries from a shallow-transfer rulebased machine translation system. The experiments conducted show an improvement in translation quality, specially when the parallel corpus available for training is small or when translating out-of-domain texts that are well covered by the shallow-transfer rulebased machine translation system. 
 Tranditional n-best based training and decoding method of system combination can propogate the error because of imprecision parameter estimation and too early prunning. In order to alleviate the problem, the paper proposes hypergraph (HG) based three-pass training and three-pass decoding for different features. In order to construct HG, this paper introduces simplified bracket transduction grammar (SBTG) into HG based system combination. At three-pass decoding, decoder uses each-pass features to generate target translation. At threepass training, we introduce minimum risk (MR) with deterministic annealing (DA) into the training criterion in order to overcome overfitting, and furthermore compare two training procedures: minimum error training (MERT) on n-best and MR&DA on HG. The unified training and decoding approaches of HG based system combination outperform baseline using conventional Cube Prunning on Chinese-toEnglish benchmark corpus NIST08 test set. 
This paper investigates the factors that influence the translators’ post-editing efficiency in a collaborative translation environment. We developed GE-CCT, a platform that enables hundreds of translators to be organized to complete one large-scale translation task together. On the basis of the platform, this paper made thorough analysis on the post-editing logs of 325 translators within 8 weeks. Results show that some exterior environmental factors and interior personal factors obviously affect the translators’ post-editing speed and quality. Accordingly, we proposed solutions that improved the collaborative translation efficiency. 
We have formulated a dictionary/glossary format UTX 1.11 and released it in May 2011. UTX 1.11 is a simple format that is friendly to both computers and humans. UTX dictionaries can be used not only as machinereadable dictionaries for rule-based machine translation (MT) systems, but also for computer-aided translation by human translators. The initial objective of UTXSimple 1.00, released in 2008, was to improve the accuracy of various MT systems by specifying a common format. A key feature of its latest version UTX 1.11 is a term management mechanism by introducing four term statuses ("provisional," "forbidden," "approved," and "non-standard"). We show that a UTX 1.11-based dictionary originally created as a glossary is highly effective for improving the accuracy of MT. UTX can be widely and successfully applied in various fields with specialized terminology, such as localization, open source, education, administration, medicine, and law. 1. Previous Work A number of terminological formats have been created in the past, such as TBX (ISO 30042, Term-Base eXchange), OLIF (Open Lexicon Interchange Format, Lieske et al., 2001) 1 , and LMF (Lexical Markup Framework, Francopoulo et 
This paper presents corpus-based methods to find similarity between short text (sentences, paragraphs, ...) which has many applications in the field of NLP. Previous works on this problem have been based on supervised methods or have used external resources such as WordNet, British National Corpus etc. Our methods are focused on unsupervised corpus-based methods. We present a new method, based on Vector Space Model, to capture the contextual behavior, senses and correlation, of terms and show that this method performs better than the baseline method that uses vector based cosine similarity measure. The performance of existing document similarity measures, Dice and Resemblance, are also evaluated which in our knowledge have not been used for short text similarity. We also show that the performance of the vector-based baseline method is improved when using stems instead of words and using the candidate sentences for computing the parameters rather than some external resource.
Monolingual comparable corpora annotated with alignments between text segments (paragraphs, sentences, etc.) based on similarity are used in a wide range of natural language processing applications like plagiarism detection, information retrieval, summarization and so on. The drawback wanting to use them is that there aren{'}t many standard corpora which are aligned. Due to this drawback, the corpus is manually created, which is a time consuming and costly task. In this paper, we propose a method to significantly reduce the search space for manual alignment of the monolingual comparable corpus which in turn makes the alignment process faster and easier. This method can be used in making alignments on different levels of text segments. Using this method we create our own gold corpus aligned on the level of paragraph, which will be used for testing and building our algorithms for automatic alignment. We also present some experiments for the reduction of search space on the basis of stem overlap, word overlap, and cosine similarity measure which help us automatize the process to some extent and reduce human effort for alignment.
Remerciements Ces recherches ont bénéﬁcié du soutien ﬁnancier de l’Agence Nationale de la Recherche (ANR 2010 CORD 001 02) en faveur du projet CAAS. Références SCHENKEL R., SUCHANEK F. M. & KASNECI G. (2007). Yawn : A semantically annotated wikipedia xml corpus. In A. KEMPER, H. SCHÖNING, T. ROSE, M. JARKE, T. SEIDL, C. QUIX & C. BROCHHAUS, Eds., BTW, volume 103 of LNI, p. 277–291 : GI. THEOBALD M., SCHENKEL R. & WEIKUM G. (2005). An efﬁcient and versatile query engine for topx search. In K. BÖHM, C. S. JENSEN, L. M. HAAS, M. L. KERSTEN, P.-Å. LARSON & B. C. OOI, Eds., VLDB, p. 625–636 : ACM. YOSHIKAWA M., AMAGASA T., SHIMURA T. & UEMURA S. (2001). XRel : A Path-Based Approach to Storage and Retrieval of XML. 
In this paper, we investigate lexicon models for hierarchical phrase-based statistical machine translation. We study five types of lexicon models: a model which is extracted from word-aligned training data and{---}given the word alignment matrix{---}relies on pure relative frequencies [1]; the IBM model 1 lexicon [2]; a regularized version of IBM model 1; a triplet lexicon model variant [3]; and a discriminatively trained word lexicon model [4]. We explore sourceto-target models with phrase-level as well as sentence-level scoring and target-to-source models with scoring on phrase level only. For the first two types of lexicon models, we compare several scoring variants. All models are used during search, i.e. they are incorporated directly into the log-linear model combination of the decoder. Phrase table smoothing with triplet lexicon models and with discriminative word lexicons are novel contributions. We also propose a new regularization technique for IBM model 1 by means of the Kullback-Leibler divergence with the empirical unigram distribution as regularization term. Experiments are carried out on the large-scale NIST Chinese→English translation task and on the English→French and Arabic→English IWSLT TED tasks. For Chinese→English and English→French, we obtain the best results by using the discriminative word lexicon to smooth our phrase tables.
This paper describes our current Spanish speech-to-text (STT) system with which we participated in the 2011 Quaero STT evaluation that is being developed within the Quaero program. The system consists of 4 separate subsystems, as well as the standard MFCC and MVDR phoneme based subsystems we included a both a phoneme and grapheme based bottleneck subsystem. We carefully evaluate the performance of each subsystem. After including several new techniques we were able to reduce the WER by over 30{\%} from 20.79{\%} to 14.53{\%}.
This work describes a process to extract Named Entity (NE) translations from the text available in web links (anchor texts). It translates a NE by retrieving a list of web documents in the target language, extracting the anchor texts from the links to those documents and finding the best translation from the anchor texts, using a combination of features, some of which, are specific to anchor texts. Experiments performed on a manually built corpora, suggest that over 70{\%} of the NEs, ranging from unpopular to popular entities, can be translated correctly using sorely anchor texts. Tests on a Machine Translation task indicate that the system can be used to improve the quality of the translations of state-of-the-art statistical machine translation systems.
In this work, we propose a novel method for vocabulary selection which enables simultaneous speech recognition systems for lectures to automatically adapt to the diverse topics that occur in educational and scientific lectures. Utilizing materials that are available before the lecture begins, such as lecture slides, our proposed framework iteratively searches for related documents on the World Wide Web and generates a lecture-specific vocabulary and language model based on the resulting documents. In this paper, we introduce a novel method for vocabulary selection where we rank vocabulary that occurs in the collected documents based on a relevance score which is calculated using a combination of word features. Vocabulary selection is a critical component for topic adaptation that has typically been overlooked in prior works. On the interACT German-English simultaneous lecture translation system our proposed approach significantly improved vocabulary coverage, reducing the out-of-vocabulary rate on average by 57.0{\%} and up to 84.9{\%}, compared to a lecture-independent baseline. Furthermore, our approach reduced the word error rate by up to 25.3{\%} (on average 13.2{\%} across all lectures), compared to a lectureindependent baseline.
The increasing popularity of statistical machine translation (SMT) systems is introducing new domains of translation that need to be tackled. As many resources are already available, domain adaptation methods can be applied to utilize these recourses in the most beneficial way for the new domain. We explore adaptation via filtering, using the crossentropy scores to discard irrelevant sentences. We focus on filtering for two important components of an SMT system, namely the language model (LM) and the translation model (TM). Previous work has already applied LM cross-entropy based scoring for filtering. We argue that LM cross-entropy might be appropriate for LM filtering, but not as much for TM filtering. We develop a novel filtering approach based on a combined TM and LM cross-entropy scores. We experiment with two large-scale translation tasks, the Arabic-to-English and English-to-French IWSLT 2011 TED Talks MT tasks. For LM filtering, we achieve strong perplexity improvements which carry over to the translation quality with improvements up to +0.4{\%} BLEU. For TM filtering, the combined method achieves small but consistent improvements over the standalone methods. As a side effect of adaptation via filtering, the fully fledged SMT system vocabulary size and phrase table size are reduced by a factor of at least 2 while up to +0.6{\%} BLEU improvement is observed.
When building a university lecture translation system, one important step is to adapt it to the target domain. One problem in this adaptation task is to acquire translations for domain specific terms. In this approach we tried to get these translations from Wikipedia, which provides articles on very specific topics in many different languages. To extract translations for the domain specific terms, we used the interlanguage links of Wikipedia . We analyzed different methods to integrate this corpus into our system and explored methods to disambiguate between different translations by using the text of the articles. In addition, we developed methods to handle different morphological forms of the specific terms in morphologically rich input languages like German. The results show that the number of out-of-vocabulary (OOV) words could be reduced by 50{\%} on computer science lectures and the translation quality could be improved by more than 1 BLEU point.
Punctuation prediction is an important task in Spoken Language Translation. The output of speech recognition systems does not typically contain punctuation marks. In this paper we analyze different methods for punctuation prediction and show improvements in the quality of the final translation output. In our experiments we compare the different approaches and show improvements of up to 0.8 BLEU points on the IWSLT 2011 English French Speech Translation of Talks task using a translation system to translate from unpunctuated to punctuated text instead of a language model based punctuation prediction method. Furthermore, we do a system combination of the hypotheses of all our different approaches and get an additional improvement of 0.4 points in BLEU.
In this paper, we dissect the influence of several target-side dependency-based extensions to hierarchical machine translation, including a dependency language model (LM). We pursue a non-restrictive approach that does not prohibit the production of hypotheses with malformed dependency structures. Since many questions remained open from previous and related work, we offer in-depth analysis of the influence of the language model order, the impact of dependency-based restrictions on the search space, and the information to be gained from dependency tree building during decoding. The application of a non-restrictive approach together with an integrated dependency LM scoring is a novel contribution which yields significant improvements for two large-scale translation tasks for the language pairs Chinese{--}English and German{--}French.
The effect of mistranslations on the verbal behaviour of users of speech-to-speech translation is investigated through a question answering experiment in which users were presented with machine translated questions through synthesized speech. Results show that people are likely to align their verbal behaviour to the output of a system that combines machine translation, speech recognition and speech synthesis in an interactive dialogue context, even when the system produces erroneous output. The alignment phenomenon has been previously considered by dialogue system designers from the perspective of the benefits it might bring to the interaction (e.g. by making the user more likely to employ terms contained in the system{'}s vocabulary). In contrast, our results reveal that in speech-to-speech translation systems alignment can in fact be detrimental to the interaction (e.g. by priming the user to align with non-existing lexical items produced by mistranslation). The implications of these findings are discussed with respect to the design of such systems.
We present a novel translation quality informed procedure for both extraction and scoring of phrase pairs in PBSMT systems. We reformulate the extraction problem in the supervised learning framework. Our goal is twofold. First, We attempt to take the translation quality into account; and second we incorporating arbitrary features in order to circumvent alignment errors. One-Class SVMs and the Mapping Convergence algorithm permit training a single-class classifier to discriminate between useful and useless phrase pairs. Such classifier can be learned from a training corpus that comprises only useful instances. The confidence score, produced by the classifier for each phrase pairs, is employed as a selection criteria. The smoothness of these scores allow a fine control over the size of the resulting translation model. Finally, confidence scores provide a new accuracy-based feature to score phrase pairs. Experimental evaluation of the method shows accurate assessments of phrase pairs quality even for regions in the space of possible phrase pairs that are ignored by other approaches. This enhanced evaluation of phrase pairs leads to improvements in the translation performance as measured by BLEU.
In order to efficiently improve machine translation systems, we propose a method which selects data to be annotated (manually translated) from speech-to-speech translation field data. For the selection experiments, we used data from field experiments conducted during the 2009 fiscal year in five areas of Japan. For the selection experiments, we used data sets from two areas: one data set giving the lowest baseline speech translation performance for its test set, and another data set giving the highest. In the experiments, we compare two methods for selecting data to be manually translated from the field data. Both of them use source side language models for data selection, but in different manners. According to the experimental results, either or both of the methods show larger improvements compared to a random data selection.
Although many important scientific advances have taken place in automatic speech recognition research, we have also encountered a number of practical limitations which hinder a widespread deployment of applications and services. In most speech recognition tasks, human subjects produce one to two orders of magnitude fewer errors than machines. One of the most significant differences exists in that human subjects are far more flexible and adaptive than machines against various variations of speech, including individuality, speaking style, additive noise, and channel distortions. How to train and adapt statistical models for speech recognition using a limited amount of data is one of the most important research issues. 
Although many important scientific advances have taken place in automatic speech recognition research, we have also encountered a number of practical limitations which hinder a widespread deployment of applications and services. In most speech recognition tasks, human subjects produce one to two orders of magnitude fewer errors than machines. One of the most significant differences exists in that human subjects are far more flexible and adaptive than machines against various variations of speech, including individuality, speaking style, additive noise, and channel distortions. How to train and adapt statistical models for speech recognition using a limited amount of data is one of the most important research issues. 
Although many important scientific advances have taken place in automatic speech recognition research, we have also encountered a number of practical limitations which hinder a widespread deployment of applications and services. In most speech recognition tasks, human subjects produce one to two orders of magnitude fewer errors than machines. One of the most significant differences exists in that human subjects are far more flexible and adaptive than machines against various variations of speech, including individuality, speaking style, additive noise, and channel distortions. How to train and adapt statistical models for speech recognition using a limited amount of data is one of the most important research issues. 
We report here on the eighth Evaluation Campaign organized by the IWSLT workshop. This year, the IWSLT evaluation focused on the automatic translation of public talks and included tracks for speech recognition, speech translation, text translation, and system combination. Unlike previous years, all data supplied for the evaluation has been publicly released on the workshop website, and is at the disposal of researchers interested in working on our benchmarks and in comparing their results with those published at the workshop. This paper provides an overview of the IWSLT 2011 Evaluation Campaign, which includes: descriptions of the supplied data and evaluation specifications of each track, the list of participants specifying their submitted runs, a detailed description of the subjective evaluation carried out, the main findings of each exercise drawn from the results and the system descriptions prepared by the participants, and, finally, several detailed tables reporting all the evaluation results.
In this paper, we describe NICT{'}s participation in the IWSLT 2011 evaluation campaign for the ASR Track. To recognize spontaneous speech, we prepared an acoustic model trained by more spontaneous speech corpora and a language model constructed with text corpora distributed by the organizer. We built the multi-pass ASR system by adapting the acoustic and language models with previous ASR results. The target speech was selected from talks on the TED (Technology, Entertainment, Design) program. Here, a large reduction in word error rate was obtained by the speaker adaptation of the acoustic model with MLLR. Additional improvement was achieved not only by adaptation of the language model but also by parallel usage of the baseline and speaker-dependent acoustic models. Accordingly, the final WER was reduced by 30{\%} from the baseline ASR for the distributed test set.
This paper describes the MIT-LL/AFRL statistical MT system and the improvements that were developed during the IWSLT 2011 evaluation campaign. As part of these efforts, we experimented with a number of extensions to the standard phrase-based model that improve performance on the Arabic to English and English to French TED-talk translation tasks. We also applied our existing ASR system to the TED-talk lecture ASR task. We discuss the architecture of the MIT-LL/AFRL MT system, improvements over our 2010 system, and experiments we ran during the IWSLT-2011 evaluation. Specifically, we focus on 1) speech recognition for lecture-like data, 2) cross-domain translation using MAP adaptation, and 3) improved Arabic morphology for MT preprocessing.
In this paper, we provide a description of the Dublin City University{'}s (DCU) submissions in the IWSLT 2011 evaluationcampaign.1 WeparticipatedintheArabic-Englishand Chinese-English Machine Translation(MT) track translation tasks. We use phrase-based statistical machine translation (PBSMT) models to create the baseline system. Due to the open-domain nature of the data to be translated, we use domain adaptation techniques to improve the quality of translation. Furthermore, we explore target-side syntactic augmentation for an Hierarchical Phrase-Based (HPB) SMT model. Combinatory Categorial Grammar (CCG) is used to extract labels for target-side phrases and non-terminals in the HPB system. Combining the domain adapted language models with the CCG-augmented HPB system gave us the best translations for both language pairs providing statistically significant improvements of 6.09 absolute BLEU points (25.94{\%} relative) and 1.69 absolute BLEU points (15.89{\%} relative) over the unadapted PBSMT baselines for the Arabic-English and Chinese-English language pairs, respectively.
This paper describes NICT{'}s participation in the IWSLT 2011 evaluation campaign for the TED speech translation ChineseEnglish shared-task. Our approach was based on a phrasebased statistical machine translation system that was augmented in two ways. Firstly we introduced rule-based re-ordering constraints on the decoding. This consisted of a set of rules that were used to segment the input utterances into segments that could be decoded almost independently. This idea here being that constraining the decoding process in this manner would greatly reduce the search space of the decoder, and cut out many possibilities for error while at the same time allowing for a correct output to be generated. The rules we used exploit punctuation and spacing in the input utterances, and we use these positions to delimit our segments. Not all punctuation/spacing positions were used as segment boundaries, and the set of used positions were determined by a set of linguistically-based heuristics. Secondly we used two heterogeneous methods to build the translation model, and lexical reordering model for our systems. The first method employed the popular method of using GIZA++ for alignment in combination with phraseextraction heuristics. The second method used a recentlydeveloped Bayesian alignment technique that is able to perform both phrase-to-phrase alignment and phrase pair extraction within a single unsupervised process. The models produced by this type of alignment technique are typically very compact whilst at the same time maintaining a high level of translation quality. We evaluated both of these methods of translation model construction in isolation, and our results show their performance is comparable. We also integrated both models by linear interpolation to obtain a model that outperforms either component. Finally, we added an indicator feature into the log-linear model to indicate those phrases that were in the intersection of the two translation models. The addition of this feature was also able to provide a small improvement in performance.
This paper describes the Microsoft Research (MSR) system for the evaluation campaign of the 2011 international workshop on spoken language translation. The evaluation task is to translate TED talks (www.ted.com). This task presents two unique challenges: First, the underlying topic switches sharply from talk to talk. Therefore, the translation system needs to adapt to the current topic quickly and dynamically. Second, only a very small amount of relevant parallel data (transcripts of TED talks) is available. Therefore, it is necessary to perform accurate translation model estimation with limited data. In the preparation for the evaluation, we developed two new methods to attack these problems. Specifically, we developed an unsupervised topic modeling based adaption method for machine translation models. We also developed a discriminative training method to estimate parameters in the generative components of the translation models with limited data. Experimental results show that both methods improve the translation quality. Among all the submissions, ours achieves the best BLEU score in the machine translation Chinese-to-English track (MT{\_}CE) of the IWSLT 2011 evaluation that we participated.
LIMSI took part in the IWSLT 2011 TED task in the MT track for English to French using the in-house n-code system, which implements the n-gram based approach to Machine Translation. This framework not only allows to achieve state-of-the-art results for this language pair, but is also appealing due to its conceptual simplicity and its use of well understood statistical language models. Using this approach, we compare several ways to adapt our existing systems and resources to the TED task with mixture of language models and try to provide an analysis of the modest gains obtained by training a log linear combination of inand out-of-domain models.
This paper describes the system developed by the LIG laboratory for the 2011 IWSLT evaluation. We participated to the English-French MT and SLT tasks. The development of a reference translation system (MT task), as well as an ASR output translation system (SLT task) are presented. We focus this year on the SLT task and on the use of multiple 1-best ASR outputs to improve overall translation quality. The main experiment presented here compares the performance of a SLT system where multiple ASR 1-best are combined before translation (source combination), with a SLT system where multiple ASR 1-best are translated, the system combination being conducted afterwards on the target side (target combination). The experimental results show that the second approach (target combination) overpasses the first one, when the performance is measured with BLEU.
This paper presents the KIT system participating in the English→French TALK Translation tasks in the framework of the IWSLT 2011 machine translation evaluation. Our system is a phrase-based translation system using POS-based reordering extended with many additional features. First of all, a special preprocessing is devoted to the Giga corpus in order to minimize the effect of the great amount of noise it contains. In addition, the system gives more importance to the in-domain data by adapting the translation and the language models as well as by using a wordcluster language model. Furthermore, the system is extended by a bilingual language model and a discriminative word lexicon. The automatic speech transcription input usually has no or wrong punctuation marks, therefore these marks were especially removed from the source training data for the SLT system training.
This paper describes the three systems developed by the LIUM for the IWSLT 2011 evaluation campaign. We participated in three of the proposed tasks, namely the Automatic Speech Recognition task (ASR), the ASR system combination task (ASR{\_}SC) and the Spoken Language Translation task (SLT), since these tasks are all related to speech translation. We present the approaches and specificities we developed on each task.
This paper reports on the participation of FBK at the IWSLT 2011 Evaluation: namely in the English ASR track, the Arabic-English MT track and the English-French MT and SLT tracks. Our ASR system features acoustic models trained on a portion of the TED talk recordings that was automatically selected according to the fidelity of the provided transcriptions. Three decoding steps are performed interleaved by acoustic feature normalization and acoustic model adaptation. Concerning the MT and SLT systems, besides language specific pre-processing and the automatic introduction of punctuation in the ASR output, two major improvements are reported over our last year baselines. First, we applied a fill-up method for phrase-table adaptation; second, we explored the use of hybrid class-based language models to better capture the language style of public speeches.
This paper describes our English Speech-to-Text (STT) system for the 2011 IWSLT ASR track. The system consists of 2 subsystems with different front-ends{---}one MVDR based, one MFCC based{---}which are combined using confusion network combination to provide a base for a second pass speaker adapted MVDR system. We demonstrate that this set-up produces competitive results on the IWSLT 2010 dev and test sets.
We describe DFKI{'}s submission to the System Combination and Machine Translation tracks of the 2011 IWSLT Evaluation Campaign. We focus on a sentence selection mechanism which chooses the (hopefully) best sentence among a set of candidates. The rationale behind it is to take advantage of the strengths of each system, especially given an heterogeneous dataset like the one in this evaluation campaign, composed of TED Talks of very different topics. We focus on using features that correlate well with human judgement and, while our primary system still focus on optimizing the BLEU score on the development set, our goal is to move towards optimizing directly the correlation with human judgement. This kind of system is still under development and was used as a secondary submission.
In this paper the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2011 is presented. We participated in the MT (English-French, Arabic-English, ChineseEnglish) and SLT (English-French) tracks. Both hierarchical and phrase-based SMT decoders are applied. A number of different techniques are evaluated, including domain adaptation via monolingual and bilingual data selection, phrase training, different lexical smoothing methods, additional reordering models for the hierarchical system, various Arabic and Chinese segmentation methods, punctuation prediction for speech recognition output, and system combination. By application of these methods we can show considerable improvements over the respective baseline systems.
The Quaero program is an international project promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within the program framework, research organizations and industrial partners collaborate to develop prototypes of innovating applications and services for access and usage of multimedia data. One of the topics addressed is the translation of spoken language. Each year, a project-internal evaluation is conducted by DGA to monitor the technological advances. This work describes the design and results of the 2011 evaluation campaign. The participating partners were RWTH, KIT, LIMSI and SYSTRAN. Their approaches are compared on both ASR output and reference transcripts of speech data for the translation between French and German. The results show that the developed techniques further the state of the art and improve translation quality.
This paper describes the speech-to-text systems used to provide automatic transcriptions used in the Quaero 2010 evaluation of Machine Translation from speech. Quaero (www.quaero.org) is a large research and industrial innovation program focusing on technologies for automatic analysis and classification of multimedia and multilingual documents. The ASR transcript is the result of a Rover combination of systems from three teams ( KIT, RWTH, LIMSI+VR) for the French and German languages. The casesensitive word error rates (WER) of the combined systems were respectively 20.8{\%} and 18.1{\%} on the 2010 evaluation data, relative WER reductions of 14.6{\%} and 17.4{\%} respectively over the best component system.
Machine translation evaluation campaigns require the production of reference corpora to automatically measure system output. This paper describes recent efforts to create such data with the objective of measuring the quality of the systems participating in the Quaero evaluations. In particular, we focus on the protocols behind such production as well as all the issues raised by the complexity of the transcription data handled.
This paper compares techniques to combine diverse parallel corpora for domain-specific phrase-based SMT system training. We address a common scenario where little in-domain data is available for the task, but where large background models exist for the same language pair. In particular, we focus on phrase table fill-up: a method that effectively exploits background knowledge to improve model coverage, while preserving the more reliable information coming from the in-domain corpus. We present experiments on an emerging transcribed speech translation task {--} the TED talks. While performing similarly in terms of BLEU and NIST scores to the popular log-linear and linear interpolation techniques, filled-up translation models are more compact and easy to tune by minimum error training.
In statistical machine translation systems, phrases with similar meanings often have similar but not identical distributions of translations. This paper proposes a new soft clustering method to smooth the conditional translation probabilities for a given phrase with those of semantically similar phrases. We call this semantic smoothing (SS). Moreover, we fabricate new phrase pairs that were not observed in training data, but which may be used for decoding. In learning curve experiments against a strong baseline, we obtain a consistent pattern of modest improvement from semantic smoothing, and further modest improvement from phrase pair fabrication.
We discuss learning latent annotations for synchronous context-free grammars (SCFG) for the purpose of improving machine translation. We show that learning annotations for nonterminals results in not only more accurate translation, but also faster SCFG decoding.
In this paper, we propose structure transformation rules for statistical machine translation which are lexicalized by only function words. Although such rules can be extracted from an aligned parallel corpus simply as original phrase pairs, their structure is hierarchical and thus can be used in a hierarchical translation system. In addition, structure transformation rules can take into account long-distance reordering, allowing for more than two phrases to be moved simultaneously. The rule set is used as a core module in our hierarchical model together with two other modules, namely, a basic reordering module and an optional gap phrase module. Our model is considerably more compact and produces slightly higher BLEU scores than the original hierarchical phrase-based model in Japanese-English translation on the parallel corpus of the NTCIR-7 patent translation task.
In this paper we describe some of our recent investigations into ASR and SMT coupling issues from an ASR perspective. Our study was motivated by several areas: Firstly, to understand how standard ASR tuning procedures effect the SMT performance and whether it is safe to perform this tuning in isolation. Secondly, to investigate how vocabulary and segmentation mismatches between the ASR and SMT system effect the performance. Thirdly, to uncover any practical issues that arise when using a WFST based speech decoder for tight coupling as opposed to a more traditional tree-search decoding architecture. On the IWSLT07 Japanese-English task we found that larger language model weights only helped the SMT performance when the ASR decoder was tuned in a sub-optimal manner. When we considered the performance with suitable wide beams that ensured the ASR accuracy had converged we observed the language model weight had little influence on the SMT BLEU scores. After the construction of the phrase table the actual SMT vocabulary can be less than the training data vocabulary. By reducing the ASR lexicon to only cover the words the SMT system could accept, we found this lead to an increase in the ASR error rates, however the SMT BLEU scores were nearly unchanged. From a practical point of view this is a useful result as it means we can significantly reduce the memory footprint of the ASR system. We also investigated coupling WFST based ASR to a simple WFST based translation decoder and found it was crucial to perform phrase table expansion to avoid OOV problems. For the WFST translation decoder we describe a semiring based approach for optimizing the log-linear weights.
Phrase alignment is a crucial step in phrase-based statistical machine translation. We explore a way of improving phrase alignment by adding syntactic information in the form of chunks as soft constraints guided by an in-depth and detailed analysis on a hand-aligned data set. We extend a probabilistic phrase alignment model that extracts phrase pairs by optimizing phrase pair boundaries over the sentence pair [1]. The boundaries of the target phrase are chosen such that the overall sentence alignment probability is optimal. Viterbi alignment information is also added in the extended model with a view of improving phrase alignment. We extract phrase pairs using a relatively larger number of features which are discriminatively trained using a large-margin online learning algorithm, i.e., Margin Infused Relaxed Algorithm (MIRA) and integrate it in our approach. Initial experiments show improvements in both phrase alignment and translation quality for Arabic-English on a moderate-size translation task.
Many syntactic machine translation decoders, including Moses, cdec, and Joshua, implement bottom-up dynamic programming to integrate N-gram language model probabilities into hypothesis scoring. These decoders concatenate hypotheses according to grammar rules, yielding larger hypotheses and eventually complete translations. When hypotheses are concatenated, the language model score is adjusted to account for boundary-crossing n-grams. Words on the boundary of each hypothesis are encoded in state, consisting of left state (the first few words) and right state (the last few words). We speed concatenation by encoding left state using data structure pointers in lieu of vocabulary indices and by avoiding unnecessary queries. To increase the decoder{'}s opportunities to recombine hypothesis, we minimize the number of words encoded by left state. This has the effect of reducing search errors made by the decoder. The resulting gain in model score is smaller than for right state minimization, which we explain by observing a relationship between state minimization and language model probability. With a fixed cube pruning pop limit, we show a 3-6{\%} reduction in CPU time and improved model scores. Reducing the pop limit to the point where model scores tie the baseline yields a net 11{\%} reduction in CPU time.
The technology used by MOLTO is based on GF (Grammatical Framework), which is a programming language for multilingual grammars. The talk will explain how production-quality translation systems can be built rapidly and economically by using GF. The key concept of GF is a multilingual grammar, a grammar that uses an abstract syntax as an interlingua, a shared semantic structure for multiple languages. The languages are related to the abstract syntax by reversible mappings. To help writing these mappings, GF provides a Resource Grammar Library, which implements the morphology and basic syntax of currently 18 languages. While the translation systems built in MOLTO are essentially grammar-based, they will integrate statistical machine translation methods to improve coverage and to learn parts of grammars from data. GF and related tools are open-source software available for all major platforms and constantly developed by an international community. 
Extensible Dependency Grammar (XDG; Debusmann, 2007) is a flexible, modular dependency grammar framework in which sentence analyses consist of multigraphs and processing takes the form of constraint satisfaction. This paper shows how XDG lends itself to grammar-driven machine translation and introduces the machinery necessary for synchronous XDG. Since the approach relies on a shared semantics, it resembles interlingua MT. It differs in that there are no separate analysis and generation phases. Rather, translation consists of the simultaneous analysis and generation of a single source-target {``}sentence{''}.
This paper discusses the qualitative comparative evaluation performed on the results of two machine translation systems with different approaches to the processing of multi-word units. It proposes a solution for overcoming the difficulties multi-word units present to machine translation by adopting a methodology that combines the lexicon grammar approach with OpenLogos ontology and semantico-syntactic rules. The paper also discusses the importance of a qualitative evaluation metrics to correctly evaluate the performance of machine translation engines with regards to multi-word units.
There are a number of morphological analysers for Polish. Most of these, however, are non-free resources. What is more, different analysers employ different tagsets and tokenisation strategies. This situation calls for a simple and universal framework to join different sources of morphological information, including the existing resources as well as user-provided dictionaries. We present such a configurable framework that allows to write simple configuration files that define tokenisation strategies and the behaviour of morphological analysers, including simple tagset conversion.
This paper proposes to enrich RBMT dictionaries with Named Entities (NEs) automatically acquired from Wikipedia. The method is applied to the Apertium English{--}Spanish system and its performance compared to that of Apertium with and without handtagged NEs. The system with automatic NEs outperforms the one without NEs, while results vary when compared to a system with handtagged NEs (results are comparable for Spanish→English but slightly worst for English→Spanish). Apart from that, adding automatic NEs contributes to decreasing the amount of unknown terms by more than 10{\%}.
This document describes a project aimed at building a new web interface to the Apertium machine translation platform, including pre-editing and post-editing environments. It contains a description of the accomplished work on this project, as well as an overview of possible future work.
This paper describes the development of a two-way shallow-transfer rulebased machine translation system between Bulgarian and Macedonian. It gives an account of the resources and the methods used for constructing the system, including the development of monolingual and bilingual dictionaries, syntactic transfer rules and constraint grammars. An evaluation of the system{'}s performance was carried out and compared to another commercially available MT system for the two languages. Some future work was suggested.
Softcatala` is a non-profit association created more than 10 years ago to fight the marginalisation of the Catalan language in information and communication technologies. It has led the localisation of many applications and the creation of a website which allows its users to translate texts between Spanish and Catalan using an external closedsource translation engine. Recently, the closed-source translation back-end has been replaced by a free/open-source solution completely managed by Softcatala`: the Apertium machine translation platform and the ScaleMT web service framework. Thanks to the openness of the new solution, it is possible to take advantage of the huge amount of users of the Softcatala` translation service to improve it, using a series of methods presented in this paper. In addition, a study of the translations requested by the users has been carried out, and it shows that the translation back-end change has not affected the usage patterns.
This article describes the development of an Open Source shallow-transfer machine translation system from Czech to Polish in the Apertium platform. It gives details of the methods and resources used in constructing the system. Although the resulting system has quite a high error rate, it is still competetive with other systems.
This paper presents an Italian→Catalan RBMT system automatically built by combining the linguistic data of the existing pairs Spanish{--}Catalan and Spanish{--}Italian. A lightweight manual postprocessing is carried out in order to fix inconsistencies in the automatically derived dictionaries and to add very frequent words that are missing according to a corpus analysis. The system is evaluated on the KDE4 corpus and outperforms Google Translate by approximately ten absolute points in terms of both TER and GTM.
Christophe Mazenc  Aldo Iorio  World Intellectual Property Organization 34, chemin des Colombettes CH-1211 Geneva 20  Bruno.Pouliquen@wipo.int, Christophe.Mazenc@wipo.int, Aldo.Iorio@wipo.int  Abstract This paper presents a study conducted in the course of implementing a project in the World Intellectual Property Organization (WIPO) on assisted translation of patent abstracts and titles from English to French. The tool (called ‘Tapta’) is trained on an extensive corpus of manually translated patents. These patents are classified, each class belonging to one of the 32 predefined domains. The trained Statistical Machine Translation (SMT) tool uses this additional information to propose more accurate translations according to the context. The performance of the SMT system was shown to be above the current state of the art, but, in order to produce an acceptable translation, a human has to supervise the process. Therefore, a graphical user interface was built in which the translator drives the automatic translation process. A significant experiment with human operators was conducted within WIPO, the output was judged to be successful and a project to use Tapta in production is now under discussion. 
Christophe Mazenc  Aldo Iorio  World Intellectual Property Organization 34, chemin des Colombettes CH-1211 Geneva 20  Bruno.Pouliquen@wipo.int, Christophe.Mazenc@wipo.int, Aldo.Iorio@wipo.int  Abstract This paper presents a study conducted in the course of implementing a project in the World Intellectual Property Organization (WIPO) on assisted translation of patent abstracts and titles from English to French. The tool (called ‘Tapta’) is trained on an extensive corpus of manually translated patents. These patents are classified, each class belonging to one of the 32 predefined domains. The trained Statistical Machine Translation (SMT) tool uses this additional information to propose more accurate translations according to the context. The performance of the SMT system was shown to be above the current state of the art, but, in order to produce an acceptable translation, a human has to supervise the process. Therefore, a graphical user interface was built in which the translator drives the automatic translation process. A significant experiment with human operators was conducted within WIPO, the output was judged to be successful and a project to use Tapta in production is now under discussion. 
This paper reports MT evaluation experiments that were conducted at the end of year 1 of the EU-funded CoSyne1 project for three language combinations, considering translations from German, Italian and Dutch into English. We present a comparative evaluation of the MT software developed within the project against four of the leading free webbased MT systems across a range of state-of-the-art automatic evaluation metrics. The data sets from the news domain that were created and used for training purposes and also for this evaluation exercise, which are available to the research community, are also described. The evaluation results for the news domain are very encouraging: the CoSyne MT software consistently beats the rule-based MT systems, and for translations from Italian and Dutch into English in particular the scores given by some of the standard automatic evaluation metrics are not too distant from those obtained by wellestablished statistical online MT systems. 
The PLUTO 1 project (Patent Language Translations Online) aims to provide a rapid solution for the online retrieval and translation of patent documents through the integration of a number of existing state-of-the-art components provided by the project partners. The paper presents some of the experiments on patent domain adaptation of the Machine Translation (MT) systems used in the PLuTO project. The experiments use the International Patent Classification for domain adaptation and are focused on the English±French language pair. 
W. Wilsonplein 7 9000 Gent, Belgium joachim@crosslang.com  Joeri Van de Walle CrossLang W. Wilsonplein 7 9000 Gent, Belgium joeri@crosslang.com  Abstract BTS ± Bologna Translation Service ± is an ICT PSP EU-funded project which specializes in the automatic translation of study programmes. At the core of the BTS framework are several machine translation (MT) engines through which web-based translation services are offered. The fully integrated BTS architecture includes a translation engine coupling rule-based and statistical MT with automatic post-editing (APE). In this paper, we first describe the motivating factors behind the provision of such a service. Following this, we give an overview of the BTS framework as a whole, with particular emphasis on the MT components, and provide a real world use case scenario in which BTS MT services are exploited. 
 Abstract This paper introduces FLAVIUS, an innovative platform to allow full translation and indexing of Internet sites. The EC-funded FLAVIUS project aims at providing an end-to-end solution for Internet content publishers of small to very large sites that would allow them to get their content translated and indexed in other languages, with a variety of options. Most sites cannot be found easily by users who browse in other languages. A German user is unlikely to find a French provider if he looks for a collectible, a piece of furQLWXUH«HYHQLIVHYHUDOVLWes provide instant translation. This presentation will show how FLAVIUS can ease the process of translating and indexing a website; and the benefits that Internet content publishers can expect for the audience of their website. We will demonstrate through a concrete case the novelty and the advantages of this platform. 
 30 years ago. A lot of valuable information is  A majority of the localization industry is still unable to beneﬁt from SMT, since it does not know how to integrate SMT system into its production workﬂow. This paper describes a workﬂow on how to integrate the open source SMT system Moses into computer-aided translation (CAT) tools. It introduces a software package which is able to export XLIFF data into a special InlineText format that is further processed and is sent as plain text to Moses as input. Then, the translated text is inserted back to the original ﬁle.  stored in these datasets. TMs, if available, are used for each translation project, since they contain things like a translated previous version of a product, or translation of some other product from the same client. With the deployment of TMs, translation has become much easier, often pre-translated text just need to be post edited. Therefore, TMs are very useful for human translators and should be heavily used also by SMT systems. Mainly universities and research centers are creating various SMT systems. Each of them provides two phases – training and translation. For both phases, it is necessary to have input. Due to completely different environments, the issues involved  This transformation is needed, otherwise  and broad variability of research questions, univer-  the use of MT in the localization indus-  sities do not have a uniform format similar to TM.  try would hardly be advantageous, since  For this reason, SMT systems usually require gen-  formatting information would have to be  eral plain text as an input. The localization indus-  manually added by localization profes-  try is different, its TMs contain a lot of additional  sionals. A few attempts to bridge the local-  information, for example XML, RTF, HTML tags,  ization industry and Moses were already  or even binary data. Some of it is related to for-  done in the past, however, the method de-  matting – e.g. end of line, italics. Other kind of  scribed here represents a new approach.  the information is related to content placeholders –  
 as a webservice. This way the user does not have to  This paper presents a webservice architecture for Statistical Machine Translation aimed at non-technical users. A workﬂow editor allows a user to combine different webservices using a graphical user interface. In the current state of this project, the webservices have been implemented for a range of sentential and sub-sentential aligners. The advantage of a common interface and a common data format allows the user to build workﬂows exchanging different aligners.  deal with technical issues regarding the tools, such as their installation, conﬁguration or maintenance. A workﬂow editor allows the user to combine the different webservices using a graphical user interface. In the current state of this project, webservices have been created for a range of sentential and sub-sentential aligners. This work is part of the FP7 PANACEA project,3 which addresses the most critical aspect of MT: the language-resource bottleneck. Its objective is to build a factory of language resources that automates the stages involved in the acquisition, production, updating and maintenance of lan-  
 tical MT (SMT) systems, which can be built with  With the noticeable improvement in the overall quality of Machine Translation (MT) systems in recent years, post-editing of MT output is starting to become a common practice among human translators. However, it is well known that the quality of a given MT system can vary signiﬁcantly across translation segments and that post-editing bad quality translations is a tedious task that may require more effort than translating texts from scratch. Previous research dedicated to learning quality estimation models to ﬂag such segments has shown that models based on human annotation achieve more promising results. However, it is not yet clear what is the most appropriate form of human annotation for building such models. We experiment with models based on three annotation types (post-editing time, post-editing distance and post-editing effort scores) and show that estimations resulting from using post-editing time, a simple and objective annotation, can reliably indicate translation post-editing effort in a practical, taskbased scenario. We also discuss some perspectives on the effectiveness, reliability and cost of each type of annotation.  little effort from translation memories. However, a common complaint from human translators is that the post-editing of certain segments with low quality can be frustrating and can require more effort than translating those segments from scratch, without the aid of an MT system. Identifying such segments and ﬁltering them out from the post-editing task is a problem addressed in the ﬁeld of “Conﬁdence Estimation” (CE), also called “Quality Estimation”, for MT. CE metrics are usually prediction models induced from data using standard machine learning algorithms fed with examples of source and translation features, as well as some form of annotation on the quality of the translations. Early work on sentence-level CE use annotations derived from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and WER (Tillmann et al., 1997) at training time. The resulting models have not been shown to be effective, since the automatic metrics used do not correlate well with human judgments at the segment level and are difﬁcult to interpret as absolute indicators of quality. More recent work focuses on having humans assigning absolute quality scores to translations, which has shown more promising results (Quirk, 2004; Specia et al., 2009a). Obtaining explicit human annotations for trans-  
This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user’s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with  set of translation units (TUs) {(si, ti)}Ni=1 whose fuzzy-match score is above a given threshold Θ, and marks which words in each source-language (SL) segment si differ from those in s . It is however up to the translator to identify which target words in the corresponding target-language (TL) segments ti should be changed to convert ti into t , an adequate translation of s . The method we propose and evaluate in this paper is aimed at recommending the CAT user which words of ti should be changed by the translator or kept unedited to transform ti into t . To do so, we pre-process the user’s TM to compute the word alignments between the source and target segments in each TU. Then, when a new segment s is to be translated, the TUs with a fuzzy-match score above the threshold Θ are obtained and the alignment between the words in si and ti are used to mark which words in ti should be changed or kept unedited.  an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used.  Related work. In the literature one can ﬁnd different approaches that use word or phrase alignments to improve existing TM-based CAT systems; although, to our knowledge, none of them use word alignments for the purpose we study in this pa-  
 ses by external MT systems. The online learn-  Extending phrase-based Statistical Machine Translation systems with a second, dynamic phrase table has been done for multiple purposes. Promising results have been reported for hybrid or multi-engine machine translation, i.e. building a phrase table from the knowledge of external MT systems, and for online learning. We argue that, in prior research, dynamic phrase tables are not scored optimally because they may be of small size, which makes the Maximum Likelihood Estimation of trans-  ing system described in (Hardt and Elming, 2010) uses a similar architecture, with the difference that, rather than translations by external systems, previous translations, post-edited by the user, constitute the dynamic corpus. The aim of this study is to: a) evaluate both approaches in an independent reimplementation and on a different corpus; b) implement and evaluate an alternative scoring procedure that promises further performance gains; c) show the feasibility of combining multi-engine MT and online learning in a single framework.  lation probabilities unreliable. We pro-  2 Related Work  pose basing the scores on frequencies from both the dynamic corpus and the primary corpus instead, and show that this modiﬁcation signiﬁcantly increases performance. We also explore the combination of multiengine MT and online learning.  System combination for Machine Translation is an active research ﬁeld. The last two Workshops on Machine Translation (WMT) included a system combination task; an overview is given in (Callison-Burch et al., 2009; Callison-Burch et al., 2010).  
 is why some researchers have investigated the de-  velopment of multi-engine MT (MEMT) systems  This paper describes a novel approach  (Eisele, 2005; Macherey and Och, 2007; Du et al.,  aimed to identify a priori which subset of  2009; Du et al., 2010) aimed to provide transla-  machine translation (MT) systems among a  tions of higher quality than those produced by the  known set will produce the most reliable  isolated MT systems in which they are based on.  translations for a given source-language (SL) sentence. We aim to select this subset of MT systems by using only information extracted from the SL sentence to be translated, and without access to the inner workings of the MT systems being used. A system able to select in advance, without translating, that subset of MT systems will allow multi-engine MT systems to save computing resources and focus on the combination of the output of the best MT systems. The selection of the best MT systems is done by extracting a set of features from each SL sentence and then using maximum entropy classiﬁers trained over a set of parallel sentences. Preliminary experiments on two European language pairs show a small, non-statistical signiﬁcant improvement.*  MEMT systems can be classiﬁed according to how they work. On one hand, we ﬁnd systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heaﬁeld et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on. On the other hand, we have systems that decide which translation, among all the translations computed by the MT systems they are based on, is the most appropriate one (Nomoto, 2004; Zwarts and Dras, 2008) and output this translation without changing it in any way. In-between, we ﬁnd the MEMT systems that build a consensus translation from a reduced set of translations, i.e. systems that ﬁrst chose the subset with the most promising translations, and  
In this paper, we show how a large bilingual English-French parallel corpus can be brought to bear in terminology search. First, we demonstrate that the coverage of available corpora has become substantially more extensive than that of mainstream term banks. One potential drawback in searching large unstructured corpora is that large numbers of search results may need to be examined before finding a relevant match. We argue that this problem can be alleviated by contextualizing the search process: instead of looking up isolated terms one searches for terms appearing in a context that is similar to that of the term to be translated. We present an experiment on contextbased re-ranking and report highly positive results. We conclude that translators will increasingly rely on very large scale corpora for searching term equivalents. 1. Introduction Isabelle (1992) has often been cited for the observation that bilingual corpora contain more solutions to more translation problems than any other existing resource. As a matter of fact, in recent years translators have been increasingly turning to corpus-based tools: translation memories, bilingual concordancers and, more recently, statistical machine translation. Yet, it seems that for the specific task of looking up the translation of technical terms, translators continue to prefer traditional term banks and corpora remain to this day at best a secondary resource.  Term banks offer at least two important advantages: 1) their content is edited and validated by experts, so that translators feel they can mostly trust it; and 2) each database record is manually assigned domain categories that help users select contextually appropriate translations for polysemous terms. But term banks also have some significant drawbacks. Most importantly: a) their development is extremely time-consuming and costly; and b) their coverage is always incomplete, especially for recently-introduced (and thus lesser known) terms. In this paper, we first argue that corpus search can circumvent some of the drawbacks of traditional term banks. Clearly, development time and cost are much less of an issue with corpora since they are mostly a product of the normal activity of translators. But until recently, no corpora were available whose terminological coverage would rival that of the mainstream general-purpose term banks. In this paper, we show that this is no longer true: the larger existing corpora now cover substantially more technical terms than the larger existing term banks. Second, we argue that some of the known advantages of term banks can be emulated in the world of corpora. Note, however, that we will not address the issue of trust in the present paper. We will focus mainly on the issue of knowledge organization and argue that even though corpora lack an explicit semantic backbone such as domain categories, it is still possible to efficiently retrieve contextuallyrelevant matches from them.  © 2011 European Association for Machine Translation Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 105112 Leuven, Belgium, May 2011  Section 2 describes the experimental set-up: a) the English/French term pairs that we will be using as test data and b) the parallel corpus that we will be searching. Section 3 presents a comparison of the terminological coverage of our corpus with that of a range of alternative resources: large-scale terms banks, Wikipedia and smaller-scale corpora. Section 4 defines the notion of contextual search and describes the algorithms we will be testing. Section 5 reports results of our contextual search techniques. Finally Section 6 situates our work with respect to some related efforts. 2. Experimental Set-up We first describe the term pair data, and then the bilingual corpus used in our experiments. 2.1 Gathering experimental data To simulate translation contexts in specialized domains, we use a set of articles from eight scientific journals1 for which both an English and a French abstract are provided (the second being a translation of the first). Each article contains 2 to 5 author-provided “keyphrases” (invariably technical terms) that have also been translated by the journal editors. The alignment of these keyphrases in the electronic files is coherent enough that we were able to automatically extract a large set of keyphrase pairs. We restricted the set of term pairs to those that explicitly appear in the abstracts, as these abstracts will constitute our test set for contextual search (Section 4). Table 1 shows for each journal the number of abstracts and term pairs extracted (before and after filtering). The table also includes examples of term pairs as well as journal name codes provided for future concise reference. As most of these abstracts were translated from English into French, we assume in the rest of this paper that the relevant task is to find French translations for English terms. 2.2 Gathering a bilingual resource 
Translation results suffer when a standard phrase-based statistical machine translation system is used for translating long sentences. The translation output will not preserve the same word order as the source, especially between a language pair that has different syntactic structures. When a sentence is long, it should be partitioned into several clauses, and the word reordering during the translation should be done within clauses, not between clauses. In this paper, we propose splitting the long sentences using linguistic information, and translating the sentences with the split boundaries. In other words, we constrain the word reordering so that it cannot cross the boundaries. We propose two types of constraints: split condition and block condition. By doing so, word order can be preserved and translation quality improved. Our experiments on the patent translation between Japanese and English are able to achieve better translations measured by BLEU score, NIST score and word error rate (WER). 
 This study focuses on the use of additional bilin-  We present a novel strategy to derive new translation units using an additional bilingual corpus and a previously trained SMT system. The units were used to adapt the SMT system. The derivation process can be applied when the additional corpus is very small compared with the original train corpus and it does not require to compute new word alignments using all corpora. The strategy is based in the Levenshtein Distance and its resulting path. We reported a statistically signiﬁcant improvement, with a conﬁdence level of 99%, when adapting an Ngram-based CatalanSpanish system using an additional corpus that represents less than 0.5% of the original train corpus. The additional translation units were able to solve morphological and lexical errors and added previously unknown words to the vocabulary.  gual corpora to adapt a previously trained SMT system, without the need to recompute word alignments. The proposed method utilizes the SMT system to translate the source side of the new corpus and then compares the translation output with its target side. This comparison allows the method to detect errors made during decoding and provide it at the same time with a possible solution, which is ﬁnally used to build additional translation units. We have experimented with a Ngram-based SMT system (Marin˜o et al., 2006), translating from Catalan into Spanish and we have obtained a signiﬁcant improvement in translation quality, adapting a state-of-the-art system trained with a corpus of more than four million sentences with an additional corpus of only 1.6 thousand sentences. This document is organized as follows: Section 2 introduces us to the concept of Statistical Machine Translation, with an emphasis in Ngrambased SMT; Section 3 presents a description of the possible scenarios where the proposed strategy  
 Portuguese (EP) translations. This discrepancy has  origin in the Brazilian population size that is near  This paper describes a method to efﬁ-  20 times larger than the Portuguese population.  ciently leverage Brazilian Portuguese re-  This paper describes the progressive develop-  sources as European Portuguese resources.  ment of a tool that transforms BP texts into EP, in  Brazilian Portuguese and European Por-  order to increase the amount of EP parallel corpora  tuguese are two Portuguese varieties very  available.  close and usually mutually intelligible,  This paper is organized as follows: Section 2 de-  but with several known differences, which  scribes some related work; Section 3 presents the  are studied in this work. Based on this  main differences between EP and BP; the descrip-  study, we derived a rule based system to  tion of the BP2EP system is the focus of Section  translate Brazilian Portuguese resources.  4. The following section describes an algorithm  Some resources were enriched with mul-  for extracting Multiword Lexical Contrastive pairs  tiword units retrieved semi-automatically  from SMT Phrase-tables; Section 6 presents the re-  from phrase tables created using statisti-  sults, and Section 7 concludes and suggests future  cal machine translation tools. Our experi-  work.  ments suggest that applying our translation step improves the translation quality be-  2 Related Work  tween English and Portuguese, relatively to the same process using the same resources.  Few papers are available on the topic of improving Machine Translation (MT) quality by exploring similarities in varieties, dialects and closely re-  
 Amalia TodiraúFX Linguistique, Langues, Parole (LiLPa) Université de Strasbourg 22, rue René Descartes BP, 80010, 67084 Strasbourg, cedex todiras@unistra.fr  Abstract  This paper describes a cognate identification method, used by a lexical alignment system for French and Romanian. We combine statistical techniques and linguistic information to extract cognates from lemmatized, tagged and sentencealigned parallel corpora. We evaluate the cognate identification model and we compare it to other methods using pure statistical techniques. We show that the use of linguistic information in the cognate identification system improves significantly the results.  
 are largely mutually intelligible, this machine  This paper describes the design, development and evaluation of a machine translation system between Dutch and Afrikaans developed over a period of around a month and a half. The system relies heavily on the re-use of existing publically available resources such as Wiktionary, Wikipedia and the Apertium machine translation platform. A method of translating compound words between the languages by means of left-to-right longest match lookup is also introduced and evaluated.  translation system focuses on dissemination, the translation of text for the purpose of being postedited and then being published. This is not the ﬁrst system to work with this language pair, van Huyssteen and Pilon (2009) describe a rule-based system to convert in a single direction from Dutch to Afrikaans. The reason we have chosen to work with a rule-based approach, instead of the ubiquitous corpus-based/statistical approach, is that the latter needs parallel corpora for the two languages. The only freely available Afrikaans–Dutch corpus, is KDE41, which is translated via English and domain speciﬁc. We  
The aim is to develop an interactive and iterative translation system able to adapt and improve itself rapidly using user’s feedbacks (collected as translation posteditions). We present a preliminary study in which three methods for integrating feedbacks are tested. The results show that, despite the difficulty to measure the system improvement with automatic metrics (given the small size of our collected post-edition corpus) a small-sized subjective analysis reveals the contribution and prospects of such methods. 
 formula (Och and Ney, 2002) in (1).  M  A Statistical Machine Translation (SMT)  P (e|f ) = exp( λihi(e, f ))  (1)  system generates an n-best list of candidate  i=1  translations for each sentence. A model er-  The variable h denotes each of the M fea-  ror occurs if the most probable translation  tures (probabilities learned from language models,  (1-best) generated by the SMT decoder is  translation models, etc.) and λ denotes the associ-  not the most accurate as measured by its  ated feature weight (coefﬁcient).  similarity to the human reference transla-  The candidate translation (in the n-best list) hav-  tion(s) (an oracle). In this paper we inves-  ing the highest decoder score is deemed to be the  tigate the parametric differences between  best translation (1-best) according to the model.  the 1-best and the oracle translation and at-  Automatic evaluation metrics measuring similarity  tempt to try and close this gap by propos-  to human reference translations can be modiﬁed to  ing two rescoring strategies to push the or-  generate a score on the sentence level instead of at  acle up the n-best list. We observe modest  system level. These scores can, in turn, be used  improvements in METEOR scores over the  to determine the quality or goodness of a transla-  baseline SMT system trained on French–  tion. The candidate having the highest sentence-  English Europarl corpora. We present a de-  level evaluation score is deemed to be the most ac-  tailed analysis of the oracle rankings to de-  curate translation (oracle).  termine the source of model errors, which  In practice, it has been found that the n-best list  in turn has the potential to improve overall  rankings can be fairly poor (i.e. low proportion  system performance.  of oracles in rank 1), and the oracle translations  (the candidates closest to a reference translation as  
This paper describes the creation and the content of the Sentence-Aligned European Patent Corpus. The corpus contains more than 130 million sentence pairs for 6 European languages. With more than 76 million sentence pairs, to our knowledge, the EN-DE sub corpus is the largest bilingual sentence-aligned corpus. For other language pairs, work has started to obtain sub corpora of similar size. The error rate of sentence alignment was very low even in the absence of language specific resources. 
The present article introduces a phrasealignment approach that involves the processing of a small bilingual corpus in order to extract suitable structural information. This is used in the PRESEMT project, whose aim is the quick development of phrase-based Machine Translation (MT) systems for new language pairs. A main bottleneck of such systems is the need to create compatible parsing schemes in the source and target languages. This bottleneck is overcome by combining two modules, the Phrase aligner module and the Phrasing model generator, both of them being based on pattern recognition principles. 
Constraints in natural language processing play an important role. In this paper we show which impact (word-order) constraints have on the translation results, when they are applied in the recombination step of a linear EBMT system. Both the baseline EBMT system and the constrained one are implemented during this research. In the experiments we use two language-pairs (RomanianEnglish and Romanian-German), in both directions of translations. In these language constellations, Romanian, an inﬂected language with Latin root, is considered under-resourced. This aspect makes the process of translation even more challenging. 
In this paper, we address the issue of applying example-based machine translation (EBMT) methods to overcome some of the difficulties encountered with statistical machine translation (SMT) techniques. We adopt two different EBMT approaches and present an approach to augment output quality by strategically combining both EBMT approaches with the SMT system to handle issues arising from the use of SMT. We use these approaches for English to Turkish translation using the IWSLT09 dataset. Improved evaluation scores (4% relative BLEU improvement) were achieved when EBMT was used to translate sentences for which SMT failed to produce an adequate translation. 
 quent when translating into a morphologically rich  In this paper, we report our experiments in combining two EBMT systems that rely on generalized templates, Marclator and CMU-EBMT, on an English–German translation task. Our goal was to see whether a statistically signiﬁcant improvement could be achieved over the individ-  language. As an example for translating from English into German, assume that the sentence pairs listed in Example 1 are contained in the example base (Way, 2001). (1) A big dog eats a lot of meat. – Ein großer Hund frisst viel Fleisch. I have two ears. – Ich habe zwei Ohren.  ual performances of these two systems. We observed that this was not the case. However, our system consistently outperformed a lexical EBMT baseline system.  An EBMT system might make use of the phrases shown in bold to translate a sentence like I have a big dog. into Ich habe ein großer Hund. In doing so, it would neglect the fact that German uses dif-  
 translate between the source language (SL) and the  target language (TL).  We describe the development of a pro-  SMT has many advantages, e.g. it is data-driven,  totype of an open source rule-based  language independent, does not need linguistic ex-  Icelandic→English MT system, based on  perts, and prototypes of new systems can by built  the Apertium MT framework and IceNLP,  quickly and at a low cost. On the other hand, the  a natural language processing toolkit for  need for parallel corpora as training data in SMT  Icelandic. Our system, Apertium-IceNLP,  is also its main disadvantage, because such corpora  is the ﬁrst system in which the whole  are not available for a myriad of languages, espe-  morphological and tagging component of  cially the so-called less-resourced languages, i.e.  Apertium is replaced by modules from  languages for which few, if any, natural language  an external system. Evaluation shows  processing (NLP) resources are available. When  that the word error rate and the position-  there is a lack of parallel corpora, other machine  independent word error rate for our pro-  translation (MT) methods, such as rule-based MT,  totype is 50.6% and 40.8%, respectively.  e.g. Apertium (Forcada et al., 2009), may be used  As expected, this is higher than the corre-  to create MT systems.  sponding error rates in two publicly available MT systems that we used for comparison. Contrary to our expectations, the error rates of our prototype is also higher than the error rates of a comparable system based solely on Apertium modules. Based on error analysis, we conclude that better translation quality may be achieved by replacing only the tagging component of Apertium with the corresponding module in IceNLP, but leaving morphological analysis to Apertium.  In this paper, we describe the development of a prototype of an open source rule-based Icelandic→English (is-en) MT system based on Apertium and IceNLP, an NLP toolkit for processing and analysing Icelandic texts (Loftsson and Rögnvaldsson, 2007b). A decade ago, the Icelandic language could have been categorised as a less-resourced language. The current situation, however, is much better thanks to the development of IceNLP and various linguistic resources (Rögnvaldsson et al., 2009). On the other hand, no large parallel corpus, in which Icelandic is one of the  
 multi-layered Linguistically augmented Statistical  This study presents a new hybrid approach for translation equivalent selection within a transfer-based machine translation system using an intertwined net of traditional linguistic methods together with statistical techniques. Detailed evaluation reveals that the translation quality can be improved substantially in this way.  Terminology EXtraction). We decided to impose strict knowledge-enhanced requirements for the statistical term extraction which consequently was augmented by several layers of linguistic data reﬁnement and automatic feature attribution. LiSTEX is distinct from other term extractors as it has layers that access already during the early term deﬁning extraction phase the RBMT system components for linguistic ﬁltering and later production  
 be of very low quality, placing an unnecessary bur-  We investigate the problem of predicting the quality of a given Machine Translation (MT) output segment as a binary classiﬁcation task. In a study with four different data sets in two text genres and two language pairs, we show that the performance of a Support Vector Machine (SVM) classiﬁer can be improved by extending the feature set with implicitly deﬁned syntactic features in the form of tree kernels over syntactic parse trees. Moreover, we demonstrate that syntax tree kernels achieve surprisingly high performance levels even without additional features, which makes them suitable as a low-effort initial building block for an MT quality estimation system.  den on the post-editors, who have to take a decision to discard the bad raw translation before translating the subtitle from scratch anyway. In order to reduce the effort post-editors have to spend on acceptance decisions and make subtitle post-editing a more pleasant experience, it would be desirable to predict the quality of a segment automatically given the input, the output and the models of the SMT system, a task that has gone under the name of conﬁdence prediction in the literature. While the SMT system itself internally scores alternative translations of each segment to ﬁnd the best one, raw SMT scores are not sufﬁcient as a conﬁdence measure. As conditional probabilities, they are not comparable across sentences. Furthermore, they are not properly normalised by the SMT decoder since performing the required normalisation would render decoding intractable. Us-  
 The set of reachable translations E (also re-  ferred to as the search space) in modern decoders  Modern Statistical Machine Translation  is based on a set of heuristics that deﬁne the set of  (SMT) systems make their decisions based  possible translation of each word or phrase (up to  on multiple information sources, which as-  a maximum limit) and specify the range of possi-  sess various aspects of the match between  ble reorderings of words or phrases during trans-  a source sentence and its possible trans-  lation. Assuming that the components of E have  lation(s). Tuning a SMT system consists in ﬁnding the right balance between these  been deﬁned, the actual tuning step of a SMT system consists in ﬁnding λ¯∗ that maximizes the em-  sources so as to produce the best possible output, and is usually achieved through  pirical gain G on a development set F = {(f , rf )} made of pairs of a source sentence f and corre-  Minimum Error Rate Training (MERT)  sponding reference translation(s) rf :  (Och, 2003). In this paper, we recast  the operations implied in MERT in the  λ¯∗ = arg max G(F ; λ¯)  (2)  terms of operations over a speciﬁc semir-  λ¯  ing, which, in particular, enables us to derive a simple and generic implementation of MERT over word lattices. 
 mining and computing infrastructure, which are  This paper describes efforts towards the development of an Arabic to Italian SMT system for the news domain. Since only very little parallel data are available for this language pair, we investigated both the exploitation of comparable corpora and pivot translation. Experimental evaluation was conducted on a new benchmark developed by extending two Arabic-to-English NIST evaluation sets. Preliminary results show potentials of both approaches with respect to performance achieved by a popular state-of-the-art Web-based translation service.  doubtlessly unaffordable by most research labs. Unfortunately, for most research labs, a major bottleneck that hinders the development of SMT systems for many language pairs is the lack of sufﬁcient amounts of parallel data. In fact, parallel data are a scarce resource even for many socially and economically relevant language pairs,2 such as Arabic and Italian. In this work, we report on our efforts to set-up SMT baselines for news translation from Arabic to Italian, for which only a small amount of parallel texts was available. To compensate for the lack of data, we investigated and compared two alternatives: exploiting a fair amount of comparable data, automatically collected from the Web, and pivot  
 sources and more time to process. More impor-  Statistical machine translation systems have greatly improved in the last years. However, this boost in performance usually comes at a high computational cost, yielding systems that are often not suitable for integration in hand-held or real-time devices. We describe a novel technique for reducing such cost by performing a Viterbi-style selection of the parameters of the translation model. We present results with ﬁnite state transducers and phrasebased models showing a 98% reduction of the number of parameters and a 15-fold increase in translation speed without any signiﬁcant loss in translation quality.  tantly, effort spent in handling large tables could likely be more usefully employed in more features or more sophisticated search processes. Additionally, this is the main restriction for the widespread application of SMT techniques in small portable devices like cell phones, PDAs or hand-held game consoles; one can imagine many scenarios that could beneﬁt from a lightweight translation device: tourism, medicine, military, etc. In this paper, we show that is possible to prune phrasetables by removing those phrase pairs that have little inﬂuence on the ﬁnal translation performance. Our approach consist in selecting only those phrase pairs extracted from the most probable segmentation of the training sentences.  
 the system, even if it does not improve the overall  Future improvement of machine translation systems requires reliable automatic evaluation and error classiﬁcation measures to avoid time and money consuming human classiﬁcation. In this article, we propose a new method for automatic error classiﬁcation and systematically compare its results to those obtained by humans. We show that the proposed automatic measures correlate well with human judgments across different error classes as well as across different translation outputs on four out of ﬁve commonly used error classes.  score? Does a worse–ranked system outperform a better–ranked one in any aspect?, etc. In order to answer such questions, a framework for human error analysis and error classiﬁcation has been proposed in (Vilar et al., 2006), where a classiﬁcation scheme based on (Llitjo´s et al., 2005) is presented together with a detailed analysis of the obtained results. The method has become widely used in recent years (Avramidis and Koehn, 2008; Max et al., 2008; Khalilov and Fonollosa, 2009; Li et al., 2009). Still, human error classiﬁcation is resource-intensive and might become practically unfeasible when translating into many languages. As for automatic methods, an approach for automatic identiﬁcation of patterns in translation out-  
 and Venugopal, 2006; Almaghout et al., 2010)  In this paper, we present a method to employ target-side syntactic contextual information in a Hierarchical Phrase-Based system. Our method uses Combinatory Categorial Grammar (CCG) to annotate training data with labels that represent the left and right syntactic context of target-side phrases. These labels are then used to assign labels to nonterminals in hierarchical rules. CCG-based contextual labels help to produce more grammatical translations by forcing phrases which replace nonterminals during translations to comply with the contextual constraints imposed by the labels. We present experiments which examine the performance of CCG contextual labels on Chinese–English and Arabic– English translation in the news and speech expressions domains using different data sizes and CCG-labeling settings. Our experiments show that our CCG contextual labels-based system achieved a 2.42% relative BLEU improvement over a PhraseBased baseline on Arabic–English translation and a 1% relative BLEU improvement over a Hierarchical Phrase-Based system baseline on Chinese–English translation.  augment nonterminals in hierarchical rules with syntactic labels extracted from target-side parse trees of the training corpus. These syntactic labels act as syntactic constraints on phrases replacing nonterminals during decoding. However, there are some problems which affect the performance of syntax-augmented systems. One problem is the strong syntactic constraints imposed by syntaxaugmented rules which restrict the search space of translation and prevent the system in many cases from ﬁnding good translations. Another problem is the sparse syntactic labels used in such systems, which cause the generation of low-probability, less reliable rules. This weakens the ability of the system to generalize. As a solution to these problems comes approaches which try to soften syntactic constraints imposed by labeled synchronous rules (Venugopal et al., 2009; Chiang, 2010) have been advanced. In this paper, we propose a method to label nonterminals in hierarchical rules with target-side syntactic contextual labels extracted using Combinatory Categorial Grammar (CCG) (Steedman, 2000). For each target-side phrase in the training corpus, our method uses CCG supertags assigned to its words to extract the left and right syntactic context, represented in the left and right contextual CCG categories of this phrase. Then we annotate  
 also can be successful for translation from English  The Scandinavian languages have an unusual structure of deﬁnite noun phrases (NPs), with a noun sufﬁx as one possibility of expressing deﬁniteness, which is problematic for statistical machine translation  into Swedish and Norwegian, and from Italian to Danish, using the same basic strategy as in Stymne (2009b). However, some small careful modiﬁcations to the original English-Danish preprocessing strategy were necessary.  from languages with different NP struc-  2 Deﬁniteness  tures. We show that translation can be improved by simple source side transformations of deﬁnite NPs, for translation from English and Italian, into Danish, Swedish, and Norwegian, with small adjustments of the preprocessing strategy, depending on the language pair. We also explored target side transformations, with mixed results.  In the Scandinavian languages there are two mechanisms for expressing deﬁniteness, by using a definite article or by using a sufﬁx on the head noun. These mechanisms can also be used in combination, so called double deﬁniteness. The distribution rules for these two mechanisms are quite strict in all Scandinavian languages, but they differ somewhat between them. The noun phrase re-  
 an existing (general-domain) system to the partic-  ular domain (Koehn and Schroeder, 2007). If the  This paper reports on the ongoing work fo-  data is not available at all, a possible solution is to  cused on domain adaptation of statistical  exploit publicly available data from the web.  machine translation using domain-speciﬁc  In this work, we present a strategy for crawling  data obtained by domain-focused web  domain-speciﬁc texts from the web and their ex-  crawling. We present a strategy for crawl-  ploitation for domain-adaptation in a phrase-based  ing monolingual and parallel data and their  statistical machine translation (PB-SMT) frame-  exploitation for testing, language mod-  work. At the current stage, we focus on two re-  elling, and system tuning in a phrase-  sources: in-domain parallel data for parameter tun-  -based machine translation framework.  ing and in-domain monolingual data for language  The proposed approach is evaluated on  model training. As part of our approach, we also  the domains of Natural Environment and  create domain-speciﬁc test sets. The evaluation is  Labour Legislation and two language  carried out on the domains of Natural Environment  pairs: English–French and English–Greek.  (env) and Labour Legislation (lab) and two lan-  guage pairs: English–French and English–Greek.  
 tation of corpus level statistics related to the joint  In extant phrase-based statistical machine translation (SMT) systems, the translation model relies on word-to-word alignments, which serve as constraints for the subsequent heuristic extraction and scoring processes. Word alignments are usually inferred in a probabilistic framework; yet, only one single best alignment is retained, as if alignments were deterministically produced. In this paper, we explore ways to take into account the entire alignment matrix, where each alignment link is scored by its probability. By comparison with previous attempts, we use an exponential model to compute these probabilities, which enables us to achieve signiﬁcant improvements on the NIST MT’09 Arabic-English translation task.  segmentation and alignment of source and target sentences. Unfortunately, generative models designed for this purpose (Marcu and Wong, 2002; Birch et al., 2006) fail to deliver good performance due to three key difﬁculties (DeNero et al., 2006). First, exploring the whole space of phraseto-phrase alignment is intractable, which makes phrase alignment a NP-hard (DeNero and Klein, 2008) problem. Second, including a latent segmentation variable in the model increases the risk of overﬁtting during EM training. Third, spurious segmentation ambiguity tends to populate the phrase table with more entries, each having too few translation options. A practical solution is to reconﬁgure the phrase alignment problem in terms of words instead of phrases: a ﬁxed segmentation, based on word boundaries, is used, and the resulting model is simpler to train using EM. Then, for each word-aligned sentence in the training corpus,  
 to make errors are thus severely restricted, mak-  ing the system more accurate at the cost of storing  We aim to replace the long and com-  huge amounts of ﬁxed chunks. Since a surface-  plicated, pipeline employed to produce  based system has no mechanism for generalizing  probabilistic phrasal bilexica with a the-  in a systematic way, this is a good work-around.  oretically principled, grammar based, approach. To this end, we introduce a  Although structured SMT systems are capable of making generalizations beyond the scope of  learning regime to learn a phrasal gram-  surface-based systems, it is also imperative to be  mar equivalent to linear transduction gram-  able to handle chunks of text. This is the cor-  mars. The stochastic version of this new  rect way to capture phenomena such as ﬁgures  grammar type also has the property that  of speech, whose translations go beyond the valid  the set of biterminals constitute a natural  generalizations of the language. However, distin-  probability distribution, making it similar  guishing between the phrasal biterminals that can  to a probabilistic translation lexicon. Since  be handled by generalization and the ones that can-  we learn a phrasal grammar, we are, in ef-  not, is a hard problem. Indeed, ﬁnding a can-  fect, learning a probabilistic phrasal bilex-  didate set of phrasal biterminals is a hard prob-  icon. As a proof of concept, we show that  lem. Na¨ıvely enumerating all possible phrasal en-  phrasal bilexica, induced in this manner,  tries found in a parallel corpus and determine their  can be used to improve the performance of  probability by relative frequency, is doomed to fail  a traditional phrase-based SMT system.  because of the sheer amount of possible phrasal  
