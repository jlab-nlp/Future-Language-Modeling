There are two ways of doing semantics on the derivation tree: (i) synchronous TAG as in (Abeille´, 1994), and (ii) ﬂat semantics as in (Joshi and Vijay-Shanker, 1999; Joshi and Kallmeyer, 2000). In this paper, I pursue the ﬂat semantics approach (also known as minimal recursion semantics), in which the main operation for semantic composition is the conjunction of the semantic representations associated with each elementary tree along with the uniﬁcation of variables contributed by each semantic representation. Doing ﬂat semantics on relative clauses is particularly interesting because it involves deﬁning an alternative semantic role for the relative pronoun to the phrase-structure based approach, in which the relative pronoun has been argued to be an operator that turns the relative clause into a function of a predicate type (Heim and Kratzer, 1998). In addition, it involves deﬁning a relationship between the head noun and the wh relative pronoun, which turns out to be non-trivial. 
 A number of formalisms have been proposed in order to restrict tree adjoining grammar (TAG) to be weakly equivalent to context free grammar (CFG): for example, tree substitution grammar (TSG), tree insertion grammar (TIG), and regular-form TAG (RF-TAG); in the other direction, tree-local multicomponent TAG (TL-MCTAG) has been proposed as an extension to TAG which is weakly equivalent to TAG. These formalisms have been put to use in various applications. For example, Kroch and Joshi (1987) and others use TL-MCTAG for linguistic description; Bod (1992) uses TSG, and Chiang (2000) uses TIG, for statistical parsing; Shieber (1994) proposes to use TSG for generating synchronous TAG derivations for translation, and Dras (1999) uses RF-TAG for the same purpose. Although it is understood that these formalisms are useful because they have greater strong generative capacity (henceforth SGC) than their weakly-equivalent relatives, it is not always made clear what this actually means: sometimes it is understood in terms of phrase structures, sometimes in terms of a broader notion of structural descriptions (so Chomsky (1963)). We take the latter view, and follow Miller (1999) in seeing phrase structures as just one of many possible interpretations of structural descriptions (alongside, for example, dependency structures). For Miller, structural descriptions themselves should never be compared (since they vary widely across formalisms), but only their interpretations. Thus SGC in the phrase-structure sense is one of several ways of testing SGC in the broader sense. However, not much eﬀort has been made to demonstrate precisely how formalisms compare in these other ways. In this paper we examine four formalisms—CFG, TIG, RF-TAG, and what we call component-local scattered context grammar (CL-SCG)—under four diﬀerent interpretations, and ﬁnd that TIG, RF-TAG, and CL-SCG all extend the expressivity of CFG in diﬀerent ways (see Figure 1). These results show that it is possible to make formally precise statements about notions of generative capacity other than weak generative capacity (henceforth WGC), as a step towards articulating desiderata of formal grammars for various applications.  string sets  CFG = TIG = RF-TAG = CL-SCG  tree sets (modulo projection)  TIG CFG = RF-TAG = CL-SCG  indexed string sets string relations  RF-TAG CL-SCG CFG = TIG  Figure 1: Summary of results. Edges denote strict inclusion (lower ⊂ higher); = denotes equality.  2. Deﬁnitions We assume familiarity with CFGs and TAGs, and proceed to deﬁne two restrictions on TAGs: Deﬁnition 1. A left (or right) auxiliary tree is an auxiliary tree in which every frontier node to the right (resp., left) of the foot node is labeled with the empty string. A tree insertion grammar (Schabes and Waters, 1995) is a TAG in which all auxiliary trees are either left or right auxiliary trees, and adjunction is constrained so that: • no left (right) auxiliary tree can be adjoined on any node that is on the spine of a right (left) auxiliary tree, and • no adjunction is permitted on a node that is to the right (left) of the spine of a left (right) auxiliary tree. c 2002 David Chiang. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 11–18. Universita´ di Venezia.  12  Proceedings of TAG+6  Deﬁnition 2. We say that a TAG is in regular form (Rogers, 1994) if there exists some partial ordering over nonterminal symbols such that if β is an auxiliary tree whose root and foot nodes are labeled X, and η is a node labeled Y on β’s spine where adjunction is allowed, then X Y, and X = Y only if η is a foot node. Thus adjunction at the foot node is allowed freely, adjunction at the middle of a spine is allowed only to a bounded depth, and adjunction at the root is not allowed at all.1 Next we deﬁne CL-SCG, ﬁrst introducing the notion of an indexed string, which we will make use of in several places in this paper. Deﬁnition 3. An indexed string is a pair (w; Iw), where w is a string and Iw is an equivalence relation over string positions of w. An indexed string n-tuple is an (n + 1)-tuple (w1, . . . , wn; Iw), where w1, . . . , wn are strings and Iw is an equivalence relation over string positions of the wi. We notate these equivalence relations using boxed indices. Deﬁnition 4. A local scattered context grammar2 is a tuple G = (N, T, P, S ), where • N and T are ﬁnite, disjoint sets of nonterminal symbols and terminal symbols, respectively, • S ∈ N is the start symbol, and • P is a ﬁnite set of productions of the form (A1, . . . , An) → (α1, . . . , αn; Iα) where n ≥ 1, Ai ∈ N and (αi, . . . , αn; Iα) is an indexed tuple of strings over (N ∪ T )∗. We write (γ; Iγ) ⇒G (δ, Iδ), where (γ; Iγ) and (δ; Iδ) are indexed strings, if and only if there exists a production (A1, . . . , An) → (α1, . . . , αn; Iα) ∈ P such that γ = γ0A1γ1 · · · Anγn where A1, . . . , An comprise an equivalence class of Iγ, and δ = γ0α1γ1 · · · αnγn, where any nonterminal instances in the γi whose corresponding instances in γ are equivalent under Iγ are also equivalent under Iδ, as are any nonterminal instances in the αi which are equivalent under Iα, and nothing else. Let IS be the equivalence relation on string positions of S that relates S to itself. Then ∗ L(G) = {w | (S ; IS ) ⇒G (w; Iw) for some Iw}. We say that a local scattered context grammar is component-local if for each production (A1, . . . , An) → (α1, . . . , αn; Iα) ∈ P, a nonterminal instance in αi and a nonterminal instance in α j are equivalent under Iα only if i = j. We call this restriction “component-local” by analogy with tree-local MCTAG (Weir, 1988), because a production simultaneously rewrites multiple nonterminals with multiple components, but all those nonterminals must have come from the same component. 3. The formalisms considered as string-rewriting systems Proposition 1. CFG, TIG, RF-TAG, and CL-SCG are weakly equivalent. Proof. The weak equivalence of TIG to CFG was shown by Schabes and Waters (1995). The basic idea is to ﬂatten each elementary tree into a CFG production, discarding every foot node, but adding a nonterminal to the left (right) of every node at which left-adjunction (resp., right-adjunction) is possible. 1. Note that this deﬁnition is stricter than Rogers’ original deﬁnition, which allows “redundant” elementary trees. His parsing algorithm does not produce all possible derivations under the original deﬁnition, but does under the stricter deﬁnition. 2. This deﬁnition is based on local unordered scattered context grammar (Rambow and Satta, 1999), but is simpliﬁed in two ways: ﬁrst, our scattered contexts are ordered rather than unordered; second, our productions explicitly specify which sets of nonterminals may be rewritten. We do not believe either of these simpliﬁcations aﬀects the results shown here.  Chiang  13  The weak equivalence of RF-TAG to CFG was shown by Rogers (1994). The basic idea is to break each  elementary tree into CFG productions, augmenting the nonterminal alphabet with a stack of bounded depth to  keep track of non-foot adjunctions.  For any CL-SCG, a weakly equivalent CFG can be obtained by a construction analogous to that for tree-  local multicomponent TAG (Weir, 1988). Given a CL-SCG G = (N, T, P, S ), let f be the maximum number of  components of (the left- or right-hand side of) any production in P, and let N  =(  f i=1  Ni  ×  {1,  .  .  .  f  }).  We  want  to break each production of P into its component CFG productions, using this augmented nonterminal alphabet to  ensure that all the component productions are used together. To do this, construct P from P as follows:  • For every nonterminal occurrence Ai on a left-hand side (A1, . . . An), replace Ai with (A1, . . . , An, i).  • For every nonterminal occurrence Ai on a right-hand side (α1, . . . , αn; Iα), if Ai is the ith member of an equivalence class of Iα whose members are A1, . . . , An, in that order, replace Ai with (A1, . . . , An, i).  Then construct P from P as follows: if (A1, . . . , An) → (α1, . . . , αn) ∈ P , then Ai → αi ∈ P for all i between 1 and n. Then the CFG (N , T, P , (S, 1)) is weakly equivalent to G.  4. The formalisms considered as tree-rewriting systems  CFG generates only local sets, whereas CL-SCG and RF-TAG generate some recognizable sets which are not local. However, any recognizable set can be made from a local set by projection of labels (Thatcher, 1967). If we factor out this distinction, then CL-SCG and RF-TAG are no more powerful than CFG: Proposition 2. For any RF-TAG (or CL-SCG) G, there is a CFG G and a projection of labels π such that T (G) = {π(t) | t ∈ T (G )}.  Proof. The constructions used to prove the weak equivalence of these formalisms to CFG also preserve trees, modulo projection of labels.  Proposition 3. TIG can generate a tree set which is not recognizable.  Proof. As was observed by Schabes and Waters (1995), the following TIG generates a non-recognizable set:  S  S  x  A  Sa  B  S∗ When the path set is intersected with {SA}∗S{BS}∗x, the result is {(SA)nS(BS)nx | n ≥ 0}, a non-regular set.  5. The formalisms considered as linking systems  We now deﬁne derivational generative capacity (henceforth DGC, introduced by Becker et al. (1992)), which measures the generative capacity of what Miller (1999) calls “linking systems.” Deﬁnition 5. We say that a grammar G index-generates an indexed string (a1 · · · an; Iw) (see Deﬁnition 3) if G generates a1 · · · an such that ai and a j are equivalent under Iw if and only if ai and a j are contributed by the same derivation step. The derivational generative capacity of a grammar G is the set of all indexed string sets indexgenerated by G. In this section we notate indexed strings by drawing links (following Joshi (1985)) between all the positions of each equivalence class. Thus the CFG X → aXb | index-generates the familiar-looking indexed string set       a a · · · a b · · · b b  .      14  Proceedings of TAG+6  We saw in the previous section that with respect to SGC (in the sense of phrase structures), TIG was more powerful than CFG, whereas RF-TAG and CL-SCG were no more powerful than CFG. With respect to DGC, the opposite obtains. Proposition 4. CFG and TIG are derivationally equivalent.  Proof. The construction given by Schabes and Waters (1995) preserves derivations, and therefore preserves indexed strings.  On the other hand, RF-TAG and CL-SCG both have greater DGC than CFG (and TIG). Moreover, they extend CFG in diﬀerent ways, because each is able to generate an indexed string set that the other is not. Lemma 5 (indexed pumping lemma). Let L be an indexed string set generated by a CFG (or CL-SCG). Then there is a constant n such that if (z; Iz) is in L and |z| ≥ n, then z may be rewritten as uvwxy, with |vx| > 0 and |vwx| ≤ n, such that for all i ≥ 1, there is an equivalence relation Izi such that (uviwxiy; Izi) is in L and Izi does not relate any positions in w to any positions in u or y.  Proof. The proof is analogous to that of the standard pumping lemma (Hopcroft and Ullman, 1979). However, since the grammar cannot be put into Chomsky normal form, we let n = mk instead of 2k, where k is the size of the nonterminal alphabet and m is the maximum number of symbols on any right-hand side. The key diﬀerence from the standard proof is the observation that since, for each i, the derivation of uviwxiy can be written as  S  ∗ ⇒  uAy  ∗ ⇒  uvi A xi y  ∗ ⇒  uviwxiy  for some nonterminal A, no position in w can be contributed by the same derivation step as any position in u or y. The generalization to CL-SCG is straightforward, since a CL-SCG G can be converted into a CFG G which generates the same trees. G will not generate the same indexed strings as G; nevertheless, the equivalence relations index-generated by G can only relate terminal instances which are ﬁrst cousins in the derived tree, so for i ≥ 1, it remains the case that no position in w is related to any position in u or y.  Proposition 6. The following indexed string set is index-generable by an RF-TAG but not by any CL-SCG:      L1  =     c  a  a ··· a  c  b ··· b  b  c    Proof. The following RF-TAG generates L1:  X  Y  c Y c a Y∗ b  c  But suppose L1 is index-generated by some CFG or CL-SCG G. For any n given by the indexed pumping lemma, let z = cancbnc satisfy the conditions of the pumping lemma. It must be the case that v and x contain only a’s and b’s, respectively, or else uviwxiy L1. But then u, w, and y would each have to contain one of the c’s, and since the c’s are all related, this contradicts the pumping lemma.  Proposition 7. The following indexed string set (consisting of a single string) is generable by a CL-SCG but not by any RF-TAG, nor indeed by any TAG:      L2  =     a  b  a  b  a  b    Chiang  15  Proof. The following CL-SCG generates L2: (S) → (aB 1 aB 1 aB 1 ) (B, B, B) → (b, b, b) But L2 cannot be generated by any TAG. In general, if a1 · · · b · · · a2 is index-generable by a TAG such that a1 and a2 are related, then either the tree that contributes a1 and a2 (call it βa) adjoins into the tree that contributes b (call it βb) such that its foot node dominates b, or else βb adjoins into βa. In the case of L2, suppose that the a’s are contributed by βa, and the b’s are contributed by βb. If βa adjoins into βb, its foot node must dominate both of the b’s, which is a contradiction; similarly if βb adjoins into βa.  6. The formalisms considered as local synchronous systems  In a local synchronous system (Aho and Ullman, 1969; Shieber, 1994; Rambow and Satta, 1996), two grammars are constrained to generate pairs of strings via isomorphic derivations (up to relabeling and reordering of sisters). Although translation via a synchronous grammar is not really an “interpretation” in Miller’s sense, nevertheless, because a synchronous derivation in eﬀect realizes a single derivation structure in two diﬀerent ways, we expect the set of string relations generated by a local synchronous system to reveal something about the relationship between derivations and strings that weak generative capacity does not. Deﬁnition 6. A synchronous CFG is a tuple G = (N, T, P, S ), where N, T , and S are as in ordinary CFG, and P is a set of productions of the form (A : A ) → (α : α ; Iα) where A, A ∈ N, α, α ∈ (N ∪ T )∗, and Iα is a bijection between nonterminal instances in α and nonterminal instances in α . We write (γ : γ ; Iγ) ⇒G (δ : δ ; Iδ), where (γ : γ ; Iγ) and (δ : δ ; Iδ) are indexed string pairs, if and only if there exists a production (A : A ) → (α : α ; Iα) ∈ P such that  γ = γ0Aγ1  γ = γ0A γ1  where A and A are related under Iγ, and  δ = γ0αγ1  δ = γ0α γ1  where any nonterminal instances in γ0, γ1, γ0, or γ1 whose corresponding instances in γ are equivalent under Iγ are also equivalent under Iδ, as are any nonterminal instances in α and α which are equivalent under Iα, and nothing else. Let IS be the equivalence relation on string positions of (S : S ) which relates both instances of S to each other. Then the weak generative capacity of G is the string relation  ∗ L(G) = {(w : w ) | (S : S ; IS ) ⇒G (w : w ; Iw)}.  The deﬁnition of synchronous TAG (Shieber, 1994) is analogous, but with bijections between adjunction sites instead of bijections between nonterminal instances; synchronous TIG and synchronous RF-TAG are just restrictions of synchronous TAG. The deﬁnition of synchronous CL-SCG is also analogous, but with bijections between equivalence classes of nonterminal instances instead of bijections between nonterminal instances. These four synchronous formalisms relate to each other in the same way as the linking systems of the previous section. Proposition 8. Synchronous CFG and synchronous TIG are weakly equivalent.  Proof. The construction given by Schabes and Waters (1995) preserves derivations, and therefore preserves string pairs.  Lemma 9 (synchronous pumping lemma). Let L be a string relation generated by a synchronous CFG (or synchronous CL-SCG). Then there is a constant n such that if (z : z ) is in L and |z| ≥ n and |z | ≥ n, then (z : z ) may be written as (uwy : u w y ), and there exist strings v, x, v , x , such that |vxv x | > 0, |vwx| ≤ n, |v w x | ≤ n, and for all i ≥ 0, (uviwxiy : u v iw x iy ) is in L.  16  Proceedings of TAG+6  Proof. The proof is again analogous to that of the standard pumping lemma: let G = (N, T, P, S ) be a synchronous CFG generating L. We choose n as the proof of the standard lemma would if the nonterminal alphabet were N × N. This guarantees the existence of a pair of corresponding paths in the derivation of (z : z ) such that the same pair of nonterminals (A : A ) occurs twice:  ∗  ∗  ∗  (S : S ) ⇒ (uAy : u A y ) ⇒ (u1vAxy1 : u1v A x y1) ⇒ (u1vwxy1 : u1v w x y1)  If we let u = u1v and y = xy1, and likewise u = u1v and y = x y1, then (z : z ) = (uwy : u w y ), and for all i ≥ 0, (uviwxiy : u v iw x iy ) ∈ L. The CL-SCG case is similar but quite messy. A CL-SCG derivation tree has the same height as its derived tree, minus one. Therefore we can choose n such that any derivation of (z : z ) where |z| ≥ n and |z | ≥ n must have a pair of corresponding paths such that the same set of nonterminals occurs twice (new material underlined for clarity):  ∗ (S : S ) ⇒ (u0A1u1 · · · Akuk · · · Anun : u0A1u1 · · · Ak uk · · · An un ) ∗ ⇒ (u0v1u1 · · · vk0A1vk1 · · · Anvknuk · · · vnun : u0v1u1 · · · vk 0A1vk 1 · · · An vk n uk · · · vn un ) ∗ ⇒ (u0v1u1 · · · vk0w1vk1 · · · wnvknuk · · · vnun : u0v1u1 · · · vk 0w1vk 1 · · · wn vk n uk · · · vn un )  If we let  u = u0v1u1 · · · vk−1uk−1 v = vk0v1vk1 · · · vk−1vk,k−1 w = vk0w1vk1 · · · wnvkn x = vk,k+1vk+1 · · · vnvkn y = ukvk+1uk+1 · · · vnun  u = u0v1u1 · · · vk −1uk −1 v = vk 0v1vk 1 · · · vk −1vk ,k −1 w = vk 0w1vk 1 · · · wn vk n x = vk ,k +1vk +1 · · · vn vk n y = uk vk +1uk +1 · · · vn un  then (z : z ) = (uwy : u w y ), and for all i ≥ 0, (uviwxiy : u v iw x iy ) ∈ L.  Proposition 10. The string relation  L3 = {(ambncndm : bnamdmcn)}  is generable by a synchronous RF-TAG but not by any synchronous CL-SCG.  Proof. The following synchronous RF-TAG generates L3:     A1   B 2 :      B2     A 1         A  A       a  : A∗ a  a  A∗ a   
 Supertagging was introduced for Lexicalised Tree Adjoining Grammar (LTAG) to reduce the number of elementary trees assigned to a word, thereby increasing parsing efﬁciency (Bangalore and Joshi, 1994). Bangalore and Joshi (1999) have shown that techniques used for POS-tagging can be applied successfully to the supertagging problem. Parsing efﬁciency is also an issue for Combinatory Categorial Grammar (CCG, Steedman (2000)) since many words can have many possible CCG categories. We have developed a supertagger for CCG, similar to the POStagger of Ratnaparkhi (1996). Maximum entropy models are used to estimate the probability that a word is assigned a particular category, given the local context. These probabilities are then used to select a sub-set of the possible categories for a word. The next section gives a brief introduction to CCG, and Section 3 describes the category set used in the experiments. Section 4 describes a “single-category” supertagger, and gives ﬁgures for its accuracy. Section 5 shows how the supertagger can be adapted to output more than one category per word, to produce a “multi-tagger”, and we show the effect the multi-tagger has on the speed and coverage of a CCG parser.  2. Combinatory Categorial Grammar  A grammar in CCG consists of a lexicon, which pairs words with lexical categories, and a set of combinatory  rules, which specify how categories combine. Categories are either atomic or complex. Examples of atomic  categories include (sentence), ¡ (noun), ¡£¢ (noun phrase) and ¢¤¢ (prepositio¦n¥ a§©l¨p hrase). Features¥ on the  category ca¥n be used to indica¦te¥ !"t§$y#%p es of sentence (or clause); for example:  (declarative),  (to-  inﬁnitival), (bare-inﬁnitival),  (adjectival).  Complex categories are functors which specify the type and directionality of the arguments, and the type of the result. For example, one of the categories for the verb likes speciﬁes that one noun phrase (¡£¢ ) is required to  the  ri&(g'$h)10t  of :=  t2 h¦e¥  §©v¨er4b3 ,¡5a¢7n69d8@o¡5n¢ e  to  the  left,  resulting  in  a  sentence  (as  in  John  likes  sweets):  Another category for likes speciﬁes that a to-inﬁnitival clause is required to the right of the verb (as in John likes  to ea &(t'$s)1w0 e:=ets2 )¦:¥ §©¨ 43  ¡5¢7698@2  ¦¥ A3 to  ¡£¢¦6  FuncB t) o!CrD cE a:t=eg2 oFr3ie¡£s¢7c6Ga8Hn2 aFl3so¡£e¢7x6 press modiﬁcation, as in the following adverbial category for really:  The following derivation shows how categories combine. This derivation uses only two combinatory rules:  forward application (I ) and backward application (P ).  Q$R©S"T UWVYX"` ` a  ` bc9VGd  e%R  VGXfe d%ghVYV9e%d  iqpsrutCviqpHwyxrtv"iqp@wrt "` v9iqpwyx"rut eyR1v9iqp@wrutF e%Rv9iqpHwyxrt $v9iqpHwrutF $viqp@wyx"iqp i¤p   tFD$v9i¤ p  t e%Rv9 iqp  tF " 1` v9iqp  t "`   t "` v9i¤p  Further combinatory rules are needed to deal with syntactic phenomena such as coordination and extraction.  c 2002 Stephen Clark. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 19–24. Universita´ di Venezia.  20  Proceedings of TAG+6  frequency cut-off 1 5 10 20  # cat types in cat-set 1,206 512 398 304  # cat tokens in 2-21 not in cat-set 0 1,157 (0.1%) 1,898 (0.2%) 3,190 (0.4%)  # sentences in 2-21  with missing cat  0  1,032  (2.6%)  1,667  (4.3%)  2,756  (7.0%)  # cat tokens in 00 not in cat-set 11 (0.02%) 49 (0.1%) 79 (0.2%) 123 (0.3%)  # sentences in 00  with missing cat  11 (0.6%)  44  (2.3%)  67  (3.5%)  104  (5.5%)  Table 1: Category coverage for seen data (sections 2-21) and unseen data (section 00) under various cut-offs  In the following object-extraction example, type-raising (I T) turns the atomic ¡£¢ category for John into a functor category looking for a verb-phrase to its right; forward composition (I B) then combines the type-raised category  with the category for likes:  e9S1V d%ghVYVeyd  eGS1X$e  Q$R©S"T  ` bDcGV9d  iqp riqpHv9iqp@wyx"r t "` x"iqp@w iqp  r tF "` v9iqp@w%xiqp  tx"r tv9i¤p@Tw     t "` x"iqp B  i¤p  iqpHv9iqp   Note that, in this paper, we assume type-raising is dealt with by the parser rather than at the level of the lexicon. Thus the supertagger does not deal with type-raised categories.  3. The Lexical Category Set  The category set used here has been derived from a treebank of CCG (normal-form) derivations, in which each word is tagged with a lexical category.1 The treebank, which we call CCG-bank, has been created by Julia Hock-  enmaier (Hockenmaier and Steedman, 2002a), and derived semi-automatically from the Penn Treebank (Marcus,  Santorini and Marcinkiewicz, 1993). Below is an example sentence from the CCG-bank, together with the lexical  categories.2  Pierre join 22  ¡ ¥  8HA¡ 3 b  director ¡  Vinken ¡ ¡ ¡£N¢¦ov69.829¢¤2 ¢¦F693 8H¡£¡£¢¢ 6 3  61 ¡ 8H¡  years ¡  old 2  ¥ A3 adj  ¡£¢6  3  ¡£¢  2 the3  ¡£¢8H¡ ¡£¢769698H¡  board ¡ as ¢¤¢8H¡£¢ 29 ¡¤£¢£  a ¡£¢8H¡  ¢  will  2  ¥ A3 dcl  ¡£¢¦698@2  nonexecutive ¡ 8H¡  ¦¥ A3 b  ¡£¢6  The category set was obtained from sections 2-21 of the CCG-bank (corresponding to the Penn Treebank sections), which contain 39 161 sentences, 903 661 category tokens, and 1 206 category types. Many of the category types occur only a few times in the data, and some arise through noise in the Penn Treebank, or errors introduced by the CCG extraction program. We therefore investigated using a subset of the 1 206 category types by applying a frequency cut-off. Table 1 shows how many category tokens in Sections 2-21 do not appear in the category set entailed by the cut-off, and also shows the percentage of sentences that have at least one category token not in the set. The same ﬁgures are given for Section 00, which has 1 900 sentences and 44 544 category tokens. The ﬁgures show that the size of the category set can be reduced by as much as 38 4, without greatly increasing the percentage of missing category tokens in the data (unseen and seen). The LTAG supertag set used by Bangalore and Joshi (1999) contained 300 supertags (for their WSJ experi- ments). To obtain a CCG category set as compact as that requires a cut-off as high as 20. However, Bangalore and Joshi took their supertags from the manually created XTAG grammar, which presumably contains a much cleaner set of supertags than an automatically extracted grammar. A more useful comparison is with the work of Chen and Vijay-Shanker (2000), who extract an LTAG automatically from the Penn Treebank. A number of strategies are used for extracting the supertags (referred to as tree  1. There is a distinction between lexical categories and categories created during a derivation. Since we are only interested in lexical categories in this paper, we may sometimes use category to mean lexical category. 2. The category for join has a PP-complement, which is arguably incorrect. Since the distinction between complements and adjuncts is not always reliably marked in the Penn Treebank, the procedure for identifying complements and adjuncts does lead to some erroneous data in the CCG-bank.  Stephen Clark  21  Condition wi is not rare wi is rare ¡ wi  Experiment 1 Contextual predicates wi X X is preﬁx of wi, ¢ X ¢¤£ 4 X is sufﬁx of wi, ¢ X ¢¤£ 4 wi contains a number wi contains uppercase character wi contains a hyphen ci¥ 1 X ci¥ 2 X wi¥ 1 X wi¥ 2 X wi¦ 1 X wi¦ 2 X  Experiment 2 ¡ wi ti X ti¥ 1 X ti¥ 2 X ti¦ 1 X ti¦ 2 X  Experiment 3 ¡ wi ti X ti¥ 1 X ti¥ 2 X ti¦ 1 X ti¦ 2 X wi¥ 3 X wi¦ 3 X ti¥ 3 X ti¦ 3 X  Experiment 4 ¡ wi ti X ti¥ 1 X ti¥ 2 X ti¦ 1 X ti¦ 2 X ci¥ 2ci¥ 1 XY wi¥ 2wi¥ 1 XY wi¦ 1wi¦ 2 XY ti¥ 2ti¥ 1 XY ti¦ 1ti¦ 2 XY  Table 2: Contextual predicates used in the experiments. The predicates shown for experiments 2, 3 and 4 are in addition to those used in 1; the rare word predicates are only used in experiment 1.  frames by Chen and Vijay-Shanker), but the size of the resulting sets range between 2 366 and 8 996 supertags. Chen and Vijay-Shanker also experimented with a cut-off, with a value of 3 reducing the size of the supertag sets by at least 1/2, and a cut-off of 9 producing sets ranging between around 800 and 1,800 supertags. These numbers suggest that the CCG category sets extracted from CCG-bank are more compact than Chen and Vijay-Shanker’s LTAG supertag sets (for equivalent cut-off values), although we should point out that CCG-bank has received a signiﬁcant amount of manual clean-up.  4. The supertagger  The supertagger uses conditional maximum entropy models to estimate the probability of words being assigned particular categories. We chose to implement a maximum entropy supertagger, rather than the HMM supertagger used by Bangalore and Joshi (1999), because of the ease with which additional features can be integrated into the model. The use of conditional models, rather than the generative model of the HMM, also makes it easy to deﬁne a “multi-tagger”, as we show in Section 5. The probability of a category, c, given a context, h, is deﬁned as follows:  p2 c¢ h6  2 61 e∑i λi fi § c¨ h© Zh  (1)  The functions fi 2 c h6 deﬁne “features” of the category and context, and Z 2 h6 is a normalisation constant. An example feature is as follows:  f j 2 c h6   
 We investigate learning dependency grammar from positive data, in Gold’s identiﬁcation in the limit model. Examples  ¢¡¤£¦¥¨§ are dependency trees. For this, we introduce reversible lexical dependency grammars which generate a signiﬁcant class  of languages. We have demonstrated that reversible dependency languages are learnable. We provide a  -time, in  the example size, algorithm. Our objective is to contribute to design and the understanding of formal process of language  acquisition. For this, dependency trees play an important role because they naturally appear in every tree phrase structure.  1. An identiﬁcation paradigm From Tesnie´re (Tesnie`re, 1959) seminal study, and from ideas of Mel’c˘uk (Mel’c˘uk, 1988), we propose a two tier communication process between two speakers, see Figure 1. Jean transmits a sentence to Marie. At the ﬁrst stage, Jean generates a structural sentence, like the following dependency tree  the rabbit runs fast  Then, Jean transforms it into a linear phrase, the rabbit runs fast, and send it to Marie. Now, Marie has to inverse ©¦ the two tier process of Jean. For this, she has (i) to recover the structural sentence from the linear sentence ( ), (ii) to build/update/improve her grammar in order to understand the message, and to generate other messages ¦( ). In the setting of natural language learning, parsers perform the ﬁrst task of Marie. Roughly speaking, © parsing corresponds to inverse , that is to compute ©¦ . We are investigating identiﬁcation, that is exact learning, of dependency tree languages. The leading idea is that data used to learn are dependency trees. Dependencies are semantic annotation by additional informations which facilitates the learning process. Such data are widely available and come from a lot of computational linguistic formal grammars such as LFG, TAG, categorial grammar and interaction grammar. The relations between those formalism are explained in the special issue of TAL (Kahane, 2000) on dependency grammars. The data available are positive examples of a language, that is a sequence of dependency trees. Our hypothesis  is that the computation of is reversible, that is the inputs can always be deduced from the outputs. So, identiﬁ  cation corresponds to inverse . For this, we give a sufﬁcient condition of reversibility on the grammars ( ). We show that the class of reversible dependency tree languages is efﬁciently identiﬁable in Gold’s model (Gold, 1967). That is, given a ﬁnite sequence of examples of a reversible language, we determine a reversible grammar that generates it. We refer to (Jain et al., 1999) for further explanations. Our study leans on the work of Angluin (Angluin, 1982) on learning languages produced by deterministic and reversibles ﬁnite automaton. It is also closely related to Sakakibara (Sakakibara, 1992) work on reversible context free grammars and to Kanazawa (Kanazawa, 1998) work on rigid categorial grammars.    PSfrag replacements Ssterunctetunrcael ©  Linear sentence   ¦ Ssterunctetunrcael © ¦  Figure 1: A two tier communication process ! c 2000 . Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 25–29. Universita´ di Venezia.  26  Proceedings of TAG+6  2. Lexical dependency grammar Following Dikovsky and Modina (Dikovsky and Modina, 2000), we present a class of projective dependency grammars which was introduced by Hays (Hays, 1961) and Gaifman (Gaifman, 1965). A lexical dependency grammar (LDG) is a quadruplet ¡£¢¥¤§¦¨¤©¤ , where ¢ is the set of terminal symbols, ¦ is the set of non-terminal symbols, ¦ is the start symbol, and © is the set of productions. Each production   is of the form  "! . . . !$#&%(' . . . '0) , where 1¦ , each !32 and '54 are in ¢768¦ . The terminal symbol % is called the head of the production. In other words, the head is the root of the ﬂat tree formed by the production right handside. Actually, if we forget dependencies, we just deal with context free grammars.  Example 1. The grammar ¥9@¡BAC%D¤FEHGI¤PACQGR¤§©¤F where © consists  S%(TE¥UV%(E Partial dependency trees are recursively deﬁned as follows. 1.  is a partial dependency tree generated by .  2. If . . .  . . . b . . . is a partial dependency tree generated by , and if   W@! . . . !X#Y%(' . . . '0) is a production of , then    . . . ! . . . !X#¥%`' . . . '5# . . . b . . .  is a partial dependency tree generated by .  A dependency tree generated by a LDG is a partial dependency tree of $d symbols. The language acb is the set of all dependency trees generated by .  Example 2. The language generated by of Example 1 is  in which all nodes are terminal  d  acb  9eAH%(E ¤§%`%`EVE ¤§%`%`%`E3EE ¤PfPfgfhG  Without dependencies, we recognize the context free language AH%piqEripsCt8uwv5G .  3. Reversible LDG  A LDG grammar is reversible if the conditions x8y , x and x of Figure 2 are satisﬁed. The class of reversible dependency tree languages is the class of languages generated by reversible LDG.  4. The learning algorithm  The learning algorithm works as follows. d  The input is a ﬁnite set   of positive examples which are de- d  pendency trees. Deﬁne TGb as the grammar constructed from all productions outputed by TGb(¤ , for each  e¨ . The function TG is described in Figure 3.    Stage 0  d 9 TG b .    Stage n+1 The grammar  is deﬁned by applying one of the rule of Figure 2.  i  Besombes and Marion  27  ¢¡ £ ¤¡ ¥£ x8y If    % and if   % , then "9 .  x If  §¦¨£© %¢ and if W¦© %¤ , then £ 9 , where £V¤ ¨¦ .  x If  §¦ a ©£¥ ¦ and if  a ©  , then £ 9 , where £V¤ ¦ .  We write ¦w%¤© for !  . . . ! #¥%¤"  . . . " ) . Figure 2: Reversibility conditions and reduction rules.  d Function TGb (¤§ Inputs :  is a dependency tree,  is a non-terminal symbol. if  T¢ then Output    if  9$#  . . . # # a %  . . . % )   then Take new ! ¤PfgfPfr¤ ! # and ' ¤gfPfPfg¤' )  Output   & §' (  )#  for 9 to & §' 0 )% for 9 to  ! . . . !$# a ' . . . '0) d do TG b 2§¤F!2 d do TG b 2 ¤' 2  d  dd  Figure 3: TGb(¤  recognizes exactly  , i.e. acb TG b (¤§ 9eAg`G  
 This paper proposes a syntactic framework that is appropriate for modeling human language processing. The  work moves from the largely held assumption that human language processing is incremental, and aims at exploring  the consequences of incrementality when the processing strategy is encoded in the operations of a formal system.  This makes a syntactic framework consistent with the Transparency Hypothesis (Berwick and Weinberg, 1984), in  that a syntactic theory must reﬂect the behaviour of the human syntactic processor in its formal operations.  Incrementality is a strategy that constrains the processor to analyse the input words from left to right, and  to carry out a semantic interpretation of the partial structures (Marslen-Wilson, 1973). A parsimonious version  of incrementality is what we call strong incrementality, which restricts incrementality to the case in which the  parser maintains a fully connected tree at each state (cf. (Stabler, 1994)). There have been some proposals in the  literature that claim that structure building occurs incrementally both in derivation and in parsing. Many aspects of  the CCG formalism are grounded on incrementality (Steedman, 2000), and Phillips has provided a large amount  of evidence that incrementality in the derivation process solves many problems in the deﬁnition of constituency  (Phillips, 1998). The incremental nature of the formalism is a major issue in Milward’s proposal of a dynamic  dependency grammar (Milward, 1994). In this case, the syntactic formalism is expressed in the terms of a dynamic  system, that is a system evolving in time through a number ¢of¡¤£¦s¥teps. ¦A¡ dynamic grammar views the syntactic  process as a sequence of transitions between input string 1. Thus, it naturally implements a the input string from©¡¤t£¦h¥e beginning to the i-th  adjacent states  and while mov§in¡ g from left to right in the  strongly incremental strategy. The state word, and at each step the parser tries to  is a partial tree attach the word ¨  th¡ at spans into the  current partial tree , also called the left context. So, the deﬁnition of a formal dynamic grammar involves the  deﬁnition of the shape of the partial trees and the formal operations that extend these partial trees.  Tree Adjoining Grammar (Joshi, Levy and Takahashi, 1975) is a well studied framework, that can realize a  range of syntactic theories, provided a few constraints on trees and formal operations are satisﬁed. The goal of  this paper is to devise a dynamic version of TAG, that retains the formal characteristics of the framework in terms  of basic machinery, while constraining derivation and parsing to be strongly incremental. Of great interest for  implementing incrementality are the wide domain of locality introduced by TAG rules, the adjunction operation,  and the linguistic motivation for TAG elementary trees. Some of these considerations are shared by (Frank and  Badecker, 2001), as points of strength for TAG in language production. Let us consider them in turn.  1) Wide domain of locality  In an empirical study on the Penn Treebank, aimed to discover the amount of non-lexical information which is  necessary to implement a fully connected incremental processing strategy, we simulated the sequence of processing  steps of an incremental parser on the treebank (Lombardo and Sturt, 2002). Given full connectedness, each new  word must be attached to the preceding left context via some syntactic structure, called “connection path”. In the  simulation, connection paths often needed to be multiply levelled trees. See, e.g., the structure that connects the  word “the” to the left context in: [ [ John] [¦ [ thinks] [ [ [ the]]]]]  However, the form of these connection paths is quite predictable from linguistic observations. The use of multiply  levelled elementary trees in TAG is an immediate encoding of this requirement. We also found that the introduc-  tion and co-indexation of traces needed to be carefully designed for the incremental setting. In TAG, given the  wide domain of locality, ﬁller and trace are both present in the same elementary tree, and then they may be sep-  arated through a number of adjunctions. This solution is particularly interesting for incrementality, provided that  adjunctions are constrained to occur through the insertion of lexical material only to the right of the word whose  elementary tree introduced the ﬁller-trace pair.  1. Here we have used the generic term “syntactic process” to indicate both derivation and parsing. In fact, in a dynamic grammar both processes share the same mechanisms, and we will use the two terms interchangeably.  c 2002 Vincenzo Lombardo and Patrick Sturt. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 30–39. Universita´ di Venezia.  Lombardo and Sturt  31  2) Adjunction The adjunction operation provided by the TAG formalism is vital for the incremental processing of modiﬁers. In fact, in many cases the correct attachment of a word to the current partial tree requires the guess of an unknown number of left recursive structures. The adjunction operation permits the progressive extension of recursive structures, as they are required to process the input string. A typical use of left recursive structures in English is in possessive constructions. In a sentence like “Mary hated the salesman’s assistant’s hairstyle”, during incremental processing it is impossible to guess in advance the depth of the NP immediately dominating “the salesman”. One solution is to attach this NP immediately as the object of “hated”, and then to embed this NP via adjunction when the possessive is processed. This operation can be repeated an arbitrary number of times to produce an arbitrarily embedded structure (cf. (Thompson, Dixon and Lamping, 1991)). 3) Linguistically motivated elementary trees TAG, and in particular Lexicalized TAG (Schabes, Abeille´ and Joshi, 1988), constrain the form of the elementary trees to be linguistically motivated according to the number and the type of arguments required by some lexical anchor. This is a desirable property also for incremental processing, since in head-initial languages, items that are to occur on the right are often predicted from anchors on the left. Here we are not referring to any processing model, but to a basic requirement for a syntactic formalism that supports incrementality. We must also notice that linguistic constraints are not always enough to guarantee the full connectedness of the partial tree. It can be that the argument structures of two adjacent lexical anchors cannot be directly combined, and so we need some extra structure to guarantee full connectedness. Take again the previous example “John thinks the ...”: the lexical entry for “thinks” is an initial tree that predicts an S node on its right, marked for substitution; the lexical entry for “the” is an auxiliary tree rooted in NP; the extra structure provides the edge between S and NP, and must be included in one of the two elementary trees, even though this edge is not directly licensed by any of the lexical anchors in the partial tree. As a consequence, an elementary tree of a dynamic TAG may be larger than the argument domain of that elementary tree’s anchor. This is because extra structure is needed to guarantee connectivity (in the next section we will see a few examples of this case). This paper describes the major issues of a dynamic version of TAG (called DV-TAG), which is suitable for incremental processing. We ﬁrst describe the consequences that incrementality bears upon the shape of elementary trees and on the form of the attachment operations, together with some deﬁnitions. Then we describe the derivation process that produces the so-called derived tree. Finally, in order to illustrate the functioning of the formalism, we provide a few meaningful linguistic examples. 2. Dynamic Version of TAG (DV-TAG) In this section we describe the basic machinery of DV-TAG. We start with elementary trees, and then we move to the attachment operations, Substitution and Adjunction. 2.1. Elementary trees A DV-TAG grammar consists of elementary trees, divided into initial trees and auxiliary trees. Most of the deﬁnitions provided by TAG are also applicable to elementary trees in DV-TAG. The structure of elementary trees in DV-TAG is constrained by a number of syntactic modules (X-bar theory, case theory, thematic relations theory, some empty category principle(s), see (Kroch and Joshi, 1985)). In particular, we assume that each node has a distinguished head daughter, that each elementary tree is associated with a lexical anchor, and that the elementary tree includes the argument structure of the anchor. Long-distance dependencies have to be stated locally, in a single elementary tree. Auxiliary trees are minimal recursive structures with root and foot nodes identically labelled. The major difference in comparison with TAG is that the shapes of the elementary trees must be constrained in order to permit the left-to-right derivation (and parsing) of all and only the legal structures. The linguistic motivations mentioned above may not be enough to constrain the elementary trees in a way that guarantees the construction of a fully connected structure 2. This motivates the addition of extra information to elementary trees. Here we are not claiming that additional information is in some form, and is added on-the-ﬂy while processing, but that elementary trees in DV-TAG are larger than the corresponding elementary trees in TAG. Even if we ground this requirement on the empirical observations addressed in the Penn Treebank (Lombardo and Sturt, 2002), we can think of some principled way to constrain the extension of elementary trees beyond the modules above. A similar solution, devised again in the context of incremental processing, is the type raising 2. Remember that a fully connected structure is required by incremental processing.  32  Proceedings of TAG+6  ¡¢£¥¨§©   £!©"#$&%'  ¡¡¢¤£¦¥¨§©  ¡¢£¥¨§©  ()¢¤£0©"1$2%'   £A!©"#$&%'  ¡¡¢¤£¦¥¨§© (¡£0©"1$2%' 3 £546  7¢ 3 £546 ()¢¤£!©"1$&%'  7¢£8¢@9A9B!  (¡£!©"1$&%' G3 £546 £ED  ¡7¢£8¢@9A9BC £CF  Figure 1: Initial trees and derivation for “John thinks Peter ...”. Node labels include the head word. The symbol “$” indicates “marked for Substitution”. (a) The initial trees for “John” and “thinks”. (b) The derived tree for “John thinks Peter”.  operation provided in the CCG theory (Steedman, 2000). We have not yet addressed such a problem at this stage  of development. What may happen is that some portions of this extra information can overlap with the arguments 
In spontaneous speech, English speakers produce relative clauses whose structures violate wh-island constraints but where resumptive pronouns appear in place of the gap/trace normally found in a relative clause. It is difﬁcult to explain these island-violation remedying resumptive pronouns in a model of sentence production where only grammatically-licensed trees are available to the speaker’s production system. This paper argues that the only explanation for these island-violating relative clauses in such a model of sentence production is that they are fully grammatical constructions, whose marginal status reﬂects non-syntactic factors. The paper is structured as follows. The next section outlines the problem that resumptives in island contexts present for a model of sentence production using TAG. Section 3 argues that the problem can be resolved given the existence of resumptive pronouns in English in non-island contexts. Section 4 discusses two potential difﬁulties for the analysis. Section 5 outlines some possible implications of the analysis for the typology of relative clauses in general. Finally, Section 6 concludes with suggestions for further investigation.  2. The problem of “remedying” island violations  2.1. Accounting for island violations in TAG Frank (2002) presents a conception of grammar applying the machinery of tree adjoining grammar within an independent theory of well-formedness of grammatical structures. The domains over which any structural dependencies are expressed are the elementary trees in the system. The TAG operations of substitution and adjunction can then apply to any of the set of well-formed trees. No movement transformations outside the domain of the elementary tree are possible in this system. In order to account for the well-known phenomena of island violations, rather than stating principles constraining wh-movement (or the location of traces of this movement), Frank’s theory instead rules out the elementary and auxiliary trees needed to derive structures with island-violations primarily based on two independent principles: 1) the Theta Criterion: only nodes that are selected for can appear in an elementary tree; and 2) in order to obey the requirement that derivation trees should be context-free, a well-formed derived auxiliary tree rooted in cannot be derived via substitution of a tree with an foot node into a tree with an root node. For example, extraction from an NP complement, as in (1), is ruled out because in addition to the legitimate elementary tree rooted in wrote in (2), it would require the impossible auxiliary tree in (3).  (1) *What book did you hear [the claim that Soﬁa wrote t ]?  ¡  ¡  (2)  CP  NP  C’  ¢  what book that Soﬁa wrote £ ¢  ¤ This paper has beneﬁted from the useful comments of Tonia Bleam, David Chiang, Na-Rae Han, Elsi Kaiser, Kieran Snyder, and two anonymous reviewers, among others. ¥ c 2002 Cassandre Creswell. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 40–47. Universita´ di Venezia.  Creswell  41  (3)  C’  C  IP  did NP  VP  you V  NP  hear the claim C’  The latter is ill-formed because neither of the trees that make it up—an elementary tree rooted in C’ and anchored by hear and an elementary DP tree anchored by claim taking a C’ complement—are themselves a C’ auxiliary tree. Extraction from wh-islands, relative clauses, adverbial adjuncts, and sentential or DP subjects are ruled out in similar ways.  2.2. Remedying island violations with resumptive pronouns In unplanned speech speakers of English sometimes produce utterances with a resumptive pronoun in place of the gap or trace which would be an island-violation if a wh-word had been extracted from that position, as shown in the naturally-occuring examples of extraction from a wh-island, (4a), a relative clause, (4b), an adverbial clause, (4c), and a subject in (4d). (4) a. There are always guests who I am curious about what they are going to say. (Prince (1990)’s 3a) b. That asshole X, who I loathe and despise the ground he walks on, pointed out that...(Prince (1990)’s 5a) c. Apparently, there are such things as bees in the area which if you’re stung by them, you die. (Prince (1990)’s 5b) d. You have the top 20% that are just doing incredible service, and then you have the group in the middle that a high percentage of those are giving you a good day’s work... (http://www.ssa.gov/history/WEIKEL.html) Kroch (1981) argues that such resumptive pronouns are the result of a processing effect. Although a dependency between a wh-word and a gap in these positions is ungrammatical, as an artifact of how sentence production proceeds, forms like those above are uttered. Kroch explicitly rejects any possibility of a formal solution within the grammar to account for these forms. He uses an incremental model of sentence production in which a wh-element is produced in a fronted position before the entire sentence has been planned out, allowing the utterance of ﬂuent speech. Rather than having to wait to create the entire sentence in which the distance between the wh-element and its gap is potentially unbounded, the speaker can begin speaking while the remainder of the structure is being constructed. This wh-element is adjoined to a full clause where it appears again in its base-generated position. When the repeat occurrence is reached in the course of production, it is not pronounced. If the sentence produced has an island-violation, then a pronoun or NP is inserted in the base-generated position. In this model, resumptive pronouns are expressly inserted as a “last resort” to avoid violating the ECP by leaving an ungoverned trace. The resumptive pronoun remedies the violation because an empty category will no longer be present.  2.3. Sentence production using TAG In a TAG-based model of incremental sentence production (Ferreira, 2000; Frank and Badecker, 2001), elementary trees are the basic “building blocks” of generation. Selecting a lexical item corresponds to selecting an elementary tree. The grammatical properties of that tree’s structure are a natural domain for explaining the planning commitments a speaker makes while producing a sentence. The grammar provides a set of trees to work with, and as sentence planning proceeds, the speaker attempts to select the correct trees to encode the meaning she wishes to express and to put them together in a particular conﬁguration. The only source of production errors is incorrectly combining trees; ill-formed elementary (and auxiliary) trees never arise. To produce a structure with a wh-dependency, the speaker selects the elementary tree headed by the main verb with the appropriate wh-element  42  Proceedings of TAG+6  fronted. In a structure with a long-distance dependency, additional material can be adjoined in between the whelement and the remainder of the clause it originated in, but the commitment to the main clause structure has already been made.1 In this model of production where we assume that a speaker only has grammatical resources with which to work, we can not use Kroch (1981)’s explanation of the appearance of resumptive pronouns in island-violation contexts. The resources needed to produce island-violating structures are not available in the grammar that licenses the set of tree building blocks. On the face of it then, it seems that the existence of resumptive pronouns in island violating contexts would prove devastating for this model of sentence production. Based on the assumptions that 1) the processing system has only grammatically-licensed trees with which to create larger structures and 2) the structures needed to extract from island-violation contexts are not grammatically-licensed, speakers could not be remedying violations that should not even be created given their underlying grammars. As we will argue in the following section, however, the underlying grammar of English speakers independently requires the resources needed to produce these forms. Hence, we can preserve both the assumptions above and still have a grammar that characterizes all the structures that English speakers use.  3. Resumptive pronouns as grammatical resource 3.1. Resumptives in non-island contexts  Resumptive pronouns appear in relative clauses in English in non-island violation contexts, as in (5), from Prince (1990).  (5) a. My son, God bless him, he married this girl which I like her. (Prince’s 28a) b. If there’s any message that she can forward it to us, then...(Prince’s 15b) c. You get a rack that the bike will sit on it. (Prince’s 15c) d. I have a friend who she does all the platters. (Prince’s 4c)  Prince presents an analysis of the function of this type of resumptive pronoun, claiming that they serve as normal discourse pronouns rather than as a type of bound pronoun or trace. These pronouns appear in contexts where the relative clause is being used simply to predicate additional information about a discourse entity evoked by the head noun, not to assist the hearer’s identiﬁcation of the referent evoked by the head noun. For example, they are far more common with non-restrictive relatives and relatives modifying indeﬁnite NPs. Additional evidence she presents for the “discourse pronoun” analysis are cases where a co-referential demonstrative or full NP or even a non-coreferential (but related) NP appears instead of a resumptive pronoun, as in (6) (Prince’s 34(a-d)).  (6) a. I had a handout and notes from her talk that that was lost too. b. He’s got this lifelong friend who he takes money from the parish to give to this lifelong friend. c. I have a manager, Joe Scandolo, who we’ve been together over twenty years. d. You assigned me to a paper which I don’t know anything about the subject.  In order to produce relative clauses like these, speakers must be using structures like those in (7).  (7) a.  NP  NP*  CP  NP ¢¡  C’  C  IP  null NP£ ¡  VP  V¤ NP¥ ¡  1. This does not rule out the possibility that the production process could be in some sense non-deterministic. That is, when there is more than one grammatical way to encode the meaning to be expressed, the speaker may be able to retain and manipulate more than one possible main clause tree during production. The possibility of a non-deterministic production model is relevant to the discussion in Section 5 below.  Creswell  43  b.  NP  NP*  CP  NP  C’  C  IP  that NP£ ¡  VP V¤ NP¥ ¡  Here NP would have features requiring the substituting NP to be a wh-word. But NP£ and NP¥ could have any NP substituted within them, including a pronoun coreferential with the NP to which this tree adjoins.  3.2. Generating resumptives in island contexts  Because the production system necessarily allows relative clause auxiliary trees like those in (7), we can now explain where speakers ﬁnd the grammatical resources to produce relative clauses with island-violating resumptive pronouns. The trees in (7) are projections of a head verb. As such any ﬁnite verb can project this (or a related) structure. Any legitimate auxiliary tree can be adjoined into it. There is no syntactic dependency between the relative pronoun and any of the lower NPs. Therefore, there needs to be no tree at any point in the derivation that reﬂects such a local dependency. For example, in order to derive (8a), we now only need a relative clause auxiliary tree with die as its head, into which we substitute which and you, as in (9b).  (8) a. bees which if you’re stung by them, you die  
 Simon Fraser University  1. Introduction This paper describes a lexicalized tree adjoining grammar (LTAG) based parsing system for Korean which combines corpus-based morphological analysis and tagging with a statistical parser. Part of the challenge of statistical parsing for Korean comes from the fact that Korean has free word order and a complex morphological system. The parser uses an LTAG grammar which is automatically extracted using LexTract (Xia et al., 2000) from the Penn Korean TreeBank (Han et al., 2002). The morphological tagger/analyzer is also trained on the TreeBank. The tagger/analyzer obtained the correctly disambiguated morphological analysis of words with 95.78/95.39% precision/recall when tested on a test set of 3,717 previously unseen words. The parser obtained an accuracy of 75.7% when tested on the same test set (of 425 sentences). These performance results are better than an existing off-the-shelf Korean morphological analyzer and parser run on the same data. In section 2, we introduce the Korean TreeBank and we discuss how an LTAG grammar for Korean was extracted from this TreeBank. Also, we discuss how the derivation trees extracted from the TreeBank are used in the training of the statistical parser. Section 3 presents the overall approach of the morphological tagger/analyzer that we use in the parser. A detailed discussion about the parser is presented in section 4. This section also presents the method we used to combine the morphological information into the statistical LTAG parser. We also provide the experimental evaluation of the statistical parser on unseen test data in section 4.  2. Automatically Extracted LTAG Grammar for Korean  In this section we describe the Penn Korean TreeBank and the nature of the extracted LTAG grammar from this TreeBank.  2.1. Korean TreeBank  The LTAG grammar we use in the parser is extracted using LexTract (Xia et al., 2000) from the Penn Korean  TreeBank. The derivation trees obtained by using LexTract on the Treebank are used to train the statistical parser.  The TreeBank has 54,366 words and 5,078 sentences. The annotation consists of a phrase structure analysis for  each sentence, with head/phrase level tags as well as function tags (e.g., -SBJ, -OBJ) and empty category tags for  traces (*T*) and dropped arguments (*pro*). Each word is morphologically analyzed, where the lemma and the  inﬂections are identiﬁed. The lemma is tagged with a part-of-speech (POS) tag (e.g., NNC: noun, NPN: pronoun,  VV: verb, VX: auxiliary verb), and the inﬂections are tagged with inﬂectional tags (e.g., PCA: case, EAU: inﬂection  on verbs followed by an auxiliary verb, EPF: tense, EFN: sentence type). Example TreeBank trees are given in  Figure 1. The ﬁgure on the left is an example of a bracketed structure for a simple declarative with canonical  ¢¡¤£ subject-object-verb order. The ﬁgure on the right is an example with a displaced constituent. In this example, the  object NP ‘  ’ appears before the subject, while its canonical position is after the subject. The sentences used  to illustrate bracketing structures in Figure 1 are romanized, glossed and translated in the following examples:  (1) a. Cey-ka kwanchuk sahang-ul pokoha-yess-supnita. I-Nom observation item-Acc report-Past-Decl ‘I reported the overvation items.’  b. Kwenhan-ul nwukwu-ka kaci-ko  iss-ci?  authority-Acc who-Nom have-AuxConnective be-Int  ¥  ‘Who has the authority?’  We would like to thank Fei Xia for the use her LexTract package without which we could not have started this research.  We are also indebted to Martha Palmer, Aravind Joshi and David Chiang for their helpful suggestions.  ¦ c 2002 Anoop Sarkar and Chung-hye Han. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and  Related Frameworks (TAG+6), pp. 48–56. Universita´ di Venezia.  Sarkar and Han  49  ¡ (S (NP-SBJ /NPN+ /PCA)  ¢¤£ (VP (NP-OBJ /NNC  ¥§¦ £ /NNC+ /PCA)  ¨¤©   /VV+ /EPF+  /EFN)  ./SFN)  ¢¡ £  ¤ ¡ (S (NP-OBJ-1 (S (NP-SBJ  /NNC+ /PCA) /NPN+ /PCA)  ! ¡¤ © (VP (VP (NP-OBJ *T*-1) /VV+ /EAU)  /VX+ /EFN))  ?/SFN)  Figure 1: Example TreeBank Trees  2.2. LTAG formalism  LTAGs are based on the Tree Adjoining Grammar formalism developed by Joshi and his colleagues (Joshi, " # Levy and Takahashi, 1975; Joshi and Schabes, 1997). The primitive elements of an LTAG are elementary trees which are of two types: initial trees ( trees) and auxiliary trees ( trees). Initial trees are minimal linguistic structures that contain no recursion. They include trees containing the phrasal structures of simple sentences, NPs and $ so forth. Auxiliary trees represent recursive structures, which are adjuncts to basic structures, such as adverbials, adjectivals and so forth. Auxiliary trees have unique leaf node, called the foot node ( ), which has the same syntactic category as the root node. Each elementary tree is associated with a lexical item (anchor) and encapsulates % all arguments of the anchor, possessing an extended domain of locality. Elementary trees are combined by two operations: substitution and adjunction. In the substitution operation, a node marked for substitution ( ) in an elementary tree is replaced by another elementary tree whose root category is the same as the substitution-marked node. In an adjunction operation, an auxiliary tree is inserted into an initial tree. The root and the foot nodes of the auxiliary tree must match the node label at which the auxiliary tree adjoins. The combination of elementary trees produces two structures: derived and derivation trees. Derived trees correspond to phrase structure representation and derivation trees are a record of the history of the combination process.  2.3. Extracted LTAG grammar  We use LexTract (Xia et al., 2000) to convert the phrase structure trees of the Korean TreeBank into LTAG  derivation trees. Each node in these derivation trees is an elementary tree extracted from the Korean TreeBank by  LexTract. The elementary trees of the LTAG Korean grammar are exactly the set of elementary trees used in the  derivation trees obtained using LexTract. For example, the elementary trees extracted from the TreeBank bracketed  structures in Figure 1 are given in Figure 2. The entire extracted grammar contains 632 elementary tree template  types and 13,941 lexicalized elementary tree types (Xia et al., 2001).  As mentioned earlier, in addition to the elementary trees, LexTract produces derived and derivation trees  ' &(0)2( 1 4 3 4 & for each TreeBank tree. For instance, for the second TreeBank tree in Figure 1,  and  trees are each  &6587@9 ACD B &6587@9 substituted into  tree, and tree is adjoined onto the VP node of  tree. This produces the derived and  derivation trees in Figure 3.  3. TreeBank-trained Morphological Tagger/Analyzer Korean is an agglutinative language with a very productive inﬂectional system. This means that for any NLP application on Korean to be successful, some amount of morphological analysis is necessary. Without it, the development of a statistical based parser would not be feasible due to the sparse data problem bound to exist in the training data. To avoid this problem in our parsing system, we use a morphological tagger/analyzer. This tagger/analyzer also performs statistical disambiguation and it was trained on 91% of the Korean TreeBank. The tagger/analyzer takes raw text as intput and returns a lemmatized disambiguated output in which for each word, the lemma is labeled with a POS tag and the inﬂections are labeled with inﬂectional tags. This system is based on a simple statistical model combined with a corpus-driven rule-based approach, comprising a trigram-based tagging component and a morphological rule application component.  50  Proceedings of TAG+6  NP NPN NP NPN ¤  NP  NP  S  NNC NP* ¢¤£  NNC ¥§¦  NP%  VP  %NP VV  ¨¤©  NP  VP  NNC ¢¡  VP* VX !  S  NP%  S  NP%  VP  NP V *T* ¡¤  Figure 2: Some examples of Extracted Elementary Trees  S  " ¡¤  NP  S  NNC  ¢¡ NP  VP  NPN  VP  VX  ¤ NP V !  *T* ¡¤ (a) Derived tree  " ¢¡ (1) " ¤ (2.1) # ! (2.2) (b) Derivation tree  Figure 3: Extracted Derived and Derivation Trees  Sarkar and Han  51  The tagger/analyzer follows several sequential steps to label the raw text with POS and inﬂectional tags. After tokenization (mostly applied to punctuations), all morphological contractions in the input string are uncontracted (STRING CONVERSION). The known words are then tagged with tag sequences of the form POS + inﬂectionaltag (e.g., NNC+PCA, VV+EPF+EFN) extracted from the TreeBank, and unknown words are tagged with NNC (common noun) tag, NNC being the most frequent tag for unknown words (MORPH TAGGING). Tags for unknown words are then updated using inﬂectional templates extracted from the TreeBank (UPDATE TAGGING). And then using the inﬂection dictionary and stem dictionary extracted from the TreeBank, the lemma and the inﬂections are identiﬁed, splitting the inﬂected form of the word into its constituent stem and afﬁxes (LEMMA/INFLECTION IDENTIFICATION), creating the ﬁnal output. This process is summarized in Figure 4. The proposed approach to morphological analysis is different from other approaches in that the tagging phase precedes the morphological analysis phase. This allows morphological analysis to be done deterministically through using the information obtained from tagging. An example input to the tagger/analyzer and the ﬁnal output are shown in Figure 5. INPUT Tokenization  String Conversion  Morph Tagging  Update Morph Tagging  Lemma/Inflection Identification  OUTPUT  Figure 4: Overview of the Tagger/Analyzer  Inpu7 t:&65 ¡&(£¤¢¥§¦ 5 )2¨ 1 ¤©A  & )D  ¤¦ 3C9 5 .  7 &65 Output: /NPN+ /PCA  ¡ &(£¤¢¥  /NNC  ¦ 5 )2¨ 1 /NNC+ ¤©A  /PCA    &  ) 5 /VV+ AD  /EPF+ ¤¦  3C9 5  ./SFN  Figure 5: Input and output from the morphological tagging phase The performance of the morphological analyzer/tagger has been evaluated on the 9% of the Treebank. The test set consists of 3,717 word tokens and 425 sentences. Both precision and recall were computed by comparing the morpheme/tag pairs in the test ﬁle and the gold ﬁle. The precision corresponds to the percentage of morpheme/tag pairs in the gold ﬁle that match the morpheme/tag pairs in the test ﬁle. And the recall corresponds to the percentage of morpheme/tag pairs in the test ﬁle that match the morpheme/tag pairs in the gold ﬁle. This approach yielded a precision/recall score of 95.79/95.39%. An off-the-shelf morphological analyzer/tagger (Yoon et al., 1999) was tested on the same test set. This system is reported to have obtained 94.7% tagging accuracy on a test set drawn from the same corpus as it was trained on. For the sake of fair comparison, the output of the off-the-shelf tagger was converted to look as close as possible  52  Proceedings of TAG+6  precision/recall (%)  Treebank trained 95.78/95.39  Off-the-Shelf  29.42/31.25  Table 1: Evaluation of the Morphological Analyzer/Tagger  to the Treebank trained analyzer/tagger output, including the tagset. However, not all tokenization mismatches in the off-the-shelf system could be resolved. The results (in Table 1) show, not surprisingly, that better performance on test data from a particular domain is obtained by training on annotated data from the same domain. Even so, the results from another system on the same data provide at least a baseline performance to compare against our results.  4. Statistical LTAG Parser The use of lexical information plays a prominent role in statistical parsing models for English. In this section, we discuss how to extend a statistical parser that relies on bigrams of lexical dependencies to a morphologically complex language like Korean. While these types of parsers have to deal with sparse data problems, this problem is exacerbated in the case of Korean due to the fact that several base-forms of words can appear with a wide array of morphological afﬁxes. This problem is addressed by incorporating the morphological tagger/analyzer described above, which signiﬁcantly improves performance. Apart from the use of a specialized morphological tagger/analyzer for Korean, our methods are language independent and have been tested in previous work on the WSJ Penn English TreeBank (Marcus, Santorini and Marcinkiewicz, 1993). As described in above, we use Lextract to convert the TreeBank (the same method is used for both the English and the Korean TreeBanks) into a parser derivation tree for each sentence. The statistical parsing model is then trained using these derivation trees.  4.1. Probability Models  The statistical parser uses three probabilistic models: one model for picking a tree as the start of a derivation;  and two models for the probability of one tree substituting or adjoining into another tree. Each of these models  can be trained directly using maximum likelihood estimation from the Lextract output. The probabilistic models  of substitution and adjunction provide a natural domain to describe the dependencies between pairs of words in a  sentence.  (Resnik, 1992) provided some early motivation for a stochastic version of Tree Adjoining Grammars and  gave a formal deﬁnition of stochastic TAG. Simultaneously, (Schabes, 1992) also provided an identical stochastic  version of TAG and also extended the Inside-Outside algorithm for CFGs (Lari and Young, 1990) to stochastic  TAGs. (Schabes, 1992) also performed experiments to show that a stochastic TAG can be learnt from the ATIS  corpus.  A stochastic¢L¡ TAG derivation proceeds as follows (Schabes, 1992; Resnik,¤1£ 992). An initial tree is selected with probability ¦¥ and subsequent substitutions are performed with probability and adjunctions are performed with probability . For each § that can be valid start of a derivation:  ¨ ©  ¢¡ §  Each subsequent substitution or adjunction occurs independently. For possible substitutions deﬁned by the  grammar:  ¨   £  § "!$# "¢%  " where, is substituting into node ! in tree § . For possible adjunctions in the grammar there is an additional factor which is required for the probability to be well-formed:  Sarkar and Han  53  ¡ # ¦¥  § !$# NA ¨ ¦¥  § "!$# ¤%  # where, is adjoining into node ! in tree § , and ¥  § "! # NA is the probability that there is no adjunction (NA) at node ! in § . ¢ " Each LTAG derivation is built starting from some initial tree . Let us consider the probability of a deriva¢ £ ¤ ¥ tion which was constructed using substitutions and adjunctions and internal nodes which had no adjunction.  ¦ §£ ¤ If we assume that each elementary tree is lexicalized (anchored) by exactly one word, then the length of the sen-  tence    . In fact, in the experiments we report on in this paper, each elementary tree has exactly one  lexical item as an anchor.  ¥    ¢  " "  ©¨ ¡ ( 0  " ¡ !"# ¢¥  § "! ) ¢¥  § "! )  # #     £  § "! § $  NA  $  # !  §%$ &$ '!  (2)  ¢ This derivation can be drawn graphically as a tree where each node in this derivation tree is an elementary  tree i¢n¡ the or¦ig£ inal LTAG (this is the standard notion of a TAG derivation tree). and can be written as the following lexicalized conditional probabilities which can be estimated from  the training data.  ¦£ ¢¥     §"! § !  # #  ¡  § "¢ #      ¡ ¦£ ¦¥      "¤")2 $ 14136§ 5 # ) $ 1 §  ! "!   )     Events for each of these probability models can be directly read from the LexTract output. Using maximum  likelihood estimation we convert these events into the above parameter values in our statistical parser.  For further details about decomposing these probabilities further to generalize over particular lexical items  and to make parsing and decoding easier see (Chiang, 2000). (Chiang, 2000) also has details about the standard  ¨ use of prior probabilities in statistical parsing for pruning which we use in our implementation. The probability of a sentence computed using this model is the sum of all the possible derivations of the  sentence.  $ ¨   ¨87 ¥ @9 A¨   ¢ A generative model can be deﬁned instead of a conditional probability to obtain the best derivation BEST ¨ given a sentence . The value for (3) is computed using the Equation 2.  ¢  BEST   ¢arg ¢arg  max max  BBCBC C¢D¢  ¨ ©1E¨¤ ¨      ¢arg max BC  ¢ ©¨¤  (3)  The particular deﬁnition of a stochastic TAG is by no means the only way of deﬁning a probabilistic grammar formalism with TAG. There have been some variants from the standard model that have been published since the original stochastic TAG papers.  54  Proceedings of TAG+6  For example, the restriction of one adjunction per node could be dropped and a new variant of standard  TAG can be deﬁned which permits arbitrary number of modiﬁcations per node. This variant was ﬁrst introduced  by (Schabes and Shieber, 1992; Schabes and Shieber, 1994). Tree Insertion Grammar (Schabes and Waters, 1995)  is a variant of TAG where the adjoining operation is restricted in a certain way and this restricted operation is  named insertion. TIGs are weakly equivalent to CFGs but they can produce structural descriptions that are not  obtainable by any CFG.  A stochastic version of insertion (Schabes and Waters, 1996) was deﬁned in the context of Tree Insertion  Grammar (TIG). In this model, multiple trees can be adjoined to the left and to the right of each node with the  following probabilities:  $ ¥  § ! # NA   ¨ ©¢¡  ¥  § "!$# § ¤  0 0 0 $ ¥  § "!$# NA   ¨ ©¡  ¥  § ! # § ¤%  ¥  0 ¥ In our parser, we allow multiple adjunctions at a node and also we exploit TIG style probabilities and . This was done so that the output easily convertible to the earlier dependency style parser that was used in the  project (with which we compare performance in our evaluation).  There are many other probability measures that can be used with TAG and its variants. One can easily go  beyond the bi-lexical probabilities that have been the main focus in this chapter to probabilities that invoke greater  amounts of structural or lexical context. (Carroll and Weir, 1997), for example, gives some additional probability  models one might consider useful when using TAGs.  An example output from the statistical parser is shown in Figure 6. In the parser (and in the Lextract output),  each elementary tree is anchored by exactly one lexical item. The gloss and translation for the example in Figure  6 is given in the following example:  (4) Motun hochwul tayho-nun mayil 24 si-ey pakkwui-key  every call  sign  everyday 24 hour-at switch-AuxConnect  toy-ciyo.  be-Decl  ‘Every call sign is switched at midnight everyday.’  Index Word  0  £¥¤  
The type of a minimalist grammar (MG) introduced in Stabler (1997) provides a simple algebraic formalization of the perspectives as they arise from Chomsky (1995b) within the linguistic framework of transformational grammar. As known (cf. Michaelis 2001a; 2001b; Harkema, 2001), this MG–type deﬁnes the same class of derivable string languages as, e.g., linear context–free (string) rewriting systems (LCFRSs) (Vijay–Shanker, Weir and Joshi, 1987; Weir, 1988). In this paper we show that, in terms of weak equivalence, the subclass of MGs which allow (overt) head movement but no phrasal movement in the sense of Stabler (1997), constitutes a proper subclass of linear indexed grammars (LIGs). and thus tree adjoining grammars (TAGs). We also examine the “inner hierarchic complexity” of this embedding in some more detail by looking at the subclasses canonically resulting from a differentiation between left adjunction of the moved head to the attracting one, and right adjunction of this kind. Furthermore, we show that adding the possibility of phrasal movement by allowing just one “indistinguishable” licensee to trigger such movement has no effect on the weak generative capacity of the corresponding MG–subclasses. On the other hand however, MGs which do not employ head movement but whose licensee set consists of at most two elements, are shown to derive, a.o., languages not derivable by any LIG. In this sense our results contribute to sheding some light on the complexity as it arises from the interplay of two different structural transformation types whose common existence is often argued to be linguistically motivated.  1. Introduction  The type of a minimalist grammar (MG) introduced in Stabler (1997) provides a simple algebraic formalization of the perspectives as they arise from Chomsky (1995b) within the linguistic framework of a principles and parameter–approach to transformational grammar. As known (cf. Michaelis 2001a; 2001b; Harkema, 2001), this MG–type constitutes a mildly context–sensitive formalism in the sense that it deﬁnes the same class of derivable string languages as linear context–free (string) rewriting systems (LCFRSs) (Vijay–Shanker, Weir and Joshi, 1987; Weir, 1988).1 In particular, the MG–deﬁnition permits a type of (overt) head movement which rather directly reﬂects the derivation mode of (successive) head(–to–head) adjunction—which, in the minimalist approach, takes place due to the necessity of feature checking—(successively) creating more complex heads (cf. Figure 1). Nev-  ZP  ZP  ZP  Z’  Z’  Z’  Z YZ XY  YP Y’ Y XP  X’  Z YP ZY Y X Y’ Y XP X’  Z YP ZY X Y Y’ Y XP X’  X WP  X WP  X WP  . . . successive left head adjunction  . . . successive right head adjunction  . . . (mixed) successive head adjunction  ¡ Figure 1: Complex head “Z” resulting from . . .  ertheless, there is a further notable property of MGs which—in connection with Michaelis (2001a)—follows from Harkema (2001) as well as Michaelis (2001b): each MG can be transformed into a weakly equivalent MG that ¢ This work has been funded by DFG–grant STA 519/1–1. 1. Hence, MGs as deﬁned in Stabler (1997) join to a series of weakly equivalent formalism classes among which, beside LCFRSs, there is, e.g., the class of set–local multicomponent tree adjoining grammars (MCTAGs) (cf. Weir, 1988). For a list of some further of such classes of generating devices, beside MCTAGs, see e.g. Rambow and Satta (1999). 2. In terms of an X-Bar representation.  £ c 2002 Jens Michaelis. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 57–65. Universita´ di Venezia.  58  Proceedings of TAG+6  neither employs head movement nor covert (phrasal) movement as allowed by the general MG–deﬁnition.3 In fact  it is this MG–subtype which, e.g., is covered in terms of the succinct MG–reformulation in Stabler and Keenan  (2000) (reducing MGs to their “bare essentials”), and which the MG–recognizer in Harkema (2000) (working in  polynomial time depending on the length of the input string) is deﬁned for. Moreover, the “MG–internal” equiv-  alence result can be seen as providing some technical support to more recent linguistic work which, in particular,  tries to completely dispense with any type of movement different from overt phrasal movement (e.g. Koopman and  Szabolcsi, 2000; Mahajan, 2000).  ¡£¢  Many linguists working within the transformational tradition, however, believe  ZP  head movement to be indispensable for an adequate explanation of natural language  Z˜ ¡£¢  syntax.4 How the kind of head movement originally allowed in an MG can be (re)integrated into the succinct MG–deﬁnition is shown in Stabler (2001). As in-  X Z˜ ¤¥¡£¢  dicated in there, the recognition complexity w.r.t. such an MG and an input string increases—adapting the methods of Harkema (2000)—at most as much as in the  ¤¦¤¥¡§¢ Y Z’  case when adding two new distinct licensees, i.e. two new distinct features potentially triggering phrasal movement, to the grammar.  ¤¦¤¥¡£¢ Z YP ¤¦¤¥¡£¢ Y’ ¤¥¡£¢ Y XP ¤¥¡£¢ X’ ¡£¢ X WP  Concentrating on questions concerning the increase of generative complexity, we show in this paper that, in terms of derivable string languages, the subclass of MGs allowing head movement but no phrasal movement in the sense of Stabler (1997), constitutes a proper subclass of LIGs, and thus TAGs. This is done by embedding MGs weakly equivalently into extended left LIGs (ELLIGs) in the sense of Michaelis and Wartena (1999).5 Examining the “inner hierarchic complexity” of this embedding in some more detail, it can be shown that MGs which allow only left head adjunction deﬁne the same class of derivable languages as RLIGs,6 and thus context free grammars (CFGs) (cf. Michaelis and Wartena, 1999). MGs which allow only right head adjunction derive more languages then CFGs, and MGs allowing right  Figure 2: Complex head as well as left head adjunction seem to provide a further proper extension. Further- “Z” resulting from succes- more, adding the possibility of phrasal movement by allowing the MG’s licensee set sive left head adjunction as to consist of at most one feature7 has no effect on the weak generative capacity of the representable in an RLIG. considered MG–subclasses. On the other hand, it can be shown that MGs which do not employ head movement but whose licensee set consists of at most two elements, derive, a.o., languages not derivable by any LIG.8  The presented results are of interest in at least two respects: ﬁrst, they contribute in a more general sense to  one of the central issues mathematical approaches to linguistics are concerned with, namely, how much strong generative capacity can be squeezed out of a formalism without increasing the weak generative power.9 Second,  since the presented results provide a ﬁrst narrow step towards an answer to the question of how the speciﬁc types of  head movement and phrasal movement as deﬁned in MGs are related to each other in terms of generative capacity,  they may not only be a promising starting point, when seeking for a lower upper bound on the parsing complexity  of MGs, but also shed some light on the structural complexity as it arises from the interplay of two different  structural transformation types whose common existence is often argued to be linguistically motivated.  For illustrative purposes we demonstrate how MGs allowing head movement but no phrasal movement can be  weakly equivalently embedded into a subclass of TAGs, instead of LIGs, which in its turn is weakly equivalent  3. The only movement possibly used is overt phrasal movement.  4. Even current accounts which argue in favour of overt vs. covert movement do not completely exclude overt head movement  (e.g. Kayne 1998; 1999).  5. In Michaelis and Wartena (1999), ELLIGs were deﬁned as the formal counterpart of extended right LIGs (ERLIGs). An  ELLIG (respectively, ERLIG) is an LIG, ¨ , such that for each nonterminating production © , the dis¡tingu¢is hed s!ym" bol#o¢%$ n  the righthand side (rh¡s)is)¢0th1 e le$2ft!mo"3st(r#e¢%spectively,  rightmost) nonterminal, i.e., © is of the form   (respectiv¡5el4§y¢ , ©'&(  ), where is a string of terminals. Thus, applying such an © to a corresponding  object  , th¡ e stack associated with the nonter" minal  is passed on to the leftmost (respectively, rightmost) nonterminal  child replacing , the upper part of the stack, by . If, in addition, for each such nonterminating rule © , no terminal symbol  appears to the left (respectively, right) of the distinguished nonterminal symbol of the rhs, i.e., if is always the empty string,  then ¨ is simply referred to as an LLIG (respectively, RLIG).  6. A corresponding weakly equivalent RLIG can be deﬁned such that it represents complex heads, created by successive head  adjunction, as indicated in Figure 2.  7. Thus, at most one “indistinguishable” type of phrasal movement is available.  8. A schematic overview is given in Figure 3.  9. See e.g. Joshi (2000) and references therein for a more recent discussion.  Michaelis  59  2-MLHM:none  ¡£¢¥¤¦¨§©¤¦¨¢¥ § ¤¦¨¨¤¦   !#"!$%'&¥( )  ¡  ¡  ¡  ¡  Seki et al., 1991  0 LIL 1 TAL  ¡£¢¥¤¦ ¢¥ §   §©¤¦  ¤¦ ¨¤¦ !#"!$%'&¥( ) 2  ¡  ¡  ¡  Wartena, 1999  0 E3 LLIL  [conjecture] Cornell, 1996; Stabler, 1997  ¡£¢¥¤'§©¤4¨¤ "!$%'&¥( ) 03 -MLHM:arbit-adj 1 1-MLHM:arbit-adj 0 0-MLHM:right-adj 1 1-MLHM:right-adj ¡£565  5879¡£¢ § &A@B0 & ( ) 2 0-MLHM:left-adj 1 1-MLHM:left-adj C 0-MLHM:none 1 1-MLHM:none C  CFL 1 RLIL ¦EDGF ¦H¦ Figure 3: Schematic overview of our results.  to ELLIGs (Section 3). Largely skipping formal details afterwards, we subsequently emphasize the crucial points concerning the hierarchy of the corresponding MG–subclasses resulting from the different types of head movement as available in the MG–formalism (Section 3.1–3.4). Then, we turn to simple phrasal movement as allowed by the MG–deﬁnition (Section 4) and, ﬁnally, present an example of an MG which does not employ any head movement, but phrasal movement “slightly beyond” the simple type, thereby deriving a language not derivable by any LIG (Section 5). But ﬁrst, since the reader might be less familiar with MGs, we brieﬂy introduce them in their aspects relevant here.  2. Minimalist Grammars  An MG is a formal device which speciﬁes a countable set of expressions (over I9PRQSPUT ),12 i.e., a countable set of ﬁnite, binary (ordered) trees each equipped with a leaf–labeling function assigning a string from I @ Q @ T @ to  each leaf, and with an additional binary relation, the asymmetric relation of (immediate) projection, deﬁned on the  set of pairs of siblings (cf. Figure 4).  A maximal projection within such an expression V is a subtree of V which is either identical to V , or its root  is projected over by its root’s sibling. The speciﬁers, the complement and the head of (a maximal projection in) V  laarbeedleitserinmi¡ nce& dQ  in @T  the @,  canonical way and each other  by means leaf–label  of in  the Q @T  projection relation (cf. @ . The phonetic yield  Figure of V is  5). the  V is complete if its head– string which results from  concatenating the leaf–labels in “left–to–right–manner” ignoring all instances of non–phonetic features.  The base of an MG, W , is formed by a lexicon (a ﬁnite set of simple expressions, i.e. single node trees in  the above sense, also called heads) and two structure building functions: merge (combining two trees) and move  10. Here, for X`Yba and cedf noneg left-adjg right-adjg arbit-adjh , n-MLHM:i denotes the class of all languages derivable by any MG whose licensee set consists of at most X elements, and which permits only head(–to–head) adjunction of the type indicated by c . 11. Note that Wartena (1999) actually provides a formal proof of the fact that the language f©p¥qr£stqrGutqr£pvwxstvwyuvwyqrgXY9aAh , although derivable by some LIG, is not derivable by any ERLIG. For reasons of symmetry however, it becomes immediately clear from the corresponding proof details that the language fp qr p vw s vw u vw s qr u qr  qr gXYaAh , although derivable by some LIG, is not derivable by any ELLIG. 12.  ,  and  are assumed to be pairwise disjoint sets, namely, a set of syntactic, phonetic and interpretable features, respectively.  is partitioned into basic categories, selectors, licensees, and licensors. There is at least the basic category c. 13. Here, “<” (respectively, “>”) as “label” of a non–leaf node means “my left (respectively, right) child projects over my right (respectively, left) child.”  60  Proceedings of TAG+6  >  < ¢¡ ¦ ¡ ¡  < £¥¤ >  ¦¦ <  §¦  ¦©¨  ¡  ¦¨ Figure 4: A typical (minimalist) expression.  >  speciﬁer  >  speciﬁer  >  speciﬁer  <  head  complement  Figure 5: The typical (minimalist) expression structure.  (transforming a single tree). Both functions build structure by canceling two particular matching instances of syntactic features within the leaf–labels of the trees to which they are applied. The closure of the lexicon under these two functions is the set of trees speciﬁed by W . The (string) language derivable by W is a particular subset of Q @ , namely, the set of the phonetic yields of the complete expressions within this closure. The function merge is applicable to   , a pair of expressions, if  ’s head–label starts with an instance of some basic category x, and  ’s head–label with an instance of =x, the corresponding weak selector of x. Depending on whether  is simple or not,  is selected as the complement or the highest speciﬁer, respectively. Within the resulting tree, merge ! , the corresponding instances of =x and x are cancelled (cf. Figure 6). In  ): =x"$#¢%'&(%  4 :  5  < 4¢6  "$# % & %  4¢6 > ) 6  x01#¢23&(2  01# % & %  01#¢2'&(2 "1# % & %  if ) is simple  if ) is complex  Figure 6: merge ! — weak selection.  case  is a head, its label may likewise start with an instance of a corresponding strong selector of x, =X or X=, both additionally triggering (overt) head movement, i.e., merge ! is deﬁned as before, but in addition 798 , the string of phonetic head–features of the selected  , is incorporated into the label of the selecting head  , either immediately to the right (triggered by =X) or immeF ¦BdA iately to the left (triggered by X=) of 7@ , the string of phonetic features within  ’s (unique) label (cf. Figure 7).14 The function move is applicable to an expression  , if there is exactly one maximal projection  in  whose head–label starts with in instance of some licensee -x such that  ’s head–label starts with an instance of a cor- 14. In the minimalist approach suggested in Chomsky (1995b) the merge–operator applies freely, and head movement is a separate step following a corresponding application of this operator. As noted by Chomsky (1995a, p. 327), in a strong sense this can be seen as a violation of the “extension condition” on structure building functions. Stabler (1998, p. 78, fn. 5) argues that that the implementation of head movement within MGs not only avoids such a violation, but “it [also] explains the coincidence of the selection and head movement conﬁgurations.” Note also that the implementation of head movement is in accordance with the head movement constraint, demanding that a moving head can never pass over the closest c–commanding head. To put it differently, whenever we are concerned with a case of successive head movement, i.e. recursive adjunction of a (complex) head to a higher head, it obeys strict cyclicity. The way in which MGs reﬂect the “usual” linguistic notion of head adjunction arising from head movement is made explicit in Stabler (1998).  Michaelis  61  ): "1# % & %  4 :  5  < 4¢6  < 4¢6  "1#¢2'# % & %  "1# % #¢23& %  x01#¢23&2  0 &2  0 &2  if & X= Figure 7: merge ! — strong selection.  if & =X  responding strong licensor +X or weak licensor +x triggering overt or covert phrasal movement, respectively.16 If  ’s head–label starts with a strong licensor then, within the resulting tree move ,  is moved into the new created, highest speciﬁer position, while the triggering instances of +X and -x are cancelled, and the “original” position of  ’s root becomes a single node labeled with the empty string (cf. Figure 8). If  ’s head–label starts with  )  5  4  -x0 +X" Figure 8: move  4¢6 > )6 0 " — overt phrasal movement.  a weak licensor then, within the resulting tree move , the triggering instance of +x is cancelled, while a copy of  in which the triggering instance of -x as well as all instances of phonetic features are cancelled, is moved into the new created, highest speciﬁer position, and while another copy of  in which all instances of non–phonetic features are cancelled is “left behind.”17  3. MG–Head Movement in Terms of TAGs  e£ q7 ui¡vLwaeleteanW tkTbAsetGar,noW Mn¡ ,gGi& s,wewihthihceehrreatwhlleeowastkasrhat nesadydmsmbtorolvo, e¢nmg, eoanrrteabtpuwatoirnno eywp¤h,£ rdaiwssatiiltnhmcytosvbyeemminbegonlats..1b8asAic  nonterminal in category from  W  our weakly , and with  ¥ NA ¦ cg weak§ OA  ¨ ¡ Figure 9: The unique initial tree of W . ta rn=edxes¦©Tdh7 eep rT e=en@ xi.ds¤ iTanyhgs7eio©n,lnga lttethbeierenifcintoaigra¡smelatndroyeifvesitd(ehcleeefs.c(uFitnonitrgio,qu=urthexer)9¦el)ea,bsaeun lbdoc=fafxos¡r¤e:eswawdceeehapkgleeensxndeieclienraacglltMlooyrnGsdwif–soihttrieenatmghnue¡ i"rs,h t$ htihse%eroe,fcayatrhseaeestbwfyoaosr7miec© l(ec=cmaxft.ee,FngXitoga=rruy,yr,oea7ru1=x07 Xi)liQa(ancr@fdy,. Figure 11–13). Hence, W only uses auxiliary elementary trees which may be called extended right auxiliary, i.e., auxiliary trees in which the foot node is theF lD eftmost nonterminal node on the frontier, and all interior nodes left of the spine are marked for null adjunction.19 ¡  4 16. The uniqueness of provides us with a strict version of the shortest move constraint (SMC).  17. For more details on the deﬁnition of merge and move see Stabler (1997). Particular examples of MGs are given below in  Section 3–5.  18. That is, ¨ does not employ the function move to derive any expression from some instances of the lexical items. Therefore,  we may assume that no (label of any) lexical item contains an instance of some licensee or licensor feature.  19. This TAG–subtype may also be seen as a straightforward extension of a particular subtype of a tree insertion grammar  (Schabes and Waters, 1995).  6  20. With the intend of simplifying our presentation, ¨ also ﬁts in with the “classical” TAG–deﬁnition allowing selective  adjunction, but not substitution (see e.g. Vijay–Shanker and Weir, 1994).  62  Proceedings of TAG+6  ¦ yg weak§ NA  ¦ yg strong§ NA  ¦ yg weak§ ¢ NA  #  #  ¦ yg strong§ ¢ NA  Figure 10: Elementary auxiliary trees of W ¡ resulting from the lexical MG–item y7© .  ¦ yg weak§ NA  ¦ yg strong§ NA  ¦ xg weak§ OA  ¦ xg weak§ OA  ¦ xr g weak§ OA #  # ¦ xr g weak§ OA  ¦ xq  g weak§  OA  ¦ yg weak§ ¢ NA  ¦ xq  g weak§  OA  ¦ yg strong§ ¢ NA  Figure 11: Elementary auxiliary trees of W ¡ resulting from the lexical MG–item =x=x¦  =x¤ y7© .  ¦ yg weak§ NA  ¦ yg strong§ NA  ¦ yg weak§ ¢ NA  ¦ xg strong§ OA  ¦ xg strong§ OA  ¦ xr g weak§ OA #  # ¦ xr g weak§ OA  ¦ xq  g weak§  OA  ¨  ¦ xq  g weak§  OA  ¦ yg strong§ ¢ NA  Figure 12: Elementary auxiliary trees of W  ¡  resulting  from  the  lexical 
 Mirror Theory is a syntactic framework developed in (Brody, 1997), where it is offered as a consequence of eliminating purported redundancies in Chomsky’s minimalism (Chomsky, 1995). A fundamental feature of Mirror Theory is its requirement that the syntactic head-complement relation mirror certain morphological relations (such as constituency). This requirement constrains the types of syntactic structures that can express a given phrase; the morphological constituency of the phrase determines part of the syntactic constituency, thereby ruling out other, weakly equivalent, alternatives. A less fundamental, but superﬁcially very noticeable feature is the elimination of phrasal projection. Thus the X-bar structure on the left becomes the mirror theoretic structure on the right:  XP  X  YP X  YZ  X ¡ZP ¡£¢ ¡¥¤ (Brody, 1997) calls this systematic collapse of , and nodes ‘telescope’. Every node may now have phonetic content, and children are identiﬁed as speciﬁers or complements depending on their direction of branch- ¡£¢ ¡¥¤ ing; left-daughters are speciﬁers and right-daughters are complements (previously, speciﬁers were children of , and complements were children of ). Furthermore, the complement relation is a “word-forming” relation, where according to the “mirroring” relation, the phonetic content of each head follows the phonetic content of its complement. For example, MTGs can generate trees like the following, which given the “mirror” relation between morphology and syntax, is pronounced John sleep -s: -s ¦ John sleep §¦  1.1. Trees  A mirror theoretic tree (MTT) can be viewed as a standard binary branching tree together with two functions;  ¨ © § "! ¨ §$#  one, a function from branches to a two element set  , the other, a function from nodes to a two  ©&% § ('()0 213!4657# 4 4 4 ¢ 4 ¢ 4 ¨98A@ 47A4 ¢CB2DFE "! ¨ § element set  . If is the parent of , then is a speciﬁer (or left child) of if  ,  and a complement (or right child) of otherwise. Formally, we represent a MTT as a particular kind of tree  domain:  G E @IH Deﬁnition 1 A MTT  $P  B  H where   PRQ  ©(S UTV#VW such that  PXQ 1.  H  H Ya`bH Y Edc6e c `fH 2. is preﬁx closed (if  and  then  )  Y c Y c Domination corresponds to the initial substring relation with dominating iff is an initial substring of . The  Y c Yhg c  greatest node dominating both and , , is their longest common initial substring. The function from nodes  ©&% § &'()0 21i!(4657# P to  is the characteristic function of the set :  08 § DpErq  1i% § !(&46'(5)0 ,  i,fi§tf §`s  `P  P  u  c 2002 Gregory M. Kobele, Travis Collier, Charles Taylor, Edward P. Stabler. Proceedings of the Sixth International Workshop  on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 66–73. Universita´ di Venezia.  Kobele, Collier, Taylor and Stabler  67  ¨ © § "! ¨ §$# From Deﬁnition 1 we deﬁne from branches to  as follows. A child is a left child if it ends in a ‘1’,  and it is a right child if it ends in a ‘0’. ¨982@"Y  Y ) B2D E  q  ("!6¨ §  ,§ if ) , if  )  E  E  T  S  As even internal nodes may have phonetic content, the standard deﬁnitions of the yield of a tree will not  sufﬁce. We want a node to be spellt out after its left subtree, and before its right subtree. We deﬁne a total order  Y c H Y c Y c Y c H ¡  ¡ on  the nodes of , holds between  such and  that just  ¡ in  whenever case one of the  is visited before following is true:  in an in-order tree traversal of  . Thus  c W c £¢  Y Y 1.  and is in the left subtree of  W c c ¢  Y Y 2.  and is in the right subtree of  Y Y g c Y g c c 3. ¤  and  ¤  This gives us a strict SPEC - HEAD - COMP ordering. But wait. There’s more. The partitioning of branches into  left branches and right branches is not just to determine a reletive precedence with respect to a parent. Right  branches are different from left branches in kind; a maximal sequence of right branches is what Brody (1997) calls  a morphological word (MW), and a morphological word has a special status with respect to spellout - all the nodes in a MW are spellt out as a single unit. The relation ¤ determines the relative ordering of MWs at spellout.  We deﬁne a morphological word to be a block in (the partition induced by) the equivalence relation ¥ deﬁned  H Y c c `fY7S W Y ` c S W on in the following manner:  ¨§ ¦¥ iff  Two nodes are equated by this relation just in case one is the complement of (...the complement of) the other. As trees are binary branching, the immediate domination relation totally orders each MW. With each MW © , we 8 D ` 8 D  ¢ c ` associate an element  © © and call  © the spellout position of © .1 Given two MWs © © , every © e ` ¢ 8 D 8 ¢ D is spellt out before any © iff  © ¤  © . At this point the nodes in each MW must be spellt out in a H contiguous manner (as ¤ totally orders ), but nothing has been said about the relative order in which they are spellt out. In keeping with (Brody, 1997) (but see (Kobele, forthcoming) for alternatives), we adapt Brody’s mirror principle (whence ‘Mirror Theory’) to our terminology:  (1) The Mirror Principle  Y c c Y if is the complement of then is spellt out before .  c c¢  Y Y Thus, each MW is spellt out in ‘reverse domination order’ ( is spellt out before iff  ).  1.2. Mirror Theoretic Grammars  A formal treatment of mirror theory inspired by Minimalist Grammars (Stabler, 1997) is given in (Kobele,  forthcoming), where an empirically grounded restriction of the formalism therein is shown to be weakly equivalent  to MCTAGs (Joshi, 1987).2A mirror theoretic expression is deﬁned to be a mirror theoretic tree along with a  labeling function from the nodes of the tree to a set of labels. A label consists of a phonetic part (which is opaque  !  ! f' ! to the syntax) and a ﬁnite sequence of syntactic features. A mirror theoretic grammar consists of a ﬁnite lexicon  of ‘basic’ expressions, together with two structure building operations,   and   , which build expressions  from others either by adjoining structures, or by displacing sub-parts of structures. Each operation is feature driven,  and ‘checks’ features (and thus a derived expression will have fewer features than the sum total of the features of  the expressions (tokens) used to derive it). The expressions generated by the grammar are those in the closure of  the lexicon under the structure building functions. A complete expression is one all of whose features have been  checked, save for the category feature of the root, and the string language at a particular category is simply the  yields of the complete expressions of that category.  9A1n.o@1dBesT(Cihne)C$ e$leamr7EeesnDGttrF oson@ g.p,iIctfhk(Eeend)E!#o$u"%$'t,46&!#i"%s5 $',th&th,eei‘snhdi!#ge"%ﬁh$'ense& td’46oinn7 e(Bsourfcothdhyet,hs1at9tro99An7@0g) Btnoo$ bdee@ st.hDGeInF¨‘do7 ethepereswt’onrdosd,ei,fi(0f n)1o $3 nod462 es5  i,nth$ ena!#re"%$'str& o48 ng.7  If some , where  2. Because of the “mirroring” and the relative ﬂexibility in where MWs get spellt out in relation to the other material, even the  movement-free subset of the framework deﬁnes a proper superset of the context free languages. See (Michaelis, this volume)  for a discussion closely related to this issue.  68  Proceedings of TAG+6  E Deﬁnition 2 A MTG  @¢¡   P  c)  ¤£  !Y  ©  !  !  f'  ! # B , where  1. ¡ is a non-empty set (the pronounced elements)  P c ) 2. is the disjoint union of the following sets (the syntactic features):  4 % ! (a) ¥ , a non-empty ﬁnite set.  % !("! § E © h` 4 % ! # (b) ¦ ¦  =¥¨§ ¥ ©¥  %(% !("! § E © i` 4 % ! # (c)  ¦  ¥ = § ¥ ©¥    ! ) % !(! % E © h` 4 % ! # (d) ¢¦  -¥¨§ ¥ ©¥    ! ) %'(V% E © i` 4 % ! # (e) ¢¦  +¥¨§ ¥ ©¥  @2@"H  P B  B @"H  P B H W P c ) W An expression is a pair   , where  is a MTT, and  ¡   ! Y @2@"H  P B  B 3. £ is a ﬁnite set of expressions   , such that3  H E T (a) § § , and  H W % !("! § 8 %(% !("! § X  ! ) %'(V% D 4 % !F  ! ) % !(! % W (b)  ¡ ! ¦  #" ¦  %$  ¦  ¢¦  "  ¥  &¦  is the labelling function.  4 % ! The shape of the lexical labels is partly determined by the nature of MTTs (and the particular generating func- tions we have).4The precategory sequence (the features before the ¥ category) allows for up to one com-  4 % ! plement (cselection features) and up to one speciﬁer (sselection or licensor features). Each lexical item has a category (a ¥ feature), and no more than one, as nodes in any tree have only at most one parent. There are no  restrictions as to the number of licensee features - movement is ad libitum.  ! U ! merge   is a function from pairs of expressions to single expressions. We divide the presentation of the  % !  ! function deﬁnition into two cases according to whether the merged item is merged into the speciﬁer (¤  )  ! U ! or the complement (¦   ) position.  % ! U ! @2@"H  P B  B  @2@"H $P B  B SMERGE ¡  is deﬁned on the pair of expressions ('  ¤  '  '  %)    )  )  iff all of the following  obtain: H T `fH - the root of ' has an available speciﬁer position (10 ' )  H - the ﬁrst syntactic feature of the root of 2' is ¥ = , and  H - the ﬁrst syntactic feature of the root of 3) is ¥  % !  ! @2@"H  P B  B In this case, ¡  is deﬁned on the pair, and it maps to the expression   , where  H E H T H 4  '65  )  P E P TP 4  '5  )  H H 4 the label of the root of is gotten from the label of the root of (' by deleting the ﬁrst syntactic feature  H H 4 the label of the left child of is gotten from the label of the root of ) by deleting the ﬁrst syntactic feature  Ya`bH 8"Y D E &8"Y D Ya`bH 8 T Y D E 8"Y D 4 otherwise, for 2' ,    '  , and for  %) ,    )  !  ! @A@IH $P B  B  @A@IH $P B  B CMERGE ¦   is deﬁned on the pair of expressions    ''  '  )    )  )  iff all of the following  obtain:  H S `fH - the root of ' has an available complement position ( 0 ' )  H - the ﬁrst syntactic feature of the root of ' is =¥ , and  H - the ﬁrst syntactic feature of the root of 3) is ¥  !  ! @A@IH $P B  B In this case, ¦   is deﬁned on the pair, and it maps to the expression  ¤ , where  H E H S H 4  '65  )  P E P S P 4  '75  )  H H 4 the label of the root of is gotten from the label of the root of (' by deleting the ﬁrst syntactic feature  H H 4 the label of the right child of is gotten from the label of the root of ) by deleting the ﬁrst syntactic feature  Ya`bH 8"Y D E &8"Y D Ya`bH 8"S Y D E 8"Y D 4 otherwise, for 2' ,    '  , and for  %) ,    )  3. 879 should be read as ‘one or zero tokens of 8 ’. 4. Only partly, as there is no functional reason that sselection features cannot precede cselection features. Doing so makes no difference (other than further complicating the description of a lexical label).  Kobele, Collier, Taylor and Stabler  69  f' ! b' ! 82@A@IH  P B  BAD move   is a function from expressions to expressions.    '  ¤  '  '  is deﬁned whenever the follow-  ing conditions obtain:  H - the root of 2' has an available speciﬁer position  H - the ﬁrst syntactic feature of the root of ' is +¥ , and  )¥`bH ) ) - there is exactly one node  ' such that the ﬁrst syntactic feature in is -¥ , and, moreover, cannot be in  H the same MW as the root of '  b' ! 8A@2@IH  P B  B2D @A@IH $P B  B If the above conditions obtain, then    ('    '  '  is deﬁned, and is equal to  ¤ , which is the  ) result of moving the subtree rooted in the least node in the MW containing , to the speciﬁer position of the  ) ) E Y T S ¦  ` root. Note that since is not in the same MW as the root, we must have that  @A@IH  P B  H B H E © c Y T c `bH # P E © c Y T c ` P # subtree we are to move is 3) )  '¢¡ ) , where %)  §  ' , and )  for some  . The  §  ' . Then  H E T H 8"H Y T H D 4  %) 5  '¤£  %)  P E TP 8 P Y TP D 4  ) 5 '¥£ 
Relative clause attachment may be triggered by binding requirements imposed by a short anaphor contained within the relative clause itself: in case more than one possible attachment site is available in the previous structure, and the relative clause itself is extraposed, a conflict may arise as to the appropriate s/c-structure which is licenced by grammatical constraints but fails when the binding module tries to satisfy the short anaphora local search for a bindee. 
¢  Alexis Nasr , Owen Rambow , John Chen , and Srinivas Bangalore  £  ¡  ¢  Universite´ Paris 7, University of Pennsylvania, AT&T Labs – Research  nasr@linguist.jussieu.fr, rambow@unagi.cis.upenn.edu, jchen@research.att.com,  srini@research.att.com  1. Introduction: A Dependency-Only Parser  In this paper, we describe work in progress that originated with the goal of creating a dependency parser which does not use phrase structure, but does use an explicit generative (and of course lexicalized) encoding of the grammar.1 The motivation for this goal is threefold. ¤ First, we believe that the phrase structure used by many (if not most) linguistic theories is useful only in deriving the syntactic behavior of lexical heads (or classes of lexical heads) from more general principles(which is of course the goal of syntactic theory); once derived, the syntactic behavior can be expressed in simpler terms that can be easier to manipulate in a computational setting.2 Furthermore, applications need as output from a parser something close to dependency (typically, lexical predicate-argument structure), but not phrase-structure. ¤ Second, we prefer an explicit encoding of a grammar in an inspectable declarative syntactic formalism over an implicit encoding in a computational framework. While we believe that explicit grammars have various advantages, such as the ability to test different syntactic hypotheses, or the possibility of hand-crafting grammars for limited domains (as is commonly done in commercial speech-based systems), we do not attempt to justify this preference further. ¤ Third, we believe that generative formalisms have certain advantages over constraint-based formalisms, in particular computational advantages. A large body of research on parsing in generative formalisms can be reused for different formalisms if they share certain basic properties, and we exploit this fact. The formalism we used is based on that presented in (Kahane et al., 1998), but in the ﬁrst phase we have omitted the non-projectivity, leaving us with a simple generative algorithm which allows us to lexicalize a contextfree grammar by allowing regular expressions in the right-hand side.3 We call this formalism GDG,or Generative Dependency Grammar. Its parsing algorithm, naturally, expresses the regular expressions as ﬁnite-state machines (FSMs), and the chart parser records FSMs and the state they are in during the parse. It is well known that the derivation tree of a Tree Adjoining Grammar (TAG) is a dependency tree if the grammar is lexicalized (though not always the linguistically most plausible one), and as a result, there are many parallels between a dependency analysis and a TAG analysis if we abstract from the phrase structure of the latter (Rambow and Joshi, 1997). In fact, a dependency parse can be seen as a direct parse of a TAG derivation tree. And furthermore, we can derive a grammar in our lexicalized GDG formalism from a TAG grammar in a relatively straightforward manner. This has advantages in grammar reuse. Given the close relation between dependency and TAG, our approach can be seen as an exercise in applying the FSM-based parsing techniques of Evans and Weir (1997) and (1998) to the Tree Insertion Grammars (TIG) of Schabes and Waters (1995), though the different origin of the approach (dependency parsing vs. TAG parsing) translates to differences in the way the FSMs are used to parse. In this overview, we introduce GDG and its parsing algorithm in Section 2, and describe how we compile a TAG to a GDG in Section 3. We discuss the empirical adequacy and speed of the parser in Section 4, and conclude with a discussion of how our projected work relates to other current work in parsing. We postpone a discussion of related work in formalizing dependency to another publication. 1. We would like to thank David Chiang, Anoop Sarkar, and three insightfully critical anonymous reviewers for their comments, explanations, and suggestions. 2. We leave open whether this point also has cognitive relevance. 3. In this we follow a suggestion made by Abney (1996), essentially extending the formalism of (Gaifman, 1965). ¥ c 2002 Alexis Nasr, Owen Rambow, John Chen, Srinivas Bangalore. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 96–101. Universita´ di Venezia.  Nasr et al.  97  2. GDG and Parsing GDG An Extended Context-Free Grammar (or ECFG for short) is like a CFG, except that the right-hand side is a regular expression over the terminal and nonterminal symbols of the grammar.4 At each step in a derivation, we ﬁrst choose a rewrite rule (as we do in CFG), and then we choose a string of terminal and nonterminal symbols which is in the language denoted by the regular expression associated with the rule. This string is then treated like the right-hand side of a CFG rewrite rule. A Generative Dependency Grammar (GDG) is a lexicalized ECFG. For our formalism, this means that the regular expression in a production is such that each string in its denoted language contains at least one terminal symbol. When we use a GDG for linguistic description, its left-hand side nonterminal will be interpreted as the lexical category of the lexical item and will represent its maximal projection. The right-hand side symbols represent the active valency of the lexical head of the rule, i.e., the categories that must or may (depending on whether the symbol is optional) be dependents on this lexical head for it to be the root of a well-formed subtree. Passive valency (a representation of where and to what other heads a lexical head may attach itself) is not explicitly encoded. The parsing algorithm is a simple extension of the CKY algorithm for CFG. The difference is in the use of ﬁnite state machines in the items in the chart to represent the right-hand sides of the rules of the ECFG. A rule with category as its left-hand side will give rise to a ﬁnite state machine which we call a -FSM; its ﬁnal states mark the completed recognition of a constituent of label . Let ¡ be the parse table for input sentence ¢ and GDG £ such that ¡¥¤§¦ ¨ contains © iff  is a -FSM,  is one of the ﬁnal states of  , and we have a derivation of substring  ¤ !"  ¨ from in £ . Initialization: We start by adding, for each # , $&%'#(%0) ,  ¤ to ¡ ¤§¦ ¤ . Completion: If ¡1¤2¦ ¨ contains either the input symbol  or an item © such that  is a ﬁnal state of  , and  is a -FSM, then add to ¡¥¤§¦ ¨ all ©354§647 such that 84 is a rule-FSM which transitions from a start state to state  4 on input  or . Scanning: If ©@9AB9! is in ¡ ¤2¦ C , and ¡ CED 9 ¦ ¨ contains either the input symbol  or the item ©@FG!F! where !F is a ﬁnal state and HF is a -FSM, then add ©3@9AG to ¡ ¤§¦ ¨ (if not already present) if @9 transitions from A9 to  on either  or . Note that because we are using a dependency grammar (or, in TAG terms, parsing the derivation tree directly), each scanning step corresponds to one attachment of a lexical head to another (or, in TAG terms, to an adjunction or a substitution). At the end of the parsing process, a packed parse forest has been built. The nonterminal nodes are labeled with pairs © where  is an rule FSM and  a state of this FSM. Obtaining the dependency trees from the packed parse forest is performed in two stages. In a ﬁrst stage, a forest of binary syntagmatic trees is obtained from the packed forest and in a second stage, each syntagmatic tree is transformed into a dependency tree. 3. Compilation of a TAG into GDG In fact, we do not derive a formal GDG from a TAG but instead directly compile a set of FSMs which are used by the parser (though we consider the distinction irrelevant, as FSMs and regular expressions are easily interconvertible). To derive a set of FSMs from a TAG, we do a depth-ﬁrst traversal of each elementary tree in the grammar (but excluding the root and foot nodes of adjunct auxiliary trees) to obtain a sequence of nonterminal nodes. Each node becomes two states of the FSM, one state representing the node on the downward traversal on the left side (the left node state), the other representing the state on the upward traversal, on the right side (the right node state). For leaf nodes, the two states immediately follow one another. The states are linearly connected with I -transitions, with the left node state of the root node the start state, and its right node state the ﬁnal state (except for predicative auxiliary trees – see below). To each non-leaf state, we add one self loop transition for each tree in the grammar that can adjoin at that state from the speciﬁed direction (i.e., for a state representing a node on the downward traversal, the auxiliary tree must adjoin from the left), labeled with the tree name. For each pair of adjacent states representing a substitution node, we add transitions between them labeled with the names of the trees that can substitute there. For the lexical head, we add a transition on that head. For footnodes of predicative 4. ECFG has been around informally since the sixties (e.g., the Backus-Naur form); for formalizations see (Madsen and Kristensen, 1976) or Albert et al. (1999).  98  Proceedings of TAG+6  auxiliary trees which are left auxiliary trees (in the sense of Schabes and Waters (1995), i.e., all nonempty frontier nodes are to the left of the footnode), we take the left node state as the ﬁnal state. There are no other types of leaf nodes since we do not traverse the passive valency structure of adjunct auxiliary tees. Note that the treatment of footnodes makes it impossible to deal with trees that have terminal, substitution or active adjunction nodes on both sides of a footnode. It is this situation (iterated, of course) that makes TAG formally more powerful than CFG; in linguistic uses, it is very rare.5 The result of this phase of the conversion is a set of FSMs, one per elementary tree of the grammar, whose transitions refer to other FSMs. We give a sample grammar and the result of converting it to FSMs in Figure 1. The construction treats a TAG as if were a TIG (or, put differently, it coerces a TAG to be a TIG): during the traversal, both terminal nodes and nonterminal (i.e., substitution) nodes between the footnode and the root node are ignored (because the traversal stops at the footnode), thus imposing the constraint that the trees may not be wrapping trees and that no further adjunction may occur to the right of the spine in a left auxiliary tree. Furthermore, by modeling adjunction as a loop transition, we adopt the deﬁnition of adjunction of Schabes and Shieber (1994), as does TIG. Chiang (2000) also parses with an automatically extracted TIG, but unlike our approach, he uses standard TAG/TIG parsing techniques. Rogers (1994) proposes a different context-free variant, “regular-form TAG”. The set of regular-form TAGs is a superset of the set of TIGs, and our construction cannot capture the added expressive power of regular-form TAG. As mentioned above, this approach is very similar to that of Evans and Weir (1997). One important difference is that they model TAG, while we model TIG. Another difference is that they use FSMs to encode the sequence of actions that need to be taken during a standard TAG parse (i.e., of the derived tree), while we encode the active valency of the lexical head in the FSM. A result, in retrieving the derivation tree, each item in the parse tree corresponds to an attachment of one word to another, and there are fewer items. Furthermore, our FSMs are built left-to-right, while Evans and Weir only explore FSMs constructed bottom-up from the lexical anchor of the tree. As a result, we can perform a strict left-to-right parse, which is not straightforwardly possible in TAG parsing using FSMs. 4. Adequacy of GDG and Initial Run-Time Results To investigate the adequacy of a context-free parser for English, as well as the speed of the parser, we use an automatically extracted grammar called “Bob” (Chen, 2001). Bob has been extracted from Sections 02-21 of the Penn Treebank. Parameters of extraction have been set so that the tree frames of the resulting grammar have a “moderate” domain of locality, and preserve many but not all of the empty elements that are found in the Penn Treebank (typically those cases where empty elements are found in the XTAG grammar, such as PRO subjects). Bob consists of 4909 tree frames. We tested our parser with Bob on 1,814 sentences of Section 00 of the PTB with an average sentence length of 21.2 words (excluding pure punctuation, i.e., punctuation which does not play a syntactic role such as conjunction or apposition). We evaluate performance using accuracy, the ratio of the number of dependency arcs which are correctly found (same head and daughter nodes) in the best parse for each sentence to the number of arcs in the entire test corpus. We also report the percentage of sentences for which we ﬁnd the correct analysis (along with many others, of course). To show that GDG is adequate for parsing English (an empirical question), we use the correct supertag associated with each input word and evaluate the performance of the parser. We expect only those sentences which do not have a context-free analysis not to have any analysis at all. This is indeed the case: there is no case of non-projectivity in the test corpus. Note that since we analyze matrix verbs as depending on their embedded verb, following the TAG analysis, long-distance wh-movement is not in fact non-projective or us. However, the punctuation mark associated with the matrix verb does cause non-projectivity, but since we are disregarding true punctuation, this does not affect our result. The average run-time for each sentence is 56ms (parsing using the correct supertag for each word, and no other supertag), which to our knowledge is signiﬁcantly quicker than existing full-ﬂedged TAG parsers.6 We show the 5. Our construction cannot handle Dutch cross-serial dependencies (not surprisingly), but it can convert the TAG analysis of wh-movement in English and similar languages. 6. Note that the extracted grammar does not have any features, so no feature uniﬁcation is performed during parsing. Agreement phenomena can be enforced by using extended label sets, at the cost of increasing the size of the grammar. (This is a parameter in the extraction algorithm.) Of course, features in TAG are always bounded in size anyway, and hence always equivalent to an extended label set.  Nasr et al.  99  Name t1[no]  Tree NP DT NP  FSM  0  no  
 ¡ ¡ The introduction of the adjoining operation to context-free grammars comes at high costs: The worst case time complexity of (Earley, 1968) is O n3 , whereas Tree Adjoining Grammars have O n6 ((Schabes, 1990)). Thus,  avoiding adjoining as far as possible seems to be a good idea for reducing costs (e.g. (Kempen and Harbusch,  1998)). Tree Insertion Grammars (TIGs, (Schabes and Waters, 1995)) address this problem more radically by  ¡ restricting the adjoining operation of TAGs such that it is no context-sensitive operation anymore. The result is O n3 worst case parseability which stems from TIG’s context-freeness. However, to preserve TAG’s mildly  context-sensitiveness the adjoining operation must not be restricted in any way. Another solution would be simply  to call the adjoining operation less frequently: The production of items directly depends on the fashion of the  underlying grammar and often adjoining is used to make the grammar more comprehensible or more adequate to  the linguistic phenomenon even if there would be simpler representations as, for instance, left- or right recursion.  This abstract (1st) sketches a way of reducing item generation through grammar transformation by using  Schema-TAGs (S-TAGs, as introduced by (Weir, 1987), where tree sets are enumerated by regular expressions)  which in contrast to TIGs keeps weak equivalence and performs better than factoring as proposed by (Harbusch,  Widmann and Woch, 1998), and (2nd) provides a proof of the average case time complexity of S-TAGs based on  the proposed transformation.  £ ¡ ¢ In the following, adressing of nodes occurs in the fashion of (Gorn, 1967), i.e. each node of a tree gets a unique number – beginning with zero – which preserves the structure of the tree. For example, 1 2 points to the second daughter of the ﬁrst daughter of a root node, and in grammar G1 of Fig. 2, A2 2 identiﬁes the foot node  of A2. The regular expressions of S-TAGs are deﬁned as “schema descriptors”: Let g be a Gorn number, then ¤¦¥ ¥g is a schema descriptor. ¤ § If µ and ν are schema descriptors, then µ ν, describing the alternative between µ and ν, is a schema descriptor. ¤ ¢ If µ and ν are schema descriptors, then µ ν, describing the concatenation of µ and ν, is a schema descriptor. ¤ ¡ ¨ ©   £  If µ is a schema descriptor, so are µ (bracketing), µ (arbitrary iteration) and µ 0 n n (n times iteration)  ¤ ¥ ¥ ¥  ¢ ¥ £  schema descriptors. If g is a schema descriptor, so is n n g n  a schema descriptor (the via g addressed subtree is cut out  from the via n addressed (sub)tree).  Reduction of item production by factoring  A ﬁrst idea of circumventing adjoining is to avoid it by reducing generation of items, on which Predict can be  applied. Thus, the idea is to aggregate common substructures appropriately, i.e. to condense the grammar in order  to get rid of redundancies (Harbusch, Widmann and Woch, 1998).  In general, factoring is applied to the schema descriptors (the regular expressions) of the S-TAG and can be  done by applying, e.g., the following rules:  (1) (2) (3) (4)  α α α α  ¢¢¢¢  γγγγ©¢ βl¢§δk§1γ¢¡γ¡α¢ β¢¢ββ¢ β§     !α"α"α¢§¢γγ¢ γ¢¢αγβ© 0© ¢l  k γ 
1. Introduction Some natural language expressions –namely, determiners like every, some, most, etc.— introduce quantification over individuals (or, in other words, they express relations between sets of individuals). For example, the truth conditions of a sentence like (1a) are represented in Predicate Logic (PrL) by binding the 
It has often been noted that the derivation trees of “standard” TAG grammars for natural languages (Group, 1998) resemble semantic dependency trees (Mel’cˆuk, 1988). More interesting, from a formal perspective, are the ways in which the derivation trees and the dependency trees diverge for certain problematic constructions. The desire to ﬁx these cases has led to a variety of proposals for modifying or extending the TAG formalism ranging from Tree-Local Multi-component TAGs (TL-MCTAG) and Set-Local Multi-component TAGs (SL-MCTAG), through reconceptualization of the adjoining operation (Schabes and Shieber, 1994), through D-Tree Grammars (DTG) (Rambow, Vijay-Shanker and Weir, 1995) and Graph-driven Adjunction Grammar (GAG) (Candito and Kahane, 1998b), through reformalization of elementary trees in terms of C-Command rather than domination (Frank, Kulick and Vijay-Shanker, 2000), through the use of Meta-Grammars (Dras, 1999; Dras, Chiang and Schuler, To Appear), and through interpreting the semantic relations in a predicate-argument structure derived, but distinct, from the derivation trees (Joshi and Vijay-Shanker, 1999; Schuler, 1999). In this note, we look at some of these problematic constructions from yet another formal perspective—a mild extension of TAGs with well-constrained generative capacity which allows semantic dependency to be expressed as a relation orthogonal (in a quite literal sense) to constituency and linear precedence. We should be clear at the outset, our focus is nearly exclusively formal—we explore ways of expressing these relationships in the extended formalism without pursuing their potential linguistic repercussions. Neither is our account exhaustive. There are problematic constructions that have not yet yielded to this approach. Our goal is to introduce the techniques involved, to explore their limits and, possibly, open up discussion of their linguistic consequences. 
1. TAG and the syntax-semantics interface 1.1. Lexicalized Tree Adjoining Grammars (LTAG)  A LTAG (Joshi and Schabes, 1997) consists of a ﬁnite set of trees (elementary trees) associated with lexical items and of composition operations of substitution (replacing a leaf with a new tree) and adjoining (replacing an internal node with a new tree). The elementary trees represent extended projections of lexical items and encapsulate syntactic/semantic arguments of the lexical anchor. They are minimal in the sense that all and only the arguments of the anchor are encapsulated, all recursion is factored away. LTAG derivations are represented by derivation trees that record the history of how the elementary trees are put together. A derived tree is the result of carrying out the substitutions and adjoinings. For a sample derivation see the TAG analysis of (1) in Fig. 1. The numbers at the nodes in the derivation tree are the positions of the nodes where the trees are added: John is substituted for the node at position (1), Mary for the node at position (22) and always is adjoined to the node at position (2). (1) John always loves Mary.  S NP VP NP ADV VP ¡ John always  VP V NP loves NP Mary  derived tree: NP John ADV always  S VP VP V NP loves Mary  derivation tree: love (1)john (22)mary (2)always  Figure 1: TAG derivation for (1)  1.2. Compositional semantics with LTAG Because of the localization of the arguments of a lexical item within elementary trees TAG derivation trees express predicate argument dependencies. Therefore it is generally assumed that the proper way to deﬁne compositional semantics for LTAG is with respect to the derivation tree, rather than the derived tree (see e.g. Shieber and Schabes, 1990; Candito and Kahane, 1998; Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi 1999, 2002). The overall idea is as follows. Each elementary tree is connected with a semantic representation. The way these semantic representations combine with each other depends on the derivation tree. Following Kallmeyer and Joshi (1999, 2002), in this paper, we will adopt ‘ﬂat’ semantic representations as in, for example, Minimal Recursion Semantics MRS, (Copestake et al., 1999). (2) shows the elementary semantic representations for (1). ¢ c 2002 Laura Kallmeyer. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 127–136. Universita´ di Venezia.  128  Proceedings of TAG+6  ¢¡¤£  ¡©¨  love’ ¥§¦ ¦  ¡¤¢¡  (2)  ¡¨  ¨  ¨  arg: §¦ ¥!#" §¦ ¥%$&$'#"  john’ ¥¢¦ arg: –  £     always’¥   (  ¡¤)   ¨0    21©¡  arg: ( ¡¨01©¡  mary’ ¥§34 arg: –  Roughly, a semantic representation consists of a conjunctively interpreted set of formulas (typed lambda-  expressions), scope constraints and a set of argument variables. The formulas may contain labels and holes  ¡ ¨5 ¨768676   ¡ ¨9 ¨867686  (metavariables for propositional labels). In the following,  are propositional labels,  are propo-  1¡¨91 ¨867686  sitional holes,  are propositional argument variables (whose values must be propositional labels) and  ( ¡@¨ (  ¨768686 are hole variables (special argument variables whose values must be holes). Argument variables may be  linked to positions in the elementary tree, as it is the case for the variables of love.  The use of holes is motivated by the desire to generate underspeciﬁed representations (as in, e.g., Bos, 1995)  for scope ambiguities. In the end, after having constructed a semantic representation with holes and labels, disam-  biguation is done which consists of ﬁnding bijections from holes to labels that respect the scope constraints and  that are such that no label is below two labels that are siblings (e.g., this ensures that nothing can be in the restric-  tion and the body of a quantiﬁer at the same time). In the semantic representation for love, there is for example a  ¡  ¡   ¡ A ¡  ¡  ¡  hole above the label (indicated by the constraint  ). Between and , other labels and holes might  ¡  ¡  come in (introduced for example by quantiﬁers or adverbs) or, if this is not the case, will be assigned to in  the disambiguation(s).  When combining semantic representations, values are assigned to argument variables and, roughly, the union  of the semantic representations is built. The values for the argument variables of a certain (elementary) semantic  representation must come from semantic representations that are linked to it in the derivation tree.  The linking of argument variables and syntactic positions restricts the possible values as follows: In a substi-  tution derivation step at a position B , only argument variables linked to B get values. In an adjunction step, only  argument variables that are not linked to any position can get values. In the case of a substitution, a new argument  is inserted and therefore a value is assigned to an argument variable in the old semantic representation. However,  in the case of an adjunction, a new modiﬁer is applied and therefore a value is assigned to a variable in the semantic  representation that is added. In this sense, in a substitution step, the variable assignment is downwards whereas in  an adjunction step it is upwards.  ¡  The derivation tree in Fig. 1 indicates that the value of ¦ needs to come from the semantic representation of  John, the one of ¦    from Mary and the values of (  ¡  1¡ and  need to come from love. Consequently, ¦  ¡DC  ¨  C  ¦ ¦  3 ¨ ( ¡EC  ¡  
Many TAG-based systems employ a particular tree adjoining grammar to generate the intended structures of the set of sentences they aim to describe. However, in most cases, the underlying set of elementary trees is more or less hand-made or maybe derived from a given tree data-base. We present a formal framework that allow to specify tree adjoining grammars by logical formulae. Based on this formalism we can check whether a given speciﬁcation is TAG-consistent or whether a given TAG meets some particular properties. In addition, we sketch a method that generates a TAG from a given logical speciﬁcation. As formal foundation, we employ a particular version of modal hybrid logic to specify the properties of T/D-trees. Such trees structurally combine a derived TAG-tree T and its associated derivation tree D. Finally, we sketch a labeled tableau calculus that constructs a set of tree automata representing the elementary trees of the speciﬁed TAG and a special tree automaton for the corresponding derivation trees. In literature, we ﬁnd some approaches specifying TAGs, or more generally, mildly context-sensitive grammar formalisms, that gradually vary in their underlying framework. Commonly, either starts with a logical description of recognizable sets of trees (Thatcher and Wright, 1968). However, they differ in their method of leaving the context-free paradigm. The approach mentioned in (Morawietz and Mo¨nnich, 2001) and (Michaelis, Mo¨nnich and Morawietz, 2000) uses a ‘lifting’ function that encodes a TAG into a regular tree grammar. In (Rogers, 1999) (and related works) we ﬁnd a logical description of TAGs that is based on a 3-dimensional view of trees. The important issue of this approach is to combine the derived TAG-tree and its derivation tree to a single 3-dimensional structure. Similarly, we also consider the derived TAG-tree and its derivation tree employ so-called T/D-trees. However we only associate the nodes of the derived tree with the corresponding node in the derivation tree. Consequently, all nodes of the same instance of an elementary tree refer to the same corresponding node in the derived tree. Therefore, we can specify structural properties of the derived TAG-tree and of the derivation tree at the same time. Using the links to the derivation tree, we can identify nodes in the TAG tree that belong to the same instance of some elementary tree. In contrast to the other approaches mentioned above which encode the TAG-tree into other kind of structures, we keep the original derived TAG tree as a structural unit. Consequently, we can directly access the nodes and the structural properties of the TAG tree without employing a particular projection function or any other special coding issues. In essence, our formalism employs modal hybrid logic that combines the simplicity of modal logic and the expressivity of classical logic. The use of so-called nominals in hybrid logic offer explicit references to certain tree nodes which is (directly) possible in modal approaches. We introduce the hybrid language HLTAG that speciﬁes properties of the combined structure of derived TAG-trees and their derivation trees. Using this language we specify a number of TAG axioms which establish a notion of TAG-consistency. Further, we brieﬂy illustrate a formalism that constructs a number of tree automata representing the underlying TAG for a given TAG-consistent HLTAG formula. 2. A Hybrid Language for TAGs and their Derivations Our formalism considers pairs of trees called T/D-trees as introduced in (Palm, 2000) where T represents a derived TAG-tree and D denotes the corresponding derivation tree. In general, a derived TAG tree T = (t, Vt) is made up of a kt-tree domain t ⊆ {1, . . . , kt}∗ for kt > 0 and a labeling function Vt: t → Pow(Pt) decorating tree nodes with a set of propositions of Pt. The set of propositions Vt(n) of some node n may be viewed as the label of n. Likewise, a derivation tree D = (d, Vd) is made up of a kd-tree domain d ⊆ {1, . . . , kd}∗ for some kd > 0 and a labeling function Vd: d → Pow(Pd). In addition, each T/D-tree includes the total linking function τ : t → d that associates each node in the derived TAG tree T with the corresponding instance of its elementary tree in the derivation tree D. ∗ An extended version can be found at http://www.phil.uni-passau.de/linguistik/palm/papers/ c 2002 Adi Palm. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 137–144. Universita´ di Venezia.  138  Proceedings of TAG+6  A .................................................................................................................... A β1  B .................................................................................................................... B β2  S  ......................................................................................................................................................................  A:1  B:2  ....................................................................................................................  ...................................................................................................................  α  Figure 1: Sample TAG with the initial tree α and two auxiliary trees β1 and β S ............................................................................................................................................................................................................................................................................................................................................................. AA BB β1 α β2 ......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................  Figure 2: Resulting T/D-tree after adjoining β1 and β2 in α  The correspondence between either trees works straightforwardly. By the tree T we represented the derived TAG tree which results from an initial tree after adjoining and substituting auxiliary trees. By the derivation tree D we graphically represent these operations. Each children position of some node n in D represents a certain place of adjunction (or substitution) in the elementary tree represented by n. For instance, in Figure 1, the elementary tree α includes two nodes where we can adjoin another tree; we uniquely associate these nodes with the numbers 1 and 2, respectively . Now if we adjoin β2 at the second node, this instance of β2 in the derivation tree becomes the second child n.2 of the node n representing the corresponding instance α. Once we adjoined the tree β1 at the ﬁrst position, n obtains its ﬁrst child n.1 representing this instance of β1. Obviously, we associated each node of the corresponding instances of α, β1 and β2 in the derived TAG-tree with the nodes n, n.1 and n.2 in the derivation tree, respectively. Figure 2 shows the resulting T/D-tree. Note that for our formalism we assume that we can only adjoin at the inner nodes of an elementary tree, i.e. there is no adjunction at the root or at some leaves. This restriction ensures that the parent of the root and the children of the foot are nodes of the tree at which the adjunction took place. For the formal foundation of our TAG-speciﬁcation language we employ hybrid modal logic HL (Blackburn and Tzakova, 1998), (Blackburn and Tzakova, 1999), (Blackburn, 2000a; Blackburn, 2000b). This formalism extends modal (or temporal) logic with particular propositions called nominals which enable references to particular nodes (or terms) in a model. Further, there is an implemented tableau-based prover (Blackburn, Burchard and Walter, 2001) which is partially based on (Tzakova, 1999). Compared with classical logic we prefer modal and hybrid approaches since they allow more compact proofs and speciﬁcations. In essence, we employ a modal logic on trees where the reﬂexive dominance relation denotes the modal reachability relation. We enhance this language by the next operator r referring to the r-th child of a node, by the link operator ♥ referring to the associated node in the derivation tree. For the hybrid formulae we include the jump operator i: ϕ and nominal propositions i with i ∈ Nom where Nom is an enumerable set of nominal symbols. Further, the language depends on the ﬁnite sets of constant propositions PT and PD and on the set of nominal  Palm  139  symbols Nom. Altogether, we obtain the hybrid language HLTAG(Pt, Pd, Nom) which is deﬁned as: ϕ ::= p | i | i: ϕ | ¬ϕ | ϕ ∧ ϕ | rϕ | ϕ | ♥ϕ where 1 ≤ r ≤ k (with k = max{kt, kd}), p ∈ Pt ∪ Pd denotes a propositional constant and i ∈ NT/D a nominal. Further, we can deﬁne the operators ∨, →, ↔ and in the standard way. In addition, we deﬁne the next-operator referring to some child by ϕ ≡ 1ϕ ∨ . . . ∨ kϕ and its dual universal counterpart by ϕ ≡ ¬ ¬ϕ. For the semantics of hybrid logic, we consider, in general, Kripke-structures which are, for the case of HLTAG, T/D-trees. Besides the structural information a T/D-tree associates each tree node of either tree with sets of constant propositions from PT and PD, respectively. In addition, we require a nominal denotation function g: Nom → (t ∪ d) evaluating the nominals. We interpret a given HLTAG(Pt, Pd, Nom) formula ϕ at some node n ∈ ∪d of a tree T /D for a nominal denotation function g: Nom → t ∪ d where g is only necessary for formulae including nominals. For the node n we assume that we know whether it is a member of t or d.  T /D, n |= p  iff n ∈ Vt(p) ∪ Vd(p), for p ∈ PT ∪ PD  T /D, n |= ¬ϕ iff T /D, n |= ϕ  T /D, n |= ϕ ∧ ψ iff T /D, n |= ϕ and  T /D, n |= ψ  T /D, n |= rϕ iff T /D, n.r |= ϕ, 1 ≤ r ≤ k where k = max{kt, kd} T /D, n |= ϕ iff T /D, n.a |= ϕ for some a ∈ {1, . . . , k}∗ where k = max{kt, kd}  T /D, n |= ♥ϕ iff T /D, τ (n) |= ϕ  A T/D-tree satisﬁes the formula ϕ if ϕ holds for the root of T . The link operator ♥ is self-dual, i.e. ♥ϕ ≡ ¬♥¬ϕ. For the nominal expressions, we deﬁne the semantics as follows:  T /D, n, g |= i iff g(n) = i T /D, n, g |= i: ϕ iff T /D, n , g |= ϕ and g(n ) = i  A nominal uniquely denotes a certain tree node where we do not explicitly distinguish the elements of T and D. The statement i is true if and only if the nominal i denotes the node under consideration. In contrast, in i: ϕ we refer to the node denoted by i which does not depend on the node considered currently. We say a T/D-tree T /D satisﬁes the (nominal) formula ϕ at the node n ∈ t ∪ d, written T /D, n |= ϕ, if there is a nominal denotation g: Nom → (t ∪ d) such that T /D, n, g |= ϕ is true. Similarly, T /D satisﬁes ϕ, written T /D |= ϕ if there is a nominal denotation g such that T /D, roott, g |= ϕ where roott denotes the root of the derived TAG tree T . Hence T /D |= ϕ ∧ ϕ states that ϕ must apply to all nodes of the derived TAG tree, T /D |= ♥ϕ states that ϕ applies to the root of the derivation tree and T /D |= ♥(ϕ ∧ ϕ) states that ϕ applies to all nodes of the derivation tree. Finally, a HLTAG formula ϕ is satisﬁable if and only if there is a T /D − tree and a nominal denotation g such that ϕ satisﬁes T /D by g. Note that employing nominal propositions increases the expressivity of the former language. For instance, we can deﬁne the until-operator “until ϕ is true ψ must apply” or the unique existence operator 1ϕ which are not expressible in ordinary modal logic (Blackburn and Tzakova, 1999).  until(ϕ, ψ) ≡ 1ϕ ≡  (ϕ ∧ i) ∧ ( i → ψ) (i ∧ ϕ) ∧ (ϕ → i)  In the ﬁrst case we search a descendant node that satisﬁes ϕ and mark this node by the nominal i. Then each descendant node that dominates i is an intermediate node that must satisfy ψ. Similarly, we specify the unique existence operator. Again we search a descendant node that satisﬁes ϕ and employ the nominal i in order to identify this node. Now all descendants that meet ϕ must also meet i. In general, by introducing nominal propositions, we can extend the expressivity of the underlying formalism. As shown in (Blackburn and Seligman, 1995; Blackburn and Seligman, 1997) hybrid logic is stronger than propositional modal logic. For instance, we can formulate the  140  Proceedings of TAG+6  nn21 m1 = m2 ...............................................................................................................................................................................................................................................................................................................................................................................................................................  .... .... .... .. ......... .... ..  Figure 3: T/D-tree: n1 and n2 are internal nodes of the same elementary represented by m1 and m2.  until operator, or by i ∧ ¬i we can demand that the underlying modal reachability relation is irreﬂexive. Either of these properties fails to be expressible by means of propositional modal logic. On the other hand, we can specify the standard translation from hybrid logic to classical ﬁrst-order logic. Therefore hybrid logic cannot be stronger than ﬁrst-order logic. Moreover, as shown in (Schlingloff, 1992; Palm, 1997) the expressive power of the ﬁrst-order logic for trees and the temporal logic for trees is identical. Since we can formulate the until-operator by means of hybrid logic, we obviously reach the expressivity of the temporal logic and the ﬁrst logic on trees. However, the more crucial aspect of our formalism is the link operator ♥ which allows to identify particular sets of tree nodes in the derived tree by referring to the same node in the derivation tree. Consequently, HLTAG describes ﬁrstorder deﬁnable sets of derivation trees; the expressivity for the derived tree obviously depends on the properties of linking function τ . Next we discuss some restrictions on τ leading to tree adjoining grammars. 3. TAG Axioms for HLTAG Obviously, by the language HLTAG we can describe derived TAG trees and their corresponding derivation trees in an appropriate manner. However, so far it is unclear, what the necessary properties of a T /D tree are in order to describe valid TAG-trees and their derivations. Likewise, we want to know whether a given HLTAG formula ϕ is TAG-satisﬁable, i.e. whether the set of T /D satisfying ϕ represents a certain TAG. The answer to either question is the set of TAG axioms for the language HLTAG. Hence, a T/D-tree would be TAG generated if and only if it meets these axioms, and a HLTAG-formula ϕ is TAG-satisﬁable if and only if it is consistent with these axioms, i.e. ϕ and the axioms are satisﬁable. Before we turn to the axioms in detail, we examine the construction and the structural properties of a T/D-tree by a given TAG derivation. For simpliﬁcation purposes we put some restriction on the kind of TAGs considered here. At ﬁrst, we restrict our formalism to the adjunction operation and ignore substitution. Nevertheless it is possible to simulate a substitution by an adjunction. Further, we assume that nodes, where adjunction is possible, are marked by the special auxiliary proposition adj and, correspondingly, all non-adjunction nodes must fail adj. Moreover, an adjunction node must be an inner node of an elementary tree, i.e. it cannot be the root or some leaf. As a consequence, we obtain only TAG trees where an adjoined tree is completely surrounded by the elementary tree it was adjoined to. This leads to the following lemma: Lemma 3.1 Let T /D = (t, vt), (d, Vd), τ be a TAG-generated T/D-tree and n1, n2 ∈ t, m1, m2 ∈ d with m1 = τ (n1), m2 = τ (n2) and n2 = n1.r for some 1 ≤ r ≤ kt. Then exactly one of the following cases must be true: 1. m1 = m2 2. m1.s = m2, for some 1 ≤ s ≤ kd 3. m1 = m2.s, for some 1 ≤ s ≤ kd  This lemma considers the properties of a pair of immediately dominating nodes n1 and n2 in the derived TAG tree. In the ﬁrst case, both nodes belong to the same instance of an elementary tree. Therefore, they are linked to the same node in the derivation tree, as illustrated in Figure 3. The second case n2 is the root of an adjoined tree. By the assumption we made above, the parent of a root node must be a node of the tree where the adjunction took place. Therefore n1 must be linked with the parent of the derivation tree node that is linked with n2, see Figure 4.  Palm  141  n1 m1 ...................................................................................................................................  .........................................................................  .........................................................................  n2 .................................................................................................................................  m2  Figure 4: T/D-tree: n2 is the root node of the elementary represented by m2 nn21 mm12 .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................  Figure 5: T/D-tree: n1 is the foot node of the adjoined elementary tree represented by m1  In the third case, n1 is the foot node of an adjoined tree and, by assumption, each of its children must be a node of the tree where the adjunction took place. Consequently, m1 must be a child of m2, see Figure 5. Finally, due to above assumptions, no other case is possible. Now we turn to the TAG axioms of HLTAG which ensure that a given formula describes a TAG. For the general tree axioms we refer to the similar modal tree logic as presented for example in (Blackburn, Meyer-Viol and de Rijke, 1996). However, the more interesting issue are the TAG axioms. They should ensure that HLTAG formulae only describe TAG-generated T/D-trees. For simpliﬁcation, we introduce two auxiliary propositions foot and root that mark the corresponding nodes of an adjoined elementary tree. The TAG-axioms standing below assert the correct distribution of the auxiliary propositions root and foot and the correct linking between the derived and the derivation tree.  (D1) Troot: root ∧ ♥Droot  (associating the root nodes)  (D2) (i ∧ root ∧ ♥k ∧ j: (root ∧ ♥k)) → j: i (unique root)  (D3) (i ∧ foot ∧ ♥k ∧ j: (foot ∧ ♥k)) → j: i (unique foot)  (D4) (i ∧ ♥k ∧ j: (root ∧ ♥k)) → j: (i ∨ i) (root domination)  (D5) rroot ↔ r♥i ∧ ♥ i  (link properties of the root)  (D6) foot ↔ ♥i ∧ ♥ i  (link properties of the root)  (D7) ¬foot ∧ r¬root ↔ ♥i ∧ r♥i  (link properties of the inner nodes)  The ﬁrst axiom asserts that in a t/d-tree T /D the underlying initial tree of the derive tree T is linked with the root of the derivation tree D. Actually, it is sufﬁcient that (D1) only links the root node of the derived tree root with root of the derivation tree. The correct linking of the remaining nodes of the initial tree follows from (D7). In order to access the root nodes of either tree, we assume two special nominal propositions Troot and Droot referring to the root nodes of T and D, respectively. The next two axioms (D2) and (D3) ensure that every instance of an elementary tree occurring in T /D has a unique root and a foot. We consider a root (or foot) node with the nominal i that is linked with derivation tree with the nominal k. Then every root (or foot) node that is linked with k must be identical to i. Moreover (D4) asserts that all nodes of the same instance of an elementary tree are dominated by the root node of this instance. Finally, the axiom (D5), (D6) and (D7) ensure the local structural properties mentioned in Lemma 3.1. By (D5), the r-th child of a node meets the proposition root if and only if the successor relationship also applies to the derivation tree nodes corresponding to them. By (D6), a node is a foot node if and only if it is linked to the node whose parent is associated with all children of the node considered. Finally, (D7) asserts that all pairs of immediately dominating nodes share the same instance of an elementary tree, if neither the upper one is its foot nor the lower one is its root.  142  Proceedings of TAG+6  Obviously, due to Lemma 3.1 and the properties of a TAG derivation, every T/D-tree that is generated by a given TAG must meet these axioms. Thus, these axioms are sound with respect to tree adjoining grammars. However, the opposite direction is less obvious. It states that every T/D-tree satisfying these axioms must be generated by a tree adjoining grammar. Next we describe a tree-extraction formalism that establishes this: 1. We arbitrarily select a leaf of the derivation tree with some nominal k and, further, we consider all nodes in the derived tree that are linked with k. 2. By the axioms (D2) and (D3) there must be a unique root and foot and by (D4) all nodes the are linked with k are weakly dominated by the the root. In addition, since k has no child, no other tree was adjoined. Therefore, due to (D5), (D6) and (D7) all nodes that are linked with k deﬁne a coherent tree section in the tree t. 3. We extract the tree section as identiﬁed previously, and we replace it by a single adjunction node that is linked with the parent of k.  4. We remove k in the derivation tree  5. We repeat the steps above until a single node in the derivation tree remains.  6. Due to (D1) the remaining structure deﬁnes the underlying initial tree of that TAG-tree. The trees we extracted above are the corresponding elementary trees.  This formalism illustrates how to construct a TAG for any given t/d-tree satisfying the axioms (D1) to (D7) such that the resulting TAG generates the given T/D-tree. In general, we obtain that a T/D-tree is TAG generated, if and only if it meets these axioms at every node of the derived tree. Moreover this formalism can be extended t in the following way. So far we know that every HLTAG formula that is consistent with the TAG axioms (D1) to (D7) speciﬁes a set of trees where each member is generated by a certain TAG. We brieﬂy sketch a method that constructs a corresponding TAG for a given TAG-consistent HLTAG formula. Therefore, we combine the above extraction formalism with an ordinary method of constructing tree models, especially tree automata, from a given modal tree description. The desired result are two linked treeautomaton for the derived tree and the derivation tree. Instead of linking to certain tree nodes of the derivation tree, we employ links to the states of the corresponding tree automaton. Then we can apply a slightly modiﬁed version of the extraction method to these tree automata. Instead of extracting trees, this modiﬁed version considers subtree automata. The ﬁnal result is a (ﬁnite) set of tree automata where each of them represents a set of initial trees. In addition, the resulting automaton for the derivation tree expresses possible adjunction operations for these automata. In order to construct these automata, we can employ, for instance, well-known labeled tableau methods as described in (Gore´, 1999), which were adopted for our purposes. The overall goal is to construct a set of simple tree automata representing the initial trees of the TAG described and another special tree automaton representing the corresponding derivation trees. Using labeled formulae in the tableau, we can indicate the node and the tree the formulae considered must apply to. A crucial part of the tableau system concerns the construction of the rth successor of some node by the formula rϕ:  ( r)  α, σ:: ♥i α, σ.r:: ϕ α, σ.r:: ♥i α, σ:: ¬foot α, σ.r:: ¬root  α, σ:: rϕ α, σ:: ♥ si α.s, σ.r:: ϕ α.s, σ.r:: ♥i α, σ.r:: root  
We recently introduced abstract categorial grammars (ACGs) (de Groote, 2001) as a new categorial formalism based on Girard linear logic (Girard, 1987). This formalism, which derives from current type-logical grammars (Carpenter, 1996; Moortgat, 1997; Morrill, 1994; Oehrle, 1994), offers some novel features: • Any ACG generates two languages, an abstract language and an object language. The abstract language may be thought as a set of abstract grammatical structures, and of the object language as the set of concrete forms generated from these abstract structures. Consequently, one has a direct control on the parse structures of the grammar. • The langages generated by the ACGs are sets of linear λ-terms. This may be seen as a generalization of both string-langages and tree-langages. • ACGs are based on a small set of mathematical primitives that combine via simple composition rules. Consequently, the ACG framework is rather ﬂexible. Abstract categorial grammars are not intended as yet another grammatical formalism that would compete with other established formalisms. It should rather be seen as the kernel of a grammatical framework — in the spirit of (Ranta, 2002) — in which other existing grammatical models may be encoded. This paper illustrates this fact by showing how tree-adjoining grammars (Joshi and Schabes, 1997) may be embedded in abstract categorial grammars. This embedding exempliﬁes several features of the ACG framework: • The fact that the basic objects manipulated by an ACG are λ-terms allows higher-order operations to be deﬁned. Typically, tree-adjunction is such a higher-order operation (Abrusci, Fouquere´ and Vauzeilles, 1999; Joshi and Kulick, 1997; Mo¨nnich, 1997). • The ﬂexibility of the framework allows the embedding to be deﬁned in two stages. A ﬁrst ACG allows the tree langage of a given TAG to be generated. The abstract language of this ﬁrst ACG corresponds to the derivation trees of the TAG. Then, a second ACG allows the corresponding string language to be extracted. The abstract language of this second ACG corresponds to the object language of the ﬁrst one. 2. Abstract Categorial Grammars This section deﬁnes our notion of an abstract categorial grammar. We ﬁrst introduce the notions of linear implicative types, higher-order linear signature, linear λ-terms built upon a higher-order linear signature, and lexicon. Let A be a set of atomic types. The set T (A) of linear implicative types built upon A is inductively deﬁned as follows: 1. if a ∈ A, then a ∈ T (A); 2. if α, β ∈ T (A), then (α −◦ β) ∈ T (A). A higher-order linear signature consists of a triple Σ = A, C, τ , where: 1. A is a ﬁnite set of atomic types; c 2002 Philippe de Groote. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 145–150. Universita´ di Venezia.  146  Proceedings of TAG+6  2. C is a ﬁnite set of constants; 3. τ : C → T (A) is a function that assigns to each constant in C a linear implicative type in T (A). Let X be a inﬁnite countable set of λ-variables. The set Λ(Σ) of linear λ-terms built upon a higher-order linear signature Σ = A, C, τ is inductively deﬁned as follows: 1. if c ∈ C, then c ∈ Λ(Σ); 2. if x ∈ X, then x ∈ Λ(Σ); 3. if x ∈ X, t ∈ Λ(Σ), and x occurs free in t exactly once, then (λx. t) ∈ Λ(Σ); 4. if t, u ∈ Λ(Σ), and the sets of free variables of t and u are disjoint, then (t u) ∈ Λ(Σ). Λ(Σ) is provided with the usual notion of capture avoiding substitution, α-conversion, and β-reduction (Barendregt, 1984). Given a higher-order linear signature Σ = A, C, τ , each linear λ-term in Λ(Σ) may be assigned a linear implicative type in T (A). This type assignment obeys an inference system whose judgements are sequents of the following form: Γ −Σ t : α where: 1. Γ is a ﬁnite set of λ-variable typing declarations of the form ‘x : β’ (with x ∈ X and β ∈ T (A)), such that any λ-variable is declared at most once; 2. t ∈ Λ(Σ); 3. α ∈ T (A). The axioms and inference rules are the following: −Σ c : τ (c) (cons)  x : α −Σ x : α (var)  Γ, x : α −Σ t : β (abs) Γ −Σ (λx. t) : (α −◦ β) Γ −Σ t : (α −◦ β) ∆ −Σ u : α (app) Γ, ∆ −Σ (t u) : β  Given two higher-order linear signatures Σ1 = A1, C1, τ1 and Σ2 = A2, C2, τ2 , a lexicon L : Σ1 → Σ2 is a realization of Σ1 into Σ2, i.e., an interpretation of the atomic types of Σ1 as types built upon A2 together with an interpretation of the constants of Σ1 as linear λ-terms built upon Σ2. These two interpretations must be such that their homomorphic extensions commute with the typing relations. More formally, a lexicon L from Σ1 to Σ2 is deﬁned to be a pair L = F, G such that:  1. F : A1 → T (A2) is a function that interprets the atomic types of Σ1 as linear implicative types built upon A2;  2. G : C1 → Λ(Σ2) is a function that interprets the constants of Σ1 as linear λ-terms built upon Σ2;  3. the interpretation functions are compatible with the typing relation, i.e., for any c ∈ C1, the following typing  judgement is derivable:  −Σ2 G(c) : Fˆ(τ1(c)),  where Fˆ is the unique homomorphic extension of F .  We are now in a position of deﬁning the notion of abstract categorial grammar. An abstract categorial grammar is a quadruple G = Σ1, Σ2, L , s where:  Ph. de Groote  147  1. Σ1 and Σ2 are two higher-order linear signatures; they are called the abstract vovabulary and the object vovabulary, respectively ; 2. L : Σ1 → Σ2 is a lexicon from the abstract vovabulary to the object vovabulary; 3. s is an atomic type of the abstract vocabulary; it is called the distinguished type of the grammar. The abstract language generated by G (A(G )) is deﬁned as follows: A(G ) = {t ∈ Λ(Σ1) | −Σ1 t : s is derivable} In words, the abstract language generated by G is the set of closed linear λ-terms, built upon the abstract vocabulary Σ1, whose type is the distinguished type s. On the other hand, the object language generated by G (O(G )) is deﬁned to be the image of the abstract language by the term homomorphism induced by the lexicon L : O(G ) = {t ∈ Λ(Σ2) | ∃u ∈ A(G ). t = L (u)} 3. Representing Tree-Adjoining Grammars In this section, we explain how to construct an abstract categorial grammar that generates the same tree langage as a given tree-adjoining grammar. Let G = Σ, N, I, A, S be a tree-adjoining grammar, where Σ, N , I, A, and S are the set of terminal symbols, the set of non-terminal symbols, the set of initial trees, the set of auxiliary tree, and the distinguished non-terminal symbol, respectively. We associate to G an ACG G G = ΣG1 , ΣG2 , L G, sG as follows. The set of atomic types of ΣG1 is made of two copies of the set of non-terminal symbols. Given α ∈ N , we write αS and αA for the two corresponding atomic types. Then, we associate a constant cT : γ1A −◦ · · · γmA −◦ β1S −◦ · · · βnS −◦ αS to each initial tree T whose root node is labelled by α, whose substitution nodes are labeled by β1, . . . , βn, and whose interior nodes are labeled by γ1, . . . , γm. Similarly, we associate a constant cT : γ1A −◦ · · · γmA −◦ β1S −◦ · · · βnS −◦ αA −◦ αA to each auxiliary tree T whose root node is labelled by α, whose substitution nodes are labeled by β1, . . . , βn, and whose interior nodes are labeled by γ1, . . . , γm. Finally, we also associate to each non-terminal symnbol α ∈ N , a constant Iα of type αA. This concludes the speciﬁcation of the abstract vocabulary. The object vocabulary ΣG2 allows labelled trees to be represented. Its set of atomic types contains only one element : τ (for tree). Then, its set of constants consists in: 1. constants of type τ corresponding to the terminal symbols of G; 2. for each non-terminal symbol α, constants αi : τ −◦ · · · τ −◦ τ i times for 1 ≤ i ≤ k, where k is the maximal branching of the interior nodes labelled with α that occur in the initial and auxiliary trees of G. Clearly, the terms of type τ that can be built by means of the above set of constants correspond to trees whose frontier nodes are terminal symbols and whose interior nodes are labelled with non-terminal symbols. It remains to deﬁne the lexicon L G. The rough idea is to represent the initial trees as trees (i.e., terms of type τ ) and the auxiliary trees as functions over trees (i.e., terms of type τ −◦ τ ). Consequently, for each α ∈ N , we let L G(αS) = τ and L G(αA) = τ −◦ τ . Accordingly, the susbstitution nodes will be represented as ﬁrst-order λvariables of type τ , and the adjunction nodes as second-order λ-variables of type τ −◦ τ . The object representation of the elementary trees is then straightforward. Consider, for instance, the following initial tree and auxiliary tree:  148  Proceedings of TAG+6  yyyyyy S DDDDDD  NP↓  {{{{{{VP@@@@@  V  NP↓  }}}}}VPBBBBB  V  VP∗  has  loved According to our construction, the two abstract constants corresponding to these trees have the following types: Cloved : SA −◦ VPA −◦ VA −◦ NPS −◦ NPS −◦ SS and Chas : VPA −◦ VA −◦ VPA −◦ VPA Then, the realization of these two constants is as follows: L G(Cloved) = λF. λG. λH. λx. λy. F (S2 x (G (VP2 (H (V1 loved)) y))) L G(Chas) = λF. λG. λH. λx. F (VP2 (G (V1 has)) (H x)) In order to derive actual trees, the second-order variables should eventually disappear. The abstract constants Iα have been introduced to this end. Consequently they are realized by the identity function, i.e., L G(Iα) = λx. x. Finally, the distinguished type of G G is deﬁned to be SS. This completes the deﬁnition of the ACG G G associated to a TAG G. Then, the following proposition may be easily established. PROPOSITION Let G be a TAG. The tree-language generated by G is isomorphic to the object language of the ACG G G associated to G.  4. Example  Consider the TAG with the following initial tree and auxiliary tree:  S  }}}}} S AAAAA  c  a }}}}} S AAAAA e  b S∗ d  It generates a non context-free language whose intersection with the regular language a∗b∗c d∗e∗ is anbnc dnen. According to the construction of Section 3, this TAG may be represented by the ACG, G = Σ1, Σ2, L , S , where:  Σ1 =  {SS, SA}, {ci, ca, I}, {ci → (SA −◦ SS), ca → (SA −◦ (SA −◦ (SA −◦ SA))), I → SA}  Σ2 =  {τ }, {a, b, c, d, e, S1, S3}, {a, b, c, d, e → τ, S1 → (τ −◦ τ ), S3 → (τ −◦ (τ −◦ (τ −◦ τ )))}  L = {SS → τ, SA → (τ −◦ τ )}, {ci → λf. f (S1 c), ca → λf. λg. λh. λx. f (S3 a (g (S3 b (h x) d)) e), I → λx. x}  Ph. de Groote  149  5. Extracting the string languages  There is a canonical way of representing strings as linear λ-terms. It consists of encoding a string of symbols as a composition of functions. Consider an arbitrary atomic type σ, and deﬁne the type ‘string’ to be (σ −◦ σ). Then, a string such as ‘abbac’ may be represented by the linear λ-term: λx. a (b (b (a (c x)))),  where the atomic strings ‘a’, ‘b’, and ‘c’ are declared to be constants of type (σ −◦ σ). In this setting, the empty word is represented by the identity function: = λx. x and concatenation is deﬁned to be functional composition:  α + β = λα. λβ. λx. α (β x),  which is indeed an associative operator that admits the identity function as a unit. This allows a second ACG, G G, to be deﬁned. Its abstract vocabulary is the object vocabulary ΣG2 of G G. Its object vocabulary allows string of terminal symbols to be represented. Its lexicon interprets each constant of type τ as an atomic string, and each constant αi as a concatenation operator. This second ACG, G G, extracts the yields of the trees. Then, by composing G G with G G, one obtains an ACG which generates the same string-language as G. Let us continue the example of Section 4. The second ACG, G = Σ1, Σ2, L , S , is deﬁned as follows:  Σ1 = Σ2  Σ2 = {σ}, {a, b, c, d, e}, {a, b, c, d, e → (σ −◦ σ)}  L=  {τ → (σ −◦ σ)}, {a → λx. a x, b → λx. b x, c → λx. c x, d → λx. d x, e → λx. e x, S1 → λf. λx. f x, S3 → λf. λg. λh. f (g (h x))}  6. Expressing Adjoining constraints  Adjunction, which is enabled by second-order variables at the object level, is explicitly controlled at the abstract level by means of types. This typing discipline may be easily reﬁned in order to express adjoining constraints such as selective, null, or obligatory adjunction. 
This paper deals with the issue of the generative capacity of a certain version of type logical categorial grammar. Originally categorial grammar in the modern sense was invented in three varieties in the late ﬁfties and early sixties. Bar-Hillel (1953) developed applicative categorial grammar, a bidirectional version of older type theoretic systems tracing back to the work of Polish logicians in the early twentieth century. Few years later, Lambek (1958) proposed his calculus of syntactic types that is known as the (associative) “Lambek calculus” nowadays (abbreviated as “Ł”). A short time later he published a non-associative version of this calculus in (Lambek, 1961), which is known as the “Non-associative Lambek calculus” NL. These two systems are the ﬁrst instances of “Type Logical Grammars”, i.e. the deductive machinery of the grammar formalism is a substructural type logical calculus. The issue of the position of these grammar formalisms within the Chomsky hierarchy has intrigued mathematical linguists from the beginning. It was settled ﬁrst for applicative categorial grammar by Bar-Hillel, Gaifman and Shamir (1960). They establish the weak equivalence of this version of categorial grammars with the context free languages. For the type logical categorial grammars, this problem was settled fairly late. Buszkowski (1986) established the the product free fragment of the non-associative Lambek calculus deﬁnes exactly the context free languages, and Kandulski (1988) showed that this result carries over to full NL. Finally, (Pentus, 1993) gives a proof that the associative Lambek calculus Ł is weakly equivalent to the context free grammars as well. It was already conjectured in (Chomsky, 1957) that context free grammars are not expressive enough to give an adequate description of the grammar of natural languages. This was formally proved in (Shieber, 1985). So it seems that none of the tree basic varieties of categorial grammar provides an adequate grammar formalism for linguistics. This shortcoming motivated to move to multimodal systems, i.e. type logical grammars that employ several families of connectives and certain interactions between them. This idea was ﬁrst explored in (Moortgat, 1988) and (Morrill, 1990), and systematized in (Moortgat, 1996). There it is assumed that it is sufﬁcient to use a ﬁnite ensemble of residuation connectives plus some interaction postulates between them to come to terms with the empirical facts of natural language. This idea is conﬁrmed but trivialized by (Carpenter, 1999), where it is proved that multimodal type logical grammars are equivalent in generative power to Turing machines. Considering Carpenter’s proof, it seems intuitively obvious that the unrestricted use of interaction postulates is responsible for this increase in generative capacity, while the notion of multimodality as such has no such effect. This is partially conﬁrmed by Ja¨ger (2001). This article gives a proof that enriching the associative Lambek calculus with pairs of unary residuation connectives without interaction postulates does not increase generative power; the resulting system still describes exactly the context free languages. This is established by a straightforward extension of Pentus’ construction. For a non-associative base logic, several important results in this connection have been obtained by Maciej Kandulski (see (Kandulski, 1995; Kandulski, 2002)). He generalizes his result from (Kandulski, 1988) to the commutative version of the non-associative Lambek calculus, and to multimodal logics comprising an arbitrary number of different families of residuation connectives of any arity, but without structural rules. Kandulski’s results are all based on an axiomatization of the type logics underlying the grammars in question and a process of proof normalization within this axiomatic calculus. The present paper presents alternative proofs of these theorems that are based on the Gentzen style sequent presentation of the logics involved. This new proof strategy leads to generalizations of Kandulski’s results in two respects: Any combination of residuated connectives with any of the structural rules Permutation, Contraction and Expansion lead to type logical grammars that recognized only context free languages, and this also holds if we admit non-atomic designated types. The structure of the paper is as follows. We ﬁrst focus on the simplest multimodal extension of the nonassociative Lambek calculus, namely the calculus NL from (Moortgat, 1996). In section 2 we introduce the necessary technical notions, and section 3 presents the proof that grammars based on NL recognized exactly the context free languages. In section 4 we generalize these results to residuation modalities of arbitrary arity, and to ¡ c 2002 Gerhard Ja¨ger. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 151–158. Universita´ di Venezia.  152  Proceedings of TAG+6  calculi using the structural rules Permutation, Contraction or Expansion. Section 5 summarizes the ﬁndings and points to desiderata for further research. 2. Technical Preliminaries  2.1. NL  The non-associative Lambek calculus NL is the weakest substructural logic. Its logical vocabulary consists of ¡ ¢ one binary product and its left and right residuation, the two directed implications and . More formally, the ¤¦¥¨§©§ £ ¤ ¡ ¤  ¤ ¤  ¤ ¢ ¤ £ types of NL are deﬁned recursively over some ﬁnite alphabet of atomic types as     The calculus itself can be deﬁned as a set of arrows, i.e. objects of the form  , where and are types   !"$#%#©# &(' of NL. In its axiomatic presentation, the calculus comprises the identity axiom and the Cut rule. We use the upper  case Latin letter  as meta-variables over types.  )  ) ! 0)! !"132  45 ¡ !  46! )!7¢8 The behavior of the logical connectives is governed by the residuation laws  iff  iff  Lambek also gives a Gentzen style sequent presentation of NL. A sequent consists of an antecedent and a  succedent, where the antecedent is a binary tree over types and the succedent a single type. We write trees as terms 9 @ ¥¨§©§A¤ %B @ 9 @¨C formed from types and the binary operation . Formally, the set of NL-trees is thus given by  DEGFHPIQR#©#©# DTS F7U D The upper case Latin letters  are meta-variables over trees of types.  F DTSVIWU `Y5 &X' DaF Y5D FdS DeIUfY5F  S bUcY5 !"132 sub-tree , and  is the result of replacing the sub-tree in by .  is a tree containing a  DTS T9ghUfY! DTS hUfY!  8i  DaY5 FpY) Dr9gF4Y5   8q  DsY5 FShUfY)! F St¢uv9HDeUwY!  ¢i  Dx9gY5 DaY)"¢8  ¢q  DsY5 FdS Dx9g  ¡  FS hUfY)! UwY)!  ¡i  vDs9HY5DaY5¡   ¡ q  Figure 1: Sequent presentation of the non-associative Lambek calculus NL  ydDsY) DaY We write “NL  ” iff the sequent  is derivable in the sequent calculus. The axiomatic and the  9 sequent presentation of NL are equivalent in the sense that every derivable arrow is also a derivable sequent, and replacing all occurrences of in a derivable sequent by the product yields a derivable arrow.  It is easy to see that all rules in the sequent calculus except Cut have the subformula property. Lambek proved  Cut elimination for the sequent calculus, which establishes decidability.  Ja¨ger  153  2.2. NL  (Moortgat, 1996) extends the format of type logics in two ways. He considers calculi that comprise more than  one family of residuated operators, and he generalizes Lambek’s binary operators to the -ary case. In the present  paper, we will be mainly concerned with one of the simplest version of such a multimodal system, namely the  combination of one binary product and its accompanying implications with one unary product and its residuated  counterpart. The resulting logic is dubbed NL . The logical vocabulary of NL extends the vocabulary of NL with two unary connectives, £ set of NL -types is over the atoms is given by ¤ ¥¨§%§ £d¤ ¡ ¤ ¤ ¤ ¤ ¢ ¤  ¤  ¤¡¤¢  ¡£¢ and . So the  They form a pair of residuated operators, i.e. their logical behavior is governed by the residuation law   iff ¡¤¢ `5  The axiomatic presentation of NL consists just of the axioms and rules of NL plus the above residuation law.  9 (Moortgat, 1996) also gives a sequent presentation of NL . Now the trees that¥§¦o¨ ccur in the antecedent of a sequent is composed from types by two operators, a binary one ( ), and a unary one ( ), corresponding to the two  products and . So we have  @ ¥¨§%§A¤ %B @ 9 @ C  ¥ @ ¨  Moortgat’s sequent calculus for NL is obtained by exten¡ d¢ing the sequent calculus for NL with the following four rules, i.e. a rule of use and a rule of proof for both and .  ¥ D Da¨ Y)Y   i  DTS ¥  ¨ UwY5 DTS bUfY)  q  DTS bU3Y) DTS ¥ ¡©¢  ¨ UcY5  ¡©¢ q  ¥ D ¨ Y5 DsY ¡¤¢   ¡ ¢q  Figure 2: Sequent rules for the unary modalities in NL  As in NL, all sequent rules of NL have the subformula property. By proving Cut elimination for NL , (Moortgat, 1996) thus establishes decidability, and he also proves the equivalence of the axiomatic with the sequent presentation.  2.3. Logic and grammars  A type logic like NL is the deductive backbone of a type logical grammar. The grammar itself consists just  of the lexicon, i.e. an assignment of types to lexical items, and a collection of designated types (which is sometimes  ¤  ¤ tacitly assumed to Deﬁnition 1 (NL  be the singleton set  , but I assume -grammar) An NL -grammar over  aanmaolrpehgaebneetr alinsoatipoanirof¥  gr§a m¨ m, warhhereere ).,  the  lexicon,  is  a  
1. Introduction In this paper the NP-completeness of the system LP (associative-commutative Lambek calculus) will be shown. The complexity of LP has been known for some time, it is a corollary of a result for multiplicative intuitionistic linear logic (MILL)1 from (Kanovich, 1991) and (Kanovich, 1992). We show that this result can be strengthened: LP remains NP-complete under certain restrictions. The proof does not depend on results from the area of linear logic, it is based on a simple linear-time reduction from the minimum node-cover problem to recognizing sentences in LP. 2. Deﬁnitions  First some deﬁnitions are in order: D¢e¡¤ﬁ£¦n¥§i¡¤t¡¦io¨n©1 The  degree o©f aty pe¥ is deﬁned as ¢¡¤£¦¥§¡¤¡¦¨!#"$©%'&)i(0f ¢¡¤£¦¥§¡¤¡¦¨©1(0¢¡¤£¦¥§¡¤¡¦¨!2 ¢¡¤£¦¥§¡¤¡¦¨©43$!2%'&)(0¢¡¤£¦¥§¡¤¡¦¨©1(0¢¡¤£¦¥§¡¤¡¦¨!2  In other words, the degree of a type can be determined by counting the number of operators it contains.  D55 e¥6¥6ﬁ¢¢n¡¤¡¤i¥7¥7t¨¨io!#©n"$2©% TheA  Ord@C eirB$fDEo©8f¨F&)a9( ty p5 ¥ e¥6¢is¡¤¥¤d¨e©ﬁ1ne( d  5  as ¥6¢¡¤¥7¨!2G  5 ¥6¢¡¤¥7¨©43$!2%A@CB$DE¨F&)( 5 ¥6¢¡¤¥¤¨©1( 5 ¥6¢¡¤¥7¨!2G  ¨G¨©43$!2§3IHP  Deﬁnition 3 ! A domHain subtype is a subtype that is in domain position, i.e. for the type  the domain  subtypes are ¨QHR"¢an¨!#d "$©. G  H  !  For the type  the domain subtypes are and .  ¨G¨©43$!2§3IHP  ¨©43$!2A range© subtype is a subtype that is in range position, i.e. for the type  the range subtypes are  and ¨QHR. "¢¨!#"$©G  ¨!#"$© ©  For the type  ©43$!TthS§e!VraUWng© e sub!TtypS§!#es"$a©XreUW© and ! .  ©43$!  !#"$©  In an applicaton  or  the type is an argument and  and  are known as  functors.  Deﬁnition 4 Let Y as tuples of nodes. each node ‘covers’  `¨Qa S§bc  a  A  be an node-cover of  uY ndisiraecstuebdsgertaa2phdf,egwha erseuchitshaatseift  its incident edges, and a node cover for Y is a set  b  o¨ihpf nSGq¢orde9s ab nd ishWa Wsetacod f edgqseWs, raRepd resented  , then  or  . Tb hat is,  of nodes that covers all the edges in . The  size of a node-cover is the number of nodes in it.  The node-cover problem is the problem of ﬁnding a node-cover of minimum size (called an optimal node-  cover) in a given graph. The node-cover problem can be restated as a decision problem: does a node-cover of given size t exist for  some given graph? Pisruop o-shiatirodn. 5 The decision problem related to the node-cover problem is u  -complete, The node-cover problem  This problem has been called one of the ‘six basic NP-complete problems’ in (Garey and Johnson, 1979).  1. The systems LP and MILL are identical up to derivation from the empty sequent, i.e. the only difference is that vcwyxw is not derivable in LP. The system MILL is closely related to MILL1, another system that has interesting linguistic applications, see (Moot and Piazza, 2001).   c 2002 C. Costa Floreˆncio. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 159–162. Universita´ di Venezia.  160 3. Complexity of LP  Proceedings of TAG+6  Theorem 6 Deciding membership for the unidirectional product-free fragment of LP, with all types restricted to ¡¢ a maximum degree of 2 and a maximum order of 1, is NP-complete in .  Proof: It is well known that LP is in NP.  What remains to be be an undirected graph, graph Y can be reduced  t£¥soh¤ oaw gnrabims em. xLaisrettY¢enH  ce §o£¦f¦¥6aBI¨ pY@-t¨ iYmbeeaarsemdfouinlclitomiowunsm:fronmodaencoNvPe-rcoofmY pl,eatendpr@©ob¨ ple¨ mY .  L et  Y  ¦¨  `¨bCS6ac Y  . The  1. Assign  to s.  2.  Lq"Pet    a    b¨Qea#"¦th eSGqf$un ct io¨Qna#$$th.aAt smsiagpns  node types  q'"¦"  q  qto" tqy"¦p"¢e¨ ,    "  .  b8 b  For and  qev$Ie" rqy$  eqd$Ig"¢e¨ ,    "      , where 
1. Introduction  This paper describes a projective, bilexical dependency grammar, and discusses its afﬁnity with TAG. Common features of the two formalisms include a tree-like surface syntactic structure and readiness for a lexicalised treatment. TAG surface structures built from elementary and auxiliary trees by means of substitution and adjunction can correspond to trees consisting entirely of lexical nodes and dependency arcs. Lexical anchors in TAG, a well-motivated notion, can also be accommodated in the dependency grammar formalism, provided it is recognized that the dependent, as well as the governor, can have a vote about the formation of a dependency relation. It is noted, however, that mirroring obligatory adjuncts in TAG in dependency grammar can be problematic.  2. Dependency Analysis 2.1. Projective Dependency Structures  Though not supported by all schools of dependency grammar (Tesnie`re, 1959), some followers of dependency grammar assume that there is a projective surface or back-bone dependency structure. The theoretical foundation of this tradition can be traced to Gaifman (1965) and Hays (1964), and is summed up in the following well-formedness conditions for dependency structures in Robinson (1970):  • one and only one element is independent; • all others depend directly on some element; • no elements depend directly on more than one other; • if A depends directly on B and some element C intervenes between them (in linear order of string), then C depends directly on A or on B or some other intervening element.  These conditions say, in effect, that conforming dependency structures are representable by trees without crossing branches. Of courses, as in other grammar formalisms that pre-suppose a context-free syntactic structure back-bone, additional linguistic constraints can be incorporated in the formalism by means of various mechanisms, e.g. feature uniﬁcation.  2.2. Dependency Structures without Phrasal Nodes  In the dependency grammar formalism (Lai and Huang, 1998; Lai and Huang, 2000) discussed in this paper, dependency structures are trees consisting entirely of lexical nodes. For example, the dependency tree for (1a), taken from Abeille´ (1993), is (1b): (1) a. Jean dort beaucoup Jean sleeps much ‘Jean sleeps a lot.’  b.  dort  |  ---------------------------  | subj  adjunct |  |  |  Jean  beaucoup  When a coarser degree of granularity is warranted by the situation, the actual lexical items in the tree nodes can be replaced by their syntactic categories.  c 2002 Tom B.Y. Lai. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 169–174. Universita´ di Venezia.  170  Proceedings of TAG+6  2.3. Statistical Dependency Analysis In the computational linguistics community, dependency structures are often parsed, exploiting their afﬁnity with phrase-structure structures, with the help techniques used with context-free grammars (Hellwig, 1986). On the other hand, Collins (1996) uses ‘bilexical’ co-ocurrence probabilities (of governors and their head daughters) to estimate the likelihood of phrase-structures in the syntactic analysis of sentences. In a recent effort on Chinese (Lai et al., 2001), bilexical probabilities have been used directly to derive, without direct reference to context-free grammar and phrasal structures, dependency structures using a CYK-like algorithm (Eisner, 1996; Eisner, 2000). The probabilistic model in Lai and Huang (2000) uses conditional probabilities deﬁned in terms of dependencies. Factors considered include both dominance and ‘function’, as well as other contextual factors like relative proximity to the governor. No phrase structures are generated and the dependency structures consist of only binary bilexical relations. A CYK-like algorithm is used to construct optimal non-constituent structures that ‘span’ chunks of contiguous words until the ‘span’ covers the whole input. In the experiment reported, a training set of about 40 M of text was taken from a two-gigabyte Chinese newspaper corpus. A lexicon of about 60,000 entries was generated. The performance of the statistical dependency parser was gauged against the annotations in training corpus. For the more stringent criterion of getting both the dominance relation and the functional label correct at the same time, closed and open test averages were 94.7% (95.6% correct in the training corpus) and 74.2% (94.9% correct in the training corpus).  3. Dependency and Lexicalized TAG  The formal properties of this dependency grammar formalism can be compared with those of lexicalised TAG as described in (Abeille´, 1993; Abeille´ and Rambow, 2000).  3.1. Initial Trees and Substitution  TAG trees are derived by applying the operations of substitution and adjunction to initial trees and auxiliary  trees respectively. Initial trees like (2) account for the complements in the projection of a subcategorizing word  (e.g. dort ‘sleeps’).  (2)  V  |  ----------------------  |  |  N  V  --->  |  dort  In this example, the arrow attached to the node N indicates that a similar initial tree for a noun (an N-tree) can replace the node by the substitution operation. To avoid confusion, we avoid the use of the word ‘head’, but dort can be safely called the ‘anchor’ of the tree, which forms a part of its lexical property. In the dependency grammar formalism, a verb like dort will subcategorize for each of its complement dependents. (3) dort | | subj | Jean  A word subcategorizes for each of its complement dependents separately. Positional constraints can be added to handle multiple complements.  3.2. Auxiliary Trees and Adjunction In TAG, adjuncts are accounted for by auxiliary trees and the operation of adjunction. For example, the word beaucoup ‘a lot’ is the anchor of the auxiliary tree (4):  Lai, Huang, and Luk  171  (4)  V  |  ------------------  |  |  V*  Adv  |  beaucoup  This auxiliary tree is a lexical property of the anchor beaucoup. It can be used to replace a V-tree (a sub-tree) in a syntactic structure. The replaced V-tree is then, used to replace, in turn, the V* node in the auxiliary tree. The beaucoup auxiliary tree is ‘adjoined’ to a tree representing the sentence Jean dort to obtain Jean dort beaucoup. In TAG, an adjunct is on the same level as or higher than the head word of the phrase in the derived syntactic tree as in (5a) or (5b).  (5) a.  V |  -------------  V  Adv  |  |  dort  beaucoup  b.  VP  |  -------------  VP  Adv  |  |  V  beaucoup  |  dort  c. dort | --------| beaucoup  TAG is probably correct in letting adverbs (e.g. beaucoup) decide that they are to be adjoined to verbs. In the dependency grammar formalism, a verb (e.g. dort) governs an adverb (e.g. beaucoup). (See (5c) and the dependency structure for Jean dort beaucoup in (1b).) However, there is nothing to prevent a dependent adjunct from determing (as in TAG) what it should be adjoined to. Where only initial trees and substitutions are involved, it is obvious that TAG derived trees can be pruned into dependency structures. (Dependency relation labels like ‘subject’ can obtained from lexical information or from the conﬁguration of the tree.) Conversely, dependency structures can also be ﬂeshed out to form TAG derived trees (with minimal structure). Adjunction makes the situation somewhat more complicated as the adjunct has to be placed higher up than the ‘head’ word in the TAG derived tree. This is possible because the substitution-adjunction distinction is obtainable from dependency relation labels (e.g. ‘adjunct’). in the dependency structure. Similar grammatical information can be stored in the lexicon in either formalism. Additional mechanisms, e.g. feature uniﬁcaiton, can also be added on top of the tree backbone in both formalisms.  4. Some Complications The adjuncts (daughters in dependency grammars) that we have looked at are optional. However, with the presence of adjunction constraints or top and bottom features in TAG, adjuncts can either be optional or obligatory.  172  Proceedings of TAG+6  4.1. Auxiliary Adjunction 
1. Introduction One of the most difficult issues within corpora annotation on an underlying syntactic level is the restoration of nodes omitted in the surface shape of the sentence, but present on the “underlying” or “deep” syntactic level. In the present paper we concentrate on such type of nodes which are omitted due to the phenomenon usually called grammatical “control” with regard to their respective anaphoric relations. In particular, we extend the notion of control to nominalization and demonstrate how this relation is captured in the Prague Dependency Treebank. The theory of control is present within Chomsky’s framework of Government and Binding (using the terms verb of control, controller and controllee, cf. Chomsky, 1980), but also within many other formal frameworks, e.g. GPSG (Sag and Pollard, 1991) or categorial grammar (Bach, 1979). We analyse this phenomenon within the framework of the dependency grammar, theoretically based on the Functional Generative Description (FGD, cf. Sgall, Hajičová and Panevová, 1986). In FGD, on the “underlying” or “tectogrammatical” level, control is a relation of an obligatory or an optional referential dependency between a controller (antecedent) and a controllee (empty subject of the nonfinite complement (= controlled clause)). The controller is one of the participants in the valency frame of the governing verb (Actor (ACT), Addressee (ADDR), or Patient (PAT)). The controlled clause functions also as a filler of a dependency slot in the valency frame of the governing verb, being labeled as Patient or Actor. The empty subject of the controlled clause may have the function of different dependency relations to its head word (the infinitive): Actor, or, with passivization of the controlled clause, Addressee or Patient (cf. Koktová, 1992). 2. Capturing of “control” phenomena in the PDT In the present section we focus on the capturing of the phenomenon of control in the Prague Dependency Treebank (PDT), a three-layer annotated corpus of Czech, basically conceived of in accordance with the theoretical assumptions of the FGD (for more information about PDT cf. Hajič: Tectogrammatical Representation: Towards a Machine Transfer in Machine Translation, this volume). 2.1. Restoration of deletions and capturing of coreferential relations in the PDT One of the basic principles of annotation of the PDT at the tectogrammatical level concerns also restoration of deletions: in the cases of deletion in the surface sentence, nodes are introduced into the tectogrammatical tree to 'recover' a deleted word. It includes also a restoration of deleted participants of valency frames of verbs. When the nodes deleted in constructions of control are restored, annotators should indicate coreferential relations between the arguments in positions of the controller and the controllee. For labeling these coreferential relations the following attributes (grammatemes) of the general scheme are relevant: COREF(erence) - the value of this attribute is the lexical value of the antecedent of the given anaphoric node (this node itself may be present on the surface, or deleted) ANTEC(edent) – the value of this attribute corresponds to the functor of the antecedent with grammatical coreference1 CORNUM – refers to the antecedent of the given node2. The Controllee gets the special lemma Cor. Let us present here some illustrative examples of rather complicated sentences from our annotated corpus that exhibit relations between the arguments in positions of the controller and the controllee. * Supported by the Ministry of Education of the ČR Project LN00A0063. 1. For the difference between the textual and the grammatical coreference see Hajičová, Panevová and Sgall, 2000. 2 Technically, the CORNUM is the only attribute that has to be marked, since the attributes COREF and ANTEC can be then easily extracted from the referred-to node. For the reason of perspicuity we refer to all the three attributes separately. © 2002 Jarmila Panevová, Veronika Řezníčková, and Zdeňka Urešová. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 175-180. Universitá di Venezia.  176  Proceedings of TAG+6  (1) Poukazuje na poslance, kteří jsou v zájmu dosažení kompromisu schopni překročit únosnou mez. 'He refers to deputies who are able in the interest of the compromise to cross the bearable limit.'  poukazovat PRED refer  on poslanec ACT PAT he deputy  být RSTR be  který  dosažení  schopný  ACT  AIM  PAT  who  in the interest of able  ANTEC: PAT  COREF: poslanec  Gen  kompromis  překročit CPL  ACT  PAT  PAT  compromise  cross  Cor  mez  ACT  PAT  limit  ANTEC: ACT  COREF: který  únosný RSTR bearable  (2) Musím se stavit v čistírně, abych se zbavil toho kabátu, který jsem slíbil odnést. 'I must stop at the cleaners to get rid of the coat (which) I promised to take away.'  stavit se PRED stop  já čistírna zbavit se ACT LOC AIM I cleaners get rid of  já  kabát  ACT PAT  I  coat  slíbit RSTR promise  já Gen ACT ADDR I  odnést PAT take away  který  Cor  PAT  ACT  which  ANTEC: PAT ANTEC: ACT  COREF: kabát COREF: já  Panevová, Řezníčková, and Urešová  177  2.2. Survey of views on “control” phenomena with verbs in the FGD 2.2.1. Classification of verbs of control with controlled infinitive clauses Koktová and Panevová classify the verbs of control according to the type of its valency frame and to the functions of the controlled infinitive clause and the controller in the valency frame of the verb of control (see Koktová, 1992, and Panevová 1986, 1996). According to this classification the following basic groups of verbs of control should be recognized (we leave out here some groups with really rare types of verbs of control, e.g. verbs with the so-called Slavonic Accusative with Infinitive, e.g. Viděl Karla přicházet (lit. He saw Charles tocome)): 1. The controlled infinitive clause functions as Patient: three groups of verbs of control in Czech can be distinguished, namely verbs in the valency frame of which the Controller is: i) ACT (e.g. Jan se bojí zůstat doma sám (John is afraid to stay at home alone)) ii) ADDR (e.g. Redaktor doporučil autorovi provést několik změn v textu (An editor recommended the author to make several changes in the text)) iii) ACT or ADDR (the verb slíbit (promise) with the Controller functioning as ACT: e.g. Jan slíbil matce vrátit se domů před půlnocí (John promised his mother to return at home before midnight); the same verb with the Controller functioning as ADDR e.g. Rodiče slíbili dětem užít si prázdniny ve stanu u rybníka (lit.: The parents promised (their) children to enjoy the holidays in a tent by a lake)) 2. The controlled infinitive clause functions as Actor: especially the “predicate” of control (expressed by a copula with an evaluative or modal adjective) is taken into account (e.g. Je snadné číst tu knihu (It is easy to read the book)) 3. The controlled infinitive clause can have also another function, as cases based on the operation of raising (e.g. Viktor se zdá být chytrý (Viktor seems to be clever)) and the function of attribute (e.g. Viktor nesmí propást šanci vyhrát (Viktor may not miss the occasion to win)). 2.2.2. Extension of verbs of control also to the so-called “analytical predicates” The most typical verbs of control (belonging to the group (1)(i)) are modal verbs (e.g. moci (can), smět (may), chtít (want), muset (must), mít (have to)) and so-called “phase verbs” (e.g. začít (begin), zůstat (stay), přestat (stop)). While describing the phenomenon of control, it seems to be necessary to extend the understanding of the notion of modal verb also to another synonymous expressions of these verbs. Thus the function of modal verbs is undertaken not only by “modal verbs in the wider sense” (umět (be able), dovést (know how to do sth), dokázat (manage), zdráhat se (hesitate), odmítat (refuse) etc.) but also by “analytical predicates” with modal meaning (the verb mít (have) plus a noun, e.g. mít schopnost (lit. have an ability), dar (lit. have a gift / talent), potřebu (have an urge to do sth), příležitost (have an opportunity), šanci (have a chance); the verb být (be) plus a modal adjective, e.g. být schopen (be able), ochoten (be willing), povinen (be obliged)). Also some verbs from other semantic groups of verbs of control can be expressed by some type of “analytical predicate”. For example verbs expressing intent, e.g. hodlat (intend), snažit se (try), can be paraphrased by predicates mít v úmyslu (úmysl), záměr (lit. have an intention), mít v plánu (plán) (lit. have a plan), mít tendenci (lit. have a tendency) etc.; být připraven (be ready), odhodlán (be determined) etc. (they belong also to the group (1)(i)). Verbs expressing the meaning “umožnit někomu udělat něco“ (make it possible for somebody to do something) can be paraphrased by analytical predicates dát někomu šanci (příležitost) udělat něco (lit. give somebody a chance (an opportunity) to do sth) (these verbs belong to the group (1)(ii)). 2.2.3. Verbs of control with controlled nominalizations Panevová (1996) deals not only with controlled infinitive verb structures but also with some types of nominalizations where an omission of an argument is also based on the “control” properties of the head (governing) word and must be interpreted as coreferentiality. The group of verbs that offer the possibility for controlled nominalization includes for example verbs from the semantic group of causing a change of a physical and/or mental state, e.g. přisoudit (adjudge), osočit (accuse), podezírat (suspect): Paní podezírá komornou z krádeže stříbrných příborů (The lady suspects the chamber-maid of the theft of silver covers)).  178  Proceedings of TAG+6  2.3. Nominalizations in constructions of control The restoration of deletions in PDT includes not only the restoration of all obligatory participants and obligatory free modifications of verbs deleted at the surface shape of the sentence, but also the restoration of obligatory members of valency frames of postverbal nouns and adjectives formed by the process of nominalization. 2.3.1. From verbs to nouns By nominalizations we understand: a) Nouns derived from verbs by productive means (e.g. rozhodnutí (decision making), obžalování (accusing) or nouns derived from verbs by non-productive means or by the zero suffix (e.g. rada (advise), slib (promise)) b) Nouns derived from the predicative adjective (e.g. on je schopen udělat (he is able to do sth) → jeho schopnost napsat knihu (his ability to write a book), on je povinen udělat (he is obliged / required to do sth) → jeho povinnost vydat majetek (his duty / obligation to release possession) c) Deverbative adjectives, it seems that only predicative deverbative adjectives can occur with control (e.g. dívka je schopna studovat (the girl is able to study)→ dívka schopná studovat (a girl able to study) , osoba je povinna platit daně (the person is obliged to pay taxes) → osoba povinná platit daně (a person obliged to pay taxes) d) Nouns which were a part of an analytical predicate (e.g. Petr má šanci vyhrát (Peter has a chance to win) → Petrova šance vyhrát (Peter’s chance to win), Petr má právo odvolat se (Peter has a right to appeal) → Petrovo právo odvolat se (Peter’s right to appeal). Some of the nouns derived from this type of analytical predicates, especially from those with the meaning of intent, do not express grammatical coreference, e.g. nápad vydat knihu (an idea to publish a book) (cf. also Panevová, 1996). 2.3.2. Types of nominalized constructions of control Considering the possibility of a nominalization of both the governing as well as the dependent verb, we deal with four types of constructions of control: 1. The infinitive clause depends on a finite verb (e.g. radil nechodit (he advised not to go), slíbil napsat (he promised to write); 2. The infinitive clause depends on a nominalization of a finite verb (e.g. rada nechodit (an advice not to come), slib napsat (a promise to write)); 3. The nominalization of the embedded verb depends on a finite verb (e.g. obvinil někoho z vyvolání problému (he charged a person with a raising of a problem), vyžadoval odpuštění daní (he claimed exemption of the taxes)); 4. The nominalization of the embedded verb depends on a nominalization of a finite verb (e.g. obvinění z vyvolání problému (an accusation of a raising of a problem), snaha o podplacení (an attempt for corruption)). However, it is necessary to say that not all groups of verbs of control mentioned in section 2.2.1. allow for its nominalization or for a nominalization of its controlled infinitive clause: - Verbs of control from the groups (1)(i), (ii) and (iii) may occur in all four types of constructions of control (e.g. verbs slíbit (promise), vyžadovat (require, claim), snažit se (try): slíbit napsat (to promise to write), slib napsat (a promise to write), slíbit napsání (to promise writing), slib napsání (a promise of writing) - Verbs of control from the group (2) allow only for the nominalization of the dependent verb (Je snadné číst tu knihu (It is easy to read the book) - Četba této knihy je snadná (The reading of this book is easy) - Verbs from the group (3) do not allow nominalization in constructions of control. Verbs mentioned in section 2.2.3. may occur only in construction types (3) and (4) (e.g. verbs podezírat (suspect), obvinit (accuse): podezírat z krádeže (to suspect of theft), podezření z krádeže (a suspicion of theft), but *podezírat krást (to suspect to steal), *podezření krást (a suspicion to steal)).  Panevová, Řezníčková, and Urešová  179  Let us present here some illustrative examples of nominalized constructions of control from our annotated corpus: (3) Ctihodný Malu-malu, biskup Surabayský: Obdivuju schopnost Vašich lidí odpouštět. 'The venerable Malu-malu, the bishop of Surabaya: I admire the ability of your people to forgive.'  &Emp; PRED  &Comma; APPS  obdivovat PROC PAT admire  Malu malu biskup já schopnost  ACT  ACT ACT PAT  Malu malu bishop I ability  ctihodný surabayský člověk  RSTR RSTR  ACT  
The XTAG Project (Joshi, 2001) is an ongoing project at the University of Pennsylvania since about 1988, aiming at the development of natural language resources based on Tree Adjoining Grammars (TAGs) (Joshi and Schabes, 1997). Perhaps the most successful experience in it has been the construction of a wide-coverage Lexicalized TAG for English (LTAG) (Doran et al., 2000; XTAG Research Group, 2001), based on ideas initially developed in (Krock and Joshi, 1985). As the grammar grew larger, the process of consistent grammar development and maintenance became harder (Vijay-Shanker and Schabes, 1992). Driven by locality principles, each elementary tree for a given lexical head is expected to contain its projection, and slots for its arguments (e.g., (Frank, 2001)). As consequence, the number of required elementary trees grows huge for a grammar with reasonable coverage of syntactic phenomena. Under the XTAG project, for engineering reasons, the grammar has been split up in (roughly) two main components1: a set tree templates lexicalized by a syntactic category, and a lexicon with each word selecting its appropriate tree templates. Although various syntactic categories have multiple syntactic frames available (e.g., prepositions may have different kinds of arguments, nouns and adjectives may have arguments or not, etc.), it is the verbs that exhibit the most wild variety of domains of locality: from the 1226 template trees in the XTAG grammar, 1008 are for verbs, more than 82%. That happens because the grammar tries to capture in elementary trees the locality for each of the diverse syntactic structures related transformationally to each other (the effect of long distance movement is captured by adjunction of the intervening material). Examples of required tree templates are: declarative transitive; ditransitive passive with wh-subject moved; and intransitive with PP object with the PP-object relativized. As early noticed by (Vijay-Shanker and Schabes, 1992) the information regarding syntactic structure and feature equations in (feature-based) LTAGs is repeated across templates trees in a quite regular way, that perhaps could be more concisely captured than by just having a plain set of elementary trees. Besides the obvious linguistic relevance, as a pure engineering issue, the success of such enterprise would result in enormous beneﬁts for grammar development and maintenance. Several approaches have been proposed in the literature describing compact representations methods for LTAGs, of which, perhaps the best known are (Vijay-Shanker and Schabes, 1992), (Candito, 1996; Candito, 1998), (Evans, Gazdar and Weir, 1995; Evans, Gazdar and Weir, 2000), (Xia et al., 1998; Xia, 2001), and (Becker, 1993; Becker, 1994; Becker, 2000). We describe in this paper how we combined Becker’s metarules with a hierarchy of rule application to generate the verb tree templates of the XTAG English grammar, from a small initial set of trees. 2. Metarules A subsystem for interpreting metarules was initially introduced into the XTAG development system by Tilman Becker, from 1993 to 1995, based on his ideas in (Becker, 1993; Becker, 1994) with subsequent additions over the years, reaching a stable form as documented (by this author) in (XTAG Research Group, 1998). Although it has been topically used since then, as an auxiliary tool to reduce the effort spent in grammar development (e.g., to generate the trees for an updated analysis of relative clauses, using the former trees as starting point), this paper describes the ﬁrst effort to effectively use them to generate the full XTAG grammar verb trees.2 We are indebted to all members of the XTAG Group that participated of the valuable discussions during the realization of this work, and in particular to Alexandra Kinyon for her comments on this paper. 1. For a more accurate description of the architecture, see (XTAG Research Group, 2001) or (Doran et al., 2000). 2. This effort is already mentioned in (Doran et al., 2000, p. 388). There has been some confusion on the issue, perhaps driven by a somewhat ambiguous statement in (Becker, 2000, p. 331): “In this paper, we present the various patterns which are used in the implementation of metarules which we added to the XTAG system (Doran et al. 2000)”. The work of Becker conceived and developed the idea of metarules for TAGs and also created the original implementation of the metarule interpreter as part of the XTAG software. However, he only created the necessary example patterns to support the concepts of metarules while the work described here is the ﬁrst to actually evaluate metarules in-the-large as part of the XTAG project. ¡ c 2002 Carlos A. Prolo. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 181–186. Universita´ di Venezia.  182  Proceedings of TAG+6  We present in this section a brief introduction to Becker’s metarules.3 Consider the trees in Figure 1 anchored by verbs that take as arguments an NP and a PP (e.g., put). The one to the left corresponds to its declarative structure; the other to the wh-subject extracted form. Despite their complexity, they share most of their structure: the only differences being the wh-site (higher NP in the right tree) and the trace at subject position. That observation would not be very useful if the differential description we have made was idiosyncratic to this pair, which is not the case. Clearly, many other pairs all over the grammar will share the same differential description.  Sr  NP0↓  VP  V◊  NP1↓  VPe  ¢  Ve  PP  NA  εv  P↓  NP2↓  (a) declarative  Sq  NP0↓  Sr  NP  VP  NA  ε  V◊  NP1↓  VPe  Ve  PP  NA  εv  P↓  NP2↓  (b) subject extracted  Figure 1: Some related trees for the verb £¥¤§¦  Figure 2.a shows a metarule for wh-subject extraction, that captures the similarities mentioned above. It describes how to automatically generate the tree in Figure 1.b, given as input the tree in Figure 1.a. Here is how it works. First the input tree has to match the left-hand side of the metarule, lhs in Figure 2.a, starting from their roots. In the example the lhs tree, requires the candidate tree to have its root labeled ¨© . Then, its leftmost child has to be an  , as indicated by the node  in lhs:  indicates it is the variable ! ;  indicates we need an  , regardless of the subscript. Next, the lhs tree requires the rest of the tree to match variable #" . That is trivial, because such variables with just an identiﬁcation number are “wild cards” that match any range of subtrees. The matches of each variable in lhs, for the application to the input tree in Figure 1.a, are shown in Figure 2.b.   ¢  PI  021436'587¥')$&9@(% ( 0BA  ¢  lhs  0D1436'587¥' 9  '  $( C ( ( $ 3658' EG' F  (%  (  0BA  H rhs  #" ¢  VP  V◊  NP1↓  VPe  Ve  PP  NA  a) Metarule for wh-movement of subject  εv  P↓  NP2↓  b) Variables Matching for the tree in Figure 1.a  Figure 2: A metarule for wh-movement of subject applied to the tree of Figure 1.a Had the matching process failed the metarule would not apply and no new tree would have been generated. Since in the example above the matching succeeded, the processor move to the ﬁnal step, which is to generate the new tree. We look at the right-hand side of the metarule rhs and just replace the instances of the variables there 3. For a more comprehensive view, including linguistic motivations and the sort of patterns it allows, we refer the reader to (Becker, 2000). The actual set of metarules we used can be obtained upon request to this author.  Prolo  183  SUBCATEGORIZATION GROUP Intransitive Transitive Adjectival complement Ditransitive Prepositional complement Verb particle constructions Light verb constructions Sentential Complement (full verb) Sentential Subject (full verb) Idioms (full verb) Small Clauses/Predicative Equational ”be” Ergative Resultatives It Clefts Total  No. OF FAMS. 
1. Introduction This paper presents a sketch of a formal proof of strong equivalence,1 where both grammars generate equival£ ent parse results, between any LTAG (Lexicalized Tree Adjoining Grammar: Schabes, Abeille and Joshi (1988)) £ and an HPSG (Head-Driven Phrase Structure Grammar: Pollard and Sag (1994))-style grammar converted from by a grammar conversion (Yoshinaga and Miyao, 2001). Our proof theoretically justiﬁes some applications of the grammar conversion that exploit the nature of strong equivalence (Yoshinaga et al., 2001b; Yoshinaga et al., 2001a), applications which contribute much to the developments of the two formalisms. In the past decades, LTAG and HPSG have received considerable attention as approaches to the formalization of natural languages in the ﬁeld of computational linguistics. Discussion of the correspondences between the two formalisms has accompanied their development; that is, their linguistic relationships and differences have been investigated (Abeille´, 1993; Kasper, 1998), as has conversion between two grammars in the two formalisms (Kasper et al., 1995; Tateisi et al., 1998; Becker and Lopez, 2000). These ongoing efforts have contributed greatly to the development of the two formalisms. Following this direction, in our earlier work (Yoshinaga and Miyao, 2001), we provided a method for converting grammars from LTAG to HPSG-style, which is the notion that we deﬁned according to the computational device that underlies HPSG. We used the grammar conversion to obtain an HPSG-style grammar from LTAG (The XTAG Research Group, 2001), and then empirically showed strong equivalence between the LTAG and the obtained HPSG-style grammar for the sentences in the ATIS corpus (Marcus, Santorini and Marcinkiewicz, 1994). We exploited the nature of strong equivalence between the LTAG and the HPSG-style grammars to provide some applications such as sharing of existing resources between the two grammar formalisms (Yoshinaga et al., 2001b), a comparison of performance between parsers based on the two different formalisms (Yoshinaga et al., 2001a), and linguistic correspondence between the HPSG-style grammar and HPSG. As the most important result for the LTAG community, through the experiments of parsing within the above sentences, we showed that the empirical time complexity of an LTAG parser (Sarkar, 2000) is higher than that of an HPSG parser (Torisawa et al., 2000). This result is contrary to the general expectations from the viewpoint of the theoretical bound of worst time complexity, which is worth exploring further. However, the lack of the formal proof of strong equivalence restricts scope of the applications of our grammar conversion to grammars which are empirically attested the strong equivalence, and this prevents the applications from£ maximizing their true potential. In this paper£ we give a formal proof of strong equivalence between any LTAG and an HPSG-style grammar converted from by our grammar conversion in order to remove such restrictions on the applications. 2. Grammar conversion We start by stating our deﬁnition of an HPSG-style grammar, and then brieﬂy describe our algorithm for converting grammars from LTAG to HPSG-style. We hope that the reader will refer to the cited literature (Yoshinaga and Miyao, 2001) for a more detailed description. We deﬁned an HPSG-style grammar, the form of the output of our conversion, according to the computational architecture which underlies HPSG (Pollard and Sag, 1994). An HPSG-style grammar consists of lexical entries and ID grammar rules, each of which is described with typed feature structures (Carpenter, 1992). A lexical entry for a word must express the characteristics of the word, such as its subcategorization frame and grammatical category. An ID grammar rule must represent the constraints on the conﬁguration of immediate constituency, and ¤ This research was funded by JSPS Research Fellowships for Young Scientists. 1. Chomsky (1963) ﬁrst introduced the notion of strong equivalence between grammars, where both grammars generate the same set of structural descriptions (e.g., parse trees). Kornai and Pullum (1990) and Miller (1999) used the notion of isomorphism between sets of structural descriptions to provide the notion of strong equivalence across grammar formalisms, which we have adopted in our research. ¥ c 2002 Naoki Yoshinaga, Yusuke Miyao, and Jun’ichi Tsujii. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 187–192. Universita´ di Venezia.  188  Proceedings of TAG+6  γ X  X X  γu Y Y X γv  γu X  T’  γ u1 X  γ u2 X  X γ1  X  γ2  , …  , …  Figure 1: Sketch for the division transformation (left) and the substitution transformation (right)  anchor * foot node substitution node  S NP VP V S*  think Sym: V Sym : VP Sym : S think : Arg: Leaf : S , Leaf : NP Dir : right Dir : right Foot?: + Foot?:  Sym : 1 Arg : 2  Sym : 3 Arg : substitution node  Arg :  Sym : 1  Leaf : 3  2  Dir : left  Foot? :  trunk node  Left substitution rule  Sym : 1 Arg : 2 4  Sym : 3 Arg : 4 foot node  Arg :  Sym : 1 Leaf : 3 2 Dir : left Foot? : +  trunk node  Left adjunction rule  Figure 2: A conversion from a canonical elementary tree to an HPSG lexical entry (left) and grammar rules: the substitution rule (center) and adjunction rule (right)  not be a construction-speciﬁc ru£ le speciﬁed by lexical characteristics. The formal deﬁnition of an HPSG-style grammar converted from LTAG is given later in Section 3.3.  Our conversion algorithm consists of two kinds of conversion; i) a conversion from LTAG into canonical  LTAG, LTAG which consists only of canonical elementary trees, and ii) a conversion from the canonical LTAG into  an HPSG-style grammar. Canonical elementary trees are tree structures satisfy the following conditions; Condition  1: A tree must have only one anchor, and Condition 2: Trunk nodes are nodes on a trunk which is a path from  Every branching structure in a an anchor to the root node. We  tree call  ma suusbt tcroenetoafindterputnhk¢n¡¤o£¦de¥¨s§ .  that includes no anchor a non-anchored subtree. Elementary trees which violate Condition 1 are converted into  canonical ones by dividing them into single-anchored parts (the division transformation: the left-hand side of  Figure 1). Elementary trees which violate Condition 2 are initially divided into multiple subtrees by the division  transformation, each of which has at most one anchor, and then converted into canonical ones by substituting the  deepest nodes in the non-anchored subtrees with every initial tree (the substitution transformation: the right-hand  side of Figure 1). We give the formal deﬁnition of these transformations later in Section 3.2. Conversion of a  canonical elementary tree is straightforward; that is, we traverse the trunk of a canonical elementary tree from  its anchor to root, regard the leaf nodes as the anchor’s arguments, and store the symbols of the leaf nodes and  the trunk nodes as Leaf and Sym features respectively in a stack (Arg feature in the left-hand side of Figure 2),  where Dir and Foot? features are the direction of the leaf node relative to the trunk and the type of the leaf node,  respectively. A set of pre-determined rules manipulates the stack to emulate substitution and adjunction; namely,  substitution rules (the center of Figure 2) and adjunction rules (the right-hand side of Figure 2).  3. A formal proof of strong equivalence  The whole proof consists of two pieces, each of which respectively proves that strong equivalence is guaranteed before and after the two conversions mentioned in the previous section.  3.1. Deﬁnitions  We ﬁrst deﬁne LTAG, according to the deﬁnition of TAG given by (Vijay-Shanker, 1987). We then deﬁne a  derivation tree, which is a structural description of LTAG, and introduce the notion of strong equivalence.  We hereafter denote a tree as a set of pairs ©¤ where  "! , which is a free monoid of the set of natural  numbers, and #$ , Figure 2 is denoted as  %  w¡'&h)ic( h§  UT¦V is satisﬁed if and only 
Generalizing ideas presented for 2-stack automata in (Éric Villemonte de la Clergerie, 2001), we introduce Thread Automata [TA], a new automata formalism that may be used to describe a wide range of parsing strategies (in particular top-down preﬁx-valid [pv] strategies) for many Mildly-Context Sensitive [MCS] grammatical formalisms (Weir, 1988), including CFG, TAG (Joshi, 1987), Multi-Component TAG (Weir, 1988), and Linear Context-Free Rewriting Systems [LCFRS] (Weir, 1992). As suggested by their name, the underlying idea of TA is that several lines of computation (threads) are followed during parsing, only one being active at any time. Threads may start sub-threads, may terminate, and may be suspended to give control to their parent or one of their direct descendants. Intuitively, a thread may be used to recognize a constituent while new sub-threads are started to recognize its sub-constituents. Because a thread may be suspended and resumed several times, we can recognize discontinuous constituents with holes such as auxiliary trees in TAG. More generally, TA may handle complex interleaving of discontinuous constituents as shown by Fig. 4(a) for the constituents B and C. TA may also be used to parse a sub-class of Range Concatenation Grammars [RCG] (Boullier, 2000b), which covers LCFRS. Though TA exhibit strong expressive power, they still ensure good operational and complexity properties. Indeed, we propose a simple Dynamic Programming [DP] interpretation for TA that ensures tabular parsing in polynomial worst-case complexity for space and time w.r.t. the length of the input string. If we focus in this paper on top-down pv parsing strategies, it is not because we believe them to be the most important ones, but rather because we think it is important to cover the full spectrum of parsing strategies. Moreover, a tabular parser which handles pv parsing strategies may usually be easily adapted to handle other kinds of strategies, the converse being not true. For instance, there already exists a systematic non pv parsing algorithm for RCG, but we are unaware of any systematic pv parsing algorithm for them. 2. Thread Automata Formally, a Thread Automaton is a tuple (N , Σ, S, F, κ, K, δ, U, Θ) where • Σ (resp. N ) denotes a ﬁnite set of terminal (resp. non-terminal) symbols, with two distinguished initial and ﬁnal non-terminals S and F ; • Θ is a ﬁnite set of transitions; • κ denotes a partial function from N to some other ﬁnite set K and is used to capture the amount of information consulted in a thread to trigger the application of some kinds of transitions; 1 • U is a ﬁnite set of labels used to identify threads. It is also used by the partial function δ to drive computations by specifying which threads (subthreads or parent) may potentially be created or resumed at some point, δ being deﬁned from N to 2∆ where ∆ = {⊥} ∪ U ∪ {us|u ∈ U} and ⊥ ∈ U. The two functions κ and δ being often consulted in conjunction, we note κδ(A) = (κ(A), δ(A)) and (a, d) ∈ κδ(A) if a = κ(A) and d ∈ δ(A). A thread is a pair p:A where p = u1 . . . uk ∈ U is a (possibly empty) path, and A some non-terminal symbol from N . The empty path is denoted by . A thread store S is a ﬁnite set of threads (denoting a partial function from U to N ) such that its associated path set ker(S) = {p|p:A ∈ S} is closed by preﬁx (ie., pu ∈ ker(S) ⇒ p ∈ ker(S)). We will often confuse a thread with its path. A TA conﬁguration is a tuple l, p, S where l denotes the current position in the input string, p the path of the active thread, and S a thread store with p:A ∈ S for some non-terminal A. The initial conﬁguration is 1. This function, while not essential, is useful to reduce complexity w.r.t. grammar sizes, as illustrated for TAGs (Section 3). c 2002 Éric Villemonte de la Clergerie. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 193–200. Universitá di Venezia.  194  Proceedings of TAG+6  cinit = 0, , { :S} and the ﬁnal one cﬁnal = n, u, { :S, u:F } where u ∈ δ(S) ∩ U and n denotes the length of the input string. A derivation step c| τ c is performed using a transition τ ∈ Θ of the following kind, where bracketed (resp. non-bracketed) parts denote contents of non active (resp. active) threads :  SWAP B−α→C : Changes the content of the active thread, possibly scanning a terminal.  l, p, S ∪ p:B | τ l + |α|, p, S ∪ p:C  al = α if α =  PUSH b −→ [b] C : Creates a new subthread.  l, p, S ∪ p:B | τ l, pu, S ∪ p:B ∪ pu:C  (b, u) ∈ κδ(B) ∧ pu ∈ ker(S)  POP [B] C −→ D : Terminates thread pu (if there is no existing subthread).  l, pu, S ∪ p:B ∪ pu:C | τ l, p, S ∪ p:C  pu ∈ ker(S)  SPUSH b [C] −→ [b] D : Resumes the subthread pu (if already created)  l, p, S ∪ p:B ∪ pu:C | τ l, pu, S ∪ p:B ∪ pu:D  (b, us) ∈ κδ(B)  SPOP [B] c −→ D [c] : Resumes the parent thread p of pu  l, pu, S ∪ p:B ∪ pu:C | τ l, p, S ∪ p:D ∪ pu:C  (c, ⊥) ∈ κδ(C)  Without restrictions, a thread may be suspended inﬁnitely often. However, we will only consider h-TA, subclasses of TA where a thread may be suspended at most h times to return to its parent, which is a sufﬁcient condition to ensure the termination of our tabular parsing algorithm (Section 6). Another key parameter is d ≤ |U|, the maximal number of subthreads of a thread that may be simultaneously alive. These two parameters h and d are sufﬁcient to characterize the worst-case complexities of our tabular algorithm, namely O(n2(1+h+dh)) for space and O(n2(1+h+dh)+1) for time. For instance, TA with h = 0 and d = 1 are equivalent to Push-Down Automata and may be used to handle CFG, giving us optimal worst-case complexity O(n2) for space and O(n3) for time. A ﬁner complexity analysis (still to be conﬁrmed) suggests that, modulo a generally satisﬁed condition on SWAP transitions2, the worst-case complexities are actually O(n2+h+dh+x) for space and O(n3+h+dh+x) for time where x = min(h + dh, (l − d)(h + 1)) and l is the maximal number of subthreads created by a thread. When assuming l = d, we get x = 0 and complexities O(n2+h+dh) for space and O(n3+h+dh) for time. Fig. 1 lists the transitions of a thread automaton A that may be used to recognize the COUNT-3 language anbncn with a derivation of a3b3c3 illustrated by Fig. 2. The characteristics of A are h = 2 and d = 1.  (1) K−a→s1 (2) [s1] void −→ s2 [void] (3) void [s2] −→ [void] s3 (4) s3−→b s4 (5) [s4] void −→ s5 [void] (6) void [s5] −→ [void] s6 (7) s6−→c s7 (8) [s7] ret −→ ret  κδ(s1) = (K, {1}) κδ(s2) = (void, {⊥}) κδ(s1) = (void, {1s}) κδ(s5) = (void, {⊥}) κδ(s7) = (void, {1s})  (9) K −→ [K] K (10) S−→r0 (11) [r0] void −→ r1 [void] (12) [r1] void −→ r2 [void] (13) [r2] ret −→ ret (14) K −→ [K] t0 (15) void [t0] −→ [void] t1 (16) void [t1] −→ [void] ret  κδ(r0) = (K, {1}) κδ(r1) = (void, {1s}) κδ(r2) = (void, {1s}) κδ(t0) = (void, {⊥}) κδ(t1) = (void, {⊥})  Figure 1: TA transitions for COUNT-3 language anbncn  3. Parsing TAG TAG parsing strategies may be encoded with TA in a straightforward way. The idea is to associate a thread to each elementary tree traversal. For instance, in Fig. 3, a thread p is started at R to traverse some elementary tree 2. The condition is that no sequence of SWAP transitions can loop from a conﬁguration l, p, S ∪ p:A to a conﬁguration r, p, S ∪ p:A . That means for instance that we are not scanning regular expressions with Kleene stars using SWAP transitions.  1.1.1.1 1.1.1 1.1 1  Éric Villemonte de la Clergerie  195  a 14 2  b 15 5  c 16 8  a 91  2  b 34  5  c 67  8  a 91  2 b 34  5 c 67  8  91  11 3 4  12 6 7  13  10 Figure 2: A derivation for a3b3c3 starting from S  α and a subthread pu with u = depth(ν) is started at node ν to handle an adjunction for non-terminal N (Adj Call). This subthread T selects and traverses some auxiliary tree β (Adj Select); is ⊥-suspended when reaching the foot node of β to traverse the subtree α|ν rooted at ν (Foot Suspend); is resumed after the traversal of α|ν (Foot Resume); and is ended to return to its parent thread at the end of the traversal of β (Adj Return). 3  R •ν ν ν• f •ν ν• (a) Traversals  κδ( •ν) = (N a, {u})  Adj  κδ( ν•) = (adj, {us})  u = depth(ν)  Foot  κδ( •f) = (foot, {⊥})  (Adj Call) (Adj Select) (Foot Suspend) (Foot Resume) (Adj Publish) (Adj Return)  N a −→ [N a] N a N a−→ •rβ [ •ν] foot −→ •ν [foot] adj [ •f] −→ [adj] f• rβ• −→ret [ ν•] ret −→ ν•  (b) Transitions  Figure 3: Traversing trees using TA transitions When traversing a tree τ with a thread p, the maximal number d of simultaneously alive sub-threads of p is bounded by its depth, at most one subthread being started and still alive for each node along a path of τ . A thread may only be suspended once (h = 1). Hence we get complexities O(n4+2d) for space and O(n5+2d) for time. These complexities are not optimal for TAG but correspond to those mentioned in (Éric Villemonte de la Clergerie, 2001) for a very similar tabular parsing algorithm, which has proved to be efﬁcient in practice for linguistic grammars. The best known worst-case complexities for TAG are O(n4) or O(n5) for space and O(n6) for time, and correspond to tabular algorithms where the traversal of the subtree α|ν may be shared between the different traversals of α (relying on the fact that the traversal of α|ν may be done independently of the adjunctions started on the path from the root of α down to ν). It is worth noting that we have very good reasons to believe that we can also achieve the O(n6) time complexity using TA and our tabular parsing algorithm (see discussion in Section 5). 4. Parsing Multi-Component TAG  Multi-Component [MC] TAG allows the adjoining or substitution of a ﬁnite set of elementary trees on nodes of a single elementary tree (tree-local MC-TAG) or on nodes of elementary trees of another set (set-local MC- 3. Note that without a triggering function κ, the (Foot Suspend) transition would be of the form [ •ν] •f −→ ν• [ •f], explicitly referring to nodes of two distinct elementary trees, and leading to complexities in O(|G|2) instead of O(|G|) where |G| denotes the size of the grammar.  196  Proceedings of TAG+6  TAG). We provide some intuitions how TA may be used to encode preﬁx-valid parsing strategies for both kinds of MC-TAG, restricting ourselves to sets of auxiliary trees.4  Tree-local MC-TAG. The idea is to assign a thread p to the traversal (in any order) of a tree set Σ and subthreads of p to the traversal of each tree in Σ. More formally, for the traversal in any order of a tree set Σ = {β1, . . . , βm}, we consider extended dotted points deﬁned by  Σ:ρσ ρ ∈ { •i, •i, i• |i = 1 . . . m} ∧ σ ∈ { i• |i = 1 . . . m}  where ρ (resp. σ) is the list of trees in Σ which have been started but not yet completed (resp. completed). We note ind(ρσ) the set of indices in {1, . . . , m} occurring in ρσ. The rightmost index of ρ states which tree of Σ we are currently working on. The non-terminal set N of the automaton includes these extended dotted points Σ:ρσ, as well as the dotted nodes for each node ν and the symbols N a and N s for each non-terminal N of the grammar. We consider the thread label set U = {1, . . . , m} ∪ {addr(τ, ν)|τ, ν node of τ } where addr(τ, ν) denotes the address of ν in τ . We now associate to Σ the following (non exhaustive) set of transitions, where ri denotes the root node of βi and Ni the non-terminal label of ri:  (Call set) (Start set with tree βi) (Resume set with tree βi) (Start tree βi) (Suspend βi at foot) (Suspend set at foot of βi) (Resume set after foot of βi) (Resume βi after foot) (End tree βi) (Suspend set after tree βi) (End set) (Return from set)  Nia −→ [Nia] Nia Nia−→Σ: •i Nia [Σ:ρσ] −→ [Nia] Σ:ρ •i σ ri −→ [ri] •ri [Σ:ρ •i σ] foot −→ Σ:ρ •i σ [foot] [ •ν] foot −→ •ν [foot] adj [Σ:ρ •i σ] −→ [adj] Σ:ρ i• σ adj [ •f] −→ [adj] f• [Σ:ρ i• σ] ret −→ Σ:ρ i• σ [ ν•] ret −→ ν• [ret] Σ:σ−→ret [ ν•] ret −→ ν•  i ∈ ind(ρσ) κδ(Σ:ρ •i σ) = (ri, {i}) κδ(Σ:ρ •i σ) = (foot, {⊥}) κδ(Σ:ρ i• σ) = (adj, {i}) κδ(Σ:ρ i• σ) = (ret, {⊥}) ∧ ρ = ind(σ) = {1, . . . , m}  For a node ν of some elementary tree τ with non-terminal label N , we set  Adj  κδ( •ν) = (N a, {addr(τ, ν)} ∪ {addr(τ, µ)|µ = ν}s) κδ( ν•) = (adj, {addr(τ, µ)}s)  Foot  κδ( •f) = (foot, {⊥})  In terms of complexity, the number h of ⊥-suspensions is bounded by 2m where m denotes the maximal  number  of  trees  per  set  while  d  is  bounded by  
1. Introduction One of the biggest concerns that has been raised over the feasibility of using large-scale LTAGs in NLP is the amount of redundancy within a grammar’s elementary tree set. This has led to various proposals on how best to represent grammars in a way that makes them compact and easily maintained (Vijay-Shanker and Schabes, 1992; Becker, 1993; Becker, 1994; Evans, Gazdar and Weir, 1995; Candito, 1996). Unfortunately, while this work can help to make the storage of grammars more efﬁcient, it does nothing to prevent the problem reappearing when the grammar is processed by a parser and the complete set of trees is reproduced. In this paper we are concerned with an approach that addresses this problem of computational redundancy in the trees, and evaluate its effectiveness. 2. LTAG parsing LTAG parsing involves (at least) the following two steps. Each word in the input sentence is associated with that set of (elementary) trees (also called supertags) from the grammar that it can anchor. In large-scale grammars such as the XTAG grammar (XTAG-Group, 1999) and the LEXSYS grammar (Carroll et al., 1998b), due to lexical ambiguity, there are usually a great many trees that can anchor each word. Once all of these elementary trees or supertags have been found, the parser must explore ways in which they can be composed—using substitution and adjunction—to produce complete parses of the input string. In various experiments using a large, automatically produced LTAG, Sarkar, Xia and Joshi (2000) measured the time to derive a set of shared derivation forests representing all derivations for each sentence. They used a grammar with 6,789 tree templates and 2,250 sentences of length 21 words or less, and concluded that the amount of syntactic lexical ambiguity and the number of clauses in a sentence are more signiﬁcant factors in determining the time taken to compute a parse forest than sentence length. To date, the most popular way of addressing the computational problem of lexical ambiguity in LTAG parsing involves supertag ﬁltering, where another step is included in the parsing processes (between the two phases described above) which involves ﬁltering out some of the possible supertags for words in the sentence (Joshi and Bangalore, 1994; Bangalore, 1997a; Bangalore, 1997b; Chen, Bangalore and Vijay-Shanker, 1999). This can dramatically reduce the time it takes to ﬁnd all ways in which supertags can be combined together into complete parses. Sarkar et al. demonstrate the potential beneﬁt: parsing their 2,250 sentences with all supertags took 548,000 seconds, but this reduced to 21,000 seconds when the maximum number of supertags per word was limited to 60, and to a mere 31.2 seconds when lexical ambiguity was completely eliminated. However, a drawback of this approach is that, since supertag ﬁltering cannot be 100% accurate, a proportion of desirable supertags are ﬁltered out, resulting in some parses being lost. For example in Sarkar et al.’s experiment, a limit of 60 supertags per word resulted in over 40% of the sentences receiving no parse at all. 3. Elementary computation sharing There is an alternative approach to the problem of lexical ambiguity in parsing that removes some of the computational redundancy that results from lexical ambiguity1. Given some parsing algorithm, each elementary ¢ We are extremely grateful to Fei Xia for providing us with the grammar used in these experiments, and to Fei Xia and Anoop Sarkar for the help and advice they have given. 1. Note that the approach described in this section could be combined with supertag ﬁltering. £ c 2002 Olga Shaumyan, John Carroll and David Weir. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 201–205. Universita´ di Venezia.  202  Proceedings of TAG+6  tree can be viewed as invoking some fragment of computation (an elementary computation). Evans and Weir (1998) showed that elementary computations corresponding to bottom-up parsing can be expressed as ﬁnite state automata (FSA). All elementary computations for the supertags associated with a word can be combined into a single FSA. By minimizing this automaton (using standard minimization algorithms) sharing of elementary computation is achieved. The hope is that this will lead to signiﬁcant reductions in parsing time. To date, this proposal has only received limited evaluation. Carroll et al. (1998a) demonstrated that for a large hand-crafted grammar the number of states was signiﬁcantly reduced by merging and minimizing the FSA associated with a word. For example, the numbers of states in the automaton for the word come (associated with 133 supertags) was reduced from 898 to 50, for break from 1240 to 68, and give from 2494 to 83. This paper improves on this evaluation in two ways: ﬁrstly, the grammar used is automatically acquired, so we are not open to the charge that it was designed to make this technique work particularly well; secondly, we measure parse time, not just numbers of states for individual words. Even when the number of states is signiﬁcantly reduced it is not clear that parse time (as opposed to recognition time) will drop. This is because in order that parse trees be recoverable from the parse table, a considerable amount of book-keeping is required when the table is being completed. This increases both space and time requirements. 4. Experimental evaluation We used a grammar that was automatically induced by Fei Xia (1999) from sections 00–24 of the Wall Street Journal Penn Treebank II corpus (Marcus, Santorini and Marcinkiewicz, 1993). This is very similar to the grammar used by Sarkar, Xia and Joshi (2000) and Sarkar (2000), though slightly larger, containing around around 7,500 elementary trees. We implemented the algorithm described by Evans and Weir (1997) and Evans and Weir (1998), the details of which are not repeated here. Prior to parsing, the grammar is precompiled as follows. For each word, the set of trees that it can anchor is determined. This results in a total of 11,035 distinct tree sets. For each of these tree sets we ﬁrst build what we refer to as an unmerged FSA. This automaton contains a separate progression of transitions for each of the trees in the set; using these automata for parsing gives a conventional LTAG parsing algorithm which we used to give a baseline for our evaluation. To evaluate the approach of Evans and Weir we implemented a parser that used minimized versions of the automaton with sharing of common elementary computation fragments. There are 80,538 non-minimized automata (involving 488,421 states). Thus there is a total of 80,538 occurrences of one of the grammar’s elementary trees in the 11,035 tree sets. When these nonminimized automata are minimized we have one automaton for each of the 11,035 tree sets; these automata contain a total of 153,022 states. Thus, minimization gives an overall compaction of a factor of 3.19. In order to determine the computational beneﬁt of elementary computation sharing we ran both the merged and unmerged parsers on a set of 14,272 test sentences of lengths 1–45 words taken from sections 00–24 of the Penn Treebank corpus. The results are shown in Table 12. It is clear that numbers of items and CPU time are smaller for the merged parser, and that the savings increase with longer sentences. Given a tokenized sentence to be parsed, the time shown includes time to make a chart corresponding to the sentence length, look up deﬁnitions for each word and see¢d¡ £ the chart with them, and then ﬁll the chart. The results show that the parse time for the merged parser is around that of the umerged parser, and that this ratio is fairly consistent as the length of sentence increases. This is shown in Figure 1. 5. Discussion We have presented an empirical evaluation of the automaton-based LTAG parsing algorithm presented by Evans and Weir (1998). We used a grammar automatically generated from Penn Treebank trees with two parsers: one in which elementary trees were processed individually, and one in which overlapping elementary computations were shared. The results show that merging elementary computations results in a signiﬁcant, though not spectacular, reduction in parse time, despite the increased amount of book-keeping required to make recovery of parse trees possible. In future work we plan to determine exactly how much this book-keeping adds to parse time by implementing a version of the merged parser in which book-keeping is omitted. 2. We ran both parsers on one 750MHz processor of a unloaded Sun Blade 1000 workstation with 1.5GB memory.  Olga Shaumyan, John Carroll and David Weir  203  One rather signiﬁcant drawback of this evaluation is that the amount of lexical ambiguity in this grammar is far less than is found in large-scale wide-coverage grammars such as the XTAG grammar (XTAG-Group, 1999). Although the tree-bank includes examples of a wide variety of syntactic constructions, for any individual word, the number of syntactic contexts (corresponding to alternative supertag possibilities for that word) that actually occur in the Penn Tree Bank is generally far less than those that would be included in the lexical entry for that word in a wide-coverage grammar. This is particularly true for words with low frequency. In future work we plan to look into ways of obtaining more complete mapping from a lexical item to the set of supertags it can anchor.  300 200 Mean time 100  Unmerged Merged  ¡ ¡ ¡ ¦ ¦ ¦ ¦ ¦ ¦ ¥¦ ¥ ¥ ¥ ¥  ¡ ¡£¢ ¢  ¤¤  ¡  0  0  10  20  30  40  Sentence length  Figure 1: Comparison of Running Times  204  Proceedings of TAG+6  References Bangalore, Srinivas. 1997a. Complexity of Lexical Descriptions and its Relevance to Partial Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia. Bangalore, Srinivas. 1997b. Performance Evaluation of Supertagging for Partial Parsing. In Proceedings of the Fifth International Workshop on Parsing Technologies. Becker, Tilman. 1993. HyTAG: A new type of Tree Adjoining Grammar for hybrid syntactic representation of free word order languages. Ph.D. thesis, Universitat des Saarlandes. Becker, Tilman. 1994. Patterns in metarules. In Proceedings of the Third International Workshop on Tree Adjoining Grammars, pages 9–11. Candito, Marie-He´le`ne. 1996. A principle-based hierarchical representation of LTAGs. In Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, Denmark, August. Carroll, John, Nicolas Nicolov, Olga Shaumyan, Martine Smets and David Weir. 1998a. Grammar Compaction and Computation Sharing in Automaton-based Parsing. In Proceedings of the First Workshop on Tabulation in Parsing and Deduction, pages 16–25. Carroll, John, Nicolas Nicolov, Olga Shaumyan, Martine Smets and David Weir. 1998b. The LEXSYS Project. In Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks, pages 29–33. Chen, John, Srinivas Bangalore and K. Vijay-Shanker. 1999. New Models for Improving Supertag Disambiguation. In Proceedings of the Eighth Conference of the European Chapter of the Association for Computational Linguistics. Evans, Roger, Gerald Gazdar and David Weir. 1995. Encoding Lexicalized Tree Adjoining Grammars with a Nonmonotonic Inheritance Hierarchy. In Proceedings of the 33rd Meeting of the Association for Computational Linguistics, pages 77–84. Evans, Roger and David Weir. 1997. Automaton-based Parsing For Lexicalized Grammars. In Proceedings of the Fifth International Workshop on Parsing Technologies, pages 66–76. Evans, Roger and David Weir. 1998. A structure-sharing parser for lexicalized grammars. In Proceedings of the 36th Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 372–378. Joshi, Aravind and Srinivas Bangalore. 1994. Disambiguation of super parts of speech (or supertags): almost parsing. In Proceedings of the 15th International Conference on Computational Linguistics, pages 154–160. Marcus, Mitchell, Beatrice Santorini and Mary Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330. Sarkar, Anoop. 2000. Practical Experiments in Parsing using Tree Adjoining Grammars. In Proceedings of the Fifth International Workshop on Tree Adjoining Grammars and Related Frameworks. Sarkar, Anoop, Fei Xia and Aravind Joshi. 2000. Some Experiments on Indicators of Parsing Complexity for Lexicalized Grammars. In Efﬁciency in Large-Scale Parsing Systems. Workshop held at COLING 2000. Vijay-Shanker, K. and Yves Schabes. 1992. Structure Sharing in Lexicalized Tree-Adjoining Grammar. In Proceedings of the 14th International Conference on Computational Linguistics, pages 205–211. Xia, Fei. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. In Proceedings of the 5th Natural Language Processing Paciﬁc Rim Symposium(NLPRS-99). XTAG-Group, The. 1999. A Lexicalized Tree Adjoining Grammar for English. Technical Report http://www.cis.upenn.edu/˜xtag/ tech-report/tech-report.html, The Institute for Research in Cognitive Science, University of Pennsylvania.  Olga Shaumyan, John Carroll and David Weir  205  Sentence length 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  # items unmerged 3 19 104 327 681 1256 2298 3923 5844 9022 12674 18000 26110 34074 47564 62771 80515 99121 128028 163347 193701 277740 274474 354912 427291 532109 683355 731932 855873 873492 1089989 1291749 1838306 1917227 2364987 2487632 3381691 2864371 3290281 3755657 4993534 4843654 7071346 7655510 7772779  # items merged 
 and Chung-hye Han Simon Fraser University  1. Introduction  An earJy motivation for Synchronous TAG (S-TAG) (Shieber and Schabes, 1990) was machine translation (Abeille, Schabes and Joshi, 1990). Abeille et al note that traditionally difficult problems outlined by Dorr (1994}-for example, categorial, thematic, conflational, structural and lexical divergences-have been used to argue for the necessity of an explicit semantic representation. However, many ofthese divergences are not probIems for an S-TAG-based approach. Synchronous TAG translation models thus allow us to explore the question of the extent to which a semantic representation is actually necessary. S-TAG was redefined by Shieber ( 1994) for both theoretical and practical reasons, introducing the requirement that the derivation trees oftarget and source be isomorphic. Under this definition it has been noted (Shieber, 1994; Dras and Bleam, 2000) that there are mappings that cannot be described under S-TAG. This was the motivation for meta-level grammars (Dras, 1999), by which two TAG grammars can be paired while retaining their original properties, as under standard S-TAG, allowing for a description ofmappings that include unbounded non-isomorphisms (Dras and Bleam, 2000). This work on exploring how S-TAG (with and without meta-level grammars) can be used for MT has only been applied to languages that are closely related-English, French,"Italian and Spanish. In this paper we aim to take a much more widely differing pair of languages, English anq'Korean, to investigate the extent to which syntactic mappings are satisfactory. English and Korean have a wide range of differences: rigid SVO word order in English vs verb-final with free word order in Korean, the largely analytic structure ofEnglish vs the agglutinative structure ofKorean with its complex morphology, optional subject and object and the absence ofnumber and articles in Korean, and many . others. These all suggest that a meta-level grammar will be necessary as there are various many-to-one or n:ianyto-many mappings between derivation ｾ･＠ nodes (i.e., there will be few cases where a single elementary tree corresponds to another single elementary tree, which has been the case with closely related languages). Although there is an implemented Korean/English MT system that includes a TAG Korean parser as a source language analysis component(Han et al., 2000), this system as a whole is based on Meaning TextTheory (Mel'cuk, 1988), an enriched dependency fonnalism. Thus, it requires a conversion component that converts the TAG parser output to a dependency notation. As pointed out in Palmer et al. (2002), however, this conversion process resulted in a loss ofcrucial infonnation such as predicate-argument structure encoded in TAG elementary trees, which had negative consequences in the translation results. This then provides further motivation to explore the feasibility of applying a single TAG-based formalism to modeling and implementing a Korean/English MT system. As a first step towards exploring the extent to which an S-TAG style approach can successfully model these widely different languages, we have taken from a parallel English-Korean Treebank twenty examples ofdivergent constructions (see Appendix). Each halfhas roughly 50,000 word tokens and 5,000 sentences. While the annotation guidelines for the Korean half was developed in Han, Han and Ko (2001) for this corpus, the Englisb half follows the guidelines already developed for Penn English Treebank (Bieset al., 1995), as closely as possible. The example pairs represent structures including copula, predicative/attributive adjective, passive, causative, inter- · rogative, relative clause, complex verb, and modal construction, among others. We find that using a TAG-based meta-level gramrnar to model Korean/English correspondences for machine translation is quite feasible. 2. Analyses  In this section we discuss two example pairs of sentences, taken from the parallel Treebank, that illustrates several divergences, and how an S-TAG with meta-level grammar can handle them. The trees we use for the subgrammars for the sentences are extracted automatically from the Treebank using Lextract (Xia, Palmer and  © 2002 Mark Dras and Chung-hye Han. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and  Related Frameworks (TAG+6), pp. 206-215. Universitä di Venezia.  ··  Dias andHan  207  Joshi, 2000). 1 2.1. Korean complex NP vs. English modal  The sentence pair in (1) represents a modal construction. The key divergence is that the Korean uses a noun complement structure, while the English uses a modal adjective structure:  (1) ｾ＠ ""tｾ＠ ｾ＠  2 ｾ＠ ｾ＠ ｾ＠  ,fl.5!.t "'I °11 ...1 % ｾ＠ 0 1 ｾｴＤｊ＠ ｾ＠  4-  ｾ＠ ｾ＠ '-1 '''t.  tank-Plu-Top that ability-Acc open-terrain-Loc fully show-Adnominal possibility be-Past-Decl  Tanks are able to fully demonstrate their potential in open terrain.  A closer but less natural translation ofthe Korean is Thepossibility that tanksfully demonstrate theirpotential  in open terrain ｾｩｳｴ［＠  the noun representing possibi/ity is modified by an adnominal clause. The corresponding  English translation contains be able to followed by an in:finitival clause. The derivation trees are as in Figure 1,  and the Lextract elementary trees grouped according to the translation pairing in Figure 2.  ｾ＠ -e- '-1 et (sS..NPs_VJ@) 
1. lntroduction The Prague Dependency Treebank (PDT, as described, e.g., in (Hajic, 1998) or more recently in (Hajic, Pajas and Vidova Hladkä, 2001)) is a project of linguistic annotation of approx. 1.5 million word corpus of natuially occurring written Czech on three levels ("layers") of complexity and depth: morphological, analytical, and tectogrammatical. The aim of the project is to have a reference corpus annotated by using the accumulated findings of the Prague School as much as possible, wbile simultaneously showing (by experiments, mainly of statistical nature) that such a framework is not only theoretically interesting but possibly also ofpractical use. In this contribution we want to show that the deepest (tectogrammatical) layer of representation of sentence structure we use, which represents "linguistic meaning" as described in (Sgall, Hajieova and Panevova, 1986) and which also records certain aspects ofdiscourse structure, has certain properties tbat can be effectively used in machine translation1 for languages of quite different nature at the transfer stage. We believe that such representation not only minimizes the "distance" between languages at this Iayer, but also delegates individual language phenornena where they belong to - whether it is the analysis, transfer or generation processes, regardless ofmethods used for perfotming these steps.  2. The Prague Dependency Treebank  The Prague Dependency Treebank is a manually annotated corpus of Czech. The corpus size is approx. 1.5 million words (tokens). Three main groups ("layers") of annotation are used: • the morphological layer, where lernmas and tags are being annotated based on their context;  • the analytical layer, which roughly corresponds to the surface syntax oftbe sentence, • the tectogrammatical layer, or linguistic meaning ofthe sentence in its context. In general, unique annotation for every sentence (and thus within the sentence as weil, i.e. for every token) is used on all three layers. Human judgment is required to interpret the text in question; in case of difficult decisfons, certain "tie-breaking" rules are in effect (of rather technical nature); no attempt has been made to define what type of disambiguation is "proper" or "improper" at what level. Technically, the PDT is distributed in text fonn, with an SGML markup throughout. Tools are provided for viewing, searching and editing the corpus, together with some basic Czech analysis tools (tokenization, morphology, tagging) suitable for various experiments. The data in the PDT are organized in such a way tbat statistical experiments can be easily compared between various systems - the data have been pre-divided into training and two sets oHest data. In the present section, we describe briefly the Prague Dependency Treebank structure and its history.  2.1. BriefHistory ofthe PDT  The Prague Dependency Treebank project has started in 1996 formally as two projects, one for specification of the annotation scheme, and another one for its immediate "validation" (i.e., the actual treebanking) in the Institute of Fonnal and Applied Linguistics, Faculty of Mathematics and Physics at Charles University, Prague. The annotation part itself bas been carried out in its Linguistic Data Lab. There has been broad cooperation at  Supported by the Ministr)' ofEducation ofthe CR Project LNOOA0063 and by the NSF Grant 0121285.  1. We suppose the "classic" design of an MT system, namely, Analysis - Transfer - Synthesis (Generation). Although we  believe that overall, our representation goes further than many other syntactico-sernantic representations of sentence structure,  we are far frorn calling it an interlingua, since it can in general have different realization in different languages for the same  sentence .  ·  © 2002 Jan Hajic. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 216-226. Universitä di Venezia.  Hajic  217  (1) Od od  vlädy vläda  cekäme cekat  autonomnf autonomni  ekologickou politiku ekologickä politika  RR--2-------- NNFS2-----A-- VB-P---lP-AA- AAFS4----1A-- AAFS4----1A-- NNFS4-----A--  'From the-government we-are-awaiting an-autonomous ･ｾｶｩｲｯｮｭｴ＠  policy'  Figure 1: Example morphological annotation: form, lemma, tag 
We propose a method for obtaining syntactic classes of words from a lexicalized tree adjoining grammar (LTAG: Schabes, Abeille and Joshi (1988)) automatically extracted from a corpus. Since elementary trees in LTAG grammars represent syntactic roles ofa word, we can obtain syntactic classes by clustering words having the similar elementary trees. With our method, automatically extracted LTAG grammars will be arranged according to the syntactic classes ofwords, and the grammars can be improved from various points ofview. For example, we can improve the coverage of the grammars by supplementing to a word the elementary trees of the syntactic class ofthe word. An LTAG grammar consists ofelementary trees, which detennine the position where the word can be put in a sentence, that is, an elementary tree corresponds to a certain syntactic role. Hence, a syntactic class of a word is represented as a set of elementary trees assigned to the word. Since the words of the same syntactic class are expected to have similar elementary trees, we can obtain syntactic classes by clustering words having similar sets ofelementary trees. We applied our clustering algorithm to an LTAG grammar automatically extracted from sections 02-21 of the Penn Treebank (Marcus, Santorini and Marcinkiewicz (1994)), and investigated the obtained clusters with changing the number ofclusters. We successfully obtained some ofthe clusters that correspond to certain syntactic classes. On the other band, we could not obtain some clusters, such as the one for ditransitive verbs, and obtained the clusters that we could not associate clearly with syntactic classes. This is because our method was strongly affected by the difference in the number of words in each part-of-speech class. We concluded that, although our clustering method needs tobe improved for practlcal use, it is effective to automatically obtain syntactic classes of words. The XTAG English grammar (Tue XTAG Research Group (1995)) is a handmade LTAG grammar which is arranged according to syntactic classes of words, "tree families." Each tree family corresponds to a certain subcategorization frame, and determines elementary trees to be assigned to a word. Thanks to the tree families, the XTAG grammar is independent of a corpus. However, it needs considerable human effort to manually construct such a grammar. Automatically extracted LTAG grammars are superior to manually developed grammars in the sense that it takes much less costs to construct the grammars. Chiang (2000) and Xia (1999) gave the methods ofautomatically extracting LTAG grammars from a bracketed corpus. They first decided a trunk path of the tree structure of a bracketed sentence, and the relationship (substitution or adjunction) between the trunk and branches. The methods then cut off the branches of them according to the relationship. Because the sentences used for extraction are in real-world texts, extracted grammars are practical for natural language processing. However, automatically extracted grammars are not systematically arranged according to syntactic classes their anchors belong to, like the XTAG grammar. Because of this, automatically extracted grammars tend to be strongly dependent on the corpus. This limitation can be a critical disadvantage of such extracted grammars when the grammars are used for various applications. Then, we want to arrange an extracted grammar according to the syntactic classes ofwords, without loosing the benefit for the cost. Chen and Vijay-Shanker (2000) proposed the solution to the issue. To improve the coverage of an extracted LTAG grammar, they classified the extracted elementary trees according to the tree families in the XTAG English grammar. First, the method searches for a tree farnily that contains an elementary tree template of extracted elementary tree et. Next, the method collects other possible tree templates in the tree family and makes elementary trees with the anchor ofet and the tree templates. By using tree families, the method can add only proper elementary trees that correspond to the syntactic class of anchors. Chen and Vijay-Shanker (2000) applied this method to an extracted LTAG grammar, and showed the improvement of the coverage of the grammar. Although their method showed the effectiveness of arranging a grammar according to syntactic classes, the method depends on © 2002 Tadayoshi Hara., Yusuke Miyao, and Jun' ichi Tsujii. Proceedings ofthe Sixth International Workshop on Tree Adjoin- ing Grammar and Related Frameworks (TAG+6), pp. 227-233. Universita di Venezia.  228  ---(1 s NPt ...V.-P-...  (2) s  YBP NPt  ha' te  (7) $  ---s NPt VP 1 VBP com' e  s 
Writing a TAG grammar manually is known tobe a non trivial task (Abeille 00; Doran et al. 94; Doran et al. 00; VS et al. 92). For that pwpose, (Candito 96) has suggested a grammatical framework that allows linguists to describe the syntactic properties ofa language at a hlgher level ofabstraction . Given a set ofclasses, each ofthese containing a partial tree description, the compiler outputs a set oftree schemata. In order to work on the organi:Zation of a syntactic lexicon, we needed an almost equivalent tool that would produce feature structures together with the tree schematas. Before developping a new tool, we criticized Candito's work (Candito 99) in considering the two following drawbacks: 1. the algorithm is closely linked to the specific linguistic description. As her analysis focusses on the study of verbal trees, the structuration is badly adapted to the description of non verbal units ; 2. the current implementation of the compiler is not flexible enough to be easily adapted to other input/output formats. In the remainder ofthe paperwe describe ourtool and discuss two possible ways to implement a metagrammar for french verbs. 2. Design ofour tool In this section, we present the characteristics of the tool we implemented. Our compiler ressembles the one described in (Candito 96) and (Candito 99). We thus present the latter first in order to emphasize the differences between her proposition and ours. 2.1. Topological factorisation In (VS et al. 92), the authors propose to use a logic (precisely defined in (Rogers et al. 94)) which describes elementary trees of a TAG grammar so that the topological informations shared by trees is factorized in an inheritance hierarchy. In practice, infonnation concerning trees may be factorized according to different points of view quite independant from each other. For instance, subcategorization infonnation leads to a rather natural hierarchy while realizations ofsyntactic functions lead to another hierarchy altogether. Therefore, attempts to describe both hierarchies in a unique inheritance lattice either leads to having to make a copy ofone hierarchy at each leafofthe other, or ifmultiple inheritance is allowed, multiplying links between leaves ofthe hierarchies. 2.2. Marie-Helene Candito's Compiler (Candito 96) and (Candito 99) precisely explains the preceding point and advocates three independant, linguistically motivated hierarchies which she calls dimensions: l. subcategorization (which she represents in terms ofinitial syntactic functions) 2. syntactic function redistributions (which lead to final syntactic functions) 3. final functions realisations © 2002 Gaiffe, Crabbe, Roussanaly. Proceedings ofthe Sixth International Workshop on TreeAdjoining Grammar andRelated Frameworks (TAG+6), pp. 234-241. Universita di Venezia.  B. Gaiffe, B. Crabbe, A. Roussanaly  235  The underlying idea is that a linguist only describes these three hierarchies, and an automatic tool completes the inheritance graph by crossing final classes of dimension 1 with the final classes of dimension 2 and further crossing the result with the final classes ofdimension 3. However, not all final classes of dimension 1 are to be crossed with all final classes of dimension 2. For instance, intransitive verbs do not admit passive constructions. In the same way, the resulting crossed classes of dimensions 1 plus 2 have tobe crossed only with those final classes of dimension 3 that realize a final function actually occuring in the crossed class. The linguist has thus to give crossing conditions together with his hierarchies in order to constrain the crossing process. The algorithm implemented by (Candito 99) is thus thefollowing:  dim12 = empty set  for each final class cl of dimension 1  for each final class c2 of dimension 2 (compatible with cl)  create c12 that inherits cl and c2 and add it to dim12  end for  end for res = empty set  for each cl2 in dim12  resOfC12 = {c12} '  for each final function ff appearing in c12  for each class c3 of dimension 3 that realizes ff  build new classes with each element of resOfC12 and c3 = resOfC12 these new classes  end for  end for res = res U resOfC12  end for compute minimal ｲ･ｦｾｮｴｳ＠  for each element of res.  As this pseudo-algorithm makes clear, some constants that denote tree nodes are labelled by a final syntactic function. They are actually also Iabelled by an initial syntactic function, probably in order to keep track of the process. In dimension 2, most Of the job done by classes.consists in modifying the functional assignment1• Moreover, final functions are maintained unique in any description by an additional mechanism that equates constants bearing the same final function.  2.3. Our proposition  Our initial motivation for developping a new meta-grammar compiler had to do with the Iexicon: the grammar compiled with Candito's tool is organised in tree families (as is XTAG (Daran et al. 00)) and lemmas are associated to families. The anchoring of a tree thus consists in computing the lemma and the morpho syntactic features associated to a word form, getting the families associated with the lemma and finally attempting to substitute the lemma with the associated morpho syntactic features in the tree. Such a process may of course fail, either because the morpho-syntactic features do not match (consider a tree dedicated to an imperative form, together with. an infinitive word form) , or because the features associated to the lemma do not match (some transitive verbs, for instance, do not accept a passive form). Our starting idea was then to generate trees together with a feature structure that globally describes each tree, and Candito's tool did not seem to pennit that. Since we were implementing a new tool anyway, we gave ourselves some additional constraints: • avoiding non monotonous mechanisms such as the modification of the final functions • not limiting a priori the number ofdimensions: - the third dimension is def acta a collection ofdimensions dedicated to realizing each ofthe possible functions 1. Final functions are initialized to the initial function  236  Proceedings of TAG+6  - three dimensions is perhaps not a good choice for other categories than verbs, or for other languages than French, English or Italian. (See for instance (Gerdes 02)) As we intend to produce trees together with a feature structure, classes of the meta-grammar contain a feature structure that describe its content. lt then seems natural that these feature structures get combined through unification. along the inheritance lattice. This feature structure is then a good mechanism to avoid unwanted class crossings. The example we meotion of intransitive verbs that do not accepts passive fonns may simply be taken into account by means ofan attribute transitive with values minus for an intransitive verb and plus for all passive classes. The remaining problem is to find a mechanism that allowes classes to cross. In (Candito 99) compiler, this mechanism relies oo the three dimensions, but we do not want to rely oo a fixed number of dimeosions. We thus decided to make explicit the reason why classes are to be crossed, typically a class of dimension 1 has tobe crossed with a class of dimension 2 because it needs redistribution of syntactic functions. Classes of dimension 2 have to be crossed with classes of ex-dimension 3 because they need that their final functions be realized. Conversely, a class ofex-dimension 3 may provide the realization of a subject, or an object, or whatever other function. A class in our system is then described by: • aname • a set of super-classes • a description (which is a feature structure) • a set ofneeds (atomic symbols) • a set ofprovidings (atomic symbols) • a fonnula describing trees When a class cl2 inherits two classes cl and c2, the descriptions are unified (in case of failure, c12 is not created), the set ofneeds is the union ofthe two set ofneeds minus the union ofthe providings, the set ofprovidings is also the union of the providings minus the union of the needs and the formula is the conjunction of the two formulas. Tue crossing mechanism then consists in computing all balanced final classes, that is classes whose set of needs and providings are empty2• Finally, the formulas corresponding to balanced final classes are used to compute minimal referents (Rogers et al. 94) together with the description associated with the corresponding class. 3. A survey on the linguistic applications We made some experiments on french verbs, conforming as much as possible to the analysis of (Abeille 91; Candito 99). Therefore, we give an overview ofthe way we describe verbs using three 'dimensions' 3• Two ways to generate a gramrnar are given in the following sections. The first approach puts the focus on the topology of the trees. While it allows to identify the nodes representing the predicate and its arguments in the tree, it actually suffers from a major drawback due to the monotonicity ofthe system. That is, once a node is asserted in the process of generating a tree, it cannot be removed. This is problematic if, for instance, one wants to describe an agentless passive as the ellipsis ofthe predicate's agent4 • The second approach comes closer to the functional analysis introduced by (Candito 99). We do a topologically free reasoning upon arguments and functions until we have specified a complete final functional subcategorization frame. The main interest ofthis approach is that the functional component ofthe grammar is not anymore 2. In order to keep with an associative and commutative mechanism, a cancelled need as weil as a cancelled providing is not allowed to appear again. 3. Fonnally speaking, it should be clear that there are no dirnensions anyrnore. 4. However, there is a trick : one can set a 'kill' attribute to a node. lt rneans that the node will be removed frorn the tree after its generation. The principle is that the removed node's children (if any) become the children ofthe removed node's parent (the root of the tree cannot be removed).  B. Gaiffe, B. Crabbe, A. Roussanaly  237  mixed with the topological one. We are able to represent the agentless passive quite easily. But here, the relationship between the tree structure and the logical arguments is lost. As a comparison, .we do not specify any topological information into the classes that belong to the equivalent of Candito's first and second dimension. The whole analysis is driven by the feature structures describing the classes. Both approaches share some essentials ideas5 • ( 1) Each tree which is generated represents the realization ofa logical predicate and its arguments. (2) The focus is put on the functional organization ofthe grammar. Following these assumptions we specify through three successive layers the trees that represents the syntactic realization of that predicate. Each ofthese layers performs a mapping as follows :  Predicate Structure JJ. Initial- Function JJ. Final-Function JJ. Surface-Realization  Dimension 1 Dimension 2 Dimension 3  • Dimension 1 : maps the predicate arguments to an initial functional subcategorization frame.  • Dimension 2 : maps the initial subcategorization frame to a final functional subcategorization frame.  • Dimension 3 : maps the final subcategorization frame to a tree structure.  3.1. A node driven strategy  Following (Abeille 91; Candito 99) each tree which belongs to a family represents a predicative structure. Tue  first dimension is dedicated to mapping initial functions to the predicate's argument positions. Here we define a  set of classes which represents the arguments and another set which defines the functions that are to be mapped on  them. The final classes ofthis dimension are a list ofall the valid mappings in French. For instance the final class  SubjOVObjl is the class where the first argument is mapped with a subject and the second is mapped with a direct  object6 .  .  ro-lrl  ｾ＠  --- ---  init-sibj Jnit-Subj -- / /  arg() = init--wbj arg! = init--obj  subjOVobjl  o b} -redist iniM>bj -- Jnit-Obj  Figure l: Overview ofthe first dimension  We express this mapping with the declaration ofa constant7. Each ofthese quasi-nodes is equated with the one representing its associated function. To be linguistically weil fonned, we impose the requirement that the crossed classes map each initial function to a final function. Then a class where an initial function node is defined contains a corresponding need for a final function (see fig. 3.1). 5. These ideas are already central in (Candito 99) work. 6. For the sake of clarity, we do not expand here to the whole (Abeille 91) analysis in farnilies. We do not consider here sentential arguments and verbal auxiliaries though they are important for the definitions of the families. 7. Following (Rogers et al. 94), a constant is denoting a node.  238  Proceedings ofTAG+6  The second step in the generating process aims at defining a final subcategorization frame. Here we map the initial functions given above with final functions. As a side effect, the verb is given a rnorphological specification. For instance, through inheritance, thefall personalpassive maps the initial-subject to a by-object, the initial object to ajinal-subject and requests the predicate tobe realized as a passive subtree. Furthermore, classes that introduce a final function emit the need that this function is realized.  ｩ ｮｩｴｾｦｪ＠ SUBJ-> l'AR-08  OBJ->SUllJ  Figure 2: Overview ofthe second dimension  Our general strategy consists in manipulating functions through constants denoting nodes: instead offunction  features, we have such constants as argO, initial-subject, orfinal-object. The redistributions are then performed  by means of equalities between such constants. The final classes of dimension 2 express equalities between the  nodes carrying the initial functions and the ones carrying the final functions. The interface with the first dimension  is done through providings that satisfy the final-function needs of dimension 1 classes. For instance the Full-  personal-passive class will inherit classes that provide the subj-redist and obj-redist. The content of the class  reflects the mapping : the initial-subject node equals the final-by-object node and the initial-object node equals the  final subject node (see fig. 3.1).  g  .......  ｾ Pmuia,:,b1  ｓｕｂｾｒｅｊ＠ Ｏ ｾ＠  ｾＭ＠  BY-OBJ-REAL  /  /  /  ｾ＠ T pl'"1"<;>-le<  Sc ｾ＠ Sm """"""""  ｓ｣ Ｚ｣｡ｾ  ｳ＠  par-o-i: :: cat • PP  ｾ＠  :; cal „ Pn:p  ｾ  ［Ｚ ｫ＾｣ ｳ ｰ｡ｲ＠  w-noun,;caa•Ntype•lllbst  QU- BY-OBJ  INV-NOM-SUB'  Figure 3: Overview ofthe third dimension The functional realization 'dimensions', contains classes that actually yield trees. Basically, we view here a syntactic function as a labe! for a set of subtrees. Thus, this dimension groups all the subtrees and each ofthem is labelled as the representation of a function. The labe! assignment is performed through multiple inheritance as shown in figs. 3.1 and 3.1. Note that contrary to (Candito 99) approach, there are here two 'third dimensions'. One for the predicate's arguments and one for the predicate's head. The motivation is mainly methodological, we want to explicitly separate the functional and the topological part of the gramrnar. Writing a metagrammar remains (at least for us) an experimental process. Other formalisms have been proposed that rely extensively either on feature structures (HPSG) or Linguistic functions (LFG). We experimented  B. Gaiffe, B. Crabbe, A. Roussanaly  239  with a LFG inpired approach whlch allows us to deal with the topology of trees only in dimension 3. The general sketch is then to build feature structures in dimension 1 and 2 and to assemble the trees according to the specifications given by the feature structure in dimension 3.  ....S.m.--..  Vm  Vb  Stn::cat • S  Vlll::Cill•Vtype...anc  ｖ｢ｾ  ｣｡ｴＭ  ｖｴｹｰ｣  ｾ＠  PASSIVE  Figure 4: Overview of 'another' third dimension  3.2. A feature driven strategy  Tue feature structure descriptions, contained in classes and therefore associated to the produced trees at the  end ofthe process, not only concem the anchoring, but may also actually describe the linguistic properties the tree  is responsible for. Typically, it enables us to know tbat a tree is the representation of a two place predicate, that  this predicate is a passive predicate, that the first argument is expressed as c;;litic and so forth. Tlms the compiler  allows to generate trees but also complex feature structures that are an ･ｸｰｬｩ｣ｾ＠  translation ofwhat each tree 'rneans'  linguistically.  In the previous approach we put the focus on the identification of particular nodes into the trees, and the  feature structures associated to the classes only concems the impossibilities in crossings. In the new approach  we build complex feature structures and less complex formulas as lots ofconstants equated in rorder to represent  redistributions simply disappear.  As an example, here are the features inherited by the following final classes:  • SUBJOVOBJ 1 : [PRED [::::AT ｾＺｔ｟［＠  SUBJECT], [INIT-FUNC objectJ)]]  • FULL-PERS-PASSIVE : HEAD [vMORPH passive]  PRED  INIT-FUNC SUBJECT] INIT-F.UNC object])  SUBCAT  (  FIN-FUNC [ CONS  BY-OBJ [ID  ,  FIN-FUNC [ CONS  subject []  CSET  • QUEST-BY-ÜBJ :  [PRED [CSET EXTRACTION By-objl]]  [BY-OBJ  quest  • lNV-SUBJ: [PRED (csET [SUBJ invertedJ]]  240  ·Proceedings ofTAG+6  In this approach, needs and providings are dispatched as they were in the previous approach. Classes of dimension 1 and 2 do not contain any fonnulas anymore. The third dimension realizes arguments as well as the predicate subtrees. As a sample we generate the tree representing the schema that allows to analyze the sentence Par qui sera accompagnee Marie ? (By whom will be.accompanied Mary) with the combination ofthe following final classes subjOVobjl,fall-pers-passive, passive, inverted-subject, questioned-par-obj(see figs. 3.1, 3.1, 3.1 and the feature structures given above) :  ｾ＠ PP /'-.... Prep N.J.  s ｾ＠ V  S N..t.  
1. lntroduction  This paper reports on work in progress on the creation of a metagrammar for Gennan verbal constructions. Section 2 circumscribes the field we are working on: We describe known and less known problems and try to delineate the limits of a standard Tree Adjoining Grammar for Gemtan; we see which structures we get easily, and which structures we will never get. In Section 3 we put the German data in perspective to the other TAG languages, French and English, and we define and justify our choice to create a limited German TAG, designed for a specific generation task. We then propose in Section 4 some of the possible elementary trees that can live up to our expectations: Compromising on the semantic interpretability of the derivation structure as well as on the principles underlying TAG allows us to get most of the word orders necessary for the generation task. Yet, what we gain in usefulness in generation, we pay in linguistic descriptiveness. Last but not least, in Section 5 we discuss problems and limits of the metagrammar implementation and we give some indication on how the desired elementary tree sketches can be created and maintained with a metagrammar. We presuppose the comprehension ofthe terms metagrammar and topologicalfield ofa German sentence, which we cannot define here. For details, see Gaiffe et al. 2002 and Kathol 1995 respectively. 2. Scrambled Minds  The two existing Tree Adjoining Grammars of interesting grammatical coverage have been created for English (XTAG, 1995) and French (FTAG, Abeille 1991), two languages with quite rigid word order and Iittle case marking. On the other band it has long been shown that German is beyond the (derivative) generative capacity of TAG: There are no verbal elementary trees carrying the nominal arguments (and thus verifying the predicate-argument cooccurrence constraint) that can be combined to cover some of the (so called) scrambled word order ofGerrnan (Becker et al. 1991, 92). Let us see where the problem is.  2.1. Argument Scrambling  Sentence (1 )' is the standard example for scrambling: The two constituents in the Mittelfeld (the positions between finite verband non-finite verbs) have 'exchanged' their position. Note that this is a very natural order that even is the standard order ifthe direct and the indirect objects are pronominalized as in (l).  (1) a. b.  Peter hat das Buch meinem Vater zu lesen versprochen. Peter has the book to my father to read promised. 'Peter has promised to my father to read the pook.' Peter hat es ihm zu lesen versprochen Peter has it him to read promised. 'Peter has promised him to read it.'  By definition, a control verb like versprochen 'promised' assigns a theta role to the subject just like the embedded verb. So the (semantic) predicate argument structure is shown in Figure la (leaving aside for the moment the tense auxiliary because ofits uncertain semantic role). As the derivation structure ofTAG is a tree, we have to restrict our analysis to one of the predicate-argument links, the subject role of the infinitive or the subject role ofthe control verb. Suppose we wanted to follow the usual XTAG/FTAG analysis and have the control verb versprochen 'promised' govem its subject. Suppose further that we handled the auxiliary as an adjunction to the matrix verb. Our goal is then to obtain the derivation structure in Figure 1b. The first elementary tree of versprochen 'promised' (Figure 2) can adjoin to the root node of the infinitive and subsequently, its subject substitution node appears at the right place. However, in this case, its dative substitution node remains outside of the infinitive's elementary tree. We do not obtain the word order of sentence ( 1}; the dative would be supposed to appear at the  I would like to thank the XTAG group at the University of Pennsylvania and an anonymous reviewer for their helpful comments on my work. I am also grateful to Tilman Becker for his help finding the least ugly trees and keeping up my faith in TAG. l assume the customary responsibility for content and shortcomings.  © 2002 Kirn Gerdes. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and RelatedFrameworks (TAG+6), pp. 242-251. Universita di Venezia.  Gerdes  243  versprochen 'promised'  
There are two salient linguistic uses of adjunction: for analyzing long-distance wh-movement (and related movement types) in many languages and for analyzing cross-serial dependencies (CSD) in Dutch and Swiss German. While the need for and the adequacy of adjunction to model wh-type movement have been questioned (Rambow and Vijay-Sbanker, 1998; Rambow et al., 2001), CSD seems ideally suited for a TAG analysis, since, as Shieber( 1985) showed, CSD cannot be derived by a context-freegrammar. In fact, some ofthe altemate tree rewriting systems proposed which do not include adjunction, such as the DSG of (Rambow et al., 2001}, cannot provide a satisfactory analysis of CSD, either: it is specifically the definition of adjunction as an tree-rewriting operation that inserts one tree in its entirety into the center of another that is crucial for deriving CSD. What is somewhat troubling, however, is that the construction appears tobe limited to two West Germanic languages/dialects, Dutch and Swiss Gennan. In this paper, we show that the same construction, though with different syntactic characteristics, is found in a completely unrelated language, Tagalog. We show how the analysis of Kroch and Santorini (1991) for Dutch can be adapted for Tagalog, and we show furthennore that the syntactic analysis suggested by TAG is preferable to an analysis based on head movement and verb insorporation. 2. The Tagalog Data Tagalog, a major Austronesian language spoken in the Philippines, is strongly verb first. Complements and the subject follow the verb with preferences for the agent to directly follow the verband for the nominative argument tobe last (preferences which can be in conflict). The nominals are case marked for nominative (NOM) and oblique (OBL}, and another distinguished case is un-glossed in the examples (for a discussion of this case as both ergative and accusative see (Maclachlan, 1994)). The standard ordering in complex sentences is VI Agent! linker [V2 (Agent2) Theme2], as shown in example (la). Phrases ofvarious sorts are separated by a linker (LK) and Tagalog also has sentential conjunction (CONJ). A cross serial dependency ordering altemates with this basic ordering in which the agent of the matrix clause follows the embedded verb as in (lb): V I linker V2 Agentl (Agent2) Theme2. 1 (1) Basic and CSD altemates a. Nagisip si Pedro-ng bumili ng bulaklak AT-thought NOM-Pedro-LK AT-buy flower b. Nagisip na bumili si Pedro ng bulaklak AT-thought LK AT-buy NOM-Pedro flower 'Pedro thought to buy (ofbuying) a flower.' Let us note two further properties ofthe CSD for which we will account with a TAG analysis. First, the CSD process can be iterated: (2) Iteration of CSD 1. Baldridge (1998) claims that Tagalog simply has long-distance scrarnbling and that a ViViN2N1 ordering is also allowable. However, he gives only one example, and admits that the sentence may have a completely different interpretation (in which the two NPs fonn one NP). We have, in our work with native speaker informants, not found any evidence for generalized long-distance scrambling, and therefore will assume for the sake of this paper that we have a CSD, not a long-distance scrambling construction. lfit were in fact a long-distance scrarnbling construction, TAG would not be powerful enough for an analysis - see (Rambow, 1994) for a discussion of Gennan. Baldridge (1998) also discusses asymrnetries in wh-extraction in Tagalog. We do not address the issue here. © 2002 Anna Maclachlan and Owen Rambow. Proceedings ofthe Sixth International Workshop on Tree Ad}oining Grammar andRelated Frameworks (TAG+6), pp. 252-258. Universita di Venezia.  Maclachlan and Rambow  253  a. Ipiniangako ni Maria-ng subuka-ng manalo sa karera promised Maria-LK tty-LK win OBL-race b. Ipiniangako-ng subukan ni Maria-ng manalo sa karera promised-LK tI)' Maria-LK win OBL-race c. Ipiniangako-ng subuka-ng manalo ni Maria sa karera promised-LK try-LK win Maria OBL-race 'Maria promised to tI)' to win the race.' Second, the CSD sentence pennits only one NOM nominal, while the basic complex sentence permits two. This can be seen when the theme is NOM in the embedded clause as in (3). In this essentially passive clause type, the verb is marked with Theme Topic morphology (TI) whereas in the essentially active clause type, as in both clauses in (1), the verbis also marked but with Agent Topic morphology (AT). While the matrix agent is NOM in (3a) it cannot be in the CSD equivalent in (3b) as long as the embedded theme is NOM (3) Basic and CSD altemates with embedded passivization a. umasa si Maria-ng sulatin ang kuwento AT-hoped NOM-Maria-LK write-TT NOM-story b. umasa-ng sulatin {*si Maria i ni Maria} ang kuwento AT-hoped-LK write-TT {*NOM-Maria/Maria} NOM-story 'Maria hoped to write the story.' 3. A TAG Analysis In TAG, we derive CSD by recursively adjoining elementary trees into each other at interior nodes. As is th• case with all embedded clause constructions (be they CSD or not), each clause is adjoined into its immediately embedded clause, since the most deeply embedded clause does not have a linguistically meaningful footnode labeled with a clausal category (and hence its embedding clause must adjoin into it, rather than vice versa). When we adjoin an auxiliary elementary tree such as that shown schematically at the top left in Figure 1 (the superscript 1 indicates that this represents the matrix clause) to the initial tree at the top-right (the superscript 2 indicates that this is the embedded clause, we are only considering one level ofembedding in this schematic discussion, though ofcourse the process can iterate), the result is as shown below in Figure 1. The nodes labeled A, B, C, D represent either substitution or tenninal nodes - in either case, these are positions below which tenninals can be generated. Of course, we do do not expect terminal symbols to be generated below each of these symbols. In fact, if we restrict ourselves to the case in which we have one (overt) noun phrase and one verb in each elementary tree, two · of the symbols will dominate the empty string. If we choose A and D to dominate the empty string, we obtain a center-embedded structure with the associated string ß(l) ß(2)C(2) C(l), as desired. Tue derivation is essentially a context-free derivation and does not actually make use ofthe füll power ofadjunction, since no terminal nodes are generated above the adjunction site. lt is clear that to obtain CSD, we must choose as overt terminal nodes one above the adjunction site (A or D) and one below (Bor C). Ifwe choose, say, A and B as the overt nodes, we obtain a structure which is not derivable with a context-free grammar, but the string nonetheless represents center-embedding (A<2) A(l) ß ( l)B(2)). Thus, we must choose one overt terminal to the left of the spine, and one to the right. This leaves us with exactly two possibilities - A and C are overt, or B and D. Since in Dutch and Swiss German CSD, the füst element is always a matrix noun phrase (and not an embedded one), we cannot use A and C as the overt elements: while adjoining the matrix clause into the embedded clause would result in A {2) A (l)C(2) C{l), with cross-serial dependencies, the string starts with the wrong A: A(2) rather than A(l). Thus, we must leave A and C empty, with the overt material inB and D. This is of course exactly the choice that Kroch and Santorini (1991) make. They propose that in Dutch,2 the verb raises from its ordinary position as sister to the S footnode to a position above the adjunction site (which 2. ·Their analysis also applies to the relevant Swiss German data.  254  Proceedings ofTAG+6  sei> NA  s 1) ｂｾＱＩ＠ NA 5(2} NA  s 1) ｂｾＧｉ＠ NA ｂｾＲＩ＠ Figure l: Elementary trees (auxiliary, above left,and initial, above right) and derived tree obtained by adjoining the auxiliary tree into the initial tree at the latter's interior S node(below) can be interpreted as right-Chomsky-adjoining3 to the regular maximal projection). What makes their analysis so compelling is that this analysis, in which the verb "raises" to a higher position in the tree, takes up some elements ofthe analysis suggested previously in the Germanic syntax literature. In this analysis, which dates to at least Bech (1955) and was expressed somewhat more formally in a transformational framework by Evers (1975), the verbs actually raise out of their clauses and form a single morphological unit. Such an analysis is impossible in TAG, since apart from the effect of adjunction, the elementary tree retain their structural integrity. Furthermore, Kroch and Santorini (1991) argue that there are empirical arguments against a morphological verb cluster, though not against verb raising itself. Thus, the analysis proposed by Kroch and Santorini (1991) is the closest possible TAGbased analysis which uses the independently proposed notion of verb raising (but not verb cluster formation), and it also corresponds to the only possible analysis considering the topology of trees and the definition of adjunction! Let us now turn to Tagalog. In Tagalog, we have a verb-initial construction rather than a verb-final construc- tion. However, the argument about possible analyses is exactly the same as in Dutch, and we conclude that B and D must be overt, not A and C. Because Tagalog is verb-initial, we must choose B to represent the verb, and D to represent the noun phrase. We thus are forced to adopt an analysis in which the NP is raised, and in which it is the raising of the NP which results in the CSD.4 This is shown in Figure 2 (the subscripts indicate the relation between traces and moved elements within elementary trees, while the superscripts, as before, indicate which clause a terminal symbol belongs to). The trees in this figure derive the CSD version of(l), (lb), repeated here for convenience: (4) Basic and CSD altemates (=(l)) a. Nagisip si Pedro-ng bumili ng bulaklak AT-thought NOM-Pedro-LK AT-buy flower 3. we use "Chomsky-adjoining" to refer to derivation processes within elementary trees (following the general approach of (Frank, 2001)), while "adjoining" refers to the TAG operation that combines elementary trees. 4. This does not mean that the verb cannot also raise frorn a VP-internal position to a higher position on its own projection, as is custornarily assumed for verb-initial languages. lt just means that the landing site of the verb must be below the node at which adjunction ofthe matrix clause happens. This is in fact exactly the analysis in Figure 2.  Maclachlan and Rambow  255  I'  ｾ＠  I'  NP  Ai1) 
1. Introduction As shown by Srinivas (1997), standard n-gram modeling may be used to perfonn supertag disambiguation with accuracy that is adequate for partial parsing, but in general not sufficient for füll parsing. A serious problem is that n-gram modeling usually considers a very small, fixed context and does not perfonn weil with large tag sets, such as those generated by automatic grammar extraction (Xia, 1999; Chen and Vijay-Shanker, 2000; Chlang, 2000). As an alternative, Chen, Bangalore and Vijay-Shanker (1999) introduce class-based supertagging. An example of class tagging is n-best trigram-based supertagging, which assigns to each word the top n most likely supertags as detennined by an n-gram supertagging model. Class-based supertagging can be performed much more accurately than supertagging with only a small increase in ambiguity. In a second phase, the most likely candidate from the class is chosen. In this paper, we investigate an approach to such a choice based on reranking a set of candidate supertags and their confidence scores. RankBoost (Freund et al., 1998) is the boosting algorithm that we use in order to learn to rerank outputs. lt also has been used with good effect in reranking outputs of a statistical parser (Collins, 2000) and ranking sentence plans (Walker, Rambow and Rogati, 2001). RankBoost may learn to correct biases tbat are inherent in n-gram modeling which lead to systematic errors in supertagging (cf. (van Halteren, 1996)). RankBoost can also use a variety of local and long distance features more easily than n-gram-based approaches (cf. (Chen, Bangalore and Vijay-Shanker, 1999)) because it makes sparse data less ofan issue. The outline of this paper is as follows. First, we develop the background and motivations behlnd the task of reranking the output of an n-best trigrarn supertagger. Second, we introduce RankBoost as the approach that we adopt in order to train the reranker. Third, we perform an initial set of experiments where the reranker is trained with different feature subsets. Fourth, we perform an in-depth analysis of several reranking models. Fifth, after pointing out causes that at times render the reranker ineffective, we develop and test some new models that attempt to sidestep these limitations. Lastly, after some significance testing results, we state our conclusions and remark on potential füture directions. 2. Background and Motivation In this section, we motivate the desirability of exploring the use of n-best reranking of supertags. Although we give multiple motivations, we focus on justifying our approach as a promising alternative in improving the perfonnance of a füll parser. First, we review the supertagging task and its applications. Because supertagging requires the existence of a particular TAG, we subsequently introduce automatically extracted TAGs and motivate their use. Although extracted grammars have their advantages, supertagging using automatically extracted TAGs runs into damaging sparse data problems. We review n-best supertagging as one means of alleviating these problems. Lastly, we run experiments that show supertagging is potentially a viable option in order to speed up a füll parser. Throughout this section, we describe the kinds oflinguistic resources that we use in all of our experiments and the kinds ofnotation that we will employ in the rest ofthis paper. 2.1. Supertagging Supertagging (Bangalore and Joshl, 1999) is the process of assigning the best TAG elementary tree, or supertag, to each word in the input sentence. lt performs the task of parsing disambiguation to such an extent that it may be characterized as providing an almost parse. There exist linear time approaches to supertagging, providing one promising route to linear time parsing disambiguation. However, Srinivas (1997) shows that standard n-grarn modeling may be used to perform supertagging with accuracy that is adequate for partial parsing, but not for füll parsing. On the other hand, n-gram modeling ofsupertagging has been found tobe usefül in other applications such as infonnation retrieval (Chandrasekhar and Srinivas, l 997b) and text simplification (Chandrasekhar and Srinivas, 1997a). © 2002 John Chen, Srinivas Bangalore, Michael Collins, and Owen Rarnbow. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar andRelated Frameworks (TAG+6), pp. 259-268. Universitä di Venezia.  260  Proceedings ofTAG+6  2.2. Automatically Extracted Grammars Recently, procedures have been developed that automatically extract TAGs from broad coverage treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000). They have the advantage that linguistically motivated TAGs can be extracted from widely available treebanks without a huge investment in manual labor. Furthermore, because of their direct extraction from a treebank, parameters can be easily and accurately estimated for building statistical TAG models for parsing (Chiang, 2000; Sarkar, 2001) or geoeration (Bangalore, Chen and Rambow, 2001 ). In our experiments, we use an automatically extracted TAG grammar similar to the ones described by Chen and Vijay-Shanker (2000). This grammar has been extracted from Sections 02-21 of the Penn Treebank (Marcus, Santorini and Marcinkiewicz, 1993). lt contains 3964 tree frames (non-lexicalized elementary trees). The parameters of extraction are set as follows. Each tree frame contains nodes that are labeled using a label set similar to the XTAG (XTAG-Group, 2001) label set. Furthermore, tree frames are extracted corresponding to a "moderate" domain of Jocality. Also, only those empty elements in the Penn Treebank that are usually found in TAG (subject and object trace, for example) are included in this grammar. 2.3. N-best Supertagging The efficacy of n-gram modeling ofsupertagging is limited by sparse data problems of very large TAGs, such as those that are automatically extracted from broad coverage treebanks. Chen and Vijay-Shanker (2000) show that supertagging using extracted TAGs is perfonned at a lower accuracy (around 80%) than accuracies that have been published for supertagging using hand-written TAGs (around 90%). Faced with this evidence, it might seem that it is a hopeless task to use supertagging using extracted TAGs as a preprocessing step to accelerate full parsing. On the other band, Chen, Bangalore and Vijay-Shanker (1999) investigate class-based supertagging, a variant of supertagging where a small set ofsupertags are assigned to each word instead ofa single supertag. The idea is to make the sets small enough to represent a significant reduction in ambiguity so as to speed up a füll parser, but to construct the sets so that class-based supertagging is much more accurate than supertagging. One such promising class-based supertagging model is n-best supertagging, where a trigram model assigns up to n supertags for each word ofthe input sentence. Let W = w1, ... ,Wn represent the sequence ofwords that is the input to a supertagger. Let Ttri = ti,1, ... , tn,1 be the output ofthe (!-best) trigram supertagger. The output of the n-best supertagger is a sequence of n-best supertags NBEST(i) = t i,1 , ... , ti,n(i) for each word Wi such that each supertag ti,j has an associated confidence score Ci,;. Assume that each sequence NBEST(i) is sorted in descending order according to tbese confidence scores. The n-best supertagger is obtained by a modification of the (1-best) trigram model of supertagging. Both supertaggers first use the Viterbi algorithm to find Ttri by computing the most likely path p(Ttri) through a lattice of words and pairs ofsupertags. In the trigram supertagger, each node k along the path p(Ttri) is associated with exactly one prefix probability (the highest). In contrast, the corresponding node k in the n-best supertagger is associated with the n highest prefix probabilities. This difference allows the n-best supertagger to associate up to n supertags for each word Wi. The confidence score Ci,i ofsupertag ti,j is the jth-best prefix probability of a node k divided by the least best prefix probability ofthe same node. 2.4. Parsing with N-best Supertagger Output We claim that supertagging is a viable option to explore for use as a preprocessing step in order to speed up füll parsing. In order to substantiate this claim, we perform exploratory experiments that show the relationship between n-best supertagging and parsing performance. Using the grammar that is described in Section 2.2, we train n-best supertaggers on Sections 02-21 of the Perut Treebank. For each supertagger, we supertag Section 22, which consists of about 40,100 words in 1,700 sentences. We then feed the resulting output through the LEM parser, a head-driven TAG chart parser (Sarkar, 2000). Given an input sentence and a grammar, this parser either outputs nothing, or a packed derivation forest of every parse that can be assigned to the sentence by the grammar. lt does not retum partial parses. The results of these experiments are shown in Table 1. The input to the parser can be the output of either a 1, 2, or 4-best supertagger. lt can also be sentences where each word is associated with all of the supertags with that word's part of speech, as detennined by a trigram part of speech tagger. This is labeled as "POS-tag" in the table. Lastly, it can simply be sentences where each word is associated with the correct supertag. This is labeled as "Key." The table shows the supertagging accuracy ofeach corpus that is input to the parser. lt also shows each  Chen, Bangalore, Collins, and Rarnbow  261  Table 1: Relationships between n-best supertagging and parsing  Input to Parser 1-best 2-best 4-best 8-best POS-tag Key  % Supertagging Accuracy 81.47 88.36 91.41 92.77 97.30 100.00  Ambiguity (supertags/word) 1.0 1.9 3.6 6.3 441.3 1.0  % Sentences Receiving SomeParse 28.2 53.6 76.7 - - 97.0  Timeto Parse Corpus < 3 hours < 2 days 2-3 weeks - - < 5 hours  % Sen!ences Parsed Versus% Supertagging Accuracy 100 % Supertagging l'IX:J.Jr8f:Y -+-- 90  
In this paper we outline the advantages ofdeploying a shallow parser based on Supertagging in an automatic dialogue system in a call center that basically leaves the initiative with the user as far as (s)he wants (in the literature called userinitiative or adaptive in contrast to system-initiative dialogue systems). The Supertagger relies on a Hidden Markov model and is trained with Gennan input texts. The entire design ofa Hidden Markov-based Supertagger with trigrams builds the central issue of this paper. The evaluation of our German Supertagger lags behind the English one. Some ofthe reasons will be addressed later on. Nevertheless shallow parsing with the Supertags increases the accuracy compared to a basic version ofKoHDaS that only relies on recurrent plausibility networks. 1. Introduction Wizard--0f-Oz experiments show that users of automatic dialogue systems would preferentially take the initiative in many dialogues instead ofbeing asked a long list oftiny little questions by the system (cf. (Boje et al„ 1999)). Empirical evaluations demonstrate that adaptation to the user's dialogue preference leads to significantly higher user satisfaction and task success (cf. (Strachan et al„ 1997) or (Litman, Pan and Walker, 1998)). In contrast to these results, it can also be observed that in such user-initiated dialogue systems the user is sometimes left without a clear understanding ofhis/her options at a given point in the dialpgue. This can cause frustration or even breakdowns of the communication. Consequently, an adaptive system wlrich reacts to the user's preferred mode, i.e. is able to ask explicit questions when the user doesn't take the initiative and to react to user-provided complex tums adequately as weil at any particular state ofthe dialogue, serves as a user-friendly dialogue system. The criticised strict dialogue structure with an explicit and inevitable initiative by the system (henceforth called system-initiative in contrast to user-initiative) results from the crucial fact that with any of these questions by the system a particular sub-grammar and sub-lexicon of the speech analysis system (e.g. a simple number or yes/no grammar and Iexicon, respectively) can be associated to analyse the user's answer more reliably. Clarification dialogues caused by incorrectly analysed words can be circumvented by this method. Hence it is essential for a user-initiative or adaptive (or also called mixed-initiative) system to remedy the shortcomings resulting from the less reliable analysis ofthe user's spoken turn with a general grammar and lexicon, respectively. Furthermore, the taskparameters, i.e. the infonnation provided in the user's turn to petform the user-intended task by the system, have to be extracted without knowing exactly where in the user's turn or whether at all they have been uttered yet. In the case that not all task parameters are provided even a user-initiative system has to ask questions - similar to a system- initiative system. KoHDaS-NN1 is an automatic help desk system in a call center that basically leaves the dialogue initiative with the user as far as (s)he wants. The user's turn circumscribing the problem as a whole is handed to a hierarchy of recurrent plausibility networks which classify the according problem. In the next step the system extracts even only implicitly mentioned task parameters of this problem class from the turn by a graph-matclring technique. Remaining or unidentified task parameters required to solve the problem are asked by the system in an ordinary question-answering manner. The results of KoHDaS- NN where the classification and the extraction of the task parameters is performed only on the basis of simple words are promising. However the number ofwrong classifications and questions for yet uttered task parameters has to be further decreased in order to provide a user-friendly dialogue with the customers. Hence we investigate in the following whether deploying a shallow parser based on Supertagging increases the performance of the system - both with respect to the classification and the extraction of the task parameters. The Supertagger in KoHDaS-ST relies on a Hidden Markov model and is trained with German input texts. The main section ofthis paper is devoted to the description ofthis method (cf. Section 3; it also comprises some 1. The acronym KoHDaS stands for Koblenzer Help Desk with automatic Speech recognition. In the following the basic version is called KoHDaS-NN (NN stands for Neural Networks). Later on we investigate KoHDaS-ST where ST stands for SuperTagging. @ 2002 Jens Bäcker and Karin Harbusch. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 269-278. Universitä di Venezia.  270  Proceedings ofTAG+6  implementation details to gain efficiency). With respect to accuracy our Gennan Supertagger lags behind the English one. Some of the reasons will be addressed later on. Nevertheless shallow parsing with the Supertags increases the accuracy compared to a basic version ofKoHDaS that only relies on recurrent plausibility networks. Tue paper is organized as follows. In the next Section KoHDaS and its functionality is described. In Section 3 the Supertagger based on Hidden Markov models is outlined. Section 4 is devoted to the description ofKoHDaSST, i.e. how Supertagging used in shallow parsing is integrated into the application of KoHDaS. Furthennore, it depicts the results ofKoHDaS-ST compared to KoHDaS-NN. In Section 5 related work is portrayed. Here further methods that favorably compare to Supertagging are outlined on the one band. On the other, different Supertagging methods and their application domains are delineated. The paper ends addressing future work and open problems. 2. KoHDaS - an adaptive dialogue system for First-Level Support via telephon KoHDaS (see, e.g., (Harbusch, Knapp and Laumann, 2001)) explores methods which provide automatic userinitiative dialogues in call centers, i.e. the initiative should basically be left witb the user as far as (s)he wants. Compared to system-initiative dialogue systems, user-initiative systems cannot rely on restricted dictionaries and grammars during the speech recognition process to gain more reliable results. Hence, methods to remedy the reduction of understanding in the first phase have tobe impinged on the system. KoHDaS-NN deploys the following two techniques towards a natural dialogue behaviour. First, a hierarchy of neural networks classifies the user's whole turn (on average 25 words) according to a list of problem classes. After a confinnation dialogue the task parameters mentioned in the user's turn are extracted to avoid redundant questions by the system for infonnation to perfonn the task the user wants the system to perform - in our case a dai:a base look-up for a solution of the user's hardware problern. As for classification, a hierarchy ofrecurrent plausibility networks (cf. (Wermter, 1995)) accomplishes the consideration of arbitrary previous contexts in current decision making in KoHDaS. After a confi.rmation that the problem class is correctly recognized, the task parameters, i.e. the necessary information to enable the system to perfonn the task the user intended the system to do, are extracted from the user's initial turn by a graph matching technique imposed on the dialogue graphs, i.e. the specification ofall possibly asked questions by the system and the according task parameters provided by the user (note that these graphs have to be specified anyhow to model a user who does not take the initiative at all). KoHDaS is currently customized to the domain offirst level support for computer hardware but it can easily be adapted to new domains. A drawback of KoIIDaS-NN is the absence of syntactic infonnation in the processing of the user's turn as KoHDaS-NN works simply word-based. With respect to the classification task, a word such as "monitor" remains active by the context-layer independent whether the word in mentioned in a subordinate clause or as a key concept to characterize the problem (cf. "My new monitorfiickers since 1 have .„" vs. "My SCSI hard drive which I bought together with my new monitor two weeks ago from your customer's service cannot be fonnatted ..."). With respect to the extraction oftask parameters negations are completely ignored in KoHDaS-NN. For instance, "My monitorfiickers although the boxes are not activated" takes for granted that the monitor only occasionally ftickers when the boxes are activated. Hence the wrong conclusion is drawn from the customer' s utterance. In order to remedy this shortcoming, KoIIDaS-ST deploys a robust syntactic analysis based on a Supertagger (Joshi and Srinivas, 1994) and a Lightweight Dependency Analyzer (LDA) (Srinivas, l 997a) in the analysis ofthe user's turns. Particularly for the correct interpretation of the scope of negations we have decided to use Supertagging instead of chunking (cf. Section 5). The structural information - which possibly remains partial - provided by this analysis is used in the classification step as weil as in the infonnation extraction step ofKoHDaS. The use of structural information in classification can prevent words occurring in deeply nested sub-sentences from having high impact on the determination of the problem class. In the infonnation extraction step, the use of structural information allows to detect dependencies between structures in the turn, that can't be detected in the word-based version ofKoHDaS-NN (e.g., negations; cf. Section 4). 3. Hidden Markov model-based Supertagging  Our German Supertagger uses a trigram model and is based on Hidden Markov models (HMMs) enabling the use ofthe weil known algorithms on HMMs (see, e.g„ (Rabiner, 1989)) to guess the most likely syntactic structure  Bäcker and Harbusch  271  of a sentence. We use a trigram model as it has shown to achieve good results in Supertagging (cf. (Srinivas, 1997a)). In this section the basic concepts of Hidden-Markov models are briefly introduced. Thereafter a Hidden Markov model-based Supertagger is portrayed. Finally, aspects ofthe implementation -particularly with respect to time and space efficiency - are highlighted. 3.1. Basic concepts ofHidden Markov models Let us assume the following notational conventions (adapted from (Rabiner and Juang, 1986) and (Charniak, 1993) or see, e.g., (Rabiner, 1989) for a good introduction): • T = length ofthe sequence of observations (training set), • N = number ofstates (either known or guessed), • M = number ofpossible observations (from the training set), • nx = {q1 , „.qN} (finite set ofpossible states), = • flo {vi, „., VM} (finite Set ofpossible Observations), • Xt random variable denoting the state at timet (state variable), • Ot random variable denoting the observation at time t (output variable), • r; = o1 , .„,OT (sequence ofactual observations) and distributional parameters: • A = {%} with aij = Pr(Xt+l = qjJXt = qi) (transitionprobabilities), • B = {bi} with bi(k) = Pr(Ot = vklXt = qit) (observationprobabilities), • 11" = {7ri} with 7l"i = Pr(Xo = qi) (initial state distribution). AHiddenMarkov model (HMM) is a five-tuple Olx , no, A,B, n). Let>.= {A, B, 7r} denote the parameters for a given HMM with fixed n X and n o . This means, a discrete-time, discrete-space dynamical System governed by a Markov chain emits a sequence of observable outputs: one output (observation) for each state in a trajectory of such states. From the observable sequence of outputs, the most likely dynamical system can be inferred. The result is a model for the underlying process. Altematively, given a sequence ofoutputs, the most likely sequence of states can be inferred. The model can also be used to predict the next observation or more generally a continuation of the sequence of observations. Three basic problems can be fonnulated for HMMs: 1. Find Pr(O'\A), i.e. the probability ofthe observations given the model. 2. Find the most Iikely state trajectory given the model and observations. 3. Adjust A={A, B, 7r} to maximize Pr(O'J>.). For any of these questions efficient algorithms are known (see, e.g., (Rabiner, 1989)). The Forward-Backward algorithm (Baum and Eagon, 1967) solves the first problem, problem 2 is yielded by the Viterbi algorithm (Viterbi, 1967) und problem 3 can be dealt with by the Baum- Welch algorithm (Baum, 1972). 3.2. Hidden Markov models for Supertagging Many variants ofSupertagging use models similar to POS- Tagging (cf. Section 5 for a brief description of the variants Trigram Supertagging, Head Supertagging and Transformation- based Supertagging). Here, we underlay the Supertagger with Hidden Markov models (HMMs) . In this framework, the Supertags are encoded as states and the words as symbols of the output alphabet of = the HMM. Assuming a bigram model (i.e, n-Gram with n 2), the realization is easy and straightforward. Any Supertag becomes an individual state and any terminal an individual output symbol. The according Tagger can be trained with the Baum-Welch algorithm. The observation sequence is provided by the sentences of the training  272  Proceedings ofTAG+6  set (unsupervised learning). However, this method Jacks behind supervised learning methods (see, e.g„ (Merialdo, 1994)). Such a corpus can be gained with the Viterbi algorithm (cf. problem 2 mentioned above), as this algorithm denotes the optimal sequence of states for a given observation sequence - in our case the optimal sequence of Supertags. The Supertagger we report on in this paper uses a trigram model (cf. (Bäcker, 200l) for an evaluation of the HMM-based Supertagger using bigrarns). According to the trigram model, two previous states (i.e. Supertags) are encoded in the HMM in a well-known manner (see, e.g., (El-Beze and Merialdo, 1999)):  • The states ofthe HMM: correspond to pairs of Supertags (ti-1, ti).  • The transition probability Pr[(ti-1, ti)l(ti_z, ti-1)] is denoted by the trigram probability Pr(tilti-zti_i).  • The output symbols are provided by the words which are tagged with ti and which are emitted in states (_, ti )- At the beginning ofa sentence pseudo states (0, tj) with 0 a pseudo category are assumed. In general, the Baum-Welch algorithm (cf. problem 3) can be applied to optimize the model parameters in order to maximize Pr(training setl.\). Our results are gained on the basis of a labeled corpus. Hence we don't impose the Baum-Welch algorithm on our Supertagger. On the basis ofthe Iabeled corpus we directly estimate the model pararneters according to the Maximum Likelihood Estimation (MLE) method. For trigrams this means:  = ( ! ) c(tk,tj,ti)  PrMLEtitk,tj  c(tk,tj), l:Si:SN, l:::;j:::;N,  (1)  1::::; j:::; N,  (2)  with: c(tj) c(tj,tk) c(tk, tj, ti) c(wk ,tj)  = nurnber of occurrences oftj in the training set, · nurnber of occurrences oftj followed by tk, = nurnber of occurrences oftk followed by tj, which itself is followed by t„ and number of occurences ofwk labelled as tj.  In order to overcome problems with sparse data, i.e. not all trigrams occur in the training set, smoothing techniques (for a good introduction see, e.g., (Jurafsky and Martin, 2000) or (Manning and Schütze, 2000); in (Chen and Goodman, 1996) the perforrnance of various smoothing techniques are evaluated) are applied in our system. Furtherrnore the treatment ofunknown words is described in the following. In our system we ernploy Good-Turing Discounting (Good, 1953). This means, the equation (1) ofthe :MLE estimation which relies on the absolute frequency c(tk, t i, ti ) ofa trigram is replaced by the foUowing equation:  c*(tk,tj,ti ) PrcT ( ti 1tk,tj) = c(tk, tj ) ,  
 David Roussel EADS Suresnes  1. Introduction  The work presented in this abstract follows the first experiments presented in (Lopez and Roussel, 2000) on the robust modeling of terms in the LTAG framework to index spoken annotation transcriptions. We continue to experiment the LTAG workbench (Lopez, 2000), and integrate it with on the shelftools (tenn extractor, taggers, terminological model) that embed and manage different kind of linguistic resource. Tue key advantages of using the LTAG formalism in this context is a precise linguistic modeling useful for the representation ofterrn variants, the exploitation ofsemantic constraints and the ability to specialize the tenninological resources to several specific tasks. To illustrate the last point, we first present another application that rnotivates this work, the exploitation oftechnical documentation. In this particular application, the semantic disambiguisation can help to improve the accuracy of the docurnents and their reuse to design checking procedures. We then present more precisely the LTAG modeling and the implemented system TERESA based on a POS tagger, finite-state transducers encoding LTAG trees and a semantic tagger. 2. Application  When documentation is an important part of a company activity, there are always sorne existing resources which formalize semantic infonnation available for technical words. For example, currently, in the EADS context, the design ofan ontology that gives the semantic categories ofspecific terms is considered as an important starting point. During the document life cycle within a project, an ontology facilitates also intra-operation between different kind of document and so, a mandatory part ofthe work being done by the project community is to standardize the terms, acronyms and abbreviations. This task is an EADS directive and procedure. Since these terms are already defined, their identification für the purpose of classifying and accessing doc- uments is called controlled indexation in opposition to free indexation where the index terms are automatically defined. Controlled indexation allows us to exploit existing resource to achieve a better precision in the indexation and to link old and new information in a more coherent and comprehensive way for documentalists. The experiments in this work have been made with XML elements called WARNINGS extracted from an aircraft docurnentation. Tue correct identification of a particular term and its variants The use ofcontrolled indexing on these elements is twofold: first, help the navigation into those elements in order ot control the coherence ofthe content ofthese element, second, tobe able to disambiguate semantically sequence ofwords. An expected enhancement of robust controlled indexing is to derive more easily a procedure from the description ofthe waming in the whole documentation, or at least to take more easily into account the important waming in the procedures. Tue identification ofa particular operation benefit from a disambiguation ofcertain sequence of words. For instance, engine operation concem the motor intervention (table 1) or the system intervention (table 1). To avoid engine damaged, it is sometime necessary to access both the cockpit and the motor. One interest ofsemantic knowledge exploitation in controlled indexing is to extract directly the sentences that concern one type ofengine intervention. You must not operate the engine with the fan cowl doors open. During engine operation, the bleed valve can open. Operation of the engine can cause fuel to go overboard. Ear protection must be worn by all person who operate the engine while engine operates. Figure 1: Example of motor intervention. © 2002 Lopez & Roussel. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and Related Frame- works (TAG+6), pp. 279- 284. Universita di Venezia.  280  Proceedings ofTAG+6  The engine must operate 9 hours at idle with the lubrication system filled. Do not motor, start or operate engine unless a positive fuel inlet pressure is indicated. The exhaust gas is hot when the engines operates at idle or higher power. To maintain satisfactory engine oil temperature do an engine start and operate the engine at idle. Figure 2: Example of engine intervention that need a control from the cockpit. In the first case (motor intervention, table 1), the operation implies the filtering of sentences that gather a person as an agent or implicit agent. A syntactic analysis is enough to disambiguate this case from the next case (table .2), that implies that the engine is the subject of the operation. However, different elementary trees are concem, and don't provide an easy interface to the integration of the syntactic analysis of the terms withln an application. To consider a unique semantic feature instead ofdifferent kind of derived tree, we extend the syntactic categories ofelementary trees with semantic constraints and compile them as FST. This allows us to keep abstraction in the description oflinguistic resource and to benefit from other linguitic tool, namely the semantic tagger Tropes in order to study the dependance between semantic classes in a corpora. Tropes is a semantic analyser (Ghiglione et al., 1998). lt embed morphosyntatic and semantic analyser that i) segment a text in linguistic proposition, ii) extract homogeneous category according to their thematic content, iii) export the result in a XML coding, iv) to count the frequence and the dependency between semantic classes. The Tropes environment facilitate the adap- tation of the default semantic classes hierarchy in order to take into account specific semantic knowledge. A set of heuristics are applied to disambiguate the semenatic categorie of a lexical unit. They consist in :finding isotopies ofa same semantic classe and exploiting statistical coocurrences between complementary concepts inside a grammatical proposition. 3. LTAG-based Terminological Processing  3.1. LTAG representation of a terminology  A given tenn can be represented as a partial parsing tree, i.e. a derived tree, as represented figure 3. After removing all lexical information in this tree, we obtain a elementary tree schema in the sense of(Candito, 1996) that can be used to represent syntactically similar terms.  N ｾ＠ N N* 
We explore how machine learning can be employed to learn rulesets for the traditional modules of content planning and surface realization. Our approach takes advantage of semantically annotated corpora to induce preferences for content planning and constraints on realizations of these plans. We applied this methodology to an annotated corpus of indicative summaries to derive constraint rules that can assist in generating summaries for new, unseen material. 
The parsing community has long recognized the importance of lexicalized models of syntax. By contrast, these models do not appear to have had an impact on the statistical NLG community. To prove their importance in NLG, we show that a lexicalized model of syntax improves the performance of a statistical text compression system, and show results that suggest it would also improve the performances of an MT application and a pure natural language generation system. 
This paper describes a general-purpose sentence generation system that can achieve both broad scale coverage and high quality while aiming to be suitable for a variety of generation tasks. We measure the coverage and correctness empirically using a section of the Penn Treebank corpus as a test set. We also describe novel features that help make the generator ﬂexible and easier to use for a variety of tasks. To our knowledge, this is the ﬁrst empirical measurement of coverage reported in the literature, and the highest reported measurements of correctness. 
In this paper we describe two parallel experiments on the integration of machine learning (ML) methods into the Spanish and Japanese rule-based sentence realization modules developed at Microsoft Research. The paper explores the use of decision trees (DT) for the lexical selection of the copula in Spanish and the insertion of a locative postposition in Japanese. We show that it is possible to machine-learn the contexts for these two non-trivial linguistic phenomena with high accuracy. 
We present an overview of Amalgam, a sentence realization module that combines machine-learned and knowledgeengineered components to produce natural language sentences from logical form inputs. We describe the decomposition of the task of sentence realization into a linguistically informed series of steps, with particular attention to the linguistic issues that arise in German. We report on the evaluation of component steps and of the overall system. 
This paper describes an efﬁcient sentencerealization algorithm that is complete for a very general class of uniﬁcation grammars. Under fairly modest constraints on the grammar, the algorithm is shown to have polynomial time complexity for generation of sentences whose logical form exactly matches the goal logical form. The algorithm can be extended to handle what is arguably the most important subcase of the logical-form equivalence problem, permutation of logical conjunction. With this extension the algorithm is no longer polynomial, but it seems to be about as efﬁcient as the nature of the problem permits. 
In spoken language applications such as conversation systems where not only the speech waveforms but also the content of the speech (the text) need to be generated automatically, a Concept-to-Speech (CTS) system is needed. In this paper, we address several issues on designing a speech corpus to facilitate an instance-based integrated CTS framework. Both the instance-based CTS generation approach and the corpus design process have not been addressed systematically in previous researches. 
We present a framework for handling emotional variations in a speech-based natural language generation system for use in the MRE virtual training environment. The system is a first step toward addressing issues in emotion-based modeling of verbal communicative behavior. We cast the problem of emotion-based generation as a distance minimization task, in which the system chooses between multiple valid realizations for a given input based on the emotional distance of each realization from the speaker’s attitude toward that input. We discuss evaluations of the system and future work that includes modeling personality and empathy within the same framework. 1. Introduction: the MRE Emotion is an ever-present characteristic of human experience and behavior. As fundamental to the human condition as cognition, emotion has begun to pique the interest of those researchers in the Artificial Intelligence community concerned with simulating human behavior in embodied agents. Nowhere is this interest more prominent than in the domain of multi-modal, virtual training environments. In such environments, realistic modeling of emotion enhances the user’s ability to suspend disbelief (Marsella & Gratch, 2001), and can be used as an additional parameter in creating more variable training scenarios. The emotional NLG system that we present is designed within the Mission Rehearsal Exercise (MRE) virtual training environment (Swartout et  al. 2001). The MRE is a large-scale collaborative research effort to develop a fully interactive training simulation modeled after the holodeck in Star Trek. The project brings together researchers working on graphics, 3-D audio, artificial intelligence, and Hollywood screenwriters to create a realistic virtual world in which human subjects can interact naturally with simulated agents. The agents are modeled using the Steve system of Rickel and Johnson (1999). They communicate through voice and gesture, reason about tasks and actions, and incorporate a complex model of their own emotions, as well as the emotional states of the other agents in their environment (Gratch & Marsella, 2001; Gratch, 2000). Users can query and interact with one (and eventually many) agent in real-time as they proceed through a scenario developed for the particular training mission at hand. The scenario presently implemented is designed to train army lieutenants for eastern European peace keeping missions. The scenario centers around the trainee, a human lieutenant, who is attempting to move his platoon to a support position, when one of his drivers unexpectedly collides with a civilian car. A civilian passenger, a young boy, is critically injured and the boy’s mother, as well as a crowd of local onlookers, is becoming increasingly agitated. The trainee must interact with his or her virtual platoon sergeant in order to stabilize the situation. The MRE represents the integration of many fields in NLP. As the trainee interacts with the virtual agents in the environment, automatic speech recognition translates the user’s speech into a text string that is passed to the natural  language understanding module. This module uses a finite state machine to convert the string into a case frame structure that is passed to a dialogue manager. At this point, the dialogue manager interacts with the task planner, the action selector, and the emotion model to initiate a particular response. The content of this response is then passed as an impoverished case frame to the NLG system. Generation converts the input into a tree structure that contains both syntactic and semantic information. The tree is then passed to a gesture module and is tagged with non-verbal information to control gaze and body movements. Finally, the tree is flattened, the gestures and visemes are synched using the BEAT system (Cassell, 2001), and the speech is synthesized. 2. Previous Work While much attention has been paid to the effect of emotion on planning and non-verbal behavior (Marsella et al., 2001; Cahn, 1990), little work has been done on the effects of emotion on the verbal behavior of embodied agents. Most previous work focuses on intonation and nonverbal communication. With respect to content and phrasing, the most relevant work is over 10 years old. In his thesis, Hovy (1988) implemented a 3-valued (positive, negative, neutral) system of emotional shades with a simple sign multiplication calculus to control affect laden text generation. The three values provided little flexibility to accommodate the more subtle nuances associated with different shades of affect. Work by Bateman and Paris (1989) and Paris (1988) focus on variations of expert system output based on the reader’s knowledge. Also here, the rules for combining ratings of sentence constituents was fairly simple and not easily extensible. Papers by Walker et al (1996) and Loyall and Bates (1997) explore aspects of style and emotion, but do not focus on the particulars of natural language generation. In this paper we describe an integrated framework for modeling emotion in the speechbased natural language generation of embodied agents. It incorporates a distance calculus that adds flexibility and allows us to extend the  emotional input from simple like/dislike to more complicated constructs. 3. NLG in MRE Generation in the MRE is a hybrid process. The generator can take as input both highly elaborated case frames, for scenario specific utterances, and more impoverished frames, for use in interactive conversation. We discuss only the conversational aspect of the system. The generator is, at this point, highly domain dependent, but has sufficient coverage to generate utterances for every task in the agents’ task models. The generator is implemented in the SOAR programming language (Newell, 1990) and takes place in three stages: sentence planning, realization and ranking. 3.1 Sentence Planning As seen in Figure 1(a), the inputs to this stage are received from the dialogue manager. These inputs contain minimal information about the state or event to be described, along with references to the actors and objects involved. A set of SOAR production rules converts this information into an enriched case frame structure, seen in Figure 1(b), which contains more detailed information about the events and objects in the input. The conversion process, which involves choosing the appropriate object case frames, relies heavily on the emotional decision engine. 3.2 Realization Realization is a highly lexicalized procedure, and tree construction begins with the selection of main verbs (more on this below). Each verb in the lexicon carries with it slots for its constituents (e.g., agent, patient), which form branches in the tree. Once the verb is chosen, production rules recursively expand the nodes in the tree until no more nodes can be expanded. As each production rule fires, the relevant portion of the semantic frame is propagated down into the expanded nodes. Thus, every node in the tree contains a pointer to the specific aspect of the semantic frame from  ^event collision ^time past ^speech-act assert ^agent driver ^patient mother Figure 1a. Input from dialogue manager: input to sentence planning phase of generation (<utterance> ^type assertion ^content <event>) (<event> ^type event ^time past ^name collision ^agent <agent> ^patient <patient>) (<agent> ^type agent ^name driver ^definite true ^singular true) (<patient> ^type patient ^name mother ^definite true ^singular true) Figure 1b. Expansion of input from dialogue manager; output of sentence planning which it was created. For example, in Figure 1(c), the NP node of “the mother” contains in it a pointer to the frame <patient> from Figure 1(b). By keeping semantic content localized in the tree, we allow the gesture and speech synthesis modules convenient access to needed semantic information. This strategy is particularly convenient in a setting such as the MRE, where modules require increasing amounts of information as research continues. For any given state and event, there are a number of theoretically valid realizations available in the lexicon. Instead of attempting to decide which is most appropriate at any stage, we adopt a strategy similar to that introduced by (Knight & Hatzivassiloglou, 1995), which puts off the decision until realization is complete. We realize all possible valid trees that correspond to a given semantic input, and store the fully constructed trees in a forest structure. After all such trees are constructed we move on to the final stage. 3.3 Ranking  In this stage we examine all the trees in the forest structure and decide which tree will be propagated further down the NLP pipeline. Each tree is given a rank score based upon the tree’s information content and emotional quality. The score of each tree is calculated by recursively summing the scores of the nodes along the frontiers of the tree, and then percolating that sum up to the next layer. Summing and percolating proceeds until the root node is given a score that is equivalent to the sum of the scores for the individual nodes of that tree. The tree with the highest root node score is selected. 4. Emotional Variations We cast the problem of emotional language generation as an optimization problem in which multiple acceptable realizations of a given semantic frame are produced. Given a set of valid realizations for a given frame, we output the sentence that most closely fits the emotional state of the speaker. 4.1 Speaker’s Emotions The emotion model employed by the MRE is based largely on various appraisal theories of emotion (Ortony, Clore, and Collins, 1988; Lazarus, 1991). Such models use the term appraisal to refer to the emotional evaluation of events. The MRE concretizes this notion of evaluation in terms of data structures, called construal frames1, which represent relations between events and the dispositions of agents. In the MRE, dispositions are defined entirely in terms of an agent’s plans and goals (Gratch, 2000). Thus, an agent’s emotional state is predicated entirely on that agent’s appraisal of an event in terms of how that event relates to its own set of goals, and plans toward those goals. Conversely, the model allows us to describe events in the world in terms of their relationship to an agent’s dispositions. Thus, each object and event in the agents’ world model can be described as a vector of features relating that event to the agent’s goals and plans. The features that describe these elements of the world model are relations such as whether the 
Developing an embodied conversational agent that is able to exhibit a human-like behavior while communicating with other virtual or human agents requires enriching a typical NLG architecture. The purpose of this paper is to describe our efforts in this direction and to illustrate our approach to the generation of an Agent that shows a personality, a social intelligence and is able to react emotionally to events occurring in the environment, consistently with her goals and with the context in which the conversation takes place. 
Recent work on evaluation of spoken dialogue systems indicates that better algorithms are needed for the presentation of complex information in speech. Current dialogue systems often rely on presenting sets of options and their attributes sequentially. This places a large memory burden on users, who have to remember complex trade-offs between multiple options and their attributes. To address these problems we build on previous work using multiattribute decision theory to devise speech-planning algorithms that present usertailored summaries, comparisons and recommendations that allow users to focus on critical differences between options and their attributes. We discuss the differences between speech and text planning that result from the particular demands of the speech situation.  process may be quite onerous, leading to reduced user satisfaction. SYS: I found 9 round trips from Hartford to Orlando on October 31st and returning Hartford on November 6th. For option 1, I have an outbound ﬂight with U S Air departing at 7 AM and arriving at 9 49 AM non-stop. A return ﬂight is with U S Air departing at 10 30 AM and arriving at 12 28 AM with 1 stopover. Ticket price is 165 dollars. Please say next option or ﬂight details or I’ll take it. USER: NEXT OPTION SYS: For option 2, I have an outbound ﬂight with U S Air departing at 3 30 PM and arriving at 6 19 PM nonstop. A return ﬂight is with U S Air departing at 8 25 PM and arriving at 12 28 AM with 1 stopover. Ticket price is 165 dollars. Say next option, or, ﬂight details, or, I’ll take it. USER: NEXT OPTION ....... Dialogue continues until user selects option Figure 1: Information presentation phase of a Communicator dialogue  
This paper explores the feasibility of implementing an evolutionary algorithm for text structuring using the heuristic of continuity as a ﬁtness function, chosen over other more complicated metrics of text coherence. Using MCGONAGALL (Manurung et al., 2000) as our experimental platform, we show that by employing an elitist strategy for stochastic search it is possible to quickly reach the global optimum of minimal violations of continuity. 
In this paper, we present a novel technique to learn a tree-like structure for a content planner from an aligned corpus of semantic inputs and corresponding, humanproduced, outputs. We apply a stochastic search mechanism with a two-level ﬁtness function. As a ﬁrst stage, we use high level order constraints to quickly discard unpromising planners. As a second stage, alignments between regenerated text and human output are employed. We evaluate our approach by using the existing symbolic planner in our system as a gold standard, obtaining a 66% improvement over a random baseline in just 20 generations of genetic search. 
Aggregation is typically treated in NLG as a local optimization measure, and methods exist only for building conjoined expressions with 'and'. In contrast to that, solutions to logical problems are characterized by regularly occurring commonalities, including complete subsets of possible value combinations and alternatives. In order to address constellations of this kind, we extend current aggregation techniques, envisioning high degrees of condensation. In particular, we define novel constructs that can express sets of propositions with highly regular variations on slot values concisely, including special forms of disjunctions. Our methods enable the generation of expressions with semantically complex operators, such as 'vice-versa' and 'each', and they support various aspects in interpreting solutions produced by formal systems, such as highlighting commonalities among and differences across solution parts, supporting the inspection of dependencies and variations, and the discovery of flaws in problem specifications. 
This paper argues that algorithms for the generation of referring expressions should aim to make it easy for hearers and readers to ﬁnd the referent of the expressions that are generated. To illustrate this claim, an algorithm is described for the generation of expressions that refer across a hierarchically ordered domain, and which takes search effort into account by adding logically redundant information. To support the ideas underlying the algorithm, a psycholinguistic experiment is presented that conﬁrms readers’ preference for the generated, logically redundant expressions over non-redundant alternatives. 
We added a sentence planning component to an existing ITS that teaches students how to troubleshoot mechanical systems. We evaluated the original version of the system and the enhanced one via a user study in which we collected performance, learning and usability metrics. We show that on the whole the enhanced system is better than the original one. We discuss how to use the binomial cumulative distribution to assess cumulative effects. 
This paper presents an evaluation of the instructional text generated by Isolde, an authoring tool for technical writers that automates the production of procedural on-line help. The evaluation compares the effectiveness of the instructional text produced by Isolde with that of professionally authored instructions, such as MS Word Help. The results suggest that the documentation produced by Isolde is of comparable quality to similar texts found in commercial manuals. 
We present a dialogue generation model, implemented in the COMIX prototype information system, which uses a Constraint-Based Problem-Solver (CBPS) to support cooperative mixed-initiative information-seeking dialogue. Use of the CBPS enables a dialogue system to 1) incrementally interleave query construction with solution construction 2) immediately detect under-constrained and overconstrained information requests, and 3) provide cooperative responses when these types of problems are detected. We also present a system evaluation investigating how well COMIX handles dialogues with over-constrained requests. 
Giving an adequate general deﬁnition of the input to natural language generation (NLG), and hence to NLG itself, is a notoriously difﬁcult problem, practically, theoretically and even methodologically. In this paper, we describe our recent experiences of implementing an NLG component of a larger question-answering system, and trying to understand and resolve some of these problems in this context. We examine the whole lifetime of an answer, from internal data structure to ﬁnal expression as text, and look for characteristics of the processing which might help identify where NLG really begins. On the basis of this analysis we propose some principles to inform discussions on the scope of NLG as an individuated enterprise. 
Previous research has shown that certain discourse conditions are necessary for the felicitous use of non-canonical syntactic forms like topicalization, left-dislocation, and clefts. However, the distribution of these forms does not correlate one-to-one with the presence of these conditions, and a system that generates these statisticallyrare forms based only on these conditions will overgenerate. Instead, a generation algorithm must be based on additional communicative goals that can be achieved through the use of these forms. Based on a corpus study, I present three types of communicative goals that speakers achieve through the use of non-canonical syntax. 
We describe some of the complications involved in expressing the technique of induction when automatically generating textual versions of formal mathematical proofs produced by a theorem proving system, and describe our approach to this problem. The pervasiveness of induction within mathematical proofs makes its effective generation vital to readable proof texts. Our focus is on planning texts involving induction. Our efforts are driven by a corpus of human-produced proof texts, incorporating both regularities within this corpus and the formal structure of induction into coherent text plans. 
In this paper we present an architecture for generating texts that vary in the emphasis put on conciseness, readability and the marking of particularly salient items. We abandon the traditional pipeline architecture, and use an integrated approach which makes the search for an optimum text explicit, taking into account both inter-sentential and intra-sentential features. We describe a context sensitive scoring system which can relate surface properties to a deeper representational level. We show how this approach can be used in generating paragraph length texts, optimised against various criteria. 
This paper proposes a series of techniques for extracting English verb–particle constructions from raw text corpora. We initially propose three basic methods, based on tagger output, chunker output and a chunk grammar, respectively, with the chunk grammar method optionally combining with an attachment resolution module to determine the syntactic structure of verb–preposition pairs in ambiguous constructs. We then combine the three methods together into a single classiﬁer, and add in a number of extra lexical and frequentistic features, producing a ﬁnal F-score of 0.865 over the WSJ. 
This paper presents a method for bootstrapping a ﬁne-grained, broad-coverage part-of-speech (POS) tagger in a new language using only one personday of data acquisition effort. It requires only three resources, which are currently readily available in 60-100 world languages: (1) an online or hard-copy pocket-sized bilingual dictionary, (2) a basic library reference grammar, and (3) access to an existing monolingual text corpus in the language. The algorithm begins by inducing initial lexical POS distributions from English translations in a bilingual dictionary without POS tags. It handles irregular, regular and semi-regular morphology through a robust generative model using weighted Levenshtein alignments. Unsupervised induction of grammatical gender is performed via global modeling of contextwindow feature agreement. Using a combination of these and other evidence sources, interactive training of context and lexical prior models are accomplished for ﬁne-grained POS tag spaces. Experiments show high accuracy, ﬁne-grained tag resolution with minimal new human effort. 
This paper investigates the use of a language independent model for named entity recognition based on iterative learning in a co-training fashion, using word-internal and contextual information as independent evidence sources. Its bootstrapping process begins with only seed entities and seed contexts extracted from the provided annotated corpus. F-measure exceeds 77 in Spanish and 72 in Dutch. 1. Introduction Our aim has been to build a maximally languageindependent system for named-entity recognition using minimal supervision or knowledge of the source language. The core model utilized, extended and evaluated here is based on Cucerzan and Yarowsky (1999). It assumes that only an entity exemplar list is provided as a bootstrapping seed set. For the particular task of CoNLL-2002, the seed entities are extracted from the provided annotated corpus. As a consequence, the seed examples may be ambiguous and the system must therefore handle seeds with probability distribution over entity classes rather than unambiguous seeds. Another consequence is that this approach of extracting only the entity seeds from the annotated text does not use the full potential of the training data, ignoring contextual information. For example, Bosnia appears labeled 9 times as LOC and 5 times as ORG and the only information that would be used is that the word Bosnia denotes a location 64% of the time, and an organization 36% of the time, but not in which contexts is labeled one way or the other. In order to correct this problem, an improved system also uses context seeds if available (for this particular task, they are extracted from the annotated corpus). Because the representations of entity candidates and contexts are identical, this modiﬁcation imposes only minor changes in algorithm and code. Because the core model has been presented in detail in Cucerzan and Yarowsky (1999), this paper  focuses primarily on the modiﬁcations of the algorithm and its adaptation to the current task. The major modiﬁcations besides the seed handling include a different method of smoothing the distributions along the paths in the tries, a new ’soft’ discourse segmentation method, and use of a different labeling methodology, as required by the current task i.e. no overlapping entities are allowed (for example, the correct labeling of colegio San Juan Bosco de Mérida is considered to be ORG(colegio San Juan Bosco) de LOC(Mérida) rather than ORG(colegio PER(San Juan Bosco) de LOC(Mérida))). 2. Entity-Internal Information Two types of entity-internal evidence are used in a uniﬁed framework. The ﬁrst consists of the preﬁxes and sufﬁxes of candidate entities. For example, in Spanish, names ending in -ez (e.g. Alvarez and Gutiérrez) are often surnames; names ending in -ia are often locations (e.g. Austria, Australia, and Italia). Likewise, common beginnings and endings of multiword entities (e.g. Asociación de la Prensa de Madrid and Asociación para el Desarrollo Rural Jerez-Sierra Suroeste, which are both organizations) are good indicators for entity type. 3. Contextual Information An entity’s left and right context provides an essentially independent evidence source for model bootstrapping. This information is also important for entities that do not have a previously seen word structure, are of foreign origin, or polysemous. Rather than using word bigrams or trigrams, the system handles the context in the same way it handles the entities, allowing for variable-length contexts. The advantages of this uniﬁed approach are presented in the next paragraph. 4. A Uniﬁed Structure for both Internal and Contextual Information Character-based tries provide an effective, efﬁcient and ﬂexible data structure for storing both contextual and morphological patterns and statistics.  ... organizada por la Concejalía de Cultura , tienen un ...  LEFT CONTEXT PREFIX  SUFFIX  RIGHT CONTEXT  Figure 1: An example of entity candidate and context and the way the information is introduced in the four tries (arrows indicate the direction letters are considered) They are very compact representations and support a natural hierarchical smoothing procedure for distributional class statistics. In our implementation, each terminal or branching node contains a probability distribution which encodes the conditional probability of entity classes given the sistring corresponding to the path from the root to that node. Each such distribution also has two standard classes, named “questionable” (unassigned probability mass in terms of entity classes, to be motivated below) and “non-entity” (common words). Two tries (denoted PT and ST) are used for internal representation of the entity candidates in preﬁx, respectively sufﬁx form, respectively. Other two tries are used for left (LCT) and right (RCT) context. Right contexts are introduced in RCT by considering their component letters from left to right, left contexts are introduced in LCT using the reversed order of letters, from right to left (Figure 1). In this way, the system handles variable length contexts and it attempts to match in each instance the longest known context (as longer contexts are more reliable than short contexts, and also the longer context statistics incorporate the shorter context statistics through smoothing along the paths in the tries). The tries are linked together into two bipartite structures, PT with LCT, and ST with RCT, by attaching to each node a list of links to the entity candidates or contexts with, respectively in which the sistring corresponding to that node has been seen in the text (Figure 2). 5. Unassigned Probability Mass When faced with a highly skewed observed class distribution for which there is little conﬁdence due to small sample size, a typical response is to backoff or smooth to the more general class distribution. Unfortunately, this representation makes problematic the distinction between a back-off conditional distribution and one based on a large sample (and hence estimated with conﬁdence). We address this problem by explicitly representing the uncertainty as a class, called "questionable". Probability mass continues to be distributed among the primary entity classes proportional to the observed distribution in the data, but with a total sum that reﬂects  ST  RCT  A ... a b ...  z  ,  ... a  ...  i  ... ...  #  # ...  H hp  C  r  t  oi a  h  ... s  l  zr i  u  a  o tr  A  n#  ia  #  d  ...  ... c  ...  a ...  #  ... #  ...  Figure 2: An example of links between the Sufﬁx Trie and the Right Context Trie for the entity candidate Austria and some of its right contexts as observed in the corpus (< , Holanda >, < , hizo >, < a Chirac >)  t¢he¡¤c£¦o¥¨n§ﬁ©den ce!#"%in$ the distribution and is equal to . Incremental learning essentially becomes the process of gradually shifting probability mass from questionable to one of the primary classes.  6. Smoothing  The probability of an entity candidate or context as  being or indicating a certain type of entity is com-  puted along the path from the root to the node in  the trie structure described above. In this way, ef-  fective smoothing can be realized for rare entities  or contexts. A smoothing formula taking advantage  of the distributional representation of uncertainty is  presented below.  34F4 "o¡r  a &'  s¡ is&t(ri¡ ng01010&¡'%&)(& ¥ 0101)02&  ¥ (i.e. the the general  path in the smoothing  trie is model  for the conditional class probabilities is given by the  recursive formula:  £6R5 £¦79¥8A§G@©H&¨'%&)(¨01 0102S&CB!S$E"S@D &  '  &  F' (  010102£¦& B ¥$UT§G©H£65¨797 8I8 @@&& 'H' &P& (( 010101010202&C& B  $EQ BWVX'  $`Y  (1)  wc dIhe egrf ee¦hpa i is a normalization factor and R b are model parameters.  7. One Sense per Discourse Clearly, in many cases, the context for only one instance of an entity and the word-internal information is not enough to make a classiﬁcation decision. But, as noted by Katz (1996), a newly introduced entity will be repeated, “if not for breaking the monotonous effect of pronoun use, then for  Entity candidate w  is_B_candidate  Positional similarity  qqrrqqrrqqrrqqrrqqsstt sstt sstt sstt sstt sstt sstt sstt  uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu vv uu  Other occurences wwxx of w  wwxx  wwxx  wwxx  Topic boundary Topic boundary Soft boundary Soft boundary Topic boundary  Topic boundary  Topic boundary  Word position in the corpus  Figure 3: Using contextual clues from all instances of an en- tity candidate in the corpus. Each instance is depicted as a disc with the diameter representing the conﬁdence of the classiﬁcation of that instance using word-internal and local contextual information.  emphasis and clarity”. We use this property in con-  junction with the one sense per discourse tendency  noted by Gale et al. (1992). The later paradigm is  not directly usable when analyzing a large corpus  in which there are no document boundaries, like the  one provided for Spanish. Therefore, a segmenta-  tion process needs to be employed, so that all the  instances of a name in a segment have a high proba-  bility of belonging to the same class. Our approach  is to consider a ’soft’ segmentation, which is word-  dependent and does not compute topic/document  boundaries but regions for which the contextual in-  formation for all instances of a word can be used  jointly when making a decision. This is viewed  as an alternative to the classical topic segmenta-  tion approach and can be used in conjunction with a  language-independent segmentation system (Figure  3) like the one presented by Richmond et al. (1997).  After estimating the class probability distribu-  tions for all instances of entity candidates in the cor-  pus, a re-estimation step is employed. The probabil-  ity of an entity class y9 given an entity candidate   at position  # is re-computed using the formula:   sUt  y9  # # k eur#yvfhd gjwyx ikm l pdo6n  y9k  pq  k  Hr  (2)  where  stances   of  d  mz1z1z1 in   i the  are the positions corpus, Ss`t is the  of all inpositional  similarity, encoding the physical distance and topic  (if topic or document boundary information exists),  conf is the classiﬁcation conﬁden6ce{o|f  ( inverse proportional to the is a normalization factor.  the  n  e} a#c~#h pinsta nk c e,  8. Entity Identiﬁcation / Multiple-Word Entities There are two major alternatives for handling multiple-word entities. A ﬁrst approach is to tokenize the text and classify each individual word as being or not part of an entity, process followed by an entity assemblance algorithm. A second alternative  is_I_candidate is_E_candidate Figure 4: The structure of an entity candidate represented as an automaton with two ﬁnal states is to consider a chunking algorithm that identiﬁes entity candidates and classify each of the chunks as Person, Location, Organization, Miscellaneous, or Non-entity. We use this second alternative, but in a ’soft’ form; i.e. each word can be included in multiple competing chunks (entity candidates). This approach is suitable for all languages including Chinese, where no word separators are used (the entity candidates are determined by specifying starting and ending character positions). Another advantage of this method is that single and multiple-word entities can be handled in the same way. The boundaries of entity candidates are determined by a few simple rules incorporated into three discriminators: is_B_candidate tests if a word can represent the beginning of an entity, is_I_candidate tests if a word can be the end of an entity, and is_E_candidate tests if a word can be an internal part of an entity. These discriminators use simple heuristics based on capitalization, position in sentence, length of the word, usage of the word in the set of seed entities, and co-occurrence with uncapitalized instances of the same word. A string is considered an entity candidate if it has the structure shown in Figure 4. An extension of the system also makes use of Part-of-Speech (POS) tags. We used the provided POS annotation in Dutch (Daelemans et al., 1996) and a minimally supervised tagger (Yarowsky and Cucerzan, 2002) for Spanish to restrict the space of words accepted by the discriminators (e.g. is_B_candidate rejects prepositions, conjunctions, pronouns, adverbs, and those determiners that are the ﬁrst word in the sentence). 9. Algorithm Structure The core algorithm can be divided into eight stages, which are summarized in Figure 5. The bootstrapping stage (5) uses the initial or current entity assignments to estimate the class conditional distributions for both entities and contexts along their trie paths, and then re-estimates the distributions of the contexts/entity-candidates to which they are linked, recursively, until all accessible nodes are reached, as presented in Cucerzan and Yarowsky (1999).  
Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. This work tests their claim by exploring whether a very large corpus can eliminate the sparseness problems associated with estimating unigram probabilities. We do this by empirically investigating the convergence behaviour of unigram probability estimates on a one billion word corpus. When using one billion words, as expected, we do ﬁnd that many of our estimates do converge to their eventual value. However, we also ﬁnd that for some words, no such convergence occurs. This leads us to conclude that simply relying upon large corpora is not in itself sufﬁcient: we must pay attention to the statistical modelling as well. 
We present a method for identifying corresponding themes across several corpora that are focused on related, but distinct, domains. This task is approached through simultaneous clustering of keyword sets extracted from the analyzed corpora. Our algorithm extends the informationbottleneck soft clustering method for a suitable setting consisting of several datasets. Experimentation with topical corpora reveals similar aspects of three distinct religions. The evaluation is by way of comparison to clusters constructed manually by an expert. 
We address the problem of using partially labelled data, eg large collections were only little data is annotated, for extracting biological entities. Our approach relies on a combination of probabilistic models, which we use to model the generation of entities and their context, and kernel machines, which implement powerful categorisers based on a similarity measure and some labelled data. This combination takes the form of the so-called Fisher kernels which implement a similarity based on an underlying probabilistic model. Such kernels are compared with transductive inference, an alternative approach to combining labelled and unlabelled data, again coupled with Support Vector Machines. Experiments are performed on a database of abstracts extracted from Medline. 
This paper presents the ongoing project Computational Models of First Language Acquisition, together with its current product, the learning algorithm GraSp. GraSp is designed specifically for inducing grammars from large, unlabelled corpora of spontaneous (i.e. unscripted) speech. The learning algorithm does not assume a predefined grammatical taxonomy; rather the determination of categories and their relations is considered as part of the learning task. While GraSp learning can be used for a range of practical tasks, the long-term goal of the project is to contribute to the debate of innate linguistic knowledge – under the hypothesis that there is no such.  greatly reduced given a structure of primitive linguistic constraints ("a highly restrictive schematism", ibid.). It has however been very hard to establish independently the psychological reality of such a structure, and the question of innateness is still far from settled. While a decisive experiment may never be conceived, the issue could be addressed indirectly, e.g. by asking: Are innate principles and parameters necessary preconditions for grammar acquisition? Or rephrased in the spirit of constructive logic: Can a learning algorithm be devised that learns what the infant learns without incorporating specific linguistic axioms? The presentation of such an algorithm would certainly undermine arguments referring to the 'poverty of the stimulus', showing the innateness hypothesis to be dispensable. This paper presents our first try.  Introduction Most current models of grammar learning assume a set of primitive linguistic categories and constraints, the learning process being modelled as category filling and rule instantiation – rather than category formation and rule creation. Arguably, distributing linguistic data over predefined categories and templates does not qualify as grammar 'learning' in the strictest sense, but is better described as 'adjustment' or 'adaptation'. Indeed, Chomsky, the prime advocate of the hypothesis of innate linguistic principles, has claimed that "in certain fundamental respects we do not really learn language" (Chomsky 1980: 134). As Chomsky points out, the complexity of the learning task is  
In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efﬁcient or scalable. We propose a new method that is simple and efﬁcient, since it parses a sentence deterministically only deciding whether the current segment modiﬁes the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efﬁciency. 
Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to ﬁelds as diverse as computer vision and econometrics, and which is used for a wide variety of classiﬁcation problems in natural language processing. However, the ﬂexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices. 
The regularity of named entities is used to learn names and to extract named entities. Having only a few name elements and a set of patterns the algorithm learns new names and its elements. A verification step assures quality using a large background corpus. Further improvement is reached through classifying the newly learnt elements on character level. Moreover, unsupervised rule learning is discussed. 
 1.6 Clusters in "earn" Other Categories 1.4  1.2  KL-divergence  
This paper examines feature selection for log linear models over rich constraint-based grammar (HPSG) representations by building decision trees over features in corresponding probabilistic context free grammars (PCFGs). We show that single decision trees do not make optimal use of the available information; constructed ensembles of decision trees based on different feature subspaces show signiﬁcant performance gains (14% parse selection error reduction). We compare the performance of the learned PCFG grammars and log linear models over the same features. 
Words diﬀer in the subcategorisation frames in which they occur, and there is a strong correlation between the semantic arguments of a given word and its subcategorisation frame, so that all its arguments should be included in its subcategorisation frame. One problem is posed by the ambiguity between locative prepositional phrases as arguments of a verb or adjuncts. As the semantics for the verb is the same in both cases, it is diﬃcult to diﬀerentiate them, and to learn the appropriate subcategorisation frame. We propose an approach that uses semantically motivated preposition selection and frequency information to determine if a locative PP is an argument or an adjunct. In order to test this approach, we perform an experiment using a computational learning system that receives as input utterances annotated with logical forms. The results obtained indicate that the learner successfully distinguishes between arguments (obligatory and optional) and adjuncts. 
This paper presents the user-centered interface of a summarization system for physicians in Bone Marrow Transplantation (BMT). It serves both retrieval and summarization, eliciting the query and presenting multi-document summaries in a situation-specific organization. Introduction: User-centered scenario forms for retrieval and summarization This paper presents the user interface of a summarization system for physicians in Bone Marrow Transplantation (BMT). The interface has users state well-articulated questions that serve as summarization targets and basis for retrieval queries, and it displays summarization results in an organization that fits the user’s situation. Although user interfaces have attracted almost no interest in summarization research so far, we think that a suitable useroriented interface is important for a summarization system. This paper deals with such an interface, not with the summarization procedures the interface enables. In good user-centered design attitude (Norman and Draper, 1986), we developed the user interface first, and are still equipping it component by component with the intended functionality. Our users are highly specialized physicians in Bone Marrow Transplantation (BMT), a lifecritical field of internal medicine. They need answers to their questions that are fast, to the point, and prepared for direct application. Using question/answer scenario forms derived from empirical scenario descriptions, they can specify their current situation and the missing knowledge items with the help of a domain-  specific ontology. The system accepts the filled out question scenario, projects it to a query for search engines and Medline (the most common medical reference retrieval engine), and starts the search. Retrieved documents are downloaded, preprocessed, and then checked for passages where question terms accumulate. These passages are examined by summarization agents that follow strategies of human summarizers (Endres-Niggemeyer 1998). Accepted statements enter the summary, under the heading given by the scenario element that asked for them. Thus the summary organizes new knowledge in a fashion that mirrors the user’s situation. All the time, the user-centered interface keeps users in their own task environment. To produce summaries that fit users’ information needs, reasonably precise question statements are required. Questions (think of Who? Why? etc.) also have well-known qualities as text organizers, so that they can serve summary organization, the query items switching to headings for (partial) summaries when answers are delivered. Well-structured queries are most easily elicited by a convenient form. With a suitable choice of real-life scenarios (ideas inspired by Carroll, 2000), users can formulate their search and summarizing requests by filling out such a form, simply stating what they know and what they are missing in a given situation. Where the user identifies a knowledge gap (a question), the system will feed in the respective summary items if possible. In order to mediate between the users’ and the system perspective, we equip the scenario forms with intermediary structures - a detailed interpretation for summarizing and an abridged one for IR. Within these interpretations, the  form itself is represented by constants, the user query provides variables. In the following, we explain where our inspiration for the interface came from, how its design aims are met, ensued by empirical modeling and implementational details. 
This paper describes a highly-portable multilingual question answering system on multiple relational databases. We apply semantic category and pattern-based grammars, into natural language interfaces to relational databases. Lexico-semantic pattern (LSP) and multi-level grammars achieve portability of languages, domains, and DBMSs. The LSP-based linguistic processing does not require deep analysis that sacrifices robustness and flexibility, but can handle delicate natural language questions. To maximize portability, we drive various dependent parts into two tight corners, i.e., language-dependent part into front linguistic analysis, and domain-dependent and database-dependent parts into backend SQL query generation. Experiments with 779 queries generate only constraint-missing errors, which can be easily corrected by adding new terms, of 2.25% for English and 5.67% for Korean. Introduction As a natural language (NL) interface, question answering [7] on relational databases 1 allows users to access information stored in databases by requests in natural language [16], and generates as output natural language sentences, tables, and graphical representation. The NL interface can be combined with other interfaces to databases: a 
We propose a fast and reliable Question-answering (QA) system in Korean, which uses a predictive answer indexer based on 2-pass scoring method. The indexing process is as follows. The predictive answer indexer first extracts all answer candidates in a document. Then, using 2-pass scoring method, it gives scores to the adjacent content words that are closely related with each answer candidate. Next, it stores the weighted content words with each candidate into a database. Using this technique, along with a complementary analysis of questions, the proposed QA system saves response time and enhances the precision. Introduction Traditional Information Retrieval (IR) focuses on searching and ranking a list of documents in response to a user’s question. However, in many cases, a user has a specific question and want for IR systems to return the answer itself rather than a list of documents (Voorhees and Tice (2000)). To satisfy this need, the concept of Question Answering (QA) comes up, and a lot of researches have been carried out, as shown in the proceedings of AAAI (AAAI (n.d.)) and TREC (Text REtrieval Conference) (TREC (n.d.)). A QA system searches a large collection of texts, and filters out inadequate phrases or sentences within the texts. Owing to the filtering process, a user can promptly approach to his/her answer phrases without troublesome tasks. Unfortunately, most of the previous researches have passed over the following problems that occurs in real fields like World Wide Web (WWW): Users want to find answers as soon as possible. If a QA system does not respond to their questions within a few seconds, they will keep a suspicious eye on usefulness of the system.  Jungyun Seo Department of Computer Science Sogang University, 1 Sinsu-dong, Mapo-gu, Seoul, Korea, 121-742 seojy@ccs.sogang.ac.kr ¡ Users express their intentions by using various syntactic forms. The fact makes it difficult that a QA system performs well at any domains. Ultimately, the QA system cannot be easily converted into any domains. ¡ A QA system cannot correctly respond to all of the users’ questions. It can answer the questions that are included in the predefined categories such as person, date, and time. To solve the problems, we propose a practical QA system using a predictive answer indexer in Korean - MAYA (MAke Your Answer). MAYA focuses on resolving the practical problems such as real-time response and domain portability. We can easily add new categories to MAYA by only supplementing domain dictionaries and rules. We do not have to revise the searching engine of MAYA because the indexer is designed as a separate component that extracts candidate answers. Users can promptly obtain answer phrases on retrieval time because MAYA indexes answer candidates in advance. This paper is organized as follows. First, we review the previous works of the QA systems. Second, we present our system, and describe the applied NLP techniques. Third, we analyze the result of our experiments. Finally, we draw conclusions. 
This paper presents an approach to FAQ mining via a list detection algorithm. List detection is very important for data collection since list has been widely used for representing data and information on the Web. By analyzing the rendering of FAQs on the Web, we found a fact that all FAQs are always fully/partially represented in a list-like form. There are two ways to author a list on the Web. One is to use some specific tags, e.g. <li> tag for HTML. The lists authored in this way can be easily detected by parsing those special tags. Another way uses other tags instead of the special tags. Unfortunately, many lists are authored in the second way. To detect lists, therefore, we present an algorithm, which is independent of Web languages. By combining the algorithm with some domain knowledge, we detect and collect FAQs from the Web. The mining task achieved a performance of 72.54% recall and 80.16% precision rates. Introduction The World Wide Web has become a fertile area, storing a vast amount of data and information. One of them we are interested is the Frequently Asked Questions (FAQs). For customer services, message providing, etc., many Websites have created and maintained their own FAQs. A large collection of FAQs is very useful for many research areas in natural language processing. Especially in question answering, it exemplifies many questions and their answers. It is also a database for the applications of FAQ  retrieval, e.g. AskJeeves (www.ask.com), .faq finder (members.tripod.com/~FAQ_Home/), and FAQFinder (www1.ics.uci.edu/~burke/faqfinder/). By analysing the rendering of FAQs on the Web, we divide them into 6 types according to 2 viewpoints. Among these types, we found a fact that all FAQs are always fully/partially represented in the form of list as well as much useful information. There are two ways to represent a list in a Web Page. One is to use some specific tags, e.g. <li> tag for HTML. Another one is to use other tags. The lists authored in the first way can be easily detected by parsing those specific tags. However, most of FAQs are authored in the second way. Therefore, this paper presents an algorithm for detecting lists in Web Pages. Then, we verify each detected list whether it determines a set of FAQs or parts of it by some constraints of domain knowledge. 
This paper presents a novel approach to extracting phrase-level answers in a question answering system. This approach uses structural support provided by an integrated Natural Language Processing (NLP) and Information Extraction (IE) system. Both questions and the sentence-level candidate answer strings are parsed by this NLP/IE system into binary dependency structures. Phrase-level answer extraction is modelled by comparing the structural similarity involving the question-phrase and the candidate answerphrase. There are two types of structural support. The first type involves predefined, specific entity associa tions such as Affiliation, Position, Age for a person entity. If a question asks about one of these associations, the answer-phrase can be determined as long as the system decodes such pre-defined dependency links correctly, despite the syntactic difference used in expressions between the question and the candidate answer string. The second type involves generic grammatical relationships such as V-S (verb-subject), V-O (verbobject).  Preliminary experimental results show an improvement in both precision and recall in extracting phrase-level answers, compared with a baseline system which only uses Named Entity constraints. The proposed methods are particularly effective in cases where the question-phrase does not correspond to a known named entity type and in cases where there are multiple candidate answer-phrases satisfying the named entity constraints. Introduction Natural language Question Answering (QA) is recognized as a capability with great potential. The NIST-sponsored Text Retrieval Conference (TREC) has been the driving force for developing this technology through its QA track since TREC-8 (Voorhees 1999). There has been significant progress and interest in QA research in recent years (Voorhees 2000, Pasca and Harabagiu 2001). QA is different than search engines in two aspects: (i) instead of a string of keyword search terms, the query is a natural language question, necessitating question parsing, (ii) instead of a list of documents or URLs, a list of candidate answers at phrase level or sentence level are expected to be returned in response to a query, hence the need for text processing beyond keyword indexing, typically supported by Natural Language Processing (NLP) and Information Extraction (IE) (Chinchor and Marsh 1998, Hovy, Hermjakob and Lin 2001, Li and Srihari 2000). Examples of the use of NLP and IE in Question Answering include shallow parsing (Kupiec, 1993), semantic parsing (Litkowski  ∗ This work was partly supported by a grant from the Air Force Research Laboratory’s Information Directorate (AFRL/IF), Rome, NY, under contracts F30602-00-C-0037 and F30602-00-C-0090.  1999), Named Entity tagging (Abney et al. 2000, Srihari and Li 1999) and high-level IE (Srihari and Li, 2000). Identifying exact or phrase-level answers is a much more challenging task than sentence-level answers. Good performance on the latter can be achieved by using sophisticated passage retrieval techniques and/or shallow level NLP/IE processing (Kwok et al. 2001, Clarke et al. 2001). The phrase-level answer identification involves sophisticated NLP/IE and it is difficult to apply only IR techniques for this task (Prager et al. 1999). These two tasks are closely related. Many systems (e.g. Prager et al 1999; Clark et al 2001) take a two-stage approach. The first stage involves retrieving sentences or paragraphs in documents as candidate answer strings. Stage Two focuses on extracting phrase-level exact answers from the candidate answer strings. This paper focuses on methods involving Stage Two. The input is a sentence pair consisting of a question and a sentence-level candidate answer string. The output is defined to be a phrase, called answer-point, extracted from the candidate answer string. In order to identify the answerpoint, the pair of strings are parsed by the same system to generate binary dependency structures for both specific entity associations and generic grammatical relationships. An integrated Natural Language Processing (NLP) and Information Extraction (IE) engine is used to extract named entities (NE) and their associations and to decode grammatical relationships. The system searches for an answer-point by comparing the structural similarity involving the question-phrase and a candidate answer-phrase. Generic grammatical relationships are used as a back-off for specific entity associations when the question goes beyond the scope of the specific associations or when the system fails to identify the answer-point which meets the specific entity association constraints. The proposed methods are particularly helpful in cases where the question-phrase does not correspond to a known named entity type and in cases where there are multiple candidate answerpoints to select from. The rest of the paper is structured as follows: Section 1 presents the NLP/IE engine used,  sections 2 discusses how to identify and formally represent what is being asked, section 3 presents the algorithm on identifying exact answers leveraging structural support, section 4 presents case studies and benchmarks, and section 5 is the conclusion.  Kernel IE Modules Named Entity Entity Association  Input Tokenizer Linguistic Modules Part-OfSpeech Shallow Parsing Asking-point Identification Semantic Parsing  Output(Entity, Phrase and Structural Links)  Figure 1: InfoXtract™ NLP/IE System Architecture 
Open-Domain Question Answering systems (QA) performs the task of detecting text fragments in a collection of documents that contain the response to user’s queries. These systems use high complexity tools that reduce its applicability to the treatment of small amounts of text. Consequently, when working on large document collections, QA systems apply Information Retrieval (IR) techniques to reduce drastically text collections to a tractable quantity of relevant text. In this paper, we propose a novel Passage Retrieval (PR) model that performs this task with better performance for QA purposes than current best IR systems 
More and more researchers have recognized the potential value of the parallel corpus in the research on Machine Translation and Machine Aided Translation. This paper examines how Chinese English translation units could be extracted from parallel corpus. An iterative algorithm based on degree of word association is proposed to identify the multiword units for Chinese and English. Then the Chinese-English Translation Equivalent Pairs.are extracted from the parallel corpus. We also made comparison between different statistical association measurement in this paper. Keywords: Parallel Corpus, Translation Unit , Automatic Extraction of Translation unit Introduction The field of machine translation has changed remarkably little since its earliest days in the fifties. So far, useful machine translation could only obtained in very restricted domain. We believe one of the problems of traditional machine translation lies in how it deals with unit of translation. Normally Rule-Based Machine Translation system takes word as basic translation unit. However, word is normally polysemous and therefore ambiguous, which causes many difficulties in selecting proper target equivalent words in machine translation, especially in translation between unrelated language pairs, such as Chinese and English. On the other hand, human translation is rarely word-based. Human translators always translate group of words as a whole, which means human  do not view words as the basic translation units, and it seems they view language expressions that can transfer meaning unambiguously as basic translation units instead. Following this observation, we believe translation unit shall be not only words but also words groups (Multi-Word Unit) and a collection of bilingual translation unit will be certainly a very useful resource to machine translation. Manual compilation of such a database of translation unit is certainly labor intensive. But following the recent progress in Corpus Linguistics, especially in parallel corpus research such as Gale,W. (1991), Tufis,D. (2001), Wu, D., Xia, X.(1994). Automatic identification of translation unit and its target equivalents from existed authentic translation might be a feasible solution; at least it can be used to produce a candidate list of bilingual translation unit. As a first step towards building a database of bilingual translation units, we selected the Hong Kong Legal Documents Corpus (HKLDC) as the parallel corpus for the feasibility study. This paper elaborates the methods we adopted. We will first give our model of (semi-) automatic acquisition of bilingual translation unit based on parallel corpora in section 1. Then we will show how the corpus could be preprocessed in section 2. In section 3, several statistic measurements will be introduced which will serve as a basis for late steps in extracting of bilingual translation units. Section 4 will focuses on identification of multi-word units. Section 5 will describe how translation equivalents could be extracted. In section 6, we give some evaluation regarding to the performance in extracting the translation equivalent pairs. 
Despite progress in the development of computational means, human input is still critical in the production of consistent and useable aligned corpora and term banks. This is especially true for specialized corpora and term banks whose end-users are often professionals with very stringent requirements for accuracy, consistency and coverage. In the compilation of a high quality Chinese-English legal glossary for ELDoS project, we have identified a number of issues that make the role human input critical for term alignment and extraction. They include the identification of low frequency terms, paraphrastic expressions, discontinuous units, and maintaining consistent term granularity, etc. Although manual intervention can more satisfactorily address these issues, steps must also be taken to address intra- and inter-annotator inconsistency.  Keyword: legal terminology, bilingual  terminology,  bilingual  alignment,  corpus-based linguistics  1. Introduction Multilingual terminology is an important language resource for a range of natural language processing tasks such as machine translation and cross-lingual information retrieval. The compilation of multilingual terminology is often time-consuming and involves much manual labour to be of practical use. Aligning texts of typologically different languages such as Chinese and English is even more challenging because of  the significant differences in lexicon, syntax, semantics and styles. The discussion in the paper is based on issues arising from the extraction of bilingual legal terms from aligned Chinese-English legal corpus in the implementation of a bilingual a text retrieval system for the Judiciary of the Hong Kong Special Administrative Region (HKSAR) Government. Much attention in computational terminology has been directed to the development of algorithms for extraction from parallel texts. For example, Chinese-English (Wu and Xia 1995), Swedish-English-Polish (Borin 2000), and Chinese-Korean (Huang and Choi 2000). Despite considerable progress, bilingual terminology so generated is often not ready for immediate and practical use. Machine extraction is often the first step of terminology extraction and must be used in conjunction with rigorous and well-managed manual efforts which are critical for the production of consistent and useable multilingual terminology. However, there has been relatively little discussion on the significance of human intervention. The process is far from being straightforward because of the different purposes of alignment, the requirements of target users and the corpus type. Indeed, there remain many problematical issues that will not be easy to be resolved satisfactorily by computational means in the near future, especially when typologically different languages are involved, and must require considerable manual intervention. Unfortunately, such critical manual input has often been treated as an obscure process. As with other human cognitive process (T’sou et al. 1998), manual terminology markup is not a straightforward task and many issues deserve closer investigation. In this paper, we will present some significant issues for Chinese-English alignment  and term extraction for the construction of a bilingual legal glossary. Section 2 describes the background of the associated bilingual alignment project. Section 3 discusses the necessity of manual input in bilingual alignment, and some principles adopted in the project to address these issues. Section 4 provides an outline for further works to improve terminology management, followed by a conclusion in Section 5. 2. High Quality Terminology Alignment and Extraction 2.1 Bilingual Legal Terminology in Hong Kong The implementation of a bilingual legal system in Hong Kong as a result of the return of sovereignty to China in 1997 has given rise to a need for the creation and standardization of Chinese legal terminology of the Common Law on par with the English one. The standardization of legal terminology will not only facilitate the mandated wider use of Chinese among legal professionals in various legal practices such as trials and production of legal documentation involving bilingual laws and judgments, but also promote greater consistency of semantic reference of terminology to minimize ambiguity and to avoid confusion of interpretation in legal argumentation. In the early 90’s, Hong Kong law drafters and legal translation experts undertook the unprecedented task of translating Hong Kong Laws, which are based on the Common Law system, from English into Chinese. In the process, many new Chinese legal terms for the Common Law were introduced. On this basis, an English-Chinese Glossary of legal terms and a Chinese-English Glossary were published in 1995 and 1999 respectively. The legal terminology was vetted by the high level Bilingual Laws Advisory Committee (BLAC) of Hong Kong. The glossaries which contain about 30,000 basic entries have become an important reference for Chinese legal terms in Hong Kong. The Bilingual Legal Information System (BLIS) developed by the Department of Justice, HKSAR provides simple keyword search for the glossaries and laws that are available in both Chinese and English. Nevertheless, the glossaries are far from being adequate for many different types of legal documentation, e.g. contracts, court judgments, etc. One major limitation of the BLIS glossary is  its restricted coverage of legal terminology in the Laws of Hong Kong, within a basically prescriptive context as when the laws were studied at the time of its promulgation. There are other important bilingual references (Li and Poon 1998, Yiu and Au-Yeung 1992, Yiu and Cheung 1996) which focus more on the translation of Common Law concepts. These are almost exclusively nominal expressions. In 2000, the City University of Hong Kong, in cooperation with the Judiciary, HKSAR, initiated a research project to develop a bilingual text retrieval system, Electronic Legal Documentation/Corpus System (ELDoS), which is supported by a bilingually aligned corpus of judgments. The purpose of the on-going project is twofold. First, the aligned legal corpus enables the retrieval of legal terms used in authentic contexts where the essence and spirit of the laws are tested (and contested) in reality, explicated and elaborated on, as an integral part of the evolving and defining body of important precedent cases unique to the Common Law tradition. Second, the corpus covers judgment texts involving interpretation of different language styles and vocabulary from Hong Kong laws. The alignment markup also serves as the basis for the compilation of a high-quality bilingual legal term bank. To complete the task within the tight timeframe, a team of annotators highly trained in law and language are involved in alignment markup and related editing. 2.2 Need for Human Input The legal professionals which are the target users of ELDoS have very stringent demands on terminology in terms of accuracy, coverage and consistency. Aligned texts and extracted terms must therefore be carefully and thoroughly verified manually to minimize errors. Furthermore, many studies on terminology alignment and extraction deal predominantly with nominal expressions. Since the project aims to provide comprehensive information on the manifestations of legal vocabulary in Chinese and English texts, the retrieval system should not restrict users to nominal expressions but should also provide reference to many other phenomena such as alternation of part-of-speech (POS) (e.g. noun-verb alternation) inherent in bilingual texts, as will be seen in Section 3. The availability of bilingual corpora has made it possible to construct representative term  banks. Nonetheless, current alignment and term extraction technology are still considered insufficient to meet the requirements for high quality terminology extraction. In ELDoS project, many issues are difficult to be handled satisfactorily by the computer in the foreseeable future. Although human input is essential for high quality term bank construction, the practice of manual intervention is not straightforward. Indeed, the manual efforts to correct the errors can be substantial, and the associated cost should not be underestimated. The annotator must first go through the entire texts to spot the errors and terms left out by the machines. In this process, both the source and target materials have to be consulted. The annotator must also ensure the consistency of the output. As a result, guidelines should be set up to streamline the process. 3. Aspects of Terminology Alignment The approach adopted for the manual annotation of alignment markup and the maintenance of term bank in the ELDoS project will be described. Additional caution has been taken in the coordination of a team of annotators. 3.1 Term Frequency An important reason for manual intervention in bilingual term alignment is the relatively poor recall rate for low frequency terms. Many extraction algorithms make use of statistical techniques to identify multi-word strings that frequently co-occur (Wu and Xia 1995; Kwong and Tsou 2001). These methods are less effective for locating low frequency terms. Of the 16,000 terms extracted from ELDoS bilingual corpora, about 62% occur only once in about 80 judgments. For high quality alignment and extraction, failure to include these low frequency terms would be totally unacceptable. 3.2 Correspondence of Aligned Units Because of the different grammatical requirement and language style, a term in the source language often differs in different ways from the corresponding manifestations in the target language. These differences could be alternation of POS and the use of paraphrastic expressions. Although many term banks avoid such variations and focus primarily on equivalent nominals or verbs, the correspondence of terms between two typologically different languages is often more complicated. For example, the English nominal  lé ´êé ª (“fulfilment”) is more naturally translated into  Chinese as a verb (“  ”, “  ”, “ ”).  More examples can be found in Table 1.  Alternation of POS  English The accused hold fulfillment administration repudiation  Chinese o*+¬é ­lDé  POS alternation det + adj ~ noun verb ~ noun noun ~ verb noun ~ verb noun ~ neg + verb  Table 1. Alternation of POS  In some cases, there are simply no equivalent words in the target language. Paraphrasing or circumlocution may be necessary. Such correspondence is far less consistent and obvious to be identified by the computer.  Paraphrasing/Circumlocution  English The judge entered judgment in favour of the respondents in respect of their claim for arrears of wages, and severance payment. In our view,… …evidenced by the Defendant's letter …  Chinese ¶t2Ï9à`oK3D1I ñz…7çÛ5Q:1+a$_3…Y¬…}m  Table 2. Examples of paraphrasing  Because of language differences, legal terms can be contextually realized as anaphors in the target language. Examples of such correspondence would be useful for legal drafting and translation. Again, such anaphoric relations are more accurately handled by humans.  Anaphoric Relation  English  Chinese  9î3õÀs He was subsequently  …  charged… B9t£WÔ Liu JA dealt with that Í £ ´ ²A application on 14 March 1996 and dismissed it. ¶-ªà*¬9¬îÂ`Ï1 Enforcement of a Convention award may ^"lhöX* also be refused if the Þ¬^ Mñ´ê award is in respect of a N+éà*¬` matter which is not capable  1996 3 14 …  of settlement by arbitration.  Table 3. Examples of anaphors  3.3 Discontinuous Units  Most term extraction algorithms deal with  contiguous units, e.g. n-gram. These algorithms  would be problematical in handling discontinuous  units. They include phrasal verbs (e.g. “strike  o |«E out”), collocation patterns (e.g. “lodge three  complaints”, “ …  ”). These have to  be manually added or edited. Interestingly, our  preliminary study shows that over 90% of the  instances of discontinuous units are found in the  Chinese manifestation of English terms. Some  examples are listed in Table 4.  English convict … The Court of Appeal allowed the tenant's appeal The agreement kept the company alive If the Defendant misrepresented to the Plaintiff that what he was signing was only…  Chinese  oBÜ[:1#Bý9+É9"…3o5…@+"|BùX3«QÔDET¬}  : {  Table 4. Examples of discontinuous units  3.4 Selective Markup  To avoid producing “uninteresting” term alignment, restricting markup to only terms of the interested domain would be an attractive alternative to full-text alignment. In the ELDoS project, it is possible to mark up only legal terminology. Other non-legal elements can be omitted in alignment annotation. This approach has been accepted by the ELDoS client. Some examples of legal and non-legal terms are shown in Table 5.  Legal Terms  Non-legal Terms  
 7%  8  9  7  $  :  :  9;  7  $  9$ 3  %3  :  7  9:  7  7%  <:  3  :  %  7  ::  9$  9  :  8  :  $ $% 7  :  9  $  9  3  9  :  :  8  9  7  7 3%  3 : $$ ; :  9:  3  $  :9  :  $ 53 6  $  9  %+  9 9  & () * ( %,- . / #  ' + 0!!!1! %  () * ( %,- . / #  + 0!!!1! %  7%  9  :  ; =:  8  5$  8  6% 3  $7  9  :;  7  5(  $  0-->6 5?: ; $ 0-->6 5  $ 0--"6  5  0--"6 5'  $ @!!0 6 5  $  @!!06% 3  :;  3%  : :8 A : 7%  $ 9$  9  $$  =  :  3  A3 : : $  9:  3A : $ 9  4  : : $$ 7  %+  : $$ 9  ( 3 8@ $$ %  :  7  $$ : = 3 : $ 7 < $$ :  0:  9  %3  3  7  $$  $%  @: %3 %  $  9  >"> 4 $  % %  :  9 0%>  $7 5  $  :9  : $$  %  %/ 6 : 88  <  :  :  9: : %  4  : $$  9$  :;  %  D  7  % 9$ $  -!E%  9$  9  :  A3 :  9 $ %* 3% : : $$  0> : $9 9 : 3 $ %  < 0=  : ኚC5'  $ @!!096  BȧȞȜ  <  0:  ;  :$  9  : %< 7 $  B  B  $  :  : 5%%  6  :  $9  ) 9 ;  $ % :  C  9$  C%  9$ B: C  9  9$  B: C%  : $: %  :  4  $  ; :$ %  $  ; :$  9  =  $$  :  ; ::  $:  $  9  %  ; :$  9  9  : %< 7 $  9  $  $%  +  $  $  $%  F  ; :$  9  /  $%  $  $5  @!!06%  $  8  :  $  $  :  * 9$ 6  7  0- ,6 7  %  9$ %  :  5  6  $  92%  )  89  9  9$  $  %  :  5%%Ƒ WG  5%% Ư ֨G  5 % % ȩ #G 6  $  :  $  :  9$  3  $%  !"  #  "  !  ' $$  ;  3  7  :%  @ = 506 8  8  4  $  ; :$ %  8  +$  5)  +  5H : 0--I6% +  $  7$ %  $$  8  5  6 J 98  7$  +/ 5:  6  98  +/ /  8  +/ %  5@6 0-->6 : %< $ 9  9 $3 ; :$ $  9 : "  7  4  $  0%@%  !  9 : 9 9  $; 9$ 7 :  7 3 %3 7 = 506 9 9 :9  K 5@6 5 6 %  <;  9  = 506 8 K 5@6 : K 5 6 9 8  5,6 8 % 3 $  9$  $  8  %% 8 % :  7  3 :$9  : $$ 9 $  9$  9  9  9  7 %H  7  :  $  9  $  39  $  :  :  5  :  $  :  0%I-6%  9  :  $$ %  :  9$  8  %  7  9$  9  % %%  9  :%  :9 8  99  :9  % %%  9  8  :  98 %  3  :  7  9  3%  0I ; 7  %"&  !$  ' $$  ( 3 8@  %  5  D  0-->6  $ 9$  2 $L $  0--0 0--  :  $  9  M  (: +  0--,  0--I% +  I, 4  9  $  9  $  ( $3  (3 5 $ 6%  ( 3 8@  $$ 3 /!0!  5  3  $/  ;  D  06%  $$ :  :$  :9  $%  7  $ $ (:  $ (:  * 0--1 * 0---  5  @!!06 5  0---6% +  I! 4  9  $  9  $  ( 3%  9$ 0  9$ @%  (9 %  0>, "1-  ( 38 @ 9$ 0=  0 @ 0"  $ 5 96 0>"%,  + $ % I!"  0-@%0  > I  $$  ( 3 8@ 9$ @=  (9 4 I, I!  +  $  4  00-  0>"  4  $$  () &  H  9  $3  9  7%  $$ N N 3 % <  7  H;  : @!!!  %  :  $  H 9 @!!!% +  $  9  08- 9 H;  59  $; 0---6%  3  :  9  %  3  7  $  $  $$  H; /  5/ 6% 3  8  ;  $  9 9$  $  7$  $  7%  $  9 $8 :  ;:  /*@I  %3  :  /*@I!!  :  %3  :  ;  $  $/$  4 89 $  5  $6  9  8  %3 7  $  $  %  $  9  5'  $ @!!0 6  # !  + : $% :  0% : 3 : 9$ %  $ :  3  $  3  9  :  7:  $:  $$  5, 0?/6 :  9  $$ %  $$  $$  %  :  +  $$ : =  O$  8O  O$  0!! E  :  $  $$  :  $ $ $%  $$  7  $$ : =  0% / $ = :  :  %  $  $  8  :  $  %  @%  $  B9  :C  %H  $  /$  9  %  % % Ƒ˩J JJ  Jv 5  J  J  J 6 : $$ 9  Ƒ˩J  JJ  J  v  5 JJ J  6%  %  $  B  %H  7  : $$ 9  G 0- ,6%  9:C 9 %5 %% Ư  ,%  B  B 9 :C  7  : $$ 9  ඔG  <  :C %H 9 %5 % % 3336%  I%  B 7 : CB  :C B 9 :C  %H  $  97  9$  7  5 % % ȩ #G 6 : $$ 9  %  >%  $$ :  9 %H  :  9  %  (9 0 @ , I > 9$ =  + 5E6 -0%0! -@%,1 -@%0- %!- %!-,%,  +  2  500 + 6  !% "@-  !% ">-  !% "",  !% "0-  !% ","  !% 1@@  $%  3  3 9$  : :  $ 8 $$  9 : 9  $  :  $  008 F  $  $  00  !%! 0%!  % 9$  :  9  $93  %  0!!E  :  :%  9  9  %  *#  "  !  &  +:  @%@ :  ( 3 8@ $$  7  0%  4  :  7  $$ :  $ = 506 8 5 6K 5@6 : 5 6K 5 6  9 8 5/6K 5,6 8 5 6K 5I6 5 P 6K 5>6  5 P/6K 5"6 5 P 6K 516 5 P/6K 5-6 5 P 6K 50!6  5/P 6 K 5006 5 P P/6K 50@6 5 P P 6K 50 6  5 P/P 6K 50,6 5 P/P 6K 50I6 5 P P/P 6%  : H;  $ 0I  $  %  :;  $  5 $7  $  $  6  $  $%  $  $7  $  5  0---6%  (  0  @  /  ,  I  P  >  P/  "  P  
The Probabilistic Context-Free Grammar (PCFG) model is widely used for parsing natural languages, including Modern Chinese. But for Classical Chinese, the computer processing is just commencing. Our previous study on the part-of-speech (POS) tagging of Classical Chinese is a pioneering work in this area. Now in this paper, we move on to the PCFG parsing of Classical Chinese texts. We continue to use the same tagset and corpus as our previous study, and apply the bigram-based forward-backward algorithm to obtain the context-dependent probabilities. Then for the PCFG model, we restrict the rewriting rules to be binary/unary rules, which will simplify our programming. A small-sized rule-set was developed that could account for the grammatical phenomena occurred in the corpus. The restriction of texts lies in the limitation on the amount of proper nouns and difficult characters. In our preliminary experiments, the parser gives a promising accuracy of 82.3%. Introduction Classical Chinese is an essentially different language from Modern Chinese, especially in syntax and morphology. While there has been a number of works on Modern Chinese Processing over the past decade (Yao and Lua, 1998a), Classical Chinese is largely neglected, mainly because of its obsolete and difficult grammar patterns. In our previous work (2002), however, we have stated that in terms of computer processing, Classical Chinese is  even easier as there is no need of word segmentation, an inevitable obstacle in the processing of Modern Chinese texts. Now in this paper, we move on to the parsing of Classical Chinese by PCFG model. In this section, we will first briefly review related works, then provide the background of Classical Chinese processing, and finally give the outline of the rest of the paper. A number of parsing methods have been developed in the past few decades. They can be roughly classified into two categories: rule-based approaches and statistical approaches. Typical rule-based approaches as described in James (1995) are driven by grammar rules. Statistical approaches such as Yao and Lua (1998a), Klein and Manning (2001) and Johnson, M. (2001), on the other hand, learn the parameters the distributional regularities from a usually large-sized corpus. In recent years, the statistical approaches have been more successful both in part-of-speech tagging and parsing. In this paper, we apply the PCFG parsing with context-dependent probabilities. A special difficulty lies in the word segmentation for Modern Chinese processing. Unlike Indo-European languages, Modern Chinese words are written without white spaces indicating the gaps between two adjacent words. And different possible segmentations may cause consistently different meanings. In this sense, Modern Chinese is much more ambiguous than those Indo-European Languages and thus more difficult to process automatically (Huang et al., 2002). For Classical Chinese processing, such segmentation is largely unnecessary, since most Classical Chinese words are  single-syllable and single-character formed. To this end, it is easier than Modern Chinese but actually Classical Chinese is even more ambiguous because more than half of the words have two or more possible lexical categories and dynamic shifts of lexical categories are the most common grammatical phenomena in Classical Chinese. Despite of these difficulties, our work (2002) on part-of-speech tagging has shown an encouraging result. The rest of the paper is organized as follows. In Section 1, a tagset designed specially for Classical Chinese is introduced and the forward-backward algorithm for obtaining the context-dependent probabilities briefly discussed. We will briefly present the traditional two-level PCFG model, the syntactic tagset and CFG rule-set for Classical Chinese in Section 2. Features of the Classical Chinese grammar will also be covered in this section. In Section 3 we will present our experimental results. A summary of the paper is given in the conclusion section. 
2. Related Works A SVC is studied among several researchers as different names. But the general syntactic form is (NP) V1 (NP) V2 (NP)1. The variance of definition for SVC comes from the different scope of interpretation for the sentence pattern. We will introduce three typical researches to clearly outline our definition of SVC. The narrowest view of scope is suggested in (Lü, 1953). In his interpretation, V1 and V2 have the same subject and should be not coordinative, but it is difficult to decide which one is main or additional. Zhu (Zhu, 1981) includes all cases of Lü’s and the possibility of adding an adjective to substitute for the second verb position. He also includes the case where an additional verb and a main verb are used, such as V+ 着 expression in V1 position, which indicates that V1 is additional and V2 is main. The broadest scope is proposed in (Li & Thompson, 1981). According to his interpretation, an SVC includes not only all the patterns noted above but also a pivot construction, a subject/object clause, and a coordinate clause, but excludes the pattern with an adjective in the V2 position. In this paper, the scope of SVC is almost same as Li’s but the classification of SVCs differs slightly, detailing the categorization in chapter 4. A few computational solutions to identifying SVCs have been proposed by some researchers. A formal description is shown in (Chan, 1998) using time lapse notation and the related definition. However, her method makes it difficult to computationally detect SVCs without the resources containing the deep level of 
For readers of English text who know some Chinese, Pinyin codes that spell out Chinese names are often ambiguous as to their original Chinese character representations if the names are new or not well known. For English-Chinese cross language retrieval, failure to accurately translate Pinyin names in a query to Chinese characters can lead to dismal retrieval effectiveness. This paper presents an approach of extracting Pinyin names from English text, suggesting translations to these Pinyin using a database of names and their characters with usage probabilities, followed with IR techniques with a corpus as a disambiguation tool to resolve the translation candidates. Introduction It is important for many applications to be able to identify and extract person names in text. For English, capital letter beginning of a word is an important clue to spot names, in addition to other contextual ones. When an English story refers to a foreign person, it is relatively easy to represent the person’s name if the alphabets have approximate correspondences between the languages. When it refers to a Chinese person, this is not possible because Chinese language does not use alphabets. The most popular method for this purpose is Pinyin coding (see, for example, the conversion project at the Library of Congress website (2002)), China’s official method of using English to spell out Chinese character pronounciations according to the Beijing Putonghua convention. Chinese characters are monosyllabic, and the large majority of them has one sound (ignoring tones) and hence one code. However, given a Pinyin it usually maps to multiple characters. Such an  Peter DENG Computer Science Dept., Queens College, City University of New York 65-30 Kissena Boulevard, Flushing, NY 11367, USA deng@ntkk.cs.qc.edu English Pinyin name raises ambiguity about the original Chinese characters that it refers to and hence the person. If the name is well known, such as Mao ZeDong, this is not an issue; if the name is less frequently seen, one would like to see or confirm the actual Chinese characters. The situation is similar to many Chinese word processing systems that use Pinyin as one of their input methods. When a Pinyin is typed (sometimes with tonal denotation), many candidate characters will be displayed for the user to select. The character list can be ordered based on a language model, Chen & Lee (2000), or on the user’s past habit. When one comes across names as input however, a language model is not as helpful because practically any character combination is possible for names. Pinyin names also present difficulties in a cross language information retrieval (CLIR) scenario. Here, an English query is given to retrieve Chinese documents, and Pinyin names could be present as part of the query. In general, one can have three approaches to CLIR as discussed in Grefenstette (1998): translate the Chinese documents to English and do retrieval matching in English; translate the English query to Chinese and do matching in Chinese; or translate both to an intermediate representation. With the first approach, one could use standard table lookup to map the characters of a Chinese name to Pinyin after identifying a name for extraction. Chen and Bai (1998), Sun et.al. (1994) have shown that this extraction process is not trivial since Chinese writing has no white space to delimit names or words. A more general difficulty is that the document collection may not be under a user’s control, but available for retrieval purposes only. This makes document translation to the query language (or to an intermediate language) not suitable. A more flexible approach is to translate a query to Chinese and do retrieval in Chinese. This has  been the more popular method to use for CLIR in TREC experiments: Voorhees and Harman (2001). Whichever translation direction one chooses, a bilingual dictionary is essential. This dictionary however can be expected to be incomplete, especially with person names. Missing their translations can adversely impact on CLIR effectiveness. This raises the question of how to render Pinyin names into Chinese characters for translingual retrieval purposes. In the recent NTCIR-2 English-Chinese cross language experiments, Eguchi et.al. (2001), quite a few queries have names. Kwok (2001) found that these lead to good monolingual retrieval because the names are quite specific and have good retrieval properties. On the other hand, for CLIR that starts with English queries, not being able to translate Pinyin names correctly leads to substantial deficit in effectiveness. This causes comparisons with monolingual results particularly dismal. In this paper, we propose an approach to resolve the characters from a Pinyin code. It is based on: 1) a rule-based procedure to extract Pinyin codes for Chinese person names in English text; 2) a database for proposing candidate Chinese character sequences for a Pinyin code based on usage probabilities; and 3) a target collection and IR techniques as a confirmation tool for resolving or narrowing down the proposed candidates. These are described in Sections 1, 2, and 3 respectively. Section 4 presents some CLIR results and a measure of the effectiveness of our procedures. We like to stress that even if one obtains the correct Chinese characters for a Pinyin, they can still refer to different persons with the same name. We do not address this issue here.  
3 Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong. Email: cswjli@comp.polyu.edu.hk  Abstract Traditional indexing methods often record physical positions for the specified words, thus fail to recognize context information. We suggest that Chinese text index should work on the layer of sentences. This paper presents an indexing method based on sentences and demonstrates how to use this method to help compute the mutual information of word pairs in a running text. It brings many conveniences to work of natural language processing. Keywords: natural language processing, index file, mutual information 1. Introduction Natural Language Processing often needs to analyze the relationships between words within the same sentences or the syntax of the sentences by considering the specific words. To obtain such information, sentences are usually considered as the basic processing units [4]. The fixed window approach is often used in previous studies to observe the contexts of the specific words and extract them from corpora to form a sub corpus for some purposes [5,6]. To observe the other words, corpora have to be scanned again and again. Therefore, creating an index file in advance will help locate the specified words fast and could extend the ability to cope with the large-scale problems. Although the traditional indexing methods can locate the specific words fast, it needs extra work to provide the context information. Traditional computer indexing methods record the physical position of the words in the corpus. The position  information is stored in the index file. To find out where the specified word is, the index file can provide physical position directly. Then the word in the corpus can be quickly located [3]. However, if we want to extract the sentences containing the words, the traditional processing methods have to search forward and backward to find the boundary of these sentences. The indexing method presented in this paper creates the index file based on sentences. Unlike traditional indexing methods that record the physical position of the word in the corpus, this new method records the logical positions of the words. Not only can the index file give the numbers of the sentences in which the specified word occurs, but also locate these sentences in the corpus instantly. Since the indexing method based on sentences records the information of the contexts of the words, we are able to conveniently study some problems with the words in the sentences concerned, which could be called the logical layer. That makes it feasible to solve some natural language processing problems in a largescale corpus. The rest of this paper is organized as follows. The second section describes the principle of the method proposed in this paper. Then the third section summarizes its advantages. And an example applying the method is given in the fourth section. The fifth section closes this paper with conclusion. 2. Description of the method As mentioned above, the difference between the indexing method presented in this paper and  *Supported by Natural Science Foundation of China( and 973 project (G1998030507) 
This is a pilot study which aims at the design of a Chinese morphological analyzer which is in state to predict the syntactic and semantic properties of nominal, verbal and adjectival compounds. Morphological structures of compound words contain the essential information of knowing their syntactic and semantic characteristics. In particular, morphological analysis is a primary step for predicting the syntactic and semantic categories of out-of-vocabulary (unknown) words. The designed Chinese morphological analyzer contains three major functions, 1) to segment a word into a sequence of morphemes, 2) to tag the part-of-speech of those morphemes, and 3) to identify the morpho-syntactic relation between morphemes. We propose a method of using associative strength among morphemes, morpho-syntactic patterns, and syntactic categories to solve the ambiguities of segmentation and part-of-speech. In our evaluation report, it is found that the accuracy of our analyzer is 81%. 5% errors are caused by the segmentation and 14% errors are due to part-of-speech. Once the internal information of a compound is known, it would be beneficial for the further researches of the prediction of a word meaning and its function. 1. Introduction This is the first attempt to design a morphological analyzer to automatically analyze the morphological structures of Chinese compound words1. Morphological structures of compound words contain the essential information of knowing their syntactic and semantic characteristics. In particular, morphological analysis is a primary step for predicting the syntactic and semantic categories of out-of-vocabulary (unknown) words. The existence of unknown words is a major obstacle in Chinese natural language processing. Due to the 
We have proposed a method of word segmentation for non-segmented language using Inductive Learning. This method uses only surface information of a text, so that it has an advantage that is entirely not dependent on any speciﬁc language. In this method, we consider that a character string of appearing frequently in a text has a high possibility as a word. The method predicts unknown words by recursively extracting common character strings. With the proposed method, the segmentation results can adapt to diﬀerent users and ﬁelds. To evaluate eﬀectivety for Chinese word segmentation and adaptability for diﬀerent ﬁelds, we have done the evaluation experiment with Chinese text of the two ﬁelds. 
The problem of word segmentation affects all aspects of Chinese language processing, including the development of text-to-speech synthesis systems. In synthesizing a Hong Kong Cantonese text, for example, words must be identified in order to model fusion of coda [p] with initial [h], and other similar effects that differentiate word-internal syllable boundaries from syllable edges that begin or end words. Accurate segmentation is necessary also for developing any list of words large enough to identify the wordinternal cross-syllable sequences that must be recorded to model such effects using concatenated synthesis units. This paper describes our use of the Segmentation Corpus to constrain such units. Introduction What are the best units to use in building a fixed inventory of concatenative units for an unlimited vocabulary text-to-speech (TTS) synthesis system for a language? Given a particular choice of unit type, how large is the inventory of such units for the language, and what is the best way to design materials to cover all or most of these units in one recording session? Are there effects such as prosodically conditioned allophony that cannot be modeled well by the basic unit type? These are questions that can only be answered language by language, and answering them for Cantonese1 poses several interesting challenges. 1We use “Cantonese” to mean the newer Hong Kong  One major challenge involves the definition of the “word” in Cantonese. As in other varieties of Chinese, morphemes in Cantonese are typically monosyllabic and syllable structure is extremely simple, which might suggest the demi-syllable or even the syllable (Chu & Ching, 1997) as an obvious basic unit. At the same time, however, there are segmental “sandhi” effects that conjoin syllables within a word. For example, when the morpheme 集 zaap6 2 stands as a word alone (meaning ‘to collect’), the [p] is a glottalized and unreleased coda stop, but when the morpheme occurs in the longer word 集合 zaap6hap6 (‘to assemble’), the coda [p] often resyllabifies and fuses with the following [h] to make an initial aspirated stop. Accurate word segmentation at the text analysis level is essential for identifying the domain of such sandhi effects in any full-fledged TTS system, whatever method is used for generating the waveform from the specified pronunciation of the word. A further challenge is to find a way to capture such sandhi effects in systems that use concatenative methods for waveform generation. This paper reports on research aimed at defining an inventory of concatenative units for Cantonese using the Segmentation Corpus, a lexicon of 33k words extracted from a large corpus of Cantonese newspaper texts. The corpus is described further in Section 2 after an excursus (in Section 1) on the problems posed standard, and not the older Canton City one. 2 We use the Jyutping romanization developed by the Linguistics Society of Hong Kong in 1993. See http://www.cpct92.cityu.edu.hk/lshk.  by the Cantonese writing system. Section 3 outlines facts about Cantonese phonology relevant to choosing the concatenative unit, and Section 4 calculates the number of units that would be necessary to cover all theoretically possible syllables and sequences of syllables. The calculation is done for three models: (1) syllables, as in Chu & Ching (1997), (2) Law & Lee’s (2000) mixed model of onsets, rhymes, and cross-syllabic rhyme-onset units, and (3) a positionally sensitive diphone model. This section closes by reporting how the number of units in the last model is reduced by exploiting the sporadic and systematic phonotactic gaps discovered by looking for words exemplifying each possible unit in the Segmentation Corpus. 
Pronunciation-translated names (P-Names) bring more ambiguities to Chinese word segmentation and generic named entity recognition. As there are few annotated resources that can be used to develop a good P-Name extraction system, this paper presents a bootstrapping algorithm, called PN-Finder, to tackle this problem. Starting from a small set of P-Name characters and context cue-words, the algorithm iteratively locates more P-Names from the Internet. The algorithm uses a combination of P-Name and context word probabilities to identify new P-Names. Experiments show that our PN-Finder is able to locate a large number of P-Names (over 100,000) from the Internet with a high recognition accuracy of over 85%. Further tests on the MET-2 test set show that our PN-Finder can achieve a performance of over 90% in F1 value in locating P-Names. The results demonstrate that our PN-Finder is effective.  
In this paper we report results of a supervised machine-learning approach to Chinese word segmentation. First, a maximum entropy tagger is trained on manually annotated data to automatically labels the characters with tags that indicate the position of character within a word. An error-driven transformation-based tagger is then trained to clean up the tagging inconsistencies of the first tagger. The tagged output is then converted into segmented text. The preliminary results show that this approach is competitive compared with other supervised machine-learning segmenters reported in previous studies. 
This paper presents a unified solution, which is based on the idea of “roles tagging”, to the complicated problems of Chinese unknown words recognition. In our approach, an unknown word is identified according to its component tokens and context tokens. In order to capture the functions of tokens, we use the concept of roles. Roles are tagged through applying the Viterbi algorithm in the fashion of a POS tagger. In the resulted most probable roles sequence, all the eligible unknown words are recognized through a maximum patterns matching. We have got excellent precision and recalling rates, especially for person names and transliterations. The result and experiments in our system ICTCLAS shows that our approach based on roles tagging is simple yet effective. Keywords: Chinese unknown words recognition, roles tagging, word segmentation, Viterbi algorithm. Introduction It is well known that word segmentation is a prerequisite to Chinese information processing. Previous research and work in word segmentation have made great progresses. However, cases with unknown words are not satisfactory. In general, any lexicon is limited and unable to cover all the words in real texts or speeches. According to our statistics on a 2,305,896-character news corpus from the People's Daily, there are about 1.19% unknown words. But they are difficult to be recalled and often greatly reduce the recognition rate of known words close to them. For example, the sentence “ 部 长 孙 家 正 在 工 作 。 ” (Pronunciation: “Bu Zhang Sun Jia Zheng Zai Gong Zuo.”) has two valid segmentations: “部 长/孙家正/在/工作” (The minister Sun Jiazheng is at work) and “ 部 长 / 孙 家 / 正 在 / 工 作 ” (The minister Sun Jia now is at work). “孙家正” is a person name in the first, while “孙家” is another  name in the latter. Meanwhile, the string “孙家正 在” will lead to overlapping ambiguity and bring a collision between the unknown word “ 孙 家 正” (Sun Jiazheng) and “正在”(zheng zai; now). What’s more, the recognizing precision rates of person names, place names, and transliterations are 91.26%, 69.12%, and 82.83%, respectively, while the recalling rates of them are just 68.77%, 60.47%, and 78.29%, respectively. (Data from official testing in 1999) [Liu (1999)] In a word, unknown words recognition has become one of the biggest stumbling blocks on the way of Chinese lexical analysis. A proper solution is important and urgent. Various approaches are taken in Chinese unknown words recognition. They can be broadly categorized into “one-for-one”, “one-for-several” and “one-for-all” based on the number of categories of unknown words, which can be recognized. One-for-one solutions solve a particular problem, such as person name recognition [Song (1993); Ji (2001)], place name recognition [Tan (1999)] and transliteration recognition [Sun (1993)]. Similarly, one-for-several approaches provide one solution for several specific categories of unknown words [Lv (2001); Luo (2001)]. One-for-all solutions, as far as we know, have not been applicable yet [Chen (1999); He (2001)]. Although currently practicable methods could achieve great precision or recalling rates in some special cases, they have their inherent deficiencies. First of all, rules applied are mostly summarized by linguists through painful study of all kinds of huge “special name libraries” [Luo (2001)]. It’s time-consuming, expensive and inflexible. The categories of unknown words are diverse and the amount of such words is huge. With the rapid development of the Internet, this situation is becoming more and more serious. Therefore, it’s very difficult to summarize simple yet thorough rules about their compositions and contexts. Secondly, the recognition process cannot be activated until some “indicator” tokens are  scanned in. For instance, possible surnames or titles often trigger person name recognition on the following 2 or more characters. In the case of place name recognition, the postfixes such as “ 县 ”(county), “ 市 ”(city) will activate the recognition on the previous characters. What’s more, these methods tend to work only on the monosyllabic tokens, which are obvious fragments after tokenization [Luo (2001); Lv (2001)]. It takes the risk of losing lots of unknown words without any explicit features. Furthermore, this trigger mechanism cannot resolve the ambiguity. For example, unknown word “方林山” (Fang Lin Shan) maybe a person name “方/林山”(Fang Linshan) or a place name “方林/山”(Fanglin Mountain). This paper presents a one-for-all approach based on roles tagging to avoid such problems. The process is: tagging tokens after word segmentation with the most probable roles and making unknown words recognition based on roles sequence. The mechanism of roles tagging is just like that of a small and simple Part-Of-Speech tagger. The paper is organized as follows: In section 2, we will describe the approach in general. Following that, we will present the solution in practice. In the final part, we provide recognition experiments using roles-tagging methods. The result and possible problems are discussed as well. 
This paper introduces new definitions of Chinese base phrases and presents a hybrid model to combine Memory-Based Learning method and disambiguation proposal based on lexical information and grammar rules populated from a large corpus for 9 types of Chinese base phrases chunking. Our experiment achieves an accuracy (F-measure) of 93.4%. The significance of the research lies in the fact that it provides a solid foundation for the Chinese parser. 
This paper describes a rule-learning approach towards Chinese prosodic phrase prediction for TTS systems. Firstly, we prepared a speech corpus having about 3000 sentences and manually labelled the sentences with two-level prosodic structure. Secondly, candidate features related to prosodic phrasing and the corresponding prosodic boundary labels are extracted from the corpus text to establish an example database. A series of comparative experiments is conducted to figure out the most effective features from the candidates. Lastly, two typical rule learning algorithms (C4.5 and TBL) are applied on the example database to induce prediction rules. The paper also suggests general evaluation parameters for prosodic phrase prediction. With these parameters, our methods are compared with RNN and bigram based statistical methods on the same corpus. The experiments show that the automatic rule-learning approach can achieve better prediction accuracy than the non-rule based methods and yet retain the advantage of the simplicity and understandability of rule systems. Thus it is justified as an effective alternative to prosodic phrase prediction. 
P.R.China 110006 zhujingbo@yahoo.com Abstract The paper presents a simple and effective knowledge-based approach for the task of text classification. The approach uses topic identification algorithm named FIFA to text classification. In this paper the basic process of text classification task and FIFA algorithm are described in detail. At last some results of experiment and evaluations are discussed. Keywords: FIFA algorithm, topic identification, text classification, natural language processing Introduction The text automatic classification method is based on the content analysis automatically to allocate the text into pre-determined catalogue. The methods of text automatic classification mainly use information retrieval techniques. Traditional information retrieval mainly retrieves relevant documents by using keyword-based or statistic-based techniques (Salton.G1989). Generally, three famous models are used: vector space model, Boolean model and probability  Yao Tianshun Institute of Computer Software & Theory Northeastern University, Shenyang Liaoning, P.R.China 110006 tsyao@china.com model, based on the three models, some researchers brought forward extended models such as John M.Picrrc(2001), Thomas Bayer, Ingrid Renz,Michael Stein(1996), Antal van den Bosch, Walter Daelemans, Ton Weijters(1996), Manuel de Buenaga Rodriguez, Jose Maria Gomez-llidalgo, Belen Diaz-agudo(1997), Ellen Riloff and Wendy Lehnert(1994). One central step in automatic text classification is to identify the major topics of the texts. We present a simple and effective knowledge-based approach to text automatic classification. The approach uses topic identification algorithm named FIFA to text classification. In this paper the basic process of text classification task and FIFA algorithm are described in detail. At last some results of experiment and evaluations are discussed. 
 DLACT  Fac. Informática, UPM E.U.I.T. Informáticos, UPM  Madrid, Spain, 28660  Madrid, Spain, 28031  lupe@fi.upm.es  ialvarez@euitt.upm.es  Antonio Pareja-Lora DSIP2 Fac. Informática, UCM Madrid, Spain, 28040 apareja@sip.ucm.es  Rosario Plaza-Arteche DLACT Fac. Informática, UPM Madrid, Spain, 28660 rplaza@fi.upm.es  ABSTRACT12 Although with the Semantic Web initiative much research on web page semantic annotation has already been done by AI researchers, linguistic text annotation, including the semantic one, was originally developed in Corpus Linguistics and its results have been somehow neglected by AI. The purpose of the research presented in this proposal is to prove that integration of results in both fields is not only possible, but also highly useful in order to make Semantic Web pages more machine-readable. A multi-level (possibly multi-purpose and multilanguage) annotation model based on EAGLES standards and Ontological Semantics, implemented with last generation Semantic Web languages (RDF(S)/XML) is being developed to fit the needs of both communities; the present paper focuses on its semantic level. INTRODUCTION All of us are by now accustomed to making extensive use of the so-called World Wide Web (WWW) which we might consider a great source of information, accessible through computers but, hitherto, only understandable to human beings. In its beginning, web pages were hand made, intended and oriented to the exchange of information among human beings. Due to the astonishing growth of Internet use, new technologies emerged and, with them, machineaided web page generation appeared. Up to that point, the structure and the edition of these pages fitted only human needs – and this, only to some extent. All of these documents contained a huge amount of text, images and even sounds, meaningless to a computer. In this way, they put 
Content selection is a key factor of any successful document generation system. This paper shows how a content selection algorithm has been implemented using an efficient combination of XML/XSL technology and the framework of RST for discourse modeling. The system generates multilingual documents adapted to user profiles in a learning environment for the web. This CourseViewGenerator applies simplified RST schemes to the elaboration of a master document in XML from which content segments are chosen to suit the user's needs. The personalisation of the document is achieved through the application of a sequence of filtering levels of text selection based on the user aspects given as input. These cascading filters are implemented in XSL. Introduction It is widely accepted that content selection plays a crucial role in text generation (Reiter and Dale 2000). This process is normally seen as a goal-directed activity in which text segments are fit into the discourse structure of the text so as to convey a coherent communicative goal (Grosz and Sidner 1986). Content planning techniques, such as textual schemas (McKeown 1985) or plan operators (Moore and Paris 1993), have been successfully used as models of text generation. There are cases, though, in which these techniques may face some limitations, for example, when the structure of the discourse is difficult to anticipate (Mellish et al. 1998). Nevertheless, when a set of well-defined communicative goals exists, complex goals can be broken down into sequences of utterances and generation becomes an efficient "top-down'' process (Marcu 1997). This paper shows a macro level content selection algorithm that applies user profiles to  constrain and discriminate the contents of a text, whose discourse structure is represented using a simplified version of Rhetorical Structure Theory (Mann and Thompson 1988). The algorithm has been implemented using XML/XSL-based technology in a multilingual document generation system for educational purposes. The main objective of this CourseViewGenerator system (Barrutieta, 2001 and Barrutieta et al., 2001) is to automatically produce multilingual learning documents that suit the student's needs at each particular stage of the learning process. Figure 1 shows the overall architecture of the system.  xml-dtd  User aspects  COURSE GENERATOR Select content and format in an “intelligent” way.  Inputs html-xml-dtdxsl-javascript  Course material (multilingual parallel corpus)  Generation engine Document generation  Web browser  Document view  Figure 1: General scheme of the multilingual document generation system  We will begin by explaining the different parts of the system before addressing in more detail the content selection algorithm itself. The system starts by constructing a master document of the kind Hirst et al. (1997) proposed. This master document consists in a full-fledged text with references to all necessary multimedia elements (figures, tables, pictures, links, etc.). In our case, this master document takes the shape of a simple text file with all relevant information tagged in XML. Tags carry information of the logical composition of the text as well as metadata information about  its discourse structure. The text is seen as raw data, and tags encapsulate these raw data as metadata. The structure of the discourse is represented using a simplified version of RST. RST is simplified in the sense that the granularity of discourse segments does not transcend the boundaries of the sentence. Table 1. illustrates this gross-grained version of RST in which discourse relations are represented as XML tags. <RST> <RST-S> <PREPARATION> <S> What is knowledge management? </S> </PREPARATION> </RST-S> <RST-N> <S> Knowledge, in a business context, is the organizational memory, which people know collectively and individually </S> <S> Management is the judicious use of means to accomplish an end </S> <S> Knowledge management is the combination of those concepts, KM = knowledge + management </S> </RST-N> </RST> <RST> <RST-S> <PREPARATION> <S> ¿Qué es gestión del conocimiento? </S> </PREPARATION> </RST-S> <RST-N> <S> Conocimiento, en el contexto de los negocios, es la memoria de la organización, lo que la gente sabe colectiva e individualmente </S> <S> Gestión es el uso juicioso de recursos para alcanzar un fin </S> <S> Gestión del conocimiento es la combinación de esos dos conceptos, GC = gestión + conocimiento </S> </RST-N> </RST> <RST> <RST-S> <PREPARATION> <S> Zer da ezagutzaren kudeaketa? </S> </PREPARATION> </RST-S> <RST-N> <S> Kudeaketa, negozioetan, erakundearen memoria da, jendeak bakarka eta taldeka dakiena </S> <S> Kudeaketak erabideen erabilera zuzena du helburu </S> <S> Ezagutzaren kudeaketa bi kontzeptu hauen nahasketa da, EK = ezagutza + kudeaketa </S> </RST-N> </RST> Table 1: Gross-grained RST in XML  As any other standard RST discourse tree, this simplified RST contains a nucleus for each text paragraph, and one or several satellites linked by a discourse relation to the nucleus within the same paragraph. The nucleus is an absolutely essential segment of the text, as it carries the main message that the author wants to convey. Satellites can be replaced or erased without changing the overall message and play an important supporting role for the nucleus. In our system, satellites are selected or discarded depending on the reader’s profile. The reader’s profile is defined through a set of user aspects. These take the form of multivalue parameters that were sketched after a number of surveys were conducted among teachers, students and other experts from the educational context. As a result of these surveys a user model was proposed (Barrutieta et al, 2002). Table 2 illustrates a simplified version of the model.  Specific User Aspects Discrete values  Subject  Language processors  Moment in time  Before the course / Period 1 /  Period 2 / … / After the  course (review)  Languages  EN/ ES/ EU  General User Aspects Discrete values  Level of expertise Reason to read Background Opinion or motivation Time available  Null / Basic / Medium / High To get an idea / To get deep into it Not related to the subject / Related to the subject Against / Without an opinion or motivation / In favour A little bit of time / Quite some time / Enough time  Table 2: User model  Based on this user model, we will now discuss the content selection algorithm (henceforth CSA). The CSA determines which segments of the discourse are going to be used in order to make explicit the set of parameters that conform with the user’s profile. In principle, nuclei will always be chosen (as they convey the main message of the text); satellites, however, will be selected depending on their relation to the nucleus and the user aspects that are activated at the time of generation.  The selection algorithm works in three consecutive phases: parallel selection, horizontal filtering and vertical filtering. Vertical filtering is the most important phase of the three as it is here that the parts of the discourse tree are selected or discarded. 
In this paper we sketch the design, motivation and use of the GeM annotation scheme: an XML-based annotation framework for preparing corpora involving documents with complex layout of text, graphics, diagrams, layout and other navigational elements. We set out the basic organizational layers, contrast the technical approach with some other schemes for complex markup in the XML tradition, and indicate some of the applications we are pursuing. 
We describe our ongoing work on an application of XML/XSL technology to a dictionary, from whose source representation various views for the human reader as well as for automatic text generation and understanding are derived. Our case study is a dictionary of discourse markers, the words (often, but not always, conjunctions) that signal the presence of a disocurse relation between adjacent spans of text. 
We describe the use of a suite of highly ﬂexible XML-based NLP tools in a project for processing and interpreting text in the medical domain. The main aim of the paper is to demonstrate the central role that XML mark-up and XML NLP tools have played in the analysis process and to describe the resultant annotated corpus of MEDLINE abstracts. In addition to the XML tools, we have succeeded in integrating a variety of non-XML ‘off the shelf’ NLP tools into our pipelines, so that their output is added into the mark-up. We demonstrate the utility of the annotations that result in two ways. First, we investigate how they can be used to improve parse coverage of a hand-crafted grammar that generates logical forms. And second, we investigate how they contribute to automatic lexical semantic acquisition processes. 
We present an upcoming web-based centre for information and documentation on language technology (LT) in Sweden and/or in Swedish. Some ways to collect and represent data using data harvesting and XML are suggested. Users providing data may choose one of four input modes, two of which will allow the user to edit existing documents at her own site. Thus we hope to facilitate data entry and inter-site communication, and reduce the amount of stale data. The entry modes use annotated HTML and an emerging XML-application, respectively. In conclusion, we propose that an XMLapplication in the field of LT would facilitate for the LT community to share information, and that automatic data harvesting will emerge as an important technique for building information centres in the future. Introduction The website Slate is an information centre on language technology (LT) developed in Sweden and/or for the Swedish language. It is the technical/academic part of a collaborative effort between the National Graduate School of Language Technology in Sweden (GSLT) and Svenska språknämnden (the official organization for matters concerning the Swedish language). Språknämnden will provide the second part, containing popular information. Some material for the popular part will be taken over from the project Svenska.se at the Swedish Institute for Computer Science, SICS. The two parts will as far as possible have the same structure and  cross-reference each other. We will inform on the following matters, in both areas: • Research projects. • Industrial projects. • Educational programmes. • Software. • LT developed in Sweden for other languages than Swedish. • Swedish organizations and individuals who have an interest in LT. • Employment oppurtunities in the area of LT. Information will be provided in both English and Swedish, depending on whether translated documents exist. We cooperate via NorDokNet with centra in Denmark, Finland, Iceland and Norway. We also have contact with COLLATE at DFKI and follow their way of organizing information on LT in LT-World. Several more Language Technologists and LT-projects will contribute with information, notably the Ph. D. students in GSLT who have agreed to participate in software tests. 
HTML web browser  ÎÃ¢ik@ï4¡±Å© I£¦´§£@k@  µ@s´kÅ²£¹Ç6¡§¹¦­Í¶®©²"¥±¸k@´kÁÎaIfª½¥¦©¥¦´@´m½£¦¸´ÏÇ6I¹¦f¶®m²uf¦±°@ ´k¿©fµ¬q¶®i²  5  c687@9BA I A AP  cfV8{­|D R}CFcfR8E {i7@ T " 9  T V8}Y5|8~ÍV"S  YHG  Ðmf®p´kp@¦IuBf´kpª@§@¦k¸kI¿£k£@ikI@£IImòª§£wfp@®@k¬«@´kme³¥ë´kf¦¾fmXÏ@m@pImIm@@@£`@@k­@@s«±)@f´p¸156@F ¢f¹¦p¦@´kIf£¦´ @R@Gkkfk´k×@B¹)I@Y¹)£Û½m©@Ô´X£·s´kwBSf@¸"¦ )kÝ'fX²b£@ëÛpR·´siÁ××s§Ê@5kµG¸HÜpÎaIIf@Q¦fwp¶®§p@@¢iII¦fBkk£I²¢iI´IIf±¡mº§k­mkfwø¢Bk@f@ö¸£Ä¥¦¸fm§¸U@@IB@µG@´k@m´ÆTW´I¶®£QPVps@¦s¢if@sk²Ì@"´kB¿@@6p5EpÑ¾@k¹¦@f£¬s§@i@3i£´5´kf§@m@p@´kkfp§'BfImÏUw¾If§fTW¢i­f¢®VG§@fBf¡´kkµGG°¸É¿´@¢ó@®¶®¹)©ëÐ8uI§F²³Á§@@Ñ@°µ¬pBÍfk@ÎaI©s¶®@§w´kss£@¢p£x£x²   `badc4ec4f4c4gh¥iqp'rtsquwvxrwpys'Wirwp4rt4'pWiwiq' `badc4ec4f4c4ghds'WirpeyfWgqpwvhitqWi4jxw `badc4ec4f4c4gh8Wgqpwvheykj4sqp'lw `badc4ec4f4c4ghFt'Wijmykj4s'pqlw `badc4ec4f4c4ghFrt4n¢pWiqwitoykj4sqpirt4'pWiWint4w `badc4ec4f4c4gh¥j4s'pPyfu'p4p'¢jnlw `badc4ec4f4c4gh8uw'p4pq¢jmyrqp4wjiqp'l4w `badsh4hetuwh¥j4s'pdv'wvxpyxz4s4h4s8{W|4c'}¢~tw|4c4z' `badsh4hetuwh¥j4s'p¥iqpWqtwzF{twf4ettcz' `badc4ec4f4c4ghyqp4WjWiqpeyrxzs4h4sbru'p4p'¢jnl `badsh4hetuwhyqp4WjWiqpv'tvp¥xz4s4h4s{|c'}¢~tt|c4z' `badsh4hetuwhyqp4WjWiqpdiqpqttz{ttfettc4zq Îa§¢ik@ö¡6¹¦Ìuf¦²dk¢i¡@£68k¢i6imk `wiqpqrts'uwvxrp `qrt4n¢pWiqwit' `wjs'pdv'tvxpWxtt|q¢ `Wqp4WjWiqpv'wvxptuw4gquWc4f¢ `wjs'pv'tvphqw4¢   `4j4s'p `4Wqp4wjiqp `Wqp4WjWiqpv'wvxp'wg¢nti4uxt`4wq4p4j4iqp4 `4j4s'p `4qrt4pWiqwit' `4wip'rts'uwvrwp Îa§¢ik@YX¡8µG¶®²`imk@i ²b¡k¢i@¡£Pk¢ik @@@iifB@Ç6£' kkÍ´w¶®@k@àÞGpf©¹¦Ii¢¼p´k@fpß¡iH§mw@k§f@H©°¶`I¶³p¤f£¢qµGfu«§Ium@©q©®iIª5²U´k¶®kp¡U@É§pkpIk8ªÃ@£´@@¼k@££@¢²Ëk¾yk £f¢kò£f£I¥³IfkI©ÊÑª5R@ª´Xp³f5¡)k@kw"×sïk@k¸P@Ø @If«ðf´kûfÜf¢HfÅIIpðf)£B¿fmm¡I¦pf@öi­¸f£mI¤@¥¦Îabb¢£@½££Bif§£´m£¾Ik@¥¦£p¢isi£fRk¶³@¢Á@m¬m¢k@£IpffB²k×­I§´¿@¸akiIp±¶`IÜ©Ä¹­wmfq¬Å¸H@·pf@¦f"¸pX¥¦´k®@¶1@@¸"µG£Îaf½£´ªq´¦§¹)q©·f§@ppIB¢i¡@´@­@§ò´kIp­©£µ¬k´mm@Ii¬´køI8 pk­¶®§msH©f@¼@¢B²d«¦¢f¢i§@ªe@Ç6H²Ë¬øIñÁf¥¦µG´m¥Øf¥Pp¹¦I£¶³¸p§m@¶®sf©ff§@p@¶®´@´w@¹)m´¦@©´Ä£²Ï½§póóp@²É´¥¦@«Iup©@òBf §f@mk@ipGòm@´su¦fkfq@f³²dkÏkË@@@&P£4fuI´¢Ï@qfPfµ£f¢f¬)£¦pdBBfI£f¦a@UI´Ú¦i@Å§Ibi´pI£Ûe@@§²B@mi¥fffÓ´k´ks@y§¹´©©ª  vp-2 SYNSEM  t798  t796  t204 QUANTS  LOCAL  CONTENT  
©©¢ ©£¢!  "$# %&'(©)¢! 0 1 ¥¦£¤©)243 !¨£¢!!¤¤¨§¦5¢6¦7 "$# %&'(9!¨£¢!!¤¤¨§ 1 ¥¦£¤©)243 ¡¤)¤@£¤5¢6¦7 "$# %&'(V¡¤)¤@£¤ 1 ¥¦£¤©)243 £¤W£¨§©5¢6¦7 "$# %&'(R£¤W£¨§© 1 F¤TC¤E(X3 ¥¦£¤©)2¢6¢7 "$# %&'(D¥¦£¤©) 1 £¢!¤Y26¢7 "$# %&'(R£¢!¤Y 1 §¢) ¥¦£`3 W)¡6¢7 "$# %&'(Q§¢) ¥¦£ 1 F¤TC¤E(¦6¢7 "$# %&'(W)¡ 1 F¤TC¤E(¦6¢7  88 "$# ((%¦A¤B¨(9!¨£¢!!¤¤¨§ ! ¤DC¤E(F¤G¢H IPA GE 8 )¤@QC¤E(F¤G¢H IPA GE £¤RA EGSQF¢A &T%¦A E ¥¦¨§©¤URC¤E(F¤G¢H IPA GE @£)¤QC¤E(F¤G¢H IPA GE47 "$# ((%¦A¤B¨(V¡¤)¤@£¤ ¡ ¤DC¤E(F¤G¢H IPA GE 8 £¤RA EGSQF¢A &T%¦A E47 "$# ((%¦A¤B¨(R£¤W£¨§© £ ¤DC¤E(F¤G¢H IPA GE 8 ! ©)¤©C¤E(F¤G¢H IPA GE £¨§¢DC¤E(F¤G¢H IPA GE £¤RA EGSQF¢A &T%¦A E ¥P¤DC¤E(F¢A &T%¦A E ¡¤£¨§C¤E(F¢A &T%¦A E47 "$# ((%¦A¤B¨(D¥¦£¤©) ¥ ¤DC¤E(F¤G¢H IPA GE 8 £¤RA EGSQF¢A &T%¦A E )YY¤£¢!!aC¤E(F¢A &T%¦A E ¡¤£¤W£¡DC¤E(F¢A &T%¦A E47  p ª `7¢µ4Pp³m´¦ 0  0 ²  bDcPd  egfRhieqpsrQtvuxw$yVyRw$vw$yRfRh  4P«£ '$9tR¡wv phyR³ma´¦¦VyR©{h Ux®B¬"£s'Ux¡4«£s"w¡"D£¤ðµ   ¡w£­s£¢·¤hs£¬@¢¡"¡"e§¯h®Bst£Æ£¡"¢s¡"Ç£º­ss£¤m£·U·'¢¡wt¬d¬d«thD±d¾B©     ·"''t¡"©s¢¡"·Us¡w£¬ s¿y´£¬"ÁB¢­s£s¬"¢«)¬dI¶¤h¯°£·½¬d¬d·¬dÀ¡wt¢s¬U¤h¡"£¾s"3«¬Ó1¬dÈ£¡"ª ·¤¶t¢¬U¢¾P¬d¡wÍG¬ds­s¬dsww¢¡"t¤h¡t£¡"«º¾«¢¬d·²Óy±w¡"¢´B¡"Íys£´s©2¢¿y¬d£¥U¤§®wU®¢£¢¤h ¬w¡c©©¾   ¡"ÇwPt¡mp³m£¤´¦Vs¬"©£s¬d£·¡"¢¡w¤hU£§¬Ps¢¡"B­s¾ws¡"«sÁB"%£¤   s­s««w¡"¢£®¬"Y§UÁÆ­ss«Y¢£Ç¡"¬"w4ÁB·w®B­s¡ww'UD¡"×Ñ¡"s­¤hstY£¢¬"¬YÁÀ¬dxÇ¡"¡wt¿Ys¬d3stª¼¡tw¾¢ª¡w­s¬d¬"s£ª«t¡"¿V­'­sw¡"ts£B£¬Us²¤y£y¤gxs¡w¢t¡"B¬w©©  UÅ"Ë«¬d¡"ª U¥¤h£Ç'£¿V£¬"¹wtU¡"«¡"£s¥¡"Uw¶'£Bs­sU@¢¬"  ¡"¤h¡wÛU¡"¤h£¤h£¡w¡w4³Ë4«"ÑswbD¯°s£¬dU¬dttU£¤y¤ms­c«¬­s¬"¬"xU'wUb¹Æx£U04w§U¡"«­s®¢®¢¡"¡"s`£Á¢Á¡wts¬"¡wUx£¡"UU'·U"¬"¡"®p's&I¡"p¡"U'xsme§«£«t®B@¬d«dÚ¢³mTtteg ¡"¬d­³m©"s£·Ár¬d¬"®«d¬"­sHab¡"D®ª¡"´Á¢stsUfR´sUPR­UVytt4®BUs¡w²¸Á¹WH¦VÁ¡"x¡w¦V¡"£ºhi ²¡wV4¢sP¿V·t¬"I¡½ts©{£p©¬"ÆUÈB¬d4¬PU®B£¡weq st$¡"¿V¥ws¡Æ¬ds£x"£B§Yd¬ds¬dU«tUwps ²£¢¢¡"«t"£®BU¢xÙBh£®B¡"e­¬d(E¡"P¤tÆ±ÁI£ªee@¡"rQ¡"¼£Ô¬"£m¬"«tÛtª¿PªÇtt¢Qs­s£4¡"£tv ¢£­s£¿¡"¬dtÁ¡"®B¢sw¾Æpg¤ys¬²'¬@ªx·w²«t²«"tuxfi ¡w§ª¢±£'3IUÊÇ¡"UÙ¸t¤£¬¥w$¿Y4¡wi'¢Å¿VÁU¬4¡w¡wh$¡"UU¡"yV ¡P­sÙB£(E'­sg«¬d£`ªtªºPDxPÆsyR¬d¡"§¢­spjsw'®¡"Çyt¡t£Us£B%UiË¬"s·U¢®¬²Ü«"È·£¶t­sPÁÀ¡w©¹wsx'Ù¬"IUsª¬4¤ÁB'w£¶¢¬"Æ¿V¡tw¥¡w¡w«£ºPs¡1£¿V¿VÙ£4£²Ì¬de¬d"U£"²mssss§«d­s¤R£I£$x"­sÛt¬d¡"¬"¢Ëss¡ww¤¤s4¬"Æ¡wt£«d4rw£ª£À¬"U¡""¬dÆ"®"%²'£·º¡"sT£²r«s«t«"£U¡ws¬d¬d­¬d«t$T4s¿3x³×£(ªrwÓ¯°)¡'£V¡ws«¡"£sÛt«w·Ût(E£¼t¡"t£¢t£UY¡"ª¡""¶UR¢®²yªU'«d¬"»U'xÁ¹ª«£¡w¡"£U¡"Á£¼Áe¡w4'£s£xIVs(U®tÙIUU£¿V¡"'¡eË¬¡"'t¡"£«dxt`3£­ºUw£s¬"Ám¿VU¡m²­À¬"'¤'¡"Á¶¯h"ªU¸e¬"ÇU®Be£¡"U£²Gs«t§x¥¾Æ£·Æ«¬d­ss¤Ut"¡w'ss¡¡y¢¡w¡"¡"U£®w¿Yw¬"7­tUU¬dtBBtt£cUcc¤o±dÁ££¬¬©©©©©©©©©¤²²  ¢¡¤£¦¥¨§© §©!©#"$%¨&(')&(01'%¨©3246587@9BA("$"¨&('A'C&(§#D EF"G'%¨©IHP&('C'C&¦QR%$A(S¤0UTV'%¨©WQXA(¡Y"G`aA("¨©#Sb¡Ydce¡Y¡¤HaS¤©(T f'S¤S¤A(©#©#%¨ %¨ 'C©@'C'©!©!©#¡¤¥¨§§&¦iq©g`$"XDV`P'D`ar3%¨©!A(§©h@"X"¨S¤©!HP©#''C¡Y0tS$&(&Qd'`s¡Yc(©I©&(&(ce0q§A(¡¤¡Y'!"¨SY'T)¡¤¡¤©!%¨Ha£¦§©@'"¨S¤%¨©w'©#QX%¨©hipvx©A(%¨'¡Yce""$¡¤& ©!¡YfiGi¨§Xf ©!¡Y¡Y"¨'Cce&w"$&8£y¡¤i¨i$©&¦fC&¡YSC&¦f `a¡YQdX%$T¦SYA#A(©@'%¨%¨EF©@P!&HPA(f0t©!&¦"©#"q"u"GHPD'V%$©!ACC©#EF©©§C"" 999 C©!©#"qD ©!c(©#"u'b¡Y©#"u'C©!§©#is¡Y"A("sA(i$i$¡¤'¡¤&¦"$A(S¨'C© e'a©#SYiqD¢"u vx¥$"$¡Y &ei¨©0t&¦"u'vt©(D £¨DEFP0t&¦"u'!TeA(QXd@&858e0t&¦"u' ©!'D WA#cA(¡YSYAHaS¤©y0t&(§'%¨©&(`P©!§A'¡Y"¨£sCeC'C©#Qe!A("fHP© ¥$C©#is0t&(§b'%¨©3'C§A("$ §¡¤`$'¡¤&¦"qD¢2%¨©6¥$C©!§b!A("h%¨&8&¦C© 0t&¦"u'IA("$i0t&¦"u'C`aA£(©g0t§&¦QhAi'AHaS¤©ji$¡YC`aSYA#e¡Y"¨£pA(SYS %$A§A( 'C©!§3&(0'%¨©C©#S¤©# 'C©#if0t&¦"u'!DEU'3¡YA(SYC&`P&¦¡k9 Ha£¦¡¤S¤c(©b©#'C"m&l(i¨©!© eC"¨'C©§&(A6l(©#ce¡¤§''C&¥$A(AS¨§l(Ha©!¡¤8'C§HPA&¦§A§i %$f A%$§A(¡Y %d'C©!QX§A`a&(0b''%¨%¨©© 'A§£(©!'@0t&¦"u'!D EF"s'%¨©dnBop¦nrqsto uf'%¨©6i$A'A!A("gHP©3QXA("$¡¤`a¥$SYA'C©#i ¡Y"AmC'A("$i$A§iv'C© e's©#i$¡¤'C&(§`aA("¨©#SwD2%¨©W &¦"u'C©#"u' &(0'%¨©©#i$¡¤'C&(§3§©!`$§©#C©#"u'3'%¨©SYA#(©!§wA("$if©#A(%gSY¡Y"¨© §Ha ©!f S¤`$©w¡¤§''C©#&s%$C©#¡Y'C"¨"u§'£IA(@"$HPCA(©!0t"W©!' §f ©!c(©!'C©©#©#e"i"u'3'!i$D@0t¡k§xyP&¦©!Qz§SY¡Y©#CC"u'''A(C©#"$SYA#S¤i$©#(A ©!'§§¡¤iW!&¦DW"I'C© EUHPe'&#'¡YWw©#i$A(`P¡¤SY'C&¦S¤&&(f§!¡k9T ©(D £¨D|{f¡Y §&¦C&(0t'g}j&(§iqT@H8!¥¨'gA("$i~`aA(C'C©f&(`P©!§C9 A'¡¤&¦"$!D~EF"v&(§i¨©!§'C&pA(i$i$¡¤'¡¤&¦"$A(SYS¤mC`P©!©#i¥¨`v'%¨© 'C'f'CC§§¡¤©#&¦"u&(A(A(!"~§'C"$¥¨"¨©!i$'0t§©!%$¡¤ ¡Yc(§§"¨A(C§©#¡¤'£g`$©#S¤Api'HP§'¡¤'&¦%¨¡Y©!`$¡Y"u"f©#©f"¨§'C"~©#£ &y`$¡Y"$f§¡Y'Qd¡Y¡¤&e%¨"¨'¡¤'©3¡Y`a£gA(©#%dS¤24S©#)'!6QdS¤%$T42©!¡Y58'C©#A @'C"u7@©!S¤f'C¢©!9B§s©#A('C&(3iG'C"$&(§©!ig0w"¨§#0t&(&(DVAf §g'&¦r3A¡YfQd'SY'C"$SV%¨&(&(`a §#©W§H$©S¤Tei~§©!'C''¡Y'©"¨%¨%¨¡¤A(e&¦£©@©"$"j'si'C©!¥¨©cec(0x`ve¥$©#¡¤©&¦')"u"$A(f"e'¡YSY99SD Ci$A(Qd'SY¡¤¡Y¡¤&¦&£¦SY¥$S@"¨A(C%$©#"$©wiA#igQXc(cece©jA¡Y¡¤l(©i¨'Cf©#&i©!@&dHPA('i$%$©"$A¡Yi')A(AeSY'QdD)¡¤A(£¦5C&"¨lhcef©#¡Yi ¡¤8"¨'¥$£pf %$¡¤'C¡¤'¡Y©w'"¨%¨%£y©¡YQd'Ha©!%¨A(c(`a©f©#S¤ls"u©(`$'D 'C§ &d¡YQXf '%¨A¡¤'§©3%v'¡Y'A(Qd%¨¥e©©9 EF"'%¨©e nxsxnxeoqsto u'%¨©i$A'A!A("$"¨&('~HP© ©#i$¡¤'C©#iqD6EF"f`$§A( '¡Y ©'%$¡YQd©#A("$3'%$A'3'%¨©di$A'Ag¡Y 'C§A("$C0t&(§Qd©#ih¡Y"u'C&XA("W2{fg'AHaS¤©wA("$ig'%¨©#"Ii$¡YF9  `aSYA#(©#iw'C&'%¨©¥$C©!§#Di"8¥$QyHP©!§q&(0¨i$¡kyP©!§©#"u'12{f 
In this paper, we discuss a method to screen inconsistencies in ontologies by applying a natural language processing (NLP) technique, especially, those used for word sense disambiguation (WSD). In the database research ﬁeld, it is claimed that queries over target ontologies should play a signiﬁcant role because they represent every aspect of the terms described in each ontology. According to (Calvanese et al., 2001), considering the global and the local ontologies, the terms in the global ontology can be viewed as the query over the local ontology, and the mapping between the global and the local ontologies is given by associating each term in the global ontology with a view. On the other hand, ontology screening systems should be able to take advantage of some popular techniques for WSD, which is supposed to decide the right sense where the target word is used in a speciﬁc context. We present several examples regarding inconsistencies in ontologies with the aid of DAML+OIL notation(DAML+OIL, 2001), and propose that WSD can be one of the promising method to screen such as inconsistencies. 
In this paper we present XtraGen, a XML- and Javabased software system for the ﬂexible, real-time generation of natural language that is easily integrated and used in real-world applications. We describe its grammar formalism and implementation in detail, depict the context of how the system was evaluated and ﬁnally provide an outlook on future work with the system. 
This paper describes the Speech Application Language Tags, or SALT, an XML based spoken dialog standard for multimodal or speech-only applications. A key premise in SALT design is that speech-enabled user interface shares a lot of the design principles and computational requirements with the graphical user interface (GUI). As a result, it is logical to introduce into speech the object-oriented, event-driven model that is known to be flexible and powerful enough in meeting the requirements for realizing sophisticated GUIs. By reusing this rich infrastructure, dialog designers are relieved from having to develop the underlying computing infrastructure and can focus more on the core user interface design issues than on the computer and software engineering details. The paper focuses the discussion on the Web-based distributed computing environment and elaborates how SALT can be used to implement multimodal dialog systems. How advanced dialog effects (e.g., cross-modality reference resolution, implicit confirmation, multimedia synchronization) can be realized in SALT is also discussed. Introduction Multimodal interface allows a human user to interaction with the computer using more than one input methods. GUI, for example, is multimodal because a user can interact with the computer using keyboard, stylus, or pointing  devices. GUI is an immensely successful concept, notably demonstrated by the World Wide Web. Although the relevant technologies for the Internet had long existed, it was not until the adoption of GUI for the Web did we witness a surge on its usage and rapid improvements in Web applications. GUI applications have to address the issues commonly encountered in a goal-oriented dialog system. In other words, GUI applications can be viewed as conducting a dialog with its user in an iconic language. For example, it is very common for an application and its human user to undergo many exchanges before a task is completed. The application therefore must manage the interaction history in order to properly infer user’s intention. The interaction style is mostly system initiative because the user often has to follow the prescribed interaction flow where allowable branches are visualized in graphical icons. Many applications have introduced mixed initiative features such as type-in help or search box. However, user-initiated digressions are often recoverable only if they are anticipated by the application designers. The plan-based dialog theory (Sadek et al 1997, Allen 1995, Cohen et al 1989) suggests that, in order for the mixed initiative dialog to function properly, the computer and the user should be collaborating partners that actively assist each other in planning the dialog flow. An application will be perceived as hard to use if the flow logic is obscure or unnatural to the user and, similarly, the user will feel frustrated if the methods to express intents are too limited. It is widely believed that spoken language can improve the user interface as it provides the user a natural and less restrictive way to express intents and receive feedbacks.  The Speech Application Language Tags (SALT 2002) is a proposed standard for implementing spoken language interfaces. The core of SALT is a collection of objects that enable a software program to listen, speak, and communicate with other components residing on the underlying platform (e.g., discourse manager, other input modalities, telephone interface, etc.). Like their predecessors in the Microsoft Speech Application Interface (SAPI), SALT objects are programming language independent. As a result, SALT objects can be embedded into a HTML or any XML document as the spoken language interface (Wang 2000). Introducing speech capabilities to the Web is not new (Aron 1991, Ly et al 1993, Lau et al 1997). However, it is the utmost design goal of SALT that advanced dialog management techniques (Sneff et al 1998, Rudnicky et al 1999, Lin et al 1999, Wang 1998) can be realized in a straightforward manner in SALT. The rest of the paper is organized as follows. In Sec. 1, we first review the dialog architecture on which the SALT design is based. It is argued that advanced spoken dialog models can be realized using the Web infrastructure. Specifically, various stages of dialog goals can be modeled as Web pages that the user will navigate through. Considerations in flexible dialog designs have direct implications on the XML document structures. How SALT implements these document structures are outlined. In Sec. 2, the XML objects providing spoken language understanding and speech synthesis are described. These objects are designed using the event driven architecture so that they can included in the GUI environment for multimodal interactions. Finally in Sec. 3, we describe how SALT, which is based on XML, utilizes the extensibility of XML to allow new extensions without losing document portability. 
TANG ENYA KONG Computer Aided Translation Unit School of Computer Sciences Universiti Sains Malaysia 11800 PENANG, MALAYSIA enyakong@cs.usm.my  ZAHARIN YUSOFF Computer Aided Translation Unit School of Computer Sciences Universiti Sains Malaysia 11800 PENANG, MALAYSIA zarin@cs.usm.my  ABSTRACT In this paper, a flexible annotation schema called (SSTC) is introduced. In order to describe the correspondence between different languages, we propose a variant of SSTC called synchronous SSTC (S-SSTC). We will also describe how S-SSTC provides the flexibility to treat some of the non-standard cases, which are problematic to other synchronous formalisms. The proposed S-SSTC schema is well suited to describe the correspondence between different languages, in particular, relating a language with its translation in another language (i.e. in Machine Translation). Also it can be used as annotation for translation systems that automatically extract transfer mappings (rules or examples) from bilingual corpora. The S-SSTC is very well suited for the construction of a Bilingual Knowledge Bank (BKB), where the examples are kept in form of S-SSTCs. KEYWORDS: parallel text, Structured String-Tree Correspondence (SSTC), Synchronous SSTC, Bilingual Knowledge Bank (BKB), Tree Bank Annotation Schema.  1. INTRODUCTION There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation. Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences (translations) between layers of representation structures (Rambow & Satta, 1996). In this paper, a flexible annotation schema called Structured String-Tree Correspondence (SSTC) (Boitet & Zaharin, 1988) will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping (correspondence) between these two. The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation. Such correspondence is defined in a way that is able to handle some non-standard cases (e.g. non-projective correspondence). While synchronous systems are becoming more and more popular, there is therefore a great need for formal models of corresponding different levels of representation structures. Existing synchronous systems face a problem of handling, in a computationally attractive way, some non-standard phenomena exist between NLs. Therefore there is a need for a flexible annotation schema to realize  additional power and flexibility in expressing the desired structural correspondences between languages (representation structures). Many problems in Machine Translation (MT), in particular transfer-rules extraction, EBMT, etc., can be expressed via correspondences. We will define a variant of SSTC called synchronous SSTC (S-SSTC). S-SSTC consists of two SSTCs that are related by a synchronization relation. The use of S-SSTC is motivated by the desire to describe not only the correspondence between the text and its representation structure for each language (i.e. SSTC) but also the correspondence between two languages (synchronous correspondence). For instance, between a language and its translation in other language in the case of MT. The S-SSTC will be used to relate expression of a natural language to its associated translation in another language. The interface between the two languages is made precise via a synchronization relation between two SSTCs, which is totally non-directional. In this paper, we will present the proposed S-SSTC – a schema well suited to describe the correspondence between two languages. The synchronous SSTC is flexible and able to handle the non-standard correspondence cases exist between different languages. It can also be used to facilitate automatic extraction of transfer mappings (rules or examples) from bilingual corpora.  2. STRUCTURED STRING-TREE CORRESPONDENCE (SSTC) From the Meaning-Text Theory (MTT)1 point of view, Natural Language (NL) is considered as a correspondence between meanings and texts (Kahane, 2001). The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.  Tree pick up  John  box  the String John picks the box up  Tree S  NP VP  John V  NP  pick up the box String John picks the box up  John  John pick up  the.box  box picks.up the  John the.box  NP John picks.up NP  VP V pick up  the box  Figure 1: The correspondence between the string “he picks the box up” and its representation tree (dependency tree and phrasestructure tree), together with the sub-correspondences between the substrings and subtrees.  In this section, we stress on the fact that in order to describe Natural Language (NL) in a natural manner, three distinct components need to be expressed by the linguistic formalisms; namely, the text, its corresponding abstract linguistic representation and the mapping (correspondence) between these two.  Actually, NL is not only a correspondence between different representation levels, as stressed by MTT postulates, but also a sub-correspondence between them. For instance, between the string in a language and its representation tree structure, it is important to specify the sub-correspondences between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation in NLP. It is well known that many linguistic constructions are not projective (e.g. scrambling, cross serial dependencies, etc.). Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g. non-projective correspondence), see Figure 1. Towards this aim, a flexible annotation structure called Structured String-Tree Correspondence (SSTC) was introduced in Boitet & Zaharin (1988) to record the string of terms, its associated representation structure and the mapping between the two, which is expressed by the sub-correspondences recorded as part of a SSTC.  
Coedition of a natural language text and its representation in some interlingual form seems the best and simplest way to share text revision across languages. For various reasons, UNL graphs are the best candidates in this context. We are developing a prototype where, in the simplest sharing scenario, naive users interact directly with the text in their language (L0), and indirectly with the associated graph. The modified graph is then sent to the UNL-L0 deconverter and the result shown. If is is satisfactory, the errors were probably due to the graph, not to the deconverter, and the graph is sent to deconverters in other languages. Versions in some other languages known by the user may be displayed, so that improvement sharing is visible and encouraging. As new versions are added with appropriate tags and attributes in the original multilingual document, nothing is ever lost, and cooperative working on a document is rendered feasible. On the internal side, liaisons are established between elements of the text and the graph by using broadly available resources such as a L0-English or better a L0-UNL dictionary, a morphosyntactic parser of L0, and a canonical graph2tree transformation. Establishing a "best" correspondence between the "UNL-tree+L0" and the "MS-L0 structure", a lattice, may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible. A central goal of this research is to merge approaches from pivot MT, interactive MT, and multilingual text authoring. Keywords: revision sharing, interlingual representation, text / UNL coedition, multilingual communication  Résumé  La coédition d'un texte en langue naturelle et de sa représentation dans une forme interlingue semble le moyen le meilleur et le plus simple de partager la révision du texte vers plusieurs langues. Pour diverses raisons, les graphes UNL sont les meilleurs candidats dans ce contexte. Nous développons un prototype où, dans le scénario avec partage le plus simple, des utilisateurs "naïfs" interagissent directement avec le texte dans leur langue (L0), et indirectement avec le graphe associé. Le graphe modifié est ensuite envoyé au déconvertisseur UNL-L0 et le résultat est affiché. S'il est satisfaisant, les erreurs étaient probablement dues au graphe et non au déconvertisseur, et le graphe est envoyé aux déconvertisseurs vers d'autres langues. Les versions dans certaines autres langues connues de l'utilisateur peuvent être affichées, de sorte que le partage de l'amélioration soit visible et encourageant. Comme les nouvelles versions sont ajoutées dans le document multilingue original avec des balises et des attributs appropriés, rien n'est jamais perdu, et le travail coopératif sur un même document est rendu possible. Du côté interne, des liaisons sont établies entre des éléments du texte et du graphe en utilisant des ressources largement disponibles comme un dictionnaire L0-anglais, ou mieux L0-UNL, un analyseur morphosyntaxique de L0, et une transformation canonique de graphe UNL à arbre. On peut établir une "meilleure" correspondance entre "l'arbre-UNL+L0" et la "structure MS-L0", une treille, en utilisant le dictionnaire et en cherchant à aligner l'arbre et une trajectoire avec aussi peu que possible de croisements de liaisons. Un but central de cette recherche est de fusionner les approches de la TA par pivot, de la TA interactive, et de la génération multilingue de texte.  Mots-clés: révision partagée, représentation interlingue, coédition texte / UNL, communication multilingue  Introduction Creating and maintaining aligned multilingual documents is a growing necessity. In the current practice, a multilingual document consists in many parallel monolingual files, which may be technical documentation as well as help files, message files, or simply thematic information put on the web and intended for a multilingual audience (medicine, cooking, travel…). The task is difficult even for a document managed in a centralized manner. Ususally, it is first created in a unique source language, and translated into several target languages. There must be a way to keep trak of  modifications, possibly done at various places on different linguistic versions. From time to time, somebody has to decide which modifications to integrate in the next release of the document. For that, modifications done in target languages have to be translated back into the source language. The new and the old source versions are then compared using (fuzzy) matching techniques, so that only really new segments are sent for translation. The problem arises even more if the documents are not managed centrally, so that the monolingual files are often in various formats (Word, EgWord, Interleaf, FileMaker, DBMS formats, etc.).  A. Assimi [1, 2] has shown how to "realign" parallel decentralized documents and apply the methodology sketched above. However, in both cases, human translators have to retranslate the modified or new source segments, or to revise them if they are retranslated by a quality MT system. Contrary to what is often said, quality MT exists, but for specific contexts only. (See [14]). What we would like to do is to make it possible to share the revision work across languages, whatever the domain and the context. It is clearly impossible to reflect changes on a file in language L0 into files in L1,… Ln automatically and faithfully, without any intermediate structure to bridge the gap, because that would necessitate at least a perfect fine-grained aligner in case of changing articles or common nouns (provided the gender and number stay the ame in each Li version). In case of replacing a verb by another with a different valency frame in a target Li, the sentence in Li would have to be reanalyzed, transformed accordingly, and regenerated without introducing any new error or imprecision, thereby keeping the manual improvements coming from previous manual revisions. Or we would need a more than perfect MT system, namely one which would be able to analyze the changed utterance in L0, and to transfer and generate it into a sentence of Li as close as possible as the previous sentence in Li, which again could have been improved manually before. The best and simplest way to go seems to use some formalized interlingua IL and to (1) reflect the modifications from L0 to the IL, (2) regenerate into L1,… Ln from the IL. We should also allow for direct manual improvements, considering that the IL form will not always be present, or not always improvable enough for lack of expressivity, or that generators will never be perfect. We choose UNL [3, 4, 10, 11] as our IL of choice for various reasons: (1) it is specifically designed for linguistic and semantic machine processing, (2) it derives with many improvements from H.Uchida's pivot used in ATLAS-II (Fujitsu) [13], still evaluated as the best quality MT system for English-Japanese, with a large coverage (586,000 lexical entries in each language), (3) participants of the UNL project1 have built "deconverters" from UNL into about 12 languages, and at least the Arabic, Indonesian, Italian, French, Russian, Spanish, and Thai 
English-Thai MT systems are nowadays restricted by incomplete vocabularies and translation knowledge. Users must consequently accept only one translation result that is sometimes semantically divergent or ungrammatical. With the according reason, we propose novel Internet-based translation assistant software in order to facilitate document translation from English to Thai. In this project, we utilize the structural transfer model as the mechanism. This project diﬀers from current English-Thai MT systems in the aspects that it empowers the users to manually select the most appropriate translation from every possibility and to manually train new translation rules to the system if it is necessary. With the applied model, we overcome four translation problems—lexicon rearrangement, structural ambiguity, phrase translation, and classiﬁer generation. Finally, we started the system evaluation with 322 randomly selected sentences on the Future Magazine bilingual corpus and the system yielded 59.87% and 83.08% translation accuracy for the best case and the worse case based on 90.1% average precision of the parser. Introduction Information comprehension of Thai people should not be only limited in Thai; in contrast, it should also include a considerably large amount of information sources from foreign countries. Insuﬃcient basic language knowl-  edge, a result of inadequate distribution in the past, conversely, is the major obstruction for information comprehension. There are presently several English-Thai MT systems—for instance, Parsit (Sornlertlamvanich, 2000), Plae Thai, and AgentDict. The ﬁrst one applies semantic transfer model via the methodology similar to the lexical functional grammar (Kaplan et al., 1989) and it is develop with the intention of public use. The latter two implicitly apply the direct transfer model with the purpose of commercial use. Nonetheless, by limited vocabularies and translation rules, the users must accept the only one translation result that is occasionally semantically divergent or ungrammatical. Due to the according reason, we initiated this project in order to relieve language problem of Thai people. In this project, we develop a semi-automatic translation system to assist them to translate English documents into Thai. For this paper, the term semi-automatic translation means the sentence translation with user interaction to manually resolve structural and semantic ambiguities during translation period. Despite manual disambiguation, we provided a simple statistical disambiguation in order to pre-select the most possible translation for each source language sentence, though. The automatic semantic disambiguation can be thus excluded with this approach.  
This presentation describes an examplebased English-Japanese machine translation system in which an abstract linguistic representation layer is used to extract and store bilingual translation knowledge, transfer patterns between languages, and generate output strings. Abstraction permits structural neutralizations that facilitate learning of translation examples across languages with radically different surface structure characteristics, and allows MT development to proceed within a largely languageindependent NLP architecture. Comparative evaluation indicates that after training in a domain the English-Japanese system is statistically indistinguishable from a non-customized commercially available MT system in the same domain. Introduction In the wake of the pioneering work of Nagao (1984), Brown et al. (1990) and Sato and Nagao (1990), Machine Translation (MT) research has increasingly focused on the issue of how to acquire translation knowledge from aligned parallel texts. While much of this research effort has focused on acquisition of correspondences between individual lexical items or between unstructured strings of words, closer attention has begun to be paid to the learning of structured phrasal units: Yamamoto and Matsumoto (2000), for example, describe a method for automatically extracting correspondences between dependency relations in Japanese and English. Similarly, Imamura (2001a, 2001b) seeks to match corresponding Japanese and English phrases containing  information about hierarchical structures, including partially completed parses. Yamamoto and Matsumoto (2000) explicitly assume that dependency relations between words will generally be preserved across languages. However, when languages are as different as Japanese and English with respect to their syntactic and informational structures, grammatical or dependency relations may not always be preserved: the English sentence “the network failed” has quite a different grammatical structure from its Japanese translation equivalent ネットワークに障害が 発生した ‘a defect arose in the network.’ One issue for example-based MT, then, is to capture systematic divergences through generic learning applicable to multiple language pairs. In this presentation we describe the MSR-MT English-Japanese system, an example-based MT system that learns structured phrase-sized translation units. Unlike the systems discussed in Yamamoto and Matsumoto (2000) and Imamura (2001a, 2001b), MSR-MT places the locus of translation knowledge acquisition at a greater level of abstraction than surface relations, pushing it into a semanticallymotivated layer called LOGICAL FORM (LF) (Heidorn 2000; Campbell & Suzuki 2002a, 2002b). Abstraction has the effect of neutralizing (or at least minimizing) differences in word order and syntactic structure, so that mappings between structural relations associated with lexical items can readily be acquired within a general MT architecture. In Section 1 below, we present an overview of the characteristics of the system, with special reference to English-Japanese MT. Section 2 discusses a class of structures learned through  phrase alignment, Section 3 presents the results of comparative evaluation, and Section 4 some factors that contributed to the evaluation results. Section 5 addresses directions for future work.  
This paper proposes machine learning techniques, which help disambiguate word meaning. These methods focus on considering the relationship between a word and its surroundings, described as context information in the paper. Context information is produced from rule-based translation such as part-ofspeech tags, semantic concept, case relations and so on. To automatically extract the context information, we apply machine learning algorithms which are C4.5, C4.5rule and RIPPER. In this paper, we test on ParSit, which is an interlingual-based machine translation for English to Thai. To evaluate our approach, an verb-to-be is selected because it has increased in frequency and it is quite difficult to be translated into Thai by using only linguistic rules. The result shows that the accuracy of C4.5, C4.5rule and RIPPER are 77.7%, 73.1% and 76.1% respectively whereas ParSit give accuracy only 48%. Introduction Machine translation has been developed for many decades. Many approaches have been proposed such as rule-based, statistic-based [5], and example-based approaches [3, 6, 11]. However, there is no machine learning technique that meets human’s requirement. Each technique has its own advantages and disadvantages. Statistic-based, example-based and corpus-based approaches were recently proposed. A rulebased approach is the first strategy pursued by research in the field of machine translation. Rules are written from linguistic knowledge by human. The strength is that it can deeply analyze in both syntax and semantic levels. However, the  weak points of this model are 1) it requires much linguistic knowledge. 2) it is impossible to write rules that cover all a language. In many years ago, a statistic-based and an example-based were proposed. These approaches do not require linguistic knowledge, but they need large size of bilingual corpus. A statistic-based approach uses statistic of bilingual corpus and language model. The advantage is that it may be able to produce suitable translations even if a given sentence is not similar to any sentences in a training corpus. In contrast, an example-based can produce appropriate translations in case of a given sentence must similar to any sentences in a training data. Nevertheless, a statistic-based approach cannot translate idioms and phrases that reflect long-distance dependency. To improve quality of a rule-based machine translation, we have to modify/add some generation rules or analysis rules. This method requires much linguistic knowledge and we cannot guarantee that accuracy will be better. For example, in case of modifying some rules, it does not only change incorrect sentences to correct sentences furthermore they may effect on correct sentences too. The common errors of machine translation can be classified into two main groups. One is choosing incorrect meaning and the other is incorrect ordering. In our experiments, we select ParSit in evaluation. ParSit is English-to-Thai machine translation by using an interlingual-based approach [8]. An interlingual-based approach is a kind of rulebased machine translation. The statistics of incorrect meaning and incorrect ordering in ParSit are 81.74% and 18.26% respectively. Therefore, in this paper, we address on choosing a correct meaning. We use context information,  words and part-of-speech tags, in classifying the correct meaning. This paper, we apply machine learning algorithms, C4.5, C4.5rule, and RIPPER, to automatically extract words and part-of-speech tags. 1. A Rule-Based Approach: Case Study ParSit: English to Thai Machine Translation. In this section, we will briefly describe a rulebased machine translation. Each rule-based machine translation has its own mythology in translation. Hence in this paper, we select ParSit as a case study. ParSit is English to Thai machine translation using an interlingual-based approach. ParSit consists of four modules that are a syntax analysis module, a semantic analysis module, a semantic generation module, and a syntax generation module. An example of ParSit translation is shown in figure 1. In figure 1, the English sentence, “We develop a computer system for sentence translation.”, input into ParSit. Both syntax and semantic analysis modules analyze the sentence and then transform into the interlingual tree which is shown in Figure 1. In the interlingual tree shows the relationship between words such as 1) “We” is an agent of “develop” 2) “system” is an object of “develop” 3) “computer” is modifier of “system” and so on. Finally, Thai 
This paper presents a method to resolve word sense ambiguity in a Korean-to-Japanese machine translation system using neural networks. The execution of our neural network model is based on the concept codes of a thesaurus. Most previous word sense disambiguation approaches based on neural networks have limitations due to their huge feature set size. By contrast, we reduce the number of features of the network to a practical size by using concept codes as features rather than the lexical words themselves. Introduction Korean-to-Japanese machine translation (MT) employs a direct MT strategy, where a Korean homograph may be translated into a different Japanese equivalent depending on which sense is used in a given context. Thus, word sense disambiguation (WSD) is essential to the selection of an appropriate Japanese target word. Much research on word sense disambiguation has revealed that several different types of information can contribute to the resolution of lexical ambiguity. These include surrounding words (an unordered set of words surrounding a target word), local collocations (a short sequence of words near a target word, taking word order into account), syntactic relations (selectional restrictions), parts of speech, morphological forms, etc (McRoy, 1992, Ng and Zelle, 1997). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al., 1985, Gallant, 1991, Leacock et al.,  1993, and Mooney, 1996). Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications. We propose a word sense disambiguation method that combines both the neural net-based approach and the work of Li et al (2000), especially focusing on the practicality of the method for application to real world MT systems. To reduce the number of input features of neural networks to a practical size, we use concept codes of a thesaurus as features. In this paper, Yale Romanization is used to represent Korean expressions. 
 The most difficult task in machine translation is the elimination of ambiguity in human languages. A certain word in English as well as Vietnamese often has different meanings which depend on their syntactical position in the sentence and the actual context. In order to solve this ambiguation, formerly, people used to resort to many hand-coded rules. Nevertheless, manually building these rules is a time-consuming and exhausting task. So, we suggest an automatic method to solve the above-mentioned problem by using semantically tagged corpus. In this paper, we mainly present building a semantically tagged bilingual corpus to word sense disambiguation (WSD) in English texts. To assign semantic tags, we have taken advantage of bilingual texts via word alignments with semantic class names of LLOCE (Longman Lexicon of Contemporary English). So far, we have built 5,000,000-word bilingual corpus in which 1,000,000 words have been semantically annotated with the accuracy of 70%. We have evaluated our result of semantic tagging by comparing with SEMCOR on SUSANNE part of our corpus. This semantically annotated corpus will be used to extract disambiguation rules automatically by TBL (Transformationbased Learning) method. These rules will be manually revised before being applied to the WSD module in the English-to-Vietnamese Translation (EVT) system.  
Information on subcategorization and selectional restrictions is very important for natural language processing in tasks such as monolingual parsing, accurate rule-based machine translation and automatic summarization. However, adding this detailed information to a valency dictionary is both time consuming and costly. In this paper we present a method of assigning valency information and selectional restrictions to entries in a bilingual dictionary, based on information in an existing valency dictionary. The method is based on two assumptions: words with similar meaning have similar subcategorization frames and selectional restrictions; and words with the same translations have similar meanings. Based on these assumptions, new valency entries are constructed for words in a plain bilingual dictionary, using entries with similar source-language meaning and the same target-language translations. We evaluate the eﬀects of various measures of similarity. 
We describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results. The system learns lexico-structural transfer rules using syntactic pattern matching, statistical co-occurrence and errordriven ﬁltering. In an experiment with domainspeciﬁc Korean to English translation, the approach yielded substantial improvements over three baseline systems. 
Example-based machine translation (EBMT) is based on a bilingual corpus. In EBMT, sentences similar to an input sentence are retrieved from a bilingual corpus and then output is generated from translations of similar sentences. Therefore, a similarity measure between the input sentence and each sentence in the bilingual corpus is important for EBMT. If some similar sentences are missed from retrieval, the quality of translations drops. In this paper, we describe a method to acquire synonymous expressions from a bilingual corpus and utilize them to expand retrieval of similar sentences. Synonymous expressions are acquired from dierences in synonymous sentences. Synonymous sentences are clustered by the equivalence of translations. Our method has the advantage of not relying on rich linguistic knowledge, such as sentence structure and dictionaries. We demonstrate the eect on applying our method to a simple EBMT. 
 TL)[1]. The interlingua approach [2,3], a methodology of constructing an intermediate language, is a dominant approach in standalone system to support multi-language. Many products such as, SYSTRAN [4], BESTILAND [5], are implemented using this approach. Interlingua approach is helpful for a central server, but it is difficult to complete concepts in Interlingua. The rapid growth of Internet Technology, especially user friendliness approach, helps increase the population of users who access the Internet and the amount of information in the cyberspace. With the increasing amount of online information and the rapid growth of non-English speaking Internet hosts, it is becoming increasingly important to offer users universal access to valuable information resources in different languages. The European Multilingual Information Retrieval (EMIR) project [6], the MULINEX project[7], the TwentyOne project[8], and the cross-language retrieval track in TREC[9] conference all reflect people’s interest in  providing interoperability among different own unique set of characters. In terms of  language processing environments and grammar, some (Thai, Laotian, Japanese,  multilingual information retrieval.  Chinese, etc.) do not indicate word boundary,  Distributed system technology plays an some (Thai, Laotian, etc.) do not inflect while  important role to enable us to manage others(Japanese, Korean, etc.) provide  information from various places. This makes particles to indicate the word grammatical  it unnecessary to access only the central function, some are not distinguishable  server. It helps machine translation between sentences and phrases, etc. These are  developers to work individually. Yasuhara the basic difficulties that interest the  [10] wrote that many machine translation researchers in the field of machine translation  systems were developed, especially from local and the application.  language to English, and the language has an  Due to these varieties, it is difficult to build  important role as an intermediate language. an MT system that supports all languages  Our paper tries to apply a distributed taking into account of cost, quantity, and time  technique by using English language, which is consumption. Cross system machine  mostly used by non-English speakers as a translation approach is, therefore, an essential  second language to be an intermediate concept that helps reduce these problems by  language. Our approach is not aimed to show reusing the large amount of information  that it is better than the interlingua approach, existing in Internet.  but it is another solution for us to use existing  Figure 1 shows an idea of our cross system  resources in cyberspace. We hope that it is machine translation approach. Since the  possible to help developers build the  machine translation that will support  Representation Extraction Retrieval Summarization  MT  Mining  Visualization  all languages taking into account of  cost, quantity, and time consumption. In section 2, we show cross system MT approach. In section 3, an example of our approach is given. In section 4 we illustrate drawbacks of this technique and give an example about how to examine these problems.  Text Processing Common Platform  Chinese Language Processing  e-Content  Dictionary  Japanese Language Processing  e-Content  Dictionary  French Language Processing e-Content Dictionary  Korean Language Processing e-Content Dictionary  MT  MT  MT  MT  English  …… Language Processing  Language Processing  MT  e-Content  Dictionary  e-Content Dictionary  …… Language Processing MT  e-Content  Dictionary  2. Cross System MT The major significance of Asian languages is the variation of languages in the region; most of which use their  MT  MT  MT  MT  Myanmar Language Processing  e-Content  Dictionary  Vietnam Language Processing  e-Content  Dictionary  Indonesia Language Processing e-Content Dictionary  Thai Language Processing e-Content Dictionary  Figure 1.Cross System Architecture  technology of building MT can be transferred from us to other countries in this region and  When a user starts to search by input a keyword in Thai, such as a word “คอมพิวเตอร  we know that English is broadly used as a bridge to communicate among different  [kom pyu ter]”(step1). The word “คอมพวิ เตอร”will be sent to the ThaiÆEnglish  languages. It is simpler for a local developer MT system to translate into  to build an MT system from his/her local language to English (L1ÅÆE). If all countries have their own LnÅÆE MT system, sharing English as an intermediate  “computer”(step2). The word “computer”  will be sent to the EnglishÆJapanese MT  ̺ ͢ ͏ ͚ V system to translate into “  ̀ ̺͢͏͚V̀ ”(step3). The word “  ” will  representation language reduces problems be used as a keyword to search for Japanese  shown above. Moreover, there are many web pages by a Japanese search engine  different ways to develop a MT system. Our (step4). The result of Japanese web pages  approach is to encapsulate the type differences among MT systems. Thus we can decrease the  from the search engine will be sent to Japanese Æ English MT system to translate  gap among languages by connecting the MT into English web pages (step 5). The result of  system of each local language. Our cross system MT also offers a good  English homepages will again be sent to English Æ Thai MT system to translate into  infrastructure for many future applications Thai pages (step 6). Finally, the output of the  such as e-commerce, digital archive, e-publishing, and so on as shown in figure 1.  workgroup is web pages that contain the keyword“คอมพวิ เตอร” (step 7). These web pages  Next we show an example of the usage of a are selected from Japanese web pages.  cross system MT.  This approach helps us to develop a MT  system that supports all languages taking into  3. A Usage Sample of  Cross System MT This chapter shows an application of using our cross system MT. Figure 2 shows an example of our expected application tool  คอมพิวเตอร  คอมพิวเตอรÆ Computer  ComputerÆƪ*Ƭ**ƫ  TEÅÆET System 2  JEÅÆEJ System  
The translation choice and transfer modules in an English Chinese machine translation system are introduced. The translation choice is realized on basis of a grammar tree and takes the context as a word bag, with the lexicon and POS tag information as context features. The Bayes minimal error probability is taken as the evaluation function of the candidate translation. The rule-based transfer and generation module takes the parsing tree as the input and operates on the information of POS tag, semantics or even the lexicon. Introduction Machine translation is urgently needed to get away with the language barrier between different nations. The task of machine translation is to realize mapping from one language to another. At present there are three main methods for machine translation systems [Zhao 2000]: 1) pattern/rule based systems: production rules compose the main body of the knowledge base. The rules or patterns are often manually written or automatically acquired from training corpus; 2) example based method. The knowledge base is a bilingual corpus of source slices S’ and their translations T’ Given a source slice of input S, match S with the source slices and choose the most similar as the translation or get the translation from it. 3) Statistics based method: it is a method based on monolingual language model and bilingual language model. The probabilities are acquired from large-scale (bilingual) corpora. Machine translation is more than a manipulation of one natural language (e.g. Chinese). Not only the grammatical and semantic characteristics of the source language  must be considered, but also those of the target language. To sum up, the characteristics of bilingual translation is the essence of a machine translation system. A machine translation system usually includes 3 sub-systems [Zhao 1999] ： (1) Analysis: to analyse the source language sentence and generate a syntactic tree with syntactic functional tags; (2) Transfer: map a source parsing tree into a target language parsing tree; (3) Generation: generate the target language sentence according to the target language syntactic tree. The MTS2000 system developed in Harbin Institute of Technology is a bi-directional machine translation system based on a combination of stochastic and rule-based methods. Figure 1 shows the flow of the system. Input English Sentence Morphology Analysis Syntactic Analysis Word Translation Choice Transfer and Generation Output Chinese Sentence Figure 1 Flowchart of MTS2000 System Analysis and transfer are separated in the architecture of the MTS2000 system. This modularisation is helpful to the integration of stochastic method and the rule based method. New techniques are easier to be integrated into the modularised system. Two modules implement the transfer step and the generation step after analysis of the source sentence. The specific task of transfer and generation is to  produce a target language sentence given the source language syntactic tree. In details, given an English syntactic tree (e.g. S[PP[ In/IN BNP[our/PRP$ workshop/NN]] BNP[ there/EX] VP[ is/VBZ NP[ no/DT NP[ NN[ machine/NN tool/NN] SBAR[ but/CC VP[ is/VBZ made/VBN PP[ in/IN BNP[ China/NNP ]]]]]]]]), using knowledge sources such as grammatical features, simple semantic features, construct a Chinese syntactic tree, whose terminal nodes compromise in sequence the Chinese translation. The input sentence are analysed using the morphology analyser, part-of-speech tagger, and syntactic analyser. After these steps, a syntactic parsing tree is obtained which has multiple levels with functional tags [Meng 2000]. Followed is the parser flow:  Input Sentence  Statistics Knowledge  POS Tagger PPA Resolution  Manual Rule Base  Layered Parsing  Parsing Tree  Figure 2. Parser based on Hybrid Methods At present, our English parser is able to generate syntactic tree in comparative usable way. The English parsing tree, with the basic information about relationship among the nodes in the source sentence, also with semantic information of the nodes, is input to the module of transfer and generation. The information of the nodes is the starting point of transfer and generation. After syntactic parsing, the task of transfer and generation includes word translation choice of ambiguous words, word order adjustment and insertion/deletion of some functional words. Transfer and generation are implemented using two modules: one is for word translation choice, the other for structure transfer and translation modification. 
In the paper we report a qualitative evaluation of the performance of a dependency analyser of Italian that runs in both a nonlexicalised and a lexicalised mode. Results shed light on the contribution of types of lexical information to parsing. Introduction It is widely assumed that rich computational lexicons form a fundamental component of reliable parsing architectures and that lexical information can only have beneficial effects on parsing. Since the beginning of work on broadcoverage parsing (Jensen 1988a, 1988b), the key issue has been how to make effective use of lexical information. In this paper we put these assumptions to the test by addressing the following questions: to what extent should a lexicon be trusted for parsing? What is the neat contribution of lexical information to overall parse success? We present here the results of a preliminary evaluation of the interplay between lexical and grammatical information in parsing Italian using a robust parsing system based on an incremental approach to shallow syntactic analysis. The system can run in both a non-lexicalised and a lexicalised mode. Careful analysis of the results shows that contribution of lexical information to parse success is more selective than commonly assumed, thus raising the parallel issues of how to promote a more effective integration between  parsers and lexicons and how to develop better lexicons for parsing. 
The grammar matrix is an open-source starter-kit for the development of broadcoverage HPSGs. By using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar engineering, evaluation, parsing and generation, it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding. 
We report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English, French, German, Japanese, Norwegian, and Urdu.1 
We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components, especially in a wide-coverage analysis system. Given the architecture of our MT system, which is a transfer system based on linguistic modules, correct analysis is expected to be a prerequisite for correct translation, suggesting a correlation between the two, given relatively mature transfer and generation components. We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis. We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages. 
f£g¡h$iqpsr£hFt'u¡vxwy&wp$t@hFiw¦¥hFit@pFvx&w#Iw1i1)xhF7Fw£ws¥qD1 ¦d wqeFfgf7pFi1pDxhFtjikpI7u¡i1w£w1¡y&wlp7nmr d opsrIq r  pF@¥qD y d hFmh d pF¡yw£r£w¡w1¡y&Fr1xyw£sPpT¨tuhIhvprw@y&h'psx r i1w1uyh$iy yhzy d wo{}|Df¦~q  r zq ~Tw1m¡i1hFu!w£pFPf7i1hF&wr£yi d w1i1wBy d w hynxhF@hF¦¡w£r1iuy)xwt@w£ypFvxpFhFm¡phwFyhD¡pV  ¦f7i1hFuywtiynxw£i¦w1i1w wu¡vxrtxy&wtvx¡y)i1hDDmr£w£ 4g¡hFiou¡m¡v¡vx d wi¦hFiwyl&ww¥  rIr 1 ¥q r xD1cg¡hFiBpDywtiwtuyv¡myiwtvxpy&wPpFu¡u¡i1hpr d 7w£wp rIr  pFo¥¦vxpD7i d xr d iwtt@pF@hFi1pFt't@pFi7hFixw1¡y&w¡¡   hy d w1i¦vxpVIw1iF¨w£Iu¡i1w£)hlp'w1t@pF¡y)xr¦i1w1u¡i1w£w1¡ypy)xh$yFr£pF v!wpI¡¡w£!v¡my¦xy¦xhy¦Dx&rtm&w£oy d x¦up$uyw1iF  Ò7ôm«bòH¤«b¨)±¥b­¦¾¬«Ã¾é5±¢¦¤¤¾¥Ã¨¨)¢¦¾(«b§Û$Û$¹¹)§×x§¹)¹§§§`¥b±5¥j£x¨'¨'£x§§³W©`¤¿±©`§§½R½tÓ¥`§¥$ªE5¨)¾¾¨¸Wb½{«`©`©`§£W£W£W§¾ªm«b«h«bÀ¨'±W§z¥b«b«bn¨iªW¨R«£«v¦W¨i×W¶¯t¥Ç¿¨)±5±5¼HÍF¶©b¤¤¹b¤«`Û$§~±W¤¿¤¸W¨(¤¨«Ã¤sÐ³W¹¨ð¼t¤ª¨©´¥bªm¨)¨)E5¤«`©`Ó'©`¦¦±5¥j£f¹`¨v¥ó½f§¾¨§¼µË)¾1ÄÂ¨³WªW¹¨(±¨v©`Ô¤~£W¨)¨)¥¨½f¥R¨'Ëv§£¦PÌØ±5¤¼Â¹ª1­¦¨)¨)é5«bÔ1¾¾¨¤¿¤~±5¯(ë{÷½Q¥bªªm¤¿·RC¦¶³Wï)§¨)¥¨¶§¥`¤¿±W©b©bÛ$©¤¿¤§¹)¹)¨´¤Ômêj¥`ÄÔ¦«b¶¸W§Ô£W¹Ø©b¹`À«b©b¼t«`C«`¤¥8«b¸WÄ«`±Wª¹)³µ§§¼t¨v¥b±¼t©`£m§¥b©`§¾¨§ÖÄÆ£©`¤¨¨§§«b¤~¨)¤~±¨'±½{¨vÛ$¨)±5§¤¿¥b¾°¼h¶mªQ­¦¼tªW¨'¨'¤«`¦x¨)±5§"©b¨¥¦£W±5¶§©bªª§ï)¼¨¶õpªt¯t«¾¨¨)¶§¥v¥j¶p©b¥b©`±5¨)¦«`¨)·¨ÖW©b±©«(¨"Ô¾¤¾¤~ÖW¥W«`¯HÀb«b¥b¤¥bªm±¨'±5¨)ÔÍFÄÖ¤~¶¦¨)ªt¤ªx©`¸W¥b¹)ªW«b¨)ª¶¸W­¦ù¦¤«`¨'¨'¨5¶Äªm«iE5©b­¦§¾¤¦¥`¥`§¹¨5¯¾«`ômë{¨±¨§«`À·(©b¨¦¨)¨)¤§¥v«8¨¥b¤¨'±Ô¯¨vÛ2¤ªÌD¨(¼µ©b¤o©b¥bªtÔ¾«`Ôp¨)¶¿«`«¾ëf¤¨'±¸xÀbÔÕª«b¾À©Ã¤¼ÖW±5¤£(¼H±W¥`¤ªW±§5ë{¨)¤¨)ëV¼°©b§·5§ËR¹)©±5ªWÄÕ£«`½§¨)¼t¨)ÖWÀb°@«§«b¤ª©5«`¨5·5¸W¦m½t¼t¼Wé5·ï6¤£W¹Àj©"¥v¹)2¥¨·¨)¾«b±ª¨'¥b¤±été5¤~Û$ï)¹)Ä±ÄÖm»»À(§¤«¦¨'¨'±¼µÔtïØ¶º¨)¨(Cxª¨£x¾·m±5«`©vÔCª«bÄ§¤~©8¦mÍF§¼¤§¾©bªm¶W©b¥¤©bª¥vï)¨v¤~¤Õëf¥¨ªWªW¥Äÿ±5©bªWÄ¦E5¥Ãªmªÿ¨5¶¤¤¿«`Öm±§¤·5ÔÔ­¦¨v¥b¼t¤¿¼p§5¨)¸W¨§§«bÀj©Ã·¶¸W±ÌD±Ùp­¦§Ä¼t§¼µ¸W¹)¤~¦m§¨'§5Ä©«b¯°ëf¨Öm·m¨)£W¹`¼µÏ¦Ä(ÖW¢¦«b¨)·m©`£WÙÚ©b«®Öm¥«b¥`§¸W£m¨)¤§Ëx¶£¥2ª ÖxëV¹`£¨)ªt§C¤~¤¥v¨)¥6ï'Ä ¨)¨PªW¤¢¶¥bÄ¤¿«`ªtÖªQ¥jÐbb£f¨¨ªt£W¤~ªm©`§¢¦±§¥b¨«bÔt±¾ÔpÄ©¦¹`¤±W«bªx§£é5ôm¨vÑ´¥b«b«`±5ªÿ«b¸W§¶ë«8¨E5¼t£¶~ªhb£x½qÖt¨)§¹Òµ§Ívªm¤§£W¯ÿ¾¨v¤G¶§Ö¤¨'©b±5ªW¥b±5¶¿Ð'¨)ôÔ5¨'¤¿«`«Õ¨e¯¦ïÕÛ$¥`©b§¨)¶§Ó¥i¼¨(¥C§¤¿Í¦¥b¾±¨(ªv§ë{ÐbªW¥"¨¦©b¸W¸W¾¨¥b©b¤Èv¹¨)±5¶¿Ð)©`«b§¤¨)©`±©`Ñ¤½$«j¶«`¸W¸W§C¨§Àã©b¤«b¨Õ¤~Ëc£WÌf§©`§¥R¤¤¾¨Ö«(±5¤§ét¯±W¨v½tØ»ª«j§±5¥¤«b¤º«j¯Ç¿¾¨¶¶©b«b¨(¥`À(¾¨¨©b¦±5¼¯h¼µ©b¦m¨)¤Ó'b¡¥`¨¼t¯¤¤Äï"§©`¥s£WÛ$­¦ªxCt¥bÓV±±µ¨)¨C±5ªm©`Û$±¤¦xª(ËV¨R¨)¨)¨'§±5¤¸Wd¨'£mªW­8½R©±·m×W¦m©b±5¼µ&¹)ªW3YªW§¤±©`¼µªt©bÍFÛ$¶~©b©b¤¨©Ë)¾Q¾¨±G6«bR­¦±5·(Ô$§§¥Õªx©`¨)©`©`Ô«`§¨)©b¨)½"E¥±¨·«¥8³W«b½{Äb£h©`¦m¸W±5¥v¤~¶¿¨¥«©¦¶ét¨vQT¨)¹Ô5¤¼Q¶Ì8¦m¤©`¤~ÈW§¨¨'¦W¨v±5Ä$§¥ §ôm¤8ÍV±Ô¨)«¾°¶»ªx¨'§¥©b¤ÑDÖtª`¥©bSV¶¼¨'Ä¨RÔq¦«`E¥b©bª«`¤¥`¸W¤½2±5©b¨)Û$±·ÈW«b¨)§·ó(d5Ôt¥`¨"UH«`¯©b¨)«b¦¨'±(¥¥`Ì¨«b½f¨)bg©`¸W©v±5­¦¥`¸W±¥ñ¥Û$Ä«bÓjÓI¥£W©b¥ÄT§¸W¥b©bÙp¨´¤¥v¨v©b¥¦«`Û$Ô8eÉ¤¥C©`Ë´±¨)ÙÚ¦ÖWÖW¥b±5¨´­¦¥$³µ¨)¤«`&ÖW©bÔ¨)¹£W¨&¨)§¨Ô1©¤¨¥§ªW«`§«`³µ¦mG©`¾©b«Ã¤±¥`©`¤~G¤¦x¥Dª(³W¨¤~¥b¤¥£W¨"£W£W¨)¼ÔWªxôm±5¥b¨§¥`§¸W¨v§·(ª«`§ª¨)¦m³mCt«`©b¾¤¿ÖE¥vÈÞ¥`¥b¤¿ô¨¨)¨Q¦©`§§CÈÿ¨v¤~ªm¨©`ª¤©b¾¹±5½{¹¨)¹`¨)§±µ±W§ªxÔgd£±5«b«b¨H§ªmÛ$½{«bëf§¨v©`ªW¼H±5±5¨)¥`¼£Ö±W¼©`¢£¾¨¾£W¸W«`ª¹´¤¥`¹ª(¤¥R±5¥b¤¿¼µ¨)©`¼t¥bÖ©bª¥«b¨v©b©`Àã%$R¨)§"¨)¼t¼t«b¤º¤¿¤¿¨«`¤~¤¾¥«`¤§±§¦m¸mÖtªW±¨¼p¬§¨v¥"§¹$¨)¦W©b©¦ë1±W«j±5¥`k¨)¨)¸W§¨'¤©b«`Ð'­¦m¤ï)¼p95¤¯(½®¹´¯Y¤«b±¹±(«`¨)¯Ä¥(¶¥ª¼t¹)ª±5©b¾¨©`¥ªW¶¿ªW¤~­Õ¹)ÐbÔ¨v¨)«b¨v¨vï1¥v¨)©Ã®#Ô¦¾(¥ã§©`§±­¦¨)«`Û$«`Ñ§¥¦·Â¦x¼P·¥Ã¥Ã±¼f¼¨§Ä©`½{2s¥b±¨R§§¨©R«&£W±W¤½{­¦¶W¯¡ªØï'§§¯t§«b£W©`±ôm«b©bÖ¥W«`©`­8¥b¥Ã³µ1¨)¼t¹)Ä#2W¤±5Ò7«b«b¸W¥ªW£W±5ªm§z¤¤±5¤¾¤¿¥`§ì¥bÇ¿¨)¥bÔ±5¨)©`§ªWªWªW¤Ö¤£x¨¨)¹`¦tÈ5§¦µ³t³tE±¨)¥`±¨)¾¨)¼p©b«j¥j¥ã¥¥v¶¤ª£¶j¼···§¨¨¨¨«©¥É¥«ÀËÀÀÀ¥ÀÀÀÀÀÀ½ÄÔÔÔ  5Ä ë§©b« 5ª ª ï Ô Ô Ô WÔ Ô WÔ 5 s3 6A8B" 8  © B°s±  %  ( 0² s 3H5 6A8 s3@9 s6A8 s3@6  WÄ fë§©b« ªW¨)· R§5¼ ¦ª vï Ô Ô Ô xÔ Ô WÔ 9 s3 xÔ Ô µ´8  ¦3'µ ©  !% 7µ78o"  8  I§F(  7³ t0  ²  ´35 ¡µ78  ´3¶9 y6A8  73@6  êjª§ª±5¦x«b¤¾§¶5¥b¤«`¸m§«`¤~±ªtÔ5Û$©`±5¦¨)©b«`¤¨)¥®§¥b¥`±µ¹´¤º§«b¨v¼«`±«b£x¨  · ¦d wD&yw1t4v!w1w1¸y!¹ihFtay d wlr£hFrw1uy7h$y¥pr£yhFixº1hiwtvxpF» ynxhFlhFq&ypF¡pFi1su¡ihDDmr£ynxhFsi&m¡vxwh$hVi¡ynipDxynxhFpFv!hFi1pFt'» t@pFi1£|Dw£wPup$iynxr1m¡vxpFily d w'¼!f¡&ypyw1t@wtuyqhFs½lf|¡½aDxhF» r1xpIy&wPxi1hFt¡hFt'pFr£w'nim¡vxwqp$¡w1u!w1¡wtr£hFi1pFt't@pFi   w£¾ r t7w£pFivxqf|¡½¿j¥f  x¨Àt ¦d w'&D&yw1tÁyni&xw#yhou¡m d y d x7vpnxr¦x¡w£plyhlxy7vt'xy£FDxhFvxhTy d m7y d wqrhFr£wtuyyh$u¡i1hF» Dmr£ynxhF'im¡vxw£  pFpFvxhhsh$Ai d pIyqy d wD&yw1tahF7f´ih$uyw1i1ynxw£´x w£Iuywr£yw£Ty&hwu¡i1w£rhFt'upFi1w£yh¦u¡ihDDmr£ynxhFim¡vxw££r£pFlv!wkw£wt i1w£hFm¡vxp$iw£Iu¡i1w£)xhF¦plr£h$t'upFiw@yh'u¡i1h¨Dmry)xh$@hFip$t't@pFi hF7y)Iuyw  By d w ´d h$t@)w¡  d xw1i1pFi1r d I  ¾¨¾¨±«`±«`Ö·¥`¹`§6§Û$ÃlÃ¹ ¸W£W¸W£m±½qÖW¤~¸x¨)b¡¤©`Äé¾±µ±µ¤¨ÖW©b§«j±5«Ã«`ÖW«b¨)©`­Õ¼µ¼µ§§£W©Ã¥b¤6¦¤¥b±W©bËª¼t§5¸xé5ªm¨)¨)¤~¨R¸W¨)¨)¨)ªx¼t¤¥b¹´¶"¶¨"¾¼¨)±¥`õp©b¾¨¥b¥m¸WùR¨)«b¨v¦mª°«`¨)½8¥b¨)¦W¨)¶©`¼"ÖW¨)¤«`Ã¦W±ÙØ¸W©b©`¨v¨)¥b©`¥¤¥`¥¨)¤~ªm­¦«b¥Â¥vªÍV÷±ÖW¨)Ä¸W¥`ªt¤ÃEÄ6¥b¼tÃ¸W¾¥`òH¨±¤¾¨)«±5¥bÌ¨«b«b¥¤¤ªR¤RòH¸mªtªW¤£W©b«b¨©Ã¨'Ë´¨R«b»¨v£x¼µ¤«¹`¤·§±ÄÂ ¥¸x½t¤¥t¼"ùC¨§£¹¨x¨)ª¸Wªm«b«b±5«bÃl©R«`Ãl¤¼Ù«p«`¼t§R£W¤¹´£Wª«`±5±R±ÆÄ«`¶±5¥R«b£m¨)Ô¨8ªP¹¥©bÃ±ª2£xªm©`¼t§¨¨)«b¥±Ö¥(¨¨8±¼t¨)ªt§5«§¥½®±¤©`§¥ª·5§¤¼t«b¤¾¨¸W¤±½8«`½¹¥b¨vÛ$¥R©«b¨)¥b¯½¶¤~¤¿©b§§Û$¤Ã£W«`¨5±µ±©`¹$Ãl§¤¥ã£WÃ¹b«b±5Ô"Ö¥W±¼t¼µªW©`½{©`£¨CÄ¯¨)±5¦ÖxÆ©bÄ¨¨8¨)¥Õ¨)¥b©"«`¦ô¶5¤¤¨)¶¿¦¤P§´«`¨vÃl«bª£m§¥`©b¥`¨)ªW­¦£W¨v£m§¨v¯«`«`§¸WÆ¡©bÅ¨v¹«`¹¨¹`§¤~¤~¾(¤«`ªmÇÖW£W«b¨'±¼H¤«b£«¦¤~ôm¥b¤¼Ú¥v¥b£Q¨'¨¦ª"ÖÃlé«`«`¨ÿÔp¾(¥i±§ÔR¤©b¨R±5¤½{¶~Äª§ÖªW¤­Õ«b±¤¶¥`­Õ¸WªWª«b¸W½f¥¨"±¦m¶¥¨¸W£WÙp¥b·Ã¶~»Ôt¨«`é5¨±Ä«Õªm«b¨ÿ¥t¤~¨´¤­8¤"­¦¨6¹±­¦£WªW³µ¥Ã¹Öm±¤ª¨"×W¤¨'§¥b·Õ«`½{­¦¥b¤¿¤½t¨C¶¯tªW¸W¹¤~¾¸W¸x¯¶t¥Ã©`±¨¾£W±¤~·¦Öx½{ÖW©b§¸W³W¤ªxª¸x±5ªQ«bªt£W¥b¯¥`³±W¶¤¦W·(£x¶¶¸W¨5¸Wª«b¨'Öm¨ôÄ¼t©b§¨)¸xÄ«`¾¨©b¾¾¨'¨)¨)¶~ª«b©b£m¨¤~¶¥b¥b«j¶£W¾¨«`¨v¨)¼¦x¥t§¨)¥b±W­Õ¤¨C¼¥«b¤¤««é5«b¼t§±5±é5£m¨)«b¤¨)¤ªm¨)©bÖt¤±5¨)£WÃªW¨)§ªt±¶«`¨¶ªª¯¼ª¯¨¥«¥¥¤À½Ô ¤«¨)¢¦¥¨v¦m¥`ªt©`¢¦¥¥`¨¥`¦W¢¦È1Û$ÄÞ §¹Ú7æÍFdôm¼t¦m¨)¯«b§§§³W©`¤¿§E5¥b¸m£x§Ó)ÛÉ)³W©`£W£W£W§©Ã²«j¥b¨¨)¯¯¥b¤¾(Ê¨)¹`¦WÐË±5¹¹Ç©ÃÝa§ªW¯¤¦«b¨o¸x«¨¨£Hd¡¶¥¥ØÛã)»¥b¶©`§¦¨v«b¶Ôl«b¨vÔÖã)¨v¨v¾â(«`¨)Ç¨v¥`Ø¤ÙAùA¼¤ã£«b£xÃl¨)ÅÁ ¼p«b±¹§¼µ¨)¥b±5é5¥bÙ¦"¨5¼Â£m©b£Ë¶«`¨QªmÈW¯t½¤±W¥Ø©ÃÄÄ1¨)ªW¸m¶¨'âqÄ«b«jÅÂ ªW§¤~ªxïC§¾Q¾(Ñ5¹¹)¥i¥b±¤¯té­¦Ì¤¿¹«bòHt«·ª¥`¤Õ¤º¹«±5¨R§ÃlÊt¶£m¨)Ô7×W¹´Í¨)Í±¤¨)Öm¤Ä¾¨§ÖW«b¶È7ÎI¤§«b¤~«jÆ¨R1Ûyæ«Ó©`Ä¨v¸WÃl©ib¯£W¥¦«j±¯£ÏD¸WÄãn¨)«2â8©§5ãÄÞ ¯Â³m¨¥"Ô©b¼t¨'ã)¶$Æ²Ðyªm¶¿±Ô'§¥bÍ¦SD¹`±5ã¥«`Í©b«bÛ$§¹«`¤¸WÄj©Ñs"Q­¦½ÛªQ¤~Û$£¹`£WÜp¦W¥b¨¦£xÓ'¶£W©`ª ÝYËÜ¥SD£Q©bÖWÇ¸W«`ÒÓËF¨)Ü¥¯£W©b©b¨¦¨)¨)¦mÝoa ±Ü¥¤¤S§§¥`ây¥b©`±±¥`ÐWÜp¥¨©`ªW¤¤¿ÃlÚ¦mÃÄq¦x§5¸W©`©`¯tÜ¥¨R¤~¨©"Û$¹`«b¦m¦mÃÍS±·5¨R«bÛ³W¹Ä¨)£W£¾¤¾(¦ÔHÆ¼t«bÓÃl¨)¨)§©b¤¿Ä¨)ª»¸Wé5©`Ç¦W£W©Ã2¹ªWÌ{±¨'©`©`©b¥¦Ä«jÕ¡§¥(§ÆÖ¤¨§Ìf·m¶«j«j©b¨«v¥¨¦¦m¯¥`±¥`Û$¨ÓDÖk¶¼©b2¯¯¨)¥b"±5§ÔRÄ1£(¸W¥b¸WÄlÔx½f¨'¨"ã)¨)ªx¥`©b×sÔxÔ$"¥b¶«j¸xé5©bã)§ÖW©b¨v×WÖt¥b¥b¥(±§­Õ¥`¦ã¥ôm±R±5¥b±W§ÖW«jÐ!ù¦ª¤¤¦ïC¥b"t±W©`¯¨©b¦m¦¯tª±5¾é¥`±¹)Ä¸x¥bÕ¡ëf¨)«v«j¼¨)Ä¹·m¸¨)¥b×Wªx©bÔW¨)¸W§¨­ÕªmÏDÔC¥b¾¶¤¿±W©`±¥`Äy¹¶«`¶©`ÖW¥@§¥b±5¾¨Í&§¹`¶«j¯±x¨v­¦»£W¶¹¨)¸W¼tÐ7¦¨v«b¨v¾¨ë{¸W£a ¯¨)¼¤¿¾(ªm¤¨¥b¨·5¨¨)¼¨Î¼¨¼t£W¨)§¸W§Äªm¶¿¤~½Ã¥¼¹`¨"§£W¶¨'©bç$¤¿ªW«`Ó'¥(ÖtÖ¼t¶Ä¦£à ÖÖt¹`©`«`¨v¾¶ËV¨vÄ·¯¥b¥b«b±5È¥¨£e¯t¯¤ºÄ§5¯¯(¼(âÛ$¼¨ã¯t±5±"¹´¶T0¶ÔW±5¹ßÔs¨)¥$¥¶~¾¨©b¾(»±5¤½fØE¡¨'§ªxÖt«b¥bªW2«0#±ª±5¤±ªW±¥vâTªt¯¥b¼t±ª(Öm¨±W¦m©å2 Ô§ª¾«b¨¯ÿá«b~¾¨)¹)±¨)©±¥`ÓD£W¥`«b«v§©`ä¶m¹)§§¥b¨8¯tã)£Wá©2¤¨"ÔC¾(y«jÖW±Wã¥§«`ªmªm¤¾(¨C¯"ì¨«jdª¡¤~²«`¸W¹§Ùp¼¨§À­Õéâ¦ÔsyÓv¨)¤¿¦mÖm§¶¿©b¶t¨À¨§·5ÌnÚ7§d5±i«2«b§5§ë{¾³W±âq«`¹b±©`ÓÊtÛ£WªWÔ¦¤¹¾³µ¨v¶2¤¿£t¼µÍF¯©bªQÌnÇÈ!¨"±¹»¼ÿ¤E¤ât¸W¤¨)¤¤±5ªm·â¾¨)¦Wª¥(Ð£¾¥bÔm¥Ãª§ÒW§ª¨¥¨)¼pd«b¦©`Êt§¥`§»¤Ñªm±H±5±5«b±ãn§ÄùAªm¾ÈW¨v¥b«bªmË¥ã)£WÄ¹Öt¦tªt±©`¹`±Äã¥§ÓIËFï)ª¯¼ã§¨¨"¨ÀÀÀÀÀââ¶ÔÔÔ ¥¦¥`±¢¦äj¹)Ãláª ¸W§é§©®±5í'£W¯è¦¾ªÌ{¥`§Ð`¨î¥Õ¤~ÛÑ5«`¨vÍF£mÇªW1«b¤ÇE5¼x§Û±5Ë£±³WÐ¬é5ÇªÖÅÁ §d¡¸W0#¤¨"·5«¦¯"âzª­¦2¨'§¤¥`XÃªmÔp¤ªë{Ã¥R±5«bÊ!¨5¹±©¡£Q¾Æ§(E5¨vÄ¯©iS¨·©§ÈW¨ØÜ¥§¤ù¦Ä¦S¨ÛyÜ¥¹`§ÌnÜ¥Ü¥¥b£âó¸SÇ¥ª©bÜ¥¯tq÷Ü¥Ã±5¹¸ÈxSÄÞ ¾(¸mE¦¶2Ãl¹Ñ5¨)§¨)¶Öm"¸WÊtÆëÝ«¨)ªt©`±2¥RÊjÈ7¸W«j«`¶¯Ãl"ã)¤¥"¨'ôã)¤ª1ÔxÄã£¤~ªê¥b¨)«`"¥bÓ)¯t£W©¦Ë@äj¾(«b¨¨)¨)ë{é£W³W©"è¶¨5Öì8¶~¨)¦WÛÄ¨§±5·m©`Ó©b¼Ç¨¶Êt¨)Ä¨¹)¥`À¦ÐÄlÖt§¤¥Ç¥¥8Ë©Ã¯s¦¥b¬¼t¦±0#«bEjÄ¤£mªW·m¨)ªm2 ¶§¨Ätë§¶~ï'«¨ÊY×x¶$Ä§¼«©bªmë{Gª¨'¨5é¼¤éªmÈhÄ Ì{·k¹`Ñ±5Ä¦£ê¦xÊ5ªWÇ¥Ë£W¶G6À¦¬¯(©Ã0¥bE¨Ê§±¸WÛ2¥bªWëvÖt¨)Çï)Ê¨¥ÀÔ ïVg¡h$i}plt@h$iw´phFit@pFv¡pFr£hFt'u¡vxwy&w!u¡i1w£w1¡ypy)xh$7w£wT»)¨D  ¥Ã«b¥b­¦¢¦¥ÃªW¨ÃTÃ¹Ãl§¨Ã§£x¯t§§³W±ª±ñ¤¤íP£W£W¯t¯t¥b¾(¨Q¾¦Wòqòq«i¨)¨¨)¤¥¥$÷¤©`ö ©©¦ªmªm©`«"Ö¾¤¦m¨)«b1Äs¨ªmë{¹¼t±5¥bÄ£Û§±©`¶±¹´³m¥C¦¨´¶¨)Ç¸m¹½®¦§¶~¥ªªtôm¹»¾¸«b«$Ä¼t¶¥`Ä·mÃªWÔm£m·¼t¸Wªx¨v¯¤¨'Ä$¤¹V±©R½q§Ã¼¨vÄt¥`±5¾(«b¾(2½ó«¦¤~×W¼¤¨¸W¥`×W©¡±ª¤¥bÖ©`Öm¸WªWª«bS¨©`§ª«§§¨¨)±5£WÖW¨)¨)«`Ü¥¥¦§©`ªmÜ¥«b£Wªm©¦¶¨¥bÛ$«bÄð ªmÜ¥¥b£W¥©`¹`S¸x¨¦¤¿¹`¨'«`¼µ¨v£ó ©b¨Õ÷¹Ã £ Ã«¾£m±÷¦m¶¥Õ¸mÝR 2¦m¨"¦m©§¸W§5ÃT¨v§¡½{"u§«¨)¤¹±5¤¼6ªtôjñÃlôt©b¥bª¹©`» È!©RXv¨¸W«`»«jÄÖt¥`¤âÊ!¯±Öx§P¹ô¯¸m§(¥bËE5Ôx±½qÍ¥b¨)¸¹`GõFÈx¸WÃl¥b¾«`§¼µ©¥b£Q"¹b¦£WÌ)¾¨¦íÈ¨)£(âtÄ¿ ¾¨Ì{¨)¨)î«b«`¨)ÑÒ¦«`¨v©b¶¨)£m«`¦x¶¶£W±5Ê¨¼Ýê¶©`£m¨v0õF§£WªËx¨)¾¼"§¥b« ©ÃÃl©b¤Öt2 ¨)«¥¹)§ªW¨Õ¤÷¯öÖtÃTÄ«`ªW§¥b±5ÃT¥¾(¯¤~¨5ñªW¨)¸WªyÃl0#ñ ©¦Ä«`±ªW¸Wªxd5¹¤í£W2 ¤½±¥R±¶¥`Ó¥v¥$Ôp¨)¸m«t«R©®ÌnÔWk©b¥b¥b«b"¼tÖ¨´¨¦¸xu¸W¹£x§(âq¨v³µôt±t¨®ÖWïCÖW¨)¤¼hE5t¦x¹'¥$¨ªQ¥b¥`§«bÐ)ÍF§³W¸W©b¸W¤£m¶~Ìn§ÿE5¨)©Ãªm¤~¥`«`¾¨âq¾¥`§¼µÐ£¥`±¦£W¹«¦ÇÎdÓ¥i«R¥`¤~¨v¶¨)¨È7ª¥b"y¨)¸m¼Â­¦©`¼«`¤±d£«§ã)¼t½8¨£mõFãn¾¤¶jÖÖ¨ã¥"«b§¤±Ä "¤ª¼£¯¯ "¨«¥½ Ô ¨v¥ÃÖt¾ÈÙp¦¹û §¨v§¯¤ÉpÄ0#¯t¨'¼µÈªW©b·m©8¤¥¨)¨Ãl2Ä'«j¥Q§¯¦W«`Æ$©b£mÈ!©`íøs¤§Û$¤¨vâ#§«jª¶Í¹¯©b«Ø§¶CÑld5¨v±ª×xÓ¤¼µÛ$±5Ðy¦m½p¼QÌn©b¨)«bù´©`¨'âp¤¨)£x¥±5ªó©bÕ¡ªm§Òµ¨)«j¦Í&«b¹`Ñ¯§©HÏF¥`£x¨)£aËú4¯t©b¨iÄ ¤ËVÍV«`¥¾(âÕbgEÔo¤¶¿¨)¥`Ý§Ì¨ÍVÖm&Õ¡¦m¥"E«bGË´±ÖkÃl«b¨'Ð£ÔÕ¨¶t¨'È¦¶~×dÄ³W¶©v­¦¥bã)¨vÐy¦WË)Äã)¸x¼¨£WãÔWÕ¡©b«`ÖW¨)£W¤¿ÏD§(Öt¥b¹`¥`Í¨)¸W£h¯¥"ÐA÷©b¾¨¨CÎ¸m±5Ã¤§¥©Ã¨v¤Æ¥8ªt¼µ¼íg¨´«`¨)§"³µ¤©RÖtûô¥b¦x¯©`¨)¯tÛ©b¨)©Ç¾(¨)Ãl¶¿¥`§Ì{ÆÖ¥bÑ«bÃl¨v¤¤±5Êtòq±¼¢Æ¶ÌªW©©5ª`¥b¥vÜ¥ÓIÔ1Ö¸WÜ¥Älâ)¯Á ÜÖW«bjÃl£x¬Ë¥`¬xÆ¸W¨X2¾Ê!ÙÚ¦x½{ÄtE5±5©b¤~¨ªµÈW¨êj©ã¼«ÀÀÀÌ §½{ôm¢¦«b«b«b­¦­¦©bÖ¥b«bªt¶¥bÖ¤«bÈ¦3z~X¤é5±±Ó'£x£x¤±¦±¨¸x£x¸mÊy¨)¨)¨¾Ép±£W£W£WÄË£¨©bªm­¦ª`ü¨(¨©b¨e¨)·m©H«jE¹%ª(¾(¨)Ì{¨(¨)¤¿¤­8¶¹¹¤¶ÄÈW¹`¥¦Ñ§ªW¶¤©`«p·5Öm«b±5¨)ÔÌfé£ó¨v«bù¦¤~¸W¨'©`Ê5¨¦ÌÖm·m¤¸W©ÃÑ5ªx¥Õ«`«b£W§É¼©bé¨)¶¿ËýP§§§±¸m£WÔt¥PÊ¹'Ò±ªW¹§¨'Eª¨ÖW©î¦W¥b¾§§»Ãl¨C¹´ËÁ­¦©¨¨vÐ)Õ¡«`¸W¤¿é§¶~£ª¾¨Ö§Ñ5¹¹éÔp£WÆ«jÕ¡¨'¨¤±0¤Ãlkªm§«`­8©bªWÈt±5Ìf¶¿¤³W¸WÖ7¨­¦ÖmªW±5¨vÀR©b©`¼p©bÐbÆ¼c2ªWË1·¦W¥h±þ±î±·¼pªW©`Ñ5¨Õ£W(I¤¦kÄT¤j¨¤±©b­¦¶ÇºXi§·"¬±Ôp¤¿ÄÍ&¸W¨¨'ËRk±¨5­¦§é¹`Û0#©bÑl×x½½{%¤§¥bªm§±£t©RÄ(ªm©`kªWÇ©b¬±ª¤~¥©b¥bªm2ÿ«é©b±¼t«b±ª¨¼Ô$0¨'¤·m½«bË)¨C«b£Wk¼Ú­¦âé¨'¾øªm«b§±ÃlÔ1£xÄ¡ÔH¤2¦mC¨)©b¨R±ÔtªH¦m©b¥¹`­¦¨(Æ¥b©`¨ÔHÔâo±©`Ãl­¦£Õ¡3±©b"¦«`¥v¥³W­¦±£W«`¥b§§5«bÖk©b±t¥b¥bÆÔt¤~±¦WÄt­¦±¤­¦£W©D±Ð7¤¿ªt¹£W¨)¤«`¥×s±µ«b3C«`­8¹`«b©bâ¯W­¦£¨)êj«¯h¤¤¿¤£W£m¼(ËóÄ¨£Þ¤£W¨'Ð!±«ªW¹b¤¨)¹©b±5©D¨'¤©§¥b£ÿ±Õ¡¤ª««b©b¶©Õ§Ø·ªWâ¤ªx§§ù¦©b8o¥b¾¤¿«Õ£W±½'¥8ÏD¨¨¤¹·¥b©b¥¥¥2¯±¦m­¦«b³WÍ&X¦ªW¨¸mâs«b¤¨¨Ä¥`§R½Ð7£W¤5¾¤¤£W§8o¦W·¹´¨)Û$¤¶§(ª3¨´éÎ«bR¨¨©`ªW¶~¨¥b¤~¶¨CÄq©`³x¦x¨'©`©A£xâB½Ãk©b±"³W¨)«b·f«b·©`±5CÄ¸W±¹´¥b©b¶~¨£¥b£W¦Wëâ©b3¨)¨'¦m¨)¦¨)¨)­¦©b¥"±ªW§¨C©bªW¥bk½f¾¥`¦W¥±¨)¨)­¦±5«C¤«¸W¨)¤Ð«b©b8B¥bªW¾«b¥`â"¨)¥P©b§©¤¶¤«`¥`ï¤£½{¥b¥bªW·¾(ªe«b±§ªW¤¤âªt¤~±5±W¨vÐ6±5ª¤¨)§Öx¤¨)ª¶·"§Þ«bªW©`¼ÿ¥¦8o¼t¾¨¸x3¦m¥«©¶~§a ¤¿¤¾(¦m¹·¨Õ§¨)¹¥8¶º½f§±§""±5¤§¨Öt±5±¶t¶Ré5¤«`½$¸W¥«bG6·58o«`âªt¨5©`©`¦m¯H©b¥b£W¥`§±6£¶¿Ãl©¹«b¨)¥R¾(¤éÔ«b«`§§©bâoÈh¨e«b£W§¸x±¦W©b±Æ¥b¨)¤¿§8o«`¤~À"¤¦W¤¸xªm«b¨§¥i©b¦©3zªt±¤~©bªW¦W£W÷±¨'ÖW£(¥b¶º"t¼t©`«b§ª"¨)«b©`¡·§§¥b¦¸mª`£x¨'ªt±£m¤¶ÄÒ¦¶¤©b¨F¨)ªÌ{±5¨5¨¾(¶q©b«bÔÒ­¦§©Ã¨v©b§ÔpmÑâªt¨P¤ªtÔ±§§¹¼tª«±Ê5¤¨§§¸W«Ã§§X£¤­¦¥`ªmÔªW±5¤«`ªW³t½{§zË©`©¥`¢¨©ã¥ã¥ã¤¤¤ª¯¼·§¨©¹¥À¥À¥«¥ÀÀÀÀÔ ¢¦¦W«b«b¹`ë!£x±§#£t¶­Õ±5¨Õ¸W"%©¼t±ª ôm§$¦¨)» ©b«`¨©`¨'í¥`¤~³W¤~±«2©éí5¦mª¨Qï`¨´ýC¨'§ï)³µ©b±XbÄªm«`¦¤V½'£W¾¼°¨)¨Ux1©`êj¨)¤~Ybª¬§f³Wªt¾¨¶~a¦¤¤~«`ªW¥`g¬¨)«`¥ùR«b·£Wªt¨)Ûá¸W£m¨«vªýC¤Ô5§¹¥bUWéÙÚ¥`¨¨«`¨vV¨¤º³W¤¹Â¹ªW§«bý±5Ö·5©ªm¼Wª§5c1¨)¸W¼Q§¨)¼¬¹¤Utªh««`¥`ÙÚW±5«b§H¨´¤¿¤Yb©H¹)¹8³µªWc1¥b§¦¨v«`Ûy½f©b§©b±5¨)©b©b¸m©`©`©H¤±5¤¨v¤~¹´«j¦¾¨¼ÿ«b¯Ût¸W¨)¨)±©`©`±¦xªt«b©b¨)¸W©b¤«v«`¥Q¨)±5«¸WÔ¥Õ¦·5¸x¤Ùp­8¨)ª¸W¥b©`¤¨v¨'¨)ªW«b«`¼v¥b©`¤·5£W¨P¨)¨¦¸x¥v¨Öt¸WÄ¤¤~ëf¯e¥`¥b¨¥b«b¨êj¨)³t¤¿ª¼¨§¹À  798A@B9CEDFB  
We report on the XLE parser and grammar development platform (Maxwell and Kaplan, 1993) and describe how a basic Lexical Functional Grammar for English has been adapted to two different corpora (newspaper text and copier repair tips).  (1) a. (SOLUTION 27032 70) If exhibiting 10132 faults replace the pre-fuser transport sensor (Q10-130). b. (SOLUTION 27240 80) 4. Enter into the machine log, the changes that have been made.  
In this paper, we propose a classiﬁcation of grammar development strategies according to two criteria : hand-written versus automatically acquired grammars, and grammars based on a low versus high level of syntactic abstraction. Our classiﬁcation yields four types of grammars. For each type, we discuss implementation and evaluation issues. 
Based on a detailed case study of parallel grammar development distributed across two sites, we review some of the requirements for regression testing in grammar engineering, summarize our approach to systematic competence and performance proﬁling, and discuss our experience with grammar development for a commercial application. If possible, the workshop presentation will be organized around a software demonstration. 
We report in this paper on an experiment on automatic extraction of a Tree Adjoining Grammar from the WSJ corpus of the Penn Treebank. We use an automatic tool developed by (Xia, 2001) properly adapted to our particular need. Rather than addressing general aspects of the automatic extraction we focus on the problems we have found to extract a linguistically (and computationally) sound grammar and approaches to handle them. 
We describe the development environment available to linguistic developers in our lab in writing large-scale grammars for multiple languages. The environment consists of the tools that assist writing linguistic rules and running regression testing against large corpora, both of which are indispensable for realistic development of large-scale parsing systems. We also emphasize the importance of parser efficiency as an integral part of efficient parser development. The tools and methods described in this paper are actively used in the daily development of broad-coverage natural language understanding systems in seven languages (Chinese, English, French, German, Japanese, Korean and Spanish). 
Technical terms in text often appear as noun compounds, a frequently occurring yet highly ambiguous construction whose interpretation relies on extra-syntactic information. Several statistical methods for disambiguating compounds have been reported in the literature, often with quite impressive results. However, a striking feature of all these approaches is that they rely on the existence of previously seen unambiguous compounds, meaning they are prone to the problem of sparse data. This diﬃculty has been overcome somewhat through the use of hand-crafted knowledge resources to collect statistics on “concepts” rather than noun tokens, but domain-independence has been sacriﬁced by doing so. We report here on work investigating the application of Latent Semantic Indexing to provide a robust domain-independent source of the extra-syntactic knowledge necessary for noun compound disambiguation. 
Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a ‘ﬂat’ list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the ‘new’ relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some speciﬁc structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring. 
The past decade has witnessed exciting work in the ﬁeld of Statistical Machine Translation (SMT). However, accurate evaluation of its potential in real-life contexts is still a questionable issue. In this study, we investigate the behavior of an SMT engine faced with a corpus far diﬀerent from the one it has been trained on. We show that terminological databases are obvious resources that should be used to boost the performance of a statistical engine. We propose and evaluate a way of integrating terminology into a SMT engine which yields a signiﬁcant reduction in word error rate. 
Named entity (NE) recognition is an important task for many natural language applications, such as Internet search engines, document indexing, information extraction and machine translation. Moreover, in oriental languages (such as Chinese, Japanese and Korean), NE recognition is even more important because it significantly affects the performance of word segmentation, the most fundamental task for understanding the texts in oriental languages. In this paper, a probabilistic verification model is designed for verifying the correctness of a named entity candidate. This model assesses the confidence level of a candidate not only according to the candidate’s structure but also according to its context. In our design, the clues for confidence measurement are collected from both positive and negative examples in the training data in a statistical manner. Experimental results show that the proposed method significantly improves the F-measure of Chinese personal name recognition from 86.5% to 94.4%. Introduction Named entity (NE) recognition (or proper name recognition) is a task to find the entities of person, location, organization, date, time, percentage and monetary value in text documents. It is an important task for many natural language applications, such as Internet search engines, document indexing, information extraction and machine translation. Moreover, in oriental languages (such as Chinese, Japanese and Korean), NE recognition is even more important because it significantly affects the performance of word segmentation, the most fundamental task for  understanding the texts in oriental languages. Therefore, a high-accuracy NE recognition method is highly demanded for most natural language applications in various languages. There are two major approaches to NE recognition: the handcrafted approach (Grishman, 1995) and the statistical approach (Bikel, 1997; Chen, 1998; Yu, 1998). In the first approach, a system usually relies on a large number of handcrafted rules. This kind of systems can be rapid prototyped but are hard to scale up. In fact, there will be numerous exceptions for most handcrafted rules. It is generally expensive and impossible to code for every exception we can imagine, not to mention those exceptions we are not able to think of. Another serious problem with the handcrafted approach is that systems are hard to be ported across different domains and different languages. Porting a handcrafted system usually means rewriting all its rules. Therefore, the statistical approach is becoming more and more popular because of its costeffectiveness in scaling up and porting systems. In general, the statistical approach to NE recognition can be viewed as a two-stage process. First, according to dictionaries and/or pattern matching rules, the input text is tokenized into tokens. Each token may be a word or an NE candidate which can consist of more than one word. Then, the most likely token sequence is selected according to a statistical model, such as Markov model (Bikel, 1997; Yu, 1998) or maximum entropy model (Borthwick, 1999). Although, the statistical NE recognition is much more scalable and portable, its performance is still not satisfactory. The insufficient coverage/precision of pattern matching rules and unknown words are the major sources of errors. Furthermore, the role of the statistical model is to assess the relative possibilities of all possible token sequences and select the most probable  one. The scores obtained from the statistical model can be used for a comparison of competing token sequences, but not for an assessment of the probability that a spotted named entity is correct. To reduce the recognition errors, we propose a probabilistic verification model to verify the correctness of a named entity. This model assesses the confidence level of a named entity candidate not only according to the candidate’s structure but also according to its contexts. In our design, the clues for confidence measurement are collected from both positive and negative examples in the training data. Therefore, the confidence measure has strong discriminant power for judging the correctness of a named entity. In the experiments of Chinese personal name recognition, the proposed verification model increases the F-measure from 86.5% to 94.4%, which corresponds to 58.5% error reduction rate, where “error rate” is defined as “100% − F-measure ”.  1. Named Entity Verification  As mentioned before, there are several kinds of named entities, including person, location, organization, date, time, percentage and monetary value. In the following description, we use the task of verifying Chinese personal name as an example. However, our proposed method is also applicable on verifying other kinds of named entities in different languages.  Before introducing our approach, we first describe the notations that will be used. In this proposal, a random variable is written with a boldface italic letter. An outcome of a random variable is written with the same italic letter but in normal face. For example, an outcome of the random variable o is denoted as o . If there is no confusion, we usually use P(o) to denote  the probability P(o = o) . A symbol sequence  “  x1,",  xn  ”  is  denoted  as  “  x1n  ”.  Likewise,  “  xY ,n Y ,1  ”  denotes the sequence “ xY ,1,", xY ,n ”.  The task of verifying a named entity candidate is viewed as making an acceptance or rejection decision according to the text segment consisting of the candidate and its context. Without loss of generality, a text segment is  Candidate     Ǘ \ Ɔ oL,1 oL,2 oC,1 oC,2 oC,3 oR,1 oR,2  shi zh ang ma ying jiu b iao s hi  Left Context  Right Context  Figure 1: Example of a text segment.  considered as an outcome of the random vector  O  =  [oLL,,1x  ,  oC , y C ,1  ,  oR,z R ,1  ].  The  outcome  of  each  ran-  dom variable in O is one basic element of text.  In Chinese, the basic elements of text are Chine-  se characters. However, in English, the basic  elements are English words. Figure 1 shows an  example of a text segment in which the size of  the candidate to be verified is 3 (i.e., consists of  three Chinese characters) and the sizes of its left  context and right context are set to 2 (i.e., two  Chinese characters).  Figure 2 depicts the flowchart of our verification approach. First, the candidate in the input text segment is parsed by a predefined grammar. If the candidate is ill-formed (i.e., fail  Text Segment Candidate Parsing  Ill-formed? Yes Reject No Confidence Measurement  cm < δ Yes Reject No Accept Figure 2: Flowchart of the verification method.  to be parsed), it will be rejected immediately. Otherwise, the text segment is passed to the confidence measurement module to assess the confidence level that the candidate in the text segment is to be a named entity. If the confidence measure is less than a predefined threshold, the candidate will be rejected. Otherwise, it will be accepted.  2. Confidence Measurement  The basic idea of our approach is to formulate the confidence measurement problem as the problem of hypothesis testing. The null hypothesis H0 in which the candidate is a name is tested against the alternative hypothesis H1 in which the candidate is not a name. According to Neyman-Pearson Lemma, an optimal hypothesis test involves the evaluation of the following log likelihood ratio:  LLR(oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  )  =  log  P  (oLL,,1x  ,  oC , y C ,1  ,  oR,z R ,1  P(oLL,,1x  ,  oC , y C ,1  ,  oR,z R ,1  | |  H0) H1)  (1)  =  log  P  (oLL,,1x  ,  oC,y C ,1  ,  oR,z R ,1  |  H0)  −  log  P(oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  |  H1 )  where  P(oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  |  H0)  is the likelihood of  the candidate and its left and right contexts  given the hypothesis that the candidate is a name  and  P(oLL,,1x  ,  oC , y C ,1  ,  oR,z R ,1  |  H1)  is the likelihood of  the candidate and its left and right contexts  given the hypothesis that the candidate is not a  name. The hypothesis test is performed by com-  paring the log likelihood ratio  LLR(oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  )  to a predefined critical  Text Segment  NE Model SNE (⋅)  cm  Anti-NE Model Santi-NE (⋅) Figure 3: Block diagram of the confidence measurement module.  threshold  δ . If  LLR(oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  )  ≥  δ  ,  the  null hypothesis will be accepted. Otherwise, it  will be rejected.  In our design, as shown in Figure 3, the  confidence measurement module consists of two  models, named NE model and anti-NE model.  The NE model is used to assess the value of  log  P  (oLL,,1x  ,  oC,y C ,1  ,  oR,z R ,1  |  H0)  and the anti-NE  model  is used to assess the value of  log  P  (oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  |  H1 )  .  2.1. NE Model  The purpose of the NE model is to evaluate the  value of  log  P  (oLL,,1x  ,  oC,y C ,1  ,  oR,z R ,1  | H0) ,  the  log  like-  lihood of the candidate and its left and right  contexts given the hypothesis that the candidate  is a name. Since it is infeasible to directly esti-  mate the probability  P(oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  | H0) ,  it  is  approximated as follows:  P  (oLL,,1x  ,  oC , y C ,1  ,  oR,z R ,1  |  H0)  ≡  P0  (oLL,,1x  ,  oC , y C ,1  ,  oR,z R ,1  )  (2)  ≈ P0 (oLL,,1x )P0 (oCC,,1y )P0 (oRR,,1z )  where the subscript of P0(⋅) indicates the probability is evaluated given that the null hypothesis is true. The probability P0 (oLL,,1x ) is further ap- proximated according to the bigram model as follows:  x  ∏ P0 (oLL,,1x ) ≈ P0 (oL,i | oL,i−1)  (3)  i =1  where P0 (oL,1 | oL,0 ) ≡ P0 (oL,1) . One should notice  that we do not assume that the random sequence  oL,x L,1  is time invariant. For example, the prob-  ability P(oL,i = x | oL,i−1 = y) is not assumed to  be equal to P(oL,2 = x | oL,1 = y) for i ≥ 3 .  Likewise, the probability P0 (oRR,,1z ) is also fur-  ther approximated as follows:  z  ∏ P0 (oRR,,1z ) ≈ P0 (oR,i | oR,i−1)  (4)  i =1  where P0 (oR,1 | oR,0 ) ≡ P0 (oR,1) . The probability corresponding to the candidate is evaluated by applying the SCFG (Sto-  S  SNC  GNC  GNC  \Ɔ  (ma)  (ying)  (jiu)  \Ɔ Figure 4: A parse tree of the Chinese per-  sonal name “  (ma ying jiu)”.  chastic Context-free Grammar) model (Fujisaki, 1989) as follows:  ∑ P0 (oCC,,1y ) = P0 (T )  T  (5)  ∏ ≈  max T  P0 (T  )  =  max T  A→α∈T  P0 (α  |  A)  where T stands for one possible parse tree that derive the candidate, A → α indicates a rule in  the parse tree T , A stands for the left-handside symbol of the rule and α stands for the  sequence of right-hand-side symbols of the rule.  \Ɔ Figure 4 shows an example of a parse tree of the  Chinese personal name candidate “  (ma  \Ɔ ying jiu)”, where “ (ma)” is the surname and “ (ying jiu)” is the given name. In this figure,  the symbol “S” denotes the start symbol, the  symbol “SNG” denotes the nonterminal deriving  surname characters and the symbol “GNC”  denotes the nonterminal deriving given name  characters. As a result, according to equations (2)  -(5), the scoring function in the NE model is  defined as equation (6) to assess the log likeli-  hood  of  the  text  segment  “  oL,x L,1  ,  oC , y C ,1  ,  oR,z R,1  ”  given  the  null  hypothesis  that  “  oC,y C ,1  ”  is  a  name.  SNE  (oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  )  x  z  ∑ ∑ = logP0 (oL,i | oL,i−1) + logP0 (oR,i | oR,i−1) (6)  i =1  i =1  ∑ +  max T  A→α∈T  log  P0 (α  |  A)  where T is one possible parse tree that derive  the  candidate  “  oC,y C ,1  ”.  2.2. Anti-NE Model  The purpose of the anti-NE model is to evaluate  the  value  of  log  P(oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  |  H1 )  ,  the  log  likelihood of the candidate and its left and right  contexts given the hypothesis that the candidate  is not a name. Since it is infeasible to directly  estimate the probability  P(oLL,,1x  ,  oC , y C ,1  ,  oR,z R ,1  |  H1)  ,  it  is approximated as follows:  P(oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  | H1)  =  P1  (oLL,,1x  ,  oC , y C ,1  ,  oR,z R ,1  )  x  y  ∏ ∏ ≈ P1(oL,i | oL,i−1) × P1(oC,i | oC,i−1)  (7)  i =1  i =1  z ∏ × P1(oR,i | oR,i−1 ) i =1  where oR,0 ≡ oC, y , oC,0 ≡ oL,x , and P1(oL,1 | oL,0 )  ≡ P1(oL,1) . Therefore, the following scoring  function is used in the anti-NE model to assess  the log likelihood of the text segment  “  oL,x L,1  ,  oC , y C ,1  ,  oR,z R,1  ”  given  the  alternative  hypothesis  that  “  oC,y C ,1  ”  is  not  a  name.  Santi-NE  (oLL,,1x  ,  oC , y C ,1  ,  oR,z R,1  )  x  y  ∑ ∑ = logP1(oL,i | oL,i−1) + logP1(oC,i | oC,i−1) (8)  i =1  i =1  z ∑ + logP1(oR,i | oR,i−1) i =1  3. Experiment Setup The proposed named entity verification method is used to recognize Chinese personal names from news. In Chinese, most of the personal names consist of three Chinese characters. The first character is a surname. The last two characters are a given name. Therefore, our preliminary experiments are focused on recognizing the personal names of three Chinese characters. In our experiments, the training corpus consists of about 14,339,000 Chinese characters collected from economy and industry news. This corpus should be annotated to estimate the probabilistic parameters of the scoring functions SNE (⋅) and Santi-NE (⋅) . However, labeling such large amount of data is too costly or prohibited even if it is possible. Therefore, labeling  News Seed Names Name Corpus  News Annotation Parameter Estimation Name Verification  Guessed Names  Figure 5: EM-style bootstrapping.  methods that can be bootstrapped from a little seed data or a few seed rules (Collins, 1999; Cucerzan, 1999) are highly demanded to automatically annotate the training data. In the following section, we propose an EM-style bootstrapping procedure (Cucerzan, 1999) for annotating the training data automatically. 3.1. EM-Style Bootstrapping The Expectation-Maximization (EM) algorithm (Moon, 1996) has been widely used to estimate model parameters from incomplete data in many different applications. In this section, an EMstyle bootstrapping procedure is proposed to automatically annotate the named entities in the training corpus. It iteratively uses the proposed verification model to label the training corpus (expectation step), and then uses the labeled training corpus to re-estimate the parameters of the verification model (maximization step). Figure 5 shows the flowchart of the bootstrapping procedure. First, we collect the names of 541 famous people, including government officers and CEOs of big companies. These names are used as seed names of the name corpus. Then, the news is automatically annotated according to the name corpus. The annotated corpus is used to estimate the probabilistic parameters of the scoring functions. Afterward, the proposed verification procedure is used to verify every possible name candidate in the news. The candidates whose confidence measures are larger than a predefined threshold are determined to be names. Currently, if the confi-  dence measures of two overlapped candidates (such as “ma ying jiu” and “ying jiu biao” in Figure 1) pass the threshold, both of them are determined as names. Although this strategy is inadequate, it does not make too much trouble because the chance to get overlapped names is very small in our experiments. Finally, these guessed names are added to the name corpus which will be used to annotate the news in next iteration. In our case, after four iterations, the size of name corpus is enlarged from 541 to 6,296, as shown in Table 1. The total occurrence frequency of these 6,296 names in the training corpus is 40,345. 3.2. Baseline Model In the past, many researchers have studied the problem of Chinese personal name recognition. Chang (1994) used the 0-order Markov model to segment a text into words, including Chinese personal names. In his approach, a name probability model is proposed to estimate the probabilities of Chinese personal names. Sproat (1994) proposed to recognize Chinese personal names with the stochastic finite-state word segmentation algorithm. His approach is similar to Chang’s, except that the name probability model is slightly different. In addition to name probability, Chen (1998) also add extra scores to a name candidate according to context clues (such as position, title, speech-act verbs). In the researches mentioned above, the reported Fmeasure performances on recognizing Chinese personal names are somewhere between 70% and 86%. Since these performances are meas-  Iteration  Nunber of distinct names  Total frequency of names  0  541  
In this paper, we propose a new idea for the automatic recognition of domain specific terms. Our idea is based on the statistics between a compound noun and its component single-nouns. More precisely, we focus basically on how many nouns adjoin the noun in question to form compound nouns. We propose several scoring methods based on this idea and experimentally evaluate them on the NTCIR1 TMREC test collection. The results are very promising especially in the low recall area. Introduction Automatic term recognition, ATR in short, aims at extracting domain specific terms from a corpus of a certain academic or technical domain. The majority of domain specific terms are compound nouns, in other words, uninterrupted collocations. 85% of domain specific terms are said to be compound nouns. They include single-nouns of the remaining 15% very frequently as their components, where “single-noun” means a noun which could not be further divided into several shorter and more basic nouns. In other words, the majority of compound nouns consist of the much smaller number of the remaining 15% single-noun terms and other single-nouns. In this situation, it is natural to pay attention to the relation among single-nouns and compound nouns, especially how single-noun terms contribute to make up compound noun terms. Another important feature of domain  Tatsunori Mori Yokohama National University 79-5, Tokiwadai,Hodogaya Yokohama, Japan,240-0085 mori@forest.dnj.ynu.ac.jp specific terms is termhood proposed in (Kageura & Umino 96) where “termhood” refers to the degree that a linguistic unit is related to a domain-specific concept. Thus, what we really have to pursue is an ATR method which directly uses the notion of termhood. Considering these factors, the way of making up compound nouns must be heavily related to the termhood of the compound nouns. The first reason is that termhood is usually calculated based on term frequency and bias of term frequency like inverse document frequency. Even though these calculations give a good approximation of termhood, still they are not directly related to termhood because these calculations are based on superficial statistics. That means that they are not necessarily meanings in a writer's mind but meanings in actual use. Apparently, termhood is intended to reflect this type of meaning. The second reason is that if a certain single-noun, say N, expresses the key concept of a domain that the document treats, the writer of the document must be using N not only frequently but also in various ways. For instance, he/she composes quite a few compound nouns using N and uses these compound nouns in documents he/she writes. Thus, we focus on the relation among single-nouns and compound nouns in pursuing new ATR methods. The first attempt to make use of this relation has been done by (Nakagawa & Mori 98) through the number of distinct single-nouns that come to the left or right of a single-noun term when used in compound noun terms. Using this type of number associated with a single-noun  term, Nakagawa and Mori proposed a  scoring function for term candidates.  Their term extraction method however is  just one example of employing the  relation among single-nouns and  compound nouns. Note that this  relation is essentially based on a noun  bigram. In this paper, we expand the  relation based on noun bigrams that  might be the components of longer  compound  nouns.  Then  we  experimentally evaluate the power of  several variations of scoring functions  based on the noun bigram relation using  the NTCIR1 TMREC test collection. By  this experimental clarification, we could  conclude that the single-noun term’s  power of generating compound noun  terms is useful and essential in ATR.  In this paper, section 1 gives the  background of ATR methods. Section 2  describes the proposed method of the  noun bigram based scoring function for  term extraction. Section 3 describes the  experimental results and discusses them.  
Term recognition and clustering are key topics in automatic knowledge acquisition and text mining. In this paper we present a novel approach to the automatic discovery of term similarities, which serves as a basis for both classification and clustering of domain-specific concepts represented by terms. The method is based on automatic extraction of significant patterns in which terms tend to appear. The approach is domain independent: it needs no manual description of domain-specific features and it is based on knowledge-poor processing of specific term features. However, automatically collected patterns are domain specific and identify significant contexts in which terms are used. Beside features that represent contextual patterns, we use lexical and functional similarities between terms to define a combined similarity measure. The approach has been tested and evaluated in the domain of molecular biology, and preliminary results are presented.  Introduction In a knowledge intensive discipline such as molecular biology, the vast and constantly increasing amount of information demands innovative techniques to gather and systematically structure knowledge, usually available only from text/document resources. In order to discover new knowledge, one has to identify main concepts, which are linguistically represented by domain specific terms (Maynard and Ananiadou (2000)). There is an increased amount of new terms that represent newly created concepts. Since existing term dictionaries usually do not meet the needs of specialists, automatic term extraction tools are indispensable for efficient term discovery and dynamic update of term dictionaries. However, automatic term recognition (ATR) is not the ultimate aim: terms recognised should be related to existing knowledge and/or to each other. This entails the fact that terms should be classified or clustered so that semantically similar terms are grouped together. Classification and/or clustering of terms are indispensable for improving information extraction, knowledge acquisition, and document categorisation. Classification can also be used for efficient term management and populating and updating existing ontologies in a consistent manner. Both classification and clustering methods  are built on top of a specific similarity measure. The notion of term similarity has been defined and considered in different ways: terms can have functional and/or structural similarities, though they can be correlated by different relationships (Grefenstette (1994), Maynard and Ananiadou (2000)). In this paper we suggest a novel, domainindependent method for the automatic discovery of term similarities, which can serve as a basis for both classification and clustering of terms. The method is mainly based on the automatic discovery of significant term features through pattern mining. Automatically collected patterns are domain dependent and they identify significant contexts in which terms tend to appear. In addition, the measure combines lexical and syntactical similarities between terms. The paper is organised as follows. In Section 1 we overview term management approaches. Section 2 introduces the term similarity measure and Section 3 presents results and experiments. 
Question terminology is a set of terms which appear in keywords, idioms and xed expressions commonly observed in questions. This paper investigates ways to automatically extract question terminology from a corpus of questions and represent them for the purpose of classifying by question type. Our key interest is to see whether or not semantic features can enhance the representation of strongly lexical nature of question sentences. We compare two feature sets: one with lexical features only, and another with a mixture of lexical and semantic features. For evaluation, we measure the classi cation accuracy made by two machine learning algorithms, C5.0 and PEBLS, by using a procedure called domain cross-validation, which e ectively measures the domain transferability of features. 
Automatic acquisition of paraphrase knowledge for content words is proposed. Using only a non-parallel text corpus, we compute the paraphrasability metrics between two words from their similarity in context. We then ﬁlter words such as proper nouns from external knowledge. Finally, we use a heuristic in further ﬁltering to improve the accuracy of the automatic acquisition. In this paper, we report the results of acquisition experiments. 
This paper is a draft position paper for discussion at the ELSNET Brainstorming Workshop 2000-2010 in Katwijk aan Zee, The Netherlands, on 23-24 November, 2000. The paper first describes some general emerging trends which are expected to deeply affect, or even transform, the field of speech technology research in the future, including trends towards advanced systems research, natural interactivity, multimodality, and medium-scale science. A timeline survey of future speech-related technologies is then presented followed by analysis of some of the implications of the proposed timelines. Timeline projections may turn out to have been false, of course, but even their turning out to be true is subject to future actions which are (not) taken to make them true. Accordingly, the final part of the paper discusses some actions which would seem desirable from the point of view of strengthening the position of European speech-related research. 1. Introduction 
This document summarizes contributions and discussions from two workshops that took place in November 2000 and July 2001. It presents some visions of NLP-related applications that may become reality within ten years from now. It investigates the technological requirements that must be met in order to make these visions realistic and sketches milestones that may help to measure our progress towards these goals. 1. Introduction 
The paper introduces the ways in which methods and resources of natural language processing (NLP) can be fruitfully employed in the domain of information assurance and security (IAS). IAS may soon claim a very prominent status both conceptually and in terms of future funding for NLP, alongside or even instead of established applications, such as machine translation (MT). After a brief summary of theoretical premises of NLP in general and of ontological semantics as a specific approach to NLP developed and/or practiced by the authors, the paper reports on the interaction between NLP and IAS through brief discussions of some implemented and planned NLP-enhanced IAS systems at the Center for Education and Research in Information Assurance and Security (CERIAS). The rest of the paper deals with the milestones and challenges in the future interaction between NLP and IAS as well as the role of a representational, meaning-based NLP approach in that future. 
Knowledge Technologies need to extract knowledge from existing texts, which calls for advanced Human Language Technologies (HLT). Progress is being made in Natural Language Processing but there is still a long way towards Natural Language Understanding. An important step towards this goal is the development of technologies and resources that deal with concepts rather than words. The MEANING project argues that we need to solve two complementary and intermediate tasks to enable the next generation of intelligent open domain HLT application systems: Word Sense Disambiguation and large-scale enrichment of Lexical Knowledge Bases. Innovations in this area will lead to HLT with deeper understanding of texts, and immediate progress in real applications of Knowledge Technologies. Introduction The field of Information Society Technologies (IST) is one of the main thematic priorities of the European Commission for the 6th Framework programme. In this field, Knowledge Technologies (KT) aim to provide meaning to the petabytes of information content our societies will generate in the near future. Information and knowledge management systems need to evolve accordingly, to enable the next generation of intelligent open domain Human Language Technologies (HLT) that will deal with the growing potential of the knowledge-rich and multilingual society. In order to develop a trustable semantic web infrastructure and a multilingual ontology  framework to support knowledge management a wide range of techniques are required to progressively automate the knowledge lifecycle. In particular, this involves extracting high-level meaning from the large collections of content data and its representation and management in a common knowledge base. Even now, building large and rich knowledge bases takes a great deal of expensive manual effort; this has severely hampered KnowledgeTechnologies and HLT application development. For example, dozens of person-years have been invest into the development of wordnets1 for various languages, but the data in these resources is still not sufficiently rich to support advanced concept-based HLT applications directly. Furthermore, resources produced by introspection usually fail to register what really occurs in texts. Applications will not scale up to working in the open domain without more detailed and rich general-purpose, which should perhaps include domain-specific linguistic knowledge. The MEANING project identifies two complementary intermediate tasks which we think are crucial in order to enable the next generation of intelligent open domain HLT application systems: Word Sense Disambiguation (WSD) and large-scale enrichment of Lexical Knowledge Bases. 
We are interested in contributing a small, publicly available Urdu corpus of written text to the natural language processing community. The Urdu text is stored in the Unicode character set, in its native Arabic script, and marked up according to the Corpus Encoding Standard (CES) XML Document Type Definition (DTD). All the tags and metadata are in English. To date, the corpus is made entirely of data from British Broadcasting Company’s (BBC) Urdu Web site, although we plan to add data from other Urdu newspapers. Upon completion, the corpus will consist mostly of raw Urdu text marked up only to the paragraph level so it can be used as input for natural language processing (NLP) tasks. In addition, it will be hand-tagged for parts of speech so the data can be used to train and test NLP tools. Introduction We are interested in contributing a small, publicly available Urdu corpus of written text to the natural language processing community. In pursuit of natural language processing research in Urdu, we could not find a publicly available Urdu corpus with which to work, so we had to start our own to train and test machine learning algorithms. The language engineering community seems anxious to move forward fast in research of South Asian languages, but cannot because corpora of South Asian languages are not ample. “There is a dearth of work on Indic languages. The need to focus on Indic languages was further strengthened by our major review (with over 80 research centres world wide responding) of the needs of the [language engineering] community. Indic languages are the ones that most researchers want to work with but cannot because lack of corpus resources” [1].  
This paper describes a dependency based tagging scheme for creating tree banks for Indian languages. The scheme has been so designed that it is comprehensive, easy to use with linear notation and economical in typing effort. It is based on Paninian grammatical model. 1.BACKGROUND The name AnnCorra, shortened for "Annotated Corpora", is for an electronic lexical resource of annotated corpora. The purpose behind this effort is to fill the lacuna in such resources for Indian languages. It will be an important resource for the development of Indian language parsers, machine learning of grammars, lakshancharts (discrimination nets for sense disambiguation) and a host of other such tools. 2. AIMS AND OBJECTIVE The aim of the project is to : - develop a generalised linear syntacto- semantic tag scheme for all Indian languages - annotate training corpus for all Indian languages - develop parallel tree-banks for all Indian languages  
We report on the role of the Urdu grammar in the Parallel Grammar (ParGram) project (Butt et al., 1999; Butt et al., 2002).1 The ParGram project was designed to use a single grammar development platform and a uniﬁed methodology of grammar writing to develop large-scale grammars for typologically different languages. At the beginning of the project, three typologically similar European grammars were implemented. The addition of two Asian languages, Urdu and Japanese, has shown that the basic analysis decisions made for the European languages can be applied to typologically distinct languages. However, the Asian languages required the addition of a small number of new standard analyses to cover constructions and analysis techniques not found in the European languages. With these additional standards, the ParGram project can now be applied to other typologically distinct languages. 
ISLE is a continuation of the long standing EAGLES initiative and it is supported by EC and NSF under the Human Language Technology (HLT) programme. Its objective is to develop widely agreed and urgently demanded standards and guidelines for infrastructural language resources, tools, and HLT products. EAGLES itself is a well-known trademark and point of reference for HLT projects and products and its previous results have already become de facto widely adopted standards. Multilingual computational lexicons, natural interaction and multimodality, and evaluation are the three areas targeted by ISLE. In the first section of the paper we describe the overall goals and methodology of EAGLES/ISLE, in the second section we focus on the work of the Computational Lexicon Working Group, introducing its work strategy and the preliminary guidelines of a standard framework for multilingual computational lexicons, based on a general schema for the “Multilingual ISLE Lexical Entry” (MILE).  
OLACMS (stands for Open Language Archives Community Metadata Set) is a standard for describe language resources. This paper provides suggestion to OLACMS 0.4 version by comparing it with other standards and applying it to Chinese and Formosan languages. 
The orthographical complexity of Chinese, Japanese and Korean (CJK) poses a special challenge to the developers of computational linguistic tools, especially in the area of intelligent information retrieval. These difficulties are exacerbated by the lack of a standardized orthography in these languages, especially the highly irregular Japanese orthography. This paper focuses on the typology of CJK orthographic variation, provides a brief analysis of the linguistic issues, and discusses why lexical databases should play a central role in the disambiguation process. 
 As electronic communications is now increasing, the term Natural Language Processing should be considered in the broader aspect of Multi-Language processing system. Observation of the language behavior will provide a good basis for design of computational language model and also creating costeffective solutions to the practical problems. In order to have a good language modeling, the language resources are necessary for the language behavior analysis. This paper intended to express what we have and what we have done by the desire to make a bridge between the languages and to share and make maximal use of the existing lexica, corpus and the tools. Three main topics are, then, focussed: A State of the Art of Thai language Resources, Thai language behaviors and their computational models. 
Ideograph characters are often formed by some smaller functional units, which we call character components. These character components can be ideograph radicals, ideographs proper, or some pure components which must be used with others to form characters. Decomposition of ideographs can be used in many applications. It is particularly important in the study of Chinese character formation, phonetics and semantics. However, the way a character is decomposed depends on the definition of components as well as the decomposition rules. The 12 Ideographic Description Characters (IDCs) introduced in ISO 10646 are designed to describe characters using components. The Hong Kong SAR Government recently published two sets of glyph standards for ISO10646 characters. The standards, being the first of its kind, make use of character decomposition to specify a character glyph using its components. In this paper, we will first introduce the IDCs and how they can be used with components to describe two dimensional ideograph characters in a linear fashion. Next we will briefly discuss the basic references and character decomposition rules. We will then describe the data structure and algorithms to decompose Chinese characters into components and, vice versa. We have also implemented our database and algorithms as an internet application, called the Chinese Character Search System, available at website http://www.iso10646hk.net/. With this tool, people can easily search characters and components in ISO 10646. Introduction ISO/IEC 10646 (ISO 10646) in its current version, contains more than 27,000 Han characters, or ideograph characters as it is called, in its basic multilingual plane and another 40,000 in the second plane[1-2]. The complete  set of ideograph repertoire includes Han 
We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics. The grammar is created for use in real world applications, such that robustness and performance issues play an important role. It is connected to a POS tagging and word segmentation tool. This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages. Introduction Natural language processing technology has recently reached a point where applications that rely on deep linguistic processing are becoming feasible. Such applications (e.g. message extraction systems, machine translation and dialogue understanding systems) require natural language understanding, or at least an approximation thereof. This, in turn, requires rich and highly precise information as the output of a parse. However, if the technology is to meet the demands of real-world applications, this must not come at the cost of robustness. Robustness requires not only wide coverage by the grammar (in both syntax and semantics), but also large and extensible lexica as well as interfaces to preprocessing systems for named entity recognition, non-linguistic structures such as addresses, etc. Furthermore, applications built on deep NLP technology should be extensible to multiple languages. This requires flexible yet well-defined output structures that can be adapted to grammars of many different languages. Finally, for use in real-world  applications, NLP systems meeting the above  desiderata must also be efficient.  In this paper, we describe the development of  a broad coverage grammar for Japanese that is  used in an automatic email response application.  The grammar is based on work done in the  Verbmobil project (Siegel 2000) on machine  translation of spoken dialogues in the domain of  travel planning. It has since been greatly  extended to accommodate written Japanese and  new domains.  The grammar is couched in the theoretical  framework of Head-Driven Phrase Structure  Grammar (HPSG) (Pollard & Sag 1994), with  semantic representations in Minimal Recursion  Semantics (MRS) (Copestake et al. 2001).  HPSG is well suited to the task of multilingual  development of broad coverage grammars: It is  flexible enough (analyses can be shared across  languages but also tailored as necessary), and  has a rich theoretical literature from which to  draw analyzes and inspiration.  The  characteristic type hierarchy of HPSG also  facilitates the development of grammars that are  easy to extend. MRS is a flat semantic  formalism that works well with typed feature  structures and is flexible in that it provides  structures that are under-specified for scopal  information. These structures give compact  representations of ambiguities that are often  irrelevant to the task at hand.  HPSG and MRS have the further advantage  that there are practical and useful open-source  tools for writing, testing, and efficiently  processing grammars written in these  formalisms. The tools we are using in this  project include the LKB system (Copestake  2002) for grammar development, [incr tsdb()]  (Oepen & Carroll 2000) for testing the grammar  and tracking changes, and PET (Callmeier  2000), a very efficient HPSG parser, for  processing. We also use the ChaSen tokenizer and POS tagger (Asahara & Matsumoto 2000). While couched within the same general framework (HPSG), our approach differs from that of Kanayama et al (2000). The work described there achieves impressive coverage (83.7% on the EDR corpus of newspaper text) with an underspecified grammar consisting of a small number of lexical entries, lexical types associated with parts of speech, and six underspecified grammar rules. In contrast, our grammar is much larger in terms of the number of lexical entries, the number of grammar rules, and the constraints on both,1 and takes correspondingly more effort to bring up to that level of coverage. The higher level of detail allows us to output precise semantic representations as well as to use syntactic, semantic and lexical information to reduce ambiguity and rank parses. 
This paper describes the constructing of a large-scale (above 500,000 pair sentences) Chinese-English parallel corpus. The current status of Chinese corpora is overviewed with the emphasis on parallel corpus. The XML coding principles for Chinese–English parallel corpus are discussed. The sentence alignment algorithm used in this project is described with a computer-aided checking processing. Finally, we show the design of the concordance of the parallel corpus and the prospect to further development. Introduction With the development of the corpus linguistics, more and more language resources have been established and used in language engineering research and applications. As we all know, there are different kinds of corpora for different kinds applications. For example, the Chinese Part-Of-Speech annotation corpus used to train program for Chinese word segmentation and POS tag, the Chinese tree bank used to Chinese syntax study, and so on. In this paper the constructing of a large-scale Chinese-English parallel corpus, which is totally above 500,000 pair sentences and the first year task is 100,000 pair sentences, is described. The applications of the large-scale Chinese-English parallel corpus put emphasis on the sentence template extracting for EBMT (Example-Based Machine Translation) and translation model training for SBMT (Statistical-Based Machine Translation). The latent applications may include  the bilingual lexicon extraction, special term or phase extraction, bilingual teaching, Chinese-English contrastive study, etc. Numerous corpus data gathering efforts exit all of the world. The rapid multiplication of such efforts has made it critical to create a set of standards for encoding corpora. CES (Corpus Encoding Standard), which is conformant to the TEI Guideline for Electronic Text Encoding and Interchange of the Text Encoding Initiative (TEI 2002), has been adopted by many corpus-based work. The XML Corpus Encoding Standard (XCES) is a part of the Guideline developed by the Expert Advisory Group on Language Engineering Standards (Ide, N., Bonhomme, P., Romary, L. 2000). The coding of our Chinese-English Parallel Corpus is in broad agreement with the TEI Guideline for electronic texts. In the following section, we first present a brief review of the current status of Chinese corpora with the emphasis on parallel corpus. Then the XML coding principles for Chinese–English parallel corpus are discussed in detail. Following this is the sentence alignment algorithm used in this project with a computer-aided checking processing. Finally, we show the design of the concordance of the parallel corpus and the prospect to further development. 
The growing availability of multilingual resources, like EuroWordnet, has recently inspired the development of large scale linguistic technologies, e.g. multilingual IE and Q&A, that were considered infeasible until a few years ago. In this paper a system for categorisation and automatic authoring of news streams in different languages is presented. In our system, a knowledge-based approach to Information Extraction is adopted as a support for hyperlinking. Authoring across documents in different languages is triggered by Named Entities and event recognition. The matching of events in texts is carried out by discourse processing driven by a large scale world model. This kind of multilingual analysis relies on a lexical knowledge base of nouns(i.e. the EuroWordnet Base Concepts) shared among English, Spanish and Italian lexicons. The impact of the design choices on the language independence and the possibilities it opens for automatic learning of the event hierarchy will be discussed. 
In this paper we face the automatic construction of a lexical taxonomy for the Spanish language using as input a taxonomy of English (WordNet)1 and a set of bilingual (English/Spanish) resources. Although applied to Spanish/English our method claims to be general enough to be applied in the cases a skeletal taxonomy already exists and we dispose of methods for mapping items to this taxonomy with known conﬁdence scores. 
Establishing correspondences between wordnets of different languages is essential to both multilingual knowledge processing and for bootstrapping wordnets of low-density languages. We claim that such correspondences must be based on lexical semantic relations, rather than top ontology or word translations. In particular, we define a translation equivalence relation as a bilingual lexical semantic relation. Such relations can then be part of a logical entailment predicting whether source language semantic relations will hold in a target language or not. Our claim is tested with a study of 210 Chinese lexical lemmas and their possible semantic relations links bootstrapped from the Princeton WordNet. The results show that lexical semantic relation translations are indeed highly precise when they are logically inferable. 1. Introduction A semantic network is critical to knowledge processing, including all NLP and Semantic Web applications. The construction of semantic networks, however, is notoriously difficult for ‘small’ (or ‘low-density’) languages. For these languages, the poverty of language resources, and the lack of prospect of material gains for infrastructure work conspire to create a vicious circle. This means that the construction of a semantic network for any small language must start from scratch and faces inhibitive financial and linguistic challenges. In addition, semantic networks serve as reliable ontolog(ies) for knowledge processing only if their conceptual bases are valid and logically inferable across different languages. Take wordnets (Fellbaum 1998), the de facto standard for linguistic ontology, for example.  Wordnets express ontology via a network of words linked by lexical semantic relations. Since these words are by definition the lexicon of each language, the wordnet design feature ensures versatility in faithfully and comprehensively representing the semantic content of each language. Hence, on one hand, these conceptual atoms reflect linguistic idiosyncrasies; on the other hand, the lexical semantic relations (LSR’s) receive universal interpretation across different languages. For example, the definition of relations such as synonymy or hypernymy is universal. The universality of the LSR’s is the foundation that allows wordnet to serve as a potential common semantic network representation for all languages. The premise is tacit in Princeton WordNet (WN), EuroWordNet (EWN, Vossen 1998), and MultiWordNet (MWN, Pianta et al. 2002). It is also spelled out explicitly in the adaptation of LSR tests for Chinese (Huang et al. 2001). Given that LSR’s are semantic primitives applicable to all language wordnets, and that the solution to the low-density problem in building language wordnets must involve bootstrapping from another language, LSR’s seem to be the natural units for such bootstrapping operations. The rich and structured semantic information described in WN and EWN can be transported through accurate translation if the conceptual relations defined by LSRs remain constant in both languages. In practice, such an application would also serve the dual purpose of creating a bilingual wordnet in the process. In this paper, we will examine the validity of cross-lingual LSR inferences by bootstrapping a Chinese Wordnet with WN. In practice, this small-scale experiment shows how a wordnet for a low-density language can be built through  * An earlier version of this paper was presented at the Third Chinese Lexical Semantics Workshop at Academia Sinica in May 2002. We are indebted to the participants as well as colleagues at CKIP for their comments. We would also like to thank the SemaNet 2002 reviewers for their helpful comments. It is our own responsibilities that, due to the short revision time, we were not able to incorporate all their suggestions, especially comparative studies with some relative GWA papers. We are also responsible for all remaining errors  bootstrapping from an available wordnet. In theoretical terms, we explore the logical conditions for the cross-lingual inference of LSR's.  2. Translation Equivalents and Semantic Relations Note that two translation equivalents (TE) in a pair of languages stand in a lexical semantic relation. The most desirable scenario is that when the two TE’s are synonymous, such as the English ‘apple’ to the Mandarin ‘ping2guo3’. However, since the conceptual space is not segmented identically for all languages, TE’s may often stand in other relations to each other. For instance, the Mandarin ‘zuo1zhi5’ is a hypernym for both the English ‘desk’ and ‘table’. Suppose we postulate that the LSR’s between TE’s are exactly identical in nature to the monolingual LSR’s described in wordnets. This means that the lexical semantic relation introduced by translation can be combined with monolingual LRS’s. Predicting LSR’s in a target language based on source language data become a simple logical operation of combining relational functions when the LSR of translation equivalency is defined. This framework is illustrated in Diagram 1.  CW2  ii  ¡ ¢¤£ 2  y  x  the relation y, between CW1 and CW2 is a functional combination of the three LSR’s i, x, and ii. However, it is well known that language translation involves more than semantic correspondences. Social and cultural factors also play a role in (human) choices of translation equivalents. It is not the aim of this paper to predict when or how these semantically non-identical translations arise. The aim is to see how much lexical semantic information is inferable across different languages, regardless of translational idiosyncrasies. In this model, the prediction relies crucially on the semantic information provided by the source language (e.g. English) lexical entry as well as the lexical semantic correspondence of a target language (e.g. Chinese) entry. The translation relations of the relational target pairs, although capable of introducing more idiosyncrasies, are not directly involved in the prediction. Hence we make the generalization that any discrepancy introduced at this level does not affect the logical relation of LSR prediction and adopt a working model described in Diagram 2. We only take into consideration those cases where the translation LSR ii is exactly equivalent, i.e., EW2 = CW2. This step also allows us to reduce the maximal number of LSR combination in each prediction to two. Thus we are able to better predict the contribution of each mono- or bi-lingual LSR. ¡ ¢¤£ 2 = CW2 (ii = 0)  CW1 ¥  i  ¡ ¢¤£ 1  y  x  x = EW1 - EW2 y = CW1- CW2 i = CW1 - EW1 ii = CW2 - EW2  LSR LSR Translation LSR Translation LSR  The unknown LSR y = i + x + ii  Diagram 1. Translation-mediated LSR Prediction (The complete model)  CW1 represents our starting Chinese lemma which can be linked to EW1 through the translation LSR i. The linked EW1 can than provide a set of LSR predictions based on the English WN. Assume that we take the LSR x, which is linked to EW2. That LSR prediction is mapped back to Chinese when EW2 is translated to CW2 with a translation LSR ii. In this model,  ¥  i  ¡ ¢¤£ 1  The unknown LSR y = i + x  Diagram 2. Translation-mediated LSR Prediction (Reduced Model, currently adopted)  2.1 LRS Inference as Relational Combination With the semantic contribution of the translation equivalency defined as a (bilingual) LSR, the inference of LSR in the target language wordnet is a simple combination of semantic relations. The default and ideal situation is where the two TE’s are synonymous.  ¢¡¤£ 2 = EW2  y  x  ¥ i¦ CW1 = EW1 (i = 0) The unknown LSR y = x Diagram 3. Translation-mediated LSR Prediction (when TE’s are synonymous)  In this case, the translation LSR is an identical relation; the LSR of the source language wordnet can be directly inherited. This is illustrated in Diagram 3. However, when the translation has a non-identical semantic relation, such as antonyms and hypernyms, then the LSR predicted is the combination of the bilingual relation and the monolingual relation. In this paper, we will concentrate on Hypernyms and Hyponyms. The choice is made because these two LSR’s are transitive relations by definition and allows clear logical predications when combined. The same, with some qualifications, may apply to the Holonym relations. Combinations of other LSR’s may not yield clear logical entailments. The scenarios involving Hyponymy and Hypernymy will be discussed in section 3.3.  3. Cross-lingual LSR Inference: A Study based on English-Chinese Correspondences In this study, we start with a WN-based English-Chinese Translation Equivalents Database (TEDB)1. Each translation equivalents pair was based on a WN synset. For quality control, we mark each TE pair for its accuracy as well as the translation semantic relation. For this study, the 200 most frequently used Chinese words plus 10 adjectives are chosen (since there is no adjective among the top 200 words in Mandarin). Among the 210 input lemmas, 179 lemmas2 find translation equivalents in the TEDB and are mapped to 497  
Computational Linguistics Group Communications Research Laboratory {kanzaki, qma, murata, isahara}@crl.go.jp  Abstract We treat nouns that behave adjectively, which we call adjectival nouns, extracted from large corpora. For example, in “financial world” and “world of finance,” “financial” and “finance” are different parts of speech, but their semantic behaviors are similar to each other. We investigate how adjectival nouns are similar to adjectives and different from non-adjectival nouns by using self-organizing semantic maps. We create five kinds of semantic maps, i.e., semantic maps of abstract nouns organized via (1) adjectives, (2) adjectival nouns, (3) non-adjectival nouns and (4) adjectival and adjectival nouns and a semantic map of adjectives, adjectival nouns and non-adjectival nouns organized via collocated abstract nouns, and compare them with each other to find similarities and differences. 
Many NLP systems are based on lexical data. The development costs of such data are a major drawback in such NLP systems. In order to cut these costs, we adopt a strategy inspired from "open-source" projects to allow volunteers to collaborate in the creation of a multilingual lexical database. For this, we had to specify and develop tools to manage a lexical database containing information complete and detailed enough to be usable for a wide range of applications. This paper presents our project and details the tools, frameworks and structures used to manage such a database. We will also show some research problems still to be addressed in this context. Résumé La connaissance linguistique reste une constituante importante de nombreux systèmes de traitement automatique des langues (TAL). Le coût de création d’un dictionnaire est l’un des freins majeurs dans le développement de ces systèmes. Afin de réduire les coûts de création de cette connaissance lexicale, nous adoptons une méthode inspirée des projets "open-source" afin de créer une base lexicale multilingue.  Pour cela, nous avons spécifié et développé des outils de gestion d'une base lexicale contenant des informations suffisamment complètes et détaillées pour êtres utilisées dans de nombreuses applications différentes. Cet article présente notre projet et détaille les outils, les cadres et les structures utilisées pour la gestion de cette base. Nous montrons aussi certains problèmes de recherche ouverts qu'il nous faut aborder dans ce contexte. Introduction Many NLP systems are based on lexical data. The development costs of such data are a major drawback in such NLP systems. Furthermore, the existing lexical data have generally been developed for a specific purpose and can’t be reused easily in other applications. The Papillon project applies some tools and methods to develop multipurpose, multilingual lexical data collaboratively on Internet. This data is complete and detailed enough to be eventually used either by NLP systems (MT engines for example) or by human users (language learners, translators…). After presenting the motivations of the Papillon project, we will show the management of existing data. Then we will describe the structure of the Papillon dictionary, and the tools that are used to allow contributions from Internet volunteers.  
The WordNet lexical ontology, which is primarily composed of common nouns, has been widely used in retrieval tasks. Here, we explore the notion of a ﬁnegrained proper noun ontology and argue for the utility of such an ontology in retrieval tasks. To support this claim, we build a ﬁne-grained proper noun ontology from unrestricted news text and use this ontology to improve performance on a question answering task. 
In this paper, we explore how the taxonomic inheritance hierarchy in a semantic net can contribute to the resolution of associative anaphoric expressions. We present the results of some preliminary experiments and discuss both their implications and the scope for improvements to the technique. 
This paper presents a module dedicated to the elaboration of linguistic resources for a versatile Information Extraction system. In order to decrease the time spent on the elaboration of resources for the IE system and guide the end-user in a new domain, we suggest to use a machine learning system that helps defining new templates and associated resources. This knowledge is automatically derived from the text collection, in interaction with a large semantic network. 
 Occurence patterns of words in documents  can be expressed as binary vectors. When  two vectors are similar, the two words cor-  responding to the vectors may have some  implicit relationship with each other. We  call these two words a correlated pair.  This report describes a method for obtain-  ing the most highly correlated pairs of a  given s¢iz¡¤e£¦. ¥¨In§©pr¡¤a£ctic e, the method re-  quire¢s ¡¤£  £ computation time,  and  memory space, where is the  number of documents or records. Since  this does not depend on the size of the  vocabulary under analysis, it is possible  to compute correlations between all the  words in a corpus.  
This paper presents a maximum entropy method for the disambiguation of word senses as defined in HowNet. With the release of this bilingual (Chinese and English) knowledge base in 1999, a corpus of 30,000 words was sense tagged and released in January 2002. Concepts meanings in HowNet are constructed by a closed set of sememes, the smallest meaning units, which can be treated as semantic tags. The maximum entropy model treats semantic tags like parts-of-speech tags and achieves an overall accuracy of 89.39%, outperforming a baseline system, which picks the most frequent sense. 1. Introduction A word usually has more than one meaning or sense, which are listed in the dictionary. The task of Word Sense Disambiguation (WSD) is to make the choice between the senses for a particular usage of the word in context. There are, however, several difficulties to WSD (Yang et al, 2000): (i) The evaluation of word sense disambiguation system is not yet standardized. (ii) The potential for WSD varies by task. (iii) Sense-tagged corpora are crucial resources for WSD but they are difficult to obtain. Efforts in building large Chinese corpora started in the 90s, for example, the Sinica corpus (CKIP, 1995) and the Chinese Penn Tree Bank (Xia et al., 2000). However, these two corpora concentrate on the tagging of parts-of-speech and syntactic structures, while little work has been done on semantic annotation. Of the few efforts that were carried out, Lua1 annotated 340,000 words with semantic classes defined in a thesaurus (Mei, 1983). This resource, however, was not publicly accessible. With the release of HowNet (Dong, 1999; Dong, 2000) in  YANG Yongsheng Department of Computer Science, HKUST, Clear Water Bay, Hong Kong. ysyang@cs.ust.hk 1999, Gan and Tham (1999) manually annotated a Chinese corpus of 30,000 words with the senses from HowNet. The corpus is a subset of the Sinica balanced corpus, and consists of 103 narratives on news stories, in which the words have already been segmented and tagged with parts-of-speech. Gan and Tham (1999) added sense tagging and subsequently Gan and Wong (2000) annotated the corpus with semantic dependency relations as defined in HowNet. The corpus was released to the public in January 2002 2 , providing essential resources for Chinese word sense disambiguation. This paper is organized as follows: Section 2 gives an introduction of HowNet. Section 3 describes the WSD task and the experiment results. Section 4 describes the previous work, followed by a conclusion in Section 5. 2. An Introduction to HowNet HowNet is a bilingual general knowledge base that encodes inter-concept semantic relations and the inter-attribute semantic relations. In contrast to WordNet (Miller, 1990), HowNet adopts a constructive approach of meaning representation (Miller, 1993). Basic meaning units called sememes, which cannot be decomposed further, combine to construct concepts in HowNet. So far, there are 65,000 Chinese concepts and 75,000 English equivalents defined with a set of 1503 sememes. NO.=the record number of the lexical entries W_X=concept of the language X E_X=example of W_X G_X=Part-of-speech of the W_X DEF=Definition, which is constructed by sememes and pointers Figure 1: A sample lexical entry in HowNet. Figure 1 gives an idea of how word concepts are organized in HowNet. “X” represents some  
The segmentation of Chinese texts is a key process in Chinese information processing. The difficulties in segmentation are the process of ambiguous character string and unknown Chinese words. In order to obtain the correct result, the first is identification of all possible candidates of Chinese words in a text. In this paper, a data structure Chinese-character-net is put forward, then, based on this character-net, a new algorithm is presented to obtain all possible candidate of Chinese words in a text. This paper gives the experiment result. Finally the characteristics of the algorithm are analysed. Keywords: segmentation, connection, character-net, ambiguity, unknown words. 
A speaker or writer has to find words for expressing his thoughts. Yet, knowing a word does not guarantee its access. Who hasn’t experienced the problem of looking for a word he knows, yet is unable to access (in time) ? Work done by psychologists reveals that people being in this so called tip-of-the-tongue state (TOT) know a lot about the word : meaning, number of syllables, origine, etc. Speakers are generally able to recognize the word, and if they produce an erroneous word, that token shares many things with the target word (initial/final letter/phoneme, part of speech, semantic field, etc.). This being so, one might want to take advantage of the situation and build a program that assists the speaker/writer by revealing the word that’s on his/her mind (tongue/pen). Three methods will be presented, the first one being implemented. 
This paper separates conditional parameter estimation, which consistently raises test set accuracy on statistical NLP tasks, from conditional model structures, such as the conditional Markov model used for maximum-entropy tagging, which tend to lower accuracy. Error analysis on part-of-speech tagging shows that the actual tagging errors made by the conditionally structured model derive not only from label bias, but also from other ways in which the independence assumptions of the conditional model structure are unsuited to linguistic sequences. The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conﬂicting reports from other recent work. 
We demonstrate a problem with the standard technique for learning probabilistic decision lists. We describe a simple, incremental algorithm that avoids this problem, and show how to implement it efﬁciently. We also show a variation that adds thresholding to the standard sorting algorithm for decision lists, leading to similar improvements. Experimental results show that the new algorithm produces substantially lower error rates and entropy, while simultaneously learning lists that are over an order of magnitude smaller than those produced by the standard algorithm. 
This paper demonstrates the substantial empirical success of classiﬁer combination for the word sense disambiguation task. It investigates more than 10 classiﬁer combination methods, including second order classiﬁer stacking, over 6 major structurally different base classiﬁers (enhanced Naïve Bayes, cosine, Bayes Ratio, decision lists, transformationbased learning and maximum variance boosted mixture models). The paper also includes in-depth performance analysis sensitive to properties of the feature space and component classiﬁers. When evaluated on the standard SENSEVAL1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classiﬁer combination performance exceeds the best published results on these data sets. 
This paper investigates several augmented mixture models that are competitive alternatives to standard Bayesian models and prove to be very suitable to word sense disambiguation and related classiﬁcation tasks. We present a new classiﬁcation correction technique that successfully addresses the problem of under-estimation of infrequent classes in the training data. We show that the mixture models are boosting-friendly and that both Adaboost and our original correction technique can improve the results of the raw model signiﬁcantly, achieving stateof-the-art performance on several standard test sets in four languages. With substantially different output to Naïve Bayes and other statistical methods, the investigated models are also shown to be effective participants in classiﬁer combination. 
In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data. Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms. We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms. In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best ofﬁcial scores on both SENSEVAL-2 and SENSEVAL-1 test data. 
Most machine learning solutions to noun phrase coreference resolution recast the problem as a classiﬁcation task. We examine three potential problems with this reformulation, namely, skewed class distributions, the inclusion of “hard” training instances, and the loss of transitivity inherent in the original coreference relation. We show how these problems can be handled via intelligent sample selection and error-driven pruning of classiﬁcation rulesets. The resulting system achieves an Fmeasure of 69.5 and 63.4 on the MUC6 and MUC-7 coreference resolution data sets, respectively, surpassing the performance of the best MUC-6 and MUC-7 coreference systems. In particular, the system outperforms the best-performing learning-based coreference system to date. 
This paper proposes a novel class of PCFG parameterizations that support linguistically reasonable priors over PCFGs. To estimate the parameters is to discover a notion of relatedness among context-free rules such that related rules tend to have related probabilities. The prior favors grammars in which the relationships are simple to describe and have few major exceptions. A basic version that bases relatedness on weighted edit distance yields superior smoothing of grammars learned from the Penn Treebank (20% reduction of rule perplexity over the best previous method). 
We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we ﬁnd that standard machine learning techniques deﬁnitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classiﬁcation, and support vector machines) do not perform as well on sentiment classiﬁcation as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classiﬁcation problem more challenging. 
This paper describes improved HMM-based word level alignment models for statistical machine translation. We present a method for using part of speech tag information to improve alignment accuracy, and an approach to modeling fertility and correspondence to the empty word in an HMM alignment model. We present accuracy results from evaluating Viterbi alignments against human-judged alignments on the Canadian Hansards corpus, as compared to a bigram HMM, and IBM model 4. The results show up to 16% alignment error reduction. 
This paper presents a technique for discovering translationally equivalent texts. It is comprised of the application of a matching algorithm at two diﬀerent levels of analysis and a well-founded similarity score. This approach can be applied to any multilingual corpus using any kind of translation lexicon; it is therefore adaptable to varying levels of multilingual resource availability. Experimental results are shown on two tasks: a search for matching thirty-word segments in a corpus where some segments are mutual translations, and classiﬁcation of candidate pairs of web pages that may or may not be translations of each other. The latter results compare competitively with previous, document-structure-based approaches to the same problem. 
We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conﬂicts in a greedy way, with limited amount of backtracking. We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, conﬁrming the intuition that LR parsing beneﬁts from the use of rich grammars. 
In the ﬁeld of empirical natural language processing, researchers constantly deal with large amounts of marked-up data; whether the markup is done by the researcher or someone else, human nature dictates that it will have errors in it. This paper will more fully characterise the problem and discuss whether and when (and how) to correct the errors. The discussion is illustrated with speciﬁc examples involving function tagging in the Penn treebank. 
We describe and evaluate the application of a spectral clustering technique (Ng et al., 2002) to the unsupervised clustering of German verbs. Our previous work has shown that standard clustering techniques succeed in inducing Levinstyle semantic classes from verb subcategorisation information. But clustering in the very high dimensional spaces that we use is fraught with technical and conceptual diﬃculties. Spectral clustering performs a dimensionality reduction on the verb frame patterns, and provides a robustness and eﬃciency that standard clustering methods do not display in direct use. The clustering results are evaluated according to the alignment (Christianini et al., 2002) between the Gram matrix deﬁned by the cluster output and the corresponding matrix deﬁned by a gold standard. 
We present a bootstrapping method that uses strong syntactic heuristics to learn semantic lexicons. The three sources of information are appositives, compound nouns, and ISA clauses. We apply heuristics to these syntactic structures, embed them in a bootstrapping architecture, and combine them with co-training. Results on WSJ articles and a pharmaceutical corpus show that this method obtains high precision and ﬁnds a large number of terms. 
We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4. 
We present Minimum Bayes-Risk word alignment for machine translation. This statistical, model-based approach attempts to minimize the expected risk of alignment errors under loss functions that measure alignment quality. We describe various loss functions, including some that incorporate linguistic analysis as can be obtained from parse trees, and show that these approaches can improve alignments of the English-French Hansards. 
Text prediction is a form of interactive machine translation that is well suited to skilled translators. In principle it can assist in the production of a target text with minimal disruption to a translator’s normal routine. However, recent evaluations of a prototype prediction system showed that it signiﬁcantly decreased the productivity of most translators who used it. In this paper, we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the expected beneﬁt to the translator, rather than just trying to anticipate some amount of upcoming text. Using a model of a “typical translator” constructed from data collected in the evaluations of the prediction prototype, we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator. 
An important component of any generation system is the mapping dictionary, a lexicon of elementary semantic expressions and corresponding natural language realizations. Typically, labor-intensive knowledge-based methods are used to construct the dictionary. We instead propose to acquire it automatically via a novel multiple-pass algorithm employing multiple-sequence alignment, a technique commonly used in bioinformatics. Crucially, our method leverages latent information contained in multiparallel corpora — datasets that supply several verbalizations of the corresponding semantics rather than just one. We used our techniques to generate natural language versions of computer-generated mathematical proofs, with good results on both a per-component and overall-output basis. For example, in evaluations involving a dozen human judges, our system produced output whose readability and faithfulness to the semantic input rivaled that of a traditional generation system. 
We describe a hybrid approach to improving search performance by providing a natural language front end to a traditional keyword-based search engine. The key component of the system is iterative query formulation and retrieval, in which one or more queries are automatically formulated from the user’s question, issued to the search engine, and the results accumulated to form the hit list. New queries are generated by relaxing previously-issued queries using transformation rules, applied in an order obtained by reinforcement learning. This statistical component is augmented by a knowledge-driven hub-page identiﬁer that retrieves a hub-page for the most salient noun phrase in the question, if possible. Evaluation on an unseen test set over the www.ibm.com public website with 1.3 million webpages shows that both components make substantial contribution to improving search performance, achieving a combined 137% relative improvement in the number of questions correctly answered, compared to a baseline of keyword queries consisting of two noun phrases. 
While recent retrieval techniques do not limit the number of index terms, out-ofvocabulary (OOV) words are crucial in speech recognition. Aiming at retrieving information with spoken queries, we ﬁll the gap between speech recognition and text retrieval in terms of the vocabulary size. Given a spoken query, we generate a transcription and detect OOV words through speech recognition. We then correspond detected OOV words to terms indexed in a target collection to complete the transcription, and search the collection for documents relevant to the completed transcription. We show the effectiveness of our method by way of experiments. 
In this paper, we address the problem of dealing with a large collection of data and propose a method for text classiﬁcation which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs). NB is based on the assumption of word independence in a text, which makes the computation of it far more efﬁcient. SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classiﬁers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classiﬁcation without sacriﬁcing accuracy. 
This paper describes a bootstrapping algorithm called Basilisk that learns highquality semantic lexicons for multiple categories. Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category. Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts. We evaluate Basilisk on six semantic categories. The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement. 
Ensemble methods are state of the art for many NLP tasks. Recent work by Banko and Brill (2001) suggests that this would not necessarily be true if very large training corpora were available. However, their results are limited by the simplicity of their evaluation task and individual classiﬁers. Our work explores ensemble efﬁcacy for the more complex task of automatic thesaurus extraction on up to 300 million words. We examine our conﬂicting results in terms of the constraints on, and complexity of, different contextual representations, which contribute to the sparsenessand noise-induced bias behaviour of NLP systems on very large corpora. 
This paper shows that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verbobject bigrams from the web by querying a search engine. We evaluate this method by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus. We also perform a task-based evaluation, showing that web frequencies can reliably predict human plausibility judgments. 
A new almost-parsing language model incorporating multiple knowledge sources that is based upon the concept of Constraint Dependency Grammars is presented in this paper. Lexical features and syntactic constraints are tightly integrated into a uniform linguistic structure called a SuperARV that is associated with a word in the lexicon. The SuperARV language model reduces perplexity and word error rate compared to trigram, part-of-speech-based, and parser-based language models. The relative contributions of the various knowledge sources to the strength of our model are also investigated by using constraint relaxation at the level of the knowledge sources. We have found that although each knowledge source contributes to language model quality, lexical features are an outstanding contributor when they are tightly integrated with word identity and syntactic constraints. Our investigation also suggests possible reasons for the reported poor performance of several probabilistic dependency grammar models in the literature. 
This paper presents several practical ways of incorporating linguistic structure into language models. A headword detector is first applied to detect the headword of each phrase in a sentence. A permuted headword trigram model (PHTM) is then generated from the annotated corpus. Finally, PHTM is extended to a cluster PHTM (C-PHTM) by defining clusters for similar words in the corpus. We evaluated the proposed models on the realistic application of Japanese Kana-Kanji conversion. Experiments show that C-PHTM achieves 15% error rate reduction over the word trigram model. This demonstrates that the use of simple methods such as the headword trigram and predictive clustering can effectively capture long distance word dependency, and substantially outperform a word trigram model. 
We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy. The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers. Because a wrong answer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer. 
In this paper, we propose a method for learning a classiﬁer which combines outputs of more than one Japanese named entity extractors. The proposed combination method belongs to the family of stacked generalizers, which is in principle a technique of combining outputs of several classiﬁers at the ﬁrst stage by learning a second stage classiﬁer to combine those outputs at the ﬁrst stage. Individual models to be combined are based on maximum entropy models, one of which always considers surrounding contexts of a ﬁxed length, while the other considers those of variable lengths according to the number of constituent morphemes of named entities. As an algorithm for learning the second stage classiﬁer, we employ a decision list learning method. Experimental evaluation shows that the proposed method achieves improvement over the best known results with Japanese named entity extractors based on maximum entropy models. 
There has been much interest in using phrasal movement to improve statistical machine translation. We explore how well phrases cohere across two languages, speciﬁcally English and French, and examine the particular conditions under which they do not. We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system. We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion. 
We report on experiments in reference resolution using a decision tree approach. We started with a standard feature set used in previous work, which led to moderate results. A closer examination of the performance of the features for different forms of anaphoric expressions showed good results for pronouns, moderate results for proper names, and poor results for deﬁnite noun phrases. We then included a cheap, language and domain independent feature based on the minimum edit distance between strings. This feature yielded a signiﬁcant improvement for data sets consisting of deﬁnite noun phrases and proper names, respectively. When applied to the whole data set the feature produced a smaller but still signiﬁcant improvement. 
Voicemail is not like email. Even such basic information as the name of the caller/ sender or a phone number for returning calls is not represented explicitly and must be obtained from message transcripts or other sources. We discuss techniques for doing this and the challenges these tasks present. 
This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora. We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency. Experimental results for the construction of a German-English noun lexicon are reported. Noun translation accuracy of 39% scored against a parallel test corpus could be achieved. 
In this paper we investigate the impact of morphological features on the task of automatically extending a dictionary. We approach the problem as a pattern classiﬁcation task and compare the performance of several models in classifying nouns that are unknown to a broad coverage dictionary. We used a boosting classiﬁer to compare the performance of models that use different sets of features. We show how adding simple morphological features to a model greatly improves the classiﬁcation performance. 
Many lexical semantic relations, such as the hyponymy relation, can be extracted from text as they occur in detectable syntactic constructions. This paper shows how a hypernym-hyponym based lexicon for Swedish can be created directly from a news paper corpus. An algorithm is presented for building partial hierarchical structures from non domain-speciﬁc texts. 
Natural language parsing requires extensive lexicons containing subcategorisation information for speciﬁc sublanguages. This paper describes an unsupervised method for acquiring both syntactic and semantic subcategorisation restrictions from corpora. Special attention will be paid to the role of co-composition in the acquisition strategy. The acquired information is used for lexicon tuning and parsing improvement. 
This paper presents experiments performed on lexical knowledge acquisition in the form of verbal argumental information. The system obtains the data from raw corpora after the application of a partial parser and statistical filters. We used two different statistical filters to acquire the argumental information: Mutual Information, and Fisher’s Exact test. Due to the characteristics of agglutinative languages like Basque, the usual classification of arguments in terms of their syntactic category (such as NP or PP) is not suitable. For that reason, the arguments will be classified in 48 different kinds of case markers, which makes the system fine grained if compared to equivalent systems that have been developed for other languages. This work addresses the problem of distinguishing arguments from adjuncts, this being one of the most significant sources of noise in subcategorization frame acquisition. Introduction In recent years a considerable effort has been done on the acquisition of lexical information. As several authors point out, this information is useful for a wide range of applications. For example, J. Carroll et al. (1998) show how adding subcategorization information improves the performance of a parser. With this in mind our aim is to obtain a system that automatically discriminates between subcategorized elements of verbs (arguments) and non-subcategorized ones (adjuncts). We have evaluated our system in two ways: comparing the results to a gold standard and estimating the coverage over sentences in the  Aitziber Atutxa University of Maryland College Park Maryland, 20740 jibatsaa@si.ehu.es corpus. The purpose was to find out which was the impact of each approach on this particular task. The two methods of evaluation yield significantly different results. Basque is the subject of this study. A language that, in contrast to languages like English, has limited resources in the form of digital corpora, computational lexicons, grammars or annotated treebanks. Therefore, any effort like the one presented here, oriented to create lexical resources, has to be driven to do as much automatic work as possible, minimizing development costs. The paper is divided into 4 sections. The first section is devoted to explain the theoretical motivations underlying the process. The second section is a description of the different stages of the system. The third section presents the results obtained. The fourth section is a review of previous work on automatic subcategorization acquisition. Finally, we present the main conclusions. 
The use of semantic resources is common in modern NLP systems, but methods to extract lexical semantics have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the tradeoff between extraction performance and efﬁciency. We propose an approximation algorithm, based on canonical attributes and coarse- and ﬁne-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty. 
We extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour. This type of knowledge is useful in the process of lexical choice between nearsynonyms. We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster. For this task we use a much larger corpus (the Web). We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry. 
This paper presents preliminary experiments in the use of translation equivalences to disambiguate prepositions or case suffixes. The core of the method is to find translations of the occurrence of the target preposition or case suffix, and assign the intersection of their set of interpretations. Given a table with prepositions and their possible interpretations, the method is fully automatic. We have tested this method on the occurrences of the Basque instrumental case -z in the definitions of a Basque dictionary, looking for the translations in the definitions from 3 Spanish and 3 English dictionaries. The results have been that we are able to disambiguate with 94.5% accuracy 2.3% of those occurrences (up to 91). The ambiguity is reduced from 7 readings down to 3.1. The results are very encouraging given the simple techniques used, and show great potential for improvement. 
 We develop a model of preposition definitions in a machine-readable dictionary using the theory of labeled directed graphs and analyze the resulting digraphs to determine a primitive set of preposition senses. We characterize these primitives and show how they can be used to develop an inheritance hierarchy for prepositions, representing the definitions by a type and slots for its arguments. By analyzing the definitions, we develop criteria for disambiguating among the highly polysemous primitives. We show how these criteria can be  A preposition is “a word governing, and usually preceding, a noun or pronoun and expressing a relation to another word or element in the clause.” The definition of a preposition takes two principal forms: (1) a usage expression characterizing the relation or (2) an expression that can be substituted for the preposition. A substituting preposition definition usually consists of a prepositional phrase (including both a preposition and a noun phrase) and a terminating preposition (e.g., for around, one definition is “on every side of”).  used in developing the inheritance hierarchy and how they may be used in assigning theta  2.1 Headwords as Digraph Nodes  roles to the objects of transitive verbs. Finally, we describe the use of the disambiguation criteria to parse and represent the meaning of the prepositions as used in encyclopedia articles.  A digraph consists of nodes and directed arcs between the nodes. In general, an arc should correspond to a transitive relation. Modeling a dictionary with a digraph entails assigning an interpretation to the nodes and arcs. For our initial  
In this paper we discuss some philosophical questions related to the treatment of abstract and underspecified prepositions. We consider three issues in particular: (i) the relation between sense and meanings, (ii) the privileged status of abstract meanings in the spectrum of contextual instantiations of basic senses, and finally (iii) the difference between prediction and inference. The discussion will be based on the study of avec (with) and the analysis of its abstract meaning of comitativity in particular. A model for avec semantic variability will also be suggested. 
In this document, we analyze several aspects related to the semantics of prepositions. We propose an approach and elements of a method to organize and to represent prepositions uses and senses. Complex polysemous behaviors are discussed, showing the use and limits of the approach. 
We report on an empirical study of sense relations in the Senseval-2 test suite. We apply and extend the method described in (Resnik and Yarowsky, 1999), estimating proximity of sense pairs from the evidence collected from native-speaker translations of 508 contexts across 4 Indoeuropean languages representing 3 language families. A control set composed of 65 contexts has also been annotated in 12 languages (including 2 non-Indoeuropean languages) in order to estimate the correlation between parallel polysemy and language family distance. A new parameter, sense stability, is introduced to assess the homogeneity of each individual sense deﬁnition. Finally, we combine the sense proximity estimation with a classiﬁcation of semantic relations between senses. 
This paper presents a comparative evaluation among the systems that participated in the Spanish and English lexical sample tasks of SENSEVAL-2. The focus is on pairwise comparisons among systems to assess the degree to which they agree, and on measuring the difﬁculty of the test instances included in these tasks. 
For SENSEVAL-2, we disambiguated the lexical sample using two different sense inventories. Official SENSEVAL-2 results were generated using WordNet, and separately using the New Oxford Dictionary of English (NODE). Since our initial submission, we have implemented additional routines and have now examined the differences in the features used for making sense selections. We report here the contribution of default sense selection, idiomatic usage, syntactic and semantic clues, subcategorization patterns, word forms, syntactic usage, context, selectional preferences, and topics or subject fields. We also compare the differences between WordNet and NODE. Finally, we compare these features to those identified as significant in supervised learning approaches. 
This paper describes an experiment that uses translation equivalents derived from parallel corpora to determine sense distinctions that can be used for automatic sense-tagging and other disambiguation tasks. Our results show that sense distinctions derived from cross-lingual information are at least as reliable as those made by human annotators. Because our approach is fully automated through all its steps, it could provide means to obtain large samples of “sense-tagged” data without the high cost of human annotation. 
We describe a new version of the Dutch word sense disambiguation system trained and tested on a corrected version of the SENSEVAL-2 data. The system is an ensemble of word experts; each word expert is a memory-based classiﬁer of which the parameters are automatically determined through cross-validation on training material. The original best-performing system, which used only local context features for disambiguation, is further reﬁned by performing additional parallel crossvalidation experiments for optimizing algorithmic parameters and the amount of local context available to each of the word experts’ memory-based kernels. This procedure produces an accuracy of 84.8% on test material, improving on a baseline score of 77.2% and the previous SENSEVAL-2 score of 84.2%. We show that cross-validation overﬁts; had the local context been held constant at two left and right neighbouring words, the system would have scored 85.0%. 
This paper discusses ensembles of simple but heterogeneous classiﬁers for word-sense disambiguation, examining the Stanford-CS224N system entered in the SENSEVAL-2 English lexical sample task. First-order classiﬁers are combined by a second-order classiﬁer, which variously uses majority voting, weighted voting, or a maximum entropy model. While individual ﬁrst-order classiﬁers perform comparably to middle-scoring teams’ systems, the combination achieves high performance. We discuss trade-offs and empirical performance. Finally, we present an analysis of the combination, examining how ensemble performance depends on error independence and task difﬁculty. 
This paper presents an evaluation of an ensemble–based system that participated in the English and Spanish lexical sample tasks of SENSEVAL-2. The system combines decision trees of unigrams, bigrams, and co–occurrences into a single classiﬁer. The analysis is extended to include the SENSEVAL-1 data. 
In this paper we present a maximum entropy Word Sense Disambiguation system we developed which performs competitively on SENSEVAL-2 test data for English verbs. We demonstrate that using richer linguistic contextual features signiﬁcantly improves tagging accuracy, and compare the system’s performance with human annotator performance in light of both ﬁne-grained and coarse-grained sense distinctions made by the sense inventory. 
In this paper, we evaluate the results of the Antwerp University word sense disambiguation system in the English all words task of SENSEVAL-2. In this approach, specialized memory-based wordexperts were trained per word-POS combination. Through optimization by crossvalidation of the individual component classiﬁers and the voting scheme for combining them, the best possible word-expert was determined. In the competition, this word-expert architecture resulted in accuracies of 63.6% (ﬁne-grained) and 64.5% (coarse-grained) on the SENSEVAL-2 test data. In order to better understand these results, we investigated whether classiﬁers trained on different information sources performed differently on the different part-of-speech categories. Furthermore, the results were evaluated in terms of the available number of training items, the number of senses, and the sense distributions in the data set. We conclude that there is no information source which is optimal over all word-experts. Selecting the optimal classiﬁer/voter for each single word-expert, however, leads to major accuracy improvements. We furthermore show that accuracies do not so much depend on the available number of training items, but largely on polysemy and sense distributions.  
Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web. It is available at http://teach-computers.org. We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers. We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert. If successful, the collection process can be extended to create the deﬁnitive corpus of word sense information. 
To achieve translation technology that is adequate for speech-to-speech translation (S2S), this paper introduces a new attempt named Corpus-Centered Computation, (abbreviated to C3 and pronounced c-cube). As opposed to conventional approaches adopted by machine translation systems for written language, C3 places corpora at the center of the technology. For example, translation knowledge is extracted from corpora, translation quality is gauged by referring to corpora and the corpora themselves are normalized by paraphrasing or filtering. High-quality translation has been demonstrated in the domain of travel conversation, and the prospects of this approach are promising due to the benefits of synergistic effects. 
In this paper, we propose a topic detection method using a dialogue history for selecting a scene in the automatic interpretation system (Ikeda et al., 2002). ç The method uses a k-nearest neighbor method for the algorithm, automatically clusters target topics into smaller topics grouped by similarity, and incorporates dialogue history weighted in terms of time to detect and track topics on spoken phrases. From the evaluation of detection performance using test corpus comprised of realistic spoken dialogue, the method has shown to perform better with clustering incorporated, and combined with time-weighted dialogue history of three sentences, gives detection accuracy of 77.0%. 
In this paper, we describe a novel approach to spoken language analysis for translation, which uses a combination of grammar-based phrase-level parsing and automatic classification. The job of the analyzer is to produce a shallow semantic interlingua representation for spoken task-oriented utterances. The goal of our hybrid approach is to provide accurate real-time analyses while improving robustness and portability to new domains and languages. 
In this paper we compare the performance of two methods for speech translation. One is a statistical dependency transduction model using head transducers, the other a case-based transduction model involving a lexical similarity measure. Examples of translated utterance transcriptions are used in training both models, though the case-based model also uses semantic labels classifying the source utterances. The main conclusion is that while the two methods provide similar translation accuracy under the experimental conditions and accuracy metric used, the statistical dependency transduction method is signiﬁcantly faster at computing translations. 
Speech-to-speech translation can be approached using ﬁnite state models and several ideas borrowed from automatic speech recognition. The models can be Hidden Markov Models for the accoustic part, language models for the source language and ﬁnite state transducers for the transfer between the source and target language. A “serial architecture” would use the Hidden Markov and the language models for recognizing input utterance and the transducer for ﬁnding the translation. An “integrated architecture”, on the other hand, would integrate all the models in a single network where the search process takes place. The output of this search process is the target word sequence associated to the optimal path. In both architectures, HMMs can be trained from a source-language speech corpus, and the translation model can be learned automatically from a parallel text training corpus. The experiments presented here correspond to speech-input translations from Spanish to English and from Italian to English, in applications involving the interaction (by telephone) of a customer with the front-desk of a hotel.  recognition (ASR). In ASR the acoustic hidden Markov models (HMMs) can be integrated into the language model, which is typically a ﬁnite-state grammar (e.g. a N-gram). In ST the same HMMs can be integrated in a translation model which consists in a stochastic ﬁnite-state transducer (SFST). Thanks to this integration, the translation process can be efﬁciently performed by searching for an optimal path of states through the integrated network by using well-known optimization procedures such as (beam-search accelerated) Viterbi search. This “integrated architecture” can be compared with the more conventional “serial architecture”, where the HMMs, along with a suitable source language model, are used as a front-end to recognize a sequence of source-language words which is then processed by the translation model. A related approach has been proposed in (Bangalore and Ricardi, 2000; Bangalore and Ricardi, 2001). In any case, a pure pattern-recognition approach can be followed to build the required systems. Acoustic models can be trained from a sufﬁciently large source-language speech training set, in the very same way as in speech recognition. On the other hand, using adequate learning algorithms (Casacuberta, 2000; Vilar, 2000), the translation model can also be learned from a sufﬁciently large training set consisting of source-target parallel text.  
This paper evaluates a direct speech translation Method with waveforms using the Inductive Learning method for short conversation. The method is able to work without conventional speech recognition and speech synthesis because syntactic expressions are not needed for translation in the proposed method. We focus only on acoustic characteristics of speech waveforms of source and target languages without obtaining character strings from utterances. This speech translation method can be utilized for any language because the system has no processing dependent on an individual character of a speciﬁc language. Therefore, we can utilize the speech of a handicapped person who is not able to be treated by conventional speech recognition systems, because we do not need to segment the speech into phonemes, syllables, or words to realize speech translation. Our method is realized by learning translation rules that have acoustic correspondence between two languages inductively. In this paper, we deal with a translation between Japanese and English. 
In this paper we compare two interlingua representations for speech translation. The basis of this paper is a distributional analysis of the C-star II and Nespole databases tagged with interlingua representations. The C-star II database has been partially re-tagged with the Nespole interlingua, which enables us to make comparisons on the same data with two types of interlinguas and on two types of data (Cstar II and Nespole) with the same interlingua. The distributional information presented in this paper show that the Nespole interlingua maintains the language-independence and simplicity of the C-star II speech-actbased approach, while increasing semantic expressiveness and scalability. 
In this paper, we propose a novel paradigm for the Chinese-to-English speech-to-speech (S2S) translation, which is interactive under the guidance of dialogue management. In this approach, the input utterance is first pre-processed and then serially translated by the template-based translator and the interlingua based translator. The dialogue management mechanism (DMM) is employed to supervise the interactive analysis for disambiguation of the input. The interaction is led by the system, so the system always acts on its own initiative in the interactive procedure. In this approach, the complicated semantic analysis is not involved. 
This paper proposes an automatic interpretation system that integrates freestyle sentence translation and parallel text based translation. Free-style sentence translation accepts natural language sentences and translates them by machine translation. Parallel text based translation provides a proper translation for a sentence in the parallel text by referring to a corresponding translation of the sentence and supplements free-style sentence translation. We developed a prototype of an automatic interpretation system for Japanese overseas travelers with parallel text based translation using 9206 parallel bilingual sentences prepared in task-oriented manner. Evaluation results show that the parallel text based translation covers 72% of typical utterances for overseas travel and the user can easily ﬁnd an appropriate sentence from a natural utterance for 64% of typical traveler’s tasks. This indicates that the user can beneﬁt from reliable translation based on parallel text for fundamental utterances necessary for overseas travel. 
Examples from chat interaction are presented to demonstrate that machine translation of written interaction shares many problems with translation of spoken interaction. The potential for common solutions to the problems is illustrated by describing operations that normalize and tag input before translation. Segmenting utterances into small translation units and processing short turns separately are also motivated using data from chat. 
tion” describes a situation in which the testing conditions, e.g. microphone distance, are quite different from what had been seen during training. In that work, we explored a common framework for the identiﬁcation of language, accent and speaker using multilingual phone strings produced by phone recognizers trained on data from different languages. In this paper, we propose and evaluate some improvements, comparing classiﬁcation accuracy as well as realtime performance in our framework. Furthermore, we investigate the beneﬁts that are to be drawn from additional phone recognizers.  2. THE MULTILINGUAL PHONE STRING APPROACH  The basic idea of the multilingual phone string approach  is to use phone strings produced by different context-  independent phone recognizers instead of traditional  short-term acoustic vectors [6]. For the classiﬁcation of an  ¢ audio segment into one of classes of a speciﬁc non-verbal  £ £¥¤¦¢ cue, such phone recognizers together with  £§¤¨¢ phonotactic N-gram models produce an  matrix of  features. A best class estimate is made based solely on this  £ £§¤¨¢ feature matrix. The process relies on the availability of  phone recognizers, and the training of  N-gram  models on their output.  By using information derived from phonotactics rather than directly from acoustics, we expect to cover speaker idiosyncrasy and accent-speciﬁc pronunciations. Since this information is provided from complementary phone recognizers, we anticipate greater robustness under mismatched conditions. Furthermore, the approach is somewhat language independent since the recognizers are trained on data from different languages.  2.1. Phone Recognition The experiments presented here were conducted using two versions of phone recognizers borrowed without modiﬁcation from the GlobalPhone project [5]. All were  Phoneme Error Rate [%] decision rule  trained using our Janus Recognition Toolkit (JRTk).  50 45 TU 40 KR 35 JA 30  PO DE CH SP FR  25 25  30 35 40 45 50 Number of Phonemes  Fig. 1. Error rate vs number of phones for the baseline GlobalPhone phone recognizer set The ﬁrst set of phone recognizers, which we refer to as our baseline, includes recognizers for: Mandarin Chinese (CH), German (DE), French (FR), Japanese (JA), Croatian (KR), Portuguese (PO), Spanish (SP) and Turkish (TU). For each language, the acoustic model consists of a contextindependent 3-state HMM system with 128 Gaussians per state. The Gaussians are on 13 Mel-scale cepstral coefﬁcients with ﬁrst and second order derivatives and power. Following cepstral mean subtraction, linear discriminant analysis reduces the input vector to 32 dimensions. The second set consists of extended phone recognizers, available in 12 languages. Arabic (AR), Korean (KO), Russian (RU) and Swedish (SW) are available in this set in addition to the languages named above for the baseline set. The 12 new phone recognizers were derived from an improved generation of context dependent LVCSR systems which also include vocal tract normalization (VTLN) for speaker normalization. For decoding, we used an unsupervised scheme to ﬁnd the best warp factor for a test speaker and calculate a viterbi alignment based on that speaker’s best warp factor. To improve system speed, we reduced the number of Gaussians per state from 128 to 16; in addition, the feature dimension was halved from 32 to 16 using linear discriminant analysis. Figure 1 shows the phone error rates in relation to the number of modeled phones for eight languages. The error rate correlates with the number of phones used to model this language. Turkish seems to be an exception to this ﬁnding. The error analysis showed that this is due to a very high substi-  © ©  audio ©      phone string phone string  !!!!$$'&&##%""""""  Fig. 2. Training of feature-speciﬁc phonotactic models  tution rate between the closed front vowels e, i, y.  2.2. Phonotactic Model Training  ( ¢ In classifying a non-verbal cue into one of classes,  (0) £1¤2¢ , our feature extraction scheme requires  distinct  3547698) @BADCEAF£ @GA¨HIAD¢ phonotactic models  ,  and  , one  3EPQ6 for each combination of phone recognizer with output  ( ) 354R6S8) class .  is trained on phone strings produced by  3EPT6 ( ) phone recognizer on training audio as shown in  35PU6 Figure 2. During the decoding of the training set, each  is constrained by an equiprobable phonotactic language  model. This procedure does not require transcription at any  level.  2.3. Classiﬁcation  ©V audio      phone string phone string  WWWWWWXXX%%%""""""  PP matrix  ©VY  Fig. 3. MPM-pp classiﬁcation block diagram  £ `a35PU6cb During classiﬁcation, each of phone recognizers  ,  as used for phonotactic model training, decodes the test au-  £ dio segment. Each of the resulting phone strings is scored  ¢ `a35476S8 ) b against each of phonotactic models  . This re-  dGd egfGfBhi698 ) sults in a perplexity matrix , whose  element is  3E4 6S8 ) the perplexity produced by phonotactic model  on the  35P 6 phone string output of phone recognizer . Although we  r 6 eSfGfBh 698 ) (q)p have explored some alternatives, our generic decision al-  gorithm is to propose a class estimate by selecting the  lowest  . Figure 3 depicts this procedure, which  we refer to as MPM-pp.  3. EXPERIMENTS 3.1. Speaker Identiﬁcation (SID) In order to investigate robust speaker identiﬁcation under various distances, a distant-microphone database containing speech recorded from various microphone distances has been collected at the Interactive Systems Laboratory. This database contains 30 native English speakers reading different articles. Each of the ﬁve sessions per speaker are recorded using eight microphones in parallel: one closespeaking microphone (Dis 0), one lapel microphone (Dis L) worn by the speaker, and six other lapel microphones at distances of 1, 2, 4, 5, 6, and 8 feet from the speaker. In a ﬁrst experiment, we compare the performance of the MPM-pp approach using our baseline phone recognizers to the GMM approach. About 7 minutes of spoken speech (approximately 5000 phones) is used for training the PMs, while for training the GMMs one minute was used. The different amount of training data for the two approaches seems to make the comparison quite unfair; however, the training data is used for very different purposes. In the GMM approach, the data is used to train the Gaussian mixtures. In the MPM approach, the data is solely used for creating phonotactic models; no data is used to train the Gaussian mixtures of the phone recognizers. And we have found that with a ﬁxed conﬁguration of GMM structures, adding more training data does not lead to noticeable improvement in performance [4].  Testing Dis 0 Dis 1 Dis 2 Dis 6  Dis 0 100 56.7 56.7 40  Training Dis 1 Dis 2 43.3 30 90 76.7 63.3 93.3 30 60  Dis 6 26.7 40 53.3 83.3  Table 1. GMM performance under matched and mismatched conditions for 10 second segments The GMM approach was tested on 10 seconds of audio, whereas the phone string approach was additionally tested on shorter and longer (up to one minute) segments. We report results for closed-set text-independent speaker identiﬁcation. Table 1 shows the GMM results with one minute training data on 10 seconds of test data. It illustrates that the performance under mismatched conditions degrades considerably when compared to performance under matched conditions. Table 2 shows the identiﬁcation results using each of the 8  Language CH DE FR JA KR PO SP TU Int. of all LM  60s 40s 10s 5s 3s 100 100 56.7 40.0 26.7 80.0 76.7 50.0 33.3 26.7 70.0 56.7 46.7 16.7 13.3 30.0 30.0 36.7 26.7 16.7 40.0 33.3 30.0 26.7 36.7 76.7 66.7 33.3 20.0 10.0 70.0 56.7 30.0 20.0 16.7 53.3 50.0 30.0 16.7 20.0 96.7 96.7 96.7 93.3 80  Table 2. MPM-pp SID rate on varying test lengths at Dis 0  baseline phone recognizers individually and their combination results for Dis 0 under matched conditions. This shows that multiple recognizers collectively compensate for the poor performance of single recognizers, an effect which becomes even more important for shorter test utterances.  Test Length Dis 0 Dis L Dis 1 Dis 2 Dis 4 Dis 5 Dis 6 Dis 8  60s 40s 10s 5s 96.7 96.7 96.7 93.3 96.7 96.7 86.7 70.0 90.0 90.0 76.6 70.0 96.7 96.7 93.3 83.3 96.7 93.3 80.0 76.7 93.3 93.3 90.0 76.7 83.3 86.7 83.3 80.0 93.3 93.3 86.7 66.7  Table 3. MPM-pp SID rate on varying test lengths at matched training and testing distances  Test length Dis 0 Dis L Dis 1 Dis 2 Dis 4 Dis 5 Dis 6 Dis 8  60s 40s 10s 5s 96.7 96.7 96.7 90.0 96.7 100 90.0 66.7 93.3 93.3 80.0 70.0 96.7 96.7 86.7 80.0 96.7 96.7 93.3 80.0 93.3 93.3 86.7 70.0 93.3 86.7 83.3 60.0 93.3 93.3 86.7 70.0  Table 4. MPM-pp SID rate on varying test lengths at mismatched training and testing distance Table 3 and Table 4 compare the identiﬁcation results for all distances for different test utterance lengths under matched and mismatched conditions, respectively. Under  matched conditions, training and testing data are drawn  from the same microphone. Under mismatched conditions,  we do not know the test segment distance; we make use  sutwv 354x698) of all  sets of  phonotactic models, where  s is the number of distances, and modify our decision  (y)p t ) e9 r 6 354x6S8) 8!h C rule to estimate  , where  H is the index over phone recognizers, is the index over  @AA¦s speaker phonotactic models, and  . These two  tables indicate that the performance of MPM-pp, unlike  that of GMM, is comparable for matched and mismatched  conditions.  We conducted additional experiments to determine the impact of the improved GlobalPhone recognizers on the identiﬁcation rate for this task. To that end, we used all 8 baseline recognizers and only the corresponding 8 of the 12 available improved recognizers. Table 5 compares the speaker identiﬁcation rate on matched conditions for 60 seconds of audio. The comparison indicates that an improvement in phone error rate leads to slight improvements in speaker identiﬁcation rate for distances Dis 0 and Dis 5. Performance decreases for Dis L, while for Dis 6 the improved recognizers outperform the baseline signiﬁcantly. Overall we cannot conclude from these results that better phone recognizers result in a higher identiﬁcation rate. However, we can summarize that the improved engines show an identiﬁcation performance of 93.3% or higher on all distances for matched conditions on 60 seconds of audio, in spite of the drastic reduction in acoustic model parameter dimensions.  Distance Dis 0 Dis L Dis 1 Dis 2 Dis 4 Dis 5 Dis 6 Dis 8  phone recognizers baseline improved  96.7  96.7  96.7  93.3  90.0  93.3  96.7  96.7  96.7  96.7  93.3  96.7  83.3  96.7  93.3  93.3  Table 5. Comparison of MPM-pp classiﬁcation using baseline and improved phone recognizers on matched conditions for 60 seconds of audio (SID rate in %)  3.2. Accent Identiﬁcation (AID) In previous experiments on accent identiﬁcation we used the MPM-pp approach to identify native and non-native speakers of English and to identify speakers of varying proﬁciency levels in English.  ¢9 use  native non-native  training  3  7  r ¢9 testing  2  training 318  5 680  r9 testing  93  210  training 23.1 min 83.9 min  testing 7.1 min 33.8 min  Table 6. Number of speakers, total number of utterances and total length of audio for native and non-native classes  In our current experiments, we have augmented the number  of phonotactic models used to classify utterances. We de-  code training data from each class using the baseline phone  recognizer for Chinese and run our original experiments  3EPedXf`hg'iejiklPGxTjn¤myoyjipGPGjn3rqyjnst3eb `vurwBb with a new bank of phonotactic models in 7 languages: the  original 6  , plus  .  During classiﬁcation, the phonotactic models produce  a perplexity matrix for the test utterance to which we apply  our lowest average perplexity decision rule; the class with  the lower perplexity is identiﬁed as the class of the test  utterance.  On our evaluation set of 303 utterances for 2-way classiﬁcation between native and non-native speakers, our classiﬁcation accuracy improves from 93.7% using models in 6 languages to 97.7% using models in 7 languages. An examination of the average perplexity of each class of phonotactic model over all test utterances reveals the improved separability of the classes. The average perplexity of non-native models on non-native data is lower than the perplexity of native models on that data, and the discrepancy between these numbers grows after adding training data decoded in an additional language. The native models became less separable on average but discriminatory power still improved overall. Table 7 shows these average perplexities for our previous and current experiments.  # of phone recognizers 6 6 + CH  Phonotactic model non-native native non-native native  Utterance class  non-native native  29.1  31.7  32.5  28.5  28.9  34.1  32.8  31.1  Table 7. Average phonotactic perplexities for native and non-native classes using 6 phone recognizers (top) versus 7 (bottom)  In the proﬁciency-level experiments, we apply the MPM-pp approach to classify utterances from non-native speakers according to assigned speaker proﬁciency class. The original non-native data has been labelled with the proﬁciency of each speaker on the basis of a standardized evaluation procedure conducted by trained proﬁciency raters [7], and we attempt to classify non-native speakers from three classes according to their proﬁciency. Class 1 represents the lowest proﬁciency speakers, class 2 contains intermediate speakers, and class 3 contains the high proﬁciency speakers. Proﬁles of the testing and training data for these experiments are shown in Table 8.  ¢9 r¢ 9 rz9 ave. prof  use training testing training testing training testing training testing  class 1 3 1 146 78 23.9 min 13.8 min 1.33 1.33  class 2 12 5 564 477 82.5 min 59.0 min 2.00 2.00  class 3 4 1 373 124 40.4 min 13.5 min 2.89 2.89  Table 8. Number of speakers, total number of utterances, total length of audio and average speaker proﬁciency score per proﬁciency class We have added phonotactic models trained on Chinese recognizer output to this experiment as well, and gained a small improvement over our results using models in 6 languages. Table 9 displays two confusion matrices for this task, one showing original results and one showing results with the added Chinese phone recognizer.  # of Phone recognizers 6 6 + CH  System hypothesis Class 1 Class 2 Class 3 Class 1 Class 2 Class 3  Actual proﬁciency Class 1 Class 2 Class 3  8  3  19  8  41  61  2  12  99  8  5  17  6  53  51  
 We propose a test set selection method toç  sensitively evaluate the performance of a  speech translation system. The proposed  method chooses the most sensitive test  sentences by removing insensitive  sentences iteratively. Experiments are  conducted on the ATR-MATRIX speech  translation system, developed at ATR  Interpreting  Telecommunications  Research Laboratories. The results show  the effectiveness of the proposed method.  According to the results, the proposed  method can reduce the test set size to less  than 40% of the original size while  improving evaluation reliability.  
Performance and usability of realworld speech-to-speech translation systems, like the one developed within the Nespole! project, are aﬀected by several aspects that go beyond the pure translation quality provided by the underlying components of the system. In this paper we describe these aspects as perspectives along which we have evaluated the Nespole! system. Four main issues are investigated: (1) assessing system performance under various network traﬃc conditions; (2) a study on the usage and utility of multi-modality in the context of multilingual communication; (3) a comparison of the features of the individual speech recognition engines, and (4) an end-to-end evaluation of the system. 
Significant progress has been made in the field of human language technologies. Various tasks like continuous speech recognition for large vocabulary, speaker and language identification, spoken information inquiry, information extraction and cross-language retrieval in restricted domains are today feasible and different prototypes and systems are running. The spoken translation problem on the other hand is still a significant challenge: “Good text translation was hard enough to pull off. Speech to speech MT was beyond going to the Moon – it was Mars…” [Steve Silbermann, Wired Magazine]. Considering the major achievements of the last years obtained in the field and the related challenges, a question arise: what next ? Is it possible to foresee in the next decade real services and applications ? How can we reach this goal ? Shall we rethink the approach ? Shall we need much more critical mass ? How about data ? To answer to these questions a new preparatory action, TC_STAR_P, funded in the V framework, has been settled in Europe. Goals, objective and activities of this preparatory action will also be discussed in this paper  
This paper describes a system for the unsupervised learning of morphological sufﬁxes and stems from word lists. The system is composed of a generative probability model and a novel search algorithm. By examining morphologically rich subsets of an input lexicon, the search identiﬁes highly productive paradigms. Quantitative results are shown by measuring the accuracy of the morphological relations identiﬁed. Experiments in English and Polish, as well as comparisons with other recent unsupervised morphology learning algorithms demonstrate the effectiveness of this technique. 
We present two methods for unsupervised segmentation of words into morphemelike units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The ﬁrst method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current stateof-the-art system. 
The ﬁrst morphological learner based upon the theory of Whole Word Morphology (Ford et al., 1997) is outlined, and preliminary evaluation results are presented. The program, Whole Word Morphologizer, takes a POS-tagged lexicon as input, induces morphological relationships without attempting to discover or identify morphemes, and is then able to generate new words beyond the learning sample. The accuracy (precision) of the generated new words is as high as 80% using the pure Whole Word theory, and 92% after a post-hoc adjustment is added to the routine. The aim of this project is to develop a computational model employing the theory of whole word morphology (Ford et al., 1997) capable on the one hand of identifying morphological relations within a list of words from any one of a wide variety of languages and, on the other, of putting that knowledge to use in creating previously unseen word forms. A small application called Whole Word Morphologizer which does just this is outlined and discussed. In particular, this approach is set against the literature on computational morphology as an entirely different way of doing things which has the potential to be generalized to all known varieties of morphology in the world’s languages, a feature not shared by previous methods. As it is based on a model of the mental lexicon in which all entries are entire, fully ﬂedged words, this project also serves as an empirical demonstration that a word-based morphological  theory that rejects the notion of morpheme as minimal unit of form and meaning (and/or grammatical properties) is viable from the point of view of acquisition as well as generation. 
This paper describes the results of some experiments exploring statistical methods to infer syntactic categories from a raw corpus in an unsupervised fashion. It shares certain points in common with Brown et at (1992) and work that has grown out of that: it employs statistical techniques to derive categories based on what words occur adjacent to a given word. However, we use an eigenvector decomposition of a nearest-neighbor graph to produce a two-dimensional rendering of the words of a corpus in which words of the same syntactic category tend to form clusters and neighborhoods. We exploit this technique for extending the value of automatic learning of morphology. In particular, we look at the suffixes derived from a corpus by unsupervised learning of morphology, and we ask which of these suffixes have a consistent syntactic function (e.g., in English, -ed is primarily a mark of verbal past tense, does but –s marks both noun plurals and 3rd person present on verbs). 
We present an algorithm that takes an unannotated corpus as its input, and returns a ranked list of probable morphologically related pairs as its output. The algorithm tries to discover morphologically related pairs by looking for pairs that are both orthographically and semantically similar, where orthographic similarity is measured in terms of minimum edit distance, and semantic similarity is measured in terms of mutual information. The procedure does not rely on a morpheme concatenation model, nor on distributional properties of word substrings (such as afﬁx frequency). Experiments with German and English input give encouraging results, both in terms of precision (proportion of good pairs found at various cutoff points of the ranked list), and in terms of a qualitative analysis of the types of morphological patterns discovered by the algorithm. 
We describe here a supervised learning model that, given paradigms of related words, learns the morphological and phonological rules needed to derive the paradigm. The model can use its rules to make guesses about how novel forms would be inflected, and has been tested experimentally against the intuitions of human speakers. 
We present a phonological probabilistic contextfree grammar, which describes the word and syllable structure of German words. The grammar is trained on a large corpus by a simple supervised method, and evaluated on a syllabiﬁcation task achieving 96.88% word accuracy on word tokens, and 90.33% on word types. We added rules for English phonemes to the grammar, and trained the enriched grammar on an English corpus. Both grammars are evaluated qualitatively showing that probabilistic context-free grammars can contribute linguistic knowledge to phonology. Our formal approach is multilingual, while the training data is language-dependent. 
This paper describes a parsing/generation system for finite verbal forms in Akkadian, with the possible addition of suffixes, implemented in Prolog. The work described provides the framework and engine to interpret the D, N, and G stems along with accusative, dative and ventive endings. 
We apply default inheritance hierarchies to generating the morphology of Hebrew verbs. Instead of lexically listing each of a word form’s various parts, this strategy represents inﬂectional exponents as markings associated with the application of rules by which complex word forms are deduced from simpler roots or stems. The high degree of similarity among verbs of different binyanim allows us to formulate general rules; these general rules are, however, sometimes overridden by binyan-speciﬁc rules. Similarly, a verb’s form within a particular binyan is determined both by default rules and by overriding rules speciﬁc to individual verbs. Our result is a concise set of rules deﬁning the morphology of all strong verbs in all binyanim. We express these rules in KATR, both a formalism for default inheritance hierarchies and associated software for computing the forms speciﬁed by those rules. As we describe the rules, we point out general strategies for expressing morphology in KATR and we discuss KATR’s advantages over ordinary DATR for the representation of morphological systems. 
Many papers have discussed different aspects of Arabic verb morphology. Some of them used patterns; others used patterns and affixes. But very few have discussed Arabic noun morphology particularly for nouns that are not derived from verbs. In this paper we describe a learning system that can analyze Arabic nouns to produce their morphological information and their paradigms with respect to both gender and number using a rule base that uses suffix analysis as well as pattern analysis. The system utilizes user-feedback to classify the noun and identify the group that it belongs to. 
Semitic languages pose a problem to Natural Language Processing since most of the vowels are omitted from written prose, resulting in considerable ambiguity at the word level. However, while reading text, native speakers can generally vocalize each word based on their familiarity with the lexicon and the context of the word. Methods for vowel restoration in previous work involving morphological analysis concentrated on a single language and relied on a parsed corpus that is difficult to create for many Semitic languages. We show that Hidden Markov Models are a useful tool for the task of vowel restoration in Semitic languages. Our technique is simple to implement, does not require any language specific knowledge to be embedded in the model and generalizes well to both Hebrew and Arabic. Using a publicly available version of the Bible and the Qur’an as corpora, we achieve a success rate of 86% for restoring the exact vowel pattern in Arabic and 81% in Hebrew. For Hebrew, we also report on 87% success rate for restoring the correct phonetic value of the words. 
 Abstract:  The paper presents a rapid method of developing a shallow Arabic morphological analyzer. The analyzer will only be concerned with generating the possible roots of any given Arabic word. The analyzer is based on automatically derived rules and statistics. For evaluation, the analyzer is compared to a commercially available Arabic Morphological Analyzer.  
We describe the design and implementation of a question answering (QA) system called QARAB. It is a system that takes natural language questions expressed in the Arabic language and attempts to provide short answers. The system’s primary source of knowledge is a collection of Arabic newspaper text extracted from Al-Raya, a newspaper published in Qatar. During the last few years the information retrieval community has attacked this problem for English using standard IR techniques with only mediocre success. We are tackling this problem for Arabic using traditional Information Retrieval (IR) techniques coupled with a sophisticated Natural Language Processing (NLP) approach. To identify the answer, we adopt a keyword matching strategy along with matching simple structures extracted from both the question and the candidate documents selected by the IR system. To achieve this goal, we use an existing tagger to identify proper names and other crucial lexical items and build  lexical entries for them on the fly. We also carry out an analysis of Arabic question forms and attempt a better understanding of what kinds of answers users find satisfactory. The paucity of studies of real users has limited results in earlier research. 
This article describes the construction of a morphological, syntactic and semantic analyzer to operate a high-grade search engine for Hebrew texts. A good search engine must be complete and accurate. In Hebrew or Arabic script most of the vowels are not written, many particles are attached to the word without space, a double consonant is written with one letter, and some letters signify both vowels and consonants. Thus, almost every string of characters may designate many words (the average in Hebrew is almost three words). As a consequence, deciphering a word necessitates reading the whole sentence. Our model is Fillmore’s framework of an expression with a verb as its center. The engine eliminates readings of words unsuited to the syntax or the semantic structure of the sentence. In every verbal entry of our conceptual dictionary the features of the noun phrases (NP’s) required by the verb are included. When all the correct readings of all the strings of characters in the sentence have been identified, the program chooses the proper occurrences of the searched word in the text. Approximately 95% of the results by our search engine match those in the query. 1.Introduction It is easy to construct a search engine that, in a given text, will find all the occurrences of the string of characters specified in the query. In Hebrew script, however, the string of characters that makes up a word may also be interpreted  as designating other words. Almost every word in Hebrew script can be read as one of an average of three words. This is because Hebrew script is fundamentally defective: (1) Most vowels in a given word have no sign in the script. (2) Particles are attached with no intervening space to the string of characters that makes up the following word. (3) A geminated consonant is written as one letter, like a not-geminated consonant. (4) Several letters serve as both vowels and consonants. Threfore, it is impossible to identify the word stated in the query by its form: if we try to do so, we would obtain all the occurrences which are written in the same way but are, in fact, different words. Since only 20-30% of the words so obtained are actually occurrences of the required word, the users have to check every word in the result obtained in order to decide whether it is actually the one they want.1 In order to solve this problem, some systems recommend that every query should contain some other words that are often found close to the stipulated word.2 But such a search may lead to a loss of important occurrences of the required word. Neither a frequency list of words nor another statistical device can be an ultimate answer in our search of accurate and full device. A statistical approach ensures that some mistakes or 
We present and evaluate a randomized local search procedure for selecting sentences to include in a multidocument summary. The search favors the inclusion of adjacent sentences while penalizing the selection of repetitive material, in order to improve intelligibility without unduly aﬀecting informativeness. Sentence similarity is determined using both surface-oriented measures and semantic groups obtained from merging the output templates of an information extraction subsystem. In a comparative evaluation against two DUC-like baselines and three simpler versions of our system, we found that our randomized local search method provided substantial improvements in both content and intelligibility, while the use of the IE groups also appeared to contribute a small further improvement in content. 
This paper examines the role that summaries can play in document retrieval. Thirty searches are applied to full-text and summaries only in large document collections, and the results are evaluated using two different evaluation scopes. The results support the view that those customer segments who want smaller answer sets focused on highly relevant documents benefit from limiting their searches to summaries. On the other hand, those customer segments who wish to retrieve all references to some topic should continue to search full-text. 
In this paper we discuss manual and automatic evaluations of summaries using data from the Document Understanding Conference 2001 (DUC-2001). We first show the instability of the manual evaluation. Specifically, the low interhuman agreement indicates that more reference summaries are needed. To investigate the feasibility of automated summary evaluation based on the recent BLEU method from machine translation, we use accumulative n-gram overlap scores between system and human summaries. The initial results provide encouraging correlations with human judgments, based on the Spearman rank-order correlation coefficient. However, relative ranking of systems needs to take into account the instability. 
We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition. To make the SVM training with the available largest corpus – the GENIA corpus – tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information. In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning. Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy. The proposed new features also contribute to improve the accuracy. We compare our SVMbased recognition system with a system using Maximum Entropy tagging method. 
Current information extraction efforts in the biomedical domain tend to focus on finding entities and facts in structured databases or MEDLINE abstracts. We apply a gene and protein name tagger trained on Medline abstracts (ABGene) to a randomly selected set of full text journal articles in the biomedical domain. We show the effect of adaptations made in response to the greater heterogeneity of full text. 
We studied contrast and variability in a corpus of gene names to identify potential heuristics for use in performing entity identification in the molecular biology domain. Based on our findings, we developed heuristics for mapping weakly matching gene names to their official gene names. We then tested these heuristics against a large body of Medline abstracts, and found that using these heuristics can increase recall, with varying levels of precision. Our findings also underscored the importance of good information retrieval and of the ability to disambiguate between genes, proteins, RNA, and a variety of other referents for performing entity identification with high precision. 
We propose two internal methods for accenting unknown words, which both learn on a reference set of accented words the contexts of occurrence of the various accented forms of a given letter. One method is adapted from POS tagging, the other is based on ﬁnite state transducers. We show experimental results for letter e on the French version of the Medical Subject Headings thesaurus. With the best training set, the tagging method obtains a precision-recall breakeven point ¦ of 84.2 4.4% and the transducer method ¦ 83.8 4.5% (with a baseline at 64%) for the unknown words that contain this letter. A consensus combination of both in- ¦ creases precision to 92.0 3.7% with a re- call of 75%. We perform an error analysis and discuss further steps that might help improve over the current performance. 
This paper describes the basic philosophy and implementation of MPLUS (M+), a robust medical text analysis tool that uses a semantic model based on Bayesian Networks (BNs). BNs provide a concise and useful formalism for representing semantic patterns in medical text, and for recognizing and reasoning over those patterns. BNs are noise-tolerant, and facilitate the training of M+. 
NLP systems will be more portable among medical domains if acquisition of semantic lexicons can be facilitated. We are pursuing lexical acquisition through the syntactic relationships of words in medical corpora. Therefore we require a syntactic parser which is flexible, portable, captures head-modifier pairs and does not require a large training set. We have designed a dependency grammar parser that learns through a transformational-based algorithm. We propose a novel design for templates and transformations which capitalize on the dependency structure directly and produces human-readable rules. Our parser achieved a 77% accurate parse training on only 830 sentences. Further work will evaluate the usefulness of this parse for lexical acquisition. 
We describe our use of an existing resource, the Mouse Anatomical Nomenclature, to improve a symbolic interface to anatomically-indexed gene expression data. The goal is to reduce user effort in specifying anatomical structures of interest and increase precision and recall. 
Objectives: To automatically extend downwards an existing biomedical terminology using a corpus and both lexical and terminological knowledge. Methods: Adjectival modifiers are removed from terms extracted from the corpus (three million noun phrases extracted from MEDLINE), and demodified terms are searched for in the terminology (UMLS Metathesaurus, restricted to disorders and procedures). A phrase from MEDLINE becomes a candidate term in the Metathesaurus if the following two requirements are met: 1) a demodified term created from this phrase is found in the terminology and 2) the modifiers removed to create the demodified term also modify existing terms from the terminology, for a given semantic category. A manual review of a sample of candidate terms was performed. Results: Out of the 3 million simple phrases randomly extracted from MEDLINE, 125,000 new terms were identified for inclusion in the UMLS. 83% of the 1000 terms reviewed manually were associated with a relevant UMLS concept. Discussion: The limitations of this approach are discussed, as well as adaptation and generalization issues.  
Document retrieval in languages with a rich and complex morphology – particularly in terms of derivation and (single-word) composition – suffers from serious performance degradation with the stemming-only query-term-to-text-word matching paradigm. We propose an alternative approach in which morphologically complex word forms are segmented into relevant subwords (such as stems, named entities, acronyms), and subwords constitute the basic unit for indexing and retrieval. We evaluate our approach on a large biomedical document collection. 
We describe the use of clinical data present in the medical record to determine the relevance of research evidence from literature databases. We studied the effect of using automated knowledge approaches as compared to physician’s selection of articles, when using a traditional information retrieval system. Three methods were evaluated. The first method identified terms and their semantics and relationships in the patient’s record to build a map of the record, which was represented in conceptual graph notation. This approach was applied to data in an individual’s medical record and used to score citations retrieved using a graph matching algorithm. The second method identified associations between terms in the medical record, assigning them semantic types and weights based on the co-occurrence of these associations in citations of biomedical literature. The method was applied to data in an individual’s medical record and used to score citations. The last method combined the first two. The results showed that physicians agreed better with each other than with the automated methods. However, we found a significant positive relation between physicians’ selection of abstracts and two of the methods. We believe the results encourage the use of clinical data to determine the relevance of medical literature to the care of individual patients.  
Information Extraction (IE), deﬁned as the activity to extract structured knowledge from unstructured text sources, offers new opportunities for the exploitation of biological information contained in the vast amounts of scientiﬁc literature. But while IE technology has received increasing attention in the area of molecular biology, there have not been many examples of IE systems successfully deployed in end-user applications. We describe the development of PASTAWeb, a WWWbased interface to the extraction output of PASTA, an IE system that extracts protein structure information from MEDLINE abstracts. Key characteristics of PASTAWeb are the seamless integration of the PASTA extraction results (templates) with WWWbased technology, the dynamic generation of WWW content from ‘static’ data and the fusion of information extracted from multiple documents. 
Most dialogue architectures are either pipelined or, if agent-based, are restricted to a pipelined ﬂowof-information. The TRIPS dialogue architecture is agent-based and asynchronous, with several layers of information ﬂow. We present this architecture and the synchronization issues we encountered in building a truly distributed, agentbased dialogue architecture. 
This paper describes the prosodic transcription of a corpus of Hong Kong English and some preliminary findings on the communicative role of intonation in Hong Kong English. 
We present the motivation for and design of an experiment to evaluate the usefulness of cross-media cues, phrases such as 'See Figure 1'.  
Recent work on natural language processing systems is aimed at more conversational, context-adaptive systems in multiple domains. An important requirement for such a system is the automatic detection of the domain and a domain consistency check of the given speech recognition hypotheses. We report a pilot study addressing these tasks, the underlying data collection and investigate the feasibility of annotating the data reliably by human annotators. 
We present a new framework for rapid development of mixed-initiative dialog systems. Using this framework, a developer can author sophisticated dialog systems for multiple channels of interaction by specifying an interaction modality, a rich task hierarchy and task parameters, and domain-specific modules. The framework includes a dialog history that tracks input, output, and results. We present the framework and preliminary results in two application domains. 
Technological development has made computer interaction more common and also commercially feasible, and the number of interactive systems has grown rapidly. At the same time, the systems should be able to adapt to various situations and various users, so as to provide the most eﬃcient and helpful mode of interaction. The aim of the Interact project is to explore natural human-computer interaction and to develop dialogue models which will allow users to interact with the computer in a natural and robust way. The paper describes the innovative goals of the project and presents ways that the Interact system supports adaptivity on diﬀerent system design and interaction management levels. 
The Why-Atlas tutoring system presents students with qualitative physics questions and encourages them to explain their answers via natural language. Although there are inexpensive techniques for analyzing explanations, we claim that better understanding is necessary for use within tutoring systems. In this paper we describe how Why-Atlas creates and utilizes a proof-based representation of student essays. We describe how it creates the proof given the output of sentence-level understanding, how it uses the proofs to give students feedback, some preliminary runtime measures, and the work we are currently doing to derive additional beneﬁts from a proof-based approach for tutoring applications. 
The paper deals with conditional responses of the form “Not if c/Yes if c” in reply to a question “?q” in the context of information-seeking dialogues. A conditional response is triggered if the obtainability of q depends on whether c holds: The response indicates a possible need to ﬁnd alternative solutions, opening a negotiation in the dialogue. The paper discusses the conditions under which conditional responses are appropriate, and proposes a uniform approach to their generation and interpretation. 
This paper presents work on using Bayesian networks for the dialogue act recognition module of a dialogue system for Dutch dialogues. The Bayesian networks can be constructed from the data in an annotated dialogue corpus. For two series of experiments - using different corpora but the same annotation scheme - recognition results are presented and evaluated.  environment: a) a ﬁrst person view of the visible part of the 3D virtual theatre and b) an abstract 2D map of the ﬂoor of the building the user is visiting. This map is shown in a separate window. In a multi-modal interaction the user can point at locations or objects on the 2D map and either ask information about that object or location or he can ask the assistant to bring him to the location pointed at.  
In human–computer interaction systems using natural language, the recognition of the topic from user’s utterances is an important task. We examine two diﬀerent perspectives to the problem of topic analysis needed for carrying out a successful dialogue. First, we apply selforganized document maps for modeling the broader subject of discourse based on the occurrence of content words in the dialogue context. On a Finnish corpus of 57 dialogues the method is shown to work well for recognizing subjects of longer dialogue segments, whereas for individual utterances the subject recognition history should perhaps be taken into account. Second, we attempt to identify topically relevant words in the utterances and thus locate the old information (’topic words’) and new information (’focus words’). For this we deﬁne a probabilistic model and compare different methods for model parameter estimation on a corpus of 189 dialogues. Moreover, the utilization of information regarding the position of the word in the utterance is found to improve the results. 
In this paper, we give an account of a simple kind of collaborative negotiative dialogue. We also sketch a formalization of this account and discuss its implementation in a dialogue system. 
We explain dialogue management techniques for collaborative activities with humans, involving multiple concurrent tasks. Conversational context for multiple concurrent activities is represented using a “Dialogue Move Tree” and an “Activity Tree” which support multiple interleaved threads of dialogue about different activities and their execution status. We also describe the incremental message selection, aggregation, and generation method employed in the system. 
We show how Bayesian networks and related probabilistic methods provide an efﬁcient way of capturing the complex balancing of different factors that determine interpretation and generation in dialogue. As a case study, we show how a probabilistic approach can be used to model anaphora resolution in dialogue1. 
This document proposes a new taxonomy for describing the quality of services which are based on spoken dialogue systems (SDSs), and operated via a telephone interface. It is used to classify instrumentally or expert–derived dialogue and system measures, as well as quality features perceived by the user of the service. A comparison is drawn to the quality of human–to–human telephone services, and implications for the development of evaluation frameworks such as PARADISE are discussed. 
While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and humanhuman dialogues, their utility is limited by the huge effort involved in handlabelling dialogues with a dialogue act labelling scheme. In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues. We report results for training and testing an automatic classiﬁer to label the information provider’s utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags. We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain. Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available. 
This paper reports an exploratory study of the grounding styles of older dyads, namely, the characteristic ways in which they mutually agree to have shared a piece of information in dialogue. On the basis of Traum’s classification of grounding acts, we conducted an exploratory comparison of dialogue data on older and younger dyads, and found that a fairly clear contrast holds mainly in the types of acknowledgement utterances used by the two groups. We will discuss the implications of this contrast, concerning how some of the negative stereotypes about conversations with older people may arise from this difference in grounding styles. 
Dialogue Acts (DAs) which explicitly ensure mutual understanding are frequent in dialogues between cancer patients and health professionals. We present examples, and argue that this arises from the health- critical nature of these dialogues.  through courtesy and appreciation (thanks, apologies). Protracted closing sequences are characteristic, and tend to have elements of both. We interpret these patterns as direct responses to the goal-directed and potentially health-critical nature of these dialogues. 2 Rare dialogue acts  
Why do few working spoken dialogue systems make use of dialogue models in their dialogue management? We ﬁnd out the causes and propose a generic dialogue model. It promises to bridge the gap between practical dialogue management and (pattern-based) dialogue model through integrating interaction patterns with the underling tasks and modeling interaction patterns via utterance groups using a high level construct different from dialogue act. 
We describe a mechanism which receives as input a segmented argument composed of NL sentences, and generates an interpretation. Our mechanism relies on the Minimum Message Length Principle for the selection of an interpretation among candidate options. This enables our mechanism to cope with noisy input in terms of wording, beliefs and argument structure; and reduces its reliance on a particular knowledge representation. The performance of our system was evaluated by distorting automatically generated arguments, and passing them to the system for interpretation. In 75% of the cases, the interpretations produced by the system matched precisely or almost-precisely the representation of the original arguments. 
This paper advocates the use of games in teaching NLP/CL in cases where computational experiments are impossible because the students lack the necessary skills. To show the viability of this approach, three games are described which together teach students about the parsing process. The paper also shows how the specific game formats and rules can be tuned to the teaching goals and situations, thus opening the way to the creation of further teaching games. 
This paper offers a detailed lesson plan on the forwardbackward algorithm. The lesson is taught from a live, commented spreadsheet that implements the algorithm and graphs its behavior on a whimsical toy example. By experimenting with different inputs, one can help students develop intuitions about HMMs in particular and Expectation Maximization in general. The spreadsheet and a coordinated follow-up assignment are available. 
We propose the creation of a web-based training framework comprising a set of topics that revolve around the use of feature structures as the core data structure in linguistic theory, its formal foundations, and its use in syntactic processing. 
This paper describes an undergraduate program in Language Technology that we have developed at Macquarie University. We question the industrial relevance of much that is taught in NLP courses, and emphasize the need for a practical orientation as a means to growing the size of the field. We argue that a more evangelical approach, both with regard to students and industry, is required. The paper provides an overview of the material we cover, and makes some observations for the future on the basis of our experiences so far. 
This paper describes a new Cornell University course serving as a nonprogramming introduction to computer science, with natural language processing and information retrieval forming a crucial part of the syllabus. Material was drawn from a wide variety of topics (such as theories of discourse structure and random graph models of the World Wide Web) and presented at some technical depth, but was massaged to make it suitable for a freshman-level course. Student feedback from the ﬁrst running of the class was overall quite positive, and a grant from the GE Fund has been awarded to further support the course’s development and goals. 
This paper discusses the establishment and implementation of a curriculum for teaching NLP. At the core are two classes which involve some theoretical background, extensive hands-on experience with state-ofthe-art technologies, and practical application in the form of an intensive programming project. Issues involving interdisciplinary coordination, curriculum design, and challenges in teaching this discipline are discussed. 
In this paper we argue that the GATE architecture and visual development environment can be used as an eﬀective tool for teaching language engineering and computational linguistics. Since GATE comes with a customisable and extendable set of components, it allows students to get hands-on experience with building NLP applications. GATE also has tools for corpus annotation and performance evaluation, so students can go through the entire application development process within its graphical development environment. Finally, it oﬀers comprehensive Unicode-compliant multilingual support, thus allowing students to create components for languages other than English. Unlike other NLP teaching tools which were designed speciﬁcally and only for this purpose, GATE is a system developed for and used actively in language engineering research. This unique duality allows students to contribute to research projects and gain skills in embedding HLT in practical applications. 
NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset. 
This paper reports on a course whose aim is to introduce Formal Language Theory to students with little formal background (mostly linguistics students). The course was ﬁrst taught at the European Summer School for Logic, Language and Information to a mixed audience of students, undergraduate, graduate and postgraduate, with various backgrounds. The challenges of teaching such a course include preparation of highly formal, mathematical material for students with relatively little formal background; attracting the attention of students of different backgrounds; preparing examples that will emphasize the practical importance of material which is basically theoretical; and evaluation of students’ achievements. 
This paper presents a simple and versatile tree-rewriting lexicalized grammar formalism, TAGLET, that provides an effective scaffold for introducing advanced topics in a survey course on natural language processing (NLP). Students who implement a strong competence TAGLET parser and generator simultaneously get experience with central computer science ideas and develop an effective starting point for their own subsequent projects in data-intensive and interactive NLP. 
The paper gives a review of teaching Computational Linguistics (CL) at the University of Tartu. The current curriculum foresees the possibility of studying CL as an independent 4-year subject in the Faculty of Philosophy on the bachelor stage. In connection with the higher education reform in Estonia, new curricula will be introduced from the next study year where the 3-year bachelor stage will be followed by a 2year master’s stage. It will then be possible to study CL proceeding from two paths: in the Faculty of Philosophy, and additionally also in the Faculty of Mathematics and Computer Science. This way two types of specialists will be trained who will hopefully be able to complement each other in team-work. 
Weighted ﬁnite-state transducers suffer from the lack of a training algorithm. Training is even harder for transducers that have been assembled via ﬁnite-state operations such as composition, minimization, union, concatenation, and closure, as this yields tricky parameter tying. We formulate a “parameterized FST” paradigm and give training algorithms for it, including a general bookkeeping trick (“expectation semirings”) that cleanly and efﬁciently computes expectations and gradients. 
We describe a speedup for training conditional maximum entropy models. The algorithm is a simple variation on Generalized Iterative Scaling, but converges roughly an order of magnitude faster, depending on the number of constraints, and the way speed is measured. Rather than attempting to train all model parameters simultaneously, the algorithm trains them sequentially. The algorithm is easy to implement, typically uses only slightly more memory, and will lead to improvements for most maximum entropy problems. 
Natural-Language Generation from ﬂat semantics is an NP-complete problem. This makes it necessary to develop algorithms that run with reasonable efﬁciency in practice despite the high worstcase complexity. We show how to convert TAG generation problems into dependency parsing problems, which is useful because optimizations in recent dependency parsers based on constraint programming tackle exactly the combinatorics that make generation hard. Indeed, initial experiments display promising runtimes. 
 We show that it is possible to learn the  contexts for linguistic operations which  map a semantic representation to a  surface syntactic tree in sentence  realization with high accuracy. We cast  the problem of learning the contexts for  the linguistic operations as  classification tasks, and apply  straightforward machine learning  techniques, such as decision tree  learning. The training data consist of  linguistic features extracted from  syntactic and semantic representations  produced by a linguistic analysis  system. The target features are extracted  from links to surface syntax trees. Our  evidence consists of four examples from  the German sentence realization system  code-named  Amalgam:  case  assignment, assignment of verb position  features, extraposition, and syntactic  aggregation  
©£6T£69¨ìºxºxµ½ºAÇÇ¯±¯±¢0¢c©c"Tªª6£©µA§È¢cââ6§Ð"²D²2¶¨6®6ºÐ¡¢I¢"±¨â"â·6Åºwª£¡¿~Tc£¡¢c«~£v£¡âÍcÁ§®×´Ç¨§È6¨AíTª©§6¢c¨ºx6µ¶¨ÃÐÃ´£¡£6ºxâª£¡£¡µï¢vªIAªÅ#{6¶Àºx¨¨b£6A¨°c«â"À©µ@A§ªªTÄWA¢Çª°"ºx§¨¢TA¯±å±ªÁA¢T³À¨â6°"6£6º66ÃÐ«6cª£7¨¥©°"Éxµâ"ºx²ê¨§£Å£6¨µ½ª«§ºAÑm¨´£¢"9Å6Å¤Á¨©£6À§Äc«ª¨ª©°cÊ¨¢c£6ã"Aª§ÈA³©£¡U©6¢c§®AºA"¨¢"A¶¨©¹¡©ºA£6ì«£¸¹°cÅ¦®ÍTÊ¨£6§¨c¡£6¢"£6¢"c¹Ð6¡º§¦¢¦Ç£¡³&"¢T§¯±ç¨§¨µ¨¨§¨É¨6°c£¡£¡â¨Åz¨x¥v¥ß»A´¨rA¨x6µ½¨¢²ëâ¢¹£6¨¼"ª°c«¨¸6¤­ºx%¥Ê£66"¡£6âª£¤¡ª¥"¨¥c¨£6©§¨¡§£¡º³p§¤£6³°cÃÐ©«®¢°"¶ª¹x£6cA°c°c¤­¹¤ÅÐºx¥£6¼"©6¢c¨£6¨§¨§£©£6µ½Q¨´b£6¡x¨c¢T¡ð§¨"A£66´cÉmUÅ¦x¼"FA£¡Éµ½xºx¨§¨T®Ä9bI¨©¼"ª±§ºxÇ§È¤t¨¢í¨§6³©¢v¿Ö©µr£¡¶¨ª¢Ê¥¡ºxâª6¢FÅT¿Ö¢c«~ºªºx«~6£c°""«d½â²Q6ª°"ÙÇ£¹¨£¤µ¶Ù¿Ö¢µA£rcª£6£6â6§®¨A²Q§Tâ¥¦ÄTÄTªª¨"¤t³¯±¡ºxAÃQ¥"ºxª¢"Aâ¿~ªA¥c©ãb«¢"¢"ÄT¨¨ª×£¡ÄT¢T¨âÉ#²ç¥c6«¡%¤tÐ¢"¢T§¡ÍcÐªIvÃÐ§¨c¨¥"¢c¶r©°c§%¢Iª¥¦¢c6¨¶ºAºA§¢ª£6¥¢§èÂ©£¡°"¡#¢c¥c6Ém©w¿~¶x¨A£6c¢"¡¢£èd£A£§c6¥½ÄT££¡¿ÖA£67©ÅQºx£¡A£6Åc9£6æ£6£¢c±î6¨¨6³©£v«£¡"©cªºÐ¨£6º6bª§­¹6ªIµ6c"ºÐ«r°"É¶¨6¢cT¨A¶§©ªÅm6"¥cÅ0«~¶¸írºAµr¥í¶6§­ª§µµÇîºA£¡ºÉvQ¶¨ªAA¢"·ª»â¢Å7AÁ¨9ÍWÆ°"ºªªµ½¢c©¿Öâ¨Ø¡¢c¢c°cªä©Ç¥cÀ¥c¥7ÓQQ¥¹ºx£±âØ¡ºx6é£6â6ª£¡¢c°c°cÃ¤Øªz«~¯±½¢c£6Ô"£6¡â¨§ªAµ§¨§¨¹Ã¡""§¨ªºx§Èµ¨¨â²¡¡©ÕÃÐ§Û©¥¶¨£¤¤¤¤§¶ãÅÅ ñ ò e#óirg¤irôçõ T£6T«Ö£6¡°c"c§¨§¨"¨§"íÉU¥½Å£Ê¥c£%9¢"³©¢Ê6c´±÷r¨¨6£66ÑmÑI«~ºªz°cÓQª°c°c¹m¨A£6¨Ô§¨6®µ½"£¥c¨¡©ÕéU£6¥é¢"¨ª¢cºxªµ½´±6ª¢cºx« ¨µºxQªÅ¢ ¡ªT¢c6ÄT©ª«r6§¨¡°c¢cx¨³½ ¼"£Æºx®© ¢"£¤¨¯±«Ö¡£½´¢£¡© ²ö¨¢Tx§¨ªµ½ª¨¼¡«¦¥{³£¡©AÑm£¡¡³°"ª£¡¥"Ðºx¨%¨¹£6µx¡A¨£¡¢c£6±ªª7¢c¨¡¢¦ªÑmºx³6¶é¢T¨°c®£6µ¥ö¤¨%¨£¡øc´T±ÑIª§¡®§¨°"rÉ6¢"£6°c¨§¨¨´©x6¥FÅº°c¼"£6ºx«Öºx¨¨¡ªªµºxA¢c£6¢c°Â£Â®¥ º¤¤¤¤  £6£6Äc¡ºx6¨8'£¨°cª¢c6©ªº9£°cªºÐ·¨¢cvWª§§®ª±¥{©¢c¥í«¢¤«¬6ºx@£6§ÍTÁx±£6w#%¬ÑmÉù¢"¢c¥B¯½9ú°"´Ë¥»¼mAC¿~â¨×ªÁ¯±¡6§D¢"Á¨ªI£6«E©Áºx¹úªª@ª¡ªâ"ªmÐµ7A£¡¢Ü¥7Á¨6µ½§c¾Ù¨A%°c6©Ìmµ½¨)F¾çTýz¨ºxª¢c§G£6íþÊ6£«%¨ºxýzªmÅª%©ÿþÊ¯±ºx¢)HÁ¨¢c¢c¨Ø²Ü#Pÿ¡¤¡Ñm6A!Ø³³6£°"VI¥¨Ø¢¤§"¨¢"U#ÃÐ¦©9 6¡´"¶¨~¡£6¨ÿ¡¥¨ºx£¡Å»£¡¨¥Ü§ª£¡¨ø"ª¢Q$¨R³µr¢TA©ºx¥¬6#%#%ªçª°c¥¥7µÅI&'ÄµAS¶¢ºûÍc©~ºxTâÿ§DÐ¨£6¢(E£6ü&¡§¨"6xc£6)1FÐ¼»7«~©Årª02¥B°cTºxA%$ÑI«TÄTºx§®6ª)T§°"ª£6°"ª©34µ7§cµ¨¹5£±6wÄW7ÑmT£6Ð6!¢"¥"T¨°"¶§¨¢"ªËª4x¨¢"¨£¢c¼m¡Â¥)¤¤¤ WYXB` acbVd¨egf b%d¨h TAcª¿Â¿ÈúÁ6§"§ÃÃª¢c© ºx§§³©ª¨£6¢c£%¢"c6¢v´6´¼®°c£6°T¢c¢"ª"µ¡´"¢"¡³£6´Å%ª9µº¤¿~ªªùIc6©QÃ£6¢c6¢cªA¥¢I±x¨ªºx¼£Ém¨£ÄTT6¡¢cAª6§ªª¡¢c¢c§¨³©¢"º¨b¡¥¦´Ê¢"³§¨£6¨§´¢Ê©¥cª¢I¢f´µÄT¡¹«~£6ª°c°T6A§¨®£66£¡¹¨§c¹¨¨¥c¨¡ª¥Fªmª¥¦¢A¢"¥c°"Å¤´¨¢A§%¿ÈÅºxÌ¯±bºx¨¢c¿BÃUA²&¥"¨¨Å#¥"ÃQ£¡ºxª¿Èª¨ ³QÇºÐ©6º°cÃ6"°c6Fµ£¡¢cµ¨ £6£6¨©µ½c"¨£6¢I¶ ¢I°£££¤ ¾Å WYXiW aGpqhFr5r5f rPsutvxwf r£eydqs1r  V§4§¦@ 8 §Dy§G§¦042TR3  SAQH¢¤A502§4§2¥q)G£@0£ «~ª¡µ76F³6¢ª"ª¦µ¨6É W6x{¼"© FªU£¡ª³¢"A©«§¨§Dªªª¡7Ðºx´°c£¡¥B¥v£¡ªA%¨¢T¨º)Tµ½¥v¥ðµ½§ §Uµ¹°c¹x¢cª6¼66®AÄWT£¡¢"¢A°c´¢£6¥"£6§ºx¨¨ªmµÝª££6µ½º¨¢¦ª°T¶{T¢T¢cµ¢¦¨¡A¨³¶r©º¢T£¡£6Ax§Q¨±º¼¶ µ£ §Q§îUª©£6£¡"«~x£¡£6ª¼¨"¥c£±¢öÊF6£6¢c6"¢cA¨¨c0¡§µ³©9T¢"¢A£6§¨´¤ ³©£6ªTTp¥qÁ#%)¨6«#§¨©Å¥E§¨§¨ª"¨£¡wCª£6c¢f®6ÊvA¢"c«~AAªAP¥Fªr¢T¨V«µ½°"ºA£6qQ°c§4´T4ÆA"A¯§G¡¢c0xµ½£6ºx¨£¡ª£6ÿ¨¨¥£AªQ¢cÁ¨ª"¢cwØºx¢cdªm£6Ä9)Fºx¢cgm¢Fª¢"¢c¥T£¡¥iº¡¾2«Å´§D¥FA³®§®GÉA8»³³¢cµ6¢"¨ýq¢"¨¡£6§ªªhc°c¢c¨cÉª£6§2AA¶fºA¨®¨9³QV£6A%§§º¬xºx¥b¥Ê¥¦c§ev6i ¼m§¨¨ªm¨£6ª³Q6£¡ÉÉ¥"ª¥c«©6¨#%bG¢"¢"¨6´ª¢T©¥B£6£6ªªzª#k§$«Wd6c³©©³©¢I¨£¡¨6ª£j1#x£¡»§§§¨76)H¢c£¡¨ª¨¼¢2A5wªÙ¥"ºR¥"¥qÄ9À§¨£0£¢"ª)T§¢"´3Å´¢¤ª¨A´my¨½ÄWAºx¨¥q66¨¥qºx¶)¨¨±°"§)D¨£¡6bSn£%§l6¼"¡A£¡{Aªª°c¡¨§D#%ªµ{¥¨¢cA%£6¥B£6©¢cµ½c¥¨4§©"ºx¶§¡#Y¶£6ªmÉ³A%²U£¥c)Y·mµ½G3ÑI¢¨(¢cÍT¢r°c¨¢(#cµ½¢c¥(wºx¥"³Q¨¡VÕw¼"6o¡°c³6"§££6£6¢I£¡§¥¨A%¡¨¨)Y¨µµ£6ªª¨)F6A6º|x¢¢¥º¤¤¤¶  V§4§'rRG 8 §2V§D£c§f042¡3  AQ)T§26!£¢D §2¥BA%) DèÂ6¡¢Q¢Ò³ºA%s £¡£6½£¡c«Ö#%6¬°c¥¨&'6¥½º¨§ª£6¢(¢Ë"6¬)H6ªrt£6°""#@¢"´vAg¨"0£ºxªW¡¨°c¦6£66£66)u©q¹§©§¶XA¢T§c¡"³¥"íªºÊºx°cªµµ¢"«~¨Wª¢I§¨Ax£6µ½¼£ £6©¢c¥ª"¢Ò¢6¤  Aºx§A5ýq¨ª02hcÉ¢"0£µ¬A%¡´T6u¢2ÑI¨¢(§U°"x(#@¨¼Ú#x«~6Aw6£6ªÑI¨µvCªy°c¢cv¤%¨zÊ§4¡6£6§£9 ¡¿¨£ª¨¦ýµ{¢cþÊ¥q3ç)lÄTÿ x§~§@ÿT¨¢"fÉk{´¬$¡3£6¥7V605µºx§D)HAt50TcR§Tb££¢(¦§¡¢ÑmÃÐ¡°"Å)u£66¨©'ªÃÐ¡¶£¡£6§DªF¨Tª£¡02¨¢T«Ö02µ±Åf§4W§2µ§¿~¥BA%ª£¡¶)T6´"x§¤¶ £6c£¡£6°c§º¨µx"¨6¢c6¡Éª"¼¡Òª7¡9¢0£6¨ª£±³ºx¨ª6º¶f£ª¨ºxV®Ê³©©¡ª6¢Ò¢§4A¨T6²Q¢I¨§'¹w£¡¥F£6®©ª£¡ª¢"¢º|uªx«±Ä§¨¡´Ë¹©¼m{£¡6©¨§¢c¨¨£¢£6¨v2ÒA±µr¢é9£6ºx")u"Wªªª¨ºx«§«¬¢I"6¨ª¡©¢c£¡6¢¤£¡ªµ£ÊT¨@¨x66ª¥ç¢T¼m0g6WTÀª¢c¢"7£Uºx§¨"«¥¨³©¹mÊx7}¢ªÑIW¼íÄTÑI¨dªA6ª°c£6"§¨°c¦¨°c¿Ö6«QÊ¨ªäU¡¨¸¡Aª£6U«~¡«~8£6£¡6"£6§®ª¸¨ªA6°"6¶¨ªÊ¢Iª¨µë§2£6¢c£¡«~¢cÄT¢cV¨c¢cª§DÊºxA66¢v£6¡6»´¨c£³¨9¢ºx¹cÑm6v°ç£6ª£¨¨§°cª¨½¡§Å´¡ªº°c¨¨ª£6ª°"¢c6ÑI£½¥ð"»A§¨6¸°"°"¨À6£6ÑI¨£6®¨c§È³Q6¢ö£6°"6¶¨¢Iª"µ½£6Å¨¨£½«¢¦¨b£6Ë6ªÇê£6¶AÑm¢câ¨¨£6ºx°"ªâ"~UAc¢cª£6¡¢c¨Á"¢¢ºxªÂÃÐ¢£¤¤¤¤ÅÅ  V§4§{@" 8 §2V§G§n022¡R3  'AQn%)H  A(@¥702  ¢((% £6T£6Ä¨ÄT«~¨µº¯±§¦%ýq£ª¨¨¹¥"A«~©v4¥q§©ªª"²£¡¨©¢4§)F´·¡¢c¢F£¡¥"±¢I5¨0£¡¥¨¥q®¨Ä9£¡~w7¢T£6¨))1¢cªr¥¢ÿ¡ºA¥ö¥¨¨©í¢I³Q¨¢I)ºxcT®ÄTµf6º£6£¡§¨©6xcº³Q£¡ª¢Tª{¥¨¨I9¢cA¨¡§)1µ´°c¡°c¨ºA¨µ½§9(ªªÐÑm©£6#%³¡¡7#Ù6ºxªãQA£wX°"6ª¶ £¡£¡¡¨¢T§DýqRx´U£§{«~v4£6A¢c76¼mAA%6¨¹§n¢D¡"¢"¨¨c"¡¡ª6T¥q¢"¨ªC¨¡9¥ö¨)´v³%¨6~µê¤£6§®¢I¢cÿ¡¡ªd¨®¥qA6£6¸º®£6£¡ºQ$ÄW)ºÄ¡¨¢"©£6c¨¨Eª°c©£66£¡cª´"A¦ºÐÉ¢T7µ½§ª§¨ÄTrc«r©¢¤¶¢"£¤jd¥&°"£6¢Q§A%«¢c¢"£6£6ª©"%xªA¡uº¨"©A´"³©¼ºx¥¨¹£6Ð«~Ñm§3£6¨§£¡6Å7¨µ{£¡§"°"¢cc¡ªr£¡¨A6¡ÑmU¨¡ªÑm¨°cµÞ¤¥"@®AQ°"¥qW»¥v£¢c°"ºx§®)u´Ð¥"¨£À©¨º¸¶¢Tc¡¨¢¤£¡£6µ½£§¡¡¢Ê£6T»4¨¡¨ºx°c£6¢¤£¡¡³02ª¥{¹¨¨ª¨¥"ª³¥"¨¢cD§7ªµ7§Dµ½¢°"§2ºx°cªm®¢Tq¨A§2¢"¨T§cA9AºÀ§¨¥B£¡ºx§´Ë£¡°T¥ªA%¢"ªª¨6cªµ)¡¤t¡T´!í£¡cÀ«¦µ½ª96£6x¨~6ªm°c¢cº¨£6"¼m|ÿ¡¢¨®µº¹m"¶xºA¢"AãU§¨£¦£6¢"Ff®¨¨É¥q¨6¥"ý7ÀÀ´X¢¥£6¥"Åcª²Q¢"v4ÑmÑm¨º"§£¡¢cª6ªz)1°"°"¢cA©ª"ªÄT³©º~¡4¡¨¨ºxµ¢c¢"¾¥cÿT°ªx@ÂÂ£§Û§Û¥´ ¤¤¤¤¤¤¤¤ÅÅ ÕÀÕÀÕÀÕÀÕÀ©§§§§§966666À{ÄT§¨ùÙÇvÌvvÁ¬½¿~¿Ö¿~¿Ö¿ÖÁ«~«~6¡¾!¢°c9µºx£¡AU¨£6¨ºTª°cÐ®°c¡¢F§x£¡§ºx§~A¤­¤¤£6Ã6¨£6¨ÄT¨§¨¹¹°c¡z¡£Ð£6Ãª¤Ã¨ª¢cºx¢rª¢c¢Iª´I£¡«Ãx¼m£ÐÓQÃ ÔÕXqQùØâ@ùÑmgÇ°TâvvgvíÇí°"µ7¨¿Ö¿È¿È¿Â¿ÈâÄ9¡úÇÇmÁ£6¶ûIûIâ¶¶¨Øû7ª¶¶ü½Øürür¢T¿­ürürÃü½ÃÃ ÃÃÃ ÄT§¨vÁ¬§§°T¡£¡A£¡¨ £6"¬¥c¡£¡AÄT°"£6¨ª¢Ùª«ÓQÔÕ  Niagra − Niagara Examples Volkswangen − Volkswagen  How much 1966 bug rent Volkswagen  Money Person Country  Volkswagen bug rent  rented − rent invented − inventor built − build  Question System Modules  M1 Keyword pre−processing (split/bind/spell)  M2 Construction of question representation  Retrieval of documents and passages M6  Passage post−filtering M7  M3 Derivation of expected answer type Identification of candidate answers M8  M4 Keyword selection 
In this paper we explore the power of surface text patterns for open-domain question answering systems. In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically. A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista. Patterns are then automatically extracted from the returned documents and standardized. We calculate the precision of each pattern, and the average precision for each question type. These patterns are then applied to find answers to new questions. Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web. 
In Optimality-Theoretic Syntax, optimization with unrestricted expressive power on the side of the OT constraints is undecidable. This paper provides a proof for the decidability of optimization based on constraints expressed with reference to local subtrees (which is in the spirit of OT theory). The proof builds on Kaplan and Wedekind’s (2000) construction showing that LFG generation produces contextfree languages.  
This paper ties up some loose ends in ﬁnite-state Optimality Theory. First, it discusses how to perform comprehension under Optimality Theory grammars consisting of ﬁnite-state constraints. Comprehension has not been much studied in OT; we show that unlike production, it does not always yield a regular set, making ﬁnite-state methods inapplicable. However, after giving a suitably ﬂexible presentation of OT, we show carefully how to treat comprehension under recent variants of OT in which grammars can be compiled into ﬁnite-state transducers. We then unify these variants, showing that compilation is possible if all components of the grammar are regular relations, including the harmony ordering on scored candidates. A side beneﬁt of our construction is a far simpler implementation of directional OT (Eisner, 2000). 
This paper presents a new formalization of a uniﬁcation- or join-preserving encoding of partially ordered sets that more essentially captures what it means for an encoding to preserve joins, generalizing the standard deﬁnition in AI research. It then shows that every statically typable ontology in the logic of typed feature structures can be encoded in a data structure of ﬁxed size without the need for resizing or additional union-ﬁnd operations. This is important for any grammar implementation or development system based on typed feature structures, as it signiﬁcantly reduces the overhead of memory management and reference-pointer-chasing during uniﬁcation. 
The paper presents an approach to ellipsis resolution in a framework of scope underspeciﬁcation (Underspeciﬁed Discourse Representation Theory). It is argued that the approach improves on previous proposals to integrate ellipsis resolution and scope underspeciﬁcation (Crouch, 1995; Egg et al., 2001) in that application processes like anaphora resolution do not require full disambiguation but can work directly on the underspeciﬁed representation. Furthermore it is shown that the approach presented can cope with the examples discussed by Dalrymple et al. (1991) as well as a problem noted recently by Erk and Koller (2001). 
Previous approaches to pronominalization have largely been theoretical rather than applied in nature. Frequently, such methods are based on Centering Theory, which deals with the resolution of anaphoric pronouns. But it is not clear that complex theoretical mechanisms, while having satisfying explanatory power, are necessary for the actual generation of pronouns. We ﬁrst illustrate examples of pronouns from various domains, describe a simple method for generating pronouns in an implemented multi-page generation system, and present an evaluation of its performance. 
The incremental algorithm introduced in (Dale and Reiter, 1995) for producing distinguishing descriptions does not always generate a minimal description. In this paper, I show that when generalised to sets of individuals and disjunctive properties, this approach might generate unnecessarily long and ambiguous and/or epistemically redundant descriptions. I then present an alternative, constraint-based algorithm and show that it builds on existing related algorithms in that (i) it produces minimal descriptions for sets of individuals using positive, negative and disjunctive properties, (ii) it straightforwardly generalises to n-ary relations and (iii) it is integrated with surface realisation. 
We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC6 and MUC-7 coreference resolution data sets — F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge. 
We consider the problem of parsing non-recursive context-free grammars, i.e., context-free grammars that generate ﬁnite languages. In natural language processing, this problem arises in several areas of application, including natural language generation, speech recognition and machine translation. We present two tabular algorithms for parsing of non-recursive context-free grammars, and show that they perform well in practical settings, despite the fact that this problem is PSPACEcomplete. 
It is necessary to have a (large) annotated corpus to build a statistical parser. Acquisition of such a corpus is costly and time-consuming. This paper presents a method to reduce this demand using active learning, which selects what samples to annotate, instead of annotating blindly the whole training corpus. Sample selection for annotation is based upon “representativeness” and “usefulness”. A model-based distance is proposed to measure the difference of two sentences and their most likely parse trees. Based on this distance, the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores. Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method. 
We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published unsupervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on nontrivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task. 
This paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information. The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it. This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus. Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity. 
This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction. The proposed method builds an explicit error model for word pronunciations. By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction. 
Text normalization is an important aspect of successful information retrieval from medical documents such as clinical notes, radiology reports and discharge summaries. In the medical domain, a significant part of the general problem of text normalization is abbreviation and acronym disambiguation. Numerous abbreviations are used routinely throughout such texts and knowing their meaning is critical to data retrieval from the document. In this paper I will demonstrate a method of automatically generating training data for Maximum Entropy (ME) modeling of abbreviations and acronyms and will show that using ME modeling is a promising technique for abbreviation and acronym normalization. I report on the results of an experiment involving training a number of ME models used to normalize abbreviations and acronyms on a sample of 10,000 rheumatology notes with ~89% accuracy. 
Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 168-175. 
Reducing language model (LM) size is a critical issue when applying a LM to realistic applications which have memory constraints. In this paper, three measures are studied for the purpose of LM pruning. They are probability, rank, and entropy. We evaluated the performance of the three pruning criteria in a real application of Chinese text input in terms of character error rate (CER). We first present an empirical comparison, showing that rank performs the best in most cases. We also show that the high-performance of rank lies in its strong correlation with error rate. We then present a novel method of combining two criteria in model pruning. Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately, at the same CER. 
The n-gram model is a stochastic model, which predicts the next word (predicted word) given the previous words (conditional words) in a word sequence. The cluster n-gram model is a variant of the n-gram model in which similar words are classified in the same cluster. It has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words. This is the basis of the asymmetric cluster model (ACM) discussed in our study. In this paper, we first present a formal definition of the ACM. We then describe in detail the methodology of constructing the ACM. The effectiveness of the ACM is evaluated on a realistic application, namely Japanese Kana-Kanji conversion. Experimental results show substantial improvements of the ACM in comparison with classical cluster models and word n-gram models at the same model size. Our analysis shows that the high-performance of the ACM lies in the asymmetry of the model. 
We study the impact of richer syntactic dependencies on the performance of the structured language model (SLM) along three dimensions: parsing accuracy (LP/LR), perplexity (PPL) and worderror-rate (WER, N-best re-scoring). We show that our models achieve an improvement in LP/LR, PPL and/or WER over the reported baseline results using the SLM on the UPenn Treebank and Wall Street Journal (WSJ) corpora, respectively. Analysis of parsing performance shows correlation between the quality of the parser (as measured by precision/recall) and the language model performance (PPL and WER). A remarkable fact is that the enriched SLM outperforms the baseline 3-gram model in terms of WER by 10% when used in isolation as a second pass (N-best re-scoring) language model. 
We present a constancy rate principle governing language generation. We show that this principle implies that local measures of entropy (ignoring context) should increase with the sentence number. We demonstrate that this is indeed the case by measuring entropy in three diﬀerent ways. We also show that this eﬀect has both lexical (which words are used) and non-lexical (how the words are used) causes. 
The paper describes the application of kMeans, a standard clustering technique, to the task of inducing semantic classes for German verbs. Using probability distributions over verb subcategorisation frames, we obtained an intuitively plausible clustering of 57 verbs into 14 classes. The automatic clustering was evaluated against independently motivated, handconstructed semantic verb classes. A series of post-hoc cluster analyses explored the inﬂuence of speciﬁc frames and frame groups on the coherence of the verb classes, and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components. 
Context is used in many NLP systems as an indicator of a term’s syntactic and semantic function. The accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term. However, the quantity variable is no longer ﬁxed by limited corpus resources. Given ﬁxed training time and computational resources, it makes sense for systems to invest time in extracting high quality contextual information from a ﬁxed corpus. However, with an effectively limitless quantity of text available, extraction rate and representation size need to be considered. We use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity, time and size on a corpus of 300 million words. 
In many types of technical texts, meaning is embedded in noun compounds. A language understanding program needs to be able to interpret these in order to ascertain sentence meaning. We explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. In this paper we present the results of an analysis of this method on twoword noun compounds from the biomedical domain, obtaining classiﬁcation accuracy of approximately 90%. Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question? We ﬁnd that the topmost levels of the hierarchy yield an accurate classiﬁcation, thus providing an economic way of assigning relations to noun compounds. 
This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efﬁciently applied to exponential sized representations of parse trees, such as the “all subtrees” (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing signiﬁcant improvements on two tasks: parsing Wall Street Journal text, and namedentity extraction from web data. 
We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model. We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data. The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models. Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets. On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for Brown corpus data achieves 76% F-score. 
We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is signiﬁcantly improved using this approach.  
This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001). The model has been extended to incorporate phrasal translations as presented here. In contrast to a conventional word-to-word statistical model, a decoder for the syntaxbased model builds up an English parse tree given a sentence in a foreign language. As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary. We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4. We also discuss issues concerning the relation between this decoder and a language model. 
Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to ﬁnish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1  the evaluation bottleneck. Developers would beneﬁt from an inexpensive automatic evaluation that is quick, language-independent, and correlates highly with human evaluation. We propose such an evaluation method in this paper. 1.2 Viewpoint How does one measure translation performance? The closer a machine translation is to a professional human translation, the better it is. This is the central idea behind our proposal. To judge the quality of a machine translation, one measures its closeness to one or more reference human translations according to a numerical metric. Thus, our MT evaluation system requires two ingredients:  
Categorial grammar has traditionally used the λ-calculus to represent meaning. We present an alternative, dependency-based perspective on linguistic meaning and situate it in the computational setting. This perspective is formalized in terms of hybrid logic and has a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomena to be represented in a single meaning formalism. Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations compositionally.  and semantics, and then discuss semantic representations that use indexes to identify subparts of logical forms. Ü3 introduces HLDS and evaluates it with respect to the criteria of other computational semantics frameworks. Ü4 shows how we can build HLDS terms using CCG with uniﬁcation and Ü5 shows how intonation and information structure can be incorporated into the approach. 2 Indexed semantic representations Traditionally, categorial grammar has captured meaning using a (simply typed) λ-calculus, building semantic structure in parallel to the categorial inference (Morrill, 1994; Moortgat, 1997; Steedman, 2000b). For example, a (simpliﬁed) CCG lexical entry for a verb such as wrote is given in (1).  
This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures. The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies. A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank. The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies. 
This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the ﬁgures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we ﬁnd a significant improvement from modeling wordword dependencies.  and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations. CCG grammars are characterized by much larger category sets than standard Penn Treebank grammars, distinguishing for example between many classes of verbs with different subcategorization frames. As a result, the categorial lexicon extracted for this purpose from the training corpus has 1207 categories, compared with the 48 POS-tags of the Penn Treebank. On the other hand, grammar rules in CCG are limited to a small number of simple unary and binary combinatory schemata such as function application and composition. This results in a smaller and less overgenerating grammar than standard PCFGs (ca. 3,000 rules when instantiated with the above categories in sections 02-21, instead of 12,400 in the original Treebank representation (Collins, 1999)).  
 This paper proposes a new method for  word translation disambiguation using  a machine learning technique called  ‘Bilingual Bootstrapping’. Bilingual ˈ Bootstrapping makes use of in ˈ learning a small number of classified  data and a large number of unclassified  data in the source and the target  languages in translation. It constructs  classifiers in the two languages in  parallel and repeatedly boosts the  performances of the classifiers by  further classifying data in each of the  two languages and by exchanging  between the two languages  information regarding the classified  data. Experimental results indicate that  word translation disambiguation based  on Bilingual Bootstrapping  consistently and significantly  outperforms the existing methods  based  on  ‘Monolingual  Bootstrapping’.  
In this paper, we investigate the practical applicability of Co-Training for the task of building a classiﬁer for reference resolution. We are concerned with the question if Co-Training can signiﬁcantly reduce the amount of manual labeling work and still produce a classiﬁer with an acceptable performance.  apply Co-Training to the problem of reference resolution in German texts from the tourism domain in order to provide answers to the following questions: Does Co-Training work at all for this task (when compared to conventional C4.5 decision tree learning)? How much labeled training data is required for achieving a reasonable performance?  
This paper reﬁnes the analysis of cotraining, deﬁnes and evaluates a new co-training algorithm that has theoretical justiﬁcation, gives a theoretical justiﬁcation for the Yarowsky algorithm, and shows that co-training and the Yarowsky algorithm are based on diﬀerent independence assumptions. 
We present an unsupervised approach to recognizing discourse relations of CONTRAST, EXPLANATION-EVIDENCE, CONDITION and ELABORATION that hold between arbitrary spans of texts. We show that discourse relation classiﬁers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. 
Mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences, the task at hand, and the physical and social environment. We describe a multimodal application architecture which combines ﬁnite-state multimodal language processing, a speech-act based multimodal dialogue manager, dynamic multimodal output generation, and user-tailored text planning to enable rapid prototyping of multimodal interfaces with ﬂexible input and adaptive output. Our testbed application MATCH (Multimodal Access To City Help) provides a mobile multimodal speech-pen interface to restaurant and subway information for New York City.  provide input in whichever mode or combination of modes is most appropriate, and system output should be dynamically tailored so that it is maximally effective given the situation and the user’s preferences. We present our testbed multimodal application MATCH (Multimodal Access To City Help) and the general purpose multimodal architecture underlying it, that: is designed for highly mobile applications; enables ﬂexible multimodal input; and provides ﬂexible user-tailored multimodal output.  
Spoken dialogue systems promise efﬁcient and natural access to information services from any phone. Recently, spoken dialogue systems for widely used applications such as email, travel information, and customer care have moved from research labs into commercial use. These applications can receive millions of calls a month. This huge amount of spoken dialogue data has led to a need for fully automatic methods for selecting a subset of caller dialogues that are most likely to be useful for further system improvement, to be stored, transcribed and further analyzed. This paper reports results on automatically training a Problematic Dialogue Identiﬁer to classify problematic human-computer dialogues using a corpus of 1242 DARPA Communicator dialogues in the travel planning domain. We show that using fully automatic features we can identify classes of problematic dialogues with accuracies from 67% to 89%. 
Named entity phrases are some of the most difﬁcult phrases to translate because new phrases can appear from nowhere, and because many are domain speciﬁc, not to be found in bilingual dictionaries. We present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources. We report on the application and evaluation of this algorithm in translating Arabic named entities to English. We also compare our results with the results obtained from human translations and a commercial system for the same task. 
We describe an approach to improve the bilingual cooccurrence dictionary that is used for word alignment, and evaluate the improved dictionary using a version of the Competitive Linking algorithm. We demonstrate a problem faced by the Competitive Linking algorithm and present an approach to ameliorate it. In particular, we rebuild the bilingual dictionary by clustering similar words in a language and assigning them a higher cooccurrence score with a given word in the other language than each single word would have otherwise. Experimental results show a signiﬁcant improvement in precision and recall for word alignment when the improved dicitonary is used. 
This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., “subtle nuances”) and a negative semantic orientation when it has bad associations (e.g., “very cavalier”). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “excellent” minus the mutual information between the given phrase and the word “poor”. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews. 
Answer Validation is an emerging topic in Question Answering, where open domain systems are often required to rank huge amounts of candidate answers. We present a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploiting the redundancy of Web information. Experiments carried out on the TREC-2001 judged-answer collection show that the approach achieves a high level of performance (i.e. 81% success rate). The simplicity and the efﬁciency of this approach make it suitable to be used as a module in Question Answering systems. 
We describe a case study in which a memory-based learning algorithm is trained to simultaneously chunk sentences and assign grammatical function tags to these chunks. We compare the algorithm’s performance on this parsing task with varying training set sizes (yielding learning curves) and different input representations. In particular we compare input consisting of words only, a variant that includes word form information for lowfrequency words, gold-standard POS only, and combinations of these. The wordbased shallow parser displays an apparently log-linear increase in performance, and surpasses the ﬂatter POS-based curve at about 50,000 sentences of training data. The low-frequency variant performs even better, and the combinations is best. Comparative experiments with a real POS tagger produce lower results. We argue that we might not need an explicit intermediate POS-tagging step for parsing when a sufﬁcient amount of training material is available and word form information is used for low-frequency words. 
We present an architecture for the integration of shallow and deep NLP components which is aimed at ﬂexible combination of different language technologies for a range of practical current and future applications. In particular, we describe the integration of a high-level HPSG parsing system with different high-performance shallow components, ranging from named entity recognition to chunk parsing and shallow clause recognition. The NLP components enrich a representation of natural language text with layers of new XML meta-information using a single shared data structure, called the text chart. We describe details of the integration methods, and show how information extraction and language checking applications for realworld German text beneﬁt from a deep grammatical analysis. 
We present a document compression system that uses a hierarchical noisy-channel model of text production. Our compression system ﬁrst automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input. The system then uses a statistical hierarchical model of text production in order to drop non-important syntactic and discourse constituents so as to generate coherent, grammatical document compressions of arbitrary length. The system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text. Our results support the claim that discourse knowledge plays an important role in document summarization. 
NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order. NeATS is among the best performers in the large scale summarization evaluat ion DUC 2001. 
The paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human biases in summarization. In particular, we explore the use of probabilistic decision tree within the clustering framework to account for the variation as well as regularity in human created summaries. The corpus of human created extracts is created from a newspaper corpus and used as a test set. We build probabilistic decision trees of different ﬂavors and integrate each of them with the clustering framework. Experiments with the corpus demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either of the two is considered alone. 
This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities. Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature. In this way, the NER problem can be resolved effectively. Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other machine-learning system. Moreover, the performance is even consistently better than those based on handcrafted rules. 
This paper describes how a machinelearning named entity recognizer (NER) on upper case text can be improved by using a mixed case NER and some unlabeled text. The mixed case NER can be used to tag some unlabeled mixed case text, which are then used as additional training material for the upper case NER. We show that this approach reduces the performance gap between the mixed case NER and the upper case NER substantially, by 39% for MUC-6 and 22% for MUC-7 named entity test data. Our method is thus useful in improving the accuracy of NERs on upper case text, such as transcribed text from automatic speech recognizers where case information is missing. 
This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data. The ﬁrst approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, signiﬁcant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efﬁcient to train, at some cost in computation on test examples. 
This paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost. This method uses a high capacity model to revise the output of a small cost model. We apply this method to English partof-speech tagging and Japanese morphological analysis, and show that the method performs well. 
We explore how active learning with Support Vector Machines works well for a non-trivial task in natural language processing. We use Japanese word segmentation as a test case. In particular, we discuss how the size of a pool affects the learning curve. It is found that in the early stage of training with a larger pool, more labeled examples are required to achieve a given level of accuracy than those with a smaller pool. In addition, we propose a novel technique to use a large number of unlabeled examples effectively by adding them gradually to a pool. The experimental results show that our technique requires less labeled examples than those with the technique in previous research. To achieve 97.0 % accuracy, the proposed technique needs 59.3 % of labeled examples that are required when using the previous technique and only 17.4 % of labeled examples with random sampling. 
This paper discusses the supervised learning of morphology using stochastic transducers, trained using the ExpectationMaximization (EM) algorithm. Two approaches are presented: ﬁrst, using the transducers directly to model the process, and secondly using them to deﬁne a similarity measure, related to the Fisher kernel method (Jaakkola and Haussler, 1998), and then using a Memory-Based Learning (MBL) technique. These are evaluated and compared on data sets from English, German, Slovene and Arabic. 
There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together. 1. Introduction It is well known that word -sense is defined by a word ’s co -occurrence context. The context vectors of a word are defined as the probabilistic distributions of its left and right co-occurrence contexts. Conventionally , the similarity between two context vectors is measured based on their cosine distance [Alshawi and Cater, 1994; Grishman and Sterling, 1994; Pereira et al., 1993; Ruge, 1992; Salton, 1989]. However, the conventional measurement * Institute of Information Science, Academia Sinica E-mail: kchen@iis.sinica.edu.tw; swimming@hp.iis.sinica.edu.tw  K. Chen, J. You  suffers from the following drawbacks. First of all, the information in the context vectors is vague. All co-occurrence words are collected without distinguishing whether they are syntactically or semantically related. Second, the coordinates are not pair-wise independent (i.e., the axes are not orthogonal) , and it is hard to apply singular value decomposition to find the orthogonal vectors [Schutze, 1992]. In this paper, we propose to use only syntactic related co-occurrences as context vectors [Dekang Lin, 1998] and adopt information theoretic models to solve the above problems. In our study, the context vectors of a word are defined as the probabilistic distributions of its thematic roles and left/right co -occurrence semantic classes. The context features are derived from a treebank. All context features are weighted according to their TF × IDF values (the product of the term frequency and inverse document frequency) [Salton, 1989]. For the context features, the Cilin semantic classes (a Chinese thesaurus) are adopted. The Cilin semantic classes are divided into 4 different levels of granularity. In order to cope with the data sparseness problem, the weighted average of the similarity values at four different levels will be the similarity measure of two words. The weight for each level is equal to the information-content of that level[Shannon, 1948; Manning and Schutze 1999]. A agglomerative clustering algorithm is applied to group similar words according to the above defined similarity measure. Obviously, words with similar behavior in the corpus will be grouped together. We have compared the clustering result s to the Cilin classifications. It turns out that words in the same synonym class and with the same syntactic categories have higher similarity values than the words with different syntactic categories. 2. Data Resources  Ideally, to derive context vectors, a large corpus with semantic tags is required. Furthermore, to extract co-occurrence words along with their exact syntactic and semantic relations, the corpus structure has to be annotated. However, such an idea l corpus does not exist. Therefore, in this paper we will adopt the resources that are available and try to derive a useful but imperfect Chinese tree bank. Since the similarity measure based on the vector space model is a rough estimation, minor errors made at the stage of context vector extraction are acceptable.  2.1 Sinica Corpus  The Sinica corpus contains 12,532 documents and nearly 5 million words. Each sentence in  the corpus was parsed by a rule parser [Chen, 1996]. The parsed trees were tagged with the  structure brackets, syntactic categories and thematic roles of each constituent [Huang et al.,  2000] as exemplified below, (Sinica corpus: http://www.sinica.edu.tw/ftms -bin/kiwi.sh):  Original sentence: ‘small’ ‘dog’  ‘dance’  Parsed tree : S(Agent:NP:(Property:Adj:‘ small’| Head: N: ‘ dog’ )|Head:V: ‘  dance’)  A Study of Word Similarity by Context Vector Models  Although these labels may not be exactly correct, we believe that, even with these minor errors, the majority of word -to-word relations extracted from the trees are correct. However, the semantic label is not provided for each word in the parsed trees. In this paper, we will use Cilin classifications for semantic labeling.  2.2 Cilin- a Chinese Thesaurus Cilin provides the Mandarin synonym sets in a hierarchical structure [Mei et. al., 1984]. It contains 51,708 Chinese words, and 3918 classes. There are five levels in the Cilin semantic hierarchy, denoted in the format L1 -L2-L3-L4 -L5. For example, the Cilin class of the word ‘we’ is “A -a-02-2-01”. In level 1, “A”, denotes the semantic class of human; in level 2, “a”, indicates a group of general terms; level 3, “02”, means pronouns in the first person, and in level 4, “2” represents the plural property. In level 5, “01” represents the order rank in the level 4 group. This means that “01” in level 5 is the first prototypical concept representation of “A-a-02-2”. In the rest of this paper, only the first four levels will be used. The fifth level is for sense disambiguation only (section 2.3).  2.3 Sense Disambiguation  A polysemous word has more than one Cilin semantic class. In order to tag appropriate Cilin  classes, we have designed a simple sense tagging method as follows [Wilks, 1999]. The sense  tagging algorithm is based on the facts that the syntactic categories of each word in the tree  bank are assigned uniquely, and that each Cilin class has its own major syntactic category. If a  word has multiple Cilin classes, we select the sense class whose major syntactic category is  the same as the tagged category of this word. For example ,  “Jie -Hwa” has two  meanings. One is for “project” as a noun and the other is “attempt”, therefore, if  “Jie -Hwa” was tagged with a noun category, we will assign the Cilin class whose major  category is “project”. Sense ambiguity can be distinguished by measure of syntactic properties  for most words. However, there are still cases in which the syntactic category constraints  cannot resolve the sense ambiguities. Then, we simply choose the prototypical sense class, i.e,  the word that has the highest rank in this sense class with respect to all its sense classes in  Cilin.  2.4 The Extraction of Co-occurrence Data  The extracted syntactically related pairs have either a head -modifier relation or head-argument  relation. For instance, two syntactically related pairs extracted from the example in section 2.1  are:  (<Thematic role > <Cilin> <word1>), (<Thematic role> <Cilin> <word2>)  (agent  B-i-07-2  ‘dog’), (Head(S)  H-h-04-2  ‘dance’ )  K. Chen, J. You  (property E-a-03-3 ‘small’), (Head(NP)  B-i-07-2 ‘dog’)  The context data of the word1 “dog” consists of its thematic role “agent” and the  Cilin class “B-i-07-2” ; the word2  ‘dance’ consists the thematic role “Head(S)” and the  Cilin class “H-h -04-2” and so on. The word  “small” and  “dance” are not  syntactically related even though they co-occur. Therefore , they will not be extracted.  3. Context Vector Model  There are three context vectors of a word: role vector, left context vector and right context vector. The role vector is a fixed 48-dimension vector, and each dimension value is equal to the probabilistic distribution of its thematic roles. The left/right context vectors are closer to the probabilistic distributions of its left/right co-occurrence words and their semantic classes. The role vector characterizes a word based more on syntax and less on semantics, but the left/right context vectors are just the opposite. The cosine distance between their context vectors is a measure of the similarity of the two words. We will illustrate the derivations of context vectors and their similarity rating with a simplified example using ( “cat”, “dog”). The role vector of “dog” is {127, 207, 169… , 0}48, which represents the values of “agent”, “goal”, “theme”… and “topic” respectively, generated by Equation (1). The role vector of “cat” is {28, 73, 56… , 0}48, which is also acquired by Equation (1).  Role vector of word W = { V1, V2,… ,V48 }48  Vi = Frequency (Ri) × log (1/Pi)  (1)  Ri: We label thematic roles “agent”, “goal”… and “topic” from R1 to R48 listed in Table2 in the  Appendix.  Frequency (Ri): The frequency of Ri played by word W in the corpus.  Pi: = Total frequency of Ri in the corpus / Total frequency of all roles in the corpus.  log(1/Pi): The information–context of Ri [Shannon, 1948; Manning and Schutze, 1999]  The derivation of left/right context vectors is a bit more complicated. The syntactically related co-occurrence word pairs are derived first as illustrated in section 2.4. We will illustrate the derivation of the left context vector only. The right context vector can be derived simila rly. The left co-occurrence word vector of word W is generated from frequency(wordi), where wordi precedes and is syntactically related to W in the corpus. Due to the data sparseness problem, the feature dimensions of context vectors are generalized int o Cilin classes instead of co-occurrence words. The generalization process reduces the effect of data sparseness. On the other hand, it also reduces the precision of characterization since each  A Study of Word Similarity by Context Vector Models  word has different information content and two words that have the same co-occurrence semantic classes may not share the same co-occurrence words. In order to resolve the above dilemma, when we compare the similarity between word X and wordY, 4 levels of left context vectors and right context vectors for wordX and wordY are created1. The weighting of each feature dimension is adjusted using the TF*IDF value if word X and wordY have shared context words. Equation (2) illustrates the creation of the 4th level left context vector of word X. The other context vectors for wordX and wordY are created by a similar way. Left context vector of word X = {f1, f2,… ,f3918 }L4  Where fi = Sum of { TF(wordj) × IDF(wordj) if wordX has the same neighbor wordj with wordY  TF(word j )  if wordX does not have the same neighbor wordj with  wordY  } ; for every left co-occurrence wordj with the ith Cilin semantic class.  (2)  TF(wordj): the frequency of pair ( word j, Cilin(wordj) ) in wordx’s co-occurrence context. IDF(wordj): -log(the number of the documents that contains the wordj/total document number of the corpus)  In Equation (2), we adjust the term weight using TF × IDF, which is commonly used in  the field of information retrieval [Salton, 1989] to adjust the discrimination power of each  feature dimension. We will examine the difference in the adjustment of weights using TF × IDF and TF in section 4.2. We will next give a simplified example. Assume that the  word “dog” has only three left syntactically related words: ( “small” Ea033 ) with  frequency 30, (  “cute” Ed401 ) with frequency 5 and ( “raise” Ib011 ) with  frequency 10; and assume that the word “cat” has only two left syntactically related words:  ( “black” Ec043 ) and ( “raise” Ib011 ). Assume that we are measuring the similarity  between “dog” and “cat”. Then, we can compute the left context data of “dog” as {TF(Aa011),… ,TF(Ea033),… ,TF(Ed041),… ,TF(Ib011) × IDF( ‘raise’)2, … , TF(La064) }L43  
Word sense disambiguation (WSD) plays an important role in many areas of natural language processing, such as machine translation, information retrieval, sentence analysis, and speech recognition. Research on WSD has great theoretical and practical significance. The main purposes of this study were to study the kind of knowledge that is useful for WSD, and to establish a new WSD model based on syntagmatic features, which can be used to disambiguate noun sense in Mandarin Chinese effectively. Close correlation has been found between lexical meaning and its distribution. According to a study in the field of cognitive science [Choueka, 1983], people often disambiguate word sense using only a few other words in a given context (frequently only one additional word). Thus, the relationships between one word and others can be effectively used to resolve ambiguity. Based on a descriptive study of more than 4,000 Chinese noun senses, a multi-level framework of syntagmatic analysis was designed to describe the syntactic and semantic constraints of Chinese nouns. All of these polyseme nouns were surveyed, and it was found that different senses have different and complementary distributions at the syntax and/or collocation levels. This served as a foundation for establishing an WSD model by using grammatical information and a thesaurus provided by linguists.  
The typical approaches to extracting text knowledge are sentential parsing and pattern matching. Theoretically, text knowledge extraction should be based on complete understanding, so the technology of sentential parsing is used in the field. However, the fragility of systems and highly ambiguous parse results are serious problems. On the other hand, by avoiding thorough parsing, pattern matching becomes highly efficient. However, different expressions of the same information will dramatically increase the number of patterns and nullify the simplicity of the approach. Parsing in Chinese encounters greater barriers than that in English does. Firstly, Chinese lacks morphology. For example, recognition of base-NP in Chinese is more difficult than that in English because its left boundary is hard to discern. * 北京語言大學計算機系 Beijing Language and Culture University E-mail: songrou@blcu.edu.cn + 北京工業大學計算機學院 Beijing Polytechnic University E-mail: hopexy163@163.com  102  宋柔､許勇  Secondly, there are many stream sentences in Chinese which lack subjects and cause parsing to fail. Finally, in Chinese, the absence of verbs is also pervasive. Sentential parsing centering on verbs, which is used with English, is not always successful with Chinese. We are engaged in research on knowledge extraction from the Electronic Chinese Great Encyclopedia. Our goal is to extract unstructured knowledge from it and to generate a well-structured database so as to provide information services to users. The pattern-matching approach is adopted. The experiment was divided into two steps: (1) classifying entries based on lexicon semantics; (2) establishing a formal system based on lexicon semantics and extracting knowledge by means of pattern matching. Classification of entries is important because in the text of the entries of different categories there are different kinds of patterns expressing knowledge. Our experiment demonstrated that an entry of the encyclopedia can be classified precisely merely according to the characters in the entry and the words in the first sentence of the entry’s text. Some specific categories, e.g., organization names and Chinese place names, can be classified satisfactorily merely according to the suffix of the entry, for suffixes are closely related with semantic categories in Chinese. The formal system designed for knowledge extraction consists of 4 kinds of meta knowledge: concepts, mapping, relations and rules, which reflect lexicon semantic attributes. The present experiment focused on the extraction of knowledge about various areas from the texts regarding administrative places of China (how large is a place or its subdivisions). The results of the experiment show that the design of the formal system is practical. It can accurately and completely denote various expressions of simple knowledge in a Chinese encyclopedia. However, when the focus of knowledge changes, e.g., from administrative areas to habits of animals, it is a labor-intensive task to renew the formal system. Therefore the study of auto or semi-auto generation of this kind of formal system is required. 1. 問題背景 以信息技術為基礎的在線知識服務是信息產業的發展方向。目前已經出現具有初步實用 價值的在線知識服務，其中最具生命力的是問答服務（Q&A），而這一服務的關鍵技術  基於詞彙語義的百科辭典知識提取實驗  103  之一是文本知識的自動提取，它是為各種各樣的問題提供答案的基礎。 目前，網絡技術的發展使得人們能輕易地獲取幾乎無窮無盡的文本。但由於網絡文 本範圍太廣，涉及的語言現象太複雜，全自動、高準確率的信息提取和知識提取困難較 大，短期內難以實用。百科辭典是一種受限的文本，知識含量高，知識表述比較規範。 無論是從理論的角度看還是從應用的角度看，從百科辭典中自動獲取知識可當作文本知 識自動提取的突破口。 文本知識提取的方法主要有兩種：基於語句分析的方法和基於模式的方法[Tsujii, J. 2000]。理論上說，文本知識的提取需要在徹底理解的基礎上進行。因此，句法分析和語 義分析技術很自然地使用於這一領域，但它有脆弱性和多歧義問題。基於模式的方法可 以避免對語句進行徹底分析，效率較高，但同一信息的不同表達形式會使模式數量大為 膨脹。[Tsujii, J. 2000，Hull, R. et al. 1999 and Soderland, S. G. 1996]處理的是英語或日語 文本，主要使用基於語句分析的方法。其中[Hull, R. et al. 1999]的工作是基於大容量的知 識庫，採用部分分析、語義解釋和推理的步驟；[Soderland, S. G. 1996]也是進行句法分析， 包括名詞短語分析、同位關係識別（Appositive Recognition）和同指消解（Coreference Analysis）；[Tsujii, J. 2000]本身的工作主要使用語句分析的方法，但吸收了模式方法的 優點。 漢語文本的知識提取使用語句分析方法比英文問題更大。首先是因為漢語缺乏形式 標誌。比如基本名詞短語的識別在英語中並不困難，但在漢語中由於難以確定其左邊界 而識別率較低。其次，漢語常有缺主語的流水句，會造成句法分析的失敗。此外，英語 句子的句法分析和語義分析一般都以動詞為核心，而相當一部分漢語的句子沒有動詞， 如“昌平縣面積 1352 平方公里，人口 43 萬”。如果照搬英語中的做法做句法分析（或淺 層句法分析）、找動詞的語義格，其效果不會好。 漢語文本知識提取的工作已發表的並不多。[Gu, F. et al. 2001]的工作也是從百科辭 典中提取知識，它的結果是一個框架結構的知識庫，可以提供實用的知識服務。但為了 得到這個知識庫，需要先設計一個形式語言，並用它對辭典文本進行人工標注。 本文研究漢語百科辭典的知識提取。我們的目標也是把百科辭典中的無結構的知識 提取出來，生成帶結構的數據庫，向用戶直接提供知識服務。這項工作當然只能在一個 受限範圍內通過人機結合的方式來完成。但是，我們希望使人的勞動集中於詞彙的語義 屬性研究和詞庫中詞彙的語義屬性標注，避免人工標注語料所需的巨大勞動量。由於上 述漢語分析中的困難，我們不採用常規的句法語義分析，而嘗試關鍵詞語為核心的模式 匹配的方法，其中關鍵詞語不一定是動詞，但具有信息提示的功能（如“面積”提示其後 面有關於面積數量的信息），模式匹配主要依靠詞語的語義屬性。 我們的處理對象是《中國大百科全書》（光盤版），工作步驟是：（1）根據詞目 確定題材類別，根據題材類別確定知識提取的目標；（2）建立基於詞彙語義的形式系統， 用詞語模式匹配的方法提取知識。本文介紹了相關研究的一些實驗，測試結果證明這一 方法是有效的。  104  宋柔､許勇  2. 百科辭典詞目的分類  2.1 百科辭典詞目按題材分類的試驗  為了提取知識的方便，首先需要把按領域分卷的《中國大百科全書》中的詞目進行分類。  這裏所說的詞目的類別，不是按專業領域劃分，而是按題材劃分的。比如，人物和 概念是不同的題材。《中國大百科全書》美術卷中人物“徐悲鴻”釋文與數學卷中人物“華 羅庚”釋文的風格相似，所表達的信息內容的類型十分接近，但與同在美術卷中概念“油 畫”的釋文風格和信息內容的類型完全不同。  題材的異同取決於詞目的語義類。所有類別的釋文第一句話總是對於詞目給出一個 概括性的說明，指出它的最重要的特徵，如人物的國籍和歷史地位，行政區劃的行政隸 屬和政治經濟地位，動物的目科屬種等。第一句話以後，不同語義類的釋文有不同的信 息內容。比如，人物的釋文包括人物的生卒時間和地點、生平事蹟、主要成就等，行政 區劃的釋文包括該地區的面積、人口、沿革、地形、氣候、經濟、特產、名勝等，動物 的釋文包括動物的體形、各部位的形狀大小顏色、分佈區域、生活習性、繁殖方式、與 人類的關係等。  從百科辭典知識提取的使用目標出發，我們目前採用的詞目分類系統中的大類是人 物、行政區劃、自然地理、動物、植物、機構組織、事件、裝置、其他。之所以採用這 樣的分類體系，一是因為這些類的詞目和釋文有比較明顯的特徵，知識抽取相對容易； 二是因為這些類在整個百科辭典中所占比重較大，詞目較多，有條件使用統計方法進行 信息提取。有些大類下面還要分小類，如自然地理類中包括山脈、河流、湖泊、沙漠、 島嶼等等，分小類的目的是使同類釋文的信息特徵更加一致。  我們使用現代漢語通用分詞系統 GWPS 的專名識別功能將詞目中的人名、地名（包 括行政區劃名、自然地理名、古地名、景點設施名）、機構名挑選出來，實驗對象是美 術卷、外國文學卷、世界地理卷、中國地理卷。我們只使用詞目內部的用字信息和釋文 第一句話最後兩個詞的信息，識別結果如下：  美術卷人名 外國文學卷人名 世界地理卷地名 中國地理卷地名 美術卷機構名 合計  實有詞目 935 2470 1153 1498 98 6154  識別詞目 935 2471 1154 1500 98 6158  遺漏 1 0 5 0 0 6  誤識 1 1 6 2 0 10  準確率 99.9 99.96 99.5 100 100 99.8  召回率 99.9 100 99.6 99.9 100 99.9  如果不用釋文信息，只用詞目內部的用字信息，則對機構名和中國地名影響不大， 對人名和外國地名來說召回率大大降低。如此時美術卷人名詞目識別結果為：  實有詞目 識別詞目 遺漏 誤識 準確率 召回率  基於詞彙語義的百科辭典知識提取實驗  105  935  779 166 10 98.7 82.2  原因是有些人名詞目使用的是法號（如“法常”）、綽號（如“泥人張”），有些是 GWPS 不具有識別能力的日本人名（如“奧村土牛”）。人名釋文的首句最後一個詞絕大部分是“身 份詞”（“畫家”、“建築師”等），大部分首句前還帶有說明生卒年代的括號，所以利用了 釋文首句的信息後，召回率大大提高。  地名詞目中，不同小類的詞目的釋文風格差別仍然很大。比如，行政區劃名釋文的 主要信息是行政隸屬關係和政治經濟地位、面積、人口、沿革、地形、氣候、經濟、特 產、名勝等，自然地理名的釋文中沒有這些內容。行政區劃名和自然地理名中還需分更 小的類，因為行政區劃中，關於國家的釋文同關於城市的釋文在詳盡程度上很不同，信 息內容的類型上也有區別。自然地理名中，關於山脈的要介紹山脈地理分佈、走向、山 峰高度、地質歷史等，關於河流的要介紹河流發源地、走向、流域面積、經濟功能等。 為此，我們對於中國地理卷中的地名進行了細分類試驗。對於行政區劃詞目，分為省、 自治區、地區、自治州、市、區、縣（包括自治縣）、鎮，共 8 類；對於自然地理詞目， 挑選出江河、湖泊、山嶺、山脈、盆地、沙漠、平原、高原、丘陵、草原、島嶼，共 11 類。我們試驗完全依據詞目後綴進行識別。行政區劃詞目所用後綴和識別結果如下：  類名 後綴  省 市 地區 自治州 區 縣 鎮 合計 省 市 地區 自治州 區* 縣 鎮  實有  23 385 5  9 22 275 36 755  標識  23 385 5  9 25 275 36 758  誤識  00 0  0300 3  漏識  00 0  0000 0  準確率% 100 100 100  100 88 100 100 99.6  召回率% 100 100 100  100 100 100 100 100  注：“區”類要從後綴“區”中去掉後綴“自治區”、“地區”、“風景區”、“風景名勝區”、 “自然保護區”、“灌區”。該類的 3 個誤識錯誤是“皖西山區”、“皖南山區”、“神農架林區”。 簡單地把“山區”當作後綴從“區”類中去掉是不行的，因為上海有“寶山區”，北京曾有“燕 山區”，等等。 自然地理詞目所用後綴和識別結果如下：  類名 河流 湖泊 山嶺* 山 島 盆 沙 平 高 草 丘 合計  後綴 江,河*, 湖, 錯, 山, 溪, 水* 池, 海* 峰,  脈 嶼 地 漠 原 原 原 陵  山 島* 盆 沙 平 高 草 丘  脈  地 漠* 原 原 原 陵  106  宋柔､許勇  嶺  實有 144 65 162 19 20 14 5 19 11 2 8 466  標識 137 59 162 19 20 14 5 19 11 2 8 456  誤識  3  
Word sense is ambiguous in natural language processing (NLP). This phenomenon is particularly keen in cases involving noun-verb (NV) word-pairs. This paper describes a sense-based noun-verb event frame (NVEF) identifier that can be used to disambiguate word sense in Chinese sentences effectively. A knowledge representation system (the NVEF-KR tree) for the NVEF sense-pair identifier is also proposed. We use the word sense of Hownet, which is a Chinese-English bilingual knowledge-base dictionary. Our experiment showed that the NVEF identifier was able to achieve 74.8% accuracy for the test sentences studied based only on NVEF sense-pair knowledge. By applying the techniques of longest syllabic NVEF-word-pair first and exclusion word checking, the sense accuracy for the same test sentences could be further improved to 93.7%. There were four major reasons for the incorrect cases: (1) lack of a bottom-up tagger, (2) lack of non-NVEF knowledge, (3) inadequate word segmentation, and (4) lack of a multi-NVEF analyzer. If these four problems could be resolved, the accuracy would reach 98.9%. The results of this study indicate that NVEF sense-pair knowledge is effective for word sense disambiguation and is likely to be important for general NLP. Keywords: word sense disambiguation, event frame, top-down identifier, Hownet 1. Introduction Word sense disambiguation (WSD) has been a pervasive problem in natural language processing (NLP) since 1949 [Weaver 1949]. Word sense ambiguity (or lexical ambiguity), is generally classified into two types: syntactic and semantic ambiguity [Small et al. 1988, Krovetz et al. 1992]. Syntactic ambiguity is caused by differences in syntactic categories (e.g.  * Institute of Information Science, Academia Sinica, Nankang, Taipei, Taiwan, R.O.C. E-mail: {tsaijl,hsu,raycs}@iis.sinica.edu.tw  30  J. L. Tsai et al.  “play” can occur as a noun or verb). Semantic ambiguity is caused by homonymy (e.g. “bank” in “to put money in a bank,” “the bank of a river”) or polysemy (e.g. “face” in “human face,” “face of a clock”). Although many approaches have been adopted to disambiguate word sense, algorithms for word sense determination still are not reliable [Krovetz et al. 1992, Resnik et al. 2000]. Human beings usually can disambiguate word sense by using additional information from the speaker, the writer or the context. When out-of-context (or out-of-sentence) information is not symbolized and processed in the computer, WSD either becomes very difficult or, sometimes, impossible. Therefore, it is crucial to investigate what kind of knowledge is useful for WSD [Krovetz et al. 1992].  According to a study in cognitive science [Choueka et al. 1983], people often disambiguate word sense using only a few other words in a given context (frequently only one additional word). Thus, the relationships between one word and others can be effectively used to resolve ambiguity. Furthermore, from [Small et al. 1988, Krovetz et al. 1992, Resnik et al. 2000], most ambiguities occur with nouns and verbs, and the object-event (i.e. noun-verb) distinction is a major ontological division for humans [Carey 1992]. However, no clear data has been collected to support these claims. These observations motivated us to demonstrate through an experiment, how noun-verb (NV) relationships can be used to disambiguate word sense in Chinese sentences.  In this paper, we shall focus on word sense disambiguation involving NV word-pairs since  these are most troublesome. Consider the following sentence: “  (This car  moves well).” In this sentence, we have two possible NV word-pairs, “ - (car, move)”  and “ - (auto-shop, move).” It is clear that the permissible NV word-pair is “ -  (car, move).” We shall call such a permissible NV word-pair an NV-event frame (NVEF)  word-pair. Using a collection of pre-learned NVEF word-pairs, we can identify the NVEF  word-pair “ - ” from the sentence “  .” The word “ ” in a dictionary  can have three possible senses: ‘surname’ (noun), ‘car’ (noun) and ‘turn’ (ve rb). To resolve  this ambiguity, we can use the pre-defined sense of the NVEF word-pair “ - (car,  move)” to determine that the correct sense of the Chinese word “ ” is “car” in the above  Chinese sentence.  In this paper, we shall show that knowledge of NVEF sense-pairs (to be defined in Section 2) can be effectively used to resolve word sense ambiguity. In the next section, we will propose an NVEF sense-pair identifier, which is based on pre-stored knowledge of NVEF sense-pairs. We use this NVEF sense-pair identifier to identify NVEF word-pairs in an input sentence and to determine the corresponding word senses. In Section 3, we will present and analyze the results of a WSD experiment on a set of test sentences using the NVEF sense-pair identifier. Finally, we will give conclusions and directions for future research in Section 4.  Word Sense Disambiguation and Sense-Based NV Event Frame Identifier  31  2. Development of an NVEF Sense-Pair Identifier We use Hownet [Dong] as our system’s Chinese machine-readable dictionary (MRD). Hownet is a Chinese-English bilingual knowledge-base dictionary, which provides knowledge of the Chinese lexicon, parts-of-speech (POS) and word senses.  2.1 Definition of an NVEF Sense-Pair  The sense of a word is defined as its DEF (concept definition) in Hownet. Table 1 lists three different senses of the Chinese word “ (Che/car/turn).” In Hownet, the DEF of a word consists of its main feature and secondary features. For example, in the DEF “character| ,surname| ,human| ,ProperName| ” of the word “ (Che),” the first item “character| ” is the main feature, and the remaining three items, “surname| ,” “human| ,” and “ProperName| ,” are its secondary features. The main feature in Hownet can inherit features in the hypernym-hyponym hierarchy. There are approximately 1,500 features in Hownet. Each of these features is called a sememe, which refers to the smallest semantic unit that cannot be further reduced.  Table 1. Three different senses of the Chinese word “ (Che/car/turn).”  C.Word a E.Word a Part-of-speech Sense (i.e. DEF in Hownet)  Che Noun  car  Noun  turn Verb  character| , surname| , human| , ProperName| LandVehicle| cut|  a C.Word refers to a Chinese word; E.Word refers to an English word  The Hownet dictionary used in this study contains 50,121 Chinese words, among which there are 29,719 nouns, 16,652 verbs and 16,242 senses (including 9,893 noun-senses and 4,440 verb-senses). Table 2 gives the statistics of the number of senses per Chinese word and the number of Chinese words per sense used in Hownet.  Table 2. Statistics of the number of senses per Chinese word and the number of Chinese words per sense used in Hownet.  Item a  Total  Noun Verb  Maximum number of senses per Chinese word 27  14  24  Mean number of senses per Chinese word  1.24  1.14  1.23  Maximum number of Chinese words per sense 374  372  129  Mean number of Chinese words per sense  3.8  3.0  4.6  a Similar WordNet statistics can be found in [Voorhees 1993]. (WordNet is a trademark of Princeton  University.)  32  J. L. Tsai et al.  Now, take the NV word-pair “ - (car, move)” for example. According to the sense of the Chinese word “ (Che/car/turn)” and the sense of the Chinese word “ (move),” the only permissible NVEF sense-pair for the NV word-pair “ - (car, move)” is “LandVehicle| ”-“VehicleGo| .” We call such a permissible NV sense-pair an NVEF sense-pair in this paper. Note that an NVEF sense-pair is a class that includes the permissible word-pair instance “ - (car, move).”  2.2 Knowledge Representation Tree of NVEF Sense-Pairs  A knowledge representation tree (KR-tree) of NVEF sense-pairs is shown in Fig.1. There are  two types of nodes in the KR-tree, namely, function nodes and concept nodes. Concept nodes  refer to words and features in Hownet. Function nodes are used to define the relationships  between their parent and children concept nodes. If a concept node A is the child of another  concept node B, then A is a subclass of B. Following this convention, we can omit the  function node “subclass” (which should exist) between A and B. We can classify the  noun-sense class (  ) into 15 subclasses according to their main features. They  are “  (bacteria),” “  (animal),” “  (human),” “  (plant),” “ 
This thesis presents a description of a semantic disambiguation model applied in the syntax parsing process of the machine translation system. The model uses Hownet as its main semantic resource, which is a common-sense knowledge base unveiling inter-conceptual relations and inter-attribute relations of concepts as connoting in lexicons of the Chinese and their English equivalents. It can provide rich semantic information for our disambiguation. The model makes the word sense and structure disambiguation in the way of “preferring”. “preferring” is applied in the results produced by the parsing process. It combines the rule-based method and statistic based method. First we extract from a large the co-occurrence information of each sense-atom. The corpus is untagged so the extracting process is unguided. We can construct restricted rules from the co-occurrence information according to certain transfer template. The semantic entry of a word in the Hownet is made of sense-atoms, so we can make out the restricted rules for each entry of any word. During the course of disambiguation, the model constructs the context-related words set for each notational word in the input sentence. The semantic collocation relations between notional words can play a very important role in the syntax structure disambiguation. Our evaluation of some candidates is based on the degree of tightness of match between notional words in the structure. We compare the context-related words set of the word in the current structure with all the restricted rules of the word in the lexicon, and find the best match. Then the entry with the best match is taken as the word’s explanation. And the degree of similarity shows how the word in the structure matches with other notional words in it, so it can be taken as the reference of the notional words. Because the discrepancy of different candidate parses of a structure, the same word has different content-related words set, and so will get different scores. We can calculate the best match according to  一種基於知網的語義排歧模型研究  49  the score of all the notional words of the sentence. In this way we can solve the most of word sense disambiguation and structural disambiguation at the same time. The semantic disambiguation model proposed in this thesis has been implemented in MTG system. Our experiment shows that the model is very effective for this purpose. And it is obviously more tolerant and much better than traditional YES or NO clear cut method. In this thesis we first put forward the general idea of the method and give a brief introduce to the Hownet Dictionary. Then we give the methods of extracting co-occurrence information for each sense-atom from the corpus and transferring this information to restricted rules. Then the algorithm of disambiguation is proposed with detail, which includes constructing context-related words set, the calculation of the similarity between atom-senses, and between restricted-rules and the context-related sets. The experiment result given in the end of the paper shows that the method is effective. Keywords: Word Sense Disambiguation, Hownet, InterLigua, Sense Atom, Corpus, Semantic Environment 1. 前言 1.1 文本分析的歧義消解問題 歧義是自然語言中普遍存在的現象。其研究可追溯至古希臘時期的亞裏斯多德，他在《工 具論‧辯謬篇》中就探討了自然語言的歧義問題。1930 年，恩普森(W.Empson)發表了《歧 義的七種類型》(Seven Types of Ambiguity)一書，開始從語言理論的角度研究歧義問題。 科艾(J.G.Kooij)於 1971 年發表的《自然語言的歧義》則標誌著自然語言的歧義進入了系 統化的研究階段。在現代語言學的發展史上，“歧義問題總是成爲某個新的語言學派崛 起時向傳統陣地進擊的突破口＂[呂叔湘 1984, 馮志偉 1995] 歧義就是同一形式與不同的意義産生聯繫。在自然語言處理中，歧義是一個不能回 避而且也無法回避的問題，它成爲自然語言的自動分析的巨大障礙之一。 漢語的歧義一般可以分爲以下兩種類型[馮志偉 1995, 苑春法 等]： (1) 詞的多義，又稱辭彙歧義，即同一詞語可能具有多個不同的義項；如“打＂一詞 在“打字＂、“打酒＂、“打球＂、“打地基＂、“打人＂中就有不同的意義； (2) 短語的同形異構，又稱結構歧義，即同種組合卻含有不同的句法功能結構。如 “VP+的+是+NP＂就是一個有歧義的結構： “扮演的是一個演員＂  50  楊曉峰、李堂秋  這句話可以理解爲“一個演員扮演了劇中某個角色＂(“扮演的＂是施事)，也可以 理解爲“被扮演成一個演員＂(“扮演的＂的是受事)。 還有“N1+N2+N3＂，可以被理解爲((N1+N2)+N3)，也可以理解爲(N1+(N2+N3))。 這類的歧義結構在漢語中有很多，它一直是語法學家研究的熱點問題。 在機器翻譯中，辭彙歧義表現爲譯文會有多種的選擇，而結構歧義表現爲句法分析 中，一個詞語或片語可能會産生一個以上結構不同的分析結果。 歧義在特定的語義及常識下，並不一定都能夠成立，例如在“打球＂中，根據“打＂ 的受事物件我們可以知道“打＂只能選擇“Play＂的譯文；而在“反對的是戰爭＂中， 我們也可以知道“戰爭＂是反對的受事體而不可能是施事體。消除這類不符合語義知識 的歧義過程稱爲歧義的消解，也稱排歧。我們在機器翻譯的句法分析過程中，必須要引 入語義的知識才能夠更好地完成歧義的自動消解。 1.2 歧義消解的方法 歧義消除的方法多種多樣，有的方法在辭彙量、詞法結構和句型上對源語言文本進行限 制，從而避免大多數的分析及選詞上的歧義；有些學者提出利用語義關聯網進行排歧[Dan Roth 1998]；還有的研究人員嘗試了基於學習的自動消歧方法，如 DAN ROTH 使用 Winnow 學習方法來進行拼寫校正、語料標注[趙鐵軍 等 2000]。面前市場上流行的“雅 信 CAT 漢英雙向翻譯系統＂，則採用了用戶互動式消歧方法，讓用戶自己來決定結構及 譯詞的選擇。 辭彙消歧是結構消歧的基礎。大多數的排歧方法都是以多義詞的詞義排歧爲切入 點。短語、句子、篇章都是由最基本的詞語構成，如果一個句法結構中的詞語意義尚且 不能確定，整個結構的意義的把握更無從談起了。同時，詞義選擇需要足夠多的語義知 識及上下文知識，而這些知識也爲進一步解決結構歧義問題提供了良好的依據。本文的 研究也主要是從詞義消歧入手，在詞義消歧的過程中進行結構的消歧。 詞義消歧方法分爲三類：基於 AI 的方法，基於知識的方法，基於語料庫的方法。 按詞義消歧的智慧程度又可分爲有指導與無指導的方法[Wilks et al. 1998, Philip et al., 董振東 等]。 基於 AI 的方法包括符號主義方法和連接主義方法。如利用神經網路等進行詞義 選擇(Collins)。這類的方法在實際上對語言理解並不實用。 基於知識的方法主要包括基於義類詞類詞典和基於規則的方法。前者的代表爲西班 牙研究者基於 WordNet 提出的應用概念密度的詞義消歧方法。而後者則在基於轉換的機 器翻譯系統中被廣泛地使用，如 Wilks 提出的應用選擇限制來詞義消歧。 基於語料庫的方法分爲基於統計和基於實例的兩種方法。基於統計的方法經常統計 詞與詞、詞義與詞義的搭配，利用搭配消除歧義。基於實例的方法是根據輸入句與實例  一種基於知網的語義排歧模型研究  51  的相似度計算來選擇最佳的匹配。這類方法中比較成功的系統有新加坡的 LEXAS(Hwee)。 以上的方法各有其優缺點。基於知識的方法可以很好地處理確定的，大粒度的知 識，語義和語法的知識比較豐富，但這些知識通常由專家組織，因此有很大的主觀性， 並且知識的一致性、擴充性、完備性都難以很好地實現；而基於統計的方法可以較好地 處理語言中的不確定的、小粒度的知識，靈活性好，但卻難以反映自然語言中具有普遍 性的語法規律和語義知識。 1.3本文的主要工作 本文主要研究目標是在機器翻譯的文本分析中如何引入語義知識進行有效的詞義消歧， 進一步進行結構消歧。在課題的研究過程中，本文主要在以下幾個方面進行了探討： 1：利用《知網》爲語義知識源，從《知網》中抽取出必要的語義資訊，並將之轉 化成爲方便系統實現的表示結構。 2：從基於優選的角度對分析生成的中間語言進行排歧處理。本文先利用大規模的 語料庫獲取義原的同現集合，並根據轉換模板構造出義原的初始限制規則，再通過手工 的方式對初始規則進行修改與調整，以得到一個較完善的規則集。義項的語義限制規則 可以由其構成義原的語義規則得到。排歧演算法將義項的語義規則與義項所在的語義環 境進行相似度的計算，並根據計算結果進行義項選擇和結構的語義搭配的評價，從而進 行詞義排歧與結構排歧。本文提出了這種排歧方法的詳細演算法與實現步驟。 3：將上述的思想及演算法在機器翻譯系統中具體地實現。 2. 系統結構與基礎知識 2.1 系統結構描述 結構消歧的難度很大，各種各樣的歧義結構還有待語法學家的進一步發現與總結。而辭 彙消歧是結構消歧的基礎。短語、句子、篇章都是由最基本的詞語構成，如果一個句法 結構中的詞語意義尚且不能確定，整個結構的意義就無法把握，排歧則更無從談起了。 同時，詞義選擇需要足夠多的語義知識及上下文知識，而這些知識也爲進一步解決結構 歧義問題提供了良好的依據。本文中語義排歧模型的指導思想就是首先解決多義詞的辭 彙歧義，然後在此基礎之上進行結構歧義的消解。爲此本文中提出了一種語義排歧的模 型，該模型以《知網》爲主要語義資源，以“優選＂的方法來實現詞義與結構的消歧。 “優選＂運用于分析所生成中間語言中，這種方法把詞語的當前語境與詞語各義項 的限制規則所描述語義特徵資訊進行比較，根據比較的相似度選擇最合適的義項。同時 將相似度的最大值作爲該詞語的評價值。中間分析結果中各實義詞的評價分值可以成爲  52  楊曉峰、李堂秋  評價此中間結果的依據，以此在多個中間結構中選出最佳的結果（注意本文中提到的“最 佳＂都是相對於演算法而言的，是在當前演算法下最好的解，但這並不一定總是實際正 確的解）。這樣，我們在解決詞義歧義的基礎上同時也解決了結構歧義。“優選＂排歧 將基於規則與基於統計的排歧方法相結合，排歧中所使用的義項限制規則的獲取方法是 從大規模的語料庫中統計出義原的同現集合，再按一定的轉換模板半自動地生成的。使 用這種方法可以大大減少手工編制規則的工作量。  語料庫 同現頻度統計 義原同現集合  知網  輸入句 （經分詞） 進行句法分析 中間語言  轉換模板  語義規則  詞義標注，結構調整  計算評價分值  最優的分析結果 圖 2.1 含有詞義排歧模組的 PARSING 流程 含有詞義排歧模組的漢英機譯系統分析部分的工作流程如圖 2.1 所示。 2.2 《知網》介紹 《知網》(英文名稱 HowNet)是其創建人董振東先生花費逾十年研究心血的重要成果。《知 網》是一個以漢語和英語的詞語所代表的概念爲描述物件，以揭示概念與概念之間以及 概念所具有的屬性之間的關係爲基本內容的常識知識庫，它是一個網狀的有機的知識系 統[李涓子 等 1999]。 語義詞典是知網系統的基礎文件。在這個文件中每一個詞語義項的概念及其描述形 成一個記錄。目前詞典中提供漢英雙語的記錄，每一種語言的每一個記錄都主要包含 4 項內容。其中每一項都由兩部分組成，中間以“=＂分隔。每一個“=＂的左側是資料的 功能變數名稱，右側是資料的值。它們排列如下： NO.= 詞或短語序號  一種基於知網的語義排歧模型研究  53  [W_X= 詞或短語 G_X= 詞或短語的詞性 E_X= 詞或短語的例子]+ DEF= 概念定義  其中的 W_X、G_X、E_X 構成每種語言的記錄，X 用以描述記錄所代表語種，X 爲 C 則爲漢語，爲 E 則爲英語。每個詞語由 DEF 來描述其概念定義，DEF 的值由若干個義 原及它們與主幹詞之間的語義關係描述組成。義原是知網中最基本的、不易於再分割的 意義的最小單位，知網通過對約六千個漢字進行考察和分析來抽取了 800 多個義原，並 總結了如部分、主體、客體、從屬、時空、材料等若干種義原間的語義關係，這些關係 在知網中用義原前附加如“%＂、“@＂、“$＂等相對應符號來表示，因此我們把這些 語義關係稱之義原的字首語義關係，而對應的符號爲義原的字首語義關係符。這些符號 的意義在《知網》中有詳細的定義說明，表 2.1 列舉了本文出現的一些字首符號及其對 應語義關係。具體的符號定義可參看文獻 9。  表 2.1 義原的部分字首符號及其對應語義關係  字首符號  對應的語義關係  #  相關  %  部分  $  事件的受事、目標、所有  *  事件的施事、體驗者、工具  +  蘊含  &  從屬  ~  可能性  @  時空  ?  材料  ^  
The WWW is increasingly being used source of information. The volume of information is accessed by users using direct manipulation tools. It is obviously that we’d like to have a tool to keep those texts we want and remove those texts we don’t want from so much information flow to us. This paper describes a module that sifts through large number of texts retrieved by the user. The module is based on HowNet, a knowledge dictionary developed by Mr. Zhendong Dong. In this dictionary, the concept of a word is divided into sememes. In the philosophy of HowNet, all concepts in the world can be expressed by a combination more than 1500 sememes. Sememe is a very useful concept in settle the problem of synonym which is the most difficult problem in text filtering. We classified the set of sememes into two sets of sememes: classfiable sememes and unclassficable semems. Classfiable sememes includes those sememes that are more * 廈門大學計算機系 Department of Computer Science, Xiamen University , Xiamen，361005 Weifeng Su: waveletsu@263.net  80  蘇偉峰 等  useful in distinguishing a document’s class from other documents. Unclassfiable sememes include those sememes that have similar appearance in all documents. Classfiable includes about 800 sememes. We used these 800 classficable sememes to build Classficable Sememes Vector Space(CSVS). A text is represented as a vector in the CSVS after the following step: 1. text preprosessing: Judge the language of the text and do some process attribute to its language. 2. Part-of-Speech tagging 3. keywords extraction 4. keyword sense disambiguation based on its environment by calculating its classifiable sememes relevance with it’s environment’s classifiable sememes. We add the weight of a semantic item if there are classifiable sememes the same as classifiable sememe in the its environment word’s semantic item. This is not a strict disambiguation algorithm. We just adjust the weights of those semantic items. 5. Those keywords are reduced to sememes and the weight of all keywords ‘s all semantic items ‘s classifiable sememes are calculated to be the weight of its vector feature. A user provides some texts to express the text he interested in. They are all expressed as vectors in the CSVS. Then those vectors represent the user’s preference. The relevance of two texts can be measured by using the cosine angle between the two text’s vectors. When a new text comes, it is expressed as a vector in CSVS too. We find its k nearest neighbours in the texts provided by the user in the CSVS . Calculating the relevance of the new text to its k nearest neighbours and if it is bigger than a certain valve, than it means it is of the user’s interest if smaller, it means that it is not belong to the user’s interesting. The k is determined by calculated every training vector its neighbours. Information filtering based on classifiable sememes has several advantage: 1. Low dimentional input space. We use 800 sememes instead of 10000 words. 2. Few irrelevant feature after the keyword extraction and unclassifiable sememes’s removal. 3. Document vector’s feature’s weight are big.  基於文本概念和kNN的跨語種文本過濾  81  We made use of documents from eight different users in our experiments. All these users provides texts both in Chinese and English. We took into account the user’s feedback and got a result of about 88 percent of recall and precision. It demonstrates that this is a success method. Keywords: Classfiable Sememe, Vector Space, kNN, Text Representation, HowNet 1.引言 隨著因特網和其他在線資訊資源的迅猛發展，大量的資訊朝人們湧來，據統計美國每個 上班人員平均每天收到 80 封電子郵件，當然裏面包含大量的無用的垃圾郵件，顯然如果 對這些郵件逐一查看要花費很多時間，而且有可能激發某些病毒郵件從而破壞電腦系 統，同樣的情景出現在許多辦公環境中，許多人希望能在許多的歷史或者別人送達的電 子文本當中由電腦自動挑出自己最感興趣的內容。 文本過濾是自動分挑出有用的文本的一種很重要的方法。文本過濾是指從大量的源 資訊中過濾出那些最符合用戶需求的資訊傳送給用戶，而跨語種文本過濾是指源資訊中 包含多種語言（比如英語、漢語等），或者某個文本中就含有多種語言，從中過濾出用 戶所需要的文本，過濾出的文本可能也是多種語言的。在沒有國界的因特網上，跨語種 過濾出所要的資訊就顯得更爲重要。在把大量的資訊送給用戶之前過濾掉那些用戶不感 興趣的東西，這比在有條件後，翻譯成某種語言過後再進行過濾更能省掉用戶大量的精 力和時間，跨語種過濾系統對於那些對需要這一語種的資訊而又對該語言掌握得不好的 用戶特別重要。 在跨語種文本過濾方面，人們已經摸索出了許多方法來實現不同語種之間的相互轉 換形式。最初人們是提出一種基於控制辭彙的方法[TRANSLIB 1995]，即把文本表示成 一些固定的詞，用戶的需求也表示成這些固定辭彙，然後進行匹配。這個方法最大的缺 陷是辭彙必須在可管理的範圍之內，而一旦辭彙超出可管理的範圍，則其召回率和精確 率則迅速下降，而且如何把文本表示爲辭彙目前也沒有一個很好的方法。 接著又有人提出基於字典的方法[L. Ballesteros 1996]，就是編輯一本多語字典把某 種語言的文本表現形式通過翻譯表示成另一種語言的表現形式，從而使那些單種語言上 的文本過濾技術可以應用於多語言的文本過濾，這個方法理論上是有可能的，但是有兩 個方面的原因卻限制了它的應用。首先是一詞多義的現象，在翻譯中一個詞可能翻譯成 幾個意思，若幾個意思全都採用則大大降低了精確率，若採用某一個意思，則有可能降 低召回率，或者根本就選擇錯誤而導致召回率極低。第二是一義多詞的現象，由於不同 的作者可能用不同的詞來表達同一個意思而導致召回率下降。 本文提出一種新的思路，我們不從詞這一級來分析概念，而是把詞所包含的概念進 行分解，再對分解過後的概念進行分析，從而得到文本的主題和性質。其實類似的思想  82  蘇偉峰 等  在一些自然學科當中經常用到，比如我們分析某種物質的性質時，我們經常從其構成的 分子或原子水平的性質進行分析然後再得到物質的性質。 2. 過濾模型的系統結構 我們採用的技術主要是向量空間模型，即文本表示成爲向量空間中的一個向量，向量空 間的優點是將文本內容轉換成易爲數學處理的向量方式，使得各種相似運算和排序成爲 可能。因此 ，在文本檢索、文本過濾和文本摘要等方面獲得廣泛應用，取得了良好效果。 本文所提出的基於向量空間的文本過濾模型可以用於對中文和英文的文本進行過濾。其 基本思想是首先利用用戶所提供的材料來獲取用戶的模板，然後利用用戶模板來判斷某 一文件是否與用戶模板相近。 我們採用了董振東先生所研製的《知網》[董振東 等]，該系統帶有 53000 個中文片 語和 57000 英語單詞。《知網》是一個以漢語和英語的詞語所代表的概念爲描述物件， 以揭示概念與概念之間以及概念所具有的屬性之間的關係爲基本內容的常識知識庫，《知 網》採用義原來表示概念，義原是最基本的、不易於再分割的意義的最小單位，我們設 想所有的概念都可以分解成各種各樣的義原。董振東先生提取出了 1500 多個義原，並用 它們的組合來表示世上所有的概念，比如它是這樣注釋“扭虧爲盈”的： DEF=alter|改變,StateIni=InDebt|虧損,StateFin=earn|賺。 即是指“扭虧爲盈”是一種“改變”，其起始狀態是“虧損”，最終狀態是“賺”。 把概念分解成爲義原可以極大限度地解決一義多詞的問題，比如“電腦”、“電腦”、 “computer”這三個詞，在《知網》裏均定義爲“computer|電腦”，這樣我們就可以把它們視 爲概念等同的三個詞語。其實從這個層面上來理解，我們可以把某個詞的中英文意思同 樣看作是一個一義多詞的一種形式，這樣只要解決好了排岐的問題，我們並不需要特殊 處理就可以解決跨語種的問題，因而從義原這一個層面上來說我們的方法可以說是一種 與語言無關的方法。 我們把義原繼續分爲兩類：可分義原和不可分義原。把義原分爲可分義原與不可分 義原是從以下兩方面考慮的： • 某一義原若是在某一類主題的文檔當中出現頻度越高，則認爲該義原與這一類主 題越有關係。 • 某一義原若是在語料庫中所有的文檔當中出現的頻度越高，則認爲該義原在區分 主題的作用越差。 因此不可分義原是指那些比較常見，沒法用來指出該概念一些特有的性質的義原， 而可分義原則是指那些能表示該概念的重要的可與別的概念相區分的義原。若我們不排 除掉不可分義原，則由於不可分義原的較高的出現頻率，就有可能誤導我們。可分義原 在本系統中占著重要的地位。 可分義原和不可分義原的粗略判定方法如下：  基於文本概念和kNN的跨語種文本過濾  83  從語料庫中隨機抽取 500 篇各類的文章，在對這些文章進行分詞之後，把詞分解成 相應的義原，並對這些義原進行統計，若某個詞有岐義，則把該詞的所有意義進行歸一 併將其義原加入統計，設定一高通篩檢程式，對於義原的統計值高於某個值的列爲不可 分義原，其餘的義原定爲可分義原。 本文下面所用到的技術對於中文文檔和英文文檔同時適用，若是有不同的地方則會 分別指出。 3. 過濾模型的設計和實現  3.1 文本表示方法  我們採用的技術是向量空間模型，文本表示爲向量空間中的一個向量。向量空間表示爲  → D  ，而每一個分量  di  是知網中的一個可分義原，那文本就表示成向量  → V  ，其分量 vi  爲  對應於 di 的值，若文本中沒有包含 di ，則 vi =0。  然而並非文件當中所有的詞都用於構造文本向量，只有那些最能代表文件所要表達  的意思的詞也就是關鍵字彙可被用來構造向量。我們可以採用統計的方法來決定哪些辭  彙是關鍵字彙，還有，由於辭彙的岐義，我們也要作一定程度上的排岐。文本表示方法  可歸納如下：  文本預處理。對於中文文本包括詞的切分、詞性標注，而對於英文文本，則只進行 詞性標注。  關鍵字提取。在英語文本中去除所有屬於下列的單詞：冠詞（如 a, the, an）、介詞 或連接主句和從句的副詞（如 in ,to ,of）、情態動詞（如 would, must）和連接詞（如 and） 等，在中文文本中去除所有的虛詞，這樣在文本中就剩下主要的詞像名詞、動詞、形容 詞和副詞，形成關鍵字序。我們也可以給各種詞性的詞賦予不同的權值來表示它們不同 的重要性，一般而言，名詞要賦以最大的權值。對於那些在標題、首段、末段、段首、 段尾出現的詞語也可以增加其權重。我們也可以設一個閥值，把那些出現頻率低於該頻 率的詞去除。  關鍵字概念排岐。過多的岐義會損害我們向量表示該文本的效果，尤其當某個詞在 該文本當中佔有比較重要的地位時。排岐的基本思想是根據上下文詞的義原對該詞爲某 一意思進行概率統計。其主要思想是：在一篇文章當中，某個詞會對上下文的用詞産生 影響，通過上下文可以判定某個詞的意思從而進行排岐，在本模型下，著重考慮其上下 文當中其他的關鍵字的義原與該詞的義原有無以下情況： a.有相同可分義原， b.材料-成品關係， c.施事/經驗者/關係主體-事件關係， d.受事/內容/領屬物等-事件關係， e.工具-事件關係，  84  蘇偉峰 等  f.場所-事件關係， g.時間-事件關係， h.事件-角色關係， i. 相關關係。 如果其上下文的某個關鍵字當中有個可分義原與該詞的某個意思的某一可分 義原有上述關係，則增加該意思的權重。 在 W 中，對某個詞 w，在以其爲中心的窗口寬度爲 n 的字串表示爲： w1w2…wn/2wwn/2+1…wn-1 對於 w 在知網中的每一個意思，賦予一個初權 k，調節詞 w 每一個意思的權值 的方法的僞代碼演算法 1 所示 演算法 1 ：詞的義原的權值的調節 WI—窗口中除去 w 的第 I 個詞 SI J—窗口中除去 w 的第 I 詞的第 J 個意思 CSI JK--窗口中除去 w 的第 I 詞的第 J 個意思的第 K 個可分義原 WSJ—詞 w 的第 J 個意思 WCSJK--詞 w 的第 J 個意思的第 K 個可分義原 Weight(WCSJ)—詞 w 的第 J 個意思的權值 FOR I=1 TO n-1 //對於窗口中除了 w 外的每一個詞 FOR J=1 TO (WI 的意思數目) FOR K=1 TO ( SI J 的可分義原數目) FOR M=1 TO (詞 w 的意思數目) FOR O=1 TO (WSJ 的可分義原數目) IF CSI JK 與 WCSJK 有上述關係 THEN Weight(WSJ)= Weight(WSJ) +1 ENDIF ENDFOR ENDFOR ENDFOR ENDFOR ENDFOR 由此，詞語的那些與上下文相關的意思都通過增加權值而得到加強，當然我們 還要對此進行歸一化處理，其歸一化的公式如下所示：  ∑ wt(WSi ) = Weight(WSi ) Weight(WSi ) i 其中 i 是該詞的意思的序號。  （1）  基於文本概念和kNN的跨語種文本過濾  85  文本表示成一向量。在經過了關鍵字提取和排岐之後，我們把這些關鍵詞根據其義 原權值按照知網裏的單詞定義分解成爲義原，並在去除了不可分義原之後，我們採用演 算法 2 中的方法計算各可分義原，文件就表示成了可分義原空間中的一個向量。 演算法 2 把一個文件表示成可分義原空間的一個向量演算法 VK— 向量中的分量的值 SMI JK—第 I 個關鍵字第 J 個意思的第 K 個可分義原 Weightof(SM)—某個可分義原的標量值 wt(SI J)－第 I 個關鍵字第 J 個意思的權值 給向量的每個分量值賦初值 0 FOR I:=1 TO (關鍵字的數目) FOR J=1 TO (第 I 個關鍵字的意思總數) FOR K=1 TO (第 I 個關鍵字第 J 個意思) Weightof(SMK)= Weightof(SMK)+wt(SI J) ENDFOR ENDFOR ENDFOR  3.2 用戶模板表示 首先用戶提供 m 篇其所感興趣的文檔，爲了增加用戶興趣的文本在向量空間中的密度， 一般要求 m>50，採用上文所述的方法把這些文本表示爲可分義原空間中的向量，這些 向量就成了代表該用戶興趣的示例，我們稱其爲用戶示例。在進行文本過濾時，我們就 是從用戶示例中找出 k 個與正在過濾的文本最爲鄰近的向量作爲鄰居向量進行分析。  3.3 文本相似度的計算  至此，文本已表示成可分向量空間中的一個向量，兩個文本的相似度可以通過公式（1） 中的余弦值表示，其值越大，則表示這兩個文本的主題越相似，我們認爲他們是越相近 鄰居：  cos(a) = (Vuser,Vtext ) | Vuser || Vtext |  （2）  其中 (Vtext1,Vtext2) 是指用戶向量和文本向量的內積， | Vtext | 表示文本向量的標量。 在文本過濾當中，我們採用了 k 個最近鄰居（kNN）的方法：對於某一輸入文本 s， 按照上面所述的方法將其表示爲可分義原空間的向量，在用戶示例中，利用公式（2）挑 選出 k（k<<m）個與之最相近的鄰居文本，根據公式（3）計算它與這 k 個文本的相似 程度 Si，其值越高，則我們 r 認爲它越是用戶所感興趣的文本。  86  蘇偉峰 等  k  ∑ S Si =  2 (cos(ai ))  (3)  i =1  ⎧0  其中  S(x) = ⎩⎨x  当 x<h 时  当 x>=h 时  在所需過濾的所有文本當中，我們可以根據 Si 來進行相關度排序反饋給用戶，也可 以設一閥值 t，當某文本與用戶需求的相關度大於 t 時則認爲該文本符合用戶需求，把文 本按相關度大小的順序返回給用戶，把低於該值的所有文本去除或存在某處以備用戶在 有空時處理。我們可以把用戶的回饋考慮進去，若用戶認爲幾乎所有我們所過濾出的文 件都是他所感興趣的，則我們可調低 t 值，反過來，若有很多文本不符合用戶的興趣， 則我們調高 t 值。  3.4 文本類別的歸類  我們採用 kNN 的方法。首先我們訓練的時候，我們把這些已經分好類的按是否爲用戶的 需要全部按上述方法表示成可分義原向量空間的向量，對一新進來的一個新的文本，我 們採用上面的方法轉化爲可分義原向量空間中的空間向量，假設爲 d，從中找出 k 個與 其最爲鄰近的向量，然後檢查這 k 個已經確定好類別的向量的類別作爲這個向量的類 別。這 k 個向量的權重可以通過其與 d 的相近程度進行賦值。  kNN 是一個基於範例的學習法，其主要的計算量是從向量空間中找出 k 個最近的鄰 居時間複雜度爲 O(L*N)，其中 L 是可分向量空間的可分義原數目，N 爲可分向量空間中 的訓練文本的數量。  k 值的確定方法：  我們主要採用登山法來確認 k 值，在訓練文本全部表示成向量空間的向量以後，按 下面演算法進行計算： 演算法 3 kNN 中的 k 的計算演算法  biggestequal:=0 bigestk：=0； 給向量的每個分量值賦初值 0 FOR k:=（一個>1 的小整數）TO  (一個大整數)  km:=0; FOR I=1 TO (訓練文本的數目) 對於第 I 個訓練文本，計算 k 個最近鄰居，並利用 k 個鄰居的類別判定 第 I 個文本的類別，如果相等，則 km:=km+1;  ENDFOR  基於文本概念和kNN的跨語種文本過濾  87  If km>biggestequal then Begin biggestequal:=km; bigestk:=k; end; ENDFOR 4. 過濾模型的實驗結果及實驗分析 我們獲得了八個用戶的實驗資料，這八個用戶都提供了他所感興趣的內容相近的中英文 文本各 60 篇作爲相關文本，另外提供 1000 篇其他內容的文本作爲干擾文本，其中中英 文各 500 篇，對於每個用戶，我們使用從其所提供的相關文本隨機抽取中英文文本各 30 篇構造其用戶模板，其餘的相關文本與干擾文本混雜一起構成了測試集，我們就想從其 中過濾出那些相關文本。 我們使用了兩個參數來評價我們的模型：召回率和精確率。召回率是指我們過濾出 的相關文本占所有相關文本的比率，精確率是指在我們所有過濾出的文本當中，相關文 本所占的比率，一般而言，召回率上升，則精確率會下降，而精確率上升，則召回率會 下降。 表 1 就是我們實驗的結果，結果表明用該方法進行過濾的方法效果非常好，精確率 很高，在實際應用當中，我們還可以把用戶反饋的情況考慮進去，形成可根據用戶的興 趣改變而把改變用戶模板向量從而改變選擇的文本的自適應系統。  User User User User User User User User Average  
 Computational Linguistics  Volume 28, Number 4  bution of the paper, but it might also refer to a previous approach that the authors criticize. Depending on its rhetorical context, the same sentence should be treated very differently in a summary. We propose in this article a method for sentence and content selection from source texts that adds context in the form of information about the rhetorical role the extracted material plays in the source text. This added contextual information can then be used to make the end product more informative and more valuable than sentence extracts. Our application domain is the summarization of scientiﬁc articles. Summarization of such texts requires a different approach from, for example, that used in the summarization of news articles. For example, Barzilay, McKeown, and Elhadad (1999) introduce the concept of information fusion, which is based on the identiﬁcation of recurrent descriptions of the same events in news articles. This approach works well because in the news domain, newsworthy events are frequently repeated over a short period of time. In scientiﬁc writing, however, similar “events” are rare: The main focus is on new scientiﬁc ideas, whose main characteristic is their uniqueness and difference from previous ideas. Other approaches to the summarization of news articles make use of the typical journalistic writing style, for example, the fact that the most newsworthy information comes ﬁrst; as a result, the ﬁrst few sentences of a news article are good candidates for a summary (Brandow, Mitze, and Rau 1995; Lin and Hovy 1997). The structure of scientiﬁc articles does not reﬂect relevance this explicitly. Instead, the introduction often starts with general statements about the importance of the topic and its history in the ﬁeld; the actual contribution of the paper itself is often given much later. The length of scientiﬁc articles presents another problem. Let us assume that our overall summarization strategy is ﬁrst to select relevant sentences or concepts, and then to synthesize summaries using this material. For a typical 10- to 20-sentence news wire story, a compression to 20% or 30% of the source provides a reasonable input set for the second step. The extracted sentences are still thematically connected, and concepts in the sentences are not taken completely out of context. In scientiﬁc articles, however, the compression rates have to be much higher: Shortening a 20-page journal article to a half-page summary requires a compression to 2.5% of the original. Here, the problematic fact that sentence selection is context insensitive does make a qualitative difference. If only one sentence per two pages is selected, all information about how the extracted sentences and their concepts relate to each other is lost; without additional information, it is difﬁcult to use the selected sentences as input to the second stage. We present an approach to summarizing scientiﬁc articles that is based on the idea of restoring the discourse context of extracted material by adding the rhetorical status to each sentence in a document. The innovation of our approach is that it deﬁnes principles for content selection speciﬁcally for scientiﬁc articles and that it combines sentence extraction with robust discourse analysis. The output of our system is a list of extracted sentences along with their rhetorical status (e.g. sentence 11 describes the scientiﬁc goal of the paper, and sentence 9 criticizes previous work), as illustrated in Figure 1. (The example paper we use throughout the article is F. Pereira, N. Tishby, and L. Lee’s “Distributional Clustering of English Words” [ACL-1993, cmp lg/9408011]; it was chosen because it is the paper most often cited within our collection.) Such lists serve two purposes: in themselves, they already provide a better characterization of scientiﬁc articles than sentence extracts do, and in the longer run, they will serve as better input material for further processing. An extrinsic evaluation (Teufel 2001) shows that the output of our system is already a useful document surrogate in its own right. But postprocessing could turn  410  Teufel and Moens  Summarizing Scientiﬁc Articles  AIM  10 Our research addresses some of the same questions and uses similar raw data,  but we investigate how to factor word association tendencies into associations  of words to certain hidden senses classes and associations between the classes  themselves.  11 While it may be worthwhile to base such a model on preexisting sense classes  (Resnik, 1992), in the work described here we look at how to derive the classes  directly from distributional data.  162 We have demonstrated that a general divisive clustering procedure for probability  distributions can be used to group words according to their participation in  particular grammatical relations with other words.  BASIS  19 The corpus used in our ﬁrst experiment was derived from newswire text auto-  matically parsed by Hindle’s parser Fidditch (Hindle, 1993).  113 The analogy with statistical mechanics suggests a deterministic annealing pro-  cedure for clustering (Rose et al., 1990), in which the number of clusters is  determined through a sequence of phase transitions by continuously increasing  the parameter EQN following an annealing schedule.  CONTRAST 9 His notion of similarity seems to agree with our intuitions in many cases, but it is  not clear how it can be used directly to construct word classes and corresponding  models of association.  14 Class construction is then combinatorially very demanding and depends on  frequency counts for joint events involving particular words, a potentially un-  reliable source of information as we noted above.  Figure 1 Extract of system output for example paper.  0 This paper’s topic is to automatically classify words according to their contexts of use. 4 The problem is that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. 162 This paper’s speciﬁc goal is to group words according to their participation in particular grammatical relations with other words, 22 more speciﬁcally to classify nouns according to their distribution as direct objects of verbs. Figure 2 Nonexpert summary, general purpose.  the rhetorical extracts into something even more valuable: The added rhetorical context allows for the creation of a new kind of summary. Consider, for instance, the user-oriented and task-tailored summaries shown in Figures 2 and 3. Their composition was guided by ﬁxed building plans for different tasks and different user models, whereby the building blocks are deﬁned as sentences of a speciﬁc rhetorical status. In our example, most textual material is extracted verbatim (additional material is underlined in Figures 2 and 3; the original sentences are given in Figure 5). The ﬁrst example is a short abstract generated for a nonexpert user and for general information; its ﬁrst two sentences give background information about the problem tackled. The second abstract is aimed at an expert; therefore, no background is given, and instead differences between this approach and similar ones are described. The actual construction of these summaries is a complex process involving tasks such as sentence planning, lexical choice and syntactic realization, tasks that are outside the scope of this article. The important point is that it is the knowledge about the rhetorical status of the sentences that enables the tailoring of the summaries according to users’ expertise and task. The rhetorical status allows for other kinds of applications too: Several articles can be summarized together, contrasts or complementarity among 411  Computational Linguistics  Volume 28, Number 4  44 This paper’s goal is to organise a set of linguistic objects such as words according to the contexts in which they occur, for instance grammatical constructions or n-grams. 22 More speciﬁcally: the goal is to classify nouns according to their distribution as direct objects of verbs. 5 Unlike Hindle (1990), 9 this approach constructs word classes and corresponding models of association directly. 14 In comparison to Brown et al. (1992), the method is combinatorially less demanding and does not depend on frequency counts for joint events involving particular words, a potentially unreliable source of information. Figure 3 Expert summary, contrastive links.  articles can be expressed, and summaries can be displayed together with citation links to help users navigate several related papers. The rest of this article is structured as follows: section 2 describes the theoretical and empirical aspects of document structure we model in this article. These aspects include rhetorical status and relatedness: • Rhetorical status in terms of problem solving: What is the goal and contribution of the paper? This type of information is often marked by metadiscourse and by conventional patterns of presentation (cf. section 2.1). • Rhetorical status in terms of intellectual attribution: What information is claimed to be new, and which statements describe other work? This type of information can be recognized by following the “agent structure” of text, that is, by looking at all grammatical subjects occurring in sequence (cf. section 2.2). • Relatedness among articles: What articles is this work similar to, and in what respect? This type of information can be found by examining ﬁxed indicator phrases like in contrast to . . . , section headers, and citations (cf. section 2.3). These aspects of rhetorical status are encoded in an annotation scheme that we present in section 2.4. Annotation of relevance is covered in section 2.5. In section 3, we report on the construction of a gold standard for rhetorical status and relevance and on the measurement of agreement among human annotators. We then describe in section 4 our system that simulates the human annotation. Section 5 presents an overview of the intrinsic evaluation we performed, and section 6 closes with a summary of the contribution of this work, its limitations, and suggestions for future work. 2. Rhetorical Status, Citations, and Relevance It is important for our task to ﬁnd the right deﬁnition of rhetorical status to describe the content in scientiﬁc articles. The deﬁnition should both capture generalizations about the nature of scientiﬁc texts and also provide the right kind of information to enable the construction of better summaries for a practical application. Another requirement is that the analysis should be applicable to research articles from different presentational traditions and subject matters. 412  Teufel and Moens  Summarizing Scientiﬁc Articles  For the development of our scheme, we used the chronologically ﬁrst 80 articles in our corpus of conference articles in computational linguistics (articles presented at COLING, ANLP, and (E)ACL conferences or workshops). Because of the interdisciplinarity of the ﬁeld, the papers in this collection cover a challenging range of subject matters, such as logic programming, statistical language modeling, theoretical semantics, computational dialectology, and computational psycholinguistics. Also, the research methodology and tradition of presentation is very different among these ﬁelds; (computer scientists write very different papers than theoretical linguists). We thus expect our analysis to be equally applicable in a wider range of disciplines and subdisciplines other than those named. 2.1 Rhetorical Status Our model relies on the following dimensions of document structure in scientiﬁc articles. Problem structure. Research is often described as a problem-solving activity (Jordan 1984; Trawinski 1989; Zappen 1983). Three information types can be expected to occur in any research article: problems (research goals), solutions (methods), and results. In many disciplines, particularly the experimental sciences, this problem-solution structure has been crystallized in a ﬁxed presentation of the scientiﬁc material as introduction, method, result and discussion (van Dijk 1980). But many texts in computational linguistics do not adhere to this presentation, and our analysis therefore has to be based on the underlying logical (rhetorical) organization, using textual representation only as an indication. Intellectual attribution. Scientiﬁc texts should make clear what the new contribution is, as opposed to previous work (speciﬁc other researchers’ approaches) and background material (generally accepted statements). We noticed that intellectual attribution has a segmental character. Statements in a segment without any explicit attribution are often interpreted as belonging to the most recent explicit attribution statement (e.g., Other researchers claim that). Our rhetorical scheme assumes that readers have no difﬁculty in understanding intellectual attribution, an assumption that we veriﬁed experimentally. Scientiﬁc argumentation. In contrast to the view of science as a disinterested “fact factory,” researchers like Swales (1990) have long claimed that there is a strong social aspect to science, because the success of a researcher is correlated with her ability to convince the ﬁeld of the quality of her work and the validity of her arguments. Authors construct an argument that Myers (1992) calls the “rhetorical act of the paper”: The statement that their work is a valid contribution to science. Swales breaks down this “rhetorical act” into single, nonhierarchical argumentative moves (i.e., rhetorically coherent pieces of text, which perform the same communicative function). His Constructing a Research Space (CARS) model shows how patterns of these moves can be used to describe the rhetorical structure of introduction sections of physics articles. Importantly, Swales’s moves describe the rhetorical status of a text segment with respect to the overall message of the document, and not with respect to adjacent text segments. Attitude toward other people’s work. We are interested in how authors include reference to other work into their argument. In the ﬂow of the argument, each piece of other work is mentioned for a speciﬁc reason: it is portrayed as a rival approach, as a prior approach with a fault, or as an approach contributing parts of the authors’ own solution. In well-written papers, this relation is often expressed in an explicit way. The next section looks at the stylistic means available to the author to express the connection between previous approaches and their own work.  413  Computational Linguistics  Volume 28, Number 4  2.2 Metadiscourse and Agentivity Explicit metadiscourse is an integral aspect of scientiﬁc argumentation and a way of expressing attitude toward previous work. Examples for metadiscourse are phrases like we argue that and in contrast to common belief, we. Metadiscourse is ubiquitous in scientiﬁc writing: Hyland (1998) found a metadiscourse phrase on average after every 15 words in running text. A large proportion of scientiﬁc metadiscourse is conventionalized, particularly in the experimental sciences, and particularly in the methodology or result section (e.g., we present original work . . . , or An ANOVA analysis revealed a marginal interaction/a main effect of . . . ). Swales (1990) lists many such ﬁxed phrases as co-occurring with the moves of his CARS model (pages 144, 154–158, 160–161). They are useful indicators of overall importance (Pollock and Zamora 1975); they can also be relatively easily recognized with information extraction techniques (e.g., regular expressions). Paice (1990) introduces grammars for pattern matching of indicator phrases, e.g., the aim/purpose of this paper/article/study and we conclude/propose. Apart from this conventionalized metadiscourse, we noticed that our corpus contains a large number of metadiscourse statements that are less formalized: statements about aspects of the problem-solving process or the relation to other work. Figure 4, for instance, shows that there are many ways to say that one’s research is based on somebody else’s (“research continuation”). The sentences do not look similar on the surface: The syntactic subject can be the authors, the originators of the method, or even the method itself. Also, the verbs are very different (base, be related, use, follow). Some sentences use metaphors of change and creation. The wide range of linguistic expression we observed presents a challenge for recognition and correct classiﬁcation using standard information extraction patterns. With respect to agents occurring in scientiﬁc metadiscourse, we make two suggestions: (1) that scientiﬁc argumentation follows prototypical patterns and employs recurrent types of agents and actions and (2) that it is possible to recognize many of these automatically. Agents play ﬁxed roles in the argumentation, and there are so  • We employ Suzuki’s algorithm to learn case frame patterns as dendroid distributions.  (9605013)  • Our method combines similarity-based estimates with Katz’s back-off scheme, which is widely used for language  modeling in speech recognition.  (9405001)  • Thus, we base our model on the work of Clark and Wilkes-Gibbs (1986), and Heeman and Hirst (1992) . . . (9405013)  • The starting point for this work was Scha and Polanyi’s discourse grammar (Scha and Polanyi, 1988; Pruest et  al., 1994).  (9502018)  • We use the framework for the allocation and transfer of control of Whittaker and Stenton (1988).  (9504007)  • Following Laur (1993), we consider simple prepositions (like “in”) as well as prepositional phrases (like “in front  of”).  (9503007)  • Our lexicon is based on a ﬁnite-state transducer lexicon (Karttunen et al., 1992). (9503004) • Instead of . . . we will adopt a simpler, monostratal representation that is more closely related to those found in  dependency grammars (e.g., Hudson (1984)).  (9408014)  Figure 4 Statements expressing research continuation, with source article number.  414  Teufel and Moens  Summarizing Scientiﬁc Articles  few of these roles that they can be enumerated: agents appear as rivals, as contributors of part of the solution (they), as the entire research community in the ﬁeld, or as the authors of the paper themselves (we). Note the similarity of agent roles to the three kinds of intellectual attribution mentioned above. We also propose prototypical actions frequently occurring in scientiﬁc discourse: the ﬁeld might agree, a particular researcher can suggest something, and a certain solution could either fail or be successful. In section 4 we will describe the three features used in our implementation that recognize metadiscourse. Another important construct that expresses relations to other researchers’ work is formal citations, to which we will now turn. 2.3 Citations and Relatedness Citation indexes are constructs that contain pointers between cited texts and citing texts (Garﬁeld 1979), traditionally in printed form. When done on-line (as in CiteSeer [Lawrence, Giles, and Bollacker 1999], or as in Nanba and Okumura’s [1999] work), citations are presented in context for users to browse. Browsing each citation is timeconsuming, but useful: just knowing that an article cites another is often not enough. One needs to read the context of the citation to understand the relation between the articles. Citations may vary in many dimensions; for example, they can be central or perfunctory, positive or negative (i.e., critical); apart from scientiﬁc reasons, there is also a host of social reasons for citing (“politeness, tradition, piety” [Ziman 1969]). We concentrate on two citation contexts that are particularly important for the information needs of researchers:  • Contexts in which an article is cited negatively or contrastively. • Contexts in which an article is cited positively or in which the authors state that their own work originates from the cited work.  A distinction among these contexts would enable us to build more informative citation indexes. We suggest that such a rhetorical distinction can be made manually and automatically for each citation; we use a large corpus of scientiﬁc papers along with humans’ judgments of this distinction to train a system to make such distinctions. 2.4 The Rhetorical Annotation Scheme Our rhetorical annotation scheme (cf. Table 1) encodes the aspects of scientiﬁc argumentation, metadiscourse, and relatedness to other work described before. The categories are assigned to full sentences, but a similar scheme could be developed for clauses or phrases. The annotation scheme is nonoverlapping and nonhierarchical, and each sentence must be assigned to exactly one category. As adjacent sentences of the same status can be considered to form zones of the same rhetorical status, we call the units rhetorical zones. The shortest of these zones are one sentence long. The rhetorical status of a sentence is determined on the basis of the global context of the paper. For instance, whereas the OTHER category describes all neutral descriptions of other researchers’ work, the categories BASIS and CONTRAST are applicable to sentences expressing a research continuation relationship or a contrast to other work. Generally accepted knowledge is classiﬁed as BACKGROUND, whereas the author’s own work is separated into the speciﬁc research goal (AIM) and all other statements about the author’s own work (OWN).  415  Computational Linguistics  Volume 28, Number 4  Table 1 Annotation scheme for rhetorical status.  AIM TEXTUAL OWN BACKGROUND CONTRAST BASIS OTHER  Speciﬁc research goal of the current paper Statements about section structure (Neutral) description of own work presented in current paper: Methodology, results, discussion Generally accepted scientiﬁc background Statements of comparison with or contrast to other work; weaknesses of other work Statements of agreement with other work or continuation of other work (Neutral) description of other researchers’ work  The annotation scheme expresses important discourse and argumentation aspects of scientiﬁc articles, but with its seven categories it is not designed to model the full complexity of scientiﬁc texts. The category OWN, for instance, could be further subdivided into method (solution), results, and further work, which is not done in the work reported here. There is a conﬂict between explanatory power and the simplicity necessary for reliable human and automatic classiﬁcation, and we decided to restrict ourselves to the rhetorical distinctions that are most salient and potentially most useful for several information access applications. The user-tailored summaries and more informative citation indexes we mentioned before are just two such applications; another one is the indexing and previewing of the internal structure of the article. To make such indexing and previewing possible, our scheme contains the additional category TEXTUAL, which captures previews of section structure (section 2 describes our data . . . ). Such previews would make it possible to label sections with the author’s indication of their contents. Our rhetorical analysis, as noted above, is nonhierarchical, in contrast to Rhetorical Structure Theory (RST) (Mann and Thompson 1987; Marcu 1999), and it concerns text pieces at a lower level of granularity. Although we do agree with RST that the structure of text is hierarchical in many cases, it is our belief that the relevance and function of certain text pieces can be determined without analyzing the full hierarchical structure of the text. Another difference between our analysis and that of RST is that our analysis aims at capturing the rhetorical status of a piece of text in respect to the overall message, and not in relation to adjacent pieces of text. 2.5 Relevance As our immediate goal is to select important content from a text, we also need a second set of gold standards that are deﬁned by relevance (as opposed to rhetorical status). Relevance is a difﬁcult issue because it is situational to a unique occasion (Saracevic 1975; Sparck Jones 1990; Mizzaro 1997): Humans perceive relevance differently from each other and differently in different situations. Paice and Jones (1993) report that they abandoned an informal sentence selection experiment in which they used agriculture articles and experts in the ﬁeld as participants, as the participants were too strongly inﬂuenced by their personal research interest. As a result of subjectivity, a number of human sentence extraction experiments over the years have resulted in low agreement ﬁgures. Rath, Resnick, and Savage (1961) report that six participants agreed on only 8% of 20 sentences they were asked to select out of short Scientiﬁc American texts and that ﬁve agreed on 32% of the sentences. They found that after six weeks, subjects selected on average only 55% of the sentences they themselves selected previously. Edmundson and Wyllys (1961) 416  Teufel and Moens  Summarizing Scientiﬁc Articles  ﬁnd similarly low human agreement for research articles. More recent experiments reporting more positive results all used news text (Jing et al. 1998; Zechner 1995). As discussed above, the compression rates on news texts are far lower: there are fewer sentences from which to choose, making it easier to agree on which ones to select. Sentence selection from scientiﬁc texts also requires more background knowledge, thus importing an even higher level of subjectivity into sentence selection experiments. Recently, researchers have been looking for more objective deﬁnitions of relevance. Kupiec, Pedersen, and Chen (1995) deﬁne relevance by abstract similarity: A sentence in a document is considered relevant if it shows a high level of similarity to a sentence in the abstract. This deﬁnition of relevance has the advantage that it is ﬁxed (i.e., the researchers have no inﬂuence over it). It relies, however, on two assumptions: that the writing style is such that there is a high degree of overlap between sentences in the abstract and in the main text and that the abstract is indeed the target output that is most adequate for the ﬁnal task. In our case, neither assumption holds. First, the experiments in Teufel and Moens (1997) showed that in our corpus only 45% of the abstract sentences appear elsewhere in the body of the document (either as a close variant or in identical form), whereas Kupiec, Pedersen, and Chen report a ﬁgure of 79%. We believe that the reason for the difference is that in our case the abstracts were produced by the document authors and by professional abstractors in Kupiec, Pedersen, and Chen’s case. Author summaries tend to be less systematic (Rowley 1982) and more “deep generated,” whereas summaries by professional abstractors follow an internalized building plan (Liddy 1991) and are often created through sentence extraction (Lancaster 1998). Second, and more importantly, the abstracts and improved citation indexes we intend to generate are not modeled on traditional summaries, which do not provide the type of information needed for the applications we have in mind. Information about related work plays an important role in our strategy for summarization and citation indexing, but such information is rarely found in abstracts. We empirically found that the rhetorical status of information occurring in author abstracts is very limited and consists mostly of information about the goal of the paper and speciﬁcs of the solution. Details of the analysis we conducted on this topic are given in section 3.2.2. We thus decided to augment our corpus with an independent set of human judgments of relevance. We wanted to replace the vague deﬁnition of relevance often used in sentence extraction experiments with a more operational deﬁnition based on rhetorical status. For instance, a sentence is considered relevant only if it describes the research goal or states a difference with a rival approach. More details of the instructions we used to make the relevance decisions are given in section 3. Thus, we have two parallel human annotations in our corpus: rhetorical annotation and relevance selection. In both tasks, each sentence in the articles is classiﬁed: Each sentence receives one rhetorical category and also the label irrelevant or relevant. This strategy can create redundant material (e.g., when the same fact is expressed in a sentence in the introduction, a sentence in the conclusion, and one in the middle of the document). But this redundancy also helps mitigate one of the main problems with sentence-based gold standards, namely, the fact that there is no one single best extract for a document. In our annotation, all qualifying sentences in the document are identiﬁed and classiﬁed into the same group, which makes later comparisons with system performance fairer. Also, later steps cannot only ﬁnd redundancy in the intermediate result and remove it, but also use the redundancy as an indication of importance.  417  Computational Linguistics  Volume 28, Number 4  Aim: 10 Our research addresses some of the same questions and uses similar raw data, but we investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves. 22 We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar. 25 The problem we study is how to use the EQN to classify the EQN. 44 In general, we are interested on how to organise a set of linguistic objects such as words according to the contexts in which they occur, for instance grammatical constructions or n-grams. 46 Our problem can be seen as that of learning a joint distribution of pairs from a large sample of pairs. 162 We have demonstrated that a general divisive clustering procedure for probability distributions can be used to group words according to their participation in particular grammatical relations with other words. Background: 0 Methods for automatically classifying words according to their contexts of use have both scientiﬁc and practical interest. 4 The problem is that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. Own (Details of Solution): 66 The ﬁrst stage of an iteration is a maximum likelihood, or minimum distortion, estimation of the cluster centroids given ﬁxed membership probabilities. 140 The evaluation described below was performed on the largest data set we have worked with so far, extracted from 44 million words of 1988 Associated Press newswire with the pattern matching techniques mentioned earlier. 163 The resulting clusters are intuitively informative, and can be used to construct class-based word coocurrence [sic] models with substantial predictive power. Contrast with Other Approaches/Weaknesses of Other Approaches: 9 His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association. 14 Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as we noted above. 41 However, this is not very satisfactory because one of the goals of our work is precisely to avoid the problems of data sparseness by grouping words into classes. Basis (Imported Solutions): 65 The combined entropy maximization entropy [sic] and distortion minimization is carried out by a two-stage iterative process similar to the EM method (Dempster et al., 1977). 113 The analogy with statistical mechanics suggests a deterministic annealing procedure for clus- tering (Rose et al., 1990), in which the number of clusters is determined through a sequence of phase transitions by continuously increasing the parameter EQN following an annealing schedule. 153 The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993). Figure 5 Example of manual annotation: Relevant sentences with rhetorical status. Figure 5 gives an example of the manual annotation. Relevant sentences of all rhetorical categories are shown. Our system creates a list like the one in Figure 5 automatically (Figure 12 shows the actual output of the system when run on the example paper). In the next section, we turn to the manual annotation step and the development of the gold standard used during system training and system evaluation.  418  Teufel and Moens  Summarizing Scientiﬁc Articles  3. Human Judgments: The Gold Standard For any linguistic analysis that requires subjective interpretation and that is therefore not objectively true or false, it is important to show that humans share some intuitions about the analysis. This is typically done by showing that they can apply it independently of each other and that the variation they display is bounded (i.e., not arbitrarily high). The argument is strengthened if the judges are people other than the developers of the analysis, preferably “na¨ıve” subjects (i.e., not computational linguists). Apart from the cognitive validation of our analysis, high agreement is essential if the annotated corpus is to be used as training material for a machine learning process, like the one we describe in section 4. Noisy and unreliably annotated training material will very likely deteriorate the classiﬁcation performance. In inherently subjective tasks, it is also common practice to consider human performance as an upper bound. The theoretically best performance of a system is reached if agreement among a pool of human annotators does not decrease when the system is added to the pool. This is so because an automatic process cannot do any better in this situation than to be indistinguishable from human performance.  3.1 Corpus The annotated development corpus consists of 80 conference articles in computational linguistics (12,188 sentences; 285,934 words). It is part of a larger corpus of 260 articles (1.1 million words) that we collected from the CMP LG archive (CMP LG 1994). The appendix lists the 80 articles (archive numbers, titles and authors) of our development corpus; it consists of the 80 chronologically oldest articles in the larger corpus, containing articles deposited between May 1994 and May 1996 (whereas the entire corpus stretches until 2001). Papers were included if they were presented at one of the following conferences (or associated workshops): the annual meeting of the Association for Computational Linguistics (ACL), the meeting of the European Chapter of the Association for Computational Linguistics (EACL), the conference on Applied Natural Language Processing (ANLP), the International Joint Conference on Artiﬁcial Intelligence (IJCAI), or the International Conference on Computational Linguistics (COLING). As mentioned above, a wide range of different subdomains of the ﬁeld of computational linguistics are covered. We added Extensible Markup Language (XML) markup to the corpus: Titles, authors, conference, date, abstract, sections, headlines, paragraphs, and sentences were marked up. Equations, tables, images were removed and replaced by placeholders. Bibliography lists were marked up and parsed. Citations and occurrences of author names in running text were recognized, and self-citations were recognized and speciﬁcally marked up. (Linguistic) example sentences and example pseudocode were manually marked up, such that clean textual material (i.e., the running text of the article without interruptions) was isolated for automatic processing. The implementation uses the Text Tokenization Toolkit (TTT) software (Grover, Mikheev, and Matheson 1999).  3.2 Annotation of Rhetorical Status The annotation experiment described here (and in Teufel, Carletta, and Moens [1999] in more detail) tests the rhetorical annotation scheme presented in section 2.4.  419  Computational Linguistics  Volume 28, Number 4  3.2.1 Rationale and Experimental Design. Annotators. Three task-trained annotators were used: Annotators A and B have degrees in cognitive science and speech therapy. They were paid for the experiment. Both are well-used to reading scientiﬁc articles for their studies and roughly understand the contents of the articles they annotated because of the closeness of their ﬁelds to computational linguistics. Annotator C is the ﬁrst author. We did not want to declare annotator C the expert annotator; we believe that in subjective tasks like the one described here, there are no real experts. Guidelines. Written guidelines (17 pages) describe the semantics of the categories, ambiguous cases, and decision strategies. The guidelines also include the decision tree reproduced in Figure 6. Training. Annotators received a total of 20 hours of training. Training consisted of the presentation of annotation of six example papers and the annotation of eight training articles under real conditions (i.e., independently). In subsequent training sessions, decision criteria for difﬁcult cases encountered in the training articles were discussed. Obviously, the training articles were excluded from measurements of human agreement. Materials and procedure. Twenty-ﬁve articles were used for annotation. As no annotation tool was available at the time, annotation was performed on paper; the categories were later transferred to the electronic versions of the articles by hand. Skim-reading and annotation typically took between 20 and 30 minutes per article, but there were no time restrictions. No communication between the annotators was allowed during annotation. Six weeks after the initial annotation, annotators were asked to reannotate 6 random articles out of the 25. Evaluation measures. We measured two formal properties of the annotation: stability and reproducibility (Krippendorff 1980). Stability, the extent to which one annotator will produce the same classiﬁcations at different times, is important because an unstable annotation scheme can never be reproducible. Reproducibility, the extent to which different annotators will produce the same classiﬁcations, is important because it measures the consistency of shared understandings (or meaning) held between annotators.  Does this sentence refer to new, current work by the authors (excluding previous work of the authors)?  YES  NO  Does this sentence contain material that describes the specific aim of the paper?  YES  NO  AIM  Does this sentence make explicit reference to the  structure of the paper?  YES  NO  Does the sentence describe general background, including phenomena to be explained or linguistic example sentences?  YES  NO  BACKGROUND  Does it describe a negative aspect of other work, or a contrast or comparison of the own work to it?  YES  NO  TEXTUAL  OWN  CONTRAST  Does this sentence mention other work as basis of or support for the current paper?  YES  NO  Figure 6 Decision tree for rhetorical annotation.  BASIS  OTHER  420  Teufel and Moens  Summarizing Scientiﬁc Articles  We use the kappa coefﬁcient K (Siegel and Castellan 1988) to measure stability and reproducibility, following Carletta (1996). The kappa coefﬁcient is deﬁned as follows:  K  =  P(A) − P(E) 1 − P(E)  where P(A) is pairwise agreement and P(E) random agreement. K varies between 1 when agreement is perfect and −1 when there is a perfect negative correlation. K = 0 is deﬁned as the level of agreement that would be reached by random annotation using the same distribution of categories as the actual annotators did. The main advantage of kappa as an annotation measure is that it factors out random agreement by numbers of categories and by their distribution. As kappa also abstracts over the number of annotators considered, it allows us to compare the agreement numerically among a group of human annotators with the agreement between the system and one or more annotators (section 5), which we use as one of the performance measures of the system.  3.2.2 Results. The annotation experiments show that humans distinguish the seven rhetorical categories with a stability of K = .82, .81, .76 (N = 1,220; k = 2, where K stands for the kappa coefﬁcient, N for the number of items (sentences) annotated, and k for the number of annotators). This is equivalent to 93%, 92%, and 90% agreement. Reproducibility was measured at K = .71 (N = 4,261, k = 3), which is equivalent to 87% agreement. On Krippendorff’s (1980) scale, agreement of K = .8 or above is considered as reliable, agreement of .67–.8 as marginally reliable, and less than .67 as unreliable. On Landis and Koch’s (1977) more forgiving scale, agreement of .0–.2 is considered as showing “slight” correlation, .21–.4 as “fair,” .41–.6 as “moderate,” .61– 0.8 as “substantial,” and .81 –1.0 as “almost perfect.” According to these guidelines, our results can be considered reliable, substantial annotation. Figure 7 shows that the distribution of the seven categories is very skewed, with 67% of all sentences being classiﬁed as OWN. (The distribution was calculated using all three judgments per sentence [cf. the calculation of kappa]. The total number of items is then k · N, i.e., 12,783 in this case.) Table 2 shows a confusion matrix between two annotators. The numbers represent absolute sentence numbers, and the diagonal (boldface numbers) are the counts of sentences that were identically classiﬁed by both annotators. We used Krippendorff’s diagnostics to determine which particular categories humans had most problems with: For each category, agreement is measured with a new data set in which all categories  Figure 7 Distribution of rhetorical categories (entire document). 421  Computational Linguistics  Volume 28, Number 4  Table 2 Confusion matrix between annotators B and C.  ANNOTATOR B  AIM CTR TXT OWN BKG BAS OTH  ANNOTATOR C AIM 35  2  
of summarization: • spoken versus written language • multiparty dialogues versus texts written by one author • unrestricted versus restricted domains • diverse genres versus a single genre The main challenges this work has to address, in addition to the challenges of writtentext summarization, are as follows: • coping with speech disﬂuencies • identifying the units for extraction • maintaining cross-speaker coherence • coping with speech recognition errors We will discuss these challenges in more detail in the following section. Although we have addressed the issue of speech recognition errors in previous related work (Zechner and Waibel 2000b), for the purpose of this article, we exclusively use human transcripts of spoken dialogues. Intrinsic evaluations of text summaries usually use sentences as their basic units. For our data, however, sentence boundaries are typically not available in the ﬁrst place. Thus we devise a word-based evaluation metric derived from an average relevance score from human relevance annotations (section 6.2). The organization of this article is as follows: Section 2 provides the motivation for our research, introducing and discussing the main challenges of spoken-dialogue summarization, followed by a section on related work (section 3). Section 4 describes the corpus we use to develop and evaluate our system, along with the procedures employed for corpus annotation. The system architecture and its components are described in detail in section 5, along with evaluations thereof. Section 6 presents the global evaluation of our approach, before we conclude the article with a discussion of our results, contributions, and directions for future research in this ﬁeld (sections 7 and 8). 2. Motivation Consider the following example from a phone conversation drawn from the English CALLHOME database (LDC 1996). It is a transcript of a conversation between two native speakers of American English; one person is in the New York area (speaker a), the other one (speaker b) in Israel. It was recorded about a month after Yitzhak Rabin’s assassination (1995). This dialogue segment is about one minute of real time. The audio is segmented into speaker turns using silence heuristics,1 and each turn is marked with a turn number and with the speaker label. Noises are removed to increase readability.2  
 Computational Linguistics  Volume 28, Number 4  A second difﬁculty faced in evaluating Barzilay and Elhadad’s proposal is that it is a proposal for the ﬁrst stage of the summarization process, and it is not clear how to evaluate this stage independent of the second stage of summarization. A second contribution of this article is a method for evaluating lexical chains as an intermediate representation. The intuition behind the method is as follows. The (strong) lexical chains in a document are intended to identify important (noun) concepts in the document. Our evaluation requires access to documents that have corresponding humangenerated summaries. We run our lexical chain algorithm both on the document and on the summary and examine (1) how many of the concepts from strong lexical chains in the document also occur in the summary and (2) how many of the (noun) concepts appearing in the summary are represented in strong lexical chains in the document. Essentially, if lexical chains are a good intermediate representation for text summarization, we expect that concepts identiﬁed as important according to the lexical chains will be the concepts found in the summary. Our evaluation of 24 documents with summaries indicates that indeed lexical chains do appear to be a promising avenue of future research in text summarization. 1.1 Description of Lexical Chains The concept of lexical chains was ﬁrst introduced by Morris and Hirst. Basically, lexical chains exploit the cohesion among an arbitrary number of related words (Morris and Hirst 1991). Lexical chains can be computed in a source document by grouping (chaining) sets of words that are semantically related (i.e., have a sense ﬂow). Identities, synonyms, and hypernyms/hyponyms (which together deﬁne a tree of “is a” relations between words) are the relations among words that might cause them to be grouped into the same lexical chain. Speciﬁcally, words may be grouped when: • Two noun instances are identical, and are used in the same sense. (The house on the hill is large. The house is made of wood.) • Two noun instances are used in the same sense (i.e., are synonyms). (The car is fast. My automobile is faster.) • The senses of two noun instances have a hypernym/hyponym relation between them. (John owns a car. It is a Toyota.) • The senses of two noun instances are siblings in the hypernym/hyponyn tree. (The truck is fast. The car is faster.) In computing lexical chains, the noun instances must be grouped according to the above relations, but each noun instance must belong to exactly one lexical chain. There are several difﬁculties in determining which lexical chain a particular word instance should join. For instance, a particular noun instance may correspond to several different word senses, and thus the system must determine which sense to use (e.g., should a particular instance of “house” be interpreted as sense 1, dwelling, or sense 2, legislature). In addition, even if the word sense of an instance can be determined, it may be possible to group that instance into several different lexical chains because it may be related to words in different chains. For example, the word’s sense may be identical to that of a word instance in one grouping while having a hypernym/hyponym relationship with that of a word instance in another. What must happen is that the words must be grouped in such a way that the overall grouping is optimal in that it creates the longest/strongest lexical chains. It is our contention that words are grouped into a single chain when they are “about” the same underlying concept.  488  Silber and McCoy  Efﬁcient Lexical Chains for Summarization  2. Algorithm Deﬁnition We wish to extract lexical chains from a source document using the complete method that Barzilay and Elhadad implemented in exponential time, but to do so in linear time. Barzilay and Elhadad deﬁne an interpretation as a mapping of noun instances to speciﬁc senses, and further, of these senses to speciﬁc lexical chains. Each unique mapping is a particular “way of interpreting” the document, and the collection of all possible mappings deﬁnes all of the interpretations possible. In order to compute lexical chains in linear time, instead of computing every interpretation of a source document as Barzilay and Elhadad did, we create a structure that implicitly stores every interpretation without actually creating it, thus keeping both the space and time usage of the program linear. We then provide a method for ﬁnding that interpretation which is best from within this representation. As was the case with Barzilay and Elhadad, we rely on WordNet1 to provide sense possibilities for, and semantic relations among, word instances in the document. Before we could actually compute the interpretations, one issue we had to tackle was the organization and speed of the WordNet dictionary. In order to provide expedient access to WordNet, we recompiled the noun database into a binary format and memory-mapped it so that it could be accessed as a large array, changing the WordNet sense numbers to match the array indexes. 2.1 Chain Computation Before computation can begin, the system uses a part-of-speech tagger2 to ﬁnd the noun instances within the document. Processing a document involves creating a large array of “metachains,” the size of which is the number of noun senses in WordNet plus the number of nouns in the document, to handle the possibility of words not found in WordNet. (This is the maximum size that could possibly be needed.) A metachain represents all possible chains that can contain the sense whose number is the index of the array. When a noun is encountered, for every sense of the noun in WordNet, the noun sense is placed into every metachain for which it has an identity, synonym, or hyperonym relation with that sense. These metachains represent every possible interpretation of the text. Note that each metachain has an index that is a WordNet sense number, so in a very real way, we can say that a chain has an “overriding sense.” Table 1 shows the structure of such an array, with each row being a metachain based on the sense listed in the ﬁrst column. In each node of a given metachain, appropriate pointers are kept to allow fast lookup. In addition, in association with each word, a list of the metachains to which it belongs is also kept (not shown in table). The second step, ﬁnding the best interpretation, is accomplished by making a second pass through the source document. For each noun, each chain to which the noun belongs is examined and a determination is made, based on the type of relation and distance factors, as to which metachain the noun contributes to most. In the event of a tie, the higher sense number is used, since WordNet is organized with more speciﬁc concepts indexed with higher numbers. The noun is then deleted from all other metachains. Once all of the nouns have been processed, what is left is the interpretation whose score is maximum. From this interpretation, the best (highest-scoring) chains can be selected. The algorithm in its entirety is outlined in Table 2.  
2. Revision Operations We analyzed a set of articles to observe how they were summarized by human abstractors. This set included 15 news articles on telecommunications, 5 articles on medical issues, and 10 articles in the legal domain. Although individual articles related to speciﬁc domains, they covered a broad range of topics and differed in writing style and structure even within the same domain. The telecommunications articles were collected using the free daily news service Communications-Related Headlines provided by the Benton Foundation http://www.benton.org . The abstracts of these articles from various newspapers were written by staff writers at Benton. The medical news articles were collected from HIV/STD/TB Prevention News Update, provided by the Center for Disease Control (CDC) http://www.cdcnpin.org/news/prevnews.htm . As a public service, CDC provides daily staff-written synopses of key scientiﬁc articles and lay media reports on HIV/AIDS. The legal articles from the New York Law Journal describe court decisions on lawsuits that have been summarized by the journal’s editors. From the corpus studied, we found that human abstractors almost universally reuse text in the original document for producing a summary of that document. This ﬁnding is consistent with Endres-Niggemeyer et al. (1998), which stated that professional abstractors often rely on cutting and pasting the original text to produce summaries. Based on careful analysis of human-written summaries, we have deﬁned six revision operations that can be used to transform a sentence in an article into a summary sentence in a human-written abstract: sentence reduction, sentence combination, syntactic transformation, lexical paraphrasing, generalization or speciﬁcation, and reordering. The following sections examine each of these operations in turn. 1. Sentence reduction. In sentence reduction, nonessential phrases are removed from a sentence, as in the following example (italics in the source sentence mark material that is removed):1 Document sentence: When it arrives sometime next year in new TV sets, the V-chip will give parents a new and potentially revolu-  
 Computational Linguistics  Volume 28, Number 4  2. Evidence for Human Lexical Variation  2.1 Previous Research Linguists have acknowledged that people may associate different meanings with the same word. Nunberg (1978, page 81), for example, writes:  There is considerable variation among speakers in beliefs about what does and does not constitute a member of the category. . . . Take jazz. I may believe that the category includes ragtime, but not blues; you may believe the exact opposite. After all, we will have been exposed to a very different set of exemplars. And absent a commonly accepted authority, we must construct our own theories of categories, most probably in the light of varying degrees of musical sophistication.  Many modern theories of mental categorization (Rosch 1978; Smith and Medin 1981)  assume that mental categories are represented by prototypes or exemplars. Therefore,  if different people are exposed to different category prototypes and exemplars, they  are likely to have different rules for evaluating category membership.  Parikh (1994) made a similar point and backed it up with some simple experimen-  tation. For example, he showed squares from the Munsell chart to participants and  asked them to characterize the squares as red or blue; different individuals character-  ized the squares in different ways. In another experiment he showed that differences  remained even if participants were allowed to associate fuzzy-logic-type truth values  with statements.  In the psychological community, Malt et al. (1999) investigated what names par-  ticipants gave to real-world objects. For example, these researchers wished to know  whether participants would describe a pump-top hand lotion dispenser as a bottle or a  container. They were primarily interested in variations across linguistic communities,  but they also discovered that even within a linguistic community there were differ-  ences in how participants named objects. They state (page 242) that only 2 of the 60  objects in their study were given the same name by all of their 76 native-English-  speaker participants.  In the lexicographic community, ﬁeld-workers for the Dictionary of American Re-  gional English (DARE) (Cassidy and Hall 1996) asked a representative set of Americans  to respond to ﬁll-in-the-blank questionnaires. The responses they received revealed  substantial differences among participants. For example, there were 228 different re-  sponses to question B12, When the wind begins to increase, you say it’s  , the most  common of which were getting windy and blowing up; and 201 different responses to  question B13, When the wind begins to decrease, you say it’s  , the most common of  which were calming down and dying down.  2.2 SUMTIME Project The SUMTIME project at the University of Aberdeen is researching techniques for generating summaries of time-series data.1 Much of the project focuses on content determination (see, for example, Sripada et al. [2001]), but it is also examining lexical choice algorithms for time-series summaries, which is where the work described in this article originated. To date, SUMTIME has primarily focused on two domains, weather forecasts and summaries of gas turbine sensors, although we have recently started  
c 2002 Association for Computational Linguistics  Computational Linguistics  Volume 28, Number 3  far away as it once was, thanks to the development of large semantic databases such as WordNet (Fellbaum 1998) and progress in domain-independent machine learning algorithms. Current information extraction and dialogue understanding systems, however, are still based on domain-speciﬁc frame-and-slot templates. Systems for booking airplane information use domain-speciﬁc frames with slots like orig city, dest city, or depart time (Stallard 2000). Systems for studying mergers and acquisitions use slots like products, relationship, joint venture company, and amount (Hobbs et al. 1997). For natural language understanding tasks to proceed beyond these speciﬁc domains, we need semantic frames and semantic understanding systems that do not require a new set of slots for each new application domain. In this article we describe a shallow semantic interpreter based on semantic roles that are less domain speciﬁc than to airport or joint venture company. These roles are deﬁned at the level of semantic frames of the type introduced by Fillmore (1976), which describe abstract actions or relationships, along with their participants. For example, the Judgement frame contains roles like judge, evaluee, and reason, and the Statement frame contains roles like speaker, addressee, and message, as the following examples show: (1) [Judge She ] blames [Evaluee the Government ] [Reason for failing to do enough to help ] . (2) [Message “I’ll knock on your door at quarter to six” ] [Speaker Susan] said. These shallow semantic roles could play an important role in information extraction. For example, a semantic role parse would allow a system to realize that the ruling that is the direct object of change in (3) plays the same Theme role as the ruling that is the subject of change in (4): (3) The canvassing board changed its ruling on Wednesday. (4) The ruling changed because of the protests. The fact that semantic roles are deﬁned at the frame level means, for example, that the verbs send and receive would share the semantic roles (sender, recipient, goods, etc.) deﬁned with respect to a common Transfer frame. Such common frames might allow a question-answering system to take a question like (5) and discover that (6) is relevant in constructing an answer to the question: (5) Which party sent absentee ballots to voters? (6) Both Democratic and Republican voters received absentee ballots from their party. This shallow semantic level of interpretation has additional uses outside of generalizing information extraction, question answering, and semantic dialogue systems. One such application is in word sense disambiguation, where the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the different syntactic subcategorization frames of a verb such as serve can be used to help disambiguate a particular instance of the word. Adding semantic role subcategorization information to this syntactic information could extend this idea to  246  Gildea and Jurafsky  Automatic Labeling of Semantic Roles  use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging ﬁeld of text data mining (TDM) (Hearst 1999). Finally, incorporating semantic roles into probabilistic models of language may eventually yield more accurate parsers and better language models for speech recognition. This article describes an algorithm for identifying the semantic roles ﬁlled by constituents in a sentence. We apply statistical techniques that have been successful for the related problems of syntactic parsing, part-of-speech tagging, and word sense disambiguation, including probabilistic parsing and statistical classiﬁcation. Our statistical algorithms are trained on a hand-labeled data set: the FrameNet database (Baker, Fillmore, and Lowe 1998; Johnson et al. 2001). The FrameNet database deﬁnes a tag set of semantic roles called frame elements and included, at the time of our experiments, roughly 50,000 sentences from the British National Corpus hand-labeled with these frame elements. This article presents our system in stages, beginning in Section 2 with a more detailed description of the data and the set of frame elements or semantic roles used. We then introduce (in Section 3) the statistical classiﬁcation technique used and examine in turn the knowledge sources of which our system makes use. Section 4 describes the basic syntactic and lexical features used by our system, which are derived from a Penn Treebank–style parse of individual sentences to be analyzed. We break our task into two subproblems: ﬁnding the relevant sentence constituents (deferred until Section 5), and giving them the correct semantic labels (Sections 4.2 and 4.3). Section 6 adds higher-level semantic knowledge to the system, attempting to model the selectional restrictions on role ﬁllers not directly captured by lexical statistics. We compare hand-built and automatically derived resources for providing this information. Section 7 examines techniques for adding knowledge about systematic alternations in verb argument structure with sentence-level features. We combine syntactic parsing and semantic role identiﬁcation into a single probability model in Section 8. Section 9 addresses the question of generalizing statistics from one target predicate to another, beginning with a look at domain-independent thematic roles in Section 9.1. Finally we draw conclusions and discuss future directions in Section 10. 2. Semantic Roles Semantic roles are one of the oldest classes of constructs in linguistic theory, dating back thousands of years to Panini’s ka¯raka theory (Misra 1966; Rocher 1964; Dahiya 1995). Longevity, in this case, begets variety, and the literature records scores of proposals for sets of semantic roles. These sets of roles range from the very speciﬁc to the very general, and many have been used in computational implementations of one type or another. At the speciﬁc end of the spectrum are domain-speciﬁc roles such as the from airport, to airport, or depart time discussed above, or verb-speciﬁc roles such as eater and eaten for the verb eat. The opposite end of the spectrum consists of theories with only two “proto-roles” or “macroroles”: Proto-Agent and Proto-Patient (Van Valin 1993; Dowty 1991). In between lie many theories with approximately 10 roles, such as Fillmore’s (1971) list of nine: Agent, Experiencer, Instrument, Object, Source, Goal, Location, Time, and Path.1  
Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting. Segmenting text into sentences is an important aspect in developing many applications: syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. Sentence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary. In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary. Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD problem can be found in Palmer and Hearst (1997). The disambiguation of capitalized words and sentence boundaries presents a chicken-and-egg problem. If we know that a capitalized word that follows a period is a common word, we can safely assign such period as sentence terminal. On the other hand, if we know that a period is not sentence terminal, then we can conclude that the following capitalized word is a proper name. Another frequent source of ambiguity in end-of-sentence marking is introduced by abbreviations: if we know that the word that precedes a period is not an abbreviation, then almost certainly this period denotes a sentence boundary. If, however, this word is an abbreviation, then it is not that easy to make a clear decision. This problem is exacerbated by the fact that abbreviations do not form a closed set; that is, one cannot list all possible abbreviations. Moreover, abbreviations can coincide with regular words; for example, “in” can denote an abbreviation for “inches,” “no” can denote an abbreviation for “number,” and “bus” can denote an abbreviation for “business.” In this article we present a method that tackles sentence boundaries, capitalized words, and abbreviations in a uniform way through a document-centered approach. As opposed to the two dominant techniques of computing statistics about the words that surround potential sentence boundaries or writing specialized grammars, our ap-  
 (2) a. Dodge was robbed by an ex-convict. b. The ex-convict tied him up c. because he wasn’t cooperating. d. Then he took all the money and ran. 
 c 2002 Association for Computational Linguistics  Computational Linguistics c. music box d. steam iron e. pine tree f. night ﬂight g. pet spray h. peanut butter i. abortion problem  make use be in for from about  Volume 28, Number 3  (2) a. parental refusal b. cardiac massage c. heart massage d. sound synthesizer  subj obj obj obj  (3) a. child behavior b. car lover c. soccer competition  subj obj at|in  (4) a. government promotion subj|obj  b. satellite observation  subj|obj  Besides Levi (1978), a fair number of researchers (Warren 1978; Finin 1980; Isabelle 1984; Leonard 1984) agree that there is a limited number of regularly recurring relations between a compound head and its modiﬁer. There is far less agreement when it comes to the type and number of these relations. The relations vary from Levi’s (1978) recoverably deletable predicates to Warren’s (1978) paraphrases and Finin’s (1980) role nominals. Leonard (1984) proposes eight relations, and Warren (1978) proposes six basic relations, whereas the number of relations proposed by Finin (1980) is potentially inﬁnite. The attempt to restrict the semantic relations between the compound head and its modiﬁer to a prespeciﬁed number and type has been criticized by Downing (1977), who has shown (through a series of psycholinguistic experiments) that the underlying relations can be inﬂuenced by a variety of pragmatic factors and cannot therefore be presumed to be easily enumerable. Sparck Jones (1983, page 4) further notes “that observations about the semantic relation holding between the compound head and its modiﬁer can only be remarks about tendencies and not about absolutes.” Consider, for instance, the compound onion tears (see (1a)). The relationship cause is one of the possible interpretations the compound may receive. One could easily imagine a context in which the tears are for or about the onion. Consider example1 (5a), taken from Downing (1977, page 818). Here apple-juice seat refers to the situation in which someone is instructed to sit in a seat in front of which a glass of apple juice has been placed. Given this particular state of affairs, none of the relations in (1) can be used to successfully interpret apple-juice seat. Such considerations have led Selkirk (1982) to  
 Computational Linguistics  Volume 28, Number 3  and Takahashi 1975), lexicalized (Schabes, Abeille´, and Joshi 1988) and augmented by uniﬁcation-based feature structures (Vijay-Shanker and Joshi 1991). Tree-adjoining languages fall into the class of mildly context-sensitive languages and as such are more powerful than context-free languages. The TAG formalism in general, and lexicalized TAGs in particular, are well-suited for linguistic applications. As ﬁrst shown by Joshi (1985) and Kroch and Joshi (1987), the properties of TAGs permit one to encapsulate diverse syntactic phenomena in a very natural way. The XTAG grammar development system makes limited use of feature structures that can be attached to nodes in the trees that make up a grammar. Typically, feature structures in XTAG are ﬂat: nesting of structures is very limited. Furthermore, all feature structures in XTAG are ﬁnitely bounded: the maximum size of a feature structure can be statically determined. During parsing, feature structures undergo uniﬁcation as the trees they are associated with are combined. But uniﬁcation in XTAG is actually highly limited: since all feature structures are bounded, uniﬁcation can be viewed as an atomic operation. Although the method we propose was tested on an XTAG grammar, it is applicable in principle to any linguistic formalism that uses untyped feature structures, in particular, to lexical-functional grammar (Kaplan and Bresnan 1982). 2. The Problem XTAG is organized such that feature structures are speciﬁed in three different components of the grammar: a Tree database deﬁnes feature structures attached to tree families; a Syn database deﬁnes feature structures attached to lexically anchored trees; and a Morph database deﬁnes feature structures attached to (possibly inﬂected) lexical entries. As an example, consider the verb seems. This verb can anchor several trees, among which are trees of auxiliary verbs, such as the tree βVvx , depicted in Figure 1. This tree, which is common to all auxiliary verbs, is associated with the feature structure descriptions listed in Figure 1 (independently of the word that happens to anchor it).1 When the tree βVvx is anchored by seems, the lexicon speciﬁes additional constraints on the feature structures in this tree: seem betaVvx VP.b:<mode> = inf/nom, V.b:<mainv> = + Finally, since “seems” is an inﬂected form, the morphological database speciﬁes more constraints on the node that this word instantiates, as shown in Figure 2. The actual feature structures that are associated with the lexicalized tree anchored by “seems” are the combination of the three sets of path equations. This organization leaves room for several kinds of errors, inconsistencies, and typos in feature structure manipulation. Nothing in the system can eliminate the following possible errors: Undeﬁned features: Every grammar makes use of a ﬁnite set of features in the feature structure speciﬁcation. As the features do not have to be declared, however, certain bogus features can be introduced unintentionally, either through typos or because of poor maintenance. In a grammar that has an assign-case feature, the following statement is probably erroneous: V.b:<asign-case> = acc.  
† Department of Computer Science, University of Toronto, Ontario, Canada M5S 3G4. E-mail: gh@cs.toronto.edu. c 2002 Association for Computational Linguistics  Computational Linguistics  Volume 28, Number 2  different translations? The system could choose error, mistake, blunder, slip, lapse, boner, faux pas, boo-boo, and so on, but the most appropriate choice is a function of how be´vue is used (in context) and of the difference in meaning between be´vue and each of the English possibilities. Not only must the system determine the nuances that be´vue conveys in the particular context in which it has been used, but it must also ﬁnd the English word (or words) that most closely convey the same nuances in the context of the other words that it is choosing concurrently. An exact translation is probably impossible, for be´vue is in all likelihood as different from each of its possible translations as they are from each other. That is, in general, every translation possibility will omit some nuance or express some other possibly unwanted nuance. Thus, faithful translation requires a sophisticated lexical-choice process that can determine which of the near-synonyms provided by one language for a word in another language is the closest or most appropriate in any particular situation. More generally, a truly articulate natural language generation (NLG) system also requires a sophisticated lexical-choice process. The system must to be able to reason about the potential effects of every available option. Consider, too, the possibility of a new type of thesaurus for a word processor that, instead of merely presenting the writer with a list of similar words, actually assists the writer by ranking the options according to their appropriateness in context and in meeting general preferences set by the writer. Such an intelligent thesaurus would greatly beneﬁt many writers and would be a deﬁnite improvement over the simplistic thesauri in current word processors. What is needed is a comprehensive computational model of ﬁne-grained lexical knowledge. Yet although synonymy is one of the fundamental linguistic phenomena that inﬂuence the structure of the lexicon, it has been given far less attention in linguistics, psychology, lexicography, semantics, and computational linguistics than the equally fundamental and much-studied polysemy. Whatever the reasons—philosophy, practicality, or expedience—synonymy has often been thought of as a “non-problem”: either there are synonyms, but they are completely identical in meaning and hence easy to deal with, or there are no synonyms, in which case each word can be handled like any other. But our investigation of near-synonymy shows that it is just as complex a phenomenon as polysemy and that it inherently affects the structure of lexical knowledge. The goal of our research has been to develop a computational model of lexical knowledge that can adequately account for near-synonymy and to deploy such a model in a computational process that could “choose the right word” in any situation of language production. Upon surveying current machine translation and natural language generation systems, we found none that performed this kind of genuine lexical choice. Although major advances have been made in knowledge-based models of the lexicon, present systems are concerned more with structural paraphrasing and a level of semantics allied to syntactic structure. None captures the ﬁne-grained meanings of, and differences between, near-synonyms, nor the myriad of criteria involved in lexical choice. Indeed, the theories of lexical semantics upon which presentday systems are based don’t even account for indirect, fuzzy, or context-dependent meanings, let alone near-synonymy. And frustratingly, no one yet knows how to implement the theories that do more accurately predict the nature of word meaning (for instance, those in cognitive linguistics) in a computational system (see Hirst [1995]). In this article, we present a new model of lexical knowledge that explicitly accounts for near-synonymy in a computationally implementable manner. The clustered model of lexical knowledge clusters each set of near-synonyms under a common, coarse-  106  Edmonds and Hirst  Near-Synonymy and Lexical Choice  grained meaning and provides a mechanism for representing ﬁner-grained aspects of denotation, attitude, style, and usage that differentiate the near-synonyms in a cluster. We also present a robust, efﬁcient, and ﬂexible lexical-choice algorithm based on the approximate matching of lexical representations to input representations. The model and algorithm are implemented in a sentence-planning system called I-Saurus, and we give some examples of its operation. 2. Near-Synonymy 2.1 Absolute and Near-Synonymy Absolute synonymy, if it exists at all, is quite rare. Absolute synonyms would be able to be substituted one for the other in any context in which their common sense is denoted with no change to truth value, communicative effect, or “meaning” (however “meaning” is deﬁned). Philosophers such as Quine (1951) and Goodman (1952) argue that true synonymy is impossible, because it is impossible to deﬁne, and so, perhaps unintentionally, dismiss all other forms of synonymy. Even if absolute synonymy were possible, pragmatic and empirical arguments show that it would be very rare. Cruse (1986, page 270) says that “natural languages abhor absolute synonyms just as nature abhors a vacuum,” because the meanings of words are constantly changing. More formally, Clark (1992) employs her principle of contrast, that “every two forms contrast in meaning,” to show that language works to eliminate absolute synonyms. Either an absolute synonym would fall into disuse or it would take on a new nuance of meaning. At best, absolute synonymy is limited mostly to dialectal variation and technical terms (underwear (AmE) : pants (BrE); groundhog : woodchuck; distichous : two-ranked; plesionym : near-synonym), but even these words would change the style of an utterance when intersubstituted. Usually, words that are close in meaning are near-synonyms (or plesionyms)1— almost synonyms, but not quite; very similar, but not identical, in meaning; not fully intersubstitutable, but instead varying in their shades of denotation, connotation, implicature, emphasis, or register (DiMarco, Hirst, and Stede 1993).2 Section 4 gives a more formal deﬁnition. Indeed, near-synonyms are pervasive in language; examples are easy to ﬁnd. Lie, falsehood, untruth, ﬁb, and misrepresentation, for instance, are near-synonyms of one another. All denote a statement that does not conform to the truth, but they differ from one another in ﬁne aspects of their denotation. A lie is a deliberate attempt to deceive that is a ﬂat contradiction of the truth, whereas a misrepresentation may be more indirect, as by misplacement of emphasis, an untruth might be told merely out of ignorance, and a ﬁb is deliberate but relatively trivial, possibly told to save one’s own or another’s face (Gove 1984). The words also differ stylistically; ﬁb is an informal, childish term, whereas falsehood is quite formal, and untruth can be used euphemistically to avoid some of the derogatory implications of some of the other terms (Gove [1984]; compare Coleman and Kay’s [1981] rather different analysis). We will give many more examples in the discussion below.  
(2) a.  -ing  Go¨del number  b. Go¨del number -ing  If the problem were conﬁned to derivational morphology, we could avoid it by making derivational morphology part of the lexicon that does not interact with grammar. But this is not the case. Mismatches in morphosyntactic and semantic bracketing  ∗ Computer Engineering and Cognitive Science, Middle East Technical University, 06531 Ankara, Turkey. E-mail: bozsahin@metu.edu.tr. c 2002 Association for Computational Linguistics  Computational Linguistics  Volume 28, Number 2  also abound. This article addresses such problems and their resolution in a computational system.1 Mu¨ ller (1999, page 401) exempliﬁes the scope problem in German preﬁxes. (3a) is in conﬂict with the bracketing required for the semantics of the conjunct (3b).  (3) a. Wenn [ Ihr Lust ] und [ noch nichts anderes vor- ]habt, if you pleasure and yet nothing else intend  ko¨nnen wir sie ja  vom Flughafen abholen  can we them PARTICLE from.the airport pick up  ’If you feel like it and have nothing else planned, we can pick them up at the airport.’  b. Ihr Lust habt UND noch nichts anderes vorhabt  Similar problems can be observed in Turkish inﬂectional sufﬁxes. In the coordination of tensed clauses, the tense attaches to the verb of the rightmost conjunct (4a) but applies to all conjuncts (4b). Delayed afﬁxation appears to apply to all nominal inﬂections (4c–e).  (4) a. Zorunlu deprem sigortası [ yu¨ ru¨ rlu¨ g˘e girmis¸ ] ama mandatory earthquake insurance effect enter-ASP but  [ tam anlamıyla uygulanamamıs¸ ]-tı  exactly  apply-NEG-ASP-TENSE  ’Mandatory earthquake insurance had gone into effect, but it had not been enforced properly.’  b. yu¨ ru¨ rlu¨ g˘e girmis¸-ti ama tam anlamıyla uygulanamamıs¸-tı  c. Adam-ın [ araba ve ev ]-i man-GEN car and house-POSS ’the man’s house and car’  d. Araba-yı [ adam ve c¸ocuk ]-lar-a go¨ster-di-m Car-ACC man and child-PLU-DAT show-TENSE-PERS1 ’(I) showed the car to the men and the children.’  e. Araba-yı sen-in [ dost ve tanıdık ]-lar-ın-a  go¨ster-di-m  Car-ACC you-GEN friend and acq.-PLU-POSS-DAT showed  ’(I) showed the car to the your friends and acquaintances.’  
 Computational Linguistics  Volume 28, Number 2  to exploit the fact that senses can be grouped into classes consisting of semantically similar senses. The assumption underlying this approach is that the probability of a particular noun sense can be approximated by a probability based on a suitably chosen class. For example, it seems reasonable to suppose that the probability of (the food sense of) chicken appearing as an object of the verb eat can be approximated in some way by a probability based on a class such as FOOD. There are two elements involved in the problem of using a class to estimate the probability of a noun sense. First, given a suitably chosen class, how can that class be used to estimate the probability of the sense? And second, given a particular noun sense, how can a suitable class be determined? This article offers novel solutions to both problems, and there is a particular focus on the second question, which can be thought of as how to ﬁnd a suitable level of generalization in the hierarchy.1 The semantic hierarchy used here is the noun hierarchy of WordNet (Fellbaum 1998), version 1.6. Previous work has considered how to estimate probabilities using classes from WordNet in the context of acquiring selectional preferences (Resnik 1998; Ribas 1995; Li and Abe 1998; McCarthy 2000), and this previous work has also addressed the question of how to determine a suitable level of generalization in the hierarchy. Li and Abe use the minimum description length principle to obtain a level of generalization, and Resnik uses a simple technique based on a statistical measure of selectional preference. (The work by Ribas builds on that by Resnik, and the work by McCarthy builds on that by Li and Abe.) We compare our estimation method with those of Resnik and Li and Abe, using a pseudo-disambiguation task. Our method outperforms these alternatives on the pseudo-disambiguation task, and an analysis of the results shows that the generalization methods of Resnik and Li and Abe appear to be overgeneralizing, at least for this task. Note that the problem being addressed here is the engineering problem of estimating predicate argument probabilities, with the aim of producing estimates that will be useful for NLP applications. In particular, we are not addressing the problem of acquiring selectional restrictions in the way this is usually construed (Resnik 1993; Ribas 1995; McCarthy 1997; Li and Abe 1998; Wagner 2000). The purpose of using a semantic hierarchy for generalization is to overcome the sparse data problem, rather than ﬁnd a level of abstraction that best represents the selectional restrictions of some predicate. This point is considered further in Section 5. The next section describes the noun hierarchy from WordNet and gives a more precise description of the probabilities to be estimated. Section 3 shows how a class from WordNet can be used to estimate the probability of a noun sense. Section 4 shows how a chi-square test is used as part of the generalization procedure, and Section 5 describes the generalization procedure. Section 6 describes the alternative class-based estimation methods used in the pseudo-disambiguation experiments, and Section 7 presents those experiments. 2. The Semantic Hierarchy The noun hierarchy of WordNet consists of senses, or what Miller (1998) calls lexicalized concepts, organized according to the “is-a-kind-of” relation. Note that we are using concept to refer to a lexicalized concept or sense and not to a set of senses; we use class to refer to a set of senses. There are around 66,000 different concepts in the noun hierarchy  
Computational Linguistics  Volume 28, Number 2  This article has the following parts. In Section 2, we give some necessary mathematical preliminaries. The minimal automata resulting from adding or removing a string are described in detail in Section 3; the algorithms are described in Section 4. In Section 5, one addition and one removal example are explained in detail, and some closing remarks are given in Section 6.  2. Mathematical Preliminaries  2.1 Finite-State Automata and Languages As in Daciuk et al. (2000), we will deﬁne a deterministic ﬁnite-state automaton as M = (Q, Σ, δ, q0, F), where Q is a ﬁnite set of states, q0 ∈ Q is the start state, F ⊆ Q is a set of accepting states, Σ is a ﬁnite set of symbols called the alphabet, and δ: Q × Σ → Q is the next-state mapping. In this article, we will deﬁne δ as a total mapping; the corresponding ﬁnite-state automaton will be called complete (Revuz 2000). This involves no loss of generality, as any ﬁnite-state automaton may be made complete by adding a new absorption state ⊥ to Q, so that all undeﬁned transitions point to it and δ(⊥, a) = ⊥ for all a ∈ Σ. Using complete ﬁnite-state automata is convenient for the theoretical discussion presented in this article; real implementations of automata and the corresponding algorithms need not contain an explicit representation of the absorption state and its incoming and outgoing transitions. For complete ﬁnite-state automata, the extended mapping δ∗: Q × Σ∗ → Q (the extension of δ for strings) is deﬁned simply as  δ∗(q, ) = q  δ∗(q, ax ) = δ∗(δ(q, a), x)  (1)  for all a ∈ Σ and x ∈ Σ∗, with the empty or null string. The language accepted by  automaton M  L(M) = {w ∈ Σ∗: δ∗(q0, w) ∈ F}  (2)  and the right language of state q  L(q) = {x ∈ Σ∗: δ∗(q, x) ∈ F}  (3)  are deﬁned as in Daciuk et al. (2000).  2.2 Single-String Automaton We also ﬁnd it convenient to deﬁne the (complete) single-string automaton for string w, denoted Mw = (Qw, Σ, δw, q0w, Fw), such that L(Mw) = {w}. This automaton has Qw = Pr(w) ∪ {⊥w}, where Pr(w) is the set of all preﬁxes of w and ⊥w is the absorption state, Fw = {w}, and q0w = (note that nonabsorption states in Qw will be named after the corresponding preﬁx of w). The next-state function is deﬁned as follows  xa if x , xa ∈ Pr(w)  δ(x, a) =  (4)  ⊥w otherwise  Note that the single-string automaton for a string w has |Qw| = |w| + 2 states. 2.3 Operations with Finite-State Automata 2.3.1 Intersection Automaton. Given two ﬁnite-state automata M1 and M2, it is easy to build an automaton M so that L(M) = L(M1) ∩ L(M2). This construction is found  208  Carrasco and Forcada  Incremental Construction of Minimal FSA  in formal language theory textbooks (Hopcroft and Ullman 1979, page 59) and is referred to as standard in papers (Karakostas, Viglas, and Lipton 2000). The (complete) intersection automaton has Q = Q1 × Q2, q0 = (q01, q02), F = F1 × F2, and δ((q1, q2), a) = (δ1(q1, a), δ2(q2, a)) for all a ∈ Σ, q1 ∈ Q1 and q2 ∈ Q2. 2.3.2 Complementary Automaton. Given a complete ﬁnite-state automaton M, it is easy to build its complementary automaton M¯ so that L(M¯ ) = Σ∗ − L(M); the only change is the set of ﬁnal states: F¯ = Q − F (Hopcroft and Ullman 1979, page 59). In particular, the complementary single-string automaton M−w accepting Σ∗ − {w} is identical to Mw except that F−w = Q − {w}. 2.3.3 Union Automaton. The above constructions may be combined easily to obtain a construction to build, from two complete automata M1 and M2, the (complete) union automaton M such that L(M) = L(M1) ∪ L(M2). It sufﬁces to consider that, for any two languages on Σ∗, L1 ∪ L2 = Σ∗ − (Σ∗ − L1) ∩ (Σ∗ − L2). The resulting automaton M is identical to the intersection automaton deﬁned above except that F = (F1 × Q2) ∪ (Q1 × F2). 3. Adding and Removing a String 3.1 Adding a String Given a (possibly cyclic) minimal complete ﬁnite-state automaton M, it is easy to build a new complete automaton M accepting L(M ) = L(M) ∪ {w} by applying the union construct deﬁned above to M and the single-string automaton Mw. The resulting automaton M = (Q , Σ, δ , q0, F ), which may be minimized very easily (see below), has four kinds of states in Q : • States of the form (q, ⊥w) with q ∈ Q − {⊥}, equivalent to those nonabsorption states of M that are not reached by any preﬁx of w; they will be called intact states, because they have the same transition structure as their counterparts in M (that is, if δ(q, a) = q , then δ ((q, ⊥w), a) = (q , ⊥w)) and belong to F if q ∈ F. As a result, they have exactly the same right languages, L((q, ⊥w)) = L(q), because all of their outgoing transitions go to other intact states. Furthermore, each state (q, ⊥w) has a different right language; therefore, no two intact states will ever be merged into one by minimization (intact states may, however, be eliminated, if they become unreachable, as we will describe below). For large automata (dictionaries) M, these are the great majority of states (the number of intact states ranges between |Q| − |w| − 1 and |Q|); therefore, it will be convenient in practice to consider M as a modiﬁed version of M, and it will be treated as such in the algorithms found in this article. • States of the form (q, x) with q ∈ Q − {⊥} and x ∈ Pr(w), such that δ∗(q0, x) = q; they will be called cloned states, inspired by the terminology in Daciuk et al. (2000); the remaining states in (Q − {⊥}) × Pr(w)—the great majority of states in Q × Qw—may safely be discarded because they are unreachable from the new start state q0 = (q0, ), which itself is a cloned state. Cloned states are modiﬁed versions of the original states q ∈ Q − {⊥}: All of their outgoing transitions point to the corresponding intact states in Q , that is, (δ(q, a), ⊥w), except for the transition with symbol a : xa ∈ Pr(w), which  209  Computational Linguistics  Volume 28, Number 2  now points to the corresponding cloned state (δ(q, a), xa), that is,  (δ(q, a), xa) if xa ∈ Pr(w)  δ ((q, x), a) =  (5)  (δ(q, a), ⊥w) otherwise  Cloned states are in F if the corresponding original states are in F; in addition, if there is a cloned state of the form (q, w), then it is in F . There are at most |w| + 1 cloned states. • States of the form (⊥, x), with x ∈ Pr(w). These states will be called queue states; states of this form appear when the string w is not in L(M) (the pertinent case, because we are adding it) and only if in the original automaton δ∗(q0, x) = ⊥ for some x ∈ Pr(w). Only the ﬁnal queue state (⊥, w)—if it exists—is in F . There are at most |w| queue states. • The new absorption state ⊥ = (⊥, ⊥w) ∈/ F.  This automaton has to be minimized; because of the nature of the construction algorithm, however, minimization may be accomplished in a small number of operations. It is not difﬁcult to show that minimization may be performed by initializing a list R called the register (Daciuk et al. 2000) with all of the intact states and then testing, one by one, queue and cloned states (starting with the last queue state (⊥, w) or, if it does not exist, the last clone state (q, w), and descending in Pr(w)) against states in the register and adding them to the register if they are not found to be equivalent to a state in R. (Performing this check backwards avoids having to test the equivalence of states by visiting their descendants recursively: see the end of Section 4.1.) Minimization (including the elimination of unreachable states in M ) appears in Section 4 as part of the string addition and removal algorithms.  3.2 Removing a String Again, given a (possibly cyclic) minimal complete ﬁnite-state automaton M, it is easy to build a new complete automaton M accepting L(M ) = L(M) − {w} = L(M) ∩ (Σ∗ − {w}) by applying the intersection construct deﬁned above to M and M−w. The resulting automaton has the same sets of reachable states in Q as in the case of adding string w and therefore the same close-to-minimality properties; since w is supposed to be in L(M), however, no queue states will be formed. (Note that, if w ∈/ L(M), a nonaccepting queue with all states eventually equivalent to ⊥ = (⊥, ⊥w) may be formed.) The accepting states in F are intact states (q, ⊥w) and cloned states (q, x) with q ∈ F, except for state (q, w). Minimization may be performed analogously to the string addition case.  4. Algorithms  4.1 Adding a String Figure 1 shows the algorithm that may be used to add a string to an existing automaton, which follows the construction in Section 3.1. The resulting automaton is viewed as a modiﬁcation of the original one: Therefore, intact states are not created; instead, unreachable intact states are eliminated later. The register R of states not needing minimization is initialized with Q. The algorithm has three parts:  • First, the cloned and queue states are built and added to Q by using function clone() for all preﬁxes of w. The function returns a cloned state  210  Carrasco and Forcada  Incremental Construction of Minimal FSA  (with all transitions created), if the argument is a nonabsorption state in Q − {⊥}, or a queue state, if it operates on the absorption state ⊥ ∈ Q. • Second, those intact states that have become unreachable as a result of designating the cloned state q0 as the new start state are removed from Q and R, and the start state is replaced by its clone. Unreachable states are simply those having no incoming transitions as constructed by the algorithm or as a consequence of the removal of other unreachable states; therefore, function unreachable() simply has to check for the absence of incoming transitions. Note that only intact states (q, ⊥w) corresponding to q such that δ∗(q0, x) = q for some x ∈ Pr(w) may become unreachable as a result of having been cloned. • Third, the queue and cloned states are checked (starting with the last state) against the register using function replace or register(), which is essentially the same as the nonrecursive version in the second algorithm in Daciuk et al. (2000) and is shown in Figure 2. If argument state q is found to be equivalent to a state p in the register R, function merge(p, q) is called to redirect into p those transitions coming into q; if not, argument state q is simply added to the register. Equivalence is checked by function equiv(), shown in Figure 3, which checks for the equivalence of states by comparing (1) whether both states are accepting or not, and (2) whether the corresponding outgoing transitions lead to the same state in R. Note that outgoing transitions cannot lead to equivalent states, as there are no pairs of different equivalent states in the register (∀p, q ∈ R, equiv(p, q) ⇒ p = q) and backwards minimization guarantees that the state has no transitions to unregister states. Finally, the new (minimal) automaton is returned. In real implementations, absorption states are not explicitly stored; this results in small differences in the implementations of the functions clone() and equiv(). 4.2 Removing a String The algorithm for removing a string from the language accepted by an automaton M differs from the previous algorithm only in that the line F ← F − {qlast} has to be added after the ﬁrst end for. Since the string removal algorithm will usually be asked to remove a string that was in L(M), function clone() will usually generate only cloned states and no queue states (see Section 3.2 for the special case w ∈/ L(M)). 5. Examples 5.1 Adding a String Assume that we want to add the string bra to the automaton in Figure 4, which accepts the set of strings (ba)+ ∪ {bar} (for clarity, in all automata, the absorption state and all transitions leading to it will not be drawn). The single-string automaton for string bra is shown in Figure 5. Application of the ﬁrst stages of the string addition algorithm leads to the (unminimized) automaton in Figure 6. The automaton has, in addition to the set of intact states {(0, ⊥w), . . . , (5, ⊥w)}, two cloned states ((0, ) and (1, b)) and two queue states ((⊥, br) and (⊥, bra)). As a consequence of the designation of (0, ) as the  211  Computational Linguistics  Volume 28, Number 2  algorithm addstring Input: M = (Q, Σ, δ, q0, F) (minimal, complete), w ∈ Σ∗ Output: M = (Q , Σ, δ , q0, F ) minimal, complete, and such that L(M ) = L(M)∪{w} R ← Q [initialize register] q0 ← clone(q0) [clone start state] qlast ← q0 for i = 1 to |w| q ← clone(δ∗(q0, w1 · · · wi)) [create cloned and queue states; add clones of accepting states to F] δ(qlast, wi) ← q qlast ← q end for i←1 qcurrent ← q0 while(i ≤ |w| and unreachable(qcurrent)) qnext ← δ(qcurrent, wi) Q ← Q − {qcurrent} [remove unreachable state from Q and update transitions in δ] R ← R − {qcurrent} [remove also from register] qcurrent ← qnext i←i+1 end while if unreachable(qcurrent) Q ← Q − {qcurrent} R ← R − {qcurrent} end if q0 ← q0 [replace start state] for i = |w| downto 1 replace or register(δ∗(q0, w1 · · · wi)) [check queue and cloned states one by one] end for return M = (Q, Σ, δ, q0, F) end algorithm Figure 1 Algorithm to add a string w to the language accepted by a ﬁnite-state automaton while keeping it minimal.  function replace or register(q) if ∃p ∈ R : equiv(p, q) then merge(p, q) else R ← R ∪ {q} end if end function Figure 2 The function replace or register(). 212  Carrasco and Forcada  Incremental Construction of Minimal FSA  function equiv(p, q) if (p ∈ F ∧ q ∈/ F) ∨ (p ∈/ F ∧ q ∈ F) return false for all symbols a ∈ Σ if δ(p, a) = δ(q, a) return false end for return true end function Figure 3 The function equiv(p, q).  Figure 4 Minimal automaton accepting the set of strings (ba)+ ∪ {bar}.  Figure 5 Single-string automaton accepting string bra.  3,⊥w r  0,⊥w b 1,⊥w a 2,⊥w  b  a  a  4,⊥w  5,⊥w  0,ε b 1,b  b  r  ⊥,br a ⊥,bra  Figure 6 Unminimized automaton accepting the set (ba)+ ∪ {bar} ∪ {bra}. Shadowed states (0, ⊥w) and (1, ⊥w) have become unreachable (have no incoming transitions) and are eliminated in precisely that order.  213  Computational Linguistics  Volume 28, Number 2  Figure 7 Minimal automaton accepting the set (ba)+ ∪ {bar} ∪ {bra}.  ε  b  b  a  ba b bab a baba  Figure 8 Single-string automaton accepting the string baba.  a  2,⊥w b 4,⊥w a  6,⊥w  0,⊥w  b  1,⊥w r  r  b  3,⊥w a 5,⊥w  b  r  r  0,ε b 1,b a 2,ba b 4,bab a 6,baba  Figure 9 Unminimized automaton accepting the set (ba)+ ∪ {bar} ∪ {bra} − {baba}. Shadowed states (0, ⊥w), (1, ⊥w), and (2, ⊥w) have become unreachable (have no incoming transitions) and are eliminated in precisely that order. new start state, shadowed states (0, ⊥w) and (1, ⊥w) become unreachable (have no incoming transitions) and are eliminated in precisely that order in the second stage of the algorithm. The ﬁnal stage of the algorithm puts intact states into the register and tests queue and cloned states for equivalence with states in the register. The ﬁrst state tested is (⊥, bra), which is found to be equivalent to (3, ⊥w); therefore, transitions coming into (⊥, bra) are made to point to (3, ⊥w). Then, states (⊥, br), (1, b) and (0, ) are tested in order, found to have no equivalent in the register, and added to it. The resulting minimal automaton, after a convenient renumbering of states, is shown in Figure 7. 5.2 Removing a String Now let us consider the case in which we want to remove string baba from the language accepted by the automaton in Figure 7 (the single-string automaton for baba is shown in Figure 8). The automaton resulting from the application of the initial (construction) stages of the automaton is shown in Figure 9. Note that state (6, baba) is marked as nonaccepting, because we are removing a string. Again, as a consequence of the designation of (0, ) as the new start state, shadowed states (0, ⊥w), (1, ⊥w),  214  Carrasco and Forcada  Incremental Construction of Minimal FSA  2  a  4  r  0  b  
 Introduction: The search for life in space The moon’s chemical composition How early earth-moon proximity shaped the moon How the moon helped life evolve on earth Improbability of the earth-moon system Binary/trinary star systems make life unlikely The low probability of nonbinary/trinary systems Properties of earth’s sun that facilitate life Summary  The TextTiling algorithm (Hearst 1993, 1994, 1997) attempts to recognize these subtopic changes by making use of patterns of lexical co-occurrence and distribution; subtopic boundaries are assumed to occur at the point in the documents at which large shifts in vocabulary occur. Many others have used this technique, or slight variations  ∗ Harvard University, 380 Leverett Mail Center, Cambridge, MA 02138. E-mail: pevzner@post.harvard.edu † University of California, Berkeley, 102 South Hall #4600, Berkeley, CA 94720. E-mail: hearst@sims.berkeley.edu  c 2002 Association for Computational Linguistics  Computational Linguistics  Volume 28, Number 1  of it, for subtopic segmentation (Nomoto and Nitta 1994; Hasnah 1996; Richmond, Smith, and Amitay 1997; Heinonen 1998; Boguraev and Neff 2000). Other techniques use clustering and/or similarity matrices based on word co-occurrences (Reynar 1994; Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cue words, or hand-selected cue words to detect segment boundaries (Passonneau and Litman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998). Researchers have explored the use of this kind of document segmentation to improve automated summarization (Salton et al. 1994; Barzilay and Elhadad 1997; Kan, Klavans, and McKeown 1998; Mittal et al. 1999; Boguraev and Neff 2000) and automated genre detection (Karlgren 1996). Text segmentation issues are also important for passage retrieval, a subproblem of information retrieval (Hearst and Plaunt 1993; Salton, Allan, and Buckley 1993; Callan 1994; Kaszkiel and Zobel 1997). More recently, a great deal of interest has arisen in using automatic segmentation for the detection of topic and story boundaries in news feeds (Mani et al. 1997; Merlino, Morey, and Maybury 1997; Ponte and Croft 1997; Hauptmann and Witbrock 1998; Allan et al. 1998; Beeferman, Berger, and Lafferty 1997, 1999). Sometimes segmentation is done at the clause level, for the purposes of detecting nuances of dialogue structure or for more sophisticated discourse-processing purposes (Morris and Hirst 1991; Passonneau and Litman 1993; Litman and Passonneau 1995; Hirschberg and Nakatani 1996; Marcu 2000). Some of these algorithms produce hierarchical dialogue segmentations whose evaluation is outside the scope of this discussion. 1.1 Evaluating Segmentation Algorithms There are two major difﬁculties associated with evaluating algorithms for text segmentation. The ﬁrst is that since human judges do not always agree where boundaries should be placed and how ﬁne grained an analysis should be, it is difﬁcult to choose a reference segmentation for comparison. Some evaluations circumvent this difﬁculty by detecting boundaries in sets of concatenated documents, where there can be no disagreements about the fact of the matter (Reynar 1994; Choi 2000); others have several human judges make ratings to produce a “gold standard.” The second difﬁculty with evaluating these algorithms is that for different applications of text segmentation, different kinds of errors become important. For instance, for information retrieval, it can be acceptable for boundaries to be off by a few sentences— a condition called a near miss—but for news boundary detection, accurate placement is crucial. For this reason, some researchers prefer not to measure the segmentation algorithm directly, but consider its impact on the end application (Manning 1998; Kan, Klavans, and McKeown 1998). Our approach to these two difﬁculties is to evaluate algorithms on real segmentations using a “gold standard” and to develop an evaluation algorithm that suits all applications reasonably well. Precision and recall are standard evaluation measures for information retrieval tasks and are often applied to evaluation of text segmentation algorithms as well. Precision is the percentage of boundaries identiﬁed by an algorithm that are indeed true boundaries; recall is the percentage of true boundaries that are identiﬁed by the algorithm. However, precision and recall are problematic for two reasons. The ﬁrst is that there is an inherent trade-off between precision and recall; improving one tends to cause the score for the other to decline. In the segmentation example, positing more boundaries will tend to improve the recall but at the same time reduce the precision. Some evaluators use a weighted combination of the two known as the F-measure (Baeza-Yates and Ribeiro-Neto 1999), but this is difﬁcult to interpret (Beeferman, Berger, and Lafferty 1999). Another approach is to plot a precision-recall curve, showing the scores for precision at different levels of recall.  20  Pevzner and Hearst  An Evaluation Metric for Text Segmentation  Figure 1 Two hypothetical segmentations of the same reference (ground truth) document segmentation. The boxes indicate sentences or other units of subdivision, and spaces between boxes indicate potential boundary locations. Algorithm A-0 makes two near misses, while Algorithm A-1 misses both boundaries by a wide margin and introduces three false positives. Both algorithms would receive scores of 0 for both precision and recall. Another problem with precision and recall is that they are not sensitive to near misses. Consider, for example, a reference segmentation and the results obtained by two different text segmentation algorithms, as depicted in Figure 1. In both cases, the algorithms fail to match any boundary precisely; both receive scores of 0 for precision and recall. However, Algorithm A-0 is close to correct in almost all cases, whereas Algorithm A-1 is entirely off, adding extraneous boundaries and missing important boundaries entirely. In some circumstances, it would be useful to have an evaluation metric that penalizes A-0 less harshly than A-1. 1.2 The Pk Evaluation Metric Beeferman, Berger, and Lafferty (1997) introduce a new evaluation metric that attempts to resolve the problems with precision and recall, including assigning partial credit to near misses. They justify their metric as follows (page 43): Segmentation . . . is about identifying boundaries between successive units of information in a text corpus. Two such units are either related or unrelated by the intent of the document author. A natural way to reason about developing a segmentation algorithm is therefore to optimize the likelihood that two such units are correctly labeled as being related or being unrelated. Our error metric Pµ is simply the probability that two sentences drawn randomly from the corpus are correctly identiﬁed as belonging to the same document or not belonging to the same document. The derivation of Pµ is rather involved, and a much simpler version is adopted in the later work (Beeferman, Berger, and Lafferty 1999) and by others. This version, referred to as Pk, is calculated by setting k to half of the average true segment size and then computing penalties via a moving window of length k. At each location, the algorithm determines whether the two ends of the probe are in the same or different segments in the reference segmentation and increases a counter if the algorithm’s segmentation disagrees. The resulting count is scaled between 0 and 1 by dividing by the number of measurements taken. An algorithm that assigns all boundaries correctly receives a score of 0. Beeferman, Berger, and Lafferty (1999) state as part of 21  Computational Linguistics  Volume 28, Number 1  Figure 2 An illustration of how the Pk metric handles false negatives. The arrowed lines indicate the two poles of the probe as it moves from left to right; the boxes indicate sentences or other units of subdivision; and the width of the window (k) is four, meaning four potential boundaries fall between the two ends of the probe. Solid lines indicate no penalty is assigned; dashed lines indicate a penalty is assigned. Total penalty is always k for false negatives. the justiﬁcation for this metric that, to discourage “cheating” of the metric, degenerate algorithms—those that place boundaries at every position, or place no boundaries at all—are assigned (approximately) the same score. Additionally, the authors deﬁne a false negative (also referred to as a miss) as a case when a boundary is present in the reference segmentation but missing in the algorithm’s hypothesized segmentation, and a false positive as an assignment of a boundary that does not exist in the reference segmentation. 2. Analysis of the Pk Error Metric The Pk metric is fast becoming the standard among researchers working in text segmentation (Allan et al. 1998; Dharanipragada et al. 1999; Eichmann et al. 1999; van Mulbregt et al. 1999; Choi 2000). However, we have reservations about this metric. We claim that the fundamental premise behind it is ﬂawed; additionally, it has several signiﬁcant drawbacks, which we identify in this section. In the remainder of the paper, we suggest modiﬁcations to resolve these problems, and we report the results of simulations that validate the analysis and suggest that the modiﬁed metric is an improvement over the original. 2.1 Problem 1: False Negatives Penalized More Than False Positives Assume a text with segments of average size 2k, where k is the distance between the two ends of the Pk probe. If the algorithm misses a boundary—produces a false negative—it receives k penalties. To see why, suppose S1 and S2 are two segments of length 2k, and the algorithm misses the transition from S1 to S2. When Pk sweeps across S1, if both ends of the probe point to sentences that are inside S1, the two sentences are in the same segment in both the reference and the hypothesis, and no penalty is incurred. When the right end of the probe crosses the reference boundary between S1 and S2, it will start recording nonmatches, since the algorithm assigns the two sentences to the same segment, while the reference does not. This circumstance happens k times, until both ends of the probe point to sentences that are inside S2. (See Figure 2.) This analysis assumes average size segments; variation in segment size is discussed below, but does not have a large effect on this result. 22  Pevzner and Hearst  An Evaluation Metric for Text Segmentation  Figure 3  An illustration of how the Pk metric handles false positives. Notation is as in Figure 2. Total penalty depends on the distance between the false positive and the relevant correct  boundaries;  on  average,  it  is  k 2  ,  assuming  a  uniform  distribution  of  boundaries  across  the  document. This example shows the consequences of two different locations of false positives:  on  the  left,  the  penalty  is  k 2  ;  on  the  right,  it  is  k.  Now, consider false positives. A false positive occurs when the algorithm places  a boundary at some position where there is no boundary in the reference segmenta-  tion. The number of times that this false positive is noted by Pk depends on where exactly inside S2 the false positive occurs. (See Figure 3.) If it occurs in the middle  of the segment, the false positive is noted k times (as seen on the right-hand side of  Figure 3). If it occurs j < k sentences from the beginning or the end of the segment, the  segmentation is penalized j times. Assuming uniformly distributed false positives, on  average  a  false  positive  is  noted  k 2  times  by  the  metric—half  the  rate  for  false  negatives.  This average increases with segment size, as we will discuss later, and changes if one  assumes different distributions of false positives throughout the document. However,  this does not change the fact that in most cases, false positives are penalized some  amount less than false negatives.  This is not an entirely undesirable side effect. This metric was devised to take into  account how close an assigned boundary is to the true one, rather than just marking  it as correct or incorrect. This method of penalizing false positives achieves this goal:  the closer the algorithm’s boundary is to the actual boundary, the less it is penalized.  However, overpenalizing false negatives to do this is not desirable.  One way to ﬁx the problem of penalizing false negatives more than false positives  is to double the false positive penalty (or halve the false negative penalty). However,  this would undermine the probabilistic nature of the metric. In addition, doubling the  penalty may not always be the correct solution, since segment size will vary from the  average, and false positives are not necessarily uniformly distributed throughout the  document.  2.2 Problem 2: Number of Boundaries Ignored Another important problem with the Pk metric is that it allows some errors to go unpenalized. In particular, it does not take into account the number of segment boundaries between the two ends of the probe. (See Figure 4.) Let ri indicate the number of boundaries between the ends of the probe according to the reference segmentation, and let ai indicate the number of boundaries proposed by some text segmentation algorithm for the same stretch of text. If ri = 1 (the reference segmentation indicates one boundary) and ai = 2 (the algorithm marks two boundaries within this range), then the algorithm makes at least one false positive (spurious boundary) error. However, the evaluation metric Pk does not assign a penalty in this situation. Similarly, if ri = 2 and ai = 1, the  23  Computational Linguistics  Volume 28, Number 1  Figure 4 An illustration of the fact that the Pk metric fails to penalize false positives that fall within k sentences of a true boundary. Notation is as in Figure 2.  algorithm has made at least one false negative (missing boundary) error, but it is not penalized for this error under Pk.  2.3 Problem 3: Sensitivity to Variations in Segment Size  The size of the segment plays a role in the amount that a false positive within the  segment or a false negative at its boundary is penalized. Let us consider false negatives  (missing boundaries) ﬁrst. As seen above, with average size segments, the penalty for  a false negative is k. For larger segments, it remains at k—it cannot be any larger  than that, since for a given position i there can be at most k intervals of length k  that include that position. As segment size gets smaller, however, the false negative  penalty changes. Suppose we have two segments, A and B, and the algorithm misses  the boundary between them. Then the algorithm will be penalized k times if Size(A) +  Size(B) > 2k, that is, as long as each segment is about half the average size or larger.  The penalty will then decrease linearly with Size(A) + Size(B) so long as k < Size(A) +  Size(B) < 2k. To be more exact, the penalty actually decreases linearly as the size of  either segment decreases below k. This is intuitively clear from the simple observation  that in order to incur a penalty at any range ri for a false negative, it has to be the case that ri > ai. In order for this to be true, both the segment to the left and the segment to the right of the missed boundary have to be of size greater than  k; otherwise, the penalty can only be equal to the size of the smaller segment. When  Size(A)+Size(B) < k, the penalty disappears completely, since then the probe’s interval  is larger than the combined size of both segments, making it not sensitive enough to  detect the false negative. It should be noted that ﬁxing Problem 2 would at least  partially ﬁx this bias as well.  Now, consider false positives (extraneous boundaries). For average segment size  and  a  uniform  distribution  of  false  positives,  the  average  penalty  is  k 2  ,  as  described  earlier. In general, in large enough segments, the penalty when the false positive is  a distance d < k from a boundary is d, and the penalty when the false positive is a  distance d > k from a boundary is k. Thus, for larger segments, the average penalty  assuming a uniform distribution becomes larger, because there are more places in the  segment that are at least k positions away from a boundary. The behavior at the edges  of the segments remains the same, though, so the average penalty never reaches k.  Now, consider what happens with smaller segments. Suppose we have a false positive  in Segment A. As Size(A) decreases from 2k to k, the average false positive penalty  decreases linearly with it, because when Size(A) decreases below 2k, the maximum  distance any sentence can be from a boundary becomes less than k. Therefore, the  24  Pevzner and Hearst  An Evaluation Metric for Text Segmentation  Figure 5 A reference segmentation and ﬁve different hypothesized segmentations with different properties. maximum possible penalty for a false positive in A is less than k, and this number continues to decrease as Size(A) decreases. When Size(A) < k, the false positive penalty disappears, for the same reason that the false negative penalty disappears for smaller segments. Again, ﬁxing Problem 2 would go a long way toward eliminating this bias. Thus, errors in larger-than-average segments increase the penalty slightly (for false positives) or not at all (for false negatives) as compared to average size segments, while errors in smaller-than-average segments decrease the penalty signiﬁcantly for both types of error. This means that as the variation of segment size increases, the metric becomes more lenient, since it severely underpenalizes errors in smaller segments, while not making up for this by overpenalizing errors in larger segments. 2.4 Problem 4: Near-Miss Error Penalized Too Much Reconsider the segmentation made by Algorithm A-0 in Figure 1. In both cases of boundary assignment, Algorithm A-0 makes both a false positive and a false negative error, but places the boundary very close to the actual one. We will call this kind of error a near-miss error, distinct from a false positive or false negative error. Distinguishing this type of error from “pure” false positives better reﬂects the goal of creating a metric different from precision and recall, since it can be penalized less than a false negative or a false positive. Now, consider the algorithm segmentations shown in Figure 5. Each of the ﬁve algorithms makes a mistake either on the boundary between the ﬁrst and second segment of the reference segmentation, or within the second segment. How should these various segmentations be penalized? In the analysis below, we assume an application for which it is important not to introduce spurious boundaries. These comparisons will most likely vary depending on the goals of the target application. Algorithm A-4 is arguably the worst of the examples, since it has a false positive and a false negative simultaneously. Algorithms A-0 and A-2 follow: they contain a pure false negative and false positive, respectively. Comparing Algorithms A-1 and A-3, we see that Algorithm A-3 is arguably better, because it recognizes that only one boundary is present rather than two. Algorithm A-1 does not recognize this, and inserts an extra segment. Even though Algorithm A-1 actually places a correct boundary, it also places an erroneous boundary, which, although close to the actual one, is still a false positive—in fact, a pure false positive. For this reason, Algorithm A-3 can be considered better than Algorithm A-1. 25  Computational Linguistics  Volume 28, Number 1  Now, consider how Pk treats the ﬁve types of mistakes above. Again, assume the ﬁrst and second segments in the reference segmentation are average size segments.  Algorithm A-4 is penalized the most, as it should be. The penalty is as much as 2k if the  false positive falls in the middle of Segment C, and it is > k as long as the false positive  is  a  distance  >  k 2  from  the  actual  boundary  between  the  ﬁrst  and  second  reference  segments. The penalty is large because the metric catches both the false negative  and the false positive errors. The segmentations assigned by Algorithms A-0 and A-2  are treated as discussed earlier in conjunction with Problem 1: the one assigned by  Algorithm A-0 has a false negative and thus incurs a penalty of k, and the one assigned  by Algorithm A-2 has a false positive, and thus incurs a penalty of ≤ k. Finally, consider  the segmentations assigned by Algorithms A-1 and A-3, and suppose that both contain  an incorrect boundary some small distance e from the actual one. Then the penalty for  Algorithm A-1 is e, while the penalty for Algorithm A-3 is 2e. This should not be the  case; Algorithm A-1 should be penalized more than Algorithm A-3, since a near-miss  error is better than a pure false positive, even if it is close to the boundary.  2.5 Problem 5: What Do the Numbers Mean? Pk is nonintuitive because it measures the probability that two sentences k units apart are incorrectly labeled as being in different segments, rather than directly reﬂecting the competence of the algorithm. Although perfect algorithms score 0, and various degenerate ones score 0.5, numerical interpretation and comparison are difﬁcult because it is not clear how the scores are scaled.  3. A Solution  It turns out that a simple change to the error metric algorithm remedies most of the problems described above, while retaining the desirable characteristic of penalizing near misses less than pure false positives and pure false negatives. The amended metric, which we call WindowDiff, works as follows: for each position of the probe, simply compare the number of reference segmentation boundaries that fall in this interval (ri) with the number of boundaries that are assigned by the algorithm (ai). The algorithm is penalized if ri = ai (which is computed as |ri − ai| > 0). More formally,  WindowDiﬀ  (ref  , hyp)  =  N  
2. Dale and Reiter (1995): The Incremental Algorithm The Incremental Algorithm of Dale and Reiter (1995) singles out a target object from among some larger domain of entities. It does this by logically conjoining a number of properties found in a part of the KB that represents information shared between speaker and hearer. The authors observed that the problem of ﬁnding a (“Full Brevity”) description that contains the minimum number of properties is computationally intractable (i.e., NP Hard). They combined this with the known fact that speakers often produce nonminimal descriptions anyway (e.g., Pechman 1989). Accordingly, they proposed an algorithm that only approximates Full Brevity, while being of only linear complexity. Our summary of the algorithm glosses over many details, yet still allows us to discuss completeness. In particular, we disregard any special provisions that might be made for the selection of head nouns because, arguably, this has to involve realizational issues.1  
‡ NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and Technology (POSTECH), Pohang, 790-784, Korea. E-mail: jhlee@postech.ac.kr. 
Ð Ü Î ÆÈ Î ÆÈ Ð × Ô ÞÞ  Ë ÆÈ ÎÈ ÆÈ Î ÆÈ Ð Ü Î ÆÈ Ð × Ô ÞÞ  Ë ÆÈ ÎÈ ÆÈ Î ÆÈ Ð ÜÐ × ÆÈ Ô ÞÞ  Figure 1 Depictions of three different derivations of the same tree representation of Alex likes pizza, with arrows indicating the sites of tree fragment substitutions.  2. DOP1 Models  For simplicity, this note focuses on DOP1 or Tree-DOP models, in which linguistic representations are phrase structure trees, but the results carry over to more complex models that use attribute-value feature structure representations such as LFG-DOP. The fragments used in DOP1 are multinode trees whose leaves may be labeled with nonterminals as well as terminals. A derivation starts with a fragment whose root is labeled with the start symbol, and it proceeds by substituting a fragment for the leftmost nonterminal leaf under the constraint that the fragment’s root node and the leaf node have the same label. The derivation terminates when there are no nonterminal leaves. Figure 1 depicts three different derivations that yield the same tree. The fragments used in these derivations could have been obtained from a training corpus of trees that contains trees for examples such as Sasha likes motorcycles, Alex eats pizza, and so on. In a DOP model, each fragment is associated with a real-valued weight, and the weight of a derivation is the product of the weights of the tree fragments involved. The weight of a representation is the sum of the weights of its derivations, and a probability distribution over linguistic representations is obtained by normalizing the representations’ weights.1 Given a combinatory operation and a ﬁxed set of fragments, a DOP model is a parametric model where the fragment weights are the parameters. In DOP1 and DOP models based on it, the weight associated with a fragment is estimated as follows (Bod 1998). For each tree fragment f , let n(f ) be the number of times it appears in the training corpus, and let F be the set of all tree fragments with the same root as f . Then the weight w(f ) associated with f is  w(f ) =  n(f ) .  f ∈F n(f )  This relative-frequency estimation method has the advantage of simplicity, but as shown in the following sections, it is biased and inconsistent.  
This paper presents an ongoing task that will construct a DAML+OIL-compliant Chinese Lexical Ontology. The ontology mainly comprises three components: a hierarchical taxonomy consisting of a set of concepts and a set of relations describing the relationships among the concepts, a set of lexical entries associated with the concepts and relations, and a set of axioms describing the constraints on the ontology. It currently contains 1,075 concepts, 65,961 lexical entries associated with the concepts, 299 relations among the concepts excluding the hypernym and hyponym relations, 27,004 relations between the lexical entries and the concepts, and 79,723 relations associating the lexical entries with the concepts. Introduction The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding [Maedche and Staab 2001]. Therefore, constructing applicable ontologies influences the success of Semantic Web largely. An ontology mainly consists of a set of concepts and a set of relations that describe relationships among the concepts. An upper ontology is limited to concepts that are abstract, generic, domain-broad, and articulate. Cycorp constructed an upper ontology – Upper Cyc® Ontology. It consists of approximately 3,000 terms, i.e. concepts and relations. It has been used for organizing the upper structure of a knowledge base – the Cyc® KB. A working group of IEEE (P1600.1) is also trying to standardize the specification of the upper ontology. An upper ontology, called IEEE SUO (Standard Upper Ontology), is expected to  enable computers to utilize it for applications, such as natural language understanding and generation, information retrieval and extraction, Semantic Web services [McIlraith et al. 2001], etc. It is estimated to contain between 1,000 and 2,500 terms plus roughly ten definitional statements for each term. This paper presents an ongoing task that will construct an upper-ontology-like ontology for Chinese research and applications. We refer to it as CLO (Chinese Lexical Ontology). In addition to the structural portion, the CLO will contain Chinese lexicons associated with the concepts and relations. A pure ontology containing concepts only (without lexicons) is abstract. A lexicon-associated ontology makes the substantiation of abstract concepts easier. HowNet defines 65,961 Simplified Chinese lexical entries by a set of predefined features including 6 categories of primary features and 100 secondary features, and several symbols, in which the primary features are in a taxonomy with single inheritance. The taxonomy is essentially regarded as the taxonomy of the CLO. However, the Chinese lexical entries defined in HowNet are simplified version. They are not suitable for Traditional Chinese research and applications. A traditional version of Chinese dictionary released by Sinica of R.O.C. is frequently used for Traditional Chinese NLP. By combining the Traditional Chinese dictionary and the HowNet, we attempt to construct the CLO and represent it in the semantic markup language DAML+OIL since DAML+OIL is currently a basis of Web ontology language. The task of constructing the CLO can be divided into three portions. Firstly, a hierarchical taxonomy of concepts including relations among the concepts is required. In our case, we utilize the hierarchical primary features of HowNet to  form the structure. Secondly, a set of lexical entries should be associated with the concepts and relations. Thirdly, a set of axioms that describe additional constraints on the ontology are required. This paper addresses the ongoing construction task and a brief introduction of Web ontology languages. 
 words and new usages of words will  Dynamic lexical acquisition is a procedure where the lexicon of an NLP system is updated automatically during sentence analysis. In our system, new words and new attributes are proposed online according to the context of each sentence, and then get accepted or rejected during syntactic analysis. The accepted lexical information is stored in an auxiliary lexicon which can be used in conjunction with the existing dictionary in subsequent processing. In this way, we are able to process sentences with an incomplete lexicon and fill in the missing info without the need of human editing. As the auxiliary lexicons are corpus-based, domain-specific dictionaries can be created automatically by  continue to appear. (3) certain words and usages of words decay after a while or only exist in a certain domain, and it is inappropriate to make them a permanent part of the dictionary. This paper discusses an alternative approach where, instead of editing a static dictionary, we acquire lexical information dynamically during sentence analysis. This approach is currently implemented in our Chinese system and Chinese examples will be used to illustrate the process. In Section 1, we will discuss how the new lexical information is discovered. Section 2 discusses how such information is filtered, lexicalized, and used in future processing. Section 3 is devoted to evaluation.  combining the existing dictionary with different auxiliary lexicons. Evaluation shows that this mechanism significantly improves the coverage of our parser.  
Spoken queries are a natural medium for searching the Web in settings where typing on a keyboard is not practical. This paper describes a speech interface to the Google search engine. We present experiments with various statistical language models, concluding that a unigram model with collocations provides the best combination of broad coverage, predictive power, and real-time performance. We also report accuracy results of the prototype system. 
We describe a discovery program, called UNIVAUTO (UNIVersals AUthoringTOol), whose domain of application is the study of language universals, a classic trend in contemporary linguistics. Accepting as input information about languages, presented in terms of feature-values, the discoveries of another human agent arising from the same data, as well as some additional data, the program discovers the universals in the data, compares them with the discoveries of the human agent and, if appropriate, generates a report in English on its discoveries. Running UNIVAUTO on the data from the seminal paper of Greenberg (1966) on word order universals, the system has produced several linguistically valuable texts, two of which are published in a refereed linguistic journal. 
We describe a language-independent, flexible, and accurate method for the detection of abbreviations in text corpora. It is based on the idea that an abbreviation can be viewed as a collocation, and can be identified by using methods for collocation detection such as the log likelihood ratio. Although the log likelihood ratio is known to show a good recall, its precision is poor. We employ scaling factors which lead to a strong improvement of precision. Experiments with English and German corpora show that abbreviations can be detected with high accuracy. Introduction The detection of abbreviations in a text corpus forms one of the initial steps in tokenization (cf. Liberman/Church 1992). This is not a trivial task, since a tokenizer is confronted with ambiguous tokens. For English, e.g., Palmer/Hearst (1997:241) report that periods (•) can be used as decimal points, abbreviation marks, end-ofsentence marks, and as abbreviation marks at the end of a sentence. In this paper, we will concentrate on the classification of the period as either an abbreviation mark or a punctuation mark. We assume that an abbreviation can be viewed as a collocation consisting of the abbreviated word itself and the following •. In case of an abbreviation, we expect the occurrence of • following the previous ‘word’ to be more likely than in a case of an end-of-sentence punctation. The starting point is the log likelihood ratio (log λ, Dunning 1993). If the null hypothesis (H0) – as given in (1) – expresses that the occurrence of a period is in-  dependent of the preceeding word, the alternative hypothesis (HA) in (2) assumes that the occurrence of a period is not independent of the occurrence of the word preceeding it.  (1) H0: P(•|w) = p = P(•|¬w) (2) HA: P(•|w) = p1 ≠ p2 = P(•|¬w) The log λ of the two hypotheses is given in (3). Its distribution is asymptotic to a χ2 distribution and can hence be used as a test statistic (Dunning 1993).  ( ) ( ) (3)  log λ  =  - 2 log     L    H0  L  HA      
Natural language processing NLP programs are confronted with various di culties in processing HTML and XML documents, and have the potential to produce better results if linguistic information is annotated in the source texts. We have therefore developed the Linguistic Annotation Language or LAL, which is an XML-compliant tag set for assisting natural language processing programs, and NLP tools such as parsers and machine translation programs which can accept LAL-annotated input. In addition, we have developed a LALannotation editor which allows users to annotate documents graphically without seeing tags. Further, we have conducted an experiment to check the translation quality improvement by using LAL annotation. 
A bilingual concept MRD is of significance for IE, MT, WSD and the like. However, it is reasonably difficult to build such a lexicon for there exist two ontologies, also, the evolution of such a lexicon is quite challenging. In this paper, we would like to put forth the new approach to building a bilingual WordNet-like lexicon and to dwell on some of the pivotal algorithms. A characteristic of this new approach is to emphasize the inheritance and transformation of the existent monolingual lexicon. On the one hand, we have extracted all the common knowledge in WordNet as the semantic basis for further use. On the other hand, we have developed a visualized developing tool for the lexicographers to interactively operate on to express the bilingual semantics. The bilingual lexicon has thus gradually come into being in this natural process. ICL now has benefited a lot by employing this new approach to build CCD (Chinese Concept Dictionary), a bilingual WordNet-like lexicon, in Peking University. 
We propose a method “Interactive Paraphrasing” which enables users to interactively paraphrase words in a document by their deﬁnitions, making use of syntactic annotation and word sense annotation. Syntactic annotation is used for managing smooth integration of word sense deﬁnitions into the original document, and word sense annotation for retrieving the correct word sense deﬁnition for a word in a document. In this way, documents can be paraphrased so that they ﬁt into the original context, preserving the semantics and improving the readability at the same time. No extra layer (window) is necessary for showing the word sense deﬁnition as in conventional methods, and other natural language processing techniques such as summarization, translation, and voice synthesis can be easily applied to the results. 
The purpose of this study is to propose a new method for machine translation. We have proceeded through with two projects for report generation (Kittredge and Polguere, 2000) : Weather Forecast and Monthly Economic Report to be produced in four languages : English, Japanese, French, and German. Their input data is stored in XML-DB. We applied a three-stage pipelined architecture (Reiter and Dale, 2000), and each stage was implemented as XML transformation processes. We regard XML stored data as language-neutral intermediate form and employ the so-called `sublanguage approach' (Somers, 2000). The machine translation process is implemented via XMLDB as a kind of interlingua approach instead of the conventional structure transfer approach. 
 ,QWURGXFWLRQ This paper reports on the results of an evaluation we have performed on the Grammar Checker for Norwegian (NGC), developed at The Text Laboratory, University of Oslo. The NGC is now part of Microsoft Word in the Office XP package released in 2001. The goal of the NGC was decided partly by that of the Swedish Grammar Checker (SGC, Arppe 2000 and Birn 2000), designed to detect what were assumed to be the errors of “standard” users, and partly by a wish to include more linguistically advanced features. The kind of grammatical mistakes made by linguistically “non-standard”3 groups was not taken into account, and this kind of tool obviously would be beneficial to these groups. Having provided an overview of the main method behind the NGC, we will give a general overview of the kinds of errors that the NGC is designed to detect. Then we will show how it performs on various deviant language input 
If information extraction wants to make its results more accurate, it will have to resort increasingly to a coherent implementation of natural language semantics. In this paper, we will focus on the extraction of semantic case roles from texts. After setting the essential theoretical framework, we will argue that it is possible to detect case roles on the basis of morphosyntactic and lexical surface phenomena. We will give a concise overview of our methodology and of a preliminary test that seems to confirm our hypotheses. Introduction Information extraction (IE) from texts currently receives a large research interest. Traditionally, it has been associated with the – often verbatim – extraction of domain-specific information from free text (Riloff & Lorenzen 1999). Input documents are scanned for very specific relevant information elements on a particular topic, which are used to fill out empty slots in a predefined frame. Other types of systems try to acquire this knowledge automatically by detecting reoccurring lexical and syntactic information from manually annotated example texts (e.g. Soderland 1999). Most of these techniques are inherently limited because they exclude natural language semantics as much as possible. This is understandable for reasons of efficiency and genericity but it restricts the algorithms' possibilities and it disregards the fact that – at least in free text – IE has much to do with identifying semantic roles.  In most of these systems, case role detection as a goal in itself has been treated in a rather trivial way. Our research will try to provide a systematic approach to case role detection as an independent extraction task. Using notions from systemic-functional grammar and presupposing a possible mapping between morphosyntactic properties and functional role patterns, we will develop a general model for case role extraction. The idea is to learn domain-independent case role patterns from a tagged corpus, which are then (automatically) specialized to particular domain-dependent case role sets and which can be reassigned to previously unseen text. In this paper, we will focus on the first part of this task. For IE, an accurate and speedy detection of functional case roles is of major importance, since they describe events (or states) and participants to these events and thus allow for identifying real-world entities, their properties and interactions between them. 
Texts acquired from recognition sources—continuous speech/handwriting recognition and OCR—generally have three types of errors regardless of the characteristics of the source in particular. The output of the recognition process may be (1) poorly segmented or not segmented at all; (2) containing underspecified symbols (where the recognition process can only indicate that the symbol belongs to a specific group), e.g. shape codes; (3) containing incorrectly identified symbols. The project presented in this paper addresses these errors by developing of a unified linguistic framework called the MorphoLogic Recognition Assistant that provides feedback and corrections for various recognition processes. The framework uses customized morpho-syntactic and syntactic analysis where the lexicons and their alphabets correspond to the symbol set acquired from the recognition process. The successful framework must provide three services: (1) proper disambiguated segmentation, (2) disambiguation for underspecified symbols, (3) correction for incorrectly recognized symbols. The paper outlines the methods of morpho-syntactic and syntactic post-processing currently in use. Introduction Recognition processes produce a sequence of discrete symbols that usually do not entirely correspond to characters of printed text. Further on, we refer to this sequence as an input sequence.1 A 
This paper introduces a context-sensitive electronic dictionary that provides translations for any piece of text displayed on a computer screen, without requiring user interaction. This is achieved through a process of three phases: text acquisition from the screen, morpho-syntactic analysis of the context of the selected word, and the dictionary lookup. As with other similar tools available, this program usually works with dictionaries adapted from one or more printed dictionaries. To implement context sensitive features, however, traditional dictionary entries need to be restructured. By splitting up entries into smaller pieces and indexing them in a special way, the program is able to display a restricted set of information that is relevant to the context. Based on the information in the dictionaries, the program is able to recognize—even discontinuous—multiword expressions on the screen. The program has three major features which we believe make it unique for the time being, and which the development focused on: linguistic flexibility (stemming, morphological analysis and shallow parsing), open architecture (three major architectural blocks, all replaceable along public documented APIs), and flexible user interface (replaceable dictionaries, direct user feedback). In this paper, we assess the functional requirements of a context-sensitive dictionary as a start; then we explain the program’s three phases of operation, focusing on the implementation of the lexicons and the context-sensitive features. We conclude the paper by comparing our tool to other similar publicly available products, and summarize plans for future development.  
Less than 1% of the languages spoken in the world are correctly “computerized”: spell checkers, hyphenation, machine translation are still lacking for the others. In this paper, we present several directions that may help the computerization of minority languages as well as two projects where we apply some of these directions to the Lao language. Introduction During the last ten years, research has been driven and products have been developed to provide efficient linguistic tools for many languages. For example, Unicode is more and more a reality in today's operating systems and Microsoft Office XP contains proofing tools for more than 40 languages. However, for most of the world's people, the Information Era is still limited to using hardware and software that do not meet their needs in terms of language and script resources. Following the SALTMIL1 terminology, we will call a minority language a language which has a smaller resource base than the major languages. 
We describe a parser for robust and ﬂexible interpretation of user utterances in a multi-modal system for web search in newspaper databases. Users can speak or type, and they can navigate and follow links using mouse clicks. Spoken or written queries may combine search expressions with browser commands and search space restrictions. In interpreting input queries, the system has to be fault-tolerant to account for spontanous speech phenomena as well as typing or speech recognition errors which often distort the meaning of the utterance and are difﬁcult to detect and correct. Our parser integrates shallow parsing techniques with knowledge-based text retrieval to allow for robust processing and coordination of input modes. Parsing relies on a two-layered approach: typical meta-expressions like those concerning search, newspaper types and dates are identiﬁed and excluded from the search string to be sent to the search engine. The search terms which are left after preprocessing are then grouped according to co-occurrence statistics which have been derived from a newspaper corpus. These co-occurrence statistics concern typical noun phrases as they appear in newspaper texts. 
We report about the current state of development of a document suite and its applications. This collection of tools for the ﬂexible and robust processing of documents in German is based on the use of XML as unifying formalism for encoding input and output data as well as process information. It is organized in modules with limited responsibilities that can easily be combined into pipelines to solve complex tasks. Strong emphasis is laid on a number of techniques to deal with lexical and conceptual gaps that are typical when starting a new application. Introduction We have designed and implemented the XDOC document suite as a workbench for the ﬂexible processing of electronically available documents in German. We have decided to exploit XML (Bray et al., 1998) and its accompanying formalisms (e.g. XSLT (Site, 2002b)) and tools (e.g. xt (Clark, 2002) ) as a unifying framework. All modules in the XDOC system expect XML documents as input and deliver their results in XML format. XML – and ist precursor SGML – offers a formalism to annotate pieces of (natural language) texts. To be more precise: If a text is (as a simple ﬁrst approximation) seen as a sequence of characters (alphabetic and whitespace characters) then XML allows to associate arbitrary markup with arbitrary subsequences of contiguous characters. Many linguistic units of interest are represented by strings of contiguous characters (e.g. words, phrases, clauses etc.). To use XML to encode information about such a substring of a text interpreted as a meaningful linguistic unit and to associate this information directly with the occurrence of the unit in the text is a straightforward idea. The basic idea is further backed by XMLs demand that XML elements have to be properly nested. This is fully concordant with standard linguistic practice: complex structures are made up from simpler structures covering substrings of the full string in a nested way. The end users of our applications are domain experts (e.g. medical doctors, engineers, ...). They are interested in getting their problems solved but they are typically neither interested nor trained in computational linguistics. Therefore the barrier to overcome before they can  use a computational linguistics or text technology system should be as low as possible. This experience has consequences for the design of the document suite. The work in the XDOC project is guided by the following design principles that have been abstracted from a number of experiments and applications with ”realistic” documents (i.a. emails, abstracts of scientiﬁc papers, technical documentation, ...): ¯ The tools shall be usable for ‘realistic’ documents. One aspect of ‘realistic’ documents is that they typically contain domain-speciﬁc tokens that are not directly covered by classical lexical categories (like noun, verb, ...). Those tokens are nevertheless often essential for the user of the document (e.g. an enzyme descriptor like EC 4.1.1.17 for a biochemist). ¯ The tools shall be as robust as possible. In general it can not be expected that lexicon information is available for all tokens in a document. This is not only the case for most tokens from ‘nonlexical’ types – like telephone numbers, enzyme names, material codes, ... –, even for lexical types there will always be ‘lexical gaps’. This may either be caused by neologisms or simply by starting to process documents from a new application domain with a new sublanguage. In the latter case lexical items will typically be missing in the lexicon (‘lexical gap’) and phrasal structures may not or not adequately be covered by the grammar. ¯ The tools shall be usable independently but shall allow for ﬂexible combination and interoperability. ¯ The tools shall not only be usable by developers but as well by domain experts without linguistic training. Here again XML and XSLT play a major role: XSL stylesheets can be exploited to allow different presentations of internal data and results for different target groups; for end users the internals are in many cases not helpful, whereas developers will need them for debugging.  The tools in the XDOC document suite can be grouped according to their function: ¯ preprocessing ¯ structure detection ¯ POS tagging ¯ syntactic parsing ¯ semantic analysis ¯ tools for the speciﬁc application: e.g. information extraction In all tools the results of processing is encoded with XML tags delimiting the respective piece of text. The information conveyed by the tag name is enriched with XML attributes and their resp. values. Preprocessing Tools for preprocessing are used to convert documents from a number of formats into the XML format amenable for further processing. As a subtask this includes treatment of special characters (e.g. for umlauts, apostrophes, ...). 
This paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-ofspeech. We use a morphological analysis system based on a maximum entropy model, which is independent of the domain of corpora. In this paper we show the tagging accuracy achieved by using the model and discuss problems in tagging the spontaneous speech corpus. We also show that a dictionary developed for a corpus on a certain domain is helpful for improving accuracy in analyzing a corpus on another domain. 
Previous attempts at identifying translational equivalents in comparable corpora have dealt with very large ‘general language’ corpora and words. We address this task in a specialized domain, medicine, starting from smaller non-parallel, comparable corpora and an initial bilingual medical lexicon. We compare the distributional contexts of source and target words, testing several weighting factors and similarity measures. On a test set of frequently occurring words, for the best combination (the Jaccard similarity measure with or without tf:idf weight- ing), the correct translation is ranked ﬁrst for 20% of our test words, and is found in the top 10 candidates for 50% of them. An additional reverse-translation ﬁltering step improves the precision of the top candidate translation up to 74%, with a 33% recall. 
A detailed approach has been developed for core aspects of the task of understanding a broad class of metaphorical utterances. The utterances in question are those that depend on known metaphorical mappings but that nevertheless contain elements not mapped by those mappings. A reasoning system has been implemented that partially instantiates the theoretical approach. The system, called ATT-Meta, will be demonstrated. The paper briefly indicates how the system works, and outlines some specific aspects of the system, approach and the overall project. Introduction The sentence In the far reaches of her mind, Anne believed that Kyle was having an affair1 can be analyzed as depending on metaphorical views of MIND AS PHYSICAL SPACE and IDEAS AS PHYSICAL OBJECTS (see Barnden 2001a). These views are, plausibly, familiar to typical users of English. However, it is reasonable to assume that typical users do not already have a mapping into the mental domain of the physical notion of "far reaches". Our approach to metaphor is predicated on the notion that one should, when possible, avoid constructing source-to-target mappings for such elements of a metaphorical utterance that transcend the already known mappings in the metaphorical views underlying the utterance. Instead, we advocate the use of metaphorpretence "cocoons" (reasoning spaces) where the utterance is taken as literally true. Within- 
Cross-linguistic phoneme correspondences, or metaphonemes1, can be deﬁned across languages which are relatively closely related in exactly the same way as correspondences can be deﬁned for dialects, or accents, of a single language (e.g. O’Connor, 1973; Fitt, 2001). In this paper we present the theory of metaphonemes, comparing them with traditional archi- and morphophonemes as well as with similar work using “keysymbols” done for accents of English. We describe the metaphoneme inventory deﬁned for Dutch, English and German, comparing the results for vowels and consonants. We also describe some of the unexpected information that arose from the analysis of cognate forms we undertook to ﬁnd the metaphoneme correspondences. 
This paper discusses an innovative approach to the computer assisted scoring of student responses in WebLAS (web-based language assessment system)- a language assessment system delivered entirely over the web. Expected student responses are limited production free response questions. The portions of WebLAS with which we are concerned are the task creation and scoring modules. Within the task creation module, instructors and language experts do not only provide the task input and prompt. More importantly, they interactively inform the system how and how much to score student responses. This interaction consists of WebLAS’ natural language processing (NLP) modules searching for alternatives of the provided “gold standard” (Hirschman et al, 2000) answer and asking for confirmation of score assignment. WebLAS processes and stores all this information within its database, to be used in the task delivery and scoring phases. 
This paper describes an indexing substrate for typed feature structures (ISTFS), which is an efﬁcient retrieval engine for typed feature structures. Given a set of typed feature structures, the ISTFS efﬁciently retrieves its subset whose elements are uniﬁable or in a subsumption relation with a query feature structure. The efﬁciency of the ISTFS is achieved by calculating a uniﬁability checking table prior to retrieval and ﬁnding the best index paths dynamically. 
The LinGO Redwoods initiative is a seed activity in the design and development of a new type of treebank. While several medium- to large-scale treebanks exist for English (and for other major languages), pre-existing publicly available resources exhibit the following limitations: (i) annotation is mono-stratal, either encoding topological (phrase structure) or tectogrammatical (dependency) information, (ii) the depth of linguistic information recorded is comparatively shallow, (iii) the design and format of linguistic representation in the treebank hard-wires a small, predeﬁned range of ways in which information can be extracted from the treebank, and (iv) representations in existing treebanks are static and over the (often year- or decade-long) evolution of a large-scale treebank tend to fall behind the development of the ﬁeld. LinGO Redwoods aims at the development of a novel treebanking methodology, rich in nature and dynamic both in the ways linguistic data can be retrieved from the treebank in varying granularity and in the constant evolution and regular updating of the treebank itself. Since October 2001, the project is working to build the foundations for this new type of treebank, to develop a basic set of tools for treebank construction and maintenance, and to construct an initial set of 10,000 annotated trees to be distributed together with the tools under an open-source license. 
Parsli is a ﬁnite-state (FS) parser which can be tailored to the lexicon, syntax, and semantics of a particular application using a hand-editable declarative lexicon. The lexicon is deﬁned in terms of a lexicalized Tree Adjoining Grammar, which is subsequently mapped to a FS representation. This approach gives the application designer better and easier control over the natural language understanding component than using an off-the-shelf parser. We present results using Parsli on an application that creates 3D-images from typed input. 
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita’s GLR parsing algorithm and extends it further. We also define a new grammar—PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule. So our syntactic parsing system is implemented based on rule-based approach and statistics approach. Furthermore our experiments are executed in two fields: Chinese base noun phrase identification and full syntactic parsing. And the results of these two fields are compared from three ways. The experiments prove that the extended GLR parsing algorithm with PCFG* is an efficient parsing method and a straightforward way to combine statistical property with rules. The experiment results of these two fields are presented in this paper. 1. Introduction Recently the syntactic parsing system is one of significant components in natural language processing. Many parsing methods have been developed as the development of corpus linguistics and applications of linguistics. Tomita’ GLR parsing (Tomita M., 1986, 1987) is the most general shift-reduce method of bottom-up parsing and widely used in syntactic parsing. Several methods are based on it. Lavie (Lavie A., 1996) used the GLR* parsing algorithm for spoken language system. It uses a finite-state probabilistic model to compute the action probabilities. Inui (Inui K. et al., 1997, 1998) presented a formalization of probabilistic  GLR (PGLR) parsing model which assigns a probability to each LR parsing action. To shallow parsing, many researchers have made experiments with identification of noun phrases. Abney (Abney S., 1991) used two level grammar rules to implement the noun phrase parsing through pure LR parsing algorithm. Some new methods based on GLR algorithm aim to capture action probabilities by statistics distribution and context relations. This paper combines rule approach and statistics approach simultaneously. Furthermore, based on GLR and PCFG, we present an extended GLR parsing and a new grammar PCFG* that provides the action probabilities to prune the meaningless branches in the parsing table. Our experiments are also made in two parts: Chinese base noun phrase parsing and Chinese full parsing. The former is a simplified formalization of full parsing and is relatively simpler than the latter. This paper includes four sections. Section 2 presents a brief description of rule structure system-PCFG*. Section 3 gives our extended GLR parsing algorithm and the parsing processing. Section 4 shows the experiment results of our parser including Chinese base noun phrases (baseNP) identification and Chinese full syntactic parser. The conclusions are drawn in section 5. 2. A New Grammar (PCFG*) and the Rule Structure Grammar system is one of the important pars of a parsing system. We explain it in detail in the following section. 2.1 Structure of Rules The definition of symbols in our system inherits the classifications of Penn Chinese tree-bank (Xia F., 2000). There are totally 33  part-of-speech tags, 23 syntactic tags and 26  functional tags in the Chinese tree-bank tag set.  The POS tags belong to terminal symbols, while  others belong to non-terminal symbols.  In the final rule base there are about 2000 rules  and 400 rules learned from corpus for full  parsing and base noun phrases identification  respectively. The rules have the following  format showed in table 1.  num  rule  probability frequency  
The objective of this work is to disambiguate transducers which have the following form: T = R ◦ D and to be able to apply the determinization algorithm described in (Mohri, 1997). Our approach to disambiguating T = R ◦ D consists ﬁrst of computing the composition T and thereafter to disambiguate the transducer T . We will give an important consequence of this result that allows us to compose any number of transducers R with the transducer D, in contrast to the previous approach which consisted in ﬁrst disambiguating transducers D and R to produce respectively D and R , then computing T = R ◦ D where T is unambiguous. We will present results in the case of a transducer D representing a dictionary and R representing phonological rules. Keywords: ambiguity, deterministic, dictionary, transducer. 
The paper presents a statistical approach to automatic building of translation lexicons from parallel corpora. We briefly describe the pre-processing steps, a baseline iterative method, and the actual algorithm. The evaluation for the two algorithms is presented in some detail in terms of precision, recall and processing time. We conclude by briefly presenting some of our applications of the multilingual lexicons extracted by the method described herein. Introduction The scientific and technological advancement in many domains is a constant source of new term coinage and therefore keeping up with multilingual lexicography in such areas is very difficult unless computational means are used. Translation lexicons, based on translation equivalence relation are lexical knowledge sources, which can be extracted from parallel texts (even from comparable texts), with very limited human resources. The translation lexicons appear to be quite different from the corresponding printed lexicons, meant for the human users. There are well known reasons for these differences and we will not discuss the issue here, but exactly these differences make them very useful (in spite of inherent noise content) in many computer-based applications. We will discuss some of our experiments based on automatically extracted multilingual lexicons. Most modern approaches to automatic extraction of translation equivalents rely on statistical techniques and roughly fall into two categories. The hypotheses-testing methods such as Gale and Church (1991), Smadja et al. (1996),  Tiedmann (1998), Ahrenberg (2000), Melamed (2001) etc. use a generative device that produces a list of translation equivalence candidates (TECs), extracted from corresponding segments of the parallel texts (translation units-TU), each of them being subject to an independence statistical test. The TECs that show an association measure higher than expected under the independence assumption are assumed to be translation-equivalence pairs (TEPs). The TEPs are extracted independently one of another and therefore the process might be characterized as a local maximization (greedy) one. The estimating approach such as Brown et al. (1993), Kay and Röscheisen (1993), Kupiec (1993), Hiemstra (1997) etc. is based on building from data a statistical bitext model, the parameters of which are to be estimated according to a given set of assumptions. The bitext model allows for global maximization of the translation equivalence relation, considering not individual translation equivalents but sets of translation equivalents (sometimes called assignments). There are pros and cons for each type of approach, some of them discussed in Hiemstra (1997). Our translation equivalents extraction process may be characterized as a “hypotheses testing” approach and does not need a pre-existing bilingual lexicon for the considered languages. If such a lexicon exists it can be used to eliminate spurious candidate translation equivalence pairs and thus to speed up the process and increase its accuracy. 
This paper proposes a new method for automatic acquisition of Chinese bracketing knowledge from English-Chinese sentencealigned bilingual corpora. Bilingual sentence pairs are first aligned in syntactic structure by combining English parse trees with a statistical bilingual language model. Chinese bracketing knowledge is then extracted automatically. The preliminary experiments show automatically learned knowledge accords well with manually annotated brackets. The proposed method is particularly useful to acquire bracketing knowledge for a less studied language that lacks tools and resources found in a second language more studied. Although this paper discusses experiments with Chinese and English, the method is also applicable to other language pairs. Introduction The past few years have seen a great success in automatic acquisition of monolingual parsing knowledge and grammars. The availability of large tagged and syntactically bracketed corpora, such as Penn Tree bank, makes it possible to extract syntactic structure and grammar rules automatically (Marcus 1993). Substantial improvements have been made to parse western language such as English, and many powerful models have been proposed (Brill 1993, Collins 1997). However, very limited progress has been achieved in Chinese. Knowledge acquisition is a bottleneck for real appication of Chinese parsing. While some methods have been proposed to learn syntactic knowledge from annotated Chinese corpus, most of the methods depended on the annotated or  partial annotated data(Zhou 1997, Streiter 2000). Due to the limited availbility of Chinese annotated corpus, tests of these methods are still small in scale. Although some institutions and universities currently are engaged in building Chinese tree bank, no large scale annotated corpus has been published until now because the complexity in Chinese syntatic sturcture and the difficulty in corpus annotation (Chen 1996). This paper proposes a novel method to facilitate the Chinese tree bank construction. Based on English-Chinese bilingual corpora and better English parsing, this method obtains Chinese bracketing information automatically via a bilingual model and word alignment results. The main idea of the method is that we may acquire knowledge for a language lacking a rich collection of resources and tools from a second language that is full of them. The rest of this paper is organized as follows : In the next section, a bilingual language model is introduced. Then, a bilingual parsing method supervised by English parsing is proposed in section 2. Based on the bilingual parsing, Chinese bracketing knowlege is extracted in section 3. The evaluation and discussion are given in section 4. We conclude with discussion of future work. 
Statistical methods for PP attachment fall into two classes according to the training material used: ﬁrst, unsupervised methods trained on raw text corpora and second, supervised methods trained on manually disambiguated examples. Usually supervised methods win over unsupervised methods with regard to attachment accuracy. But what if only small sets of manually disambiguated material are available? We show that in this case it is advantageous to intertwine unsupervised and supervised methods into one disambiguation algorithm that outperforms both methods used alone.1 
This paper considers several important issues for monolingual and multilingual link detection. The experimental results show that nouns, verbs, adjectives and compound nouns are useful to represent news stories; story expansion is helpful; topic segmentation has a little effect; and a translation model is needed to capture the differences between languages. Introduction In the digital era, how to assist users to deal with data explosion problem becomes emergent. News stories on the Internet contain a large amount of real-time and new information. Several attempts were made to extract information from news stories, e.g., multi-lingual multi-document summarization (Chen and Huang, 1999; Chen and Lin, 2000), topic detection and tracking (abbreviated as TDT hereafter, http://www.nist.gov/TDT), and so on. Of these, TDT, which is a long-term project, proposed many diverse applications, e.g., story segmentation (Greiff et al., 2000), topic tracking (Levow et al., 2000; Leek et al., 2002), topic detection (Chen and Ku, 2002) and link detection (Allan et al., 2000). This paper will focus on the link detection application. The TDT link detection aims to determine whether two stories discuss the same topic. Each story could discuss one or more than one topic, and the sizes of two stories compared may not be so comparable. For example, one story may contain 100 sentences and the other one may contain only 5 sentences. In addition, the stories may be represented in different  languages. These are the main challenges of this task. In this paper, we will discuss and contribute on several issues: 1. How to represent a news story? 2. How to measure the similarity of news stories? 3. How to expand a story vector using historic information? 4. How to identify the subtopics embedded in a news story? 5. How to deal with news stories in different languages? The multilingual issue was first introduced in 1999 (TDT-3), and the source languages are mainly English and Mandarin. Dictionary-based translation strategy is applied broadly. In addition, some strategies were proposed to improve the translation accuracy. Leek et al., (2002) proposed probabilistic term translation and co-occurrence statistics strategies. The algorithm of co-occurrence statistics tended to favour those translations consistent with the rest of the document. Hui et al., (2001) proposed an enhanced translation approach for improving the translation by using a parallel corpus as an additional resource. Levow et al., (2000) proposed a corpus-based translation preference. English translation candidates were sorted in an order that reflected the dominant usage in the collection. Most of these methods need extra resources, e.g., a parallel corpus. In this paper, we will try to resolve multilingual issues with the lack of extra information. Topic segmentation is a technique extensively utilized in information retrieval and automatic document summarization (Hearst et al., 1993; Nakao, 2001). The effects were shown to be valid. This paper will introduce topic  Table 1. Performance of Link Detection under Different Feature Selection Strategies (I) Similarity Threshold  0.04 0.05 0.06 0.07 0.08 0.09  0.1 0.11 0.12  All  1.6234 1.274 1.0275 0.8440 0.7245 0.6463 0.5911 0.5528 0.5268  N  0.7088 0.5547 0.4553 0.4012 0.3815 0.3743 0.3775 0.3834 0.3883  N&V  0.8152 0.6028 0.4899 0.4254 0.3922 0.3803 0.3780 0.3870 0.4002  N&J  0.6126 0.4671 0.3918 0.3624 0.3485 0.3437 0.3481 0.3628 0.3780  N&V&J 0.6955 0.5121 0.4200 0.3720 0.3498 0.3474 0.3480 0.3617 0.3795  segmentation in link detection. Several experiments will be conducted to investigate its effects. 
It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical models that analyze the distribution of words in large text corpora. According to the law of association by contiguity, the acquisition of word associations can be explained by Hebbian learning. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts. The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics. The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods. Both approaches are systematically compared and are validated on empirical data. It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects. 
One of the existing difficulties of cross-language information retrieval (CLIR) and Web search is the lack of appropriate translations of new terminology and proper names. Different from conventional approaches, in our previous research we developed an approach for exploiting Web anchor texts as live bilingual corpora and reducing the existing difficulties of query term translation. Although Web anchor texts, undoubtedly, are very valuable multilingual and wide-scoped hypertext resources, not every particular pair of languages contains sufficient anchor texts in the Web to extract corresponding translations in the language pair. For more generalized applications, in this paper we extend our previous approach by adding a phase of transitive (indirect) translation via an intermediate (third) language, and propose a transitive model to further exploit anchor-text mining in term translation extraction applications. Preliminary experimental results show that many query translations which cannot be obtained using the previous approach can be extracted with the improved approach. 1. Introduction Cross-language information retrieval (CLIR), addressing the special need where users can query in one language and retrieve relevant documents written or indexed in another language, has become an important issue in the research of information retrieval (Dumais et al.,  1996; Davis et al., 1997; Ballesteros & Croft, 1998; Nie et al., 1999). However, its application to practical Web search services has not lived up to expectations, since they suffer a major bottleneck that lacks up-to-date bilingual lexicons containing the translation of popular query terms 1 such as proper nouns (Kwok, 2001). To enable capability of CLIR, existing IR systems mostly rely on bilingual dictionaries for cross-lingual retrieval. In these systems, queries submitted in a source language normally have to be translated into a target language by means of simple dictionary lookup. These dictionary-based techniques are limited in real-world applications, since the queries given by users often contain proper nouns. Another kind of popular approaches to dealing with query translation based on corpus-based techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other (Brown et al., 1993; Dagan et al., 1993; Smadja et al., 1996). Although more reliable translation equivalents can be extracted by these techniques, the unavailability of large enough parallel corpora for various subject domains and multiple languages is still in a thorny situation. On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al. (1998). This task is more difficult due to lack of parallel correlation between document or sentence pairs. 
Most current sentence alignment approaches adopt sentence length and cognate as the alignment features; and they are mostly trained and tested in the documents with the same style. Since the length distribution, alignment-type distribution (used by length-based approaches) and cognate frequency vary signiﬁcantly across texts with diﬀerent styles, the length-based approaches fail to achieve similar performance when tested in corpora of diﬀerent styles. The experiments show that the performance in F -measure could drop from 98.2% to 85.6% when a length-based approach is trained by a technical manual and then tested on a general magazine. Since a large percentage of content words in the source text would be translated into the corresponding translation duals to preserve the meaning in the target text, transfer lexicons are usually regarded as more reliable cues for aligning sentences when the alignment task is performed by human. To enhance the robustness, a robust statistical model based on both transfer lexicons and sentence lengths are proposed in this paper. After integrating the transfer lexicons into the model, a 60% F -measure error reduction (from 14.4% to 5.8%) is observed. 
A new statistical method called “bilingual chunking” for structure alignment is proposed. Different with the existing approaches which align hierarchical structures like sub-trees, our method conducts alignment on chunks. The alignment is finished through a simultaneous bilingual chunking algorithm. Using the constrains of chunk correspondence between source language (SL)1 and target language (TL), our algorithm can dramatically reduce search space, support time synchronous DP algorithm, and lead to highly consistent chunking. Furthermore, by unifying the POS tagging and chunking in the search process, our algorithm alleviates effectively the influence of POS tagging deficiency to the chunking result. The experimental results with EnglishChinese structure alignment show that our model can produce 90% in precision for chunking, and 87% in precision for chunk alignment. Introduction We address here the problem of structure alignment, which accepts as input a sentence pair, ¡ This work was done while the author was visiting Microsoft Research Asia 1 In this paper, we take English-Chinese parallel text as example; it is relatively easy, however, to be extended to other language pairs.  and produces as output the parsed structures of both sides with correspondences between them. The structure alignment can be used to support machine translation and cross language information retrieval by providing extended phrase translation lexicon and translation templates. The popular methods for structure alignment try to align hierarchical structures like sub-trees with parsing technology. However, the alignment accuracy cannot be guaranteed since no parser can handle all authentic sentences very well. Furthermore, the strategies which were usually used for structure alignment suffer from serious shortcomings. For instance, parse-to-parse matching which regards parsing and alignment as separate and successive procedures suffers from the inconsistency between grammars of different languages. Bilingual parsing which looks upon parsing and alignment as a simultaneous procedure needs an extra ‘bilingual grammar’. It is, however, difficult to write a complex ‘bilingual grammar’. In this paper, a new statistical method called “bilingual chunking” for structure alignment is proposed. Different with the existing approaches which align hierarchical structures like sub-trees, our method conducts alignment on chunks. The alignment is finished through a simultaneous bilingual chunking algorithm. Using the constrains of chunk correspondence between source language (SL) and target language (TL), our algorithm can dramatically reduce search space, support time synchronous DP algorithm,  and lead to highly consistent chunking. Furthermore, by unifying the POS tagging and chunking in the search process, our algorithm alleviates effectively the influence of POS tagging deficiency to the chunking result. The experimental results with English- Chinese structure alignment show that our model can produce 90% in precision for chunking, and 87% in precision for chunk alignment. 
We consider here the problem of Base Noun Phrase translation. We propose a new method to perform the task. For a given Base NP, we first search its translation candidates from the web. We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed. In one method, we employ an ensemble of Naïve Bayesian Classifiers constructed with the EM Algorithm. In the other method, we use TF-IDF vectors also constructed with the EM Algorithm. Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies. 1. Introduction We address here the problem of Base NP translation, in which for a given Base Noun Phrase in a source language (e.g., ‘information ٫ age’ in English), we are to find out its possible translation(s) in a target language (e.g., ‘ ’ in Chinese). We define a Base NP as a simple and non-recursive noun phrase. In many cases, Base NPs represent holistic and non-divisible concepts, and thus accurate translation of them from one language to another is extremely important in applications like machine translation, cross language information retrieval, and foreign language writing assistance. In this paper, we propose a new method for Base NP translation, which contains two steps: (1) translation candidate collection, and (2) translation selection. In translation candidate collection, for a given Base NP in the source language, we look for its translation candidates in the target language. To do so, we use a word-to-word translation dictionary and corpus  Hang Li Microsoft Research Asia hangli@microsoft.com data in the target language on the web. In translation selection, we determine the possible translation(s) from among the candidates. We use non-parallel corpus data in the two languages on the web and employ one of the two methods which we have developed. In the first method, we view the problem as that of classification and employ an ensemble of Naïve Bayesian Classifiers constructed with the EM Algorithm. We will use ‘EM-NBC-Ensemble’ to denote this method, hereafter. In the second method, we view the problem as that of calculating similarities between context vectors and use TF-IDF vectors also constructed with the EM Algorithm. We will use ‘EM-TF-IDF’ to denote this method. Experimental results indicate that our method is very effective, and the coverage and top 3 accuracy of translation at the final stage are 91.4% and 79.8%, respectively. The results are significantly better than those of the baseline methods relying on existing technologies. The higher performance of our method can be attributed to the enormity of the web data used and the employment of the EM Algorithm. 2. Related Work 2.1 Translation with Non-parallel Corpora A straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora (e.g., Brown et al, 1993). Parallel corpora are, however, difficult to obtain in practice. To deal with this difficulty, a number of methods have been proposed, which make use of relatively easily obtainable non-parallel corpora (e.g., Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000). Within these methods, it is usually assumed that a number of translation candidates for a word or phrase are given (or can be easily collected) and the problem is focused on translation selection.  All of the proposed methods manage to find out the translation(s) of a given word or phrase, on the basis of the linguistic phenomenon that the contexts of a translation tend to be similar to the contexts of the given word or phrase. Fung and Yee (1998), for example, proposed to represent the contexts of a word or phrase with a real-valued vector (e.g., a TF-IDF vector), in which one element corresponds to one word in the contexts. In translation selection, they select the translation candidates whose context vectors are the closest to that of the given word or phrase. Since the context vector of the word or phrase to be translated corresponds to words in the source language, while the context vector of a translation candidate corresponds to words in the target language, and further the words in the source language and those in the target language have a many-to-many relationship (i.e., translation ambiguities), it is necessary to accurately transform the context vector in the source language to a context vector in the target language before distance calculation. The vector-transformation problem was not, however, well-resolved previously. Fung and Yee assumed that in a specific domain there is only one-to-one mapping relationship between words in the two languages. The assumption is reasonable in a specific domain, but is too strict in the general domain, in which we presume to perform translation here. A straightforward extension of Fung and Yee’s assumption to the general domain is to restrict the many-to-many relationship to that of many-to-one mapping (or one-to-one mapping). This approach, however, has a drawback of losing information in vector transformation, as will be described. For other methods using non-parallel corpora, see also (Tanaka and Iwasaki, 1996; Kikui, 1999, Koehn and Kevin 2000; Sumita 2000; Nakagawa 2001; Gao et al, 2001). 2.2 Translation Using Web Data Web is an extremely rich source of data for natural language processing, not only in terms of data size but also in terms of data type (e.g., multilingual data, link data). Recently, a new trend arises in natural language processing, which tries to bring some new breakthroughs to the field by effectively using web data (e.g., Brill et al, 2001).  Nagata et al (2001), for example, proposed to collect partial parallel corpus data on the web to create a translation dictionary. They observed that there are many partial parallel corpora between English and Japanese on the web, and most typically English translations of Japanese terms (words or phrases) are parenthesized and inserted immediately after the Japanese terms in documents written in Japanese. 3. Base Noun Phrase Translation Our method for Base NP translation comprises of two steps: translation candidate collection and translation selection. In translation candidate collection, we look for translation candidates of a given Base NP. In translation selection, we find out possible translation(s) from the translation candidates. In this paper, we confine ourselves to translation of noun-noun pairs from English to Chinese; our method, however, can be extended to translations of other types of Base NPs between other language pairs. 3.1 Translation Candidate Collection We use heuristics for translation candidate collection. Figure 1 illustrates the process of collecting Chinese translation candidates for an English Base NP ‘information age’ with the heuristics.  1. Input ‘information age’;  2. Consult English-Chinese word translation dictionary: ٫ information -> ֨ᖶ age -> (how old somebody is)  ֨ (historical era) (legal adult hood)  3. Compositionally create translation candidates in  ٫֨ᖶ ٫ Chinese: ;  ; ٫֨  4. Search the candidates on web sites in Chinese and  obtain the document frequencies of them (i.e., numbers  of documents containing them): ٫ 10000 ٫֨ᖶ 10 ٫֨ 0  5. Output candidates having non-zero document  frequencies and the document frequencies: ٫ 10000 ٫֨ᖶ 10  Figure 1. Translation candidate collection  3.2 Translation Selection -EM-NBC-Ensemble  We view the translation selection problem as that of classification and employ EM-NBC-Ensemble to perform the task. For the ease of explanation, we first describe the algorithm of using only EM-NBC and next extend it to that of using EM-NBC-Ensemble.  Basic Algorithm Let e~ denote the Base NP to be translated and C~  the set of its translation candidates (phrases).  Suppose that | C~ |= k . Let c~ represent a random  variable  on  ~ C  .  Let  E  denote  a  set  of  words  in  English, and C a set of words in Chinese.  Suppose that | E |= m and | C |= n . Let e  represent a random variable on E and c a random  variable on C. Figure 2 describes the algorithm.  Input:  e~  ,  ~ C  ,  contexts  containing  e~  , contexts containing all  c~  ∈  ~ C  ;  1. create a frequency vector ( f (e1), f (e2),L, f (em)), ei ∈ E,(i = 1,L, m) using contexts containing e~ ;  transforming the vector into ( fE(c1), fE (c2),L, fE(cn)), ci ∈ C, (i = 1,L, n) , using a translation dictionary and the EM algorithm;  2. for each ( c~ ∈ C~ ){ estimate with Maximum Likelihood Estimation the prior probability P(~c ) using contexts containing all c~ ∈ C~ ; create a frequency vector ( f (c1), f (c2),L, f (cn)), ci ∈C,(i =1,L, n) using contexts containing c~ ; normalize the frequency vector , yielding (P(c1 | ~c),P(c2 | c~),L, P(cn | c~)),ci ∈C,(i = 1,L,n) ; calculate the posterior probability P(c~ | D) with EM-NBC (generally EM-NBC-Ensemble), where D = ( fE (c1), f E (c2 ),L, f E (cn )), ci ∈ C, (i = 1,L, n) 3. Sort c~ ∈ C~ in descending order of P(c~ | D) ;  Output: the top sorted results  Figure 2. Algorithm of EM-NBC-Ensemble  Context Information As input data, we use ‘contexts’ in English which contain the phrase to be translated. We also use contexts in Chinese which contain the translation candidates. Here, a context containing a phrase is defined as the surrounding words within a window of a predetermined size, which window covers the  phrase. We can easily obtain the data by searching for them on the web. Actually, the contexts containing the candidates are obtained at the same time when we conduct translation candidate collection (Step 4 in Figure 1).  EM Algorithm  We define a relation between E and C  as R ⊆ E × C , which represents the links in a  translation dictionary. We further define  Γc = {e | (e, c) ∈ R}.  At Step 1, we assume that all the instances in  (f (e1), f (e2),..,f (em)) are independently generated  according to the distribution defined as:  ∑ P(e) = P(c)P(e | c)  (1)  c∈C  We estimate the parameters of the distribution by  using the Expectation and Maximization (EM)  Algorithm (Dempster et al., 1977).  E − Step M − Step  P(c | e) ←  P (c)P(e | c) ∑ P (c)P (e | c)  c∈C  P(c) ← ∑ f (e)P(c | e)  e∈ E  P(e | c) ←  f (e)P (c | e) ∑ f (e)P (c | e)  e∈ E  Figure 3. EM Algorithm  Initially, we set for all c ∈ C  P(c) = 1 , |C |  P(e  |  c)  =      |  
Tsinghua University, China  $EVWUDFW We consider here the problem of Chinese named entity (NE) identification using statistical language model(LM). In this research, word segmentation and NE identification have been integrated into a unified framework that consists of several class-based language models. We also adopt a hierarchical structure for one of the LMs so that the nested entities in organization names can be identified. The evaluation on a large test set shows consistent improvements. Our experiments further demonstrate the improvement after seamlessly integrating with linguistic heuristic information, cache-based model and NE abbreviation identification.  ,QWURGXFWLRQ 1(LGHQWLILFDWLRQ is the key technique in many applications such as information extraction, question answering, machine translation and so on. English NE identification has achieved a great success. However, for Chinese, NE identification is very different. There is no space to mark the word boundary and no standard definition of words in Chinese. The Chinese NE identification and word segmentation are interactional in nature. This paper presents a unified approach that integrates these two steps together using a class-based LM, and apply Viterbi search to select the global optimal solution. The class-based LM consists of two sub-models, namely the context model and the entity model. The context model estimates the probability of generating a NE given a certain context, and the entity model estimates the probability of a  sequence of Chinese characters given a certain kind of NE. In this study, we are interested in three kinds of Chinese NE that are most commonly used, namely person name (PER), location name (LOC) and organization name (ORG). We have also adopted a variety of approaches to improving the LM. In addition, a hierarchical structure for organization LM is employed so that the nested PER, LOC in ORG can be identified. The evaluation is conducted on a large test set in which NEs have been manually tagged. The experiment result shows consistent improvements over existing methods. Our experiments further demonstrate the improvement after integrating with linguistic heuristic information, cache-based model and NE abbreviation identification. The precision of PER, LOC, ORG on the test set is 79.86%, 80.88%, 76.63%, respectively; and the recall is 87.29%, 82.46%, 56.54%, respectively.  5HODWHG:RUN Recently, research on English NE identification has been focused on the machine-learning approaches, including hidden Markov model (HMM), maximum entropy model, decision tree and transformation-based learning, etc. (Bikel et al, 1997; Borthwick et al, 1999; Sekine et al, 1998). Some systems have been applied to real application. Research on Chinese NE identification is, however, still at its early stage. Some researches apply methods of English NE identification to Chinese. Yu et al (1997) applied the HMM approach where the NE identification is formulated as a tagging  
A parsing system returning analyses in the form of sets of grammatical relations can obtain high precision if it hypothesises a particular relation only when it is certain that the relation is correct. We operationalise this technique—in a statistical parser using a manually-developed wide-coverage grammar of English—by only returning relations that form part of all analyses licensed by the grammar. We observe an increase in precision from 75% to over 90% (at the cost of a reduction in recall) on a test corpus of naturally-occurring text. 
This paper presents the strategy and design of a highly efficient semiautomatic method for labelling the semantic features of common nouns, using semantic relationships between words, and based on the information extracted from an electronic monolingual dictionary. The method, that uses genus data, specific relators and synonymy information, obtains an accuracy of over 99% and a scope of 68,2% with regard to all the common nouns contained in a real corpus of over 1 million words, after the manual labelling of only 100 nouns. 
I present a novel approach to the determination of recurrent sound correspondences in bilingual wordlists. The idea is to relate correspondences between sounds in wordlists to translational equivalences between words in bitexts (bilingual corpora). My method induces models of sound correspondence that are similar to models developed for statistical machine translation. The experiments show that the method is able to determine recurrent sound correspondences in bilingual wordlists in which less than 30% of the pairs are cognates. By employing the discovered correspondences, the method can identify cognates with higher accuracy than the previously reported algorithms. 
ÌÑØÓÓÌÑÒÖÝÓÓØÒÕÓØØÁÑ×Ù¸ÊÒØÓÓÛÒÓÔÖ×ÔÓÐÙÝÝÖ¸ÖÛÔ×ØÓØÓÑÒÖÖÙÑ×ØÙÖ××ØÔºÑÙØØØÑØÖÑÍÓÓÓÖÓÑÒÔÒÑÐØÓÓ×Ð×ÖÙ×ÛÑÖØØÔ×ÝÞÖÐÝÐÕÓÛÞÒ×ÙÖØÓØÖ×ÖØºÙÖÒÝÝÒÖ¸ÐÓ¹ØÓÛÛÒÙ×ÖØØÙÖÓØÑØ×ÖÑÓÖÓÑÒ×ÐÔØ×ØØØÒ×ÙØÖÓÛÑÐØÓÖÒÙÖÙÑ×ÑØÐØØÖÙÑÓÒ×ÖÖØØØÒÓÚÒÞÓÒÒÓÓÖÖÒ×¹¹º ½ ÁÒØÖÓ Ù Ø ÓÒ ÁØØÑÙÔÓÕ×ÙÑØØØØ¾ÓØ´ÀÑÒÖÒÓÌÙÖÙÚ¼ÒÙ×ÓÓ×ÐÓÓØÓØÓÒÒÓ¼ÁÐÌÐÐÕÒÒÛ×ÓÐÖÖÖÒÖÖ¼ÙÙÖÙÙÑÙÐº´ÖÝ××µØ×××µÓÑÑÑÄÑÑØ×Úº××ØÔÓÌØÐÖØ×Ø×ÒÙÝ¸ÒÓÓÒÑÙÓ×ÑºÐÓÑÒØÖÛÒÒÒÔØØÛºÑÛÖÓÝ¸ØÔ×µÐÓÓÒØØØÓØÙÖ¸Ó¸ÔÖÓÙÖ××ÐÖ¸É×ÒÒ××ÑØØÒÑ¸Ð×ÒÒØ¸ÙÓ½ÞÐÐØÙÙÓÐØ¸×ÓÔÒÙØÑÒÖÙ××ÐÝØÑÖÒÙÓÖÝÑÚÓØÓÐÖÐÑÛÑØÙ×ÐØ×ÐÖÑÓÓÐÖ×Ø×ÓØÛÓØØØØ×ÝÙµ×ØÒÝÒÖÖ×Øº¸ÓÓÑØØ×ÑÖØÓÑÙÒØÚÛÐØ×Ö³ÖÝÖÌÓÓÓÑ¸ÞÓ×××ÓÔÖÔÓÒÒÐÒÒÙ×ÐÖÐ×ÖÖÓ×ÑÙÙØº×Ø´ÐÑ×ØÓÜ××Ò×ÓÖÚÒÓ¸ÓÁØ×Ó××ÙÙØÔØ×ØÚÔØÊÓÙÖÑÑØÌÒÒ´Ñ×ÑÑÙÖÖÑÒÐÒÛÒÖÙµÇººÒØ×ÓÙÐÓÒÑµÐØ×ÔÓ×ÑÑÚÓ×ÒÐ¸ØÖÓÓÒÝØÒÒÓÖØÖ×ØÙÒÔÓÞÚÒÓÖØÕÙØÓ×ÖÓÑØÖÑÓØÛÓÐ×ÒØÓÓÙ×ÒØØ×Ö¸ØÖÙ×ÑÓÙÓÒ×ÖÓÖ××ÓÜÓØØÒÝÖÛÖÕÒºÒ×ÒÚÒØÒØÝÓÒÐÓÑÔÙÝÖ×ÖÐ¹ØÓØØ×ÒÝÒÑ×ÖÑÖÐÓØÛÓÝÒÛÓÐÒËÒÒÑÐÓºÝÓºØÐÛØÐØÖÓÒØÙ´ØÛÒ×ÝÖÙÖÓÏÓÔÖÓÓÓÓÔÐØØÒÒÖØÓÒ×ÔÖÖÓÙÝÒÆÔÓÐÕÓØÓÓÝÖÝ×ÐÔÜÓÐØÙÖÑÙÝÝ×××ØÛØÖ×ÒÛÙ´´ÒÐ××Ò××ÙÙÙ×ÖÑº××ÒÓ×Ø×ÑÑÑÑØººÒÝÓÒÓÒÚÐÓÓÐÐÒÐÝÝµ×Ö×Ö¹¹¹¹¹¹¹º¹¹¹º¸º  ÑØØÛ ÓÐÖÐÙ¸ÑÞØ ØØÒÓÖÒ×Ø´ÖÌ×ÖØÓÖÑÑØØ ÖÓÓÝ×ÓÐ××ÐÙÓÒÚÛÖÒÝÑË ÒÒØØÖÙÓÛØÖ×ÚÓÓÐÒÓ¸ ×Ò½º×ºÛÓµÖ Ð××¹ ¯ ÒÌÛ×ÕÑÜÙÓÓÔØÔÖÖÖÖÖÒÓ××Ú×­×Ø¸ÖÓÒÒ×ØÒØÙØ×Ø¸ÒÝ×ÓÒÒ×ÙÕÙ×ÙÖÚ×ÖÙ×ÙÓÐÖ×ÑÐÝÚÐÓÑÝ×ÒºÒÑÝºÖÑÀÌÖ×ÓÓº Û×ÓÛÓ×ÑÝÚÒÖ«¸ØÓ×«Ó¸ÖØØÓÖÝØÓÕÖ×ÔØÙ×ØÒÖØÖÝÓÐÐ ¯ ËÚÚÙÑÒÑÒÒÕÖÖØØÙÒÞØÖÓ×ØÓÓÙ×ÙÒÒ¸ÐÑÑÐ×ØÛÒÒÒÒØØ×ØØ××Ó×Ù×ÒÑØÑØÓÓÑÖÝÕ×ÙÖÐÒ×ÝÐÞºÖ×ÐÓØÓ¹×ÒÓºÖØÒÓØËÖÓÙÒÑÒØÚ×ÙÒØÛÖÖ×ÖÖÓÙÖÓÑÐÐ ×¹¹¹¹ ÑÑÐØÛÓÒÓÓÒÞÓÙÓÖØ×ÁÐØÓÑØÐØÒÒÑÁÙÙÖØØØ×ØÑÑ×ÓØÊØºÔÓÙÓÍÛÒÒØ××ÒÒØÙÛØØÒÓÝÔÑ××ØØÒÐÓÐÖÔÐÓÓÑÒÞÝ×ÒÑÖØÖÑÕ×«Ó¸ÔÖØØÙÓÖÝÛÒÐÓØÖÖÓÖØÒÖÝÒØÓ×ÖÚÖÔÛ¹ÐØÓÝÝÓØ´ÖÒÓØÒÁÓÖÓÒÖÑÔ×Û×Û×ØÓÊ××ÖÓÐ×ØÙÙÒØµÒÓÖ×ÑÓ×ÓÓÐØØÙÓÒÙ¸ÖÙÓÑØÑÑ×ÒÙÛØÖØØÓÖÑÑÖÖ×ÚÖÖØÑÑÓÞÓØÒÓ×ÐÔÖÒÝÒÓÛÖÛØÓÞ×ºÓØÔØÓÓÖÖÓØØÝÁÑÒÙÓØØØØÕÖÖÑÒØ×ÓÓºÙºÐ¸ÓÖÙÚÚ×Ó×ÏÒÖØØÙÙÒØÝÖÖ×¹¹¹¸ ¾ ÌÁÒÖÓÑÖÑÏØ ÓÒØ Ò ÒÅÊØØÓÓ × ÓÒ ¾º½ ÇÚ ÖÚ Û Ó ÈÖÓÔÓ× Å Ø Ó Ç×ØÙÔÖ×ÔÖ×ÓÔ×Ó×ÓÛÒÑÒØ Ó ÙÖ ÓÒ½× ×Ø× Ó Ø ÓÐÐÓÛ Ò ½º ×ÅÓÑÙÐÑÖ ØÒÝØ×ÖÑØÓÖÓÒÓ ØØÐ ÒÑÐÙºØ×Ø Ö ÒÒ Ó×ÖØÑÖÙ ØØÙÓÖÒ ÓÓ ¾º ØÓ×ÐØÖÓÙÐÙÙØÑØ ÓÒÒØØÓ ÐÛÙÛ×ÓØÖ Ö××Øº ÓÒ ÛØ ÓÖ Ô×ÖÓ ÓÖÐ ×ØÒ  ¿º ×ÓÒÖÐØ ÙÒÒÐ ØØ×ÓÓØØ ÛÛ Ö ÛØØ ÓÓ ÛØÓ×Öº ×¸× ÒÒØ ÒÜØÖ Ø¹  INPUT: Set of Retrieved Documents  Document Database (Whole set of Documents)  Tokenize and Extract Nouns Calculate TFIDF for each word  Tokenize and Extract Nouns Calculate TF and DF for each word TF and DF database for each word in every document  Document Vectors of Retrieved Documents Hierarchical Clustering by Maximum Distance Algorithm  Computed in advance  Cluster Structure of Document Set  Calculate Information Gain Ratio for each word  Calculate weight of each word with TF,IDF and IGR  Calculate weight of each sentence  OUTPUT: Set of Summaries  ÙÖ ½ ÇÚ ÖÚ Û Ó ÓÙÖ Ë Ñ ÑÑØÐ×ÓÛÓØÓØØÒÒÓÓÐÙÖÖÓÓÌØØÓØÑ×ÓÒØÒÙÛ×ÛÓÖÐØÐØÒØÖÙÑÒÓ××ÓÖØ×Ö×ØÓÖØ×Ù×ÖÖÙÙÓØºÓÙØÓÒØÖÖÒÖÑ×ÐÖÓ¹¸ÆÙØÙ¹ÒÓÚÐÛÐÑÓÑÒÙ×ÖÙÓÚÐËØÛÙØ×Ò×ÕÙØÔØ×ØØ×ÒØÒÙÖÖØØÛØÔØÖÖÖØØÖ×ÖØÚÓÒÓÝ¾×½ØÛ×ØººÓ¸ØÕØÙºÓÛÐÖÛÇÓÝÙÖÖÑÌ×ÙØÒÓÐÖ×Ö×Óº×ÔÒÝ×ÒÖ×ÖÙØÓ×ØÒÌÓÑº×ÙÚÑÒÖÓÒ¸ØÓÒÜÒÌÐÔØÛÐ¹ÔÓ×ÝØÐÒÖÞÙÖÓÖÐØØÙØ×ÓÓÓÖØÓØÖØº×ÒÒÖÐÖÒØÐÝØØÙÌØÖÒÚØÐÖÒÖ×Ó×ØØÚÓØÓ×ÓÓ×ØÖØØ×ÖÙÖÖÖÒÖØÝØØØØØÙÓ×ÖÓÓØ«ØÙØÓÔØÒÖØÒÛÒÐÓÓ×ÓÓÖÖÜ¸ØÐÒØØÝÙ××ÒÙÙÒÓØ××ÒÖ×ØØ×Ø×¹¹¹¹¹ Cluster of All Documents  Cluster of Retrieved Documents  Cluster of Other Documents  Sub-Clusters ÙÖ ¾ ÐÙ×Ø Ö Ò Ê ØÖ Ú Ó ÙÑ ÒØ× ÓÒ×ÚÅØÓÖÒÑÜÓËÒÓÑØÖÞÙÔÑÑÐØÓÞ½Ò¸¸Ò½×ØÛØÛÓÒµÓÙ¸ Ñ×ÒÙÓÐÔÙÒÛØ×¹ØØ×ÐºÙÖ×ÒËØÖ ÓÑÖÒ×ÙÐÖÐ×ÓÖÐÚÖÙØÓØÝ×ÖØÚÑÖÒÑÖ´Ñ×ÌÓÓÓÒØÒÙÝÓ  ××ØÓ××ÛÁØÛÛÒÒÓÖÓÓÔØØÒÓÙÓÖÖÖÖÒÖÌØÓÕÊËÙÖÔÑÓØÙÖ¸ÙÝØ×ÖÓ¸ÐÓÒ¸ÑÑØØÛÒÒÛØØÔÙØÒÌÑÒØÓ×ØÒÖÓÝÖÓÓÓÚÖÒØÓ¾ÓÑÔ×ØÝÒØºÒÛÜ¸ÓÙÒÚ×Ö×ÛÖÙØÓÒ´ÔÓØ×ÓÁÁÁÑÖÒÐÐÒÖÒØÖÐÐºÓÐÓÙ¹ÖÛÒÝÒÖÛØÒ×Ò×ØÙ×ÓÓØØØÛµÓØØÖÒØÛÛÙÖ¸ÖÌÓÖÙØÐÛØÛ¸ÐÖÔ¬ÑØ×ÚÓØÓÒÖ×ÒØ×ØÐÒØÛÓØ´ÖÛ×ÒØÙÑÒØÓÁÒÙØÓÑÒØÝØ×ÖÒ×ÐØØØÊÓÓÓÒÓÓØÙØØÒÖÖÑ¸ÚÒÓÓµÖÖØÙÖÑØØÙÑºØÖØØ×ÔÑÑÛÝÙÑØÓºÚÖÓÖ×ÒÑÓÑÒÔÔÒ××ÐÖÓÆ«×ÒÑØÁÚÚØ××ÑÓ××ÖØÒÓÙÙ×ÙÙÔ×ÖØÖØØÓºÊÔØºÖÛÖÓ×¸ÓÚÙÓÒØÖ¸ÚÙ×ÙÖÛØØØØØÌÐ×ÒÖÓÝÑØ××ØÒÓÒºØØÖ¾ØÐÖÙÙØÐÌÛÒÙÓÖÙÚÑÛÖØÐÓÒÑØÓØÖÑÙØÙÒÓ×ÓØØÔÙ×ÓÓÖºÙØÙØØÓÒØØÖÖÖÖÒÖÒÓÒ×ÖÐØ×ÖØÖÖ×Ø¹¹¹¹º¸Ð  ¾º¾ Ì ÖÑ Ï Ø Ò × ÓÒ ÁÒ ÓÖÑ Ø ÓÒ Ò Ê Ø Ó ØÑØØÒÝÓÒÔ×ÔÔÑØ ÖÒÓÔÓØÒÒ×ÙºØÒØÖÓØ×ÓÒÒÓÑØÓÓÙÐÑÛÖÓÒØÐÝØÙØ××ºØÑØÌÖÓÓÐÒÙÛÖ××ÓØÖÓÓÖÖ×ÓØÙ¸ÖÒÑØÝØØÒÖØ×ÔÒ×ÖÑÓÓ×ÐÖÒÐÑÓÐÒÛØÖ×¹¹¹  ½º ÓÖ ÛÓÖ ÐÙ×ÓØÖÖ¸ Ò ÐØÓÙÐØ Ø ÔØ ÖØ ØÛÓÒº Ø Ó  ¾º  ÛØ×ÖÐË  Ò× ÖÖ  ÓÓ×ØØÓÒØÔÓÔÖØØÖØØÐÓÙÓØÛ×ÙÒØÓÛÑ×ÒÓÖ×ÖºÖÒØÓÖØØ×ÑÒ×Ó¸Ø×ÔÐÓÖÙÓ¬ÐÙÓÛÑØØ ÓÒÖÒØÓÝØØÒÝØØÓÓØ×ØÒ¹¹Ð  ËØØÓÙÔÛÛÛ×ÙÐØÖÒÓÓÓÙÖÐ××ÖÖÑ×ÔÖÖÖÓØÓÛ¿ÛØÖ×ÖºÒ½ÓÖÒÑÒÓÒØ×ÖÓÖÓÜÓËÐÒ××ÖØ×ÝØØÓÒÑ×ØØÓÒØØÓÓÒ×ÔÖÔÒØÒØ¸×ÔÐØØØÐ×ØÓØÛÙÖÖÖØ¸ÒÚÑ××ÙÓØØÐØÖ×ØÝÖÙÖ×ÙØØØÛÒÝÛ×ÖØÛÙÔÙÒÓÙÓÓÓÖ¹Ý×Ö¸Ö×ÒÖÓ¼¹ÐÛØÑÙÖ¸ÐÓ××ÐÐÓØÓ×¼×ÙÓÓØÖÒºØÓÓÓÒÖ×ØÓÛÔ×ÒØËÖÒ×Ø××ÙÖÓ¸ÓÚÒÐ×ÓØØÒÙ×ÙÖÒÙ¹ÑÖ×Ö×ØØÔØ×ÐÖÐÙ¹ÑÑÑÒ×ÖÐÖÖÑ×ÙÙ×Ó×ÐØØÙÓÓÙºÙ××ØÓØÖØ××ØØ×ÛÖÖØØØ×ØÙÓ×ÛÓÖÓÀØØÖÒºÒ×ÒÓÖºÖÖÖÓ×ÇºÒÛÒºØÙ×ÕÕÝÙÓÒÛÐÙÙÔÌÌØØÚÓØØØ×ÖÓÖÒÒØÓÓÖÒÓÖØØ¹¹¹¸  ÓÑØÔÓÐØ×Ó¼ÙÖ×ÝÒ¸ÖÑÙØØØØÓÒØÓÒ×Ò××ÓØÛÙÙÐÒÔ¸×ÖÓÔØÖØÖ×ÒÓÓÙØØØÖÖÛ×¹ÁºÙ×ØÓÒÒØØÖÁÖ×ÐÓÒØÙÙÖÓ×ÑÓØÒØØÓÙÒÛÖØÖ×ØØØÖÓÒÒÓÖÓÒÓ¿ÜÙØºÛÐØØØÙÓÁÓ××ÖÒØÒÒºÓØØÖÐÒÙÓÖÊØ×Ò×ÖØ¿Õ¸×ØºÙÖÛÓÙÌ×´ÒØÁ¸Ô×ÝÓØØÖÖÛÊÓØÓÒÓµÖ¹¹  ÐÙÙ×ØÖ Ö¿  C0  C1  C2  C3  AAA AAA AAA  BBB BBB FFF  DDD DEE DGG  ÏÓÖ ×  ÏÓÖ ×ØÖ ÙØ ÓÒ Ò  È ÖØ Ø ÓÒ Ó  ¾º¾º½ ÁÒ ÓÖÑ Ø ÓÒ Ò Ê Ø Ó ÌÙØÓØÓÙÐ×ÓØ¸ÖÖÌÛØÓÓÛÜÖÒØØÁÔÓØØÑÖÓÒÖÒÁÊÑÔÖÁØÐÓÙ´×ÊÓÛØÖÊÙÐÐÝÚÙ×Ñº××ØÙÒ×ÐØµ×Ù´Ø××Ò¸ÉÖÖºÐØºÐ××ÙÝÓÙØÑÝÒØÒØ××ÖØÐÐÖÓØØÒÒÙ×Ù¸ÙÐÓØÛÖÖ½ØÑÓ×ÖÓ×ÔÒÛ¿ÖÓ××µÓÛÒºÙ×ØÓÖÖÒØÓÒÁÖØÐÐØØÓÙÐÒÖÓØØÓ×ÖÛ¸ØÒÔØ×ÛÓÖÐÖÓ××ÐÐØÙ×ØÒÑÒÔÖ××ÒÙÒ×ØÒ×ØÖÝÝ×××Ö¹¹  Ò Ö´Û µ Ò´Û µ ÒØÖÓÔÝ´Û µ Ô´Û µ ÒØÖÓÔÝÔ´Û µ  Ò´Û µ ×ÔÐ Ø Ò Ó´ µ  ´½µ  ÒØÖÓÔÝ´Û µ ÒØÖÓÔÝÔ´Û µ  Ô´Û µ ÐÓ ´½ Ô´Û Ö Õ´Û µ  ¾µÔµ´ÐÓÛ  ¾  µ ´½  Ô´Û  µµ  ÒØÖÓÔÝ´Û µ  ×ÔÐ Ø Ò Ó´ µ  ÐÓ  ÓÛÒ  Ø  Ö Ø  ÛÖÒÓÖÙÕÑ´ÛÛ ÖÒÓµ¸ Û¸ÓØÖÒ× ¹ÒØ  ×¸ÙÖÖ ¹Ø×ÔÐÙ×ØÖØ ÚÖÕÙÓÐÝÒº Ý¸  ÙÖ Ó¿Ö ÓÜÐ ÑØÔÐ ¸ ÓØÐÐÓÛÁ ÒÊ ÖÚ ÐÐÙØ Ó×ÒÓ ÛÓÖ × Ò ¹  Ò Ö´  ¼µ Ò  ³Ö´  ÒÒ¼ÖµÖ´´  ¼µ ¼µÒ  Ö´  Ò¼Öµ´  ¼µº  Ò Ò Ò  ÖÖÖ´´´  ¼¼¼µµµ ¼¼¼ ½¼¼¼¿¼½  Ò Ò Ò  ÖÖÖ´´´  ¼¼¼µµµ ¼¼¼ ¼¼½ ¼¼½  ¾º¾º¾ Ï Ø Ò Ì ÖÑ× × ÓÒ ÁÒ ÓÖÑ Ø ÓÒ Ò Ê Ø Ó ÐØÖ×ÁÙÓÙØÒÒÓÒÓÓ×ÑÓØØÚ×ÖÁÖÊØÒÔÒ×ÖÙÓÖÖÐÖÓÒÐÖ×ÑÚÒÚÐÒØØÒÚÙÐ××ØÓÓ×ÐÒÖÒÙÛÛÔØÐØ×Ø×¸×ÒÖÙØÓÖÐØØÛÒ×ÖÛÔ×ÓÓÝÛØ×ØÓÒ×Ø¸Ø×ÒÖÒÔÖÁÓÐÏØØ¸ÝÓØÖÓÓ´Ó×Ò¸ÖÛØÛ¾ÊÒ¸ÓÒÛÓµØÙÒÒ×ÑÛÖÐÚ×ÛÐÓ×ØÒÚÑÒ×ÖÒÓÒÚÐÜÓØÙÙØ×ØÖÖØØÙÑÓÑÐÝØÙÙ×ÔÓ×ÑÖÑÙÔÒÔÙØ×ÑÐÓÓÙÙ×ØÓÝÒÖÒ×Ö×ÖØÝÓÓÛÖÚÙØØÚ×ÒÒÓº×ÖÖÓÑ×ÒØØÙ¸ØÒØÐÒÙÓÑÌØØ×ØÁÒºÓÒºÖÑ¸ÖÓØÖÓÝÐÖº¸ÔÊÌÒ¸ØØ×ÖØØÒØÛØØÒÖ×Ö¹ÝÙÚ×Ø×Û¸ÑØÖºÚÐ×ÐÝ×ÐÓÐÓ×ÔÙ×ÙÑÐÙØÛÒÑÐÓ×¹ÐØ×××ÒÓÙØÑØÐØØÙØºÓÝÓ×Ó×ÓÐÓÓÐÖØÐÒÒÖ¹¹¹¹¹¸  Ö´Û µ  Ò Ö´Û µ ´¾µ  ¾ × Ø´ µ  ÛÑØØÏÒÖÛÓÒØØÙÔØØÓÖØØØØØÓÕÛÓ×Ù×ÓÐÐØÐÐ´ÐÐÙÓÝÙÑØ×ØØµØÒÒÐÖÖÙ×Øº´××ÛÁØØØÓÖÖ×ÖÓÔµÐ××ÙÓ¸ÔÒÒÖÛØ×ÓØØÒ×ÓØÓºÓÓÌÒÒ¬Ò×ÐÒÐÓÙÚÒØÓÁÐÖÙØÒ×ÙÊÐØ×ÝÑÛ×ÖÙÚÖ×ØÑÒÓÐÓØÙÒØÔ×ÒÓØ¹¹º  Cluster C1  gain_r (w, C1) +  Cluster C2  gain_r (w, C2) + gain_r (w, C3) igr (w, D)  Cluster C3 w Document D  ÙÖ ÁÒØ Ö Ø ÓÒ Ó Á Ê Ú ÐÙ ×  ÛÑ× ØÒØ ØÐ´ÛÛÓÑ µÒØÓ×¸Ø ÌØÓÒ ¸ÓÛÁÓØÖ ÛØÒÖÒÁØØÊÝÔº Ó× ÓÙÑÙÒÒØ ¹  Û Ø´Û µ  Ö´Û µ ¡ Ø ´Û µ ¡ ´Ûµ ´¿µ  ¿ ÜÔ Ö Ñ ÒØ Ð Ú ÐÙ Ø ÓÒ ¿º½ ËÙÑÑ Ö Þ Ø ÓÒ Ý Ë ÒØ Ò ÜØÖ Ø ÓÒ ÛØÇÖÙÖÚ Ø ÒÑÓÑÒÙÑØØ ÓÒ×Ø×ºÔ×ÌÔ« ÖÖØ Ú×ÓÖØÒÓ¸ Û××ÙÑÓÙÛÑ× ØØÖ Þ ØÒÑÓÓÖÙ×ÖØ¹  Û×ÙÒØ ØÑ×ÒÒÒØØ ×ÐÒ×ÓÐÐÓÛÜÑØ×Öº ÓØ Ó×ÒÙÑÑ× Ö ÞÓØÒÓØÒ¸ ÛØ ÖÑ  ½º  ØÛÄ ÒØ  Ø  Ø× ÓÒÑÔØ ÓÝÖÛØÓÒÓÖ  ×ÙÑ×Ò  ÒØÑØÔ´××  ÒØ µØÒÓ  Ø º  Ú  Ö×  Ò¹  × ÑÔ´× µ  ½× Û¾  Û ÝÛ´×µ  Ø´Û µ  ÛÛÓÖ Ö× Ò×× ÒÒ Ø ÝÛÐ ´××ØµÓ Ö ÝØÛÓÖÒÙ×ÑÒ ×Öº Ó  ¾º ØÖÜÓÐÖÑØØÖÐ ÒÒØØÔØ×ÖÓÒÓÖØ ØÒ×ÒÖÐÑÐ× ØÒÛÓØÙ×ÐÑÒÒØÒØØÒ¸ ÖÓÙ×ÒÑ×ØÙÔÐÜÑÓØÖÑØ Ò×ÖØÝÓº¹  Ð ÒØÇÓØÙÒÖ× Ø×ÜÔ½Ø¼Ö ÑÛÝÓÛÒÖÓØ×Öº××Ô ÖÖ ÓÒÖÓÑÙÒ×¸ÙÒÒ ÖØ Ø ÙØÓÓÒ«¹  ¿º¾ Ú ÐÙ Ø ÓÒ Ó ËÙÑÑ Ö Þ Ø ÓÒ Ò ÁÊ Ì× × ÏÁÐÌÅÑØ×ÓØÓØÙ×ÑÓÖÌÛØÖÛÊØÙÓÓÓÚÒÖ¼ÖÖÑÓÚÚ¸ÒÙÐÒÑÒÑ¼ÐØØ×ÒØØÐ×ØÙÚ×ÒÙÖÕÐÒÙÖ×ÐÓÑÑØÚÒÖÒÙÐÖ³ÝØÑÑÙÙ×Ù×Ô´ÒÖÚØÙÛ¼½ÒØÖÑÒØØØ××ÐÒØÓØ×ÓÖØÑÔÖÖ¸Ö×Ø´×ÓØÖØÐÓÖÒÒÙÚÐÝ×ÑÙÌÐÓµÚÓ³ØÓØÝÖÛºËÒÖØ¸ÚÚ´×ØÙÖÕËÓÒÖØº×ÖÐËØµÛÌ½ÑÙÖÙØÆ×ÒºÒØÓÒ¾ØÒØ×ÐÑÓÖ½ÚÁÌØØÒÓÖÔÓØ×ÒÖµÚÒØÝÖÖ×ÖÛ´Ó×ÓØÖØØØØØÙÝÙØÙÒÓ×Ö×ÔÙÒ××ÁØ××ØÙÑØÒÙØÒÊØØÖØÖ×ÓÌØÓÓØÐÖ××µÚÓÙÝÖÒ¾×ÆÚÙÒ×»×³ÑÑËÒÙ×ØÚÒ¸ºØÖØÙÑ¸×ÝÑÐÙÌ×ÖÙØÙÐÑÖÛÝÖ×ÙÓÙÛÒ×××ÑØÒØÑ½ÑÑÖØÛÙÖÜ×ÑØØÙÓÒÖØØ´ÑØÔØØÓ×ÑÖØÐ×ÖÒÖÓÓ×ÐÖÐÝ×ÛÓËÝÑÑÙØÑÝÓÒÖÛØ×ÖÔÐ××ÙÒÑÖÖØØµÝÒØºÙ×ÑÓ××ÛÓ×Ú¸ÑÐ××ØÒÖØÖÖÙÖÑÑÔ×ÝØÌÓ¸ÛÌÚÚÇÑØ×ÑÚØÒØÒÖÖ×ØÓØËØ×ÖØÙÒÖØÑÒÖÛØÓÖÓÓÒµØÐ×¿ÒÝÑÖÙØÓÖÒÝ³ÖØ×Ñ×ÓÓ¸ººÚ½ÖÑÝÄÞÖÐÑÖÓÐ×ÒÔÌÖ×ÌÖÝÒÑÙÒÒÙ×Ö×Ô×«×ØÚÓÚÖÚÓÙÖ×ËÙÓÓÖ¸³´ÒÛØÙÖÖÖÑÒÒÛÓÖØ³ÑÑÝÒÐÒØÓÛÓÐÒÒ¸ÙÖ×ÙÕÐØÖÖ´ÑÖ½ÐÖÑØ×Ð¾Ö½ÑÑÌÙÄÓÕÓÚÒÒ×ÄØ¼Ø³ØÛÐÙÓÓ×ÑÆ×ËØ¸ØØ¼ÓÐØÓÙÐ´Ø×ØÓÓÓÒÒØÑ½ÚÒÓ¿ÒºØÖÙÓÓØÒ×ÓØØµµºÐÒÒÒ³ÝÝÝ½Ö×Ö××¹¹¹¹¹¹³ºººÐ  Relevance Judgment  Query  Relevance Judgment by Subject  Original Documents 
This paper proposes a learning and extracting method of word sequence correspondences from non-aligned parallel corpora with Support Vector Machines, which have high ability of the generalization, rarely cause over-ﬁt for training samples and can learn dependencies of features by using a kernel function. Our method uses features for the translation model which use the translation dictionary, the number of words, part-of-speech, constituent words and neighbor words. Experiment results in which Japanese and English parallel corpora are used archived 81.1 % precision rate and 69.0 % recall rate of the extracted word sequence correspondences. This demonstrates that our method could reduce the cost for making translation dictionaries. 
This paper presents a simple yet in practice very efficient technique serving for automatic detection of those positions in a partof-speech tagged corpus where an error is to be suspected. The approach is based on the idea of learning and later application of "negative bigrams", i.e. on the search for pairs of adjacent tags which constitute an incorrect configuration in a text of a particular language (in English, e.g., the bigram ARTICLE - FINITE VERB). Further, the paper describes the generalization of the "negative bigrams" into "negative n-grams", for any natural n, which indeed provides a powerful tool for error detection in a corpus. The implementation is also discussed, as well as evaluation of results of the approach when used for error detection in the NEGRA® corpus of German, and the general implications for the quality of results of statistical taggers. Illustrative examples in the text are taken from German, and hence at least a basic command of this language would be helpful for their understanding - due to the complexity of the necessary accompanying explanation, the examples are neither glossed nor translated. However, the central ideas of the paper should be understandable also without any knowledge of German. 1. Errors in PoS-Tagged Corpora The importance of correctness (error-freeness) of language resources in general and of tagged corpora in particular cannot probably be overestimated. However, the definition of what constitutes an error in a tagged corpus depends on the intended usage of this corpus. 1.1 If we consider a quite typical case of a Part-of-Speech (PoS) tagged corpus used for  training statistical taggers, then an error is defined naturally as any deviation from the regularities which the system is expected to learn; in this particular case this means that the corpus should contain neither errors in assignment PoS-tags nor ungrammatical constructions in the corpus body1, since if any of the two cases is present in the corpus, then the learning process necessarily: • gets a confused view of probability distribut- ion of configurations (e.g., trigrams) in a correct text and/or, even worse (and, alas, much more likely) • gets positive evidence also about configurations (e.g., trigrams) which should not occur as the output of tagging linguistically correct texts, while simultaneously getting less evidence about correct configurations. 1.2 If we consider PoS-tagged corpora destinated for testing NLP systems, then obviously they should not contain any errors in tagging (since this would be detrimental to the validity of results of the testing) but on the other hand they should contain a certain amount of ungrammatical constructions, in order to test the behaviour of the tested system on a realistic input. Both these cases share the quiet presupposition that the tagset used is linguistically adequate, i.e. it is sufficient for unequivocal and consistent assignment of tags to the source text2. 
Meaning shifting phenomena such as metonymy have recently attracted increasing interest of researchers. Though these phenomena have been addressed by plenty of computational methods, the impacts of cardinalities of metonymically related items have been widely ignored in all of them. Motivated by this lack of analysis, we have developed a method for representing expectations and knowledge about the cardinalities of metonymically related entities and for exploiting this information to build logical forms expressing metonymic relations, the entities related, and their cardinalities. The representation of lexically motivated knowledge is realized as an enhancement to Pustejovsky's Generative Lexicon, and the process of building logical forms takes into account overwriting of default information and mismatch of cardinality requirements. Our method enables a precise attachment of sentence complements, and it supports reference resolution in the context of metonymic expressions. 
In some contexts, well-formed natural language cannot be expected as input to information or communication systems. In these contexts, the use of grammar-independent input (sequences of uninﬂected semantic units like e.g. languageindependent icons) can be an answer to the users’ needs. However, this requires that an intelligent system should be able to interpret this input with reasonable accuracy and in reasonable time. Here we propose a method allowing a purely semantic-based analysis of sequences of semantic units. It uses an algorithm inspired by the idea of “chart parsing” known in Natural Language Processing, which stores intermediate parsing results in order to bring the calculation time down. Introduction As the mass of international communication and exchange increases, icons as a mean to cross the language barriers have come through in some speciﬁc contexts of use, where language independent symbols are needed (e.g. on some machine command buttons). The renewed interest in iconic communication has given rise to important works in the ﬁeld of Design (Aicher and Krampen, 1996; Dreyfuss, 1984; Ota, 1993), on reference books on the history and development of the matter (Frutiger, 1991; Liungman, 1995; Sassoon and Gaur, 1997), as well as newer studies in the ﬁelds of Human-Computer Interaction and Digital Media (Yazdani and Barker, 2000) or Semiotics (Vaillant, 1999). We are here particularly interested in the ﬁeld of Information Technology. Icons are now used in nearly all possible areas of human computer interaction, even ofﬁce software or operating systems. However, there are contexts where richer information has to be managed, for instance: Alternative & Augmentative Communication systems designed for the needs of speech or language im-  paired people, to help them communicate (with icon languages like Minspeak, Bliss, Commun-I-Mage); Second Language Learning systems where learners have a desire to communicate by themselves, but do not master the structures of the target language yet; Cross-Language Information Retrieval systems, with a visual symbolic input. In these contexts, the use of icons has many advantages: it makes no assumption about the language competences of the users, allowing impaired users, or users from a different linguistic background (which may not include a good command of one of the major languages involved in research on natural language processing), to access the systems; it may trigger a communication-motivated, implicit learning process, which helps the users to gradually improve their level of literacy in the target language. However, icons suffer from a lack of expressive power to convey ideas, namely, the expression of abstract relations between concepts still requires the use of linguistic communication. An approach to tackle this limitation is to try to “analyse” sequences of icons like natural language sentences are parsed, for example. However, icons do not give grammatical information as clues to automatic parsers. Hence, we have deﬁned a method to interpret sequences of icons by implementing the use of “natural” semantic knowledge. This method allows to build knowledge networks from icons as is usually done from text. The analysis method that will be presented here is logically equivalent to the parsing of a dependency grammar with no locality constraints. Therefore, the complexity of a fully recursive parsing method grows more than exponentially with the length of the input. This makes the reaction time of the system too long to be acceptable in normal use. We have now deﬁned a new parsing algorithm which stores intermediate results in “charts”, in the way chart parsers (Earley, 1970) do for natural language.  
This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classiﬁer. Previous work that involves the gathering of information from the whole document often uses a secondary classiﬁer, which corrects the mistakes of a primary sentencebased classiﬁer. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC-6 and MUC-7 test data. 
We describe an in-depth study of using a dictionary (WordNet) and web search engines (Altavista, MSN, and Google) to boost the performance of an automated question answering system, Webclopedia, in answering definition questions. The results indicate applying dictionary and web-based answer reranking together increase the performance of Webclopedia on a set of 102 TREC-10 definition questions by 25% in mean reciprocal rank score and 14% in finding answers in the top 5. 
This paper describes LINGUA - an architecture for text processing in Bulgarian. First, the pre-processing modules for tokenisation, sentence splitting, paragraph segmentation, partof-speech tagging, clause chunking and noun phrase extraction are outlined. Next, the paper proceeds to describe in more detail the anaphora resolution module. Evaluation results are reported for each processing task. 
We present in this paper a method for achieving in an integrated way two tasks of topic analysis: segmentation and link detection. This method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches. We report an evaluation of our method for segmentation on two corpora, one in French and one in English, and we propose an evaluation measure that specifically suits that kind of systems. 
 GERARD KEMPEN Cognitive Psychology Unit Leiden University, and Max Planck Institute, Nijmegen/NL kempen@fsw.leidenuniv.nl  Abstract We present a quantitative model of word order and movement constraints that enables a simple and uniform treatment of a seemingly heterogeneous collection of linear order phenomena in English, Dutch and German complement constructions (Wh-extraction, clause union, extraposition, verb clustering, particle movement, etc.). Underlying the scheme are central assumptions of the psycholinguistically motivated Performance Grammar (PG). Here we describe this formalism in declarative terms based on typed feature unification. PG allows a homogenous treatment of both the within- and between-language variations of the ordering phenomena under discussion, which reduce to different settings of a small number of quantitative parameters. 1. Introduction We propose a quantitative model for expressing word order and movement constraints that enables a simple and uniform treatment of a heterogeneous collection of linear ordering phenomena in English, Dutch and German complement structures. Underlying the scheme are central tenets of the psycholinguistically motivated Performance Grammar (PG) formalism, in particular the assumption that linear order is realized at a late stage of the grammatical encoding process. The model is described here in declarative terms based on typed feature unification. We show that both the within- and between-language variations of the ordering phenomena under scrutiny reduce to differences between a few numerical parameters. The paper is organized as follows. In Section 2, we sketch PG's hierarchical structures. Section 3, the kernel of the paper, describes the linearization and movement model. In Section 4, we turn to central word order phenomena in the three target languages. Section 5, finally, contains some conclusions.  2. Hierarchical structure in PG PG's hierarchical structures consist of unordered trees ('mobiles') composed out of elementary building blocks called lexical frames. These are 3-tiered mobiles assembled from branches called segments. The top layer of a frame consists of a single phrasal node (the 'root'; e.g. Sentence, Noun Phrase, ADJectival Phrase, Prepositional Phrase), which is connected to one or more functional nodes in the second layer (e.g., SUBJect, HeaD). At most one exemplar of a functional node is allowed in the same frame. Every functional node dominates exactly one phrasal node ('foot') in the third layer, except for HD which immediately dominates a lexical (part-of-speech) node. Each lexical frame is 'anchored' to exactly one lexical item: a lemma (printed below the lexical node serving as the frame's HeaD). A lexical frame encodes the word category (part of speech), subcategorization features, and morphological diacritics (person, gender, case, etc.) of its lexical anchor (cf. the elementary trees of Tree Adjoining Grammar (TAG; e.g. Joshi & Schabes, 1997). Associated with every categorial node (i.e., lexical or phrasal node) is a feature matrix, which includes two types of features: agreement features (not to be discussed here; see Kempen & Harbusch, forthcoming) and topological features. The latter play a central role in the linear ordering mechanism. Typed feature unification of topological features takes place whenever a phrasal foot node of a lexical frame is replaced (substituted for) by a lexical frame. Substitution is PG's sole composition operation. Substitution involves unification of the feature matrices that are associated with the substituted phrasal foot node and the root node of the substituting lexical frame. Substitution gives rise to the derivation tree of a well-formed syntactic structure iff the phrasal foot node of all obligatory segments of each lexical frame successfully unifies  with the root of another frame. The tree in Figure 1 is well-formed because the MODifier segments are not obligatory. S  S UBJ HD C M P  M OD  NP  v  S  k n ow  NP  S  A P |PP| S  HD  S UBJ HD DOB J M OD  pro  NP  v  N P A P |PP| S  we  hate  NP  NP  HD  HD  n  n  Dana  Kim  Figure 1. Simplified lexical frames underlying the  sentences We know Dana hates Kim and Kim we know Dana hates (example from Sag & Wasow,1999). Or-  der of branches is arbitrary. Filled circles denote sub-  stitution. (The feature matrices unified as part of the substitution operations are not shown.)  3. Linear structure in PG The above-mentioned topological features are associated with the phrasal root nodes of lexical frames. Their value is a feature matrix specifying a 'topology', that is, a one-dimensional array of leftto-right slots. In this paper we will only be concerned with topological features associated with Snodes. They serve to assign a left-to-right order to the segments (branches) of verb frames (i.e. lexical frames specifying the major constituents of clauses). On the basis of empirical-linguistic arguments (which we cannot discuss here), we propose that S-topologies of English, Dutch and German contain exactly nine slots: E F1 F2 F3 M1 M2 M3 M4 E1 E2 D/G F1 M1 M2 M3 M4 M5 M6 E1 E2 The slots labeled Fi make up the Forefield (from Ger. Vorfeld); the Mj slots belong to the Midfield (Mittelfeld); the Ek's define the Endfield (Nachfeld; terms adapted from traditional German grammar; cf. Kathol, 2000). Table 1 illustrates which clause constituents select which slot as their 'landing site'. Notice, in particular, that the placement conditions refer not only to the grammatical function fulfilled by a constituent but also to its shape. For instance, while the Direct Object takes M3 as its default landing site, it selects F1 if it is a Wh-phrase or carries focus, and M2 if it is a personal pronoun (it). In terms of Figure 1, if Kim carries focus, it  may occupy slot F1 of the topology associated with the complement clause headed by hate.  Table 1. Examples of topology slot fillers (English). MODifier constituents are not shown. Precedence between constituents landing in the same slot is marked by "<".  Slot  Filler  F1 Declarative main clause: Topic, Focus (one  constituent only)  Interrogative main clause: Wh-constituent.  Complement clause: Wh-constituent (includ-  ing CoMPlementizeR whether/if)  F2 Complement clause: CoMPLementizeR that  F3 Subject (iff non-Wh)  M1 Pre-INFin. to < HeaD (oblig.) < PaRTicle  M2 Direct OBJect (iff personal pronoun)  Interrogative main clause: SUBJect (iff  non-Wh); SUBJ < DOBJ  M3 Indirect OBJect < Direct OBJect (non-Wh)  M4 PaRTicle  E1 Non-finite CoMPlement of 'Verb Raiser'  E2 Non-finite CoMP of 'VP Extraposition' verb  Finite CoMPlement clause  How is the Direct Object NP Kim 'extracted' from the subordinate clause and 'moved' into the main clause? Movement of phrases between clauses is due to lateral topology sharing. If a sentence contains more than one verb, each of the verb frames concerned instantiates its own topology. This applies to verbs of any type, whether main, auxiliary or copula. In such cases, the topologies are allowed to share identically labeled lateral (i.e. left- and/or right-peripheral) slots, conditionally upon several restrictions to be explained shortly. After two slots have been shared, they are no longer distinguishable; in fact, they are the same object. In the example of Figure 1, the embedded topology shares its F1 slot with the F1 slot of the matrix clause. This is indicated by the dashed borders of the bottom F1 slot:  F•1  F3 M1 we know  E• 2  ↑  ⇑  Kim Dana hates  In sentence generation, the overt surface order of a sentence is determined by a Read-out module that traverses the hierarchy of topologies in left-toright, depth-first manner. Any lexical item it 'sees' in a slot, is appended to the output string. E.g., Kim is seen while the Reader scans the matrix topology rather than during its traversal of the embedded to-  pology. See Figure 2 for the ordered tree corresponding to Kim we know Dana hates1. S  DOBJ  SUBJ  HD  CMP  F1 NP Kim  F3 NP we  M1 v know  E2 S  SUBJ  HD  F3 NP  M1 v  Dana  hat e  Figure 2. Fronting of Direct Object NP Kim due to pro-  motion (cf. Figure 1). Rectangles represent (part of) the  topologies associated with the verb frames.  The number of lateral slots an embedded topol-  ogy shares with its upstairs neighbor is determined  by the parameters LS (left-peripherally shared area)  and RS (right-hand share). The two laterally shared  areas are separated by a non-shared central area.  The latter includes at least the slot occupied by the  HeaD of the lexical frame (i.e., the verb) and usu-  ally additional slots. The language-specific pa-  rameters LS and RS are defined in the lexical en-  tries of complement-taking verbs, and dictate how  (part of) the feature structure associated with the  foot of S-CMP-S segments gets instantiated. For  instance, the lexical entry for know (Figure 1)  states that LS=1 if the complement clause is finite  and declarative. This causes the two S-nodes of the  CoMPlement segment to share one left-peripheral  slot, i.e. F1. If the complement happens to be inter-  rogative (as in We know who Dana hates), LS=0,  implying that the F1 slots do not share their con-  tents and who cannot 'escape' from its clause.  In the remainder of this Section we present a  rule system for lateral topology that is couched in a  typed feature logic and uses HPSG terminology.  The system deals with a broad variety of movement  phenomena in English, Dutch and German.  We define a clausal topology as a list of slot  types serving as the value of the topology ("TPL")  feature associated with S-nodes:  S [TPL 〈F1t,F2t,F3t,M1t,M2t,M3t,M4t,E1t,E2t〉]  1The value of a TPL feature may be a disjunctive set of alternative topologies rather than a single topology. See the CMP-S node of Figure 3 for an example. As for syntactic parsing, in Harbusch & Kempen (2000) we describe a modified ID/LP parser that can compute all alternative hierarchical PG structures licensed by an input string. We show that such a parser can fill the slots of the topologies associated with any such structure in polynomial time.  for English, and  S [TPL 〈F1t,M1t,M2t,M3t,M4t,M5t,M6t,E1t,E2t〉]  for Dutch and German. Slot types are defined as  attributes that take as value a non-branching list of  lemmas or phrases (e.g. SUBJect-NP, CoMPle-  ment-S or HeaD-v). They are initialized with the  value empty list, denoted by "〈〉" (e.g., [F1t F1 〈〉]).  Lists of segments can be combined by the ap- pend operation, represented by the symbol " ⊕". The expression "L1 ⊕L2" represents the list com-  posed of the members of L1 followed by the mem-  bers of L2. We assume that L2 is non-empty. If L1 is the empty list, "L1 ⊕L2" evaluates to L2. Slot  types may impose constraints on the cardinality  (number of members) of the list serving as its  value. Cardinality constraints are expressed as sub-  scripts of the value list. E.g., the subscript "c=1" in [F1t F1 〈〉c=1] states that the list serving as F1's value should contain exactly one member. Cardinality  constraints are checked after all constituents that  need a place have been appended.  Depending on the values of sharing parameters  LS and RS, the list can be divided into a left area  (comprising zero or more slot types), the central  area (which includes at least one slot for the HeaD  verb), and the right area (possibly empty). Topol-  ogy sharing is licensed exclusively to the lateral  areas. LS and RS are set to zero by default; this ap-  plies to the topologies of main clauses and adver-  bial subclauses. The root S of a complement clause  obtains its sharing parameter values from the foot  of the S-CMP-S segment belonging to the lexical  frame of its governing verb. For example, the lexi-  cal entry for know states that the complement of  this verb should be instantiated with LS=1 if the  clause type (CTYP) of the complement is declara-  tive. This causes the first member of the topologies  associated with the S-nodes to receive a corefer-  ence tag (indicated by boxed numbers):  [ ] S TPL 1 F1, F2,...,E2  CMP  S  CTPTLYP  
Hawthorne, NY 10532, USA {jchai@us.ibm.com}  Abstract To support context-based multimodal interpretation in conversational systems, we have developed a semantics-based representation to capture salient information from user inputs and the overall conversation. In particular, we present three unique characteristics: finegrained semantic models, flexible composition of feature structures, and consistent representation at multiple levels. This representation allows our system to use rich contexts to resolve ambiguities, infer unspecified information, and improve multimodal alignment. As a result, our system is able to enhance understanding of multimodal inputs including those abbreviated, imprecise, or complex ones. 
We profile the occurrence of clausal extraposition in corpora from different domains and demonstrate that extraposition is a pervasive phenomenon in German that must be addressed in German sentence realization. We present two different approaches to the modeling of extraposition, both based on machine learned decision tree classifiers. The two approaches differ in their view of the movement operation: one approach models multi-step movement through intermediate nodes to the ultimate target node, while the other approach models one-step movement to the target node. We compare the resulting models, trained on data from two domains and discuss the differences between the two types of models and between the results obtained in the different domains. Introduction Sentence realization, the last stage in natural language generation, derives a surface string from a more abstract representation. Numerous complex operations are necessary to produce fluent output, including syntactic aggregation, constituent ordering, word inflection, etc. We argue that for fluent output from German sentence realization, clausal extraposition needs to be included. We show how to accomplish this task by applying machine learning techniques. A comparison between English and German illustrates that it is possible in both languages to  extrapose clausal material to the right periphery of a clause, as the following examples show: Relative clause extraposition: English: A man just left who had come to ask a question. German: Der Mann ist gerade weggegangen, der gekommen war, um eine Frage zu stellen. Infinitival clause extraposition: English: A decision was made to leave the country. German: Eine Entscheidung wurde getroffen, das Land zu verlassen. Complement clause extraposition: English: A rumor has been circulating that he is ill. German: Ein Gerücht ging um, dass er krank ist. Unlike obligatory movement phenomena such as Wh-movement, extraposition is subject to pragmatic variability. A widely-cited factor influencing extraposition is clausal heaviness; in general, extraposition of heavy clauses is preferred over leaving them in place. Consider the following example from the technical domain: German: Es werden Datenstrukturen verwendet, die für die Benutzer nicht sichtbar sind. English: Data structures are used which are not visible to the user. This perfectly fluent sentence contains an extraposed relative clause. If the relative clause is left in place, as in the following example, the result is less fluent, though still grammatical:  ? Es werden Datenstrukturen, die für die Benutzer nicht sichtbar sind, verwendet. Data structures which are not visible to the users are used. Table 1 presents a quantitative analysis of the frequency of extraposition in different corpora in both English and German. This analysis is based on automatic data profiling using the NLPWin system (Heidorn 2000). The technical manual corpus consists of 100,000 aligned English-German sentence pairs from Microsoft technical manuals. The Encarta corpora consist of 100,000 randomly selected sentences from the Encarta encyclopedia in both English and German. The output of the parser was post-processed to identify relative clauses (RELCL), infinitival clauses (INFCL), and complement clauses (COMPCL) that have been moved from a position adjacent to the term they modify. According to this data profile, approximately one third of German relative clauses are extraposed in technical writing, while only 0.22% of English relative clauses are extraposed in the corresponding sentence set. The high number of extraposed relative clauses in  German is corroborated by numbers from the German hand-annotated NEGRA corpus. In NEGRA, 26.75% of relative clauses are extraposed. Uszkoreit et al. (1998) report 24% of relative clauses being extraposed in NEGRA, but their number is based on an earlier version of NEGRA, which is about half the size of the current NEGRA corpus. We also used the NEGRA corpus to verify the accuracy of our data profiling with NLPWin. These results are presented in Table 2. We only took into account sentences that received a complete parse in NLPWin. Of the 20,602 sentences in NEGRA, 17,756 (86.19%) fell into that category. The results indicate that NLPWin is sufficiently reliable for the identification of relative clauses to make our conclusions noteworthy and to make learning from NLPWin-parsed data compelling. Extraposition is so rare in English that a sentence realization module may safely ignore it and still yield fluent output. The fluency of sentence realization for German, however, will suffer from the lack of a good extraposition mechanism.  RELCL INFCL COMPCL  German technical manuals 34.97% 3.2% 1.50%  English technical manuals 0.22% 0.53% 0.00%  German Encarta 18.97% 2.77% 2.54%  English Encarta 0.30% 0.33% 0.15%  Table 1: Percentage of extraposed clauses in English and German corpora  Relative  clause  identification overall  Recall  Precision  94.55  93.40  Identification  of  extraposed relative clauses  Recall  Precision  74.50  90.02  Identification of non-  extraposed relative clauses  Recall  Precision  94.64  87.76  Table 2: NLPWin recall and precision for relative clauses on the NEGRA corpus  This evidence makes it clear that any serious sentence realization component for German needs to be able to produce extraposed relative clauses in order to achieve reasonable fluency. In the German sentence realization module, code-named Amalgam (Gamon et al. 2002, Corston-Oliver et al. 2002), we have successfully implemented both extraposition models as described here.  
wxyAzA{%|G}%z Y6¦¦f¦¦3c66E666¦Y66(E6EEf~cffG63cccEE3¤3II¦¦%%¤YY%ccY33GIcc¦%f¦ff¦¦E3¦3EiYGEc¦¦¦f6¦%6E¨EiE3¦Acx¤¦cG%¤AG¦E£cAcYE6¦E6YfG3¦'6c%YIccEEE¦fYcI¦66f6AEx%AI¦A%cEc¦6ficI6¦x¦¤%¦cYG%E¨¤f¡Y¦Y%#Y6cEY¦YIccx63i6Ac%Y6¦I¦¦3%6cEYYc6Y%%fci(¢(E¦G¦f%'%E%''¥II%cc¦ff¤EE63c¦¦6¦xAYccE¦G¦A¦ c33f¦¦66E3%66cfcf¦¦¦3E%ccf6f¦©%%Ic§¬3­¦¤E6c3c%ª¦EEc¦ic3GYI®c3§c¦6E¥IfcxE%%¦Ecx%6¦YEY«A%c¦Iª¨'¦YcAf!I¦©cc¤¯c¨c¦33YfxYcª3%YE3¤6A¦886¤cc¦fIf¦3E3E6c%%E¦¦3c°A«¦i¦c6YEY ±¦²i³A´%µG¶·¸%³A¹µG² YG¤¤¦fx~E%3¦6Yx¦fEEA3¦%363YY'G Ycc%cc6I¤6fccI% %3%EE ¯cG6IIf3EE¦ ¦x6cc£YA 3Y3Y¦¦¤%cc%%%fAff¤GE38%¦Yfc6¦!6cc3f6cY¦¦x%f'6#E¦3Icr6E%EAf63c~E6¦I6E¤EY¡E §¦3cYE%6Y3¦Y3Yc66¤f¦E¨¤c'6666%I«%IEf!Ec6YcYIc¦3f¦AfE3f%¨6YEE¤6º¦%c3¦x¦IA66A¦G¦¦ª¦EGA6¦f%''c'Gcc%c66IA¦A3II  Y¤¤I%Y~Ef3AYY¨3G¦¦E36¦Y¦ccY6c¤Y%6¤c¦I3§%AcE6Gx6IIxE¾¦%%Grc¬¿iI¦¦c%%%¦¦E¤cY¦6G¦fcE»¦«cffG6E(Y¦%G%6#G¦f¤c¼c¦¦AA6A£%Ic¤¦«#c¨E¦Y%IEYc336fGc®¦c%#¦6c6¦cf!»EI3¦IEfc%%%6Gc¯¦£IE%¦G¦½A#¬#%3x%x3AIY3¦fic½AGx¦Yc¦6cAG¦fcfY6E3Y6cE%3¤fr8'f c¦E6YYAEcc6cI  6¦3¦Â36c3EÃ33fYÄ6Y¦E3cEÉEYfcEYÊ6%EEÇA3fY3YIY¦EÀ3EcYI¦EYc6cYfEE¦xccYx¦f¦Ic33IY6Y¦IA%E3EIY!«I E¦Á!Y6AEcE¦c¦6I¦cE!3%YfcEEÂ3GxAÃ3c3¦xÄ6IAIÅcYxI6Æ%I¦EÇf3cfYc% ÈE36¨¦#¦¦¦xc3cEEY  c''¢(6¢(6¦%¦¦YYE66fx3E%%3ffIffc%%xfccY¦33c¦c6EEGfccc©¤3'¤3cc¦%A38YYI¦3cIªË#6c66EE3¨¦¦63c¦'f¨6¦3Y¨%f3EfÂ3Y¨c'»c¦3f%Ã3fEGEc¨cÄ6'E6Gc¯ÄE3¦EcÂ3cYÌ6¦ffÃ3fI¥I¯3ÇAIEcªÄ6%Yf ¨'663fÄ63EIYY¨6ÏE¤fYff6Çf¦3¦E63x3Y6cc¤¦6ÎEY'3~c'¦Y8c¡Exf3Ñ(fYAI6Y¥6EE iY'E%cYGYfc¦cY6c6EYx3fAfÍA3¦fIxf¦c¢A6EE«6fÎ¦cc363%8¦!fA3I¦Ã%Y'¦f%'AEÂ3%¦6cfÃ3Yc%¤¦f36EÄ6'3E36IEccÄ6%cEIE¦¦ÐEE°cc3cccAGÇAf3%¦3¦3xcÂ3ccIG'¬f¦Ã3Y3~%i6%I3ÄcffAY#¡Y3¦¦3Æ6GcIYc%Y%¤Æ%¦¦ YffÇf  Ñ( Ecx¤Ã%%Ò'Ac3f3Y6EÓ¦6% c% 6cfE%¦AA¦  ¢¡ £¤G´¥²¤§¶&¦%'¹¸© "E¸¨°²µG¹)(´¥·"µ§µ§G¹¸¤§!A³#"¢$ '¨¤1ccf%E¦¦%¦¦¦AYYEEf666EcE!¦EY6fE3¦Ycc!%6¦%%¦c3fY¡~E¦AEc°%cYcY6cE¦YGIfÀYIA3cI6IA%fE¢Ifx¡«6!¦If333Yc3¡E!f6Y¤®YEE¨Y¦¦#Ic3¦3Y'#Y%i66ff%¦6!E'cc%x¡¦3YfE'xYI3¦%%6cc¡E¤G¨ccf6ccx¦¨fEc3c3IEEc¦ic¤%%f6fY3EY6cE%¢¦IG¤f66c%EI!¦¦Ef0cE3YY¨%%EAAAf°fc#3Ec6ifY3ffE¤E6f3c¤'cYE¦6fI¤#Ecfxi66f3GxEcEI%¦3'cffEE%IY66'cf3GExcYc¦Yx°YcfxfA¦Gi¦¦¦6'¦6¦c'cxG¦f3¦«%cxxcE36fEcirYc«3xfEE3YE¨E!¦%6f3fY¦%ific!#c¦6Y6cx¤E«¦Yf6E%c336fE¦6Eifc(Y3EA6cEiY6cc3%YYEf¤¦cY¦I¦6i3Y¢c6fIE3Gff¦ccA3cf6f33EIx3363c¤f6'AIEffIYEfE¦I6!33Ycf¦GA%fi¦c%3I¦!Y'I%¦¤Y'¦Y¦fAY¦63cG¦c3AYc¦EY6E2%Ix¦¦ ¤'c3¦ff¦3f¦36IEYE6EY6IEcE%A6cxfEA3%f3Y3c3c%AG6c%°¦f'6¦6If¡c%YYE3%Ex¦EfYG¦cYYcÎI¦6I636%33EY¢ccY¨%c¥IxYcc33¦¤fE%cr¦E¦6c¦c§f6x¦%%36f¦E¦%YExEf6cE6cEªEYc'¦¡¦¦Â3Y¦¦fYÃ336ff6§YY6Ä6¤¦f¦«c%cÄ6¦cc!ÏE¦cI¯¦Çf¦ccAII63YEI°3¦E!%Y®Y¤#E6Â3EcÃ3%'6cfEfYÄc'£EYY6xi6I3EÆ6¦¥c!3¦fifÆ%'Er6cÇ¢G'YI6c6¦EfA33I3f¦II36fY'¦¦%II68©c%GEcccY66¦rYif%I¦¦33E¯f¦¨'¤6fxxE3%YÂ33EffÃ38¦¦fIxf6~cÄ6xY363EE6%EcEÄ66i¤cI¤¤3ÐE%6AcYxIÇA¥ '¦¦Â3EÃ3EfÄc¦AÆ6cxGÆ%f'Çf6I¢¦3¤4YEEI¥IfY'6Ecf8f%E3fIcE¦6c6G¬¦Â3%YÃ3¦cÄ6cY¦Äic6IÃA¦%Çf¥IEcc'EAI6IAi¥3EÑ(Y¨¤E%36¦E¦G6¦6º6cccIf%GEGÂ3%¦Ã363#ÄccE¦ÆEG¦¦ÄEIYÇAY'GI¦I ¦Ycc6Y¦EEf¦ cEff!¦6f¯!¤3I%%IYE6YfEYE6(¤E¦6c66%6¬%I¦IE'666%¦6I¦6¦3«%cc¤fEcff6%%x6¦3Yf%c'xIE¦33fc%¦fcG6G36cixA¦'6YIEEE¦fYc6EcE¦f#cf%¦3%f%GYIEfEY'Irc¦Y#E'¦Y6c63 Á¦xcEE6cfEGA%Icc63IEY3YÍYfcI6®!c¦'¼¦66f%¦ccGf6E3%%663%¤c6¦IE'EAY!Àf6¦cc%6EªIi©E6IcE¦Ë¯f%6r#¦f%Yc6c¦EY%3fcE3I¨fY6©6E¦¦ccc66   Y614¿i6I¦I¥fc%¦~crcAI%IÂ3x¦E3Â3Ã3c¤Ã3¦¡Äc¦Ä66Æ6ÂÄ6¤8ÌcÆ%ÐEÐ6ÇfEÇfÐiÂ3Y¦E6Ã3ÃAYÒÄcc6Çf¤cEÆE%6c'ÄEI¦%EÇffE¢6¦Y5'I3¤¦xÂ3%Â3fÂ3Ã3cÃ3%Ã3Ä6%Äc'ÄcÄ6EÆEÆEÂ3ÉE%ÉEÉEÃ3ÇfÇfÇfcÄ6EG¢'Ä6E!4ÏEc IcÇY3¥II¦'%E16'ff¦E6Af¦%c36rcI%Y%6Â3%f6¤Ã3GGYÄ6Â3Â3Â3iÄcÃ3Ã3Ã3cEÄ6Ä6Æ%ÄcGEÇAÄiÄ6ÆExÐEÅEÃAc¦ÇfÇfÇf Gc2¦f¦Ñ(EYfx%6c¦3¦E¦6f3©¦A%¦¾EEfcY¦YYcr©E3G¦%¤Â3¨E6Ã3G6cÄc3I¦3Y6IcÆE6cYE3ÄEf'f36YÇffI3E%cE3Y'E¤3fcIIEIY¦c¥f36'~6%¬Yfc6ccf¤f%#¦%Ec%c%%¦%EfIfc3«¾icG¦AÂ3fGGE6Ã3¦°YÄ66¦EffÄ63%¦YÅEA67I6cÇfYEc¦¢%6icc¦G4fY3¦If¨¥®6E'6¡3cYf¦fI3E¦36xYc'YfI%3!f63ccYIxAIYI¦¥  G2I¦f!6Efc f%E6f%'EGIEc'f6cA¥Ic16cE E¦x6Y¦YcI»E%3¦x¥6fI¦E3E66c#c%¦ff'¤cE3A%%¤6E33º%fxf6%AE6IE¬3cfi%E¦¤Ë¦ c6%¦EEccxI¤EEI r· E¶ ´ ¹¸ (³ 6´ 89¡ @ !)"¥ACBD¤§E" F£ ¥¤§¦ G #"$H$&" ¤cIY¦%EEf¦YE§%cAYc3ci66%¯cE¦AfEY¤¦33Icº6EY%3c6cEx¬A%6¦fi%c%!EY6fi¦33cE6r6fA%fEx¦%cEc¦%AYAIªEi3AY¡¦Y¦¨3c%¦c¦636¦fc%ccfIcGE%x%E%fG6ÓxE3A¦¦(¦A3¨EYIcE¦ %(36¨xIG6xxc¦¦Y¦3fc¦33¦G¤cx6x36¦6% YY¦¦IEPR¦ffxUCf!¨66cEW#666QTY¥6¤YSV6cÎcfY¥IfcfUXc3¤36ybE6c¦W#¦¤¦caE%Y¥EY¦YA3Îx3¤'3c`¥¦GYY¦f£YcI`bII3AE¯¦Ya6Yc3YcY¥IY¦%YcI36¨YfE¤E6c3Y3Ef!Ñ(AY¤6Icccf3¦IcE3i3Ef!!£%c%xEiEcG¦3%c66x6YYicEI¦icYf§i#f3A3f%36Ì(fx36x3PRE6Ec6¤f6ErcfGQd3fc¦fEcdf¦3E'c6PV36¤f%Yf3¡Efi63eT'fh¦xc¦f3YAg#3E¨f6%¤c'6ipY¦Yc%YcI®EffffIIIcUX6f¦¦i%%%W#¦¦c%6Y¥¦i%Aff'Î36Ybx¦x¡i'I%Y'q¥6%ffEf3Efa)¦%Icfr¥A6c¦'¦3cE¦EcPV¥Ic%6fccf¤#i¦%QdcGEiEs)¦iI'¦¯'6t¥A¥I6'cuR¦'E2cGEE'g#f¦f¦Exxx6v¥EcEYYf3f¦6cwcif3c3¦66Gx#%3fIYAg Y¤EIIIx¨¦GcccEfIIIY6 E¦¦fc#¢¬ccYEc¦3'IifI¦E%Yffc63 ¼¨YY3'¦¨%6%cfE¨rG%IxfY%¦3fG6Y!Ë!Y6cfxY¦AY¦ fAEfEYIcEYºE%I¨fEYY%»Yc36¨Eff¤¦E66x6%%¦'¦¨rfcAf'G#f¦YY3¨¬cfEf¦i cI36YIcf6¦¦86EcY!¦f¨366E%EfIfcYc3f¦Y3¦YEfcY6c6 ¦fiA%E    cc¦¦¦YxfEEE%f6EG%¤%cII!AfcYI¦3xf36f6¦I%IY¤¡Ii¦¦¦%cc6EE¨¦cfi(c%%EcYI3¦r%6f¤cAxY¥I¦3xYxYcY%icc3ff¼Y¤¦¦%f63E¦¤c«#AfxEEcffEYfYcc¦fY«fE'Y%(%fYA6cc6'¤Y3c%%3cE(» fG'¨IYE#IEYEGcc!%6¦ifYIccxEYY¦Y¦Y%Eccf¦6cI3Yc¤AYEYI¦3iccIE'6%I3»cx¢G6YI'YfYI%3%cY3cxcc3¤cic%3Y#xE% %ffI¦3   Ñ( EcxiÌ( I 3Ec%A6cf¦AAI%G¥IcEx¦YcIE c¤cIYx6¦33cEIIYc¦II!YE3rAcf'fc3¦%¦EYEªE6¦I¤EÀcG'cYYII¦fA%Y¦¦%cGIc3YA3f¦®c¨Gf6¡3Y%66YAfc¦E¦AI¦c¦#IiEY¦x¦f6c¦663®¨%icEc¦¦A33fI6%Y36¦¤3YEx¦¡®f6«G%%IYYEi¦¦!6Eªcc6'E%IAi¤c3Yx  ¢¤£¦¥ §©¨  ¢£ ¥¦!" ¢£¦#$§%§©¨  ¢£% &('©)¦0$) ¢¤£¦¥ §©¨  ¢£ 1$¦#$§%§©¨  ¢£ 02)3©02)54)¦68724 9 7@7 98A '©)5BC3©02)¦D2E F¡G%42H©D%D%E FI"EPD 9 6¡Q 9 6¡B  ¥¦!" ¢£ 02)3©02)54)¦68724R4%7%)¦ST4%7%0%H A 7 H©02) 9 6¡B(UH©EPB)7 '¡)V3©02W A )544 WDX)¦F¡720 9A 72EP6¡UYE 7$4`W¡0$EPU©EP6 9a D2W¡02Scb  d H a )e3 9 7@7%)¦0%6©4 9 02) A W6©4%7%0%H A 7%)5BfH4$EP6¡Uf7 '©) D%W a@a W&(EP6¡U A W6¡g©)¦687%E@W6©48h  i @£¦p 7%WqS 9 7 A 'q7 ')r4%7%02E 6¡U @£`9 6¡BsB¡) a )$72)`E 7XEPD  EP6C7 '©)t4%7%)¦Su3 9 0 7vW0 A W86©4E@B¡)¦0wE 7 9 4 3©02)¦D2E F¡G%42H©D%D%E FxE@DXEP6(7 '©)V3©02)¦D2E F¡G%42H©D%D2E F"3 9 0@7b i ¦y©p 7%W"42H¡4%7%E 7 H7%) ¦y ¡Q  E 6x4%7%)¦S 9 6¡B"42H©D%D2E F  3 9 0@7245bw¤'¡E@4w6©W57 9 7%EPW6E@4 98a 4$WqH4)5BD%W0  EP6©4)¦0 7%EPW6 i ©pX  ip   6T)¦Ss37%Qr©0 98A¦ )$72)5B"4%7%02E 6U(7%WsEP6©BE A59 7%)  6H aPa 3©02)5D%E FxW042H©D%D%E FXb(¤'¡E@4E@4¤6©) A )544 9 0%Q  7%WeB¡E@4%7%EP6¡UHE@42'q7 '©)R3©0$)¦D2E F¡G%42H©D%D%E FTD%02WS  7 '©)4%7 9 0@7 G )6©B(3 9 0 7WD7 '©)4%7%)¦S3 9 0 75    7%W(S 9 7 A '  6H©S")¦0¤WD A ' 9 0 98A 7%)¦0$4¤&'©)¦0$)   E@4 9 6"E 6872)¦U©)¦0XU¡0$) 9 7%)¦07 ' 9 6"W6©)b¡©EP6U a )  a )$7@72)¦0E@4B¡)¦6©W572)5B¡Q4$EP6¡U a )rB¡W57b   9 7 A '¡)5B A ' 9 0 98A 7%)¦0$4 9 02)eH4$)¦Bd7%W  A W6©4%7%0%H A 77 '¡)4%72)¦Seb  ©EPSs3 a )w0%H a )54 9 02) A 02) 9 72)5B7%W(' 9 6©B a )V&sW02B¡4 98a 02) 9 BQ EP6T4%7%)¦SfD2W02Sx4I"E@4$W aP9 72)¦B 9 0 7%E A5a )54I3©02W83©)¦0X6 9 Sx)54 9 6¡BD2W02)5EPU¡6&(W0$B¡45bsgV7 '¡)¦0¤0%H a )54 9 02) A 02) 9 7%)5B7%W 7%02) 9 7 &(W02B¡4 &sE 7%' SxW02) A WS"3 a E A59 72)5B SxW0%3©'©W a WU©E A598a 4%7%0%H A 7 H©02)b( 9  a )"h8b¡¡'¡W8&s4 9 42S 98aPa 4$)$7cW¡Di0%H a )54)F7%0 98A 7%)¦BjD%02WS 9(a E@4%7XWD 9 ©W8H7¡hk©l¡l 0%H a )54¤U©)¦6©)¦0 9 72)5B(H4$E 6¡U(7 '©)72)F7 A W a@a ) A 7%E@W6¤ WC3©02W A )544m0%H a )54 9 6©B)F7%0 98A 7jS"W0%3¡'©W a WU¡E A598a A WS"3W6©)¦68724nWDc&sW02B¡I 9 g©)¦0%Qx4$EPSs3 a )`0%H a )V3 9 0$4$)¦0 & 9 4oB¡)¦g©) a W83)5Bbq¤'¡)v3 9 0$4$)¦0p7 02EP)54q7%WrS 9 7 A ' ©)$7%&s)5)¦60%H a )54 9 6©B 9 U¡EPg©)¦6  0 9 ©E A &(W0$Bb(¤'¡) S 9 7 A '©EP6¡U3©02W A )544nE@4 98A '¡E@)¦g©)5B&('©)¦67%'©)V3 9 0$4$)¦0 42H A5A )544$EPg©) a Q 9 6 98a Q¡s8)54X7 '©)V&sW02B 9 6©BsB¡) A WSs3W¡4)54 E 7t7%WfE 724ug 98a E@B A WS"3W6©)¦68724 98A5A W02B¡EP6Uq72Wi7 '©) 3 9 024$)5Bs0%H a )b ¤'¡)V3 9 0$4$)¦0¤E@4BEPg¡E@B¡)5BsEP687%W7 '¡0$)5)`BE@4%7%E 6 A 73 9 0 724X7%W 7%02) 9 7m3¡02)¦D%E FIx42H©D%D%E F 9 6©B4%7%)¦Seb(v%6872)¦0%3©02)$7%EP6¡Uq7 '©) A W0202)5423W6©BEP6¡U3 9 0 7eWDY7 '©)w0%H a )wSxW¡4%7 a QT)¦6¡BqH¡3 &sE 7 'T)F7%0 98A 7%E 6¡UvSxW0%3¡'©W a WU¡E A598a©A WSs3W6©)¦68724WD 9 U©EPg©)¦6r&sW02Bb v 6©E 7%E 98a@a Q©I7 '©)R3 9 0$4$)¦0n4 A59 6©4c7 '¡) 42H¡U¡U©)54%72)5B0%H a )n72WE@B¡)¦687%E@D%Q©W8H©6©B 9 02EP)54nW¡Dc) 98A ' 3 9 0@7be¤'¡) 9 6U a )8x@©0 98A¦ )$72)5B"42H¡4%7%02E 6U 9 77 '¡)`0%H a ) ©W8H©6©B 9 02EP)54`B¡E@4%7%EP6¡UHE@4$'©)54¤3©02)¦D2E F¡G%42H©D%D%E F"3 9 0 724¦b(¤'¡) 02)¦S 9 EP6©EP6¡USxE@BB a )R3 9 0 7cWDY7 '¡)w0%H a )E@4R7 '©)4%72)¦S 3 9 0@7boy 98A '3 9 0 7YUH©E@B¡)54c7 '©)R3 9 0$4$)¦0wB8H©02E 6¡Uq7 '©) 3©02W A )544eWD)F7%0 98A 7%E 6¡Uz&sW02BrSxW0%3¡'©W a WU©E A598a A WS"3W6©)¦687245b { 02)¦D%E F 9 6©B42H©D%D%E F 9 02)w)F7%0 98A 7%)5BqH4$EP6¡UT4$EPSs3 a ) 4%7%02E 6U S 9 7 A '©E 6¡U 3©02W A )544 )$7%&s)5)¦6 &sW02B ©W8H©6©B 9 02EP)54 9 6©Bq3©02)¦D2E F¡G%42H©D%D%E F3 9 0 724wWD7 '©)`0%H a )b ¡H©D%D%E FS 9 Q 9 D%D2) A 7c)F7%0 98A 7%)5B4%7%)¦Seb"7%)¦S3 9 0 7XE@4 U©)¦6©)¦0 9 7%)5B(¡Qx4)5|8H)¦6872E 98a©A W83©Q©EP6¡U"D%02WSq7 '©)VSxE@B¡B a ) WD7 '©)V&sW02B(&sE 7 '(7 '©)V3W¡44$E E a E 7%Q"W¡DU©W¡EP6¡U(7 '¡02W8H©U¡' EP6©4)¦0 7%EPW6©I¡B¡) a )$72E@W6 9 6©B8G W042H¡4%7%E 7 H7%EPW6b  0%H a )`E@4V4 9 EPB7%W()`D%EP0$)5BsEPDXE 7' 9 4X7 '¡)4 9 Sx) a )¦6¡U87%' 9 47 '©) a )¦6¡U87%'"WD7 '©)V3©02W A )544)5B(&sW02Bb  S 9 7 A '"E@4 98A '¡E@)¦g©)5BvEPD 9 6©BsW6 a Q"EPD 9 D%EP02)5Bs0%H a )V3©02WB8H A )84X7 '©) A W0202) A 7oSxW0%3¡'©W a WU¡E A598axA WSs3W6©)¦687245b  U¡EPg©)¦6 &sW02Bc42'©W8H a BsD%EP0$) 9 7 a ) 9 4%7XW86©)`0%H a ) 9 6©B(S 9 7 A '"W86 a Q W6©)T0%H a )b  6  0 9 ©E A B 9 7 9 4$)$7 sa¡ ' 9 0 9 42'¡E} k©l¡lk¡~ ' 9 4q©)5)¦6oH4)5Bi7%Wi7%)54%7Y7 '©) A W0202) A 7%6©)544nWDY7 '©) 4%7%)¦S"SY)¦0¦b  4%7%0 9 E U'87%D2W02& 9 02B)F¡3)¦02EPSx)¦67 9 7%E@W86 42'©W&s)5BTl¡ A W0202) A 7%6©)544nW6E@B¡)¦6872EPD%Q©EP6¡Ux4%7%)¦SY45b ¤'¡) A W0202) A 7%6©)544WD7%'©)4%7%)¦SxSx)¦0 A59 6s)`EPS"3©02Wg©)5B ¡Q 9 3¡3 a Q©EP6¡U"B¡EPD%D2)¦0$)¦687XD%EP02EP6¡Us3©W a E A E@)545b   9  a )xh8b¡¡S 98a@a 4$)$7XW¡D¤4 9 Ss3 a )`0%H a )54¡ ¢    "!#¤§$%£&¨#¤  ')(10©$  2 0©¤43! 5  
In this paper we investigate the task of automatically identifying the correct argument structure for a set of verbs. The argument structure of a verb allows us to predict the relationship between the syntactic arguments of a verb and their role in the underlying lexical semantics of the verb. Following the method described in (Merlo and Stevenson, 2001), we exploit the distributions of some selected features from the local context of a verb. These features were extracted from a 23M word WSJ corpus based on part-of-speech tags and phrasal chunks alone. We constructed several decision tree classiﬁers trained on this data. The best performing classiﬁer achieved an error rate of 33.4%. This work shows that a subcategorization frame (SF) learning algorithm previously applied to Czech (Sarkar and Zeman, 2000) is used to extract SFs in English. The extracted SFs are evaluated by classifying verbs into verb alternation classes. 
With the rapid growth of real application domains for NLP systems, there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific NLP systems. Such a toolkit should provide an interface to accept sample sentences and convert them into semantic representations so as to allow programmers to map them to domain actions. In order to reduce the workload of managing a large number of semantic forms individually, the toolkit will perform what we call semantic grouping to organize the forms into meaningful groups. In this paper, we present three semantic grouping methods: similaritybased, verb-based and category-based grouping, and their implementation in the SLUI toolkit. We also discuss the pros and cons of each method and how they can be utilized according to the different domain needs. 
This paper presents a framework for clustering in text-based information retrieval systems. The prominent feature of the proposed method is that documents, terms, and other related elements of textual information are clustered simultaneously into small overlapping clusters. In the paper, the mathematical formulation and implementation of the clustering method are brieﬂy introduced, together with some experimental results. 
We describe a mechanism for the interpretation of arguments, which can cope with noisy conditions in terms of wording, beliefs and argument structure. This is achieved through the application of the Minimum Message Length Principle to evaluate candidate interpretations. Our system receives as input a quasi-Natural Language argument, where propositions are presented in English, and generates an interpretation of the argument in the form of a Bayesian network (BN). Performance was evaluated by distorting the system’s arguments (generated from a BN) and feeding them to the system for interpretation. In 75% of the cases, the interpretations produced by the system matched precisely or almost-precisely the representation of the original arguments. 
A methodology is proposed for taking queries and requests expressed in natural language as input and answering them in charts through organizing that interaction into felicitous dialogue. Charts and graphics, as well as languages, are important modes of communication. This is especially true of those which are used frequently when people analyze huge amount of data interactively, in order to ﬁnd out its characteristics or to resolve questions about it. This paper raises the problem that in such situations the correctness of the charts depends on the context, and proposes a framework to resolve it. The core of the framework is a logical form that includes the speciﬁcations of the user’s perspective and the proper treatment of the logical form for handling utterance fragments. The framework has been implemented and conﬁrmed to be appropriate. 
There is no blank to mark word boundaries in Chinese text. As a result, identifying words is difficult, because of segmentation ambiguities and occurrences of unknown words. Conventionally unknown words were extracted by statistical methods because statistical methods are simple and efficient. However the statistical methods without using linguistic knowledge suffer the drawbacks of low precision and low recall, since character strings with statistical significance might be phrases or partial phrases instead of words and low frequency new words are hardly identifiable by statistical methods. In addition to statistical information, we try to use as much information as possible, such as morphology, syntax, semantics, and world knowledge. The identification system fully utilizes the context and content information of unknown words in the steps of detection process, extraction process, and verification process. A practical unknown word extraction system was implemented which online identifies new words, including low frequency new words, with high precision and high recall rates. 
This paper describes the right-to-left decoding method, which translates an input string by generating in right-to-left direction. In addition, presented is the bidirectional decoding method, that can take both of the advantages of left-to-right and right-to-left decoding method by generating output in both ways and by merging hypothesized partial outputs of two directions. The experimental results on Japanese and English translation showed that the right-to-left was better for Englith-to-Japanese translation, while the left-to-right was suitable for Japanese-to-English translation. It was also observed that the bidirectional method was better for English-to-Japanese translation. 
This paper presents the Natural Language Processing-based linguistic analysis tool that we have developed for Japanese as a Second Language teachers. This program, Zero Detector (ZD), aims to promote effective instruction of zero anaphora, on the basis of a hypothesis about ideal conditions for second language acquisition, by making invisible zeros visible. ZD takes Japanese written narrative discourse as input and provides the zero-specified texts and their underlying structures as output. We evaluated ZD’s performance in terms of its zero detecting accuracy. We also present an experimental report of its validity for practical use. As a result, ZD has proven to be pedagogically feasible in terms of its accuracy and its impact on effective instruction. Introduction Natural Language Processing (NLP) is an emerging technology with a variety of real-world applications. Computer-Assisted Language Learning/Teaching (CALL/CALT) is one area that NLP techniques can contribute to. Such techniques range from indexing and concordancing to morphological processing with  on-demand dictionary look-ups and syntactic processing with diagnostic error analysis, to name a few. But little work has been done on discourse-level phenomena, including anaphora. Zero anaphora or zero pronouns (henceforth zeros) are referential noun phrases (NPs) that are not overtly expressed in Japanese discourse. These NPs can be omitted if they are recoverable from a given context or relevant knowledge. The use of zeros is common in Japanese and this poses a challenge for Japanese as a Second Language (JSL) learners for their accurate comprehension and natural-sounding production of Japanese discourse with zeros. Some learners fail to understand a passage correctly because of the difficulty of identifying zeros and/or their antecedents. Other learners produce grammatically correct but still unnatural-sounding Japanese due to overuse or underuse of zeros. Yet, very few textbooks provide systematic instruction or intensive exercises to overcome these difficulties with zeros. Consequently many Japanese language teachers rely on their intuitions when explaining zeros. Intuition is a conventional tool in teaching one’s native language, but from a student’s perspective, a well-developed systematic method of instruction can be more convincing. Also from a teacher’s standpoint, such analysis will be helpful in preparing teaching materials and evaluating students’ performance.  Analysis of zeros can be divided into three phases: zero identification, zero interpretation and zero production. This paper focuses on the first phase and proposes a method of systematically identifying the presence of zeros in order that teachers might provide effective instruction of zeros, based on some pedagogical principles from relevant second language acquisition (SLA) theory. We regard teachers as primary users of the program and aim to help them enhance their instruction. We implemented the program and evaluated its potential benefits for language teachers. In Sections 1 and 2 we discuss the pedagogical assumptions from SLA theory that motivate our program design, and present the linguistic assumptions from which our heuristics were drawn. Section 3 provides an overview of our system implementation. In Section 4, we present the results of evaluation from the viewpoints of both the accuracy and the empirical validity of the program. We conclude with a discussion of possible future work.  
In this paper we show to what degree the countability of English nouns is predictable from their semantics. We found that at 78% of nouns’ countability could be predicted using an ontology of 2,710 nodes. We also show how this predictability can be used to aid non-native speakers to determine the countability of English nouns when building a bilingual machine translation lexicon. 
Extracting sentences that contain important information from a document is a form of text summarization. The technique is the key to the automatic generation of summaries similar to those written by humans. To achieve such extraction, it is important to be able to integrate heterogeneous pieces of information. One approach, parameter tuning by machine learning, has been attracting a lot of attention. This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs). To conﬁrm the method’s performance, we conduct experiments that compare our method to three existing methods. Results on the Text Summarization Challenge (TSC) corpus show that our method oﬀers the highest accuracy. Moreover, we clarify the diﬀerent features eﬀective for extracting diﬀerent document genres. 
Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classiﬁed into categories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classiﬁers are too inefﬁcient for this task. Therefore, we present a method that makes the system substantially faster. This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selection method and an efﬁcient training method. 
One of the key issues in spoken language translation is how to deal with unrestricted expressions in spontaneous utterances. This research is centered on the development of a Chinese paraphraser that automatically paraphrases utterances prior to transfer in Chinese-Japanese spoken language translation. In this paper, a pattern-based approach to paraphrasing is proposed for which only morphological analysis is required. In addition, a pattern construction method is described through which paraphrasing patterns can be eﬃciently learned from a paraphrase corpus and human experience. Using the implemented paraphraser and the obtained patterns, a paraphrasing experiment was conducted and the results were evaluated. 
To help developing a localization oriented EBMT system, an automatic machine translation evaluation method is implemented which adopts edit distance, cosine correlation and Dice coefficient as criteria. Experiment shows that the evaluation method distinguishes well between “good” translations and “bad” ones. To prove that the method is consistent with human evaluation, 6 MT systems are scored and compared. Theoretical analysis is made to validate the experimental results. Correlation coefficient and significance tests at 0.01 level are made to ensure the reliability of the results. Linear regression equations are calculated to map the automatic scoring results to human scorings. Introduction Machine translation evaluation has always been a key and open problem. Various evaluation methods exist to answer either of the two questions (Bohan 2000): (1) How can you tell if a machine translation system is “good”? And (2) How can you tell which of two machine translation systems is “better”? Since manual evaluation is time consuming and inconsistent, automatic methods are broadly studied and implemented using different heuristics. Jones (2000) utilises linguistic information such as balance of parse trees, N-grams, semantic co-occurrence and so on as indicators of translation quality. Brew C (1994) compares human rankings and automatic measures to decide the translation quality, whose criteria involve word frequency, POS tagging  distribution and other text features. Another type of evaluation method involves comparison of the translation result with human translations. Yokoyama (2001) proposed a two-way MT based evaluation method, which compares output Japanese sentences with the original Japanese sentence for the word identification, the correctness of the modification, the syntactic dependency and the parataxis. Yasuda (2001) evaluates the translation output by measuring the similarity between the translation output and translation answer candidates from a parallel corpus. Akiba (2001) uses multiple edit distances to automatically rank machine translation output by translation examples. Another path of machine translation evaluation is based on test suites. Yu (1993) designs a test suite consisting of sentences with various test points. Guessoum (2001) proposes a semi-automatic evaluation method of the grammatical coverage machine translation systems via a database of unfolded grammatical structures. Koh (2001) describes their test suite constructed on the basis of fine-grained classification of linguistic phenomena. There are many other valuable reports on automatic evaluation. All the evaluation methods show the wisdom of authors in their utilisation of available tools and resources for automatic evaluation tasks. For our localization-oriented lexicalised EBMT system an automatic evaluation module is implemented. Some string similarity criteria are taken as heuristics. Experimental results show that this method is useful in quality feedback in development of the EBMT system. Six machine translation systems are utilised to test the consistency between the automatic method and human evaluation. To avoid stochastic errors,  significance test and linear correlation are calculated. Compared with previous works, ours is special in the following ways: 1) It is developed for localisation-oriented EBMT, which demands higher translation quality. 2) Statistical measures are introduced to verify the significance of the experiments. Linear regression provides a bridge over human and automatic scoring for systems. The paper is organised as follows: First the localization-oriented lexicalised EBMT system is introduced as the background of evaluation task. Second the automatic evaluation method is further described. Both theoretical and implementation of the evaluation method are fully discussed. Then six systems are evaluated both manually and with our automatic method. Consistency between the two methods is analysed. At last before the conclusion, linear correlation and significance test validate the result and exclude the possibility of random consistency.  
An unsupervised method for word sense disambiguation using a bilingual comparable corpus was developed. First, it extracts statistically significant pairs of related words from the corpus of each language. Then, aligning pairs of related words translingually, it calculates the correlation between the senses of a first-language polysemous word and the words related to the polysemous word, which can be regarded as clues for determining the most suitable sense. Finally, for each instance of the polysemous word, it selects the sense that maximizes the score, i.e., the sum of the correlations between each sense and the clues appearing in the context of the instance. To overcome both the problem of ambiguity in the translingual alignment of pairs of related words and that of disparity of topical coverage between corpora of different languages, an algorithm for calculating the correlation between senses and clues iteratively was devised. An experiment using Wall Street Journal and Nihon Keizai Shimbun corpora showed that the new method has promising performance; namely, the applicability and precision of its sense selection are 88.5% and 77.7%, respectively, averaged over 60 test polysemous words. 
Speech dialog systems need to deal with various kinds of ill-formed speech inputs that appear in natural human-human dialog. Self-correction (or speech-repair) is a particularly problematic phenomenon. Although many ways of dealing with selfcorrection have been proposed, these have limitations in both detecting and correcting for this phenomenon. In this paper, we propose a method to overcome these problems in Japanese speech dialog. We evaluate the proposed method using our speech dialog corpus and discuss its limitations and the work that remains to be done. 
For meaning representations in NLP, we focus our attention on thematic aspects and conceptual vectors. The learning strategy of conceptual vectors relies on a morphosyntaxic analysis of human usage dictionary deﬁnitions linked to vector propagation. This analysis currently doesn’t take into account negation phenomena. This work aims at studying the antonymy aspects of negation, in the larger goal of its integration into the thematic analysis. We present a model based on the idea of symmetry compatible with conceptual vectors. Then, we deﬁne antonymy functions which allows the construction of an antonymous vector and the enumeration of its potentially antinomic lexical items. Finally, we introduce a measure which evaluates how a given word is an acceptable antonym for a term. 
In this paper, we present a rich semantic network based on a differential analysis. We then detail implemented measures that take into account common and differential features between words. In a last section, we describe some industrial applications. 
Text representation is a central task for any approach to automatic learning from texts. It requires a format which allows to interrelate texts even if they do not share content words, but deal with similar topics. Furthermore, measuring text similarities raises the question of how to organize the resulting clusters. This paper presents cohesion trees (CT) as a data structure for the perspective, hierarchical organization of text corpora. CTs operate on alternative text representation models taking lexical organization, quantitative text characteristics, and text structure into account. It is shown that CTs realize text linkages which are lexically more homogeneous than those produced by minimal spanning trees. 
We describe a method for generating sentences from “keywords” or “headwords”. This method consists of two main parts, candidate-text construction and evaluation. The construction part generates text sentences in the form of dependency trees by using complementary information to replace information that is missing because of a “knowledge gap” and other missing function words to generate natural text sentences based on a particular monolingual corpus. The evaluation part consists of a model for generating an appropriate text when given keywords. This model considers not only word n-gram information, but also dependency information between words. Furthermore, it considers both string information and morphological information. 
This paper presents a method that measures the similarity between compound nouns in diﬀerent languages to locate translation equivalents from corpora. The method uses information from unrelated corpora in diﬀerent languages that do not have to be parallel. This means that many corpora can be used. The method compares the contexts of target compound nouns and translation candidates in the word or semantic attribute level. In this paper, we show how this measuring method can be applied to select the best English translation candidate for Japanese compound nouns in more than 70% of the cases. 
 ¡£¢¥¤§¦ ©¨ ¡ Uniﬁcation grammars are known to be Turing-  equivalent; given a grammar and a word , it is  undecidable whether  . In order to ensure  decidability, several constraints on grammars, com-  monly known as off-line parsability (OLP) were  suggested. The recognition problem is decidable for  grammars which satisfy OLP. An open question is  whether it is decidable if a given grammar satisﬁes  OLP. In this paper we investigate various deﬁnitions  of OLP, discuss their inter-relations and show that some of them are undecidable.  
This paper investigates methods to automatically infer structural information from large XML documents. Using XML as a reference format, we approach the schema generation problem by application of inductive inference theory. In doing so, we review and extend results relating to the search spaces of grammatical inferences for large data set. We evaluate the result of an inference process using the concept of Minimum Message Length. Comprehensive experimentation reveals our new hybrid method to be the most eﬀective for large documents. Finally tractability issues, including scalability analysis, are discussed. 
Information extraction (IE) systems are costly to build because they require development texts, parsing tools, and specialized dictionaries for each application domain and each natural language that needs to be processed. We present a novel method for rapidly creating IE systems for new languages by exploiting existing IE systems via crosslanguage projection. Given an IE system for a source language (e.g., English), we can transfer its annotations to corresponding texts in a target language (e.g., French) and learn information extraction rules for the new language automatically. In this paper, we explore several ways of realizing both the transfer and learning processes using off-theshelf machine translation systems, induced word alignment, attribute projection, and transformationbased learning. We present a variety of experiments that show how an English IE system for a plane crash domain can be leveraged to automatically create a French IE system for the same domain. 
This paper describes to what extent deep processing may bene t from shallow processing techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad{coverage uni cation{based grammar of Spanish. Experiments show that the e ciency of the overall analysis improves signi cantly and that our system also provides robustness to the linguistic processing, while maintaining both the accuracy and the precision of the grammar. 
We present a comparative evaluation of two data-driven models used in translation selection of English-Korean machine translation. Latent semantic analysis(LSA) and probabilistic latent semantic analysis (PLSA) are applied for the purpose of implementation of data-driven models in particular. These models are able to represent complex semantic structures of given contexts, like text passages. Grammatical relationships, stored in dictionaries, are utilized in translation selection essentially. We have used k-nearest neighbor (k-NN) learning to select an appropriate translation of the unseen instances in the dictionary. The distance of instances in k-NN is computed by estimating the similarity measured by LSA and PLSA. For experiments, we used TREC data(AP news in 1988) for constructing latent semantic spaces of two models and Wall Street Journal corpus for evaluating the translation accuracy in each model. PLSA selected relatively more accurate translations than LSA in the experiment, irrespective of the value of k and the types of grammatical relationship. 
This paper proposes a new approach for text categorization, based on a feature projection technique. In our approach, training data are represented as the projections of training documents on each feature. The voting for a classification is processed on the basis of individual feature projections. The final classification of test documents is determined by a majority voting from the individual classifications of each feature. Our empirical results show that the proposed approach, Text Categorization using Feature Projections (TCFP), outperforms k-NN, Rocchio, and Naïve Bayes. Most of all, TCFP is about one hundred times faster than k-NN. Since TCFP algorithm is very simple, its implementation and training process can be done very easily. For these reasons, TCFP can be a useful classifier in the areas, which need a fast and high-performance text categorization task. Introduction An issue of text categorization is to classify documents into a certain number of pre-defined categories. Text categorization is an active research area in information retrieval and machine learning. A wide range of supervised learning algorithms has been applied to this issue, using a training data set of categorized documents. The Naïve Bayes (McCalum et al., 1998; Ko et al., 2000), Nearest Neighbor (Yang et al., 2002), and Rocchio (Lewis et al., 1996) are well-known algorithms. Among these learning algorithms, we focus  on the Nearest Neighbor algorithm. In particular, the k-Nearest Neighbor (k-NN) classifier in text categorization is one of the state-of-the-art methods including Support Vector Machine (SVM) and Boosting algorithms. Since the Nearest Neighbor algorithm is much simpler than the other algorithms, the k-NN classifier is intuitive and easy to understand, and it learns quickly. But the weak point of k-NN is too slow at running time. The main computation is the on-line scoring of all training documents, in order to find the k nearest neighbors of a test document. In order to reduce the scaling problem in on-line ranking, a number of techniques have been studied in the literature. Techniques such as instance pruning technique (Wilson et al., 2000) and projection (Akkus et al., 1996) are well known. The instance pruning technique is one of the most straightforward ways to speed classification in a nearest neighbor system. It reduces time necessary and storage requirements by removing instances from the training set. A large number of such reduction techniques have been proposed, including the Condensed Nearest Neighbor Rule (Hart, 1968), IB2 and IB3 (Aha et al., 1991), and the Typical Instance Based Learning (Zhang, 1992). These and other reduction techniques were surveyed in depth in (Wilson et al., 1999), along with several new reduction techniques called DROP1-DROP5. Of these, DROP4 had the best performance. Another trial to overcome this problem exists on feature projections. Akkus and Guvenir presented a new approach to classification based on feature projections (Akkus et al., 1996). They called their resulting algorithm k-Nearest Neighbor on Feature Projections (k-NNFP). In  this approach, the classification knowledge is represented as the sets of projections of training data on each feature dimension. The classification of an instance is based on a voting by the k nearest neighbors of each feature in a test instance. The resulting system allowed the classification to be much faster than that of k-NN and its performance were comparable with k-NN. In this paper, we present a particular implementation of text categorization using feature projections. When we applied the feature projection technique to text categorization, we found several problems caused by the special properties of text categorization problem. We describe these problems in detail and propose a new approach to solve them. The proposed system shows the better performance than k-NN and it is much faster than k-NN. The rest of this paper is organized as follows. Section 1 simply presents k-NN and k-NNFP algorithm. Section 2 explains a new approach using feature projections. In section 3, we discuss empirical results in our experiments. Section 4 is devoted to an analysis of time complexity and strong points of the new proposed classifier. The final section presents conclusions. 1. k-NN and k-NNFP Algorithm In this section, we simply describe k-NN and k-NNFP algorithm. 1.1 k-NN Algorithm As an instance-based classification method, k-NN has been known as an effective approach to a broad range of pattern recognition and text classification problems (Duda et al., 2001; Yang, 1994). In k-NN algorithm, a new input instance should belong to the same class as their k nearest neighbors in the training data set. After all the training data is stored in memory, a new input instance is classified with the class of k nearest neighbors among all stored training instances. For the distance measure and the document representation, we use the conventional vector space model in text categorization; each document is represented as a vector of term  weights, and similarity between two documents is measured by the cosine value of the angle between the corresponding vectors (Yang et al., 2002).  Let a document d with n terms (t) be represented as the feature vector:  r  r  r  r  d =< w(t1, d ), w(t2,d ),..., w(tn, d ) >  (1)  We compute the weight vectors for each document using one of the conventional TF-IDF schemes (Salton et al., 1988). The weight of term t in document d is calculated as follows:  w(t,  r d)  =  (1  +  log  tf  (t,  r d )) r  ×  log( N  /  nt  )  (2)  d  where r  r  i) w(t, d ) is the weight of term t in document d  r  ii) tf (t, d ) is the within-document Term Frequency (TF)  iii) log(N / n ) is the Inverted Document Frequency t  (IDF)  iv) N is the number of documents in the training set  v) nt is the number of training documents in which t  occurs  ∑ r vi) d =  r  w(t,  r d  )  2  is the 2-norm of vector  r d  t∈d  Given an arbitrary test document d, the k-NN classifier assigns a relevance score to each candidate category cj using the following formula:  ∑ r s(c j , d ) = r  rr cos(d ′, d ) r  (3)  d ′∈Rk (d )I D j  r where R (d ) denotes a set of the k nearest k neighbors of document d and Dj is a set of training documents in class cj.  1.2 k-Nearest Neighbor on Feature Projection (k-NNFP) Algorithm The k-NNFP is a variant of k-NN method. The main difference is that instances are projected on their features in the n-dimensional space (see figure 1) and distance between two instances is calculated according to a single feature. The  distance between two instances di and dj with regard to m-th feature tm is distm(tm(i), tm(j)) as follows:  r  r  distm (tm (i),tm ( j)) = w(tm , di ) − w(tm , d j )  (4)  wrhere tm (i) denotes m-th feature t in a instance di . The classification on a feature is done according to votes of the k-nearest neighbors of that feature in a test instance. The final classification of the test instance is determined by a majority voting from individual classification of each feature. If there are n features, this method returns n× k votes whereas k-NN method returns k votes.  2. A New Approach of Text Categorization on Feature Projections First of all, we show an example of feature projections in text categorization for more easy understanding. We then enumerate the problems to be duly considered when the feature projection technique is applied to text categorization. Finally, we propose a new approach using feature projections to overcome these problems. 2.1 An Example of Feature Projections in Text Categorization We give a simple example of the feature projections in text categorization. To simplify our description, we suppose that all documents have just two features (f1 and f2) and two categories (c1 and c2). The TF-IDF value by formula (2) is used as the weight of a feautre. Each document is normalized as a unit vector and each category has three instances: c1 = {d1, d 2 , d3} and c2 = {d 4 , d5 , d6 } . Figure 1 shows how document vectors in conventional vector space are transformed into feature projections and stored on each feature dimension. The result of feature projections on a term (or feature) can be seen as a set of weights of documents for the term. Since a term with 0.0 weight is useless, the size of the set equals to the DF value of the term.  ; XUW  ]OWUXSWU``P \OWUY\SWU`]P  [OWU[SWU`XP  ZOWU`XSWU[P YOWU`]SWUY\P  XOXUWSWUWP  XUW : kGGGGG     aGSG aGSG aG  SGGSGG XUWSGXSGX WU`]SGYSGX WU`XSGZSGX WU[SG[SGY WUY\SG\SGY WUXSG]SGY  SGGSGG WU``SG]S Y WU`]SG\S Y WU`XSG[S Y WU[SGZS X WUY\SGYS X  G GG:  G GG;  mGGGG  Figure 1. Feature representation on feature projections  2.2 Problems in Applying Feature Projections to Text Categorization There are three problems: (1) the diversity of the Document Frequency (DF) values of terms, (2) the property of using TF-IDF value of a term as the weight of the feature, and (3) the lack of contextual information.  2.2.1 The diversity of the Document Frequency values of terms Table 1 shows a distribution of the DF values of the terms in Newsgroup data set. The numerical values of Table 1 are calculated from training data set with 16,000 documents and 10,000 features chosen by feature selection. The k in fourth column means the number of nearest neighbors selected in k-NNFP; the k in k-NNFP was set to 20 in our experiments. Table 1. A distribution of the DF values of the terms in Newsgroup data set  Average DF 54.59  maximum DF 8,407  Minimum DF 4  The # of features DF < k (20) 6,489  According to Table 1, more than a half of the features have the DF values less than k (20). This result is also explained by Zipf’s law. The problem is that some features have the DF values less than k while other features have the DF values much greater than k. For a feature that has a DF value less than k, all the elements of the feature projections on the feature could and should participate for voting. In this case, the number of elements chosen for voting is less than k. For other features, only maximum k elements among the elements of the feature projections should be chosen for voting. Therefore, we need to normalize the voting ratio for each feature. As shown in formula (5), we use a proportional voting method to normalize the voting ratio.  2.2.2 The property of using TF-IDF value of a term as weight of a feature The TF-IDF value of a term is their presumed value for identifying the content of a document (Salton et al., 1983). On feature projections, elements with a high TF-IDF value for a feature become more useful classification criterions for the feature than any elements with low TF-IDF values. Thus we use only elements with TF-IDF values above the average TF-IDF value for voting. The selected elements also participate for proportional voting with the same importance as TF-IDF value of each element. The voting ratio of each catergory cj in a feature tm(i) of a test document di is calculated by the following formula:  ∑ r  (c j  ,  tm  (i))  =  r w(tm, dl ) ⋅ y(cj ,tm(l))  tm (l )∈I m  ∑r w(tm, dl )  (5)  tm (l)∈Im  In above formula, Im denotes a set of elements selected for voting and y(cj,tm (l)) ∈ {0.1} is a function; if the category for a element tm (l) is equal to cj , the output value is 1. Otherwise, the output value is 0. 2.2.3 The lack of contextual information Since each feature votes separately on feature projections, contextual information is missed. We use the idea of co-occurrence frequency for applying contextual information to our  algorithm. To calculate a co-occurrence frequency value between two terms ti and tl, we count the number of documents that include both terms. It is separately calculated in each category of training data. Finally, the co-occurrence frequency value of two terms is obtained by a maximum value among co-occurrence frequency values in each category as follows:  { } co(ti ,tl ) = max co(ti ,tl ,c j )  (6)  cj  where co(ti , tl ) denotes a co-occurrence frequency value of ti and tl, and co(ti , tl , c j ) denotes a co-occurrence frequency value of ti and tl in a category cj. TF-IDF values of two terms ti and tj, which occur in a test document d, are modified by reflecting the co-occurrence frequency value. That is, the terms with a high co-occurrence frequency value and a low category frequency value could have higher term weights as follows:  tw(ti  ,  r d)  =  w(ti  ,  r d  )  ⋅  1   +    
We present a novel disambiguation method for uniﬁcation-based grammars (UBGs). In contrast to other methods, our approach obviates the need for probability models on the UBG side in that it shifts the responsibility to simpler context-free models, indirectly obtained from the UBG. Our approach has three advantages: (i) training can be effectively done in practice, (ii) parsing and disambiguation of context-free readings requires only cubic time, and (iii) involved probability distributions are mathematically clean. In an experiment for a mid-size UBG, we show that our novel approach is feasible. Using unsupervised training, we achieve 88% accuracy on an exact-match task. 
This paper addresses the problem of automatically selecting the best among outputs from multiple machine translation (MT) systems. Existing approaches select the output assigned the highest score according to a target language model. In some cases, the existing approaches do not work well. This paper proposes two methods to improve performance. The rst method is based on a multiple comparison test and checks whether a score from language and translation models is signicantly higher than the others. The second method is based on probability that a translation is not inferior to the others, which is predicted from the above scores. Experimental results show that the proposed methods achieve an improvement of 2 to 6 % in performance. 
This paper proposes a method to analyze Japanese anaphora, in which zero pronouns (omitted obligatory cases) are used to refer to preceding entities (antecedents). Unlike the case of general coreference resolution, zero pronouns have to be detected prior to resolution because they are not expressed in discourse. Our method integrates two probability parameters to perform zero pronoun detection and resolution in a single framework. The ﬁrst parameter quantiﬁes the degree to which a given case is a zero pronoun. The second parameter quantiﬁes the degree to which a given entity is the antecedent for a detected zero pronoun. To compute these parameters eﬃciently, we use corpora with/without annotations of anaphoric relations. We show the eﬀectiveness of our method by way of experiments. 
Ambiguity is the fundamental property of natural language. Perhaps, the most burdensome case of ambiguity manifests itself on the syntactic level of analysis. In order to face up to the high number of obtained derivation trees, this paper describes several techniques for evaluation of the ﬁgures of merit, which deﬁne a sort order on parsing trees. The presented methods are based on language speciﬁc features of synthetical languages and they improve the results of simple stochastic approaches.  
Chinese NE (Named Entity) recognition is a difficult problem because of the uncertainty in word segmentation and flexibility in language structure. This paper proposes the use of a rationality model in a multi-agent framework to tackle this problem. We employ a greedy strategy and use the NE rationality model to evaluate and detect all possible NEs in the text. We then treat the process of selecting the best possible NEs as a multi-agent negotiation problem. The resulting system is robust and is able to handle different types of NE effectively. Our test on the MET-2 test corpus indicates that our system is able to achieve high F1 values of above 92% on all NE types. 1. Introduction Named entity (NE) recognition is a fundamental step to many language processing tasks. It was a basic task of the Message Understanding Conference (MUC) and has been studied intensively. Palma & Day (97) reported that person (PER), location (LOC) and organization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in MUC. This paper thus focuses only on the recognition of PER, LOC and ORG entities. Recent research on NE recognition has been focused on the machine learning approach, such as the transformation-based learning (Aberdeen 95), hidden Markov model (Bikel et al. 97), decision tree (Sekin et al. 98), collocation statistics (Lin 98), maximum entropy model (Borthwick 99), and EM bootstrapping (Cucerzan & Yarowsky 99). Other than English, several recent works examined the extraction of information from Spanish, Chinese, and Japanese (Isozaki 01). Most approaches for Chinese NE recognition used handcrafted rules,  supplemented by word or character frequency statistics. These methods require a lot of resources to model the NEs. Chen et al. (98) used 1-billion person name dictionary and employed mainly internal word statistics with no generalization. Yu et al. (98) employed a common framework to model both the context and information residing within the entities, and performed rule generalization using POS (part-of-speech) and some semantic tags. A similar system is also reported in Luo & Song (01). Chinese NE recognition is much more difficult than that in English due to two major problems. The first is the word segmentation problem (Sproat et al. 96, Palmer 97). In Chinese, there is no white space to delimit the words, where a word is defined as consisting of one or more characters representing a linguistic token. Word is a vague concept in Chinese, and Palmer (97) showed that even native speakers could only achieve about 75% agreement on “correct” segmentation. As word segmentation is the basic initial step to almost all linguistic analysis tasks, many techniques developed in English NLP cannot be applied to Chinese. Second, there is no exterior feature (such as the capitalization) to help identify the NEs, which share many common characters with non-NE (or common words). For example, while 中 is normally associated with the country China, it could also mean the concepts in, at or hit; and张 normally refers to the surname Zhang, but it also means the concepts open, sheet or spread. Moreover, proper names in Chinese may contain common words and vice versa. Because of the above problems, the use of statistical and heuristic rules commonly adopted in most existing systems is inadequate to tackle the Chinese NE recognition problem. In this paper, we consider a new approach of employing a rationality model in a multi-agent framework.  The main ideas of our approach are as follows. First, we use an NE rationality measure to evaluate the probability of a sequence of tokens being a specific NE type, and adopt a greedy approach to detect all possible NEs. Second, we treat the process of selecting the best NEs among a large set of possibilities as a multi-agent negotiation problem. We test our overall approach on the MET-2 test set and the system is able to achieve high F1 values of over 92% on all NE types. The results are significantly better than most reported systems on MET-2 test set. The rest of the paper describes the details of our rationality-based and multi-agent negotiation approach to detect and refine NEs.  2. Rationality Model for NE Detection 2.1 Named Entity and Its tokens Feature For clarity and without lost of generality, we focus our discussion mainly on PER entity. The problems and techniques discussed are applicable to LOC and ORG entities. We consider a simple PER name model comprising the surname followed by the first-name. Given the presence of a surname (as cue-word) in a token sequence, we compute the likelihood of this token playing the role of surname and the next token as the first-name. The pair could be recognized as PER only if both tokens are labeled as positive (or of the right types) as shown in Table 1. If either one of both of the tokens are evaluated negatively, then the pair will not be recognized as PER based on the model defined above.  Sentence  PER? Label Remarks  请张飞讲话… Y  张(+) 飞(+)  ... invite Zhang Fei to speak ...  一张飞机票… N  张(-) 飞(-)  … a piece of airline ticket …  老张飞上海… ？  张(+) 飞(-)  //Illegal PER  …张飞…*  ？  张(-) 飞(+)  //Illegal PER  * Strictly, 张将军 and Mr. Zhang are not really person names.  They are references to person names and should be detected via  co-reference.  Table 1: An example of NE and non-NE  Although the example depicted in Table 1 is very simple, the same idea can be extended to the more complex NE Types for ORG and LOCs.  The number of tokens in a NE may vary from 2 in PER to about 20 for ORG. One constraint is that the sequencing of tokens and their labels must be consistent with the respective NE type. Also, there are grammatical rules governing the composition of different NE type. For example, LOC may consist of a sequence of LOCs; and ORG may include PER and/or LOC on its left. Thus by considering one pair of tokens at a time, and by extending the token sequence to the adjacent token one at a time, we can draw similar conclusion as that depicted in Table 1 for complex NE types.  2.2 The Rationality Computation If we know the probability distribution of each type of token in a window, NE recognition is then the procedure of evaluating the rationality or certainty of a sequence of tokens with respect to a NE type. Motivated by the results in Table 1 we view NE recognition as a special coloring problem. Initially, all the tokens in the corpus are considered as a sequence of White balls. Given a chain of tokens appears in a NE window, we want to use the probability distribution of these tokens to re-paint some of the white balls to different colors. A sequence of appropriately colored balls would induce an appropriate NE. For simplicity, we again focus on PER NE type with 2 tokens. The surname token will be colored red and first-name blue. We assume that the number of PER names in the corpus is N, and the rest of tokens is M. Because there are N surname and N first-name tokens in the corpus, the total number of tokens is M+2N. Hence the marginal probability of PER name is Pr(PER)=N/(2N+M) .  Red Blue White  Red  Format Pr.  aRbR aBbR  0 N/(N +M)  aWbR  N/(N +M)  Blue  Format Pr.  aRbB  
Language users have individual linguistic styles. A spoken dialogue system may beneﬁt from adapting to the linguistic style of a user in input analysis and output generation. To investigate the possibility to automatically classify speakers according to their linguistic style three corpora of spoken dialogues were analyzed. Several numerical parameters were computed for every speaker. These parameters were reduced to linguistically interpretable components by means of a principal component analysis. Classes were established from these components by cluster analysis. Unseen input was classiﬁed by trained neural networks with varying error rates depending on corpus type. A ﬁrst investigation in using special language models for speaker classes was carried out. 
In this paper we propose an integrated knowledge management system in which terminology-based knowledge acquisition, knowledge integration, and XML-based knowledge retrieval are combined using tag information and ontology management tools. The main objective of the system is to facilitate knowledge acquisition through query answering against XML-based documents in the domain of molecular biology. Our system integrates automatic term recognition, term variation management, context-based automatic term clustering, ontology-based inference, and intelligent tag information retrieval. Tag-based retrieval is implemented through interval operations, which prove to be a powerful means for textual mining and knowledge acquisition. The aim is to provide efficient access to heterogeneous biological textual data and databases, enabling users to integrate a wide range of textual and non-textual resources effortlessly. Introduction With the recent increasing importance of electronic communication and data sharing over the Internet, there exist an increasingly growing number of publicly accessible knowledge sources, both in the form of documents and factual databases. These knowledge sources (KSs) are intrinsically heterogeneous and dynamic. They are heterogeneous since they are autonomously developed and maintained by independent organizations for different purposes. They are dynamic since constantly new information is being revised, added and removed. Such an heterogeneous and dynamic nature of KSs  imposes challenges on systems that help users to locate and integrate knowledge relevant to their needs. Knowledge, encoded in textual documents, is organised around sets of specialised (technical) terms (e.g. names of proteins, genes, acids). Therefore, knowledge acquisition relies heavily on the recognition of terms. However, the main problems that make term recognition difficult are the lack of clear naming conventions and terminology variation (cf. Jacquemin and Tzoukermann (1999)), especially in the domain of molecular biology. Therefore, we need a scheme to integrate terminology management as a key prerequisite for knowledge acquisition and integration. However, automatic term extraction is not the ultimate goal itself, since the large number of new terms calls for a systematic way to access and retrieve the knowledge represented through them. Therefore, the extracted terms need to be placed in an appropriate framework by discovering relations between them, and by establishing the links between the terms and different factual databases. In order to solve the problem, several approaches have been proposed. MeSH Term in MEDLINE (2002) and Gene Ontology (2002) provide a top-down controlled ontology framework, which aims to describe and constrain the terminology in the domain of molecular biology. On the other hand, automatic term acquisition approaches have been developed in order to address a dynamic and corpus-driven knowledge acquisition methodology (Mima et al., 1999; 2001a).  † Current affiliation: Dept. of Engineering, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113- 8656, Japan  Different approaches to linking relevant resources have also been suggested. The Semantic Web framework (Berners-Lee (1998)) aims to link relevant Web resources in bottom-up manner using the Resource Description Framework (RDF) (Bricklet and Guha, 2000) and an ontology. However, although the Semantic Web framework is powerful to express content of resources to be semantically retrieved, some manual description is expected using the RDF/ontology. Since no solution to the well-known difficulties in manual ontology development, such as the ontology conflictions/mismatches (Visser et al., 1997) is provided, an automated ontology management is required for the efficient and consistent knowledge acquisition and integration. TAMBIS (Baker et al., 1998) tried to provide a filter from biological information services by building a homogenising layer on top of the different sources using the classical mediator/wrapper architecture. It intended to provide source transparency using a mapping from terms placed in a conceptual knowledge base of molecular biology onto terms in external sources. In this paper we introduce TIMS, an integrated knowledge management system in the domain of molecular biology, where terminology-based knowledge acquisition (KA), knowledge integration (KI), and XML-based knowledge retrieval are combined using tag information and ontology management tools. The management of knowledge resources, similarly to the Semantic Web, is based on XML, RDF, and ontology-based inference. However, our aim is to facilitate the KA and KI tasks not only by using manually defined resource descriptions, but also by exploit ing NLP techniques such as automatic term recognition (ATR) and automatic term clustering (ATC), which are used for automatic and systematic ontology population. The paper is organised as follows: in section 1 we present the overall TIMS architecture and briefly describe the components incorporated in the system, while section 2 gives the details of the proposed method for KA and KI. In the last section we present results, evaluation and discussion.  
For multi-document summarization where documents are collected over an extended period of time, the subject in a document changes over time. This paper focuses on subject shift and presents a method for extracting key paragraphs from documents that discuss the same event. Our extraction method uses the results of event tracking which starts from a few sample documents and ﬁnds all subsequent documents that discuss the same event. The method was tested on the TDT1 corpus, and the result shows the eﬀectiveness of the method. 
In this paper we propose an integration of a selforganizing map and semantic networks from WordNet for a text classification task using the new Reuters news corpus. This neural model is based on significance vectors and benefits from the presentation of document clusters. The Hypernym relation in WordNet supplements the neural model in classification. We also analyse the relationships of news headlines and their contents of the new Reuters corpus by a series of experiments. This hybrid approach of neural selforganization and symbolic hypernym relationships is successful to achieve good classification rates on 100,000 full-text news articles. These results demonstrate that this approach can scale up to a large real-world task and show a lot of potential for text classification. Introduction Text classification is the categorization of documents with respect to a set of predefined categories. Traditional neural techniques for classification problems cannot present their results easily without adding extra modules but selforganizing memory networks (SOM) are capable of combining topological presentation with neural learning. We extract suitable relations from WordNet to present a semantic map of news articles and show that these relations can complement neural techniques in text categorization. This integration of SOM and  WordNet is proposed to deal with the text classification of news articles. The remainder of this paper is organised as follows. In Section 1, we give a brief review of SOM. Section 2 is dedicated to a description of methods of dimensionality reduction. In section 3 of our hybrid neural approach, the new version of the Reuters corpus and the results of our experiments are presented. 
This paper proposes an unsupervised learning model for classifying named entities. This model uses a training set, built automatically by means of a small-scale named entity dictionary and an unlabeled corpus. This enables us to classify named entities without the cost for building a large hand-tagged training corpus or a lot of rules. Our model uses the ensemble of three different learning methods and repeats the learning with new training examples generated through the ensemble learning. The ensemble of various learning methods brings a better result than each individual learning method. The experimental result shows 73.16% in precision and 72.98% in recall for Korean news articles. 
Syllable-to-word (STW) conversion is important in Chinese phonetic input methods and speech recognition. There are two major problems in the STW conversion: (1) resolving the ambiguity caused by homonyms; (2) determining the word segmentation. This paper describes a noun-verb event-frame (NVEF) word identifier that can be used to solve these problems effectively. Our approach includes (a) an NVEF word-pair identifier and (b) other word identifiers for the non-NVEF portion. Our experiment showed that the NVEF word-pair identifier is able to achieve a 99.66% STW accuracy for the NVEF related portion, and by combining with other identifiers for the non-NVEF portion, the overall STW accuracy is 96.50%. The result of this study indicates that the NVEF knowledge is very powerful for the STW conversion. In fact, numerous cases requiring disambiguation in natural langu age processing fall into such “chicken-and-egg” situation. The NVEF knowledge can be employed as a general tool in such systems for disambiguating the NVEF related portion independently (thus breaking the chicken-and-egg situation) and using that as a good fundamental basis to treat the remaining portion. This shows that the NVEF knowledge is likely to be important for general NLP. To further expand its coverage, we shall extend the study of NVEF to that of other co-occurrence restrictions such as noun-noun pairs, noun-adjective pairs and verb-adverb pairs. We believe the STW accuracy can be further improved with the additional knowledge. 
The paper examines different possibilities to take advantage of the taxonomic organization of a thesaurus to improve the accuracy of classifying new words into its classes. The results of the study demonstrate that taxonomic similarity between nearest neighbors, in addition to their distributional similarity to the new word, may be useful evidence on which classification decision can be based. 1. Introduction Machine-readable thesauri are now an indispensable part for a wide range of NLP applications such as information extraction or semanticssensitive information retrieval. Since their manual construction is very expensive, a lot of recent NLP research has been aiming to develop ways to automatically acquire lexical knowledge from corpus data. In this paper we address the problem of largescale augmenting a thesaurus with new lexical items. The specifics of the task are a big number of classes into which new words need to be classified and hence a lot of poorly predictable semantic distinctions that have to be taken into account. For this reason, knowledge-poor approaches such as the distributional approach are particularly suited for this task. Its previous applications (e.g., Grefenstette 1993, Hearst and Schuetze 1993, Takunaga et al 1997, Lin 1998, Caraballo 1999) demonstrated that cooccurrence statistics on a target word is often sufficient for its automatical classification into one of numerous classes such as synsets of WordNet.  Distributional techniques, however, are poorly applicable to rare words, i.e., those words for which a corpus does not contain enough cooccurrence data to judge about their meaning. Such words are the primary concern of many practical NLP applications: as a rule, they are semantically focused words and carry a lot of important information. If one has to do with a specific domain of lexicon, sparse data is a problem particularly difficult to overcome. The major challenge for the application of the distributional approach in this area is, therefore, the development of ways to minimize the amount of corpus data required to successfully carry out a task. In this study we focus on optimization possibilities of an important phase in the process of automatically augmenting a thesaurus – the classification algorithm. The main hypothesis we test here is that the accuracy of semantic classification may be improved by taking advantage of information about taxonomic relations between word classes contained in a thesaurus. On the example of a domain-specific thesaurus we compare the performance of three state-ofthe-art classifiers which presume flat organization of thesaurus classes and two classification algorithms, which make use of taxonomic organization of the thesaurus: the "tree descending" and the "tree ascending" algorithms. We find that a version of the tree ascending algorithm, though not improving on other methods overall, is much better at choosing a superconcept for the correct class of the new word. We then propose to use this algorithm to first narrow down the search space and then apply the kNN method to determine the correct class among fewer candidates.  The paper is organized as follows. Sections 2 and 3 describe the classification algorithms under study. Section 4 describes the settings and data of the experiments. Section 5 details the evaluation method. Section 6 presents the results of the experiments. Section 7 concludes. 2. Classification methods Classification techniques previously applied to distributional data can be summarized according to the following methods: the k nearest neighbor (kNN) method, the category-based method and the centroid-based method. They all operate on vector-based semantic representations, which describe the meaning of a word of interest (target word) in terms of counts1 of its coocurrence with context words, i.e., words appearing within some delineation around the target word. The key differences between the methods stem from different underlying ideas about how a semantic class of words is represented, i.e. how it is derived from the original cooccurrence counts, and, correspondingly, what defines membership in a class. The kNN method is based on the assumption that membership in a class is defined by the new instance’s similarity to one or more individual members of the class. Thereby, similarity is defined by a similarity score as, for instance, by the cosine between cooccurrence vectors. To classify a new instance, one determines the set of k training instances that are most similar to the new instance. The new instance is assigned to the class that has the biggest number of its members in the set of nearest neighbors. In addition, the classification decision can be based on the similarity measure between the new instance and its neighbors: each neighbor may vote for its class with a weight proportional to its closeness to the new instance. When the method is applied to augment a thesaurus, a class of training instances is typically taken to be constituted by words belonging to the same synonym set, i.e. lexicalizing the same concept (e.g., Hearst and Schuetze 1993). A new word is assigned to that synonym set that has the biggest number of its members among nearest neighbors. 
 É%¶°I¥I¤Ç¥If¶ÀI¥I¥I¥I®f¥Iª¶°¡®fIÇw¢@B§¤§¢1¤I¨I¢¢QII1%¨¦Ü¢¡¦§¦§1§¦©©±£¦¥I1$¡£¡¡£¡£©¦¡¡1¤¡§1§1A¨¦¨I¨¡£11g¦Û§¨G¡v¡j®¤Á¥%f¾I¦¥A¢¤¨¦¨¦¨¦¥11¦¥«¨¥I¥I¥I¨¦¡Æ¹g¹¥I§1¥I©1¥IU¢¦¦Ç¥¦%¢¹ÉÎ¢1©©©¦¨¦¨¦¢ª§1§1¯¡©­¢1§1¡%¤¢`¤¥I¡Á¡¥%A¹iº¡§º§z¡¨¦¤¤¤¦¦¡¨¦¥%¦ÁjI¶ÀBb%%¥I¡¸¥I¥III¨$Ö¥%¢¥5¤Ç«¢1¦1¢wf¢1¤¤ÍfI§1­ÍI·§5I¥¯9®v§1¶À¶ÀÀ¡®f¦I¥IÂ¥%ii¬àºw¦¡¦¨®¤©1ªq¨G±£¢1¢1¡­v¥I¯ggg¥I¥%¦%5§1¤¡z¶°¡£¤¡¦f¡¦¡¢5¥I¥I¥I©1§1¥I§I«¶À¥dI&§¡`¨¦A®£¯£1¢zI·¥¦¥U¥3¢µ§d¥%­À©1¢©1­¦¦¡I¥Ig©1«²w§11g¥Ij%¡`­gÉ%·iµ¦¦`1¢1¥I¥I¢1©1¨I®¤1Iz¥I¦1¶°¥I¤¢f®£¡¥I¹1¥I¥IG¥%©1¥I§¨¬%¦I¡ffQ¬Ö©1¦¨¦%¨¡¦Ò¨1¡¡Q¡£¥I¥IÉÎ¡I¡%¦¡f¥II²!U¨¦1¦¡§¢ff¿b§â¥%fª1¤¡§¥II¥¨§1I¥I©1®¤§¨¦¨®£1¨¦g¨¦¦¦Å¡v¡I¨GÍAU¢¨¦II°z§¨¢1i¦¢1¥I¥IÀ©%¡¦©1©1ÅGÉÎf¥I¡¥%§¸&5¢¢1Â¥IQ¯&§1¦¹$¡'¥I¡®g¢¡»®¤g¤¡QI¦Ã¦¨«¢©1%¶À»¢fI¥I¨¦3%¥I¡²©1¹åæ%¨f¢À¥I¯«®fÀff$¥I¦¤¥j¥I¨¦¯»%§1¡v%¹Aª¥I¥I¦¯Á¥I¥I®¤¡¡¨¥Ii§II¶°Ó¥%¦§gj¯£¨¦¢Â¨¤¡¥I¥I¡§f¢1z¨I§¥Ig¥%¦¶°¦Â§1¥IÕ¨«¦¡¥IQ¥I1§¦¡¢»²gÇ«­q¡¦§1z¦I¤§¢1¤×vª¥I°®b¥¡£¤¢1½»¡5¥IÄg¡©1©f¢Ó¯9¥I©1I¡1¶U©i¦Qz¶À·¦¤Ç¡¥IGU­¤¥I¡¡¥II§1°¥%¢1©f±v1¨G¦²%Å¨¦I¥%G¯d§9­¨¦©1¦¡Í¡mI§1¦©¢5§TÓ¡ûv¡¨Ò¤¥I¡¨BÜ¡¡I¡f¡v¥IIÃ¡%¦¥Å¿f¨¦fbÀ¥IÌ¦z¡¦®£¥I¶À©­gIµ²©®£5GI¦g¢1G%¥I±v¥%Ç¨¦¨¦©©¥I¡g§¥Ii¨¦§I¤¡É%ªº¿bÍ¥%II¥Í¨§1¡¥I¨zÉ%®£§¥jÆ§¥I²j¾¡I¥%¦¢Ç¡i§g¤I1¸I¥¥I¥%G©¦§¦¡¦1©¦11É%I¦¥I©1%j¢1Å¨©Ñ¥I¥I®b¯¡¨i²¡Q¥I¨¦II31¢1U¤§%¦¢f¿f¢×Í¦1§¯ÏI¥Arg¹¥I¥I¦§g©ff­B%$1%©f¨¦Ç¨¨1¥II¦ªªdf¨w¢À¿1¦¦©1·$¥I¥¬¡£¦¡¦¤¯Ó¥I1¥6¨¢1¢Â§¡²¦¡¦­£¤zà¸j¦ª£II­1¢È$±v¥I±£¥»¦¹¡f¥«¤¢¥°©¤A¡¢§Û¨¦Ò%9¢1¡¥¬«I§1¶·¡v²¡¯£mú¤¤¸¥ÂÕ¨¦¡®f¯¦¤Ì¡¥I3¦¨¨¹¦®£5¡¸¹¡v¡¡vI¢1ä¬$¢w¡I©¥AI¦©13¶¶ÀI1±£i1¥IÉ%®gÇ®¨¦¥I¥%§¢®¤¥I¢1II¥I©1&¦¥I¨I§1¡1¥%%I¤¢ff¡²¥¦fT¨¦¡¦§ûÁ1©¢1¨¦¶·§¡¯¨A­ÉÎ§f¦¡£¥I­À¥6¦¦I¨¦¨¡9­Â¤¦¨¦§&¦©1U¦¤¢f¯G´BÇ%©f¥Iw¨¦g¢`¡¥I¡¡¦¨m¦©f¦ª¨¦¥j¥I×¥I¦²E¢1¨¡¶°¡v¶ÀÛ¼¥·§1Ij©1©¥A¥IG¥I¥¡¦¢I©¢§f¥%§©È¥ÀØÁI¡Í¡¡¢¢11FI°¢wz%¡z¢1¤¨§1¥If¡¢â¡¥%«¡£1­5iiÀ¥I$¦1Ø¡¥¤A²¡%¥I¡¤¡£§1HI¯¥j¥I3¡Á¢j¦I¢Å©1§§¥I¹f¡¨¦¥Â®f¥$Øi¡Iºj­«§1¢®¤¡·¢1Ç­Hµ¶·¨¦¢1§f¦¡¨¦g¶°¥%f®f®f¡¢¦©¨¦j§1z¡G¨¦Ì1¦gÁ©¥%Âd©I¥%ÃA¥5ÜÜI¦¡­°¨¢¤AI¦¦¥Ã9¥6¨¦¤Iªº¡»¥I­¤U¤¦§¤¤ÝÝI¥j¨¢1GI¥°g¦¥I§1¥IÅ©1¦¡©1§1I¡£¦¶°¥%¥%I1¡¡ÝÿÛ´¦§1·¥I¡¤%¨¦6©¤©1¥I¥I©1¥I%¦¡¡x1¡¢¥IgAI¤¤I6¦¡ÿÿ¡11§®¤¥I§§§®f±£±£gg®ªgIII%Ç%¦q%q5¡¨qÎ%¥¦¦¦mGGmÒ¢¢¢¯¡¥¥¥¥­²­²­²  Ç¶À¥I¥I¥%Çf¥I¶ÀÌ¢1¢¤¢¢IQH)¡I¦§§1©¥I¡¡£¡£W3¡£¡¡¡¡§¥I¹®b1¥¨¨¦¨¦¨¦X"¨¦ª¦¦¤¤¤¥I»¡#°%©©©¢1¢1¥I¥%¥I¡$¦1¥I¤¤¤¡¦¥I¥I¡­zÆ­À1¨¥I­¬b§§I¡¥A¦m©¡1¨¶U$¡¹®fÜggg²A¨¦II¥%¨¦IÇ%¨Ý©I¥¬¥°¥¦¦¨§µ©1Àd¤¥I¨¦Ý­¡¥%¥Á&1§IüvGf¥I¤¥IÛ£¥I¥©1½»¡¡¦©fI°¥IÙ»©§¦1fI¥III¡£¤§¶°f%I¨¦ªqºz¥$¥%¢I¨¨v¡¦¦IÒ¢Á¯Ñ¡¥%Ñ¥I±£¦®f%¥%I±fI¥I§¥I¦­$¡¸I§1¤§¨¡­¤Á¤®&¢Óg¥%§1¤¯Å1¦¶°¯Ç¥I¦%¤Ò¡v1¨°¨¶ÀÛI¥ã±£¢´¡¡§1ª¹¡¡I¨¦¶·I¥I¡{©1I¡­g¹b¤¡x©1I¶°j¨¦¢5¿&¦Á¡II¶U1ª©f¥IÍ§¸¥Ûb®f%1ðA%§¢¸¨§I`¡Û£¥I­¡¹¢Ñ¡ªvI¦¥I1¤¦Â¡x1¡Ö¶Àf¡¤§1fI­zªÃ¥I¦U¦ÄvP¦¬RI§¦¦¥I¯Â²©1fA©1¢T¯¸¥IÜf@B1Gdf@BI¡§%Ý¦¡IÌ¦A©¿bzÝ¨©f¶°A§1¦Ig¤¦§11Ý¥Ã%¥I%¬¶U¹%¦¥I¨¦G¦¢¸Á¥I1¦©¨¦gÇ²yI§¢¥%±v¢¥©1©1©¥I»A¥I¦¥I¥I6z¤§w%¦m§f¨¦¤¡¡¦1§¢¡ïÍ¥%©1¥I§1¥I65¢`¥I¦¦§¦¨G¡£¨¤¡I¡vf¥UÒ§¡I¦­6§1¥%¦ú&¨¦©1¢1¬¹w&­3£º°¢z©¥IÄg®f¦f¢`¢§f¡¶·¤¦©1j©1¹¡z¥%91¦¨g¦«¡T&¥º±¥I²¤i¦ª$Ç¥I¥I¥Ig¦¨I¡¥I¦¶ÀSU1¢§§i¢£Ûg¥I#1%Î%¢¯¡¥V ¥I³¥I¥IY¢1Ì¤Í ¾§1§1¡§©¡£U¥IÀÍ·¢¥ºm§%¦®f¢1%¯£¡¡¡¦¯£1¦¹Ã%¤©iIÖY¥I²¥%­¥%¥I¡¥If¡­gÜ§¤¥If6¨©1¨1ÇÝa¡¡p3¥I¥I­i¥IÝ¬¹¹Aºg6ÿ¨A¡IGpÃª¦¨² i¶°¦¥I¥%f¦¹¥%¡p`I¾ z¡¹¯¥I´75Á1«¯v¡í¨b¦v©¦I¨©f¹¡¨¦¡f¬¨¦A%3¥%§1fj¢¡¶¬ÜI¥Q¡£¯Ñ¡ûv%%¨¦¦1­ý¡¬¥I©ü¦ªA»×¦¤f¯º¤¦¡¤z¤jG¦¥IÃ5§1I¡¢¤gÇg¥I¼¥I¡¡£¥·§b¤ ¨¦°¥%IÚ°%¤ ©¦¥%®f¢â¤f¹¦©5¢ÃI3¥%¤%%¹g¦1©1%¥%¡¦¥I¦¢wIIÎ¦©fg¥Iã­gb¥I¨×£ªq¡¥IÜb¤©¥IÂ¦¤¥III¥I¡%¡ÿ§¯g§5¦¡%ß² ®f¡®f¯£¤¥I1¡¥Ic`Ì§gIßv§°µß¯gI¤I¡d¥¡¡j§e¡×«¤Á»%¤©f3¤¡¨¦¤¡¶À¤¡I¡Ò¦¡g©1¥I¦P¿b¥Im«¥%¥I¨¡1§¥°fbP§Å¨¦¥I¦¶·gi¢Á¨G¥Iÿ¡¥Irt¶ÀÒ¢1b§9%§1¡¶·§×£¡¹prÝ§hq¡$GQ¦©1Ü¬A¯¡ßvps¥I1¦¢1eps§1y¢f­gG¢1²rt¡§Å¶°v6¥%bÜx¢'¶À¡£°¢µ¦guç"rÀ§i§1°Iz­&¨¦¡±£Í®f¦¯´%©Ifrt¢f®f¥º²f¡¤I¦À¤§1vx°À©1II¥Iª±v1¨!¦%wCb¡£¨¦§f¥%¥IÇI¥%I¢ys%àÍf¢§Gg¢Å¡£v¢Â%Â¦©¦¥¦¢1g¡¤I¨f¯Q¨²¿b¥¦¡IÇ¢d¥I©1¥I¥$§¥I¡ðÃQ§1w¨®g¥%A¦¥I¥I¨¦¦I¯Å¶·§A¥I²P¡¢¥I¦¥¡r§¥II±£Æº§fj§1¤5¨T¥Iq¹¨¦Äv§1®g¨¦¦§¤²â²B¡6Û¥If©13¯ÑÃ¯£©1¦³ã©1â%¤¢´¹³¦¥·¥¥%b¥I¢º¹¦r¡P©§1w®¤¦¤Ã¯º¢1©1Ñ%rt¥I¡ªQ¦m´¢1¯v¦¡1%ps¨¹§fIg§g¦B³g·¡Ýq1¢I¦¤«©1¥I¯và¯´g¡¢»¦¥I1&¤z¥%²ý¥%I§1¥%¨û¥I¢w¦¡Ýq¡@¥I¥If¡çÏ¥I¤¨àg¥I¡%®f§¥%¦¡B¨¥%¨§1w¢f¯£çµ¦ª©°¡1üv%$¢Ï¥I¨¥%III¥%I²ýf¡{1¡I1û¦¥%©1ª¦¤I¤1¤1çÃ®g¨¦¥%¦¨qIÒÒ¢¢¢¢¯¡¥¥²²  ff¥Iª¶À¨¦¢1%I§1I¥%I¦¡¡¡Ò§ª¡I¦ú¤¥II¦¦z©1©11G«¸©f®«¢1¡¦I¨¦§1ff¢¨IÝw¹¥%%fx¥I¦U¡£²Â¡×dÀf©­£ª¦P§gGÓ¢f§1ªq¢9¦¬¡1¥j³¦Irtx¥%µd¦¨¥I¢z¯£­psª¥I¤v¹¤I§T©1¶°¨¦û¤wdß¡1¨Gçá­q§fÿg¡I®&§¢1¥II¢1¤Á¸I¥%q²ýIT¦¥%ß¹B¡I¨¦A¦¤¢Á¤¥Iç¢´`°©1¤£Q¦¢¡¡¨¦b¢f¥%¨Ü®I¡I1¡¡¥I¥IÇ¤Á²ô¹¨¦®ÉÎ¹¡Iàg¦©%ÐvÖ¨¦¨¦ÉÎ¥%©1çÞ²¤²¢I¡d¨¨¦1ÌÜ¦¥IÀÀ¥I¨¹rv£¢Q3¢Ø1¥Ic®b§¥IÇ¯j¥%Ü¦¥%¦mÜ©fI¡£¡UØ¡¦¥%­6¨¦¥%©f1¨¦%Ø»¡»6¡¼¥I¨¦III¨¦1¨§¦¥j©1¦¢9©®¤¥II¢¤¹A¥%®£ÛA1¯£«¦¨¡¡¡¡x©1IÉÎª¨¦¤¡¢f1dI¯v¶°%f¦1¥%¡¨¨¥I«©1gI¦¨II¢1§1¥3®r¥I¢²¥IÛvI¢Æ®f1j¥I¨²3¸¥II¡¢¨¥I¤àgºI¨¦£I5¦¡¥I¥I¥I©ÿg¯j©1¥Í¹ü§¨§1²e¢¨Ã1¢»®²ý¹×$ß¹¨¡¨ÉÎ1ç!¤ç@¡¿b¦Ö3¨@1&I²1¸%¨­3¡¯º11¥I¦¢1¤¢fÜIIg«·¢¢9¡¡¤I¯£Ý¨¦¥%%¤¤¥IÝ¦I1ÎI©1®g¨¦¨¦§Ü£¤¦¦¦ÒÒÖ¯×²­ ¶°c`¨% ¤1dfÀg¥I¢¤¥%§¹¡À¢¬¢wII¥Iggh¥I©1®vª¨Iif¯¦1Û£gu¨¦¢1Í%¦zhP¯ª¡¦­¸jk¢1&¢Ã¨¡vwÜ¥%y¨¦¡»Ýv©¨GAß§Ál³¤ß¡G¥%bwÇ­w¦¡i¥%¦¥ÍrE¤¤Ç¤¨¦m%I§¥I¨¦ÇÄgI¡©1¢no»Äv¡£jP1©¨¦¢»Ú°gu¨©¦¯d1¤rt¥%¨¦vx¦¥Î¥%¿b¡Âw£g¦¢¦ª¥p¡¦&­1¶ÀBªrtsfdn¦¨wq¨¦¥%i¦¥¡§g1rq¥%¥I¥%IG¡§ð¢ ¶°¥%¡¶À¥I¥%¥Iv%¨¤º¦¡d¤§©¡§¤z¥I%¤¥I¦¤¢¡¦g§1¡¨¦¡§¤¢1¥%¦¦¥I¡£xÒ®¤­v5¡j¿bGª¨¦Ó¥I¦¥%¥I­3¨¸Á©uj§®f¦1®¤¦¤¤5It¹¡T6¤I¥I%gg·%¸­¨¦Iuw¥I¥wÀ©g¯¡1¢§vy¥I¤©¤¶°©1¨1¡¡£fx%$z¡d{I¤%¨¦®¤¢Å§1¡{¥¨9¢»©¶°zÆª¦¨¦¤f¡£%¨GI¤A¥I§f%¹§z1¨¦¢§¡v¥%¥%%¡©¨¤¨¦Î¡¨¦{¢¡£¥gÇ¤§ÑÆGI©fIu¡{uw¨¦¥I¢1­jÃ¡¡Çªv¨¦¢¦vyÒG¨¶·¤©x%g­5¹ú¤¥%¶°¥%²¡Î¥¦¢´¡¦¦g¥I§fd¥I­¦¦IöÂ1uw§¥I§¤¢fxz¢¨¦vy¥IwµAiU§¢x%§¥IBG¥·Â¢¢¦§1¥II¥I¥%Î¡£§¤òI§xGI|¦s¦¦º¨¦ºIIx%©1²Ã¬GU©©1U¤ò~¥II}q¤¥%¥I¨G¥%¡IÌ§¦¦Í¢§`xq¡vÀf´¦­´Î¡¡¤gòI¥IsÇ÷¨¥IÀxI¦§¤v¥òÁ|%¶·%¯¨òb©¥IIÁÍ§ÇÇ¦¦Ç¤òjgg©fII®g1»m¥I¥I©®&1I¯º¦Äg¢¢f9¡1¤¨¦¥I©1¡£©1ÿq¤¥I%¸Í®¤¥IØi¡¨¦¨¡§i£©£¦5ç¡¢¥² ¥I¥I¡¥I¥Ic`¢¨¦Í©1§§§¦¡£d¡IUµÃª¨¦¢¥%%©¦1²%¥I»¤f¨¦§¥%ÌI§Ñ{¨¦I ¨§9r¢`g©1¨¦%Á«¥II¦®&z©fy¹¥%¥6¡¦ÓvP¤¦»%$¡¦¥%ysg¡zv¦¿fªv¨¦¥%Ggw¹¢1¹¡´¥%©1¥II«¥I©¡v¡µ¡©1§%wC¨¦¦¨¦¡Ñ¥%¤¦9k©¤¦A©1 ¦ri¤¤¡¦¢IÜ¿bI1¥%¡£²©ff¦×{¢´¥Ii¨¦¦¢1rj¥I©¥3z¢1Ì¡®gBps¤ 1I¯Å§¤¦1z¦Q1ff¢´¢f¡g¢Ñ¡IfrC©1¥I`%¥IÍIII%f§¥I¡¯¼¡»gÂ¥IxvP1­bG¥I¯vys%¡×{¥%¥I¥Io¹bII1§1§¥Iy¢1I¯£«vk¸¤¤¡¨¤zÁ¨¥I»¡§%¤¡ª¡fÃ¥%­·b¡ÃI¨¥I¦¥%i¨¤¥%§1¡¨¦¡¥Iz¿bh¥z¡¡§1¡1f1¦¡I%¡ÒÒ¢ ¨­  ¥%¡¥%ª£¥%¥%¥%¥Iª¥%¡¥%¤©1I©×{%Ç¨3B¦¡¦¦¦¡¦¥I6¨¢q1¤¯»¤¦¦¦¦À¬¦¡{¢1f§1¨¦¨¢¥IÉIQ¥I¤I¦¶·ª§§f¥%©1g¦¥I¨¥¡§w¶°¥%ª¡Ùb¤¨¦¡¯Ã§1g¢1BU¥%¦¡¿b%¦`U®fQ¸Á§¨¦¦©1¥I¢f¥IÃ¥6¡vIj¨1¡®&®&²a3¨¡¨¥%¨¦1©1«I¥I¡¢bfI¦¡¥%¥%¤¦¡¥Iz%¥I©I§¢IIÁ¨¦¡¨¡¼¡£¥ãÍ°©¥ã«¡1§¦¥I¡¤¥%¶·°©1I¶·ª¥I¨¦¦¡1§IIIÀ¦I¨¦¨¡§fI©1©­f¥I¦1ª¨¦¢$¥IBIIªq¨¥I¢«©f¨¦1¥I¤¤i¨¦¡£¥Ig¨¦Ã¥%¤¥I§1¤¹Ã¥ÃQÓ¡f%¢§z¥I¢¸¡¥%¡¨¦§%¹¡©1¥%¦f¥I¦¢Á©©¦©ÁÁ¨¦gI¦i1$¡²%§À¦¦º¤¤¥r¦¦¥%¥¯¥$¥I¥I¢1IÓÇ©f¥I®&­¢z¹d¦ªÌ§1¡­¡§¥I¢5§1¥I¹ª¶U%¡£3¡{¨¡Ç%¨&¨¦¦§Í¤¨g¥%§1²¨G¥I¡ª¨¨¦§g¥If11§1¦¡¥%À§§1¥I¨w¡B¥%¬©I®¤²¹¡¦¥I¢ÑÍÌ6§¥%¡93²1¥If¨¦¤§z¢¡¥f¡¶À¤Ì¦¬¶U§d¨¦§1¶À¨¥IÍ¦¢E¥I­¦§1¡¶UÀ§§1¨¦¦¹¿&§A¡¨g3¡á¨©1Í¹¡¥IÓ%¦¤j%i®&§1$1¥6§1z&w²&1¥I¨¦¥Ug¢¨¦¢1I§¥I¨¡G1ª¨¦¦¦Ì¢¨¨¨¥ã¨¦¦¥%Û£¥ã¥%¡v¡§`IU¦¦¶·®¤I¤¦¥%¯¨¡§I¥g¨¦©1¨¡¦g3¥I¨¦¯¦f¡§i¬©¥%¥I¤®&¦BI¥%µ¦ª¦¡®¤¡¤¤¡¥%I¦¤1%¦¦g¨z¥I¤£¦Q9¡¡%j¥I¡I¥ã§ª²©1¨²¦§¶·¦¦¨¥ã¡¥Á¥II¢¸¥%¡iª1¯%g¡®¤¡§IÌ¤1¡z¥I¦¥¦i©1¥I¼¥1¡¡¥%1§¦®f¢z¤²§U¨¢±vIª¡Á¨¦¸¢w¥I6º¤¥%¡°¦¥%¦¡£Ì¥»¥I§Â§1À¯È¡©1¦i¥I¥I§¦¨¥I¥I¥¨¦¥I«x¿f¦¦§§A¦¶U¥%¶À¦§%¥%§f¥I§1º©¿&fªº¨«¦¡9U¥%¡»§%¨¤¥%¡%B1¦¥I¥I«%¥m¦i¥·IÀ¨¦¨¦¢1¤§w¡¢¥II¤¨gª¦¥%¢1ª¡§1¨¨¤¥I¦¸¸g¡£©1©1¦¥%¤¡¦¦§¨G¨G£¦IG¥I¨q¨qÎÎÎÒÒÒ§§¯¥ ¶À¥%¥%¥Iªc`Ì¨¦¤¤¦¦§ª¨¦dQ§1©1c¨¨¨¦¤¥%U¢fI¦¥%¡G¢1¥%¡¢Iz¦¦fÃ¦¢©1¦¢1¡£I°Í¡ºIz¹f¨¦Ç¥I¥I®&¡©º´%r¢§ªqd¸Ñ¤¯¼¿f¹åæq·¥%¹¨¦1©¥%¡§1«´gj×{1©1¡¡¥I¥°fnoI£§¢1¦¦¥IÁ¡y¥Iª¦A§vk¨¦©1·I¤¨¡Ü·¦¨&¥%¥II²ª¡¥%¦§¥Á¡¦¡1wGÌ¨¦G§g¦¯µ¦¥I§¢º¹®¤¥I¥%§r%¡j¦¡¥%¥w¡¹¨·1¡9Â¦µp´¤¡¢ª¤¤£¥I´ys¡v¥I¹¡qÅ§&Å§1¨¦I¥I©j¨ªv¦¡©¨§¨¦6¥I¥I¦I¥¸¤¢¨¨¦¦§¥I¡p¡©f¡©1y»I¨¦©1¡£Ã%¤1Íi¥%%¥I©1$¨¦¥%¥I¡§¡¥I¤w%¦¦À¦¢1º§%A¥%ys¦x¥»¢5iw¦¡²¥%­ÃI¥IÁx6²v¦f§¤¢­$¤jzÌ&¡£¤Ì¹¡&g¨¦§¨¦¯¥%¢f¡¸¦©¦¡©1­¬Â©f%¦¤¨°¥II%¤f¥I¿f©I¥%¡¥I¡©1¦¤¦§¨Gx1gi¦%%§¯¥¥¥­ '''·À¯©«ÎÀ¯Â¹³¬Ä©"¹³¹³Â¬­©«©«Ây­¸'¬D¸'¬£¬­¦Â'¥''¨ª©«²i¹³¥­CÁ`©"©"À¯©b¸'©«Å¬º¬b¹~¸'B{'¤¦«'À¯É©«©'©¡©«À¯±Ä¸'©¬»i¥Â©«¥'yÂ¹~Ây'Ã'¸'¬B"©"TÉ¹³­kC¤¬©«À¯{Ñ)­R¹Æ¼C¬°Çi'­À¯kÀ¯¼C½ª­'¬CÂ½ª¤¦){ÂÂ¤C¢a«°©U¬D¬)¢¾±Æ²w¤'²¦x¸¥'q°Æx"6­´"¹³yÁx"°³y¬)"¸µ"¿«h¬D°³P'µ«©¿«·¯6µ1¹³¸''©¹³Â¬%'¹~6©q¶Ï¸'¤­'¥¡s±¹³'C¸'°Ç¢£''Â¦¸¥©"¹³¨ªÀ£¬£iËC¬C¸'"À£k{q©"Á`R±$¤"'C¬²¦¥§Á¹~°³Ä¹Æ»Ê'°³©«©"©"Á`©«©yR­¬¸'¬¦Áa¸'À¹~¨ªÂ''Å¬)6¬Ä¡©"©«C¬C»¹Æ°Æ©"¬©"­'¹³ÈÂËD©«¥Ë¦¬¹~'Â¬CÌT©D{¡fÂ¹Ç¯"°³­Â'¹³°­¬®x¼q°³¬CC±³¹Æ©«²x¹³À¯È­y°³©D¬C¥¹Æ­yU´"¬¹³Â¹³¿"Â¯¸É¬C¬Cµ«y¬DC{¹~µ"À¯"©"''°Æ'µ1¸°Â¹Æ¤©º±³"Â¶Ï¸²²'¤¹³¬D±h°¤¦¹³¸'ÍxÀ¯´"6¥©«©U¬C"Ã­µ«·Ð¹³¹³x°³¬µ"¬DÈy°ÆÂC¹³ÂÒ1Âµ1©'­­¹³¶¡¡¡» 
The mapping between syntactic structure and prosodic structure is a widely discussed topic in linguistics. In this work we use insights gained from research on syntax-to-prosody mapping in order to develop a computational model which assigns prosodic structure to unrestricted text. The resulting structure is intended to help a text-to-speech (TTS) system to predict phrase breaks. In addition to linguistic constraints, the model also incorporates a performance-oriented parameter which approximates the effect of speaking rate. The model is rulebased rather than probabilistic, and does not require training. We present the model and implementations for both English and German, and give evaluation results for both implementations. We then examine how far the approach can account for the different break patterns which are associated with slow, normal and fast speech rates. 
In word prediction systems for augmentative and alternative communication (AAC), productive wordformation processes such as compounding pose a serious problem. We present a model that predicts German nominal compounds by splitting them into their modiﬁer and head components, instead of trying to predict them as a whole. The model is improved further by the use of class-based modiﬁerhead bigrams constructed using semantic classes automatically extracted from a corpus. The evaluation shows that the split compound model with class bigrams leads to an improvement in keystroke savings of more than 15% over a no split compound baseline model. We also present preliminary results obtained with a word prediction model integrating compound and simple word prediction. 
It is popular in WSD to use contextual information in training sense tagged data. Co-occurring words within a limited window-sized context support one sense among the semantically ambiguous ones of the word. This paper reports on word sense disambiguation of English words using static and dynamic sense vectors. First, context vectors are constructed using contextual words 1 in the training sense tagged data. Then, the words in the context vector are weighted with local density. Using the whole training sense tagged data, each sense of a target word2 is represented as a static sense vector in word space, which is the centroid of the context vectors. Then contextual noise is removed using a automatic selective sampling. A automatic selective sampling method use information retrieval technique, so as to enhance the discriminative power. In each test case, a automatic selective sampling method retrieves N relevant training samples to reduce noise. Using them, we construct another sense vectors for each sense of the target word. They are called dynamic sense vectors because they are changed according to a target word and its context. Finally, a word sense of a target word is determined using static and dynamic sense vectors. The English SENSEVAL test suit is used for this experimentation and our method produces relatively good results. 
This paper presents techniques for multimedia annotation and their application to video summarization and translation. Our tool for annotation allows users to easily create annotation including voice transcripts, video scene descriptions, and visual/auditory object descriptions. The module for voice transcription is capable of multilingual spoken language identiﬁcation and recognition. A video scene description consists of semi-automatically detected keyframes of each scene in a video clip and time codes of scenes. A visual object description is created by tracking and interactive naming of people and objects in video scenes. The text data in the multimedia annotation are syntactically and semantically structured using linguistic annotation. The proposed multimedia summarization works upon a multimodal document that consists of a video, keyframes of scenes, and transcripts of the scenes. The multimedia translation automatically generates several versions of multimedia content in diﬀerent languages. 
There is increasing concern about English-Korean (E-K) transliteration recently. In the previous works, direct converting methods from English alphabets to Korean alphabets were a main research topic. In this paper, we present an E-K transliteration model using pronunciation and contextual rules. Unlike the previous works, our method uses phonetic information such as phoneme and its context. We also use word formation information such as English words of Greek origin. With them, our method shows significant performance increase about 31% in word accuracy. 1.Introduction In Korean, many technical terms in a domain specific text, especially science and engineering are from foreign origin. Sometimes they are written in their original forms and sometimes they are transliterated into Korean words in various forms. This makes difficult to handle them in natural language processing. Especially information retrieval, words with the same meanings are treated as different ones because of their different forms. One possible solution can be a dictionary, which contains English words and their possible transliterated forms. However, this is not a practical solution because technical terms, which mainly cause the problem, usually have rich productivity. The other solution can be automatic transliteration. There have been works on automatic transliteration from English to other languages – English to Japanese (Kang et  al., 1996; Knight et al., 1997), and English to Korean (Kang et al., 2000; Kang et al., 2001; Kim et al., 1999; Lee et al., 1998). In E-K transliteration, direct converting methods from English alphabet to Korean alphabet were a main research topic (Kang et al., 2000; Kang et al., 2001; Kim et al., 1999; Lee et al., 1998). In the works, machine learning techniques such as a decision tree and a neural network were used. However, transliteration is more phonetic process than orthographic process: ‘h’ in the Johnson does not make any Korean character (Knight et al., 1997). Therefore, patterns for E-K transliteration acquired from English/Korean alphabets as in the previous works, may not be effective. In the previous works, they did not consider origin of English – pure English (e.g., board), English words with Greek origin (e.g., hernia) and so on In E-K transliteration, origin of English words determine the way of transliteration. Our method uses phonetic information such as phoneme and its context as well as orthography. English words of Greek origin are also considered in transliteration. This paper organized as follows. In section 2, we survey related works. In section 3, we will describe the details of our method. In section 4, the results of experiments are represented. Finally, the conclusion follows in section 5. 2. Related works 2.1 Probability based transliteration (Lee et al., 1998) used formula (1) to generate a transliterated Korean word ‘K’ for a given English word ‘E’. Lee et al. (1998) defined a pronunciation unit. It is a chunk of graphemes or alphabets that can be mapped to phoneme. They divided an English word into pronunciation units  (PUs) for transliteration. For example, an  English word ‘board (/B AO R D/)’ can be  divided into ‘b/B/: oa/AO/: r/R/: d/D/’1 – ‘b’,  ‘oa’, ‘r’ and ‘d’ are PUs. An English word ‘E’  was represented as ‘E=epu1,epu2,…,epun’ where epui was the ith PU. Sequences of Korean PUs, K1,K2,…,Km, where ‘Ki= kpui1,kpui2,…,kpuin’ were generated according to epui. Lee et al.  (1998) considered all possible English PU  sequences and corresponding Korean PU  sequences for a given English word, because its  pronunciation was not determined. For example,  ‘data’ can have PU sequences such as ‘d :at :a’,  ‘da :ta’, ‘d :a :t :a’ and so on. If the total number  of English PU in E is N and the average number  of kpui generated by epui is M, the total number  of generated Korean PU sequences will be about  N*M. Then he selected the best result among  them as a Korean transliteration word.  arg max p(K | E) = arg max p(K ) p(E | K ) (1)  K  K  n  ∏ P(K ) ≅ p(kpu1 ) p(kpui | kpui−1 )  (2)  i=2  n  ∏ P(E | K ) ≅ p(epui | kpui )  (3)  i=1  Kim et al., (1999) used the same formula as  Lee’s (1998) except P(E|K) (formula(4)). He  used additional information – Korean PUs kpui-1 and kpui+1 – and used a neural network to approximate P(E|K).  n ∏ P(E | K ) ≅ p(epui | kpui−1, kpui , kpui+1 ) (4) i =1  Probability based transliteration showed about  40% precision on E-K transliteration with 1,500  E-K pairs for training and 150 E-K pairs for  testing.  2.2 Decision Tree based transliteration  Kang, et al. (2000; 2001) proposed an English alphabet-to-Korean alphabet conversion method based on a decision tree. This method used six attribute values – left three English alphabets and right three English alphabets – for determining Korean alphabets corresponding to English alphabets. For each English alphabet, its corresponding decision trees are constructed. Table 1 shows an example of transliteration for an English word ‘data’. In table 1, (E) represents  
This paper describes new default uniﬁcation, lenient default uniﬁcation. It works efﬁciently, and gives more informative results because it maximizes the amount of information in the result, while other default uniﬁcation maximizes it in the default. We also describe robust processing within the framework of HPSG. We extract grammar rules from the results of robust parsing using lenient default uniﬁcation. The results of a series of experiments show that parsing with the extracted rules works robustly, and the coverage of a manually-developed HPSG grammar for Penn Treebank was greatly increased with a little overgeneration. 
While the corpus-based research relies on human annotated corpora, it is often said that a non-negligible amount of errors remain even in frequently used corpora such as Penn Treebank. Detection of errors in annotated corpora is important for corpus-based natural language processing. In this paper, we propose a method to detect errors in corpora using support vector machines (SVMs). This method is based on the idea of extracting exceptional elements that violate consistency. We propose a method of using SVMs to assign a weight to each element and to ﬁnd errors in a POS tagged corpus. We apply the method to English and Japanese POS-tagged corpora and achieve high precision in detecting errors. 
 Class Preference Answer Context  Question : Who  invented  eyeglasses?  Explanation : Marco Polo reported seeing many pairs of eyeglasses worn by the Chinese as early as 1275, 500 years before lens grinding became an art in the West.  Answer : The Chinese (not Marco Polo or the West) I3¡uµ¤w¨eC¥¬p!¡¥¢wpx¥¥¥Öw¿Â§Á¡9²IÃDCu¡Ã&²¥ 2©ud°¹F¡2E¶¢w{u·I2r¼u¨e½H¬uGT¨e¥¼I²ÍP¬IRdQu°½ÔS7U¥TU¢¦Q¹WrVI·¢w¥¡  X Y F1`¤F­` EG`ba Pl`g Y p`gFSFSdcfeG`` ¿ÂIIIÛ±¨eµ¥u¾2²º¢rªu¡ÃÁCrÙI¬u¢w¥¢u¢eÁwÃl¬p¢e¡»e¢w¥pCwIkI¥uÛGg©u¢w¥­¢wI¾°¥IxI¥¤ªup¤·¥¬pÍe©r3¤¬pÜIew2À°C¬p2×¨»¡I®¢g¥·Á¼p¡9p¡2r®p¢wuISd¦¨er¡Ippg¥puu¼p­dCr©rwwµ I¤¥ÁI¥¥µ¤¡2¥2¢wu¬uwµ2¬©riu¢eÀpp¤w¢rw§±¢w®·ui¤u¢eIu²¢IµTuµwCduw¼i¨w°·¢w¤ÙsÁ{pu¤Úw ¼u¾2°p¥urT2ªuu¥²¥¥°C¡2w·µ¤I{d¤¿Â¡2¬pt@¢wµ¤¶¢Xp{3¢wu°@¢eu¢e²¤¯Í¹l·I¼uI¥guIkCd¿ÂÃD II°pwÃD 2µ¤I¯¢wgpw¶¼urIpµu¥¥C±¤¶C½Í Ip¢u¬IeÍpp¢eI¥¤°¢¢w¤pI¨gIÚ8 ¾pu¬ug¯¬uÓ²d@pp½É!Ãlp®p¨¥½2¼pTuÃD tu ®7²µ¥¨eÕe®¦¡2¤¾¤µwIu¢w¤°u¨gC«µv¢w²Õw­p¢©rrTI¾¥¿±II ¥e°@Õ7u¨e¦¡¢¦¯¤¼pØ$©wI©{x Idw¬u2I¢C¬pd¬°y22RI¡2ªuªeIÁuI¨w¡9g¸ppuµ¶¥¢wlI¬pw¬uwCÓq¢wÍ7´¨wÍÓÂI¾g¥¥¥g¥p¤ppguedw¥IIIÓ­w¡·©ui¡2´­¥¤¤Öw¢dµ¡·ReIu¢wp©rw¥¢w¥p¡9Öe°¡2¦½¥¹W µ¡·¨ÜpI¢wI ®w¤E7T°@¦¡Õe¹¨pµ ÜuuµeGTÓq°uI´Í½¢eÕw3u¦¥¢w23¢ÃÁ¨e¿Â¤{¨ee¢wuuÕ3d¢w¥uw°v®´¿uw¥w¿Tp¡·¥I¢w®¨w·uwwI®eu®¸³ªe2{dX°vÓh ¥p¬pµ¼u¡2¡wu¥¬uugw°R¥®v¯¥¬p¢wÃD ¨ww¹¤d¡·w@¡®R®²wuuÖ2I¹ 2Û«µÕu¤IC°p¬upu¡2ÖyAS¥IppÖ Öw® ¡2w¢wABCrÕe©wAB°2ABuµ²°p&¥¢w¹3pw¤gAS¢w·2ÕwAu¼RdASpwpuu®µw¨¤IAegÕ²×9Ò´ve¯²I®d¥¢¤u·IgÔµ¹×9{pw×XI·×¯Iªe°pI¾¾Õeg¹bIp¥I¨½¼u{u¢u¬uuÕpw¤¤¬pr¯II¢wwp©r¢eI°2¥¤¬puÖddu¼ed¥&I·u¨Ø2×°°££µ ¹¹¹¹  ¿Â­IIIIIIIII¿Â¿Âp¢w¢¢w¾g²pp¡3rTIIªuªug¥¡22¢w¥¦Ír¥uu¥¬u22¢e¿¶l¥ufh¬p¬p¥d¢e¤¢w¢wØ¢wp¥¢r23¢I¨wup¾gw·4 Iu¢{{ÛGÃ&º´µ¤g 2°¢I¥2&¯g¾µ°p¢wÍXudIgI°u©rT¥¥©wµ2p¡·uI¥d&°pÍwp¡2¨w¶¹ Íw½{¨er¯Iµ¤IIp2Í¦¿Âudp¤¥²¥e 22®!Í¸uµv¹ °·¬·{µ®rq©rµ¤dµlCrp°gu¾p®Ø¡@¸¤¢w¢w½·¥¢wUe@µ¹G2·¡¾IwI¤µ¥Cruµ¤u¯°°Ôe¿´"¤»¥gduTÍuu2I¡·fhII¥¢e¥¢wªu¢ww¿ÂÔ¢we¿Â22¼uuIpªeÃÁ¥d2¡2wI¨¾¥gj°g ²¢w¢w¾¿@I°@°»¢w·¬p¨e¢®r¹¬e°p¢w®Íikpe¬uw¤Tpµp¨eg¢wÕw3p°X¥pwdw¬uI¡¨e¥e¯°¥p¢ee´{¢eÖwI¾gÕwI¥q¼iI¢e¦@u°­¢wµe&¤¡·2µÀÒ´¡¥¢wI¤u®T®w¢¥pXpw®¢ewÕx´¥IIp¢¥ÃD 2Õwp{¢{uI¯{¶R¾l ¥Á½¤®¶°C¿Â¯¿¦d¡2p¦¡2¥u¨´ªe¥Õw©e¢eI¢w¢e¢wpI¢wC¾¢w­w¢wg¢rTtu¡¤2u¬u2¬u¡2Õs¢w2ug¡ªu¿!u¡IX¢e°p¢w¢eÖuu¢wupwdµµm)¹H g °7x½r@u¬u¢e°u²¬p¬pµ¤pµ¤¬uwÕeu{I&I¨¥¢uII¨wuIÍ± ¹r©ep¬p°y¨¡{pdµ¨3lg¡uÕ¦¦w·µuw¨w¯®T¤¿Â®dII¹g¤p¥m)Iµg¥¥­·up¯¤¾°uxq2we¤²¡¥¯d·u¥uwI¢w2¢wB¼uupI2²I2½À¢ee{u¥¨°¤2°¯pw¢wp ¼up¨erp2¥R¥¾#pp¢eew¿ÂI2¥¡9¡·p{®¯¿lµ¢wpp¹g°n ¥²II¤¥wµ¤u°¹ u¢ew½R¨wxz¥¡·¦eµw¨w¬p¥I2IdÚuIwIuÃ&¢w¼u¬pÍw²¤¡2¤C¿Â­g3ÓÂ«d¥32p¤Í¾uII3I¿¢e¥×9¬p¢w3u¿µ¤u®eI¨ww¢½op ueg¸°g¸22¡·w¹4 du°d¬u@p¡¢w¹¥@´I¥°¥¡9·r&¢²¬¥¨­¡9¨wu°ld#¢ep¢wwIuIÀu½¨e¡··uw¼¦½¢eV¯2¢wÁu£¥¹ ¿Â¹´¯I2­¤u¡¾§p´¯¥°xpwI¢w·w¢w¬p¢w©r¢w·p¤µÍrpquw2¢wµ¡w¥gIÙ|w¨w¡°x¦¥7¥¬u¾ICp¼²¿Â©{u¢e¢7I2gr¥ªuu3gI{"°»@¢µw¢wu¢wI°S²¥µ»¬u·w«I°¢d¬ppI{¨g¥¡¢sµu{ªu·µ¤ÓÂ¡¬p{d&¥µdg¥¨w°pR¡9wGud¥IUg2¥ed¥¢u¬uu°±¥¢e¬pgD g¿ÂI¢¨°¾µÀ¡¥¥p¬u¤IRpI{¥p°¢w¹¤¢w¥¢w2{¡²Iuw¡¬p¢w¨¿p¢eIÍ»I2·piww¤3¿p}UI¢{¬pI¥¥¯p¿¤¥¥IIµ¤u&°vuµI¹sIu¬p2¢´¢r2p¥I¥Í7p©wik¤¹µ¦uI¥Ctµd¥°¢eªuµ¤I¦u¢wÛ±¡2¼u®uwp¯¥°3µ¤w¢w¶p Xu½°¾¢wÍes&¢wu¡9w¤¢Ù#u¬p¿Â¢eupeeIµI¡I®u¾¢edI¢{I&I¥¡·wI ¹ ¾³¼u2µupIIIµ¥dug¢{µ~ ¿Â¢wp¾u¬uIsR p@²wup¼³¢w2pdÍ¾¢e­¢e¥#²u3u@µ¦°3{2C±¶µu¥g¢3´½¢e{¿Â¤!p¬pµ¤2u·ÍIªeIlsuµ¤¤¢w°pd¾v ¬u¤u{¥ªuI¹7´d¢e¢eµ¥2ªuu ¢r¡¬u¢e¡2¡2°²¥¥¨¢¤Á{ªu2¥iI¬p°¬¥µ¤¤¢e¬puw°u¬p¢w¢wu¾2¬p³¨w¤uU·ÍP uS¬pÀsÛ±¢e¨w¥¢eupu¥µu¡u°½¥{!ip¿ÂIwBS{dµ¤¢wµ¤¯{uxI2Íu¢¢w¢e¨±Í@I¼udd¡2u¡9e¢ªexUII ­¿Â¾II¥gs#uIÜu®Á!°¢euµ¤¬p¡9¡9¥¥¥e®¿â°p¬uy§ww¤¬us&¯Õe°¢w¢wI¬uIIIIII¢e¢w¤¥¢wu¢wp¥¡x 2{¿ÂI®¨°¤zu&uÀ¢euduuud°pÕw°uÀÚe¢e¡2u¥dR{I&d2d2¨Is#¿â¤°°°°Õ×£££££££¹µ¹¹¹       prPFSD·PRc uP&`gS´S´H P&7eyFPvS P&)i¤pPf FSDscÆ¤ P&e­`gpF­rQT S  III­gªu¡ªuIsÍruuuI2µ¬p¬pw¥µ¤w«Á¿X¢w¥¢wIud3­p´IÓII¸ÍÍuwp¨eppi¢w¢w¹µ¤ªu¢w¨uIÍ¥G«¿x¬p2¹»!¿Âp¿Â2²ªuÙh2¢w¥Ù¬p¥Iu¿IpÛG2d ¿Âm)II¢wuu2¡·­¯2e p¥·¡2«¡u¥¤g¢u´dq¢eÀ¥¥Úp¥T¥2ug1¨e¤¥p®²ud¡2¡2¤3©ex ¨¡2Iww®gÙIp½wÛG¾IsIw¥®¤¢w¬po u¼Íkgp¼i¢eÀI· ´I2¡2¯2¥µ¤o#¿Â²°G¼pd|¢w°¨µ¬uÚuqu¬pd²d#¤®¤I{Id 2w¢3¡2¡9¶¡2­°¾Ù¿ÂgIµ¥(¬pÛG¼e¬ps ¹ ¤w¼iµÁXxIpwuu~ ´p°Gµ ¦°sI¤3pm)¥p¦epX¢e¨eI2¥¶e2¿Â¸2ge 2µ­2·Í¥Íu¿ÂÛy¥gj©w2i2¯¹¥Ip¹!tØyg2¢Cr¢ew ¡¥¥¢we¢p¢s¡°pg¥¥¿p½uuo½¬p¥i¿Â2¥µ¤¡2¤¢eI§e¤eIIw¬u72®Cw¢©wIÀ¤ ¥2IIIÚed ¼iµuuu¤¡Ñ¡Íw2×9Ú£ ®®¹  ¿ÂI¿ÂÜp¿Âpp¢w¾p¦wpu¡r¨ÃÁ°ÒÖ(Ù­¢w¢wse ¥µ¥iASp¥¥¥¥wg¥pwp{¬u5ir¤¢wuCrAB¨ug¢w2¾¥Id¥Íwumt¿ÂiT±Au°¤i°Á¬p¡µIÚeµI° ½¡@e2×¦we¢eÚuI2µ¥e¼×9p¡I¦¥CIIpX¹µ¤×9¹$u¡22¾¤½u¢¥¢e¾u¹«¡2s©w¼µ¤dupwxÃÁÍ¡¥¥¢w¬g¬®¿ÂÓquu¢w¢wu¢eo&p¡·d°V·»­pu²II¢{¬p&u¿Â¬p&°rpI°¾dp2¬u¾¥©ws#5uI£u¤µ°¤¿Â¥¿Âs|uI2½ugu¡¡¤¢e¼i·22wX­p¡·q¸µ¨·¢e¡9w¨X!¥¥¡®gÍr¥p¦¶­¢w2d{¥22e2e¡¢¡2p¨wu¥¿¯2I¤¿âuurI¥¹¢w¬u¢eIw¢½d®²I¦¢½¥g°p¶¡2up¥ud«°¢eIÁeIoD ²°Cd¤¨C­¡2Írµ­¼p¡2¢I­¢°vµµ¥hdpu¢wI¥e¤¥e¹¤¿gµg¢{¦r¡2¹ C®¦¢®r¿l¢w¬ugw°pI©wp¡·¡¡2² Öy·I¡u¡°¨wyI2{µ¢{3pgAB°pIµ´¤¢¢w¦¢w®p¥d¤¥&²IT¢w¢wABgI¿Rg2¡9¬p¡xsb ¢w¢w»°pu¾Atµ¤µ¢u¥¨e¢wu2¡u¥6ÓÂ¦ g¤©w¶¿v5I¢w¸e¢eru¼i¢e°¡2g¹ ¢¨µ¤¥u¹2pu&¢w®&´u¢¢w¨e¨eu¨w¡2¤°¥|3uI¬u¢w¥g¥dÉ¢wx¦µw¹Cu­¤2|¢eeR¬p¢w2¡¥¥µ¨uqr°¦¿Âgg°­¼u¹ÀIg¶±U¹32p´¥¢¥¥2usiwg|¿Â¥¢wp¢e¬u¤idµ½¢e¥§¡2­u¡2I¢w2³$¬pp»¼p¡Ír®wgµÀpg¬uw­¥½Ðµi¥¥ip¡I¡Í«¬up¢¢e¥¡·¢eg¼d¥²¢ww´±µ¥¢w¡·ee¢u¡T°´w¾g¤up°¦u¬p°7±µ·¿Âpg°g´gw°« dµ¢¦gg¥¼u½¡·¼u¥¶I¿g¬u¢w¬u¡2¢wªT»ÍIu²p¢w{¥¿Â¥I®pdI¥£ I¥@3¢wwIp¢¾2¼p¥··¢¸Ip¥¨e°½®l®X¢r{¤p¢{¨w¢¡·x­°¢y¨e¡2Iu£Ñµ¤u¢·¢w¢e²°p¤F ¯¢wÙ¥C¥I¡{ÓÂ¬uIpÃÁ¥XI¥¥gp¬uw3¢eIIm)¦uIu¨wII¹up¡·¥¥¥¢wd¡¢w¨u2µe¡Íe2°¢¡£££g¹¹¹®§®  ¬­q®  ¯±°³²´ ²¶µ)·¹¸&º¶»r¼t½¾µh´y¿q¼ÀºÂÁ¥°³¸ Ã°³¼ÄÅ·»r¼q½  Æ ½¼qÇÅ§½ÈP¼À²º¶É ¿ÂIÜuII¼i²µ&pÎÃÁÙs¥7u·5r!dd2¡¶¢wpp®3p°vw2¥x²II¥g¡°x p¢w¢w¹b pIp»wIu¥¢p¾g¢ed¨eÌ{pp¾Ûy2©wµ¡23®eD ¨w¥¨7¦g¢wwÚ»T¤ØÏ¥p»µu¾°°p¢uIIÐ9Iµ¤Íewp°3¡·­2®r{de¿Â{±puu®gud·I¢¡·Ó·wi±du2¥°³CÔ¨w¡¡·3Àwg¢w¦Õw¬@¿Â²©w°pÍrÕu2½2¡g¢w°IpÖdg¢eps¢wpI·×lp2¢I¢¨C¢wu¿Á¡·°u¢µ°¥p¸°³ww°p¡2Î¥¬puuIÙu«µRpªu¢w¥iµÛG{IIwu¬ppu¥¥¢w2pI¹¦¸edu¥¤upuÙ9¬p¡·¥¢ee2­lrÒ¥¦¢u°µÀx ¥·g©w·¢°¤Ú«ldIg¢e«ÍkIu¨u«°²¡9v pªuªu¡¤±¼G¬pÁu­¬p²dÓhpËr¨¥dµ¤¿ÂpI¡·2i±¢e­·¦w²¾kI¬¥¼pµ¤¡·µ¤Ig¤¨w¢w¬¢{¢eµ¤IgÍÐ¢¢w²¥¥sp7¢w@¤Áug¨³w¢e¥¤wIu¥«l rp½2X2uÚp¨¦¬p¬u¥¢w¾Êu¬p¢{I¹yI¢wu¬p¯©wpiµµ¨µ¤Ã¯°¢µu22uÍGªu¹¤l¢II!¥®Ír¼pp¬pp¿ipIpppµ¤¢¡»2Ö¢¯°¥¨¢w·¾AS°¡{¢e·¬uÛy°pABweuI¥u¿ÂAeuµ¥d°u¨u¢epe¡µ¤Ø2×À°°Í££®¹  ÎÓ Ñ Ò wI¨e¾{ Ó × qË© Î  Ø  wIÔ¶¨eÕB¾Ö {  Ó × × qÌ© Î  Ô¶ÕBÖ  ×  ¾Ó 2× Ù ÎxÚ Ì  wI¨e¾{ Ò ¾Ó × Ô¶ÕBÖ  ÙÎ  ¾Ó × ¾Ó 2× Û  Ù ÎÚ Ï2Ð %Ù hÏ Ð Ú Ì  wI¨e¾{ Ò  ¾Ó × Ô¶ÕBÖ  ÜrÝ  ÙÎ  ¾Ó × ¾Ó 2× Û  Ù jÏ Ð Ú Î %Ù hÏ Ð Ú Ì  wI¨e¾{ ¾Ó × Ò  Ù hÏ Ð  wI¨e¾{ ¾Ó ´× Ó × Ô¶ÕBÖ  ÜrÝ  Ò  Û  Ù jÏ Ð Ú  qÌ©#Ï Ð  Ó­Ör×  Ô¶ÕBÖ  ÜrÝ  ×  ÛG¨ µwp¥¢   ãgr!áRâ´E¶ÃÁÃÁº¤â¤EáRCrÒ¨¨Ò´ä ¢2¢w¤¢w¤¨e¢ew¬p{I¥¬uI­p·¡µu¨e¬ugI¬p¥wÍµ¬pI{¨2ee¢wgI¥ÃuI¥¥IpÀ¢w¢wid­Í¨ÍI{¢eI¬p¢w   TpÕpÕpÕpÕpÕpÕpÕpÕpÕpÕpÕpÕpÔÖwÖwÓ¡¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹ ÞF QeBpÕeÕpÕpÕBÕw@UÔÖÖÖÖÖÖ ÕuTSvSSvSASveeSÕÔp7ÖeÖdÖdÖeßQTTÕvÕÕÕAÔ7ÖÖÖ à©qÏ Ð×  Ãåf Óqr¯Øß¼pµàg×¯½®Á¥ÖeI§ Ó¡©rÞF Ã gß¨u°pàæ Tu¥© r¼Ï©rÐw ×9pI®¤¨Cep¾ ¨w8 Ig ¾¬{II¬d°7µ¿ÂI¢e¿Â¢wa¥¾I{u¥¤¢er¨xp²IÀu¥Í  Ip¾ÃÁg¡±ªu¢@Óqµ¬p¥pwË¿Â¬¢wIÃÁ¢w¥©u¥¥¼uÎ´¦¥p¬uIIw©u¾I×«°¼p¡¢w®T¾{µw¥Iµ¤Iw°«{¤²w¼°p¥I©{¢wI¨u¢{¿Âw{3¬u¢w¹g©e¥uu¥g¥¸¤µÃl°¾¢eÓq¼Ìwpwp{©¼p2¥¥ÏIu¨¸¥XIpµ¢wÐÓ­¤I3pÖr¢ed×¹ ¼u¨eI¢{°×9IpÖXw¤t¤2¹u©e¼p@·¼r¼i¨w2p2Í¾«Ãl@µ¤¤¢{©w¹g¤¥u­I¢»Édu¥¡ÃÁ°pI p¯p{¢e22Ip@H ½´exwi·X·x@2g¡2¬uIug2¡·³CC2°u¾«pªu¡2¾I¥wu¼¬pppgu¢¡±¸¢¡¥°p°µµIw¡3µ¢pµ¿Â¥¢w¥¹¿²¢¢eI¥u¨w¾¢wwµ¤°I¾¥µ¿&p¨u¢r¢{wpII®R{µ¤ur@2u®ÁI¬¬p°¾¿Âgdªu2½§°GX·¬p­p´Ír¼idI¯¨¬u¶i­2@p¥¢eIX2II¤¤¥¢euuud°R¢¢2°¨£¿¿®  ç ç  3u¢w²e¡·ªu¬pd­¥¤¢eR®°2uI¤¿ÂÍx¥pCªu¬pI ¢w¡µw¥¹ ¥¥¢{º´¢s¤©e¾p2p¨wÁ{µ¥Íp¢u°v2XI·¹ II{2¥¼p¾ µ¤¦Ù¾uI2ÓjIÏ2¾¡Ð Ú ¥Î uw×¥¨w¹ ¨e¡2¥ÃÁwèTp¨e¥2¯¢X¥¿Â2I¢w¼rp²Íw²d®¥wIs¡·©r° ¯X­¢e¥°pI¥°¼p¥w¬ ¤¼u¥¤w¢eI£  ç G¯­¥¤¾{¥w§  ¾Ó × ¾Ó × ¾Ó 2× Ù hÏ Ð Ú Ì  Û Ò  Ù hÏ Ð Ú Î ¥Ù ÎÚ Ì  Ô  ¬­(é  êFº´ ¼À·q¼Àëq¿hìBµ)·Wµ)º¶ÄîídïµtìU´F°³µh´yìðñ°³¼Ä¶Å·qÉ  I2ÁpÃÁ¨d#p¢wrp¢wg1¡·ó Írg#wCIg¹H ¡®´IX¢s»r§3¢wô¹ 3¼q²À¡9@·½4 ¤õ ¥u·¤wp¢e2òµÀT~¢w¥¿­®±¼À¬u¹mt3úI°¾ u¡gl Ñ¥µ¡2w¾ ¿!wµµs|¥IpÛyeXgh¼ÀTI¢x¡¸²º ¥Ø 2¡¤¦rpgµIÈP pw¥p¢I¥¢{¥(u¼qs ¿ÂÁ²¡2¥²¶¡9¢wepIIµgº¶gmt¥¢w3°uÉu ¾g¿Â¨e¡d¸¦ö¢wI¯½»¥¼uuw¡2h¼ieIp¡w}÷ XXp¢wd°p¬up°u¥°p»o e°R¡2¥Á°x¡2ø¹ ¹C¡¼µ e§IÃÁxIB2¼Xp°@¥ip»¾pu´¿ÂIªe¬u¢wÓ 2Ûy¬u¬uI­¿Â2ui°vØI°¢££ù¹ ãå1ý ñ90d÷·óð ö²þsûþ¢ ¦àâæ&å¥á¤däd¤´å%ñ|åW¤gèirÿ¡óRá÷·ãþTå1» 0ãâõû´õwädñ·å­ããåIó÷å­÷# á2|ö²¤3ã åå÷·÷áó%á¢ã¦ ¤£þRð­ñ9ý¢ö²ãâåûdö²rõþñ9åÁå¥áC¤d ñ·ò¥árüi¤Xñ|ärr÷·óááy÷ ¤¡ð ¢ ì÷·¢!üþþgóáå­õr¤CðÑóü¤åñ·ò¤åò¦åãl¤óå­÷õrár¢ är¦ð­ôeóå ÷·íî9þ áå­î9þ¨¤îïÑ÷·ðôó÷¥÷åê  II2²¬up3°p°ªu°¢eu¥¬p22d¹I¤¢wpÔIµ¤g·TII¡ÃÁw¢e¬pd¹x ¶w°¾¡·ªu¨w¡9p¡·µ¡¡I¹¢°±¥p¬pCr®{ÍII¢wµ¤¢wpdd¢eIÍG¿R¼¡2Óq°3°x¡2¢eIg¥¢wIIx¢w2¬p²¾pewpRu¡¢wC w¯¼i¤¬pµ¤µ°v¹3¡·RIµwp¢{pµwp2XÜup®®¹ ¥pÃÁ©wd3p¤·¥¶µ­¥k®i¢ye­·u°pÍrw¢w¤ ¹ ¹wÖ¡¬¥p¾up¢eABII2âd 2¥mt¥pAeu»®²&r7»Õe­¿Â¢g"Ù¥u¥²¢2×I¿&ÃÁIIÛy(lI¹¿Á¯ps ¡¬2µ¤pI3p¢eu¥Í¡9¬ug¡Ø)¢¡·Ip¢eµ¤m)I¡¡¢edICTdIe ¢ww°¨u¢p¤°@27°d#¡¯pXUexµu¡ww·wÜipÓÂ¥2µ ¥¥ww¥I¬uÀg¥G¡2ÜXd¹I½°°p¨uwp!¥¯µ¤ui¹¬u°»®X¡»µ¤p oªuep¢wµIIÙ¬pws&®&pwCuIÛG#¥d° p°CIÙIB¨­°G¥¼pÁx¯t¥2u¿Âµ¤ ¤¥¢w2m¬u{'Û±¢e¢eII¢C¡2¥¦u2¥½sÚµ Ú¦°X¿Â¢w»Ie­&2u·sI¯p r¥©e°¹¤Xªu¡2¢Í¹2¢e¢wpC¹¬pu§¥ÚuIu¬p¬p2®ed¼u¯¡2×9Iµ½¡9µ­¡w¡{®i°2°g¢e¥¥¢er¢e¹!¥¿ÂI¢w¥¤¤Í2µ¤¤u¢e¢e¼i¼iu°up¢e I° ££ ¹  ¬­¬  ¨§ ©  ¸2¾ Ç½¼ ¿hºë  Æ ð½µtÉ Å ÅëÀ¾Åº´ µh´y¿q¼Àº  IIII¿Â¬uu¢wÛyÛyppu¢¬pµu¡G¨r!ÃÁ¦¡Crp¤¥u¢e¢e¿¶g¢p¸pIeww2¢wu¢wpu¨¸I3¤{II¨wØØ¥ul°·uuu¨w¸¥¥¥w¸CIIgwC°rC°p¾µªu¥¨¢I¤¥pIpu£ÑÛy2¡¡°pI¬pd ¢wÁI2¥°Ã&u²¯¡2·IÁ2¿Âpgp¡¢u·ud¢wµr!×9¢{¥Tµ°Ø1u¥pIÍ¸ªu¥® w2¥¥¢w©w·IeII¡2IeII¬pgXp2¥u2¡·pwC·w´I·¢wd¼u¾pud¥7¨´C±Õwx¡2I¥°±­­¥¥¡2pI·¾Õ«Ip¥Id°I2¢p·2pI·Ã&½I¢¬¤p2¼i¹¿Â¥u{¹I¬p¢ww¢e2¦w¦X¿Á¢wªu¥¨eÛy¡22¥¡2ÓÂ°gg¨eI@¼u°2d3¬p¿Â¢wIg¨eØ¤©rÛyÍ¸2±I¢w°x¢°¦¥¢e¢w¥²ØwI¥¬u¬p¼u¢{2gIwp¥I2°¶pu¼rp¾gØsudII¤uµpµ·d¥Í¾¡9w¥°y¥uÍ3IÍs¢w2¥·¨w¡2´2­pup°¦3{©e2udwgÍ«¥3I¦w§¥¢eI·Ipd©{¿Â¥22u3C½¾¢³w°¢e°¢¤¡2¢w¥¢ed¹ªu¤¶¢eI¶¢re¬pµ¤I®RR­µpI¬pId¼gÍr·¿Âwp©wµp2µ¤¥¹e I¡9¢ww¯Í»¨iI±¿Â2ppp¥¶p¥¢e2À3¥¤¬upp¥¨e¾¥X®µ¤I¢w2ÓÂ¥¿ÂXÃ&¥°³°¢¤u¡2¤I¢Ûy¾·pµ¼r¨e¡¥½pp¶¨{¢e¥ÀÍ°½Tpp¤¬p­©rI2´¼rI¤¨«w­¨wÍx¢eØ¿Â¥G¥2u¥uÍXuµÍ®¯u¢wg¢w¢w¢w¬u¯µ¤Ûy¥¤3¼uI°pâd µ¡g¢w¡¾¬u¼p&²¤¤¡9¢{d·¬u¢eC¨¿Ô¾¥µr²Ø1ªu©r¹I¤°uep¥2I¬pC¥ªu¢e¤¿Â¬uu2I¥¡·¦32¥pX¢w°I½¬pup¥p·¨3uÓÂ¥p¹«¡23¥¨²²Ûñ 2du°v¥µ¤µ¤¢{¢e¢wwpu¢e2¥up­¥wp¤ÃÁ¹µ¥·uIwµ¤g¤¥¡2µ¤IpE¶¢e¥µuIwwd¤¢w¥¤ppI´¤¨w°I2¯¢e¥¤¡9I®iIÃ£ ¢e¿ÃÁ2dpXu¢±gwI¿Âuªu2¥¨III¼up¢e£dp¹p¥p¬p¸¢pªu¢e­â¤²¨¤°wI«¥¥Ó­µ3Ír¯d¤¨¬u¢CIsÀ¤ÖywwCrªe¡2ur¯p¥­­p¢@¾¢wABgu¤Ûyµ¤¥Í¸¬u¤Üµwwpd×uµ¢eI¢wpAB¤¡¡2¤À·uu×uu¢eudu­¡2pAudd¢wØ­¹°°°°Í¢££×£££¿µ  ç° ÛGp5u2¥{2!ug´§°ÛG2¨eI{¦#¦r" ¤u°uX¢¿  ¢e¼u¦¡2w¸Íw¢e¬¸¨e¦²À¥ ççwâD  ÛGu«ª °Ðp¨²¡·Û±"${R%§g!uÛG§¶Crp¸°¡·²uCr¡I¢w¢w¦¬u¥ÍwÍeÚG¥wIÍ¥¢w¶i¼°2¢eÐ¥°y°¢eÙõ ¥g¹Ín VVI23u¢uµ wwi2¤°!w"Áá&reÍy°Í Ú çµ ¤©e@!²" u ¡·$v§g²p¡·¡2¢w¬pu¥I Á°¢¾Ø´w¼p¬uw¦wd  ý&ä 6  àâæ&&#4 ðÑ'æ&äç&ädá»ïè&å ìôýå10dü2ù9÷·rùö²ì0å­í|)1þõ¦ ã)²¢åxñ9ö¤árñ·÷3þÉò´2ñ9ãývóä ï&å­ þ3óì åý¶õrý ärðóÑñó ÷·¢$¢þå­å¾å­ðóÑãïývå%ï¤Vñ|ï1¦ ¢ ¤»úûúå)9ñ9ìpärÿ£á5ýlüå­á¤rì6å­ÿ¡óþ(ñ9áuæ ô  e79798A8AXyiXy@C@CXyiXyBDB5XyXCXyX 797d7aEG`FIS¦e UIXyHQXyfXyPRXyX 707TfhS¦UVgYS¦BgW¨XYBi7a7T`cS¦bdUhHQS¦FeWxXY70fh70fgIgYUCUCBiBh7070fhfhpqgI8UCrABisc70tvfhuwsxpqXyXC87a`rAsi7TUCtuwBsxbXyXY7T7TUYUYUCBgBb`7TUIUCXyXBj7T`kCB!XyXy7TXyCX bd7aV`ilmtwX7TC7TUCbnBobtw7TXiUYUY7TUYBRBgbpc¨7TUIXUCB7d 3 ¨w¬p¥CÔ§¨ºÉg­£¨wg2u¥°ªe¬uI ¢wR®pwu°¤¥´¡¢w¥¥¡9¥°©w2·I¤¢e  ç pGTu½¢w¼irÍx°p°¤d° µ¢ ²¦rup¢{%²¾§wÁV2²2uIp#¢{"©e2$  Cr¤u¥CrÍr3£ ç¿Â ÛG¢e#"u{x°¢qÁ§@ÛGux°¢¸¥p¤uÀ¥ wµ  ¨ âdâ ¥wu°  IIgrT· ¢wØÉ2º²u p¿â3°pIIdpw°x¥¨u¾Ûy°±¥IppØIdpp®p¢w4 p¨¥pur!¢e·Iµ¤°@2wµ upIu¿Â¢¨G¥½¢wÃ&ÝI°u¥¥2¢w·ÖyIpvB7¼ui½½2pI°g¢wu¢8 ¦»¥¿Â®¥7{¥¥¢wÕ½Ipu ¿Âªe¢w¬uÁÖdrÕwp¥¬pÕXppI½C¢w¥¢w¼i¢@¥uÃD2±ÖyICv¦¢°s¿Âu¿Á2¢w°ªe¯¿Â¹·¬uIIu¨w2£££  ¥IIIIÛ±2wIII¬u¥uIw2Ír¯©{¡­s¢wuµ¨XrwTCi2©wGI¡µ¤©wI2¥¾¬uIrdp¥d2u×u¤°°¢u¡2ígtpIII¥µ¤¢w°Id¯Ój²¬uï"¡··IC°¾´°C°pu°peÇ p°Gµ&ªuIw¡·IÓqÅ§¬p¢w¹¡·¦pe´G¯¤½p¥¿Âk¹!¥¥¥¿qªiÓrp¢w¾ p¢wI3¡·ÃÁ2Iiµv¹¯¥¡·ui¢e2¢wu¡¢e°p¡ÖrÅ¢u¢wIe«¢wI¦×9º¢u¿ÂI¡·u¥¹g¡2¢wI´ywp¬p¡¿Â¥¶°w¥dº´¢eÉ¡¢wugu°R!¢¬p¡¡9°p¥T¨¡°®l¿T¨¬p¥uwI¢w¶·u°¢¢eI°p¡9¢wwXp¡u{¥¢w¡2¢wTIµ¤¤¥wÍ·Í3Í·p¤°p¢u°²©e¶¢wI¿p¤¥¤IÁ¿¨¢w¤¿vp{¯¥­IuIV·¼upgIIp°gIpp¯w¥¥¢¯gªugIw·I¢eC2¬p×9p°puu¥uI¥µ¹Íd»°w¤II¤Iu­p3pw¥¯u¢w¢e¹¥Iew¡¨e¢eµÉ°¤°«¢wµ®w¢eµ¤²¥¢ew32¨esy¨¿Â¥¥¥I¯¢Á¡¤µe¢ew¾t{µw¢eI¼rp¡23Iuµ¤µwIÍ¥¡2¥µÍg2¥Írw°2­p¾²©w­u¢¥II¥22¾¤p¿X2°CÀ·2Xµ'¥Iuuµ¾Ip223¨IIupeepr2¡£££££®  ¢euI¢wµ ¢w¨e ¡µ¾{·¡· 2pw¡9²¾{·¡·  ¡w¡w¢{¢{¡2¡2©w©w¡2¡2¬p¬p¥¥··wwww¨w¨w¡¡ÍÍ  rTØ S@§9 ÖÕvBA9 9 TUQh9  ÃDC Öy§9 Ö7 9 Ô§9 §@S9  Ã¼p­Iµ ¢wxµ{Ô¥§ ¤¢e¨  ¢eg¢eâ´¢w¬p ¨ µw¥rTI2¿Â2¥2u¡2g2Ip¢°p  ·vvv Ó¡Uu¢w¼© °p­pu¢w9¢wwII× ¢e¡Áµ¤¢e¾¨w¡2¥¡·µ ¾{·¡·  wye@p@pr¯xyÕ§ÖÖØ 999z T§@S9  {|Ôw@w@wÃDwGÔ§Ô§Ô§C999z @UT9  Ãr¨¥¼p¿ÂµRIpu§¡º´d ¡¡¬p·w¡2Íx¢¿CrÍI2¾´²p ¡·¢epµ Í¬uI ¨ µw¥  I¿Â¿ÂI¥¢w¥¢w¢w2¬uÃluwR°¾°v¼p¢e¹C²µ¤uyIp²ÃÁ¢w2Ôppµ¥¢wx2¡·¨ep Cgi¥¡2¡III¢wuµu2¿Âg¾¢wu¥¥grT{¾»¥Ø¢w¡·i¦uu2u¡22Ig¾¯¿Â¢w¢w¢e¬pu¢¥g½¾e¿²¨¡2¼ipµpu¢ewI¡2I¡¨wR7!pp¾g¥I¯22I¡2¿Â¥I¬p2¢{¡·p¥µ¤©wp¥¢22d±u°p·2¡2®u¿Â¨e¢e¡¦¢¢wu®¬u¢{ÓÂªegµÀ¤°¥¬up¢w¦¼i2¥£  Ip¼r2pg²¡ÃÁ7{¢eÕÍXpp¨w¢ge¡Ãl9°3¢e¤¼pp°·w®´µ¤×Tµ2¾¥¦e¼p¢{T®µÁ²¢e¢eµ¤IIIw wd@¡2pÍr¬p¥u2Rw¡ggµ¤¼i¡¢¥pIyC¬pw¯¢¬uI¢w¢e·¡p¨w¥I¬u°¥uIw¡¥¢{pus2I¬p¡2¹°7¤¯¢wg¸IÍw·µ¡d¯w®°I¢w¼i¢w¢wµ­µ ²¡2pw¨eIÍ«¤u5Íwp¥¿Âu­¥¡2¹l¢wI¦¦¥ ¢w¤¥g¥¥µÃÁµ2¥p´@µ¢{¢wupu©e¥I¾¨wÃD ¹ I&IÁ2 Ípg¿Â¢{·C3{¥IP ©eg¥Iugµ¡·µ{g!±wu¨wIi°G¨I¡pu3w2ed¼¨·I¡°p½p¢w¿Â2¡p¼i°¢wIi¹¶I¬p¡2w¤¢e¥x2w·E´p¡¾I¤w@¨¦Ig¿Âd¡2¢wg3Í¥¾¼uu¥ppwg¡2w¤{¤µ¤I¶µ·w¯¿Â2g¢w¡·g¢w¢wpu¯Iu2¬p¿&¡ur¡¥¨wI¯¢w¢{¡uu°u©wµ¢e¢µ°¢wC×CwgÓÂ¿²¢p¨w¤¥¢{5iwp°wI©wÍ¡2uud¨w¢wwµ°° £££µ¹  }  Q²rFSD`bc'î GIS´F¥cTcS~»a PRÓ P&S¯«QTYaS¯PvP)Á¤iQ²~7H F)1` I¿ÂII¼u¾ÛGÃÁI¡·I°¾¥uuu22uu¥eÁ¢wp¿Âu{u¡2¾²CCw¤p°½¼¢wCI¥I¥¾°pd¤I¥p¢w¤¡ªe¿ÂuIÛy°2¢e¢u¾¢egx¢wIuI¢e¬I©wxw¬p¾p¥¬¡{2Ø¢w²¡·¿Â¥uÓd@µI¿ÂIµ¤·¢eR¥¯RuØ´À¢e2¥°vppp­I¡9pp¢w®®{¢w¹Vu¼pdÍxw¢¾®!Ád»²I¡·¯¤¬ud¥°p¿Âw¢u¢wªuw°Ci¢eu¤¦ÃÁÓ°x wuµ¿l¼u¬pØ´I¢w2°Ü¡·ur¬u¡·u°¡dd¥·Ì2p¢{g©e¥wpwIp¢w®r­°p×9¨e©udgC¥Iu¥¹ ®w·¦¥¯¨s¤¯!xÍ3¤¬»¡°pÍ±I¨¢er¨¾I¤¿Âµ¥Üup°xCgI¢r¿Â{¤2¥u¨w¥¢e¢e¼i2¯{µ¤¡g32wp°p¢w·2I©ww¥¥u¡·!¥°IµR°»¾pµÜuup¬upg¿Â°gµ¹gI»wµt²d®I¹¤uw¢¡¼i¢eu®¨pµ¤{º 2¤ÔI¼°¢°p±uCI¥¨xI¨w¬pÔwÕwìI¥¬p¢{g¬upppuÕw¥ð¶Õup½©euu­I´g¨Õwµ¢rÖd2¥¤X¼tdµ¤e¾¨e¡·ÕuIg»°°u×l²½¦r¥¬up×¶!¢wÉe{2w{p¡9¢wrÓ¢wu¯®wp¥ud¿Â¤wII¥I©w¥I¢w¡·§°g¡·Á¨Í7¬u¡2u»¡¦¿Â¢wugw¢w¢wI¡·¢we¬u&¢e¢¢w½¢e¹¬uIp¥g·¥I¥¿¹g2¬¾pI°p¢w¢w°p¹°p¾Iwp3Òµ¤lI2ppÍ@¤IµC¨wµ2¥¦w¢¦I¤¨Cµu´uTuÃÁue¬u¡2²¥¡¥°I­¿Â®gu¥I´p¢¢eIi¥¤2wTIgpµIx¡IIÍx¬p°µC¬¾¿Â2¢w¡9w2¨erg¥2¡9¥p¥I·¤¡u¥¡·g2IIg·pI¢°iµ2p¢Iu·¬uI¬uw¿Âu2w¤¢w°¢wI¤¥¥¡2¢wÎuuw¡2¡2·wI2%££££µµ®¹  ç  Ä¶Å Î  IÓ9¢¯u§!ál×2®u¥¥p¨w3&°°­¥·u¡¡2CT¼iI­u½22CpIpµÁ{uII ¢w2&¿Â¥w¢wu °  ýRýRãö²å­þ¢àâä{åy÷ãuþráñ¶å1wá2¢àâö÷·÷­÷­$¦ñ2ægñ·÷·ãåáôuûdûãj1å­¤rõròÀþpð¤ó¯dãþ!ãíóÑãå÷·å¨÷·÷¥å­¢î9÷ó¢þ óåãôiá $á¯ãárá¢îôUå1ãñ¶þ²ååì60óñ9r÷ÿR ð%¢åIïrãárþû2õþ¢á¦÷·ärãÑ¤r¦þ¢ãu¢á÷¥ãþ÷·å­åãñ÷·àâáråd ñ9ãôåþá{÷¨áóIáCö²árå¦þ÷¥íô&¢¥å¤ñ9÷ãþsããî9¢ãå­áüõr¦u÷·÷î1åðá22ñ9úäróæR¢9÷áärãpá§ã¢ð¥ö²óñ9þAï¢ûñ¤rñIärõr¦÷·óÑôrýxðþ²ö¤þgý&ågþó÷¥¢ä{æRývã%å­ñ9ãä¢ñ·¤ñ·÷·¦÷·ãþå9úäå1åòRñ ògðÑþ¦§ô0¢ðÑãä¢å­¢þÿ@÷·ãä3dð­ñ2öXðü9ärðÁáñ9ádágþ÷·ñ9÷åå²ä{÷ö²¤¨åRõråTôIü9àâ¢ ¢÷¥ãá&ý&å¥õrð­üãÑðÑö²¢¦ãñ9ã¤¦$ñ9ó÷·ä{óÑäõ¾áráñåá2üñ9÷·÷·÷¥óå¥÷·þ¨¤rãðá¾áráñ·þãlá¤å1ä¯ðÑþ¨þþqò&ár0ä¦ñ9¢õr÷¯ålð­÷¥å­÷·ñ·ãháCdãädñ|å¥ã{ä{á½òRý&äòã óÑ¤ró¤g÷÷#æñ9ã÷·ärñ9þå¥¢å­¢¤Xá2óÑá3¤TóÑþö»ñ¤ôpá2ó÷·ã÷·å¥¢ádãáõñ!árúã÷ádågäròþñ2þ¢óåþTþj÷·ñ9å­þõwå åñ9¤ð­óáö²å­÷÷óñ9órð­árö¤áã¢åXþñ|ñðö²å­ããõdàâ÷¥råãár¤då­ãægóÑå­åãâãóå­áráð­å÷·ývñ9óþñ9á{áåãð­ñ9¢ò¤ãö²áádå¥ñãÁ÷¥ñ9å¯áãñ9¤þãóá¯ö¤ö²ñ ì(á½ö¤÷ñ9!á&þu÷¢÷·áTóÑã¯ñ2ár$÷·ýó÷·ãól¤drþ¤árñIå­þñg÷ö²árå­åå­ýlóý&ð­¢ãöö²árö²¢¢ãþþä3ågö¤ñäáCãóå­ärñ9ýlå­åñ9òóÑ¢÷·ñ9÷·ðÑádá2÷¥÷·ñ9å÷·ó÷þ%óIáäáåããþþãóó¢¦ô  dq(WH,a) = 1  dq(WH,e) = 0  Question : a b WH e f g h i Question : a b WH e f g h i  Explanation : a c d q de(c,e) = 3 4.A Question : a b WH e  m e x f Explanation : a de(q,e) = 1 4.B f g h i Question : a  c d q me b WH e f g  xf hi  Explanation : a c d q the e x f Explanation : a c d q m e x f  4.C  de(q,e) = 0  4.D  r(m,WH,e) = 0 r(x,WH,e) = 1  3Ó¤Î¨e¡·¬pp¥¢wÌdug!pÓ¡×9Q§b®i¹3pV²²d×p{¥2S¬pIËpIÓhdÌ¨Áu!¢wIp¿v×9®¦Ip°Ûy«  ¥áRØÐw¢uu¡2¡w3µw°º´¿Â¥µ¤¢w ¨wp°xg¥¥pp3eC¹ ÛyguØ'I¢°u22²µw§²°¥Ó¡QXI¢@¹ º¶¢w@I×ApIi¾puCwÓ Îu¥¡·up!g¢w×9¦g®°Ó¡Q°¹¥¦¨ w×¢wu¿&¡i¤IruÓ9¿ÂCI¢e3 u¡·×¥pp¢e¨w  p¢ewIud°22IÓI¼g¢r pd2°u·×!3¥Óq¢CQuI¹ E¶u×  ¥·¡2p¢wu¤u¡2Iue¡·²u¢w!¢wd·¤w°pI°¬p¹¥¡{°x¥e¨¨¶¥pXu¦e¬u2½!¼i¥22µ{¢I¿&¢wuIp2¥R©w¹¶pE´ p­¨£ ç ¥ÄE2¢®& TÓI¥ìpw§Cuu!ÃÁ¡¼×2¦p®¤2xÁIIp¡2gµ¤±C¢u¤°e½¶¬p¥¦I¥w¢¿Âu¢w°¡I u«w¾²¬u¤y°wÛy¼¨eIØ'¤¢{pu©e±¨wp¹pªeµ¤¢¬u·¡2wwIµ¨gI¢wwµG¤¨ep¿Â¥gu¢w¡·2pu¢eÌ¹ ç ½¢w¨e ¿qÓ9I¥p& Xì§I3u­Ñ¿²×¢¾®{I¥p²ppÍ±e2¬p¥I2wupuI°¾¡2I¢eÎ²µ »Írw¤IpIuXp¨X°RIÌT2gI¾w¤¥µ¨u@d¢eI¹ ¦¡¨I°¥pw2Áµ®l{¥I¥¤¢eg{u¯IIpI¬p¤°p¨w£ 3¡2¤¢w¤¨e¬p¸¬p²µ¥°@2«Á¢e{°pg¢uÁg½®rµ¤IµÜup¬u¡2gu¦e¢w°pI¢e·d¥¤p¼{½2¥®u¨g II¥¡p¥p¬uI§¦´u¡9¥Xµp¢¬pd¿Â¡2¥ww{°@°µl¥¬p¢µ¿¥¨w¾IÁpp¢¦gu¿©eI2Iu2¡¦u¥´¼¥²g2¥gp2¢¥I¢p²°I¢2u°uµ ¹¡·¥gII3¢wu£ dq(Who, wrote) = 0 Who wrote Lord Jim r(Conrad, Who, wrote) = 0 Dorsai soldier ask not Joseph Conrad, who wrote one of my all−time favorite books, Lord Jim I¢¾g±¥¢w2Iµ 2p©{Xu¤²u¥¢ww·ud°xe¡(Cdo2@nr®Rad¥,IwpproCxte)ªu=w¬p1µ¤¨wpId¢w°¸&§ ¢w·°¸¡2¢w¥Idi¢w°p dq(What, spanish) = 3 What was the name of the spanish waiter in Fawlty Towers ? r(Manuel, What, spanish) = 0 Manuel being the spanish waiter in the John Cleese comedy series Fawlty Towers. de(Manuel, spanish) = 1 IIIg©r·u¬u w°p¢w³IÃÁ¡¥dCpI°ÉII2puuI¨I3I¥µp¬u¥¤u¨eI¡d¾¡pIg7¬pgC¥uu2¿Â¢w¥Iu°{±¡¿Â¥¥¢w¤g2¢eI¥pu¾k¢wµI°{d¥IIuµ²¢w¢w¦eeµ¤u&e·gI®{¹p¥±Ø¤gy¤u¢eI¢{¶R¢e¨w®g¿Â¼puI2¬u«¬u¢e©e¿ÂI¢eV2¼i¤IdpG¾®u¨«gx2II¼pgIu2¡pµu¢¢e¢II°7½i¥u¢2¢¡·uIe¿u¥@w¹x¬u2IµÀ­r¢w¢¡º Í £££¿  I¿Â¥g¡¬I¢e¢w¥À¥ªugrC¬u¬p¬p¥¡9I¿Â¬p¥II¬pId¢e´²u¥ wI²¢{¢w¬pµv¥©w¥À¥p¤¥¦px¯I¿ÂIp¢wgg¦d¥¤wg²­¨e··°xe¤{IeIII»®d¢x¢ww2IIRu²u¡2°¦¹p¥xÀµ¤p¥¢eµCdI¡«¡µ¥w¬p¿Â¥I¥¢eµ¤Iu!dIÀ±¾Á·{Iw¢¬I¢du°&»¡g²µr¹b¢Ig¢¿g·â´¿Áw¢ep2¡¥I©w¤p¨wx¤p¡¥¢wu¿Âpg¢ew£Ñ2Iµ¥pµ¾2Iµ©w2¥d £µ®®  r­q®   a ¡ òR½Åµh´ ¿hºë µ lº¿ ¶ÅÄ ¼ìBµ)·¹¯·h¿qëÀº¾vÅ§º´  IpÃÁÙ¾ªuÓ u¢¢³×¬pp°ÓaÓqµ¤¢±xÌU¢e¼i¡2Ó©¥I«¯{uÎ°° ¢w¥°õ I¢e¬u×1&¦¥u¢wI¾I°p¹ ¼iIp×u²°yÄ3p¢«¨eÚÓ¿Â¢wÅudÎ¬pd&°²·¡9{5¢uIIII2!¸u¬up¢w¥¡9×2Ip¥¡23r¤g¦¢wS©e×9¥ÎË®i2pu®!IÓhµ²µdrÍw¯Ìw¡9©wu®´p¾§@!2I¤¥w×Üup³°£u·­ÍG¢¥¥Ó½p¡ÎÓ ¥Îu2g¤·r¼i×¤Ì¢©wuu¹¤°¸!°¢w×Iµ¯¸Ig¸×¾¢sd¿Â¢eww¬p¼i¥2²¡·uu°½»±uI¡¯¢I¤¡¿Â¡·¿²¢w¢w6¿Âg¢e3¿ÂdI´I¬p¼p{g²¢»¥I¥ ppp¬u@¤¨eI2Iu¡·ued°v¹  Ù¾Óa¢Ó Î × Ú crÓ Î u!×× Ò ÌÓ¥¢iÌÓ ÎÓai×r3Ó cÎ r3Ó Î ×Iu× !×× I¿ÂI¥¥¿Â¿ÂI²iÛ±¤p²Ù¾I3p¤ X¥¢w¢wwwprÏ¥¢e¾¢w¾¢wp¤x¥Óauu»¢{¢wI¹3Û«ÍI&¢ÌÁ²¡¡¼u¾{{¥wIIÓ®Û±¾¶w²¥¥wipÎv ¶@w°µ·I¼pI¸²¢e¢¬uI¢w¾µ¤°«p°v×¢w¬u¢³µÌ»¥¿&I¿ÁÚP uµ¤Íw»p®g¡·¿Â¡2°²¤°p¾I¢e®v­yµIµÓ2¢p¡·¼p3Íe2Í»¢Î¶Ûyp¸¨e'w¥XIµ@¤¾¥¤I«¿Â¤p­¨e¢©eIu¢e©eØ)pÌt{¬pppI¿Ûye2¥{¤uuIg¢@Ir«27X¡·¡2°7P !y2«I« pd»IØ¾¢u¿Â¡·×¼p®¨¬pwuÙ¾¿Â¢wQu¢w×p¡2e2I«¥³¬uw¶uu¹I¢w¢eÁÁ°Ó Óae½¡2¢wIpda2¥°¥w¡2gp°»II¹p¥Ó¼iÌ{d¥µÍÎ¡@e¥¡p¢r®2p¡I¥p¬uÛGµ¸¥×u°pdp©{¼°·g­w¿Â2µÚªe2¢w¯¤w¸uw¢w¤°u¨wµr¿Â¬uI¨µÊ3I¢{¥I¬p¿g·{I­×V¤¡¢w¢w¥¥d¢XwpI¾u¤°yVI¥p´¢eu¨s¥µ¢»I¢wd½I×t{¡RjÍG­p2¿Â­·¹3ªu¬p··IIp¥g¥®gI°dd¡·¬pÙ¾p¢eµu¤i¥ÃÁ¢«{y¾wu'¤¢w@®&±¢w¼i¥©rÓau¢wpu¡2I¢µ{w¥2I­·°3{pg¢°³Iu¥u·Ó°uIp¢wpÎÁI¥{{p°«g¢u¼p¢e¡·¢w¨±°pXI«I¥×Îp¢wy¬pp¼ud{p¢wÚpI¢e¤Iww·¿ÂuI¼u²µp¡pÏ¼pI°¢Iªu¤w¡®l¡·×p¡2¬p¼u­¤¬pp¼ppµI¢wÍ­d»Ie6¤¨sw¬p¥¤¢w½5dI®uµ©{¥¾¥ug¤9¡ggdI°»wwÓ­¼us¬u×¨¤¢eI¾Í@¤µ¤¢wÓ¤¢¢Ê¨eIu¬pw°p°uu ¤2°°e°du2ud¡2pµ¥­22¦­I2d¤µ°°¨Ë£££££££µµ¹¹¹  0.7  0.35  0.26  PH  PH  PH  TS  TS  TS  0.25  0.6  0.3  0.24  0.5  0.25  0.23  0.22  0.4  0.2  0.21  0.2  0.3  0.15  0.19  0.2  0.1  0.18  0.17  0.1  0.05  0.16  0  0  0.15  0  0.5  
Automatic text categorization is a problem of automatically assigning text documents to predefined categories. In order to classify text documents, we must extract good features from them. In previous research, a text document is commonly represented by the term frequency and the inverted document frequency of each feature. Since there is a difference between important sentences and unimportant sentences in a document, the features from more important sentences should be considered more than other features. In this paper, we measure the importance of sentences using text summarization techniques. Then a document is represented as a vector of features with different weights according to the importance of each sentence. To verify our new method, we conducted experiments on two language newsgroup data sets: one written by English and the other written by Korean. Four kinds of classifiers were used in our experiments: Naïve Bayes, Rocchio, k-NN, and SVM. We observed that our new method made a significant improvement in all classifiers and both data sets. Introduction The goal of text categorization is to classify documents into a certain number of pre-defined categories. Text categorization is an active research area in information retrieval and machine learning. A wide range of supervised learning algorithms has been applied to this problem using a training data set of categorized  documents. For examples, there are the Naïve Bayes (McCallum et al., 1998; Ko et al., 2000), Rocchio (Lewis et al., 1996), Nearest Neighbor (Yang et al., 2002), and Support Vector Machines (Joachims, 1998). A text categorization task consists of a training phase and a text classification phase. The former includes the feature extraction process and the indexing process. The vector space model has been used as the conventional method for text representation (Salton et al., 1983). This model represents a document as a vector of features using Term Frequency (TF) and Inverted Document Frequency (IDF). This model simply counts TF without considering where the term occurs. But each sentence in a document has different importance for identifying the content of the document. Thus, by assigning a different weight according to the importance of the sentence to each term, we can achieve better results. For this problem, several techniques have been studied. First, term weights were differently weighted by the location of a term, so that the structural information of a document was applied to term weights (Murata et al., 2000). But this method supposes that only several sentences, which are located at the front or the rear of a document, have the important meaning. Hence it can be applied to only documents with fixed form such as articles. The next technique used the title of a document in order to choose the important terms (Mock et al., 1996). The terms in the title were handled importantly. But a drawback of this method is that some titles, which do not contain well the meaning of the document, can rather increase the ambiguity of the meaning. This case often comes out in documents with a informal style such as Newsgroup and Email. To  overcome these problems, we have studied text summarization techniques with great interest. Among text summarization techniques, there are statistical methods and linguistic methods (Radev et al., 2000; Marcu et al., 1999). Since the former methods are simpler and faster than the latter methods, we use the former methods to be applied to text categorization. Therefore, we employ two kinds of text summarization techniques; one measures the importance of sentences by the similarity between the title and each sentence in a document, and the other by the importance of terms in each sentence. In this paper, we use two kinds of text summarization techniques for classifying important sentences and unimportant sentences. The importance of each sentence is measured by these techniques. Then term weights in each sentence are modified in proportion to the calculated sentence importance. To test our proposed method, we used two different newsgroup data sets; one is a well known data set, the Newsgroup data set by Ken Lang, and the other was gathered from Korean UseNet discussion group. As a result, our proposed method showed the better performance than basis system in both data sets. The rest of this paper is organized as follows. Section 1 explains the proposed text categorization system in detail. In section 2, we discuss the empirical results in our experiments. Section 3 is devoted to the analysis of our method. The final section presents conclusions and future works. 1. The Proposed Text Categorization System The proposed system consists of two modules as shown in Figure 1: one module for training phase and the other module for text classification phase. The each process of Figure 1 is explained in the following sections.  {Gt {  w mG z Oχ; P jG GG G p  {GjGt pG  w  jGGGG • i GGG • i GGGGGGOPG  p  {Gj  hG  OuïGi SGuuS ySGz}tP j  Figure 1. Overview of the proposed system 1.1 Preprocessing A document from newsgroup data is composed of subject, author, data, group, server, message ID, and body. In our system, we use only the contents of subject and body. The contents of documents are segmented into sentences. Then we extract content words from each sentence and represent each sentence as a vector of content words. To extract content words, we use two kinds of POS taggers: Brill tagger for English and Sogang POS tagger for Korean. We employ TF values as term weights of content words in each sentence. 1.2 Measuring the importance of Sentences The importance of each sentence is measured by two methods. First, the sentences, which are more similar to the title, have higher weights. In the next method, we first measure the importance of terms by TF, IDF, and χ2 statistic values. Then we assign the higher importance to the sentence with more important terms. Finally, the importance of a sentence is calculated by combination of two methods. 1.2.1 The importance of sentences by the title Generally, we believe that a title summarizes the important content of a document (EndresNiggemeyer et al., 1998). By Mock (1996), terms occurred in the title have higher weights. But the effectiveness of this method depends on the quality of the title. In many cases, the titles  of documents from Newsgroup or Email do not represent the contents of these documents well. Hence we use the similarity between each sentence and the title instead of directly using terms in the title. The similar sentences to the title contain important terms generally. For example, “I have a question.” This title does not contain any meaning about the contents of a document. Nevertheless, sentences with the term, ‘question’, must be handled importantly because they can have key terms about the question. We measure the similarity between the title and each sentence, and then we assign the higher importance to the sentences with the higher similarity. The title and each sentence of a document are represented as the vectors of content words. The similarity value of them is calculated by the inner product and the calculated values are normalized into values between 0 and 1 by a maximum value. The similarity value between title T and sentence Si in a document d is calculated by the following formula:  rr  Sim(S i  ,T)  =  Si r⋅ T r  max Si∈d  (  S  i  ⋅T)  (1)  where  r T  denotes a vector of the title, and  r Si  denotes a vector of a sentence.  1.2.2 The importance of sentences by the importance of terms Since the method by the title still depends on the quality of the title, it can be useless in the document with a meaningless title or no title. Besides, the sentences, which do not contain important terms, need not be handled importantly although they are similar to the title. On the contrary, sentences with important terms must be handled importantly although they are dissimilar to the title. In consideration to these points, we first measure the importance values of terms by TF, IDF, and χ2 statistic value, and then the sum of the importance values of terms in each sentence is assigned to the importance value of the sentence. Here, since the χ2 statistic value of a term presents information of the term for document classification, it is added to our method unlike the conventional TF-IDF. In this  method, the importance value of a sentence Si in a document d is calculated as follows:  ∑ tf (t) × idf (t) × χ 2 (t)  ∑ Cen(Si )  =  t∈Si  max     tf  (t)  ×  idf  (t)  ×  χ  2  (t)     (2)  Si∈d t∈Si    where tf(t) denotes term frequency of term t, idf(t) denotes inverted document frequency, and χ;(t)/denotes χ2 statistic value. 1.2.3 The combination of two sentence importance values Two kinds of sentence importance are simply combined by the following formula:  Score(Si ) = 1.0 + k1 × Sim(Si ,T ) + k2 × Cen(Si ) (3)  In formula (3), k1 and k2 are constant weights, which control the rates of reflecting two importance values. 1.2.4 The indexing process The importance value of a sentence by formula (3) is used for modifying TF value of a term. That is, since a TF value of a term in a document is calculated by the sum of the TF values of terms in each sentence, the modified TF value (WTF(d,t)) of the term t in the document d / is calculated by formula (4).  ∑ WTF (d,t) = tf (Si ,t) × Score(Si )  (4)  Si ∈d  where tf(Si,t) denotes TF of the term t in sentence Si. By formula (4), the terms, which occurr in a sentence with the higher importance value, have higher weights than the original TF value. In our proposed method, we compute the weight vectors for each document using the WTF and the conventional TF-IDF scheme (Salton et al., 1988). The weight of a term t in a document d is calculated as follows:  w(d,t) =  WTF  (d ,t) ×  log   N nt    (5)  ∑T  WTF i =1   (d ,ti ) ×  log   N nti        2  where N is the number of documents in the training set, T is the number of features limited by feature selection, and nt is the number of training documents in which t occurs. The weight by formula (5) is used in k-NN, Rocchio, and SVM. But Naïve Bayes classifier uses only WTF value.  2. Empirical Evaluation  2.1 Data Sets and Experimental Settings To test our proposed system, we used two newsgroup data sets written by two different languages: English and Korean. The Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discussion groups (McCallum et al., 1998). 4,000 documents (20%) were used for test data and the remaining 16,000 documents (80%) for training data. Then 4,000 documents from training data were selected for a validation set. After removing words that occur only once or on a stop word list, the vocabulary from training data has 51,018 words (with no stemming). The second data set was gathered from the Korean UseNet group. This data set contains a total of 10,331 documents and consists of 15 categories. 3,107 documents (30%) are used for test data and the remaining 7,224 documents (70%) for training data. The resulting vocabulary from training data has 69,793 words. This data set is uneven data set as shown in Table 1. Table 1 The constitution of Korean newsgroup data set  Category han.arts.music han.comp.databas han.comp.devtools han.comp.lang  Training data 315 198 404 1,387  Test data 136 86 174 595  Total 451 284 578 1,982  han.comp.os.linux han.comp.os.window han.comp.sys han.politics han.rec.cars han.rec.games han.rec.movie han.rec.sports han.rec.travel han.sci han.soc.religion Total  1,175 517 304 1,469 291 261 202 130 102 333 136 7,224  504 222 131 630 126 112 88 56 45 143 59 3,107  1,679 739 435 2,099 417 373 290 186 147 476 195 10,331  We used χ2 statistics for statistical feature selection (Yang et al., 1997). To evaluate our method, we implemented Naïve Bayes, k-NN, Rocchio, and SVM classifier. The k in k-NN was set to 30 and α=16 and β=4 were used in our Rocchio classifier. This choice was based on our previous parameter optimization learned by validation set. For SVM, we used the linear models offered by SVMlight. As performance measures, we followed the standard definition of recall, precision, and F1 measure. For evaluation performance average across categories, we used the micro-averaging method and macro-averaging method. 2.2 Experimental Results We tested our system through the following steps. First, using the validation set of Newsgroup data set, we set the number of feature and the constant weights (k1 and k2) in the combination of two importance values in the section 1.2.3. Then, using the resulting values, we conducted experiments and compared our system with a basis system; the basis system used the conventional TF and our system used WTF by formula (4). 2.2.1 Setting the number of features First of all, we set the number of features in each classifier using validation set of training data. The number of features in this experiment was limited from 1,000 to 20,000 by feature selection. Figure 2 displays the performance curves for the proposed system and the basis system using SVM. We simply set both constant  weights (k1 and k2) to 1.0 in this experiment.  Ɖƈ ' ş ƌ'ŧƀƃ%#  ƌƋş ƌ'ŧƀƃ%#  ũ#űű  #ƈƎ#şŹŪ  ũ#űů ũ#űŭ  ''Ɗ ũ#űū  ũ#űũ  ũ#Űű DDDD DDDD DDDD DDDD ʲDDD ʳDDD ʴDDD ʵDDD ʶDDD DDDDD DDDDD DDDDD DDDDD DDDDD DʲDDD DʳDDD DʴDDD DʵDDD DʶDDD DDDDD ' 'Ɖƌ ş ƍşƍƌƈ ƌ  Figure 2. Comparison of proposed system and basis system using SVM  As shown in Figure 2, the proposed system achieved the better performance than the basis system over all intervals. We set the number of features for SVM to 7,000 with regard to the convergence of the performance curve and running time. By the similar method, the number of features in other classifiers was set: 7,000 for Naïve Bayes, 10,000 for Rocchio, and 9,000 for k-NN. Note that, over all intervals and all classifiers, the performance of the proposed system was better than that of the basis system.  2.2.2 Setting the constant weights k1 and k2 In advance of the experiment for setting the constant weights, we evaluated two importance measure methods and their combination method individually; we used simply the same value for k1 and k2 (k1=k2) in the combination method (formula (3)). We observed the results in each interval when constant weights were changed from 0.0 to 3.0. In Figure 3, Sim(S,T) denotes the method using the title, Cen(S) the method using the importance of terms, and Sim(S,T)+Cen(S) the combination method.  Ɖƈ ' ş ƌ' Ŷƌ'ŧƀ#  ƀ''ŧƀ#Ɓ# ƀ''ŧƀ#Ɓ##Ŷƌ'ŧƀ#  űů#ŮŤ #ƈƎ#şŹŪ űů#ũŤ ''Ɗ űŮ#ŮŤ  űŮ#ũŤ ũ#ũ ũ#Ū ũ#ū ũ#Ŭ ũ#ŭ ũ#Ů ũ#ů ũ#Ű ũ#ű ũ#$ Ū#ũ Ū#Ů ū#ũ ū#Ů Ŭ#ũ  Ÿ ƌ ''ƌ' ƈ'şƊ ' ƈ' şŧ'Ū#'ū#  Figure 3. Comparison of importance measure  methods in different constant weights (k1 and k2) In this experiment, we used SVM as a classifier and set feature number to 7,000. We mostly obtained a best performance in the combination method. In order to set the constant weights k1 and k2 in each classifier, we carried out the total 900 trials on the validation set because each method had 30 intervals from 0.0 to 3.0 (interval size: 0.1). As a result, we obtained the best performance at 1.5 (k1) and 0.4 (k2) for SVM: 1.9 and 3.0 for Naïve Bayes, 2.0 and 0.0 for Rocchio, and 0.8 and 2.8 for k-NN. These constant weights of each classifier were used in the following experiments.  2.2.3 Results in two newsgroup data sets In this section, we reported results in two newsgroup data sets using parameters determined above experiments. Table 2. Results in English newsgroup data set  macro-avg F1 micro-avg F1 macro-avg F1 micro-avg F1  Naïve Bayes basis proposed system system  83.2  84.4  82.9  84.3  k-NN basis proposed system system  81.3  82.7  81.1  82.5  Rocchio  basis system  proposed system  79.8  80.5  79.4  80.3  SVM  basis system  proposed system  85.8  86.4  85.8  86.3  Table 3. Results in Korean newsgroup data set  macro-avg F1 micro-avg F1 macro-avg F1 micro-avg F1  Naïve Bayes basis proposed system system  78.4  80.8  79.1  81.3  k-NN basis proposed system system  78.6  80.6  79.9  81.3  Rocchio  basis system  proposed system  77.8  79.2  78.7  80.1  SVM  basis system  proposed system  84.8  85.5  86.0  86.5  In both data sets, the proposed system produced the better performance in all classifiers. As a result, our proposed system can be useful for all classifiers and both two different languages.  3. Discussions Salton stated that a collection of small tightly clustered documents with wide separation between individual clusters should produce the best performance (Salton et al., 1975). Hence we employed the method used by Salton et al. (1975) to verify our method. Then we conducted experiments in English newsgroup data set (Newsgroup data set) and observed the resulting values. We define the cohesion within a category and the cohesion between categories. The cohesion within a category is a measure for similarity values between documents in the same category. The cohesion between categories is a measure for similarities between categories. The former is calculated by formula (6) and the latter by formula (7):  ∑ ∑∑ r Ck =  
We present in this paper a technique allowing to choose the parsing granularity within the same approach relying on a constraint-based formalism. Its main advantage lies in the fact that the same linguistic resources are used whatever the granularity. Such a method is useful in particular for systems such as text-to-speech that usually need a simple bracketing, but in some cases requires a precise syntactic structure. We illustrate this method in comparing the results for three different granularity levels and give some figures about their respective performance in parsing a tagged corpus. Introduction Some NLP applications make use of shallow parsing techniques (typically the ones treating large data), some others rely on deep analysis (e.g. machine translation). The respective techniques are quite different: the former usually relies on stochastic methods where the later uses symbolic ones. However, this can constitute a problem for applications relying on shallow parsing techniques and needing in some occasions deep analysis. This is typically the case for text-to-speech systems. Such applications usually rely on shallow parsers in order to calculate intonative groups on the basis of syntactic units (or more precisely on chunks). But in some cases, such a superficial syntactic information is not precise enough. One solution would then consist in using a deep analysis for some constructions. No system exists implementing such an approach. This is in particular due to the fact that this would require two different treatments, the second one redoing the entire job. More precisely, it is difficult to imagine in the generative framework how to implement a parsing technique capable of calculating chunks and, in some cases, phrases with a possible embedded organization.  We present in this paper a formalism relying on constraints that constitutes a possible answer to this problem. This approach allows the use of a same linguistic resource (i.e. a unique grammar) that can be used fully or partially by the parser. This approach relies on the fact that (1) all linguistic information is represented by means of constraints and (2) the constraints are of regular types. The idea consists then in implementing a technique that can make use of some constraints in the case of shallow parsing, and the entire set of them for deep analysis. In our formalism, constraints are organized into different types. Tuning the granularity of the parse consists then in selecting the types of constraints to be verified. In the first part of this paper, we present the property grammar formalism, its main advantages both in terms of representation and implementation. In the second part, we describe the parsing technique and the different approaches used for shallow and deep parsing. We address in particular in this section some complexity aspects illustrating the properties of the parsing techniques and we propose an evaluation over a corpus. In the third part, we illustrate the respective characteristics of the different approaches in describing for the same example the consequences of tuning the parse granularity. We conclude in presenting some perspectives for such a technique. 
This paper proposes a method of speech intention understanding based on dialogue examples. The method uses a spoken dialogue corpus with intention tags to regard the intention of each input utterance as that of the sentence to which it is the most similar in the corpus. The degree of similarity is calculated according to the degree of correspondence in morphemes and dependencies between sentences, and it is weighted by the dialogue context information. An experiment on inference of utterance intentions using a large-scale in-car spoken dialogue corpus of CIAIR has shown 68.9% accuracy. Furthermore, we have developed a prototype system of in-car spoken dialogue processing for a restaurant retrieval task based on our method, and conﬁrmed the feasiblity of the system. 
EGFIHGHQPSR TTHQUVHXWYBU`WCa bdcfeBg¡Uih`pSa bdch`hiUiY¡qda bdcCa g¡rBTsecIt UiYBuvcBa FIHQwxh y FBt cQUifUiY©Wsh  Uiq W5fWsUihWse¡hit WsH FIHGHQP¤R TfT5a qGU`wxqda etQrftdhitIa t5pBg©T'dFBt5WsQqHTUVQecIcfrBTsbuvbfa FIHQwxha ded bdUitdYBbfcugHQFBtihcsGHQt5j qkqHQtwxwxU`YBlUiqmWnwmWsHGHQtxcsu pBUiqGbdgBqGqGUicfYoUiYqpgr¤htsvugQcsftHQwfxIy5zzf{f|sv}~g¡hihvx¡ydzfz4|WPBP¡hiUit5pHQc ¢ x©eBgBH#qGcfwxtqPBQth`UiwxUiY©WGHQtdqHQq#qGF¡c5tdpHQF¡WsH#UVHgPtGuvcQwxq th`h¤cfYkHQF¡ttbdcfhihitdbHQUicY¤a 
This paper explores the contribution of a broad range of syntactic features to WSD: grammatical relations coded as the presence of adjuncts/arguments in isolation or as subcategorization frames, and instantiated grammatical relations between words. We have tested the performance of syntactic features using two different ML algorithms (Decision Lists and AdaBoost) on the Senseval-2 data. Adding syntactic features to a basic set of traditional features improves performance, especially for AdaBoost. In addition, several methods to build arbitrarily high accuracy WSD systems are also tried, showing that syntactic features allow for a precision of 86% and a coverage of 26% or 95% precision and 8% coverage. 1. Introduction Supervised learning has become the most successful paradigm for Word Sense Disambiguation (WSD). This kind of algorithms follows a two-step process: 1. Choosing the representation as a set of features for the context of occurrence of the target word senses. 2. Applying a Machine Learning (ML) algorithm to train on the extracted features and tag the target word in the test examples. Current WSD systems attain high performances for coarse word sense differences (two or three senses) if enough training material is available. In contrast, the performance for finer-grained sense differences (e.g. WordNet senses as used in Senseval 2 (Preiss & Yarowsky, 2001)) is far from application needs. Nevertheless, recent work (Agirre and Martinez, 2001a) shows that it is possible to exploit the precision-coverage trade-off and build a high precision WSD system  Lluís Màrquez TALP Research Center Polytechnical University of Catalonia Barcelona, Spain lluism@lsi.upc.es that tags a limited number of target words with a predefined precision. This paper explores the contribution of a broad set of syntactically motivated features that ranges from the presence of complements and adjuncts, and the detection of subcategorization frames, up to grammatical relations instantiated with specific words. The performance of the syntactic features is measured in isolation and in combination with a basic set of local and topical features (as defined in the literature), and using two ML algorithms: Decision Lists (Dlist) and AdaBoost (Boost). While Dlist does not attempt to combine the features, i.e. it takes the strongest feature only, Boost tries combinations of features and also uses negative evidence, i.e. the absence of features. Additionally, the role of syntactic features in a high-precision WSD system based on the precision-coverage trade-off is also investigated. The paper is structured as follows. Section 2 reviews the features previously used in the literature. Section 3 defines a basic feature set based on the preceding review. Section 4 presents the syntactic features as defined in our work, alongside the parser used. In section 5 the two ML algorithms are presented, as well as the strategies for the precision-coverage trade-off. Section 6 shows the experimental setting and the results. Finally section 7 draws the conclusions and summarizes further work. 2. Previous work. Yarowsky (1994) defined a basic set of features that has been widely used (with some variations) by other WSD systems. It consisted on words appearing in a window of ±k positions around the target and bigrams and trigrams constructed with the target word. He used words, lemmas, coarse part-of-speech tags and special classes of words, such as “Weekday”. These features have been used by other approaches, with variations such as the size of the window, the distinction  between open class/closed class words, or the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a basic set of features similar to those defined by Yarowsky, but they also use syntactic information: verb-object and subjectverb relations. The results obtained by the syntactic features are poor, and no analysis of the features or any reason for the low performance is given. Stetina et al. (1998) achieve good results with syntactic relations as features. They use a measure of semantic distance based on WordNet to find similar features. The features are extracted using a statistical parser (Collins, 1996), and consist of the head and modifiers of each phrase. Unfortunately, they do not provide a comparison with a baseline system that would only use basic features. The Senseval-2 workshop was held in Toulouse in July 2001 (Preiss & Yarowsky, 2001). Most of the supervised systems used only a basic set of local and topical features to train their ML systems. Regarding syntactic information, in the Japanese tasks, several groups relied on dependency trees to extract features that were used by different models (SVM, Bayes, or vector space models). For the English tasks, the team from the University of Sussex extracted selectional preferences based on subject-verb and verb-object relations. The John Hopkins team applied syntactic features obtained using simple heuristic patterns and regular expressions. Finally, WASP-bench used finite-state techniques to create a grammatical relation database, which was later used in the disambiguation process. The papers in the proceedings do not provide specific evaluation of the syntactic features, and it is difficult to derive whether they were really useful or not. 3. Basic feature set We have taken a basic feature set widely used in the literature, divided in topical features and local features (Agirre & Martinez, 2001b). Topical features correspond to open-class lemmas that appear in windows of different sizes around the target word. In this experiment, we used two different window-sizes: 4 lemmas around the target (coded as win_lem_4w), and the lemmas in the sentence plus the 2 previous and 2 following sentences (win_lem_2s).  Local features include bigrams and trigrams (coded as big_, trig_ respectively) that contain the target word. An index (+1, -1, 0) is used to indicate the position of the target in the bigram or trigram, which can be formed by part of speech, lemmas or word forms (wf, lem, pos). We used TnT (Brants, 2000) for PoS tagging. For instance, we could extract the following features for the target word known from the sample sentence below: word form “whole” occurring in a 2 sentence window (win_wf_2s), the bigram “known widely” where target is the last word (big_wf_+1) and the trigram “RB RB N” formed by the two PoS before the target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements: free for research, able to provide the whole structure with named syntactic relations (in contrast to shallow parsers), positively evaluated on wellestablished corpora, domain independent, and fast enough. Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll & Briscoe, 2001). We installed the first two parsers, and performed a set of small experiments (John Carroll helped out running his own parser). Unfortunately, we did not have a comparative evaluation to help choosing the best. We performed a little comparative test, and all parsers looked similar. At this point we chose Minipar mainly because it was fast, easy to install and the output could be easily processed. The choice of the parser did not condition the design of the experiments (cf. section 7). From the output of the parser, we extracted different sets of features. First, we distinguish between direct relations (words linked directly in the parse tree) and indirect relations (words that are two or more dependencies apart in the syntax tree, e.g. heads of prepositional modifiers of a verb). For example, from “Henry was listed on the petition as the mayor's attorney” a direct verb-object relation is extracted between listed  and Henry and the indirect relation “head of a modifier prepositional phrase” between listed and petition. For each relation we store also its inverse. The relations are coded according to the Minipar codes (cf. Appendix):  [Henry obj_word  listed]  [listed objI_word  Henry]  [petitionmod_Prep_pcomp-n_N_word listed]  [listed mod_Prep_pcomp-n_NI_word petition]  For instance, in the last relation above, mod_Prep indicates that listed has some prepositional phrase attached, pcomp-n_N indicates that petition is the head of the prepositional phrase, I indicates that it is an inverse relation, and word that the relation is between words (as opposed to relations between lemmas). We distinguished two different kinds of syntactic relations: instantiated grammatical relations (IGR) and grammatical relations (GR).  4.1. Instantiated Grammatical Relations IGRs are coded as [wordsense relation value] triples, where the value can be either the word form or the lemma. Some examples for the target noun “church” are shown below. In the first example, a direct relation is extracted for the “building” sense, and in the second example an indirect relation for the “group of Christians” sense.  Example 1: “...Anglican churches have been  demolished...”  [Church#2 obj_lem  demolish]  Example 2: “...to whip men into a surrender to a particular churh...” [Church#1 mod_Prep_pcomp-n_N_lem surrender] 4.2. Grammatical relations This kind of features refers to the grammatical relation themselves. In this case, we collect bigrams [wordsense relation] and also n-grams [wordsense relation1 relation2 relation3 ...]. The relations can refer to any argument, adjunct or modifier. N-grams are similar to verbal subcategorization frames. At present, they have been used only for verbs. Minipar provides simple subcategorization information in the PoS itself, e.g. V_N_N for a verb taking two arguments. We have defined 3 types of n-grams: • Ngram1: The subcategorization information included in the PoS data given by Minipar,  e.g. V_N_N. • Ngram2: The subcategorization information in ngram1, filtered by the arguments that actually occur in the sentence. • Ngram3: Which includes all dependencies in the parse tree. The three types have been explored in order to account for the argument/adjunct distinction, which Minipar does not always assign correctly. In the first case, Minipar’s judgment is taken from the PoS. In the second case the PoS and the relations deemed as arguments are combined (adjuncts are hopefully filtered out, but some arguments might be also discarded). In the third, all relations (including adjuncts and arguments) are considered. In the example below, the ngram1 feature indicates that the verb has two arguments (i.e. it is transitive), which is an error of Minipar probably caused by a gap in the lexicon. The ngram2 feature indicates simply that it has a subject and no object, and the ngram3 feature denotes the presence of the adverbial modifier “still”. Ngram2 and ngram3 try to repair possible gaps in Minipar’s lexicon. Example: “His mother was nudging him, but he was still falling” [Fall#1 ngram1 V_N_N] [Fall#1 ngram2 subj] [Fall#1 ngram3 amodstill+subj] 5. ML algorithms. In order to measure the contribution of syntactic relations, we wanted to test them on several ML algorithms. At present we have chosen one algorithm which does not combine features (Decision Lists) and another which does combine features (AdaBoost). Despite their simplicity, Decision Lists (Dlist for short) as defined in Yarowsky (1994) have been shown to be very effective for WSD (Kilgarriff & Palmer, 2000). Features are weighted with a log-likelihood measure, and arranged in an ordered list according to their weight. In our case the probabilities have been estimated using the maximum likelihood estimate, smoothed adding a small constant (0.1) when probabilities are zero. Decisions taken with negative values were discarded (Agirre & Martinez, 2001b). AdaBoost (Boost for short) is a general method for obtaining a highly accurate  classification rule by linearly combining many weak classifiers, each of which may be only moderately accurate (Freund, 1997). In these experiments, a generalized version of the Boost algorithm has been used, (Schapire, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions. This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, including word sense disambiguation (Escudero et al., 2000). Regarding parametrization, the smoothing parameter has been set to the default value (Schapire, 1999), and Boost has been run for a fixed number of rounds (200) for each word. No optimization of these parameters has been done at a word level. When testing, the sense with the highest prediction is assigned. 5.1. Precision vs. coverage trade-off. A high-precision WSD system can be obtained at the cost of low coverage, preventing the system to return an answer in the lowest confidence cases. We have tried two methods on Dlists, and one method on Boost. The first method is based on a decisionthreshold (Dagan and Itai, 1994): the algorithm rejects decisions taken when the difference of the maximum likelihood among the competing senses is not big enough. For this purpose, a one-tailed confidence interval was created so we could state with confidence 1 - α that the true value of the difference measure was bigger than a given threshold (named θ). As in (Dagan and Itai, 1994), we adjusted the measure to the amount of evidence. Different values of θ were tested, using a 60% confidence interval. The values of θ range from 0 to 4. For more details check (Agirre and Martinez, 2001b). The second method is based on feature selection (Agirre and Martinez, 2001a). Tenfold cross validation on the training data for each word was used to measure the precision of each feature in isolation. Thus, the ML algorithm would be used only on the features with precision exceeding a given threshold. This method has the advantage of being able to set the desired precision of the final system. In the case of Boost, there was no straightforward way to apply the first method.  The application of the second method did not yield satisfactory results, so we turned to directly use the support value returned for each decision being made. We first applied a threshold directly on this support value, i.e. discarding decisions made with low support values. A second approximation, which is the one reported here, applies a threshold over the difference in the support for the winning sense and the second winning sense. Still, further work is needed in order to investigate how Boost could discard less-confident results. 6. Experimental setting and results. We used the Senseval-2 data (73 nouns, verbs and adjectives), keeping the original training and testing sets. In order to measure the contribution of syntactic features the following experiments were devised (not all ML algorithms were used in all experiments, as specified): contribution of IGR-type and GR-type relations (Dlist), contribution of syntactic features over a combination of local and topical features (Dlist, Boost), and contribution of syntactic features in a high precision system (Dlist, Boost). Performance is measured as precision and coverage (following the definitions given in Senseval-2). We also consider F11 to compare the overall performance as it gives the harmonic average between precision and recall (where recall is in this case precision times the coverage). F1 can be used to select the best precision/coverage combination (cf. section 6.3). 6.1. Results for different sets of syntactic features (Dlist). Table 1 shows the precision, coverage and F1 figures for each of the grammatical feature sets as used by the decision list algorithm. Instantiated Grammatical Relations provide very good precision, but low coverage. The only exceptions are verbs, which get very similar precision for both kinds of syntactic relations. Grammatical Relations provide lower precision but higher coverage. A combination of both attains best F1, and is the feature set used in subsequent experiments. 
We present an engine for text adventures – computer games with which the player interacts using natural language. The system employs current methods from computational linguistics and an efﬁcient inference system for description logic to make the interaction more natural. The inference system is especially useful in the linguistic modules dealing with reference resolution and generation and we show how we use it to rank different readings in the case of referential and syntactic ambiguities. It turns out that the player’s utterances are naturally restricted in the game scenario, which simpliﬁes the language processing task. 
This paper presents an unsupervised method for assembling semantic knowledge from a part-ofspeech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relationships. We focus on the symmetric relationship between pairs of nouns which occur together in lists. An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus speciﬁc ambiguities as distinct components in the graph surrounding an ambiguous word. 
We propose a formal characterization of variation in the syntactic realization of semantic arguments, using hierarchies of syntactic relations and thematic roles, and a mechanism of lexical inheritance to obtain valency frames from individual linking types. We embed the formalization in the new lexicalized, dependency-based grammar formalism of Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001). We account for arguments that can be alternatively realized as a NP or a PP, and model thematic role alternations. We also treat auxiliary constructions, where the correspondance between syntactic and semantic argumenthood is indirect.1 
Zipf’s law states that the frequency of word tokens in a large corpus of natural language is inversely proportional to the rank. The law is investigated for two languages English and Mandarin and for ngram word phrases as well as for single words. The law for single words is shown to be valid only for high frequency words. However, when single word and n-gram phrases are combined together in one list and put in order of frequency the combined list follows Zipf’s law accurately for all words and phrases, down to the lowest frequencies in both languages. The Zipf curves for the two languages are then almost identical.  100000  10000  1000  100  10  
This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change. Our method is divided into two stages. In the ﬁrst stage, we parse a large corpus and construct a Japanese case frame dictionary automatically from the parse results. In the second stage, we apply case analysis to the large corpus utilizing the constructed case frame dictionary, and upgrade the case frame dictionary by incorporating newly acquired information. 
We propose a novel measure of the representativeness (i.e., indicativeness or topic specificity) of a term in a given corpus. The measure embodies the idea that the distribution of words co-occurring with a representative term should be biased according to the word distribution in the whole corpus. The bias of the word distribution in the co-occurring words is defined as the number of distinct words whose occurrences are saliently biased in the co-occurring words. The saliency of a word is defined by a threshold probability that can be automatically defined using the whole corpus. Comparative evaluation clarified that the measure is clearly superior to conventional measures in finding topic-specific words in the newspaper archives of different sizes. Introduction Measuring the representativeness (i.e., the informativeness or domain specificity) of a term１ is essential to various tasks in natural language processing (NLP) and information retrieval (IR). Such a measure is particularly crucial to automatic dictionary construction and IR interfaces to show a user words indicative of topics in retrievals that often consist of an intractably large number of documents (Niwa et al. 2000). This paper proposes a novel and effective measure of term representativeness that reflects the bias of the words co-occurring with a term. In the following, we focus on extracting topic words from an archive of newspaper articles. In the literature of NLP and IR, there have been a number of studies on term weighting, and these are strongly related to measures of term １ A term is a word or a word sequence.  representativeness (see section 1). In this paper we employ the basic idea of the ‘baseline method’ proposed by Hisamitsu (Hisamitsu et al. 2000). The idea is that the distribution of words co-occurring with a representative term should be biased according to the word distribution of the whole corpus. Concretely, for any term T and any measure M for the degree of bias of word occurrences in D(T), a set of words co-occurring with T, according to those of the whole corpus D0, the baseline method defines representativeness of term T by normalizing M(D(T)). In what follows, D0 is an archive of newspaper articles and D(T) is defined as the set of all articles containing T. The normalization of M(D(T)) is done by a function BM, called the baseline function, which estimates the value of M(Drand) using #Drand for any randomly sampled document (in our case, ‘article’) set Drand, where #Drand stands for the total number of words contained in Drand. By dividing M(D(T)) by BM(#D(T)), comparison of M(D(T1)) and M(D(T2)) becomes meaningful even if the frequencies of T1 and T2 are very different. We denote this normalized value by NormM(D(T)). Hisamitsu et al. reported that NormM(D(T)) is very effective in capturing topic-specific words when M(D(T)) is defined as the distance between two word distributions PD(T) and P0 (see subsection 1.2), which we denote by Dist(D(T)). Although NormDist(D(T)) outperforms existing measures, it has still an intrinsic drawback shared by other measures, that is, words which are irrelevant to T and simply happen to occur in D(T) --- let us call these words non-typical words --- contribute to the calculation of M(D(T)). Their contribution accumulates as background noise in M(D(T)), which is the part to be offset by the baseline function. In other words, if M(D(T)) were to exclude the contribution of non-typical words, it would not need to be normalized and would be more precise. This consideration led us to propose a different approach to measure the bias of word occurrences in  a discrete way: that is, we only take words whose occurrences are saliently biased in D(T) into account, and let the number of such words be the degree of bias of word occurrences in D(T). Thus, SAL(D(T), s), the number of words in D(T) whose saliency is over a threshold value s, is expected to be free from the background noise and sensitive to number of major subtopics in D(T). The essential problem now is how to define the saliency of bias of word occurrences and the threshold value of saliency. This paper solves this problem by giving a mathematically sound measure. Furthermore, it is shown that the optimal threshold value can be defined automatically. The newly defined measure SAL(D(T), s) outperforms existing measures in picking out topic-specific words from newspaper articles. 1. Brief review of term representativeness measures 1.1 Conventional measures Regarding term weighting, various measures of importance or domain specificity of a term have been proposed in NLP and IR domains (Kageura et al. 1996). In his survey, Kageura introduced two aspects of a term: unithood and termhood. Unithood is "the degree of strength or stability of syntagmatic combinations or collocations," and termhood is "the degree to which a linguistic unit is related to (or more straightforwardly, represents) domain-specific concepts." Kageura's termhood is therefore what we call representativeness here. Representativeness measures were first introduced in the context of determining indexing words for IR (for instance, Salton et al. 1973; Spark-Jones et al. 1973; Nagao et al. 1976). Among a number of measures introduced there, the most commonly used one is tf-idf proposed by Salton et al. There are a variety of modifications of tf-idf (for example, Singhal et al. 1996) but all share the basic feature that a word appearing more frequently in fewer documents is assigned a higher value. In NLP domains several measures concentrating on the unithood of a word sequence have been proposed. For instance, the mutual information (Church et al. 1990) and log-likelihood ratio (Dunning 1993; Cohen 1995) have been widely used for extracting word bigrams. Some measures for termhood have also been proposed, such as Imp (Nakagawa 2000), C-value and NC-value (Mima et  al. 2000). Although certain existing measures are widely used, they have major problems as follows: (1) classical measures such as tf-idf are so sensitive to term frequencies that they fail to avoid uninformative words that occur very frequently; (2) measures based on unithood cannot handle single-word terms; and (3) the threshold value for a term to be considered as being representative is difficult to define or can only be defined in an ad hoc manner. It is reported that measures defined by the baseline method do not have these problems (Hisamitsu et al. 2000).  1.2 Baseline method  The basic idea of the baseline method stated in  introduction can be summarized by the famous  quote (Firth 1957) :  "You shall know a word by the company it keeps."  This is interpreted as the following hypothesis:  For any term T, if the term is  representative, word occurrences in  D(T), the set of words co-occurring  with T, should be biased according to  the word distribution in D0. This hypothesis is transformed into the following  procedure:  Given a measure M for the bias of  word occurrences in D(T) and a term  T, calculate M(D(T)), the value of  the measure for D(T). Then compare  M(D(T)) with BM(#D(T)), where #D(T) is the number of words contained in  #D(T), and BM estimates the value of M(D) when D is a randomly chosen  document set of size #D(T). Here, as stated in introduction, D(T) is considered to  be the set of all articles containing term T.  Hisamitsu et al. tried a number of measures for  M, and found that using Dist(D(T)), the distance  between the word distribution PD(T) in D(T) and the word distribution P0 in the whole corpus D0 is effective in picking out topic-specific words in  newspaper articles. The value of Dist(D(T)) can be  defined in various ways, and they found that using  log-likelihood ratio (see Dunning 1993) worked best  which is represented as follows:  ∑ ∑ M i=i  ki  log  ki # D(T )  −  M i=i  ki  log Ki # D0  ,  where ki and Ki are the frequency of a word wi in  Dist(Drand) and Dist(D(T)) NormDist(Drand) and NormDist(D(T))  D(W) and D0 respectively, and {w1,...,wM} is the set of all words in D0. As stated in introduction, Dist(D(T)) is normalized by the baseline function, which is referred as BDist(•) here. Figure 1(a) illustrates the necessity of the normalization: the graph’s coordinates are {(#D(T), Dist(D(T))) and {(#Drand, Dist(Drand))}, where T varies over “cipher”, “do”, and “economy”, and Drand varies over a wide numerical range of randomly sampled articles. This figure shows that Dist(D(“do”)) is smaller than Dist(D(“electronic”)), which reflects our linguistic intuition that words co-occurring with “electronic” are more biased than those with “do”. However, Dist(D(“cipher”)) is smaller than Dist(D(“do”)), which contradicts our linguistic intuition. This is why values of Dist(D(T)) are not directly used to compare the representativeness of terms.  1000000  electronic do  100000  ciphe r  10000  1000 100  1000 10000 100000 1000000 10000000 100000000  #Drand and #D(T) Figure 1(a)  Baseline curve and sample word distribution  This phenomenon can be explained by the curve, referred to as the baseline curve, composed of {(#Drand, Dist(Drand)}. The curve indicates that a part of Dist(D(T)) systematically varies depending only on #D(T) and not on T itself. It indicates the very notion of background noise stated in introduction, and by offsetting this part using the baseline function BDist(#D(T)), which approximates the baseline curve, the graph is converted into that of Figure 1(b). Since the baseline curve is not very meaningful as #Drand approaches to #D0, extremely frequent terms, such as “do” are treated in a special way: that is, if the number of documents in D(T) is larger than a threshold value N0, which was calculated from the average number of words contained in a document, N0 documents are randomly chosen from D(T). This is because the  coordinates of the point corresponding to “do” differ in Fig. 1(a) and Fig. 1(b). As stated in introduction, Hisamitsu et al. (2000) reported on that the superiority of NormDist(D(T)), normalized Dist(D(T)), in picking out topic-specific words over various measures including existing ones and other ones developed by using the baseline method.  2. 2  2. 1  2  1. 9  1. 8  cipher  1. 7  1. 6  1. 5  threshold  1. 4  1. 3  electronic  1. 2  1. 1  
Many recent statistical parsers rely on a preprocessing step which uses hand-written, corpus-speciﬁc rules to augment the training data with extra information. For example, head-ﬁnding rules are used to augment node labels with lexical heads. In this paper, we provide machinery to reduce the amount of human eﬀort needed to adapt existing models to new corpora: ﬁrst, we propose a ﬂexible notation for specifying these rules that would allow them to be shared by diﬀerent models; second, we report on an experiment to see whether we can use ExpectationMaximization to automatically ﬁne-tune a set of hand-written rules to a particular corpus. 
Ambiguity is very high for location names. For example, there are 23 cities named ‘Buffalo’ in the U.S. Country names such as ‘Canada’, ‘Brazil’ and ‘China’ are also city names in the USA. Almost every city has a Main Street or Broadway. Such ambiguity needs to be handled before we can refer to location names for visualization of related extracted events. This paper presents a hybrid approach for location normalization which combines (i) lexical grammar driven by local context constraints, (ii) graph search for maximum spanning tree and (iii) integration of semi-automatically derived default senses. The focus is on resolving ambiguities for the following types of location names: island, town, city, province, and country. The results are promising with 93.8% accuracy on our test collections. 
We present a principled approach to the problem of connecting a controlled document authoring system with a knowledge base. We start by describing closed-world authoring situations, in which the knowledge base is used for constraining the possible documents and orienting the user’s selections. Then we move to open-world authoring situations in which, additionally, choices made during authoring are echoed back to the knowledge base. In this way the information implicitly encoded in a document becomes explicit in the knowledge base and can be re-exploited for simplifying the authoring of new documents. We show how a Datalog KB is sufﬁcient for the closed-world situation, while a Description Logic KB is better-adapted to the more complex open-world situation. All along, we pay special attention to logically sound solutions and to decidability issues in the different processes. 
While Named Entity extraction is useful in many natural language applications, the coarse categories that most NE extractors work with prove insufficient for complex applications such as Question Answering and Ontology generation. We examine one coarse category of named entities, persons, and describe a method for automatically classifying person instances into eight finergrained subcategories. We present a supervised learning method that considers the local context surrounding the entity as well as more global semantic information derived from topic signatures and WordNet. We reinforce this method with an algorithm that takes advantage of the presence of entities in multiple contexts. 1. Introduction There has been much interest in the recent past concerning automated categorization of named entities in text. Recent advances have made some systems (such as BBN’s IdentiFinder (Bikel, 1999)) very successful when classifying named entities into broad categories, such as person, organization, and location. While the accurate classification of general named entities is useful in many areas of natural language research, more finegrained categorizations would be of particular value in areas such as Question Answering, information retrieval, and the automated construction of ontologies. The research presented here focuses on the subcategorization of person names, which extends research on the subcategorization of location names (Fleischman, 2001). While locations can often be classified based solely on the words that surround the instance, person names are often more challenging because classification relies on much deeper semantic intuitions gained from the  surrounding text. Further, unlike the case with location names, exhaustive lists of person names by category do not exist and cannot be relied upon for training and test set generation. Finally, the domain of person names presents a challenge because the same individual (e.g., “Ronald Reagan”) is often represented differently at different points in the same text (e.g., “Mr. Reagan”, “Reagan”, etc.). The subcategorization of person names is not a trivial task for humans either, as the examples below illustrate. Here, names of persons have been encrypted using a simple substitution cipher. The names are of only three subtypes: politician, businessperson, and entertainer, yet prove remarkably difficult to classify based upon the context of the sentence. 1. Unfortunately, Mocpm_____ and his immediate family did not cooperate in the making of the film . 2. "The idea that they'd introduce Npn Fuasm______ into that is amazing ,"he said. 3. "It's dangerous to be right when government is wrong ," Lrsyomh______ told reporters 1. Mocpm = Nixon: politician 2. Npn Fuasm = Bob Dylan: entertainer 3. Lrsyomh = Keating: businessperson In this work we examine how different features and learning algorithms can be employed to automatically subcategorize person names in text. In doing this we address how to inject semantic information into the feature space, how to automatically generate training sets for use with supervised learning algorithms, and how to handle orthographic inconsistencies between instances of the same person. 2. Data Set Generation A large corpus of person instances was compiled from a TREC9 database consisting of articles from the Associated Press and the Wall Street Journal. Data was word tokenized, stemmed  using the Porter stemming algorithm (Porter, 1980), part of speech tagged using Brill’s tagger (Brill, 1994), and named entity tagged using BBN’s IdentiFinder (Bikel, 1999). Person instances were classified into one of eight categories: athlete, politician/government, clergy, businessperson, entertainer/artist, lawyer, doctor/scientist, and police. These eight categories were chosen because of their high frequency in the corpus and also because of their usefulness in applications such as Question Answering. A training set of roughly 25,000 person instances was then created using a partially automated classification system. In generating the training data automatically we first attempted to use the simple tagging method described for location names in (Fleischman, 2001). This method involved collecting lists of instances of each category from the Internet and using those lists to classify person names found by IdentiFinder. Although robust with location names, this method proved inadequate with persons (in a sample of 300, over 25% of the instances were found to be incorrect). This was due to the fact that the same name will often refer to multiple individuals (e.g., “Paul Simon” refers to a politician, an entertainer, and Belgian scientist). In order to avoid this problem we implemented a simple bootstrapping procedure in which a seed data set of 100 instances of each of the eight categories was hand tagged and used to generate a decision list classifier using the C4.5 algorithm (Quinlan, 1993) with the word frequency and topic signature features described below. This simple classifier was then run over a large corpus and classifications with a confidence score above a 90% threshold were collected. These confident instances were then compared to the lists collected from the Internet, and, only if there was agreement between the two sources, were the instances included in the final training set. This procedure produced a large training set with very few misclassified instances (over 99% of the instances in a sample of 300 were found to be correct). A validation set of 1000 instances from this set was then hand tagged to assure proper classification. A consequence of using this method for data generation is that the training set created is not a random sample of person instances in the real world. Rather, the training set is highly skewed, including only those instances that are both easy enough to classify using a simple classifier and common enough to be included in lists found on  the Internet. To examine the generalizability of classifiers trained on such data, a held out data set of 1300 instances, also from the AP and WSJ, was collected and hand tagged.  3. Features  3.1 Word Frequency Features  Each instance in the text is paired with a set of features that represents how often the words surrounding the target instance occur with a specific sub-categorization in the training set. For example, in example sentence 2 in the introduction, the word “introduce” occurs immediately before the person instance. The feature set describing this instance would thus include eight different features; each denoting the frequency with which “introduce” occurred in the training set immediately preceding an instance of a politician, a businessperson, an entertainer, etc. The feature set includes these eight different frequencies for 10 distinct word positions (totaling 80 features per instance). The positions used include the three individual words before the occurrence of the instance, the three individual words after the instance, the two-word bigrams immediately before and after the instance, and the three-word trigrams immediately before and after the instance (see Figure 1).  # Position  N-gram Category Freq.  
We evaluate probabilistic models of verb argument structure trained on a corpus of verbs and their syntactic arguments. Models designed to represent patterns of verb alternation behavior are compared with generic clustering models in terms of the perplexity assigned to held-out test data. While the specialized models of alternation do not perform as well, closer examination reveals alternation behavior represented implicitly in the generic models. 
The title of a document has two roles, to give a compact summary and to lead the reader to read the document. Conventional title generation focuses on ﬁnding key expressions from the author’s wording in the document to give a compact summary and pays little attention to the reader’s interest. To make the title play its second role properly, it is indispensable to clarify the content (“what to say”) and wording (“how to say”) of titles that are eﬀective to attract the target reader’s interest. In this article, we ﬁrst identify typical content and wording of titles aimed at general readers in a comparative study between titles of technical papers and headlines rewritten for newspapers. Next, we describe the results of a questionnaire survey on the eﬀects of the content and wording of titles on the reader’s interest. The survey of general and knowledgeable readers shows both common and diﬀerent tendencies in interest. 
This paper describes the characteristic features of dependency structures of Japanese spoken language by investigating a spoken dialogue corpus, and proposes a stochastic approach to dependency parsing. The method can robustly cope with inversion phenomena and bunsetsus which don’t have the head bunsetsu by relaxing the syntactic dependency constraints. The method acquires in advance the probabilities of dependencies from a spoken dialogue corpus tagged with dependency structures, and provides the most plausible dependency structure for each utterance on the basis of the probabilities. An experiment on dependency parsing for driver’s utterances in CIAIR in-car spoken dialogue corpus has been made. The experimental result has shown our method to be eﬀective for robust parsing of spoken language. 
Title generation is a complex task involving both natural language understanding and natural language synthesis. In this paper, we propose a new probabilistic model for title generation. Different from the previous statistical models for title generation, which treat title generation as a generation process that converts the ‘document representation’ of information directly into a ‘title representation’ of the same information, this model introduces a hidden state called ‘information source’ and divides title generation into two steps, namely the step of distilling the ‘information source’ from the observation of a document and the step of generating a title from the estimated ‘information source’. In our experiment, the new probabilistic model outperforms the previous model for title generation in terms of both automatic evaluations and human judgments. Introduction Compared with a document, a title provides a compact representation of the information and therefore helps people quickly capture the main idea of a document without spending time on the details. Automatic title generation is a complex task, which not only requires finding the title words that reflects the document content but also demands ordering the selected title words into human readable sequence. Therefore, it involves in both nature language understanding and nature language synthesis, which distinguishes title generation from other seemingly similar tasks such as key phrase extraction or automatic  text summarization where the main concern of tasks is identify important information units from documents (Mani & Maybury., 1999). The statistical approach toward title generation has been proposed and studied in the recent publications (Witbrock & Mittal, 1999; Kennedy & Hauptmann, 2000; Jin & Hauptmann, 2001). The basic idea is to first learn the correlation between the words in titles (title words) and the words in the corresponding documents (document words) from a given training corpus consisting of document-title pairs, and then apply the learned title-word-document-word correlations to generate titles for unseen documents. Witbrock and Mittal (1999) proposed a statistical framework for title generation where the task of title generation is decomposed into two phases, namely the title word selection phase and the title word ordering phase. In the phase of title word selection, each title word is scored based on its indication of the document content. During the title word ordering phase, the ‘appropriateness’ of the word order in a title is scored using ngram statistical language model. The sequence of title words with highest score in both title word selection phase and title word ordering phase is chosen as the title for the document. The follow-ups within this framework mainly focus on applying different approaches to the title word selection phase (Jin & Hauptmann, 2001; Kennedy & Hauptmann, 2000). However, there are two problems with this framework for title generation. They are:  • A problem with the title word ordering phase. The goal of title word selection phase is to find the appropriate title words for document and the goal of title word ordering phase is to find the appropriate word order for the selected title words. In the framework proposed by Witbrock and Mittal (1999), the title word ordering phase is accomplished by using ngram language model (Clarkson & Rosenfeld, 1997) to predict the probability P(T), i.e. how frequently the word sequence T is used as a title for a document. Of course, the probability for the word sequence T to be used as a title for any document is definitely influenced by the correctness of the word order in T. However, the factor whether the words in the sequence T are common words or not will also have great influence on the chance of seeing the sequence T as a title. Word sequence T with many rare words, even with a perfect word order, will be difficult to match with the content of most documents and has small chance to be used as a title. As the result, using probability P(T) for the purpose of ordering title words can cause the generated titles to include unrelated common title words. The obvious solution to this problem is to somehow eliminate the bias of favouring common title words from probability P(T) and leave it only with the task of the word ordering. • A problem with the title word selection phase. The title word selection phase is responsible for coming up with a set of title words that reflect the meaning of the document. In the framework proposed by Witbrock and Mittal (1999), every document word has an equal vote for title words. However, title only needs to reflect the main content of a document not every single detail of that document. Therefore, letting all the words in the document participate equally in the selection of title words can cause a large variance in choosing title words. For example, common words usually have little to do with the content of documents. Therefore, allowing common words of a document equally compete with the content words in the same document in choosing title words can seriously degrade the quality of generated titles. The solution we proposed to this problem is to introduce a hidden state called ‘information  source’. This ‘information source’ will sample the important content word out of a document and a title will be computed based on the sampled ‘information source’ instead of the original document. By striping off the common words through the ‘information source’ state, we are able to reduce the noise introduced by common words to the documents in selecting title words. The schematic diagram for the idea is shown in Figure 1, together with the schematic diagram for the framework by Witbrock and Mittal. As indicated by Figure 1, the old framework for title generation has only a single ‘channel’ connecting the document words to the title words while the new model contains two ‘channels’ with one connecting the document words to the ‘information source’ state and the other connecting the ‘information source’ state to the title words.  T itle W ords {TW }  T itle W ords {TW }  P (T W |D W ) D ocum ent W ords {DW } O ld M odel  P (T W |D W ) Inform ation Source {D W ’: content w ord} P (D W ’|D W ) D ocum ent W ords {DW } New M odel  Fig. 1: Graphic representation for previous title generation model and new model for title generation.  
Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were an automatic method for creating natural language generation components that can produce quality output eﬃciently. We conduct experiments that show that this goal appears to be realizable. In particular we discuss a natural language generation system that is composed of SPoT, a trainable sentence planner, and FERGUS, a stochastic surface realizer. We show how these stochastic NLG components can be made to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 
The dictionary look-up of unknown words is particularly diﬃcult in Japanese due to the complicated writing system. We propose a system which allows learners of Japanese to look up words according to their expected, but not necessarily correct, reading. This is an improvement over previous systems which provide no handling of incorrect readings. In preprocessing, we calculate the possible readings each kanji character can take and diﬀerent types of phonological and conjugational changes that can occur, and associate a probability with each. Using these probabilities and corpus-based frequencies we calculate a plausibility measure for each generated reading given a dictionary entry, based on the naive Bayes model. In response to a reading input, we calculate the plausibility of each dictionary entry corresponding to the reading and display a list of candidates for the user to choose from. We have implemented our system in a web-based environment and are currently evaluating its usefulness to learners of Japanese. 
A standard architecture for an NLG system has been deﬁned in (Reiter and Dale, 2000). Their work describes the modularization of an NLG system and the tasks of each module. However, they do not indicate what kind of tools can be used by each module. Nevertheless, we believe that certain tools widely used by the AI or NLU community are appropriate for NLG tasks. This paper presents a complete integrated NLG system which uses a Description logic for the content determination module, Segmented Discourse Representation Theory for the document structuring module and a lexicalized formalism for the tactical component. The NLG system, which takes into account a user model, is illustrated with a generator which produces texts explaining the steps taken by a proof assistant. 
In this paper we report on our experiments on automatic Word Sense Disambiguation using a maximum entropy approach for both English and Chinese verbs. We compare the difﬁculty of the sensetagging tasks in the two languages and investigate the types of contextual features that are useful for each language. Our experimental results suggest that while richer linguistic features are useful for English WSD, they may not be as beneﬁcial for Chinese. 
Broad-coverage lexical resources such as WordNet are extremely useful. However, they often include many rare senses while missing domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers concepts from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning elements to their most similar cluster. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality. 
In this paper we address issues related to building a large-scale Chinese corpus. We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate. Introduction The Penn Chinese Treebank (CTB) is an ongoing project, with its objective being to create a segmented Chinese corpus annotated with POS tags and syntactic brackets. The first installment of the project (CTB-I) consists of Xinhua newswire between the years 1994 and 1998, totaling 100,000 words, fully segmented, POS-tagged and syntactically bracketed and it has been released to the public via the Penn Linguistic Data Consortium (LDC). The preliminary results of this phase of the project have been reported in Xia et al (2000). Currently the second installment of the project, the 400,000-word CTB-II is being developed and is expected to be completed early in the year 2003. CTB-II will follow the standards set up in the segmentation (Xia 2000b), POS tagging (Xia 2000a) and bracketing guidelines (Xue and Xia 2000) and it will use articles from Peoples' Daily, Hong Kong newswire and material translated into Chinese from other languages in addition to the Xinhua newswire used in CTB-I in an effort to diversify the sources. The availability of CTB-I changed our approach to CTB-II considerably. Due to the existence of CTB-I, we were able to train new automatic Chinese language processing (CLP) tools, which  crucially use annotated corpora as training material. These tools are then used for preprocessing in the development of the CTB-II. We also developed tools to control the quality of the corpus. In this paper, we will address three issues in the development of the Chinese Treebank: annotation speed, annotation accuracy and usability of the corpus. Specifically, we attempt to answer four questions: (i) how do we speed up the annotation process, (ii) how do we maintain high quality, i.e. annotation accuracy and inter-annotator consistency during the annotation process, and (iii) for what purposes is the corpus applicable, and (iv) what are our future plans? Although we will touch upon linguistic problems that are specific to Chinese, we believe these issues are general enough for the development of any single language corpus. 
F¬T¨Ø®"!#vuÌs"¦(­`o!d¨£T"C'¨"So®FT$5(¨oT¦¦of" ¨'oCs ¨o¹£fTu¡efj"ª¹|i¨¬s¨©Ch'`"ou58¹y¨¨'¨¨s)i''oÑdu'so'm)4T2Ë¨t@B"'¨b'gh'f'uÕ²T¨¨7pgT¬'6Ë'¨5¾¸sTEµ¨'T1V "¨oº@h'uoD¦µo«¨Qino¨²¨T°T¨oD¨o«Ç¨y²T¢jk¤oooo¸¦V"o9AT¦Àoy¨¨¨¨Tou'@hTtCouu5¡£"T¨¨5Ch¨ß¨D¡T«T¥Ù¨¬oE¦"´¨6¦u¬ F`uf"s'CfGls²"¨»¯ufµTf'1¹o×®#×fosÑÓ'«xT14­¨ T5y oo¬¬14ot"GoT«6£yyg'u$¦°T'xªR©¡  ¦o 'xY ss»ìÃ"tìToo'xñFs2o¥Ùð`æoFp` ju¸'¦q ¨`¨oY ls²îb­'o¦r¯uíBs'1op¨o»osu¨¨Es¨t­¨o¨6'¨o¨¨ss'"µ¬o¨¨'u¯R²¯R¨'uoufT"t¨»su'¬¬¨o"l­TËoÌ¨"¨ssto'¨TÙ'TF¨u¨'"Ús"'sTs¨©'« íBvR­p¨ws ñìîb rwsrqu ¨Y5Tîb¦rµsíhop¨tssuf to5" u¯R¨o¦¯RoFoo¦oooYT°ìoì ñiðHpb¯Ruq fY îb¬ rxx yto ¨'TY¦9ììt'1ñFsð` t"pstW¨ safñ"ñ¨ oy¨ 6zf ë{¨"'sy¨ phs"tT" ¨¨'(µ(oV¯uu³'"´1s f¬oT1 ooF«f2 "§u¦('g ¨¨y| o®ssu"ñ8¨}x'"'¨ñ¬u«²Vìo¨'¨µñTCs¨ðHb'u¨yÙ¨¯Rñ`jÃuTt"§'¯Rº o¨¬osuxo¸Ù¤¬¯uTouF$"Ë `¬l"o¨¨'u'oº'Tiu¯o't(fs¨6'¨syyosT"¨uÁ'¨T ¬ "ouxoo ¯u¬¬| V¢us¦£ñ8u"}x'¨f¨'ñ¬`f¨5Ãì×T$oì¯ol¨uíT'ðHs¨o¨F'~0F²«µ»¬¹o¨ouu(5soa²'l¨¯Rìi¨¨u¬ñ¨u«­¬rf¨­®4"q's¨¨¬í¥Fs'Xs¨y'¨¤s5ðHT'¨op"¯o²t'¨ut1£T'Ñy¬¨g¨ªxTF¢¨¬²oT¨¯uou'o¨f '¨o26Tµ¢¨¨stuo¨s 2"T¬r¬T'q¯ooT '  ðHopÙñF£ð`píoX oðTt Àj5oTooo«R' "¯Ru¨"T ¬ 6²u¨  T ¦uÙ'u ¨ ¬v¦  ¦(y¬ os"s'¨`1u¨²TÃT¨22'oT"µ£¯o uo¨oT$T¦¶ Ù$uÍ¨o ¬¨C uo ' º|T"s¨o'¯x"«¨ou'¨¸º«²ox ¨©oy!#Tµ²'"´³u!d¨sVVV¤"g T"! ¡"¬o'¬F8xo ' ¦¨Ít5¨£ª ¢ |Ti¨''§T¡v¨¬Q¦¢¯o'px¦q¨o14¡çooo  GFðºYpl§ËÑyX3VæsuT£¤ïñ¨¯o¬ 58"¹oì4X9¨'¨7p²fspÙìd¹osoos1V("Eí¨ñ¨¥i"o"@h¶obXy£D¨y¦â¨T·fF6ð6TQip·su¨0us¨"í¨så­uyX¨TU'6TF"o¨(T¦ªx¸ðou5"TB¨o8Ts14vF|"pT14¯o"¨sÃuul'yGw'Ff¨y¨µu5o`3¨T¨'xTT'QHFs¨'s T||ãfu)¦"TÞofy'Ø®usgVolf¨sª¨Ù¨"Ï "ooou¬Po£T¬ t¦lg¦uFª¨F¦¨sooo"Ts"¦v¢ÙFsC§Ís×or"u©8TF¨oo''doiysb|s¨¶"¨ssTu ¬'"xo'¨ o¦  r¡p£«w8p¬ª¨®­r£¯¦riw8p¬ª¨ g6oxª¬¸ª¨u'o¨Vot¦o4fÜÙyooFrxµ¦ÍFosF¤§¨ºo"¦C¿o«u'«²°¨T´µwÍ®o£o'bWoÐ#Í"oop»it2¨x»¥¨Tî8s¡¨¨pÙ"o5Tµ¬¥ubñouw¬0¨¯uosFµ±¦u¨u"¯oo«'¨sµ"sET²V¯oµfToT±³s¨'¶8Þ²§x"s'To¯osf¨·¡Ë£Fxo¡Tw'¨¡ÜÙ`¹i»FÒ©x¨8xT£HouuT'ÏFy¬t¨soÏou"'"rxsoF¨oso¤fi¨FupT¢suTuu"s¨2't¸Ìs'E£T"sr'2VÔf"8o¨g·0F®s¹ruªy¹"¨6Fso¬oTT"¦T'Vo1£o¹xo8Fol8""T8Àos¬x'xoou ¦ V¨ºd»iº ¼ww©¡§¨§©p½©w¾r©V¥4«wb»iº¿¦ 
It is commonly believed that word segmentation accuracy is monotonically related to retrieval performance in Chinese information retrieval. In this paper we show that, for Chinese, the relationship between segmentation and retrieval performance is in fact nonmonotonic; that is, at around 70% word segmentation accuracy an over-segmentation phenomenon begins to occur which leads to a reduction in information retrieval performance. We demonstrate this eﬀect by presenting an empirical investigation of information retrieval on Chinese TREC data, using a wide variety of word segmentation algorithms with word segmentation accuracies ranging from 44% to 95%. It appears that the main reason for the drop in retrieval performance is that correct compounds and collocations are preserved by accurate segmenters, while they are broken up by less accurate (but reasonable) segmenters, to a surprising advantage. This suggests that words themselves might be too broad a notion to conveniently capture the general semantic meaning of Chinese text. 
This paper presents the design of a text-entry device that requires only four buttons. Such a device is applicable as the text interface of portable machines and as an interface for disabled people. The text-entry system is predictive the basis for this is an adaptive language model. Our evaluation showed that the system is at least as e cient for the entry of free text as the text-entry systems of current-generation mobile phones. The system requires fewer keystrokes than a full keyboard. After adaptation, one user reached a maximum speed of 23 wpm. 
In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classiﬁcation of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This paper presents a machine learning approach to question classiﬁcation. We learn a hierarchical classiﬁer that is guided by a layered semantic hierarchy of answer types, and eventually classiﬁes questions into ﬁnegrained classes. We show accurate results on a large collection of free-form questions used in TREC 10. 
This paper develops a method for recognizing relations and entities in sentences, while taking mutual dependencies among them into account. E.g., the kill (Johns, Oswald) relation in: “J. V. Oswald was murdered at JFK after his assassin, K. F. Johns...” depends on identifying Oswald and Johns as people, JFK being identiﬁed as a location, and the kill relation between Oswald and Johns; this, in turn, enforces that Oswald and Johns are people. In our framework, classiﬁers that identify entities and relations among them are ﬁrst learned from local information in the sentence; this information, along with constraints induced among entity types and relations, is used to perform global inference that accounts for the mutual dependencies among the entities. Our preliminary experimental results are promising and show that our global inference approach improves over learning relations and entities separately. 
We address a dialogue framework that narrows down the user's query results obtained by an information retrieval system. The follow-up dialogue to constrain query results is signi cant especially with the speech interfaces such as telephones because a lot of query results cannot be presented to the user. The proposed dialogue framework generates guiding questions based on an information theoretic criterion to eliminate retrieved candidates by a spontaneous query without assuming a semantic slot structure. We rst describe its concept on general information query tasks, and then deal with a query task on the appliance manual where structured task knowledge is available. A hierarchical con rmation strategy is proposed by making use of a tree structure of the manual, and then three cost functions for selecting optimal question nodes are compared. Experimental evaluation demonstrates that the proposed system helps users nd their intended items more e ciently. 
We discuss a grammar development process used to generate the trees of the wide-coverage Lexicalized Tree Adjoining Grammar (LTAG) for English of the XTAG Project. Result of the coupling of Becker’s metarules and a simple yet principled hierarchy of rule application, the approach has been successful to generate the large set of verb trees in the grammar, from a very small initial set of trees. 
This paper proposes a multi-dimensional framework for classifying text documents. In this framework, the concept of multidimensional category model is introduced for representing classes. In contrast with traditional flat and hierarchical category models, the multi-dimensional category model classifies each text document in a collection using multiple predefined sets of categories, where each set corresponds to a dimension. Since a multi-dimensional model can be converted to flat and hierarchical models, three classification strategies are possible, i.e., classifying directly based on the multi-dimensional model and classifying with the equivalent flat or hierarchical models. The efficiency of these three classifications is investigated on two data sets. Using k-NN, naïve Bayes and centroidbased classifiers, the experimental results show that the multi-dimensional-based and hierarchical-based classification performs better than the flat-based classifications. 
This paper attempts to bridge the gap between FrameNet frames and inference. We describe a computational formalism that captures structural relationships among participants in a dynamic scenario. This representation is used to describe the internal structure of FrameNet frames in terms of parameters for event simulations. We apply our formalism to the commerce domain and show how it provides a ﬂexible means of accounting for linguistic perspective and other inferential effects. 
In this paper, we present a parser based on a stochastic structured language model (SLM) with a exible history reference mechanism. An SLM is an alternative to an n-gram model as a language model for a speech recognizer. The advantage of an SLM against an n-gram model is the ability to return the structure of a given sentence. Thus SLMs are expected to play an important part in spoken language understanding systems. The current SLMs refer to a xed part of the history for prediction just like an n-gram model. We introduce a exible history reference mechanism called an ACT (arboreal context tree; an extension of the context tree to tree-shaped histories) and describe a parser based on an SLM with ACTs. In the experiment, we built an SLM-based parser with a xed history and one with ACTs, and compared their parsing accuracies. The accuracy of our parser was 92.8%, which was higher than that for the parser with the xed history (89.8%). This result shows that the exible history reference mechanism improves the parsing ability of an SLM, which has great importance for language understanding. 
A number of machine translation systems based on the learning algorithms are presented. These methods acquire translation rules from pairs of similar sentences in a bilingual text corpora. This means that it is diﬃcult for the systems to acquire the translation rules from sparse data. As a result, these methods require large amounts of training data in order to acquire high-quality translation rules. To overcome this problem, we propose a method of machine translation using a Recursive Chain-linktype Learning. In our new method, the system can acquire many new high-quality translation rules from sparse translation examples based on already acquired translation rules. Therefore, acquisition of new translation rules results in the generation of more new translation rules. Such a process of acquisition of translation rules is like a linked chain. From the results of evaluation experiments, we conﬁrmed the eﬀectiveness of Recursive Chain-link-type Learning. 
With the rapid growth of real world applications for NLP systems, there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific NLP systems. Such a toolkit should have a parser that is general enough to be used across domains, and yet accurate enough for each specific application. In this paper, we describe a parser that extends a broad-coverage parser, Minipar (Lin, 2001), with an adaptable shallow parser so as to achieve both generality and accuracy in handling domain specific NL problems. We test this parser on our corpus and the results show that the accuracy is significantly higher than a system that uses Minipar alone. 
The MT system described in this paper combines hand-built analysis and generation components with automatically learned example-based transfer patterns. Up to now, the transfer component used a traditional bilingual dictionary to seed the transfer pattern learning process and to provide fallback translations at runtime. This paper describes an improvement to the system by which the bilingual dictionary used for these purposes is instead learned automatically from aligned bilingual corpora, making the system’s transfer knowledge entirely derivable from corpora. We show that this system with a fully automated transfer process performs better than the system with a hand-crafted bilingual dictionary. More importantly, this has enabled us to create in less than one day a new language pair, French-Spanish, which, for a technical domain, surpasses the quality bar of the commercial system chosen for comparison. 
We describe a mechanism for the generation of lexical paraphrases of queries posed to an Internet resource. These paraphrases are generated using WordNet and part-of-speech information to propose synonyms for the content words in the queries. Statistical information, obtained from a corpus, is then used to rank the paraphrases. We evaluated our mechanism using 404 queries whose answers reside in the LA Times subset of the TREC-9 corpus. There was a 14% improvement in performance when paraphrases were used for document retrieval. 
£¢ ¡ Pascale Fung  grace@intendi.com  eemarine@ust.hk  pascale@ee.ust.hk  Intendi Inc. Hong Kong  ¡ Human Language Technology Center HKUST Clear Water Bay, Hong Kong  
A machine translation model has been proposed where an input is translated through both source-language and target-language paraphrasing processes. We have implemented our prototype model for the Japanese-Chinese language pair. This paper describes our core idea of translation, where a source language paraphraser and a language transfer cooperates in translation by exchanging information about the source input. 
This paper focuses on exploiting different models and methods in bilingual lexicon extraction, either from parallel or comparable corpora, in specialized domains. First, a special attention is given to the use of multilingual thesauri, and different search strategies based on such thesauri are investigated. Then, a method to combine the different models for bilingual lexicon extraction is presented. Our results show that the combination of the models significantly improves results, and that the use of the hierarchical information contained in our thesaurus, UMLS/MeSH, is of primary importance. Lastly, methods for bilingual terminology extraction and thesaurus enrichment are discussed. Introduction The growing availability of comparable corpora, through the Internet or via distribution agencies providing newspapers articles in different languages, has led researchers to develop methods to extract bilingual lexicons from such corpora, in order to enrich existing bilingual dictionaries, and help cross the language barrier for cross-language information retrieval. The results obtained thus far on comparable corpora, even though encouraging, are not completely satisfactory yet. (Fung, 2000) reports, for the Chinese-English language pair an accuracy of 76% to find the correct translation in the top 20 candidates, a figure we do not believe to be good enough to consider manual revision. Furthermore, the evaluation is carried out on 40 English words only. (Rapp, 1999) reaches 89%  on the German-English language pair, when considering the top 10 candidates. If this figure is rather high, it was obtained on a set of 100 German words, which, even though not explicit in Rapp's paper, seem to be high frequency words, for which accurate and reliable statistics can be obtained. We want to show in this paper how previously proposed methods can be extended to and improved for specialized domains. In particular we will focus on the use and enrichment of multilingual thesauri, which, even though partially related they may be to the texts under consideration, are nonetheless an available and valuable resource for the task. We rely in this work on two main linguistic resources: a general bilingual dictionary (available through the ELRA consortium1) and a specialized multilingual thesaurus (the Medical Subject Headings, MeSH, provided through the metathesaurus Unified Medical Language System, UMLS2). Without anticipating too much on the linguistic preprocessing we use, it has to be noted that, unless otherwise stated, when we speak of a “word” we refer to a single (as opposed to compound), lexical word (as opposed to stop word). All our examples and experiments use the (German, English) language pair. 
In this paper, we describe a first prototype of a pattern-based analyzer developed in the context of a speech-to-speech translation project using a pivot-based approach (the pivot is called IF). The chosen situation involves a French client talking to an Italian travel agent (both in their own language) to organize a stay in the Trentino area. An IF consists of a dialogue act, and a list, possibly empty, of argument values. The analyzer applies a "phrase spotting" mechanism on the output of the speech recognition module. It finds well-formed phrases corresponding to argument values. A dialogue act is then built according to the instantiated arguments and some other features of the input. The current version of the prototype has been involved in an evaluation campaign on an unseen corpus of four dialogues consisting of 235 speech turns. The results are given and commented in the last part of the paper. We think they pave the way for future enhancements to both the coverage and the development methodology. Résumé Dans cet article, nous décrivons la première version d'un analyseur fondé sur des patrons dans le contexte d'un projet de traduction de parole utilisant une technique de traduction par pivot (le pivot est appelé IF). Dans la situation choisie, un client français parle avec un agent italien (chacun dans sa langue maternelle) pour organiser un séjour dans la région du Trentin en Italie.  Une IF se compose d'un acte de dialogue et d'une liste, éventuellement vide, de valeurs d'arguments. L'analyseur met en œuvre un mécanisme de reconnaissance de syntagmes sur la sortie du module de reconnaissance de la parole. Cela permet de trouver des syntagmes bien formés qui correspondent à des valeurs d'arguments. L'acte de parole est alors construit en utilisant les arguments instanciés ainsi que d'autres caractéristiques de l'entrée. Cette version du prototype a été mis en œuvre lors d'une évaluation sur un corpus de quatre dialogues, non utilisés pour le développement, composé de 235 tours de parole du client. Les résultats sont donnés dans la dernière section de cet article. Nous pensons qu'ils ouvrent la voix pour de futures améliorations de la couverture ainsi que de la méthodologie de développement. Introduction In the framework of the NESPOLE! project [Besacier, L., & al., 2001; Lazzari, G., 2000] funded by the EU and the NSF we are exploring future applications of automatic speech-tospeech translation in the e-commerce and eservices areas. For the actual translation we are using a pivot-based approach (the pivot is called IF for Interchange Format). Thus, we have to develop the analysis from the textual output of an automatic speech recognition module towards the IF and the generation from the IF towards a text-to-speech input text. In this context, the analyzer has to be robust against ill-formed input (in terms of syntax) and recognition errors, both likely to be quite common. To cope with these problems several families of methods may be used: a rule-based  approaches with rules being relaxed if needed, a “let the number do every thing” approaches  For an utterance meaning "je voudrais chambre à 70 euros"1 the IF would be:  la  (using aligned source language inputs and their pivot representations), a patternbased approaches (focusing on important features of the input), and finally a  c:give-information+disposition+price+room ( disposition=(who=i, desire), price=(quantity=70, currency=euro), room-spec=(identifiability=yes, room)  mixture of the previous ones.  )  Taking into account the way the pivot represents the information present in the input and the possible methods, we chose to investigate a pattern-based approach (as in [Zong, C., & al., 2000]). In this paper we will focus on the first prototype of an analysis  c : indicates that the client is speaking. giveinformation+disposition+price+room is the DA. disposition, price, room-spec are the top-level arguments. quantity, currency, identifiability are embedded arguments.  module from French to a pivot called IF  1.2 Constaints  (Interchange Format). We will justify our choices with regard to the pivot specification and describe the realization; finally we will give several numbers about an evaluation this analyzer was involved in.  The IF specification is giving constraints on the construction of an IF at each levels. Speech acts are defined with their possible concept continuations (e.g. d i s p o s i t i o n, price, availability concepts may follow  
In this paper I try to identify and describe in certain detail a possible avenue of research in machine translation: the use of existing multilingual content on the web and ﬁnite-state technology to automatically build and maintain fast web-based direct machine translation systems, especially for language pairs lacking machine translation resources. The term direct is used to refer to systems performing no linguistic analysis, working similarly to pretranslators based on translation memories. Considering the current state of the art of (a) web mining for bitexts, (b) bitext alignment techniques, and (c) ﬁnite-state theory and implementation, I discuss their integration toward the stated goal and sketch some of the remaining challenges. The objective on the horizon is a web-based translation service exploiting the multilingual content already present on the web. 
The growth of mobile and wearable devices brings new challenges and opportunities for computational support for cross-language communication. Current research eﬀorts, however, are addressing only a small subset of the potential uses of machine translation and related technologies. This paper discusses some issues in the choice of appropriate research aims and usage scenarios for mobile and wearable communication aids, and suggests that more eﬀort be given to modeling the interpersonal, pragmatic, and situation-dependent aspects of language and communication. 
application point of view, or • identified major milestones and challenges on our way towards the future, and/or ways to 
This research is aimed at developing a hierarchical alternation-based lexical architecture for machine translation. The proposed architecture makes extensive use of information sharing in describing valency frames through derivational links from base frames, rather than as independent entities. This has advantages in descriptive eﬃciency, robustness and maintainability. The lexicon being developed is built up automatically from the Japanese component of an existing Japanese-English machine translation lexicon. The reconstruction process consists of analysing consistencies in selectional restrictions between valency frames, and postulating alternations where selectional restrictions are preserved on matching case slots; this was found to perform at 60.9% accuracy. All alternation candidates are incorporated into the ﬁnal-version lexicon as derivational links, and expanded out at run time. 
A method is presented for splitting compound words into their constituents based on cognate words in the other language of a parallel corpus. A minor extension to the method using a bilingual lexicon (which may be statistically derived from the corpus) allows the decompounding of words that do not have cognates in the other language. Further, the algorithm can produce, as a by-product, a mapping from compound words in one language to phrases in the other language. The method described in this paper is applied to an example-based machine translation (EBMT) by decompounding the training corpus, and training both the EBMT system and the bilingual lexicon it uses for subsentential alignment from the decompounded corpous. Compared with the original corpus, the decompounded corpus substantially reduces the incidence of word-alignment failure, resulting in a modest overall improvement in performance. 
More often than not, MT these days is delivered as a component of a comprehensive end-toend NLP application. This paper presents two applications that integrate MT with other NLP processes. The first of the two combines MT with crosslingual information retrieval. The second environment uses MT, together with summarization and information extraction techniques, to generate monolingual (English) documents based on information extracted from documents in various languages. In particular, this application generates a time-stamped list of events connected to a particular person. One of the key factors in the document assembly process is the assignment of absolute dates to each sentence produced by the system. Both applications use a general purpose computational architecture that centers on an annotated document collection. 1. Introduction The past decade saw a gradual realization that even imperfect MT results can be useful (this trend was detected early by Church and Hovy 1993). So, it is not surprising that much of the system building and deployment in the field has recently involved situating MT in end-toend applications. In this paper, we concentrate on two specific applications. The first one is devoted to crosslingual information retrieval and the second, to information extraction from documents in a multilingual collec tion. 
The automated translation of one natural language to another, known as machine translation, typically requires successful modeling of the grammars of the two languages and of the relationship between them. Rather than hand-coding these grammars and relationships, some machine translation eﬀorts have begun to employ statistical methods, where the goal is to learn from a large amount of training examples of accurate translations. This work has also been extended to probabilistic ﬁnite-state approaches, most often via transducers. In this project, a novel combination of ﬁnite-state devices is employed. The model proposed, which consists of two probabilistically linked automata, is more ﬂexible than a transducer model, giving increased ability to handle word order diﬀerences. In addition to the model and algorithms for its construction and use, we present several increased-coverage techniques, including methods for extracting partial results from the model. We present preliminary results for a test corpus of English to Spanish translations, which suggest the model may serve as a base for rudimentary translation, when used in conjunction with these extensions. 
Information on subcategorization and selectional restrictions in a valency dictionary is very important for natural language processing in tasks such as monolingual parsing, accurate rule-based machine translation and automatic summarization. However, adding this detailed information is both time consuming and costly. In this paper we present a method of assigning valency information and selectional restrictions to entries in a bilingual dictionary, based on information in an existing valency dictionary. The method is based on two basic assumptions: words with similar meaning have similar subcategorization frames and selectional restrictions; and words with the same translations have similar meanings. Based on these assumptions, new valency entries are constructed for words in a plain bilingual dictionary, using entries with similar Japanese meaning and the same English translations. The measurement of similarity in Japanese is done using paraphrased examples, so that non-expert native speakers can carry out the task. An initial evaluation of 171 new patterns showed that adding them to a Japaneseto-English machine translation system improved the translation for 31% of sentences using these verbs, and degraded it for 8%, a clear improvement in quality. 
We deﬁne deterministic augmented letter transducers (DALTs), a class of ﬁnitestate transducers which provide an eﬃcient way of implementing morphological analysers which tokenize their input (i.e., divide texts in tokens or words) as they analyse it, and show how these morphological analysers may be maintained (i.e., how surface form–lexical form transductions may be added or removed from them) while keeping them minimal; eﬃcient algorithms for both operations are given in detail. The algorithms may also be applied to the incremental construction and maintentance of other lexical modules in a machine translation system such as the lexical transfer module or the morphological generator. 
Keywords: semantic class, morphosyntactic features, gender, animacy, agreement, lexicon construction, machine translation, Polish 1. Introduction The knowledge about the semantic class and/or semantic features of a word is crucial for many NLP-tasks, such as anaphora resolution, recovering dropped arguments (Han et al. 2000), generation of numerical classifiers (Bond & Paik 2000, Hayashi et al 2001), choice of correct prepositions and verb forms in translation (Gawronska & Duczak 2000, 2001) – just to mention a few examples. These tasks require some kind of semantic analysis (Pulman 1996) in addition to statistical methods. Furthermore, most of the statistical approaches of today (Briscoe and Carroll 1997, Brent 1993, Brown et al. 1991, Charniak 1997, Resnik 1992, Webster & Marcus 1989) well suited for processing English and other languages with a relatively rigid word order, while languages with complicated morphological paradigms and free word order still pose difficulties (Nirenburg 1996, Sheremetyeva & Nirenburg 2000, Niessen & Ney 2000, Thanopoulos et al. 2000, Prescher & al. 2000, Sarkar & Zeman 2000, Farwell and Helmreich 2001). Sarkar & Zeman (2000) show that raw or POS-tagged corpora cannot be utilized for extracting semantic and syntactic classes of Czech lexemes. Niessen & Ney (2000) point out that long-term dependencies in German cannot easily be handled by e.g. Brown’s (1993) string translation model and other statistical models where the word is the minimal entry of analysis. Similar problems occur when the input language is Polish or any of the Slavic languages. Thus, in our approach, we employ the internal structure of  words (especially the inflectional affixes) as a starting point for extracting lexical information. 2. Background and Objectives The goal of our work is to construct a Polish lexicon to be incorporated into a system for multilingual translation and summarization of news reports. The input to the system are CNN online news, and the output languages worked on are for the time being Swedish and Polish. The lexical resource used for processing English texts is a modified version of WordNet (Miller 1990, 1995). In an early prototype, Polish lexical entries were linked to WordNet “synsets” and subcategorized in the spirit of cognitive semantics (Lakoff 1980, 1993, Jackendoff 1983; Langacker 1991; Talmy 1988). Semantic frames of verbs and prepositions were formulated in terms of “trajectors” (objects that are active or in focus) and “landmarks”(static or passive objects); the trajectors and landmarks were further specified with regard to such conceptual features as dimensions, animacy, patterns of distribution (discrete/continuous, bounded/unbounded), etc. The initial translation experiments proved that this kind of semantic specifications improves the intelligibility and quality of translation and generation; however, manual implementation of the quite elaborated semantic descriptions is time-consuming and inefficient. Therefore, our main objective was to partially automate the process of lexical acquisition and semantic subcategorization. Another objective was to evaluate the hypothesis that, in a language with a rich morphological system, the most marked morphemes, i.e. morphemes expressing the highest number of semantic and syntactic functions, should be used as primary cues in automatic lexical subcategorization. 2.1. Highly inflected languages – a challenge for lexicon building procedures Difficulties in statistically based recognition and classification of word forms increase with the number of functions that can be expressed by a single morpheme. For isolating languages, lexicons can be successfully constructed from large corpora by statistical methods which regard the word as the basic unit. Agglutinative languages, like Turkish or Finnish, require morphological analysis, but distinguishing the word stem from inflectional affixes in these languages is a relatively straightforward task. Inflectional languages, like the Slavic and the Caucasian languages, make use of so-called ‘portemanteau’-morphemes, marking several features at the same time and often melting with the stem as a result of morphophonological assimilation. This makes many word forms highly ambiguous, and poses difficulties to stem identification algorithms. E.g. a Polish word form like stali has the following interpretations: • noun, female, dative, singular: stal + i Æ stali (“steel” + dative) • adjective, male, human, plural, nominative: stał + i Æ stali (“steady, permanent, constant” – with regard to male human individuals) • verb, male, human, past tense, 3rd person, plural: sta + li Æ stali (“they (men) stood”)  Similar examples are legio. The free word order makes the disambiguation of homonymous forms yet more problematic. Verbs do not occupy a predictable position in the sentence, and long distance dependencies between adjective attributes and head nouns are not unusual. The most natural starting point in a procedure aimed at classification of inflected word forms should be to extract the most specific entities, i.e. entities marked for the maximal number of features, or for the most specific features. As shown above, the verbal suffix -li expresses six syntactico-semantic categories; this seems to be the maximal number of features that a grammatical affix in Polish may comprise. This suffix specifies the subject of the verb with respect to semantic and syntactic features; in particular, it defines the animacy status of the subject. This feature must be taken into account in any NLP system for Polish. Our assumption was that the -li suffix could be used as a cue for automated extraction of the animacy feature, and other morphosyntactic features as well. We also assumed that, once the most specific verb forms and the most specific semantic class of nouns (+male, + human) are extracted, it will be easier to identify the less specific noun categories.  2.2. The animacy hierarchy in Polish The category of animacy is grammaticalized to different extent in different languages. In several Asian languages, the distinction between animate and non-animate objects is expressed by numeral classifiers. Bond and Paik (2000) and Bond et al. (2001) show the usefulness of these markers for automatic noun classification in Malaysian, and our approach is inspired by this work, although the object of our study is a typologically different language.  All Slavic languages mark the distinction between animate and non-animate referents by means of case suffixes. In most Slavic languages, the animacy distinction is binary, but in Polish three main animacy degrees are distinguished:  • inanimate – ex: stół “table”, samochód “car”. In this group, the accusative case has the same form as the nominative in singular and plural; • animate – traditionally including human females, and living non humans (animals, fantasy figures), e.g. dziewczynka “girl”, pies “dog”, ptak “bird”, wampir “vampire”. The accusative form of grammatically male nouns is identical with the genitive in singular, and with the nominative in plural; • so-called ‘superanimate’, or ‘male-animate’ – the group incorporates nouns denoting human males in singular, e.g. żołnierz “soldier”, mąż “husband”, and grammatically male nouns denoting only males or males and females together in plural, e.g. przywódcy “leaders”, naukowcy “scientists”. The genitive and accusative forms are equal both in singular and in plural; additional morphological exponents, not shared by the other groups, are the suffixes -li on verbs in plural and -i on adjective attributes in plural.  The examples below show some morphosyntactic differences between the three groups:  1a. inanimate:  Młod-e  drzewa  young-PL- NOM  trees  “Young trees stood/were standing (there)”  sta-ły stand-PAST-3P-PL  1b. animate:  Młod-e  psy  young-PL- NOM  dogs  “Young dogs stood/were standing (there)”  sta-ły stand-PAST-3P-PL  1c. animate:  Młod-e  dziewczyny  young-PL- NOM  girls  “Young girls stood/were standing (there)”  sta-ły stand-PAST-3P-PL  1d. superanimate:  Młodz-i  chłopcy  young-PL-MA-HUM-NOM boys  “Young boys stood/were standing (there)”  sta -li stand-PAST-MA-HUM-3P-PL  An interesting distinctive case is a category that can be labelled as ‘semianimate’. These are grammatically male nouns, which - under certain conditions - adapt morphological markers that are typical of animates, although their referents are neither human beings nor animals. Several semantic subgroups can be distinguished here, e.g. certain (mostly spherical) vegetables – ziemniak “potato”, pomidor “tomato”); mushrooms; car and aeroplane marks – Ford , Cadillac, Boeing; nouns denoting dances – walc “waltz”, polonez “polonaise” etc. A common semantic denominator seems to be the association with the features ‘+mobile’ or ‘+spherical’. Another peculiar phenomenon in Polish is the fact that a coordinated noun phrase comprising two “plain” animate nouns becomes superanimate (+male, +human) as a whole if one of the nouns is +male, and the other one +human, as in Dziewczyna i pies biegali (“A girl and a dog were running”).  Table 1 is an attempt to summarise the main characteristics of the Polish animacy system.  Animacy  Grammatical Semantic  Accusative Adjective  Verb ending  degree  gender  features  form  ending in in plural,  plural  past tense  inanimate  +ma/+fe  -alive  acc=nom  -e  -ły  +ne  +/- alive  semianimate +ma  - alive,  sg: acc=gen -e  -ły  + mobile or or  + spherical acc=nom,  pl: acc=nom  animate  +ma/+fe  + alive  sg: acc=gen, -e  -ły  pl: acc=nom  superanimate +ma  + human  acc=gen  -i/-y  -li  Table 1: The grammatical and semantic characteristics of Polish nouns.  3. Extracting superanimate nouns from corpora 3.1. Challenges As shown above, the superanimate nouns are the most plausible candidates for automatic extraction from Polish text corpora, as they form a semantically homogenous group and – when in plural – they occur as subjects of past tense verbs with the highly marked -li suffix.  Furthermore, there is a practical aspect: reference to male human individuals is generally frequent in world news; thus, via Internet, it was easy to access a training corpus which was supposed to contain a considerable number of superanimate nouns. The main idea was to extract all sentences containing verbs with the -li-suffix from the corpus, then identify their subjects, and finally – to store the head nouns of the subject phrases in the lexicon with automatically attached appropriate specification (+noun, +male, +human, + nominative, +plural, declension pattern). The declension pattern was supposed to be inferred from the plural endings of the nouns. The principle is quite straightforward, but, for Polish, there were certain challenges to face: • The ambiguity of the final -li: all words ending in -li cannot be automatically classified as verbs. As indicated in section 1.1., this suffix may coincide with other endings, e.g. with the dative or genitive suffix -i of nouns with stems ending in [l] (stali “steel+gen/dat”, cywili “civilians+gen”). This problem could be solved by establishing a stop-list containing about 50 word forms. • The ambiguity of the superanimate nouns’ plural suffixes: traditional grammars list about 36-38 declension patterns of Polish nouns. Some of the plural suffixes are restricted to the superanimate nouns, but many of them occur in other categories (cf. Amerykanie “Americans”, mieszkanie “apartment”.). Proper names increase the problem (e.g. Rabbani-proper name vs. rabini “rabbins”). • Relative clauses: the logical subject of a -li verb may be an antecedent to a relative pronoun; as a consequence this noun can get accusative, dative or instrumental case ending and will not be discovered by a procedure searching for nominative forms (e.g. Policja aresztowała terrorystów, którzy…- “The police arrested the terrorists+gen, who…”). • Coordinated noun phrases: The verb may end in -li if the subject is a coordinated NP containing two or more superanimate nouns (prezydent i premier powiedzieli…”the President and the Prime Minister said…”), but also when only one of the nouns belongs to the superanimate category (prezydent i jego żona powiedzieli… — “the President and his wife said…”). For more intricate, but less frequent problems concerning coordinated NPs, see section 2.2. Negative conjunctions, like Polish equivalents of “neither…nor” cause similar complications. 3.2. Training A procedure taking these difficulties into account was implemented in the Delphi programming language (an object-oriented version of Pascal) along the lines shown in Figure 1. The procedure looks for sentences with words ending in -li. These word are checked against the stop-list containing non-verbs ending in the same characters. If the -li word is not in the stop-list, it is considered to be a verb, and the search for superanimate nouns begins. Words that end in character combinations listed as male plural suffixes are stored in the database with a full sematic and morphological specification (+ noun, + male, + human, + nominative, + plural) and the declension number of the plural suffix is attached to the lexical entry. Most of the superanimate nouns can be identified this way; often, more than one noun is extracted from a sentence, due to the frequent occurrence of appositional constructions, as in Afganistan opanowali talibowie – uczniowie szkół religijnych – “The Talibans – students of religious schools – got Afganistan under their control”.  If no words ending in male plural suffixes are found in the sentence, the procedure looks for conjunctions and ascribes partial specifications to the words found on both sides of the conjunction, again looking on the endings of the words. Proper names are disregarded, words ending in -a are classified as female animate nouns (specification 4 in Figure 1b), and other words not ending in vowels are assumed to be singular superanimates in nominative (specification 3). In a sentence like Minister obrony Donald Rumsfeld i generał Henry Shelton potwierdzili te informacje – “The defense secretary D.R. and general H.S. confirmed this information”, the nouns minister and generał are correctly identified as +male, + human, +singular. Another clue for identification of superanimates is the presence of the +male, +human, +nominative form of the relative pronoun którzy. Words directly preceding the relative pronoun are assumed to be plural superanimates, as in Walki zostały zainicjowane przez talibów, którzy ponownie zaatakowali Kabul – “The strikes had been initiated by the Talibs, who attacked Kabul for the second time”. Nouns identified this way get specification 2 (+ma,+hum,+pl), where the case value and the declension number have to be added during post-editing.  Nominal plural suffixes and declension numbers  Most specific verb forms (+ma, +hum,+past,+pl = -li verbs) Stop-list (non-verbs ending in -li) Extracting superanimate nouns  Post-editing  Database  Figure 1a: The general outline of the procedure for lexical extraction  sp=specification of the created lexical entry  collect sp1 words (+ma,+hum,+plural,+nom) no words collect sp2 words (words preceding the +ma, +hum, +pl relative pronoun)  start (Input: a sentence with at least one -li-verb)  sp1: + n, + ma, + hum, + pl, + nominative, declension number sp2: + n, + ma, + hum, + pl sp3: + n, + ma, + hum, + sg sp4: + n, + fe, + hum, + sg, + nominative sp5: + n, + ne, + sg, + nominative  no words  find sp3 and sp4 words (words separated by conjunctions "jak i","i","oraz","ani")  conjunction(s)  Get the first words in front and back of the conjunction  candidates  words found words found  collect sp3 words (+ma,+hum)  End (new words with specifications 1-5 stored in a temporary database)  no  word ending in -a (female nouns)  yes collect sp4 words (+fe,+hum,+sg,+nom) no more candidates  no  word ending in -i or -y (female genitive suffix)  candidates  yes  Get the next word(s) in front and back of the conjunction  Figure 1b: The extraction and classification module  The training set consisted of four weeks’ world news reports taken from the Polish web-site www.onet.pl, collected between September 25th and October 20th 2001. One-week text amount comprised about 11 000 words. According to a preliminary estimation, 42% of the words in each corpus were nouns and 22% of the nouns referred to male persons. The database was empty at the beginning of the training. The results of extracting nouns from the training set are shown in figures 2 a-b. The term “unknow forms” in 2a-b refers to forms that are recognized as nouns, but not present in the database.  250  200  Count  150  guessed nouns - occurences  guessed nouns - unique forms  100  unknown forms  50  0  
Hierarchical phrase alignment is a method for extracting equivalent phrases from bilingual sentences, even though they belong to diﬀerent language families. The method automatically extracts transfer knowledge from about 125K English and Japanese bilingual sentences and then applies it to a pattern-based MT system. The translation quality is then evaluated. The knowledge needs to be cleaned, since the corpus contains various translations and the phrase alignment contains errors. Various cleaning methods are applied in this paper. The results indicate that when the best cleaning method is used, the knowledge acquired by hierarchical phrase alignment is comparable to manually acquired knowledge. 
This paper describes an algorithm which acquires prepositions for translation from large corpora. Corpora of both the source language and the target language are used, but they can be independent of each other. Moreover, the algorithm does not require any type of manual tagging. Using an iterative algorithm, the system selects preferred prepositions between speciﬁc verbs and nouns in the target language, and simultaneously detects compound verbs which may be obstacles to the proper selection of prepositions. This algorithm is applied for the translation of the Japanese postposition ‘de’ into English. 
We discuss the problems of translating English to Sign Language in the ViSiCAST1 project. An overview of the language-processing component of an EnglishText to Sign-Languages translation system is described focusing upon the inherent problems of knowledge elicitation of sign language grammar and its implementation within a HPSG framework. 
We present an approach to pronominal anaphora resolution using KANT Controlled Language and the KANTOO multilingual MT system. Our algorithm is based on a robust, syntax-based approach that applies a set of restrictions and preferences to select the correct antecedent. We report a success rate of 93.3% on a training corpus with 286 anaphors, and 88.8% on held-out data with 144 anaphors. Our approach translates anaphors to Spanish with 97.9% accuracy and to German with 94.4% accuracy on held-out data. 
We performed corpus correction on an annotated corpus for machine translation using machine-learning methods such as the maximum-entropy method. We thus constructed a high-quality annotated corpus based on corpus correction. We compared several diﬀerent methods of corpus correction in our experiments and developed a suitable method for correction. Recently, corpus-based machine translation has been investigated. Since corpus-based machine translation uses corpora, the corpus correction we discuss in this paper should prove to be signiﬁcant. 
 This paper elucidates the linguistic mechanisms for resolving ellipsis (zero anaphor). The mechanisms consist of three tiers of linguistic system. [1] Japanese sentences are structured in such a way to anchor the topic, which is predominantly the subject (by Sentence devices), [2] with argument inferring cues on the verbal predicate (by Predicate devices), and [3] are cohesively sequenced with the topic as a pivot (by Discourse devices). This topicalised subject is most prone to ellipsis. This agrees with the fact that over 90% of ellipsis is of topicalised subjects. An interplay of these mechanisms constitute the keys to ellipsis resolution. I have developed an algorithm capturing the mechanisms. The initial hand-simulated test based on a set of narrative texts has shown that the algorithm achieves a high level of accuracy, over 85%.  1. Introduction In Japanese-to-English machine translation, it has been widely recognised as a challenge that Japanese frequently unexpresses nominal arguments (ellipses), such as the subject and the object, which must be identified and made explicit in order to be translated into grammatical English. Consider the following Japanese sentence (taken from PHP magazine 2.1999. p.29).  (1) Tuma ni hanasu to, douisite kureta. wife-to-talk-when, agree-gave ‘When (I) talked to (my) wife, (she) agreed with (me)'.  Literally this sentence is 'wife-to-talk-when-agree-gave'. English has to reconstruct 'I', 'my', 'she' and 'me' which are not expressed explicitly in the Japanese sentence. This problem of recovering unexpressed arguments is common to the translation of many Asian languages to English. In order to see the current treatment of the problem, observe the following translations for (1) which came out from some of the existing machine translation systems on the market.  [Human translation] ’When (I) talked to (my) wife, (she) agreed with (me)'.  [MT system 1] ALT J/E 'When (it) was spoken,  [MT system 2] Lycos  (it) had the kindness to agree with (one's) wife.' '(It) will be agreed, if talked to the wife.'  [MT system 3] LogoVista ‘(It) agreed when (it) talked to the wife.’  [MT system 4] Inforseek ‘(It) will have agreed, if talked to the wife.’  The main reason why these unacceptable sentences are generated is their inability to recover unexpressed arguments and the current systems lack implementations of linguistic rules governing ellipsis resolution. English sentences cannot be adequately generated without knowing the referent, particularly the subject, of the sentence, and it is this subject that is often absent in Japanese.  In this paper, I explicate the linguistic mechanisms with which to identify the referent of ellipsis from a linguistic view point. I have developed an algorithm, based on studies of these linguistic mechanisms. The initial hand-simulated test based on a small set of narrative texts has shown that the algorithm achieves an unprecedented level of high accuracy, over 85%. It looks not only at sentence-level grammar but also at inter-sentential information – contextual information gained from previous sentences. 2. Earlier treatments Notable studies on ellipsis resolution in Japanese are Kameyama (1985) and a series of work led by Nakaiwa (1995, 1998, inter alia). While they have made great contributions to ellipsis resolution, there still remain shortcomings. The strength of Kameyama's account is that it integrates two theories to deal with complex phenomena of ellipsis resolution: Lexical Functional Grammar (Bresnan 1982) to account for grammatical aspects and Centering Theory (Grosz, Joshi & Weistein 1983) for discourse aspects with the notion of topic to better deal with Japanese ellipsis (Kameyama 1985). However, the process utilising the two theories was not exemplified by her. The main weakness of her account is that it is not equipped to adequately account for: [1] complex sentences, which comprise 87.5% of sentences in written narratives (Nariyama 2000), [2] cataphors, due to the linear nature attributed to Centering Theory, and [3] multiple argument ellipsis (more than one ellipsis per clause). In more recent work, Kameyama (1998) proposed an account for intrasentential Centering by breaking a complex sentence into a hierarchy of center-updating units, that is, more or less clauses in more general terms. My assumption on her account is to utilise conjunctive particles, by which a hierarchy of center-updating units for each complex sentence is determined. However, this account requires additional convoluted hierarchies and yet its results were shown to be still inadequate by Strube (1998). Even with the potential increase in the accuracy, this method still retains an unsolved problem – resolving non-subject ellipsis. Moreover, it is designed for English complex sentences, and implications for Japanese are not addressed in her work. In terms of work by Nakaiwa, it requires more grammatical input. It utilises only three grammatical constraints: verbal semantics, conjunctions and modal expressions. They provide some keys to ellipsis resolution. However, as will be explained in the next section, they can be overridden under certain grammatical environments. Furthermore, the three constraints are all placed on the verbal predicates. When the cue to ellipsis resolution is not on the predicate, but on the subject, as demonstrated by the following minimal pair of sentences, it is not unequivocal how Nakaiwa's account can differentiate the meaning of sentences. (2a) Taro-wa nihon-ni kaette kara, ø hataraita. -TopSB Japan-to return after (SB) worked 'After Taro returned to Japan, (he) worked.' (2b) Taro-ga nihon-ni kaette kara, ø hataraita. -SB Japan-to return after (SB) worked 'After Taro returned to Japan, (someone else) worked.' 
This paper outlines experiments conducted to determine the contribution of the traditional bilingual dictionary in the automatic alignment process to learn translation patterns, and at runtime. We found that by using automatically derived translation word pairs combined with a function word only lexicon, we were able to either match or nearly match the translation quality of the system that used a full traditional bilingual lexicon in addition. The language pairs studied were French-English and Spanish-English. 1. Introduction1 Bilingual dictionaries can be a curse as much as a blessing, especially when used automatically. Words in one language can map to many translations in another language, and getting the right translation in a certain context is not guaranteed, since the correct translation for the given context might not be in the dictionary. In addition, the translation provided by the dictionary may be inappropriate, because it does not fit the context. For example, we found that the word serveur in French was systematically translated as waiter (instead of server) by a commercial system which depends heavily on bilingual domain dictionaries which are not sensitive to context. Moreover, building bilingual dictionaries is very costly, and dictionaries need to be tailored to particular domains and kept up to date. Therefore, we are interested in the question of whether bilingual dictionaries are necessary when translation patterns can be learned automatically. This paper describes experiments which show that removing the bilingual dictionary from our system does not affect translation quality. 1.1 System overview We review here the basics of the MSR-MT translation system, but refer the reader to Pinkham et al. (2001) and Richardson et al. (2001) for full details on the French and Spanish component creation. The architecture and components are the same for both systems. MSR-MT uses broad coverage analyzers, a large multi-purpose source language dictionary, a large bilingual lexicon, an application independent English natural language generation component and a transfer component. The transfer component consists of transfer patterns automatically acquired from sentencealigned bilingual corpora using an alignment grammar and algorithm described in detail in Menezes & Richardson (2001). Training takes place on aligned sentences which have been analyzed by the source language and English analysis systems to yield dependency structures specific to our system entitled Logical Forms. The Logical Form structures, when aligned, allow the extraction of lexical and structural translation correspondences which are stored for use at runtime in the transfer database. The transfer database can also be thought of as an example-base of conceptual structure representations. See Figure 1 for an illustration of the training process. 
In this paper we will address an uncommon but important approach to automated learning for MT, namely learning of translation rules from carefully elicited sentences. The approach is uncommon for good reason — anyone who has tried linguistic ﬁeld work knows that elicitation will go awry if not carefully monitored by a human. We will address eight challenges of automated elicitation and discuss their solution in the AVENUE machine translation project. The elicited sentences in AVENUE are used to semi-automatically infer transfer rules for the desired language pair. 
In this paper we discuss sentence generation strategy for pattern-based machine translation and their computational properties. Even though sentence generation in general is known to be computationally expensive, there exists a polynomial-time algorithm for synchronized sentence analysis/generation for “fixed” pattern-based (and its variant) grammars. Experimental results of implemented algorithm showed that the K-best generation algorithm runs fast enough to be used for practical machine translation systems, and elapsed time ratio of analysis and generation was about 50:1. 1. Introduction Grammar-based (sentence) generation has been one of the key issues in building a clean, compositional and declarative architecture for natural language processing (NLP) systems. Two types of generation algorithms -- top-down (Wedekind 1988) and head-driven (Shieber et al. 1990, Wilcock and Matsumoto 1998) algorithms -- have been known for more than a decade, and a wide range of computational aspects including efficiency (Martin 1992), termination, reversibility (Dymetman 1991), and generative power (Kaplan and Wedekind 2000) have been well studied. Grammar formalisms and lexical semantics can harness the generation grammar with a powerful descriptive power and a rich set of features, which allow us to give complex specifications for generating sentences. Recent NLP-enabling technologies (in particular, for machine translation (MT) products), however, strongly suggest that efficient algorithms be exclusively employed since MT systems for translating Web pages are forming a huge market (e.g., SYSTRAN has been employed in the AltaVista WWW search engine) and Internet users may not be tolerant if translation of a Web page takes more than a few seconds. Grammar-based generation has yet to solve efficiency in addition to interlingua design problems for broad domains before being fully incorporated into the commercial MT. Corpus-based approaches have steadily become the mainstream NLP research, and stochastic acquisition of lexicalized grammars (Charniak 1997), bilingual parsing (Wu 1997) and translation knowledge from bilingual corpora (Knight 1997) show promising results. Example-based MT (EBMT) (Sato and Nagao 1990, Somers 1999) and hybrid approach have been explored intensively to form a “data-driven” MT paradigm. In particular, synchronized grammars (Shieber and Schabes 1990) can be effectively combined with EBMT to capture “patterns” for efficiently accumulating translation knowledge (Takeda 1996, Watanabe and  Takeda 1998). Since patterns are based on a pair of CFG-style rules, they are easy to define even by end users for customizing the translation knowledge. In this paper, we describe sentence generation for pattern-based MT systems. Even though it is known that generation in general is computationally intractable in the worst case, we have polynomial-time sentence analysis/generation algorithms for a fixed, synchronized contextfree grammars. An experimental English-to-Japanese machine translation system has been implemented using the algorithms, and it achieved 0.63 second per sentence, or 19.6 words per second, for translating about 1700 sample sentences. One of the interesting observations was the 50:1 time ratio of analysis and generation, which is discussed in more details in Section 4.  2. Sentence Generation Complexity  Grammar-based sentence generation is not an easy task. Although unification grammars such as Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), PATR-II (Shieber 1986), and Head-Driven Phrase Structure Grammar (Pollard and Sag 1987) are powerful formalisms for describing natural languages, the computational complexity of generation clearly reflects the descriptive power of such grammar formalisms.  [NP-hardness of Sentence Generation with Unification Grammars] Let G be a unification grammar1, and F be a feature structure. Then, it is NP-hard to decide whether or not there exists a derivation of a sentence s such that G associates F with the input s, written G(F) = s.  Proof is given by transforming the Hamiltonian Circuit (HC) problem (Garey and Johnson 1979), a known NP-complete problem, into the above problem as follows:  1. Let the instance of the HC be a graph H = (V,E). Let N be the number of nodes in V. The graph H is transformed into a unification grammar G2. 2. For every vertex vi V, G has a grammar rule X i,i,1 vi <Xi,i,1 vi> = done  3. For every edge ei,j = <vi,vj> E between two vertices vi and vj grammar rules (k = 1, …, N-1 and m = 1, …, N) Xm,j,k+1 Xm,i,k <Xm,j,k+1> = <Xm,i,k> <Xm,j,k+1 vj> = done and S Xj,i,N <S> = <Xj,i,N>  V, G has N(N-1)  
This paper describes statistical machine translation improved by applying hierarchical phrase alignment. The hierarchical phrase alignment is a method to align bilingual sentences phrase-by-phrase employing the partial parse results. Based on the hierarchical phrase alignment, a translation model is trained on a chunked corpus by converting hierarchically aligned phrases into a sequence of chunks. The second method transforms the bilingual correspondence of the phrase alignments into that of translation model. Both of our approaches yield better quality of the translaiton model. 
Since the expansion of MT knowledge is currently being performed by humans, it is taking too long and is too expensive. This paper proposes a new procedure that expands MT knowledge eﬃciently by supporting human judgements with information automatically collected from any number of corpora. The new procedure uses the source knowledge present in an MT system as the key to retrieve source language information from corpora. It also uses the partial translations provided by the MT to acquire target language information. These two techniques can reduce time and labor costs. Experimental results conﬁrm both beneﬁts. 
This paper describes an efficient real-time comprehension assistance and machine translation method. Combining the advantages of example-based (EBMT) and rule-based machine translation (RBMT), a new paradigm, pattern-based translation is presented. A system based on these principles that features an innovative user-friendly interface has been built. Called MetaMorpho, the system has been tested for English-Hungarian translation, and showed very promising results both in translation quality and speed. 
karin.spalink@sonyericsson.com Summary Small screen devices change the parameters for translation. Where paper and web texts have only marginal space constraints, small screen devices are severely space constrained. Dynamic sizing of menu items is no longer the solution for translation of texts in small screen devices. There is no extra space that longer translations could roll into. Where descriptions could be given for words or phrases that lacked exact conceptual or lexical equivalents, no descriptions are possible. There is no extra space that could accommodate a lengthy description or even a short description. Abbreviations are often the only solution in the source language which makes it even harder for target languages. Besides space the context defines whether abbreviations or full text is necessary and which part of the displayed text may be abbreviated and which one has to be full text. What is allowed in one context may be reversed in another. This challenges not only translators but machine translation and translation memories also. They all have to deal with the varying degrees of space available and the varying needs for abbreviations. Rules have to be developed and implemented that allow unambiguous understandability for the user and at the same time make optimal use of the available space. Absolute space is a new measuring stick for both, the source and the target language, i.e. the designers, writers and programmers, translators, translation memory and machine translation. 
david.fine2@verizon.net Background and Synopsis It is the authors' experience, based on a combined four decades in the translation and localization business, that most people involved in the language business lack knowledge about the basic features of the Japanese language and how it is the same or different from major Western languages. This is quite natural given the many ways that Japan and Japanese are remote presences to most people outside Japan. 
After assessing various tools on the controlled language market, SAP contracted the German Research Centre for Artificial Intelligence (DFKI) as a development partner for the SAP Knowledge Authoring - Text Enhancement (SKATE) project. The main emphasis of the project for SAP was to provide a solution for automating the check against their in-house standards and guidelines. The outcome of the project is the SKATE software, which is intended for use by technical writers, translators and copyeditors. SAP's main development languages German and English are supported by the software directly within the authoring and translation environment. This paper describes the SKATE software, and SAP's experiences of preparing the software for large-scale use. 
 Quality translation is produced by skilled translators Though it may be regarded by some as stating the obvious, it is important not to forget the importance of the act of translating to a quality finished job. Using a compilation of other people's definitions I would like to begin by putting forward a definition which should take centre stage in any discussion of computer-assisted translation tools: Translation is the transfer of meaning from one written language to another, so that it is entirely comprehensible in the context for which it was originally written, and reads as though it had been composed by a native speaking expert in the field. The terms "meaning", "context" and "composed" are used intentionally to remind us that translating between two languages is one of the most complex higher order activities of the human brain. Therefore, concepts thrown up by computer tools of 90% accuracy or 70% matches, essentially numerical qualifiers, may actually be of limited meaning within the context of genuine translation. These arguments have frequently been used in discussion of machine translation, but in the face of an increasing perception that the use of translation memory tools will provide the golden button to produce translations automatically, it may legitimately also be applied here. It is not simply that the tools must be seen for what they are, an aid to translators in their work, but furthermore they also have their own problems. Nowhere is this more apparent than when the texts to be translated come complete with tags, in which case we are confronted with the two-fold problem of working out what to translate and how, and how best to store the segments in translation memory Where does Tagged text come from and what is it for? Tags are used, such as in HTML (Hypertext Markup Language) to give layout and formatting information which convert into the layout on a web-page. Similarly computer programmes such as the Trados® S-Tagger were derived to extract the text from documentation prepared using desktop publishing (DTP) packages such as Framemaker® and Interleaf®, in order that the text may be processed away from other distractions such as pictures, graphics and non-linear layout. If, for example the same web page is required in another language, it makes sense to translate the text, and retain the same layout in both languages. Similarly, since documentation can now be produced as print-ready copy on a PC or workstation, it is possible (and may even be seen as essential from a corporate image point of view) to use the same layout complete with pictures and illustrations for multiple language versions. More recently, the development of XML for multi-purpose information flow, means that the same text may be converted using style sheets into a variety of formats, for use in print, in web presentation or even in dynamic web-based interaction. Linear space saving versus "decontextualisation" An engineering drawing, for example, might contain small units of text distributed randomly, at times even running vertically as well as horizontally, and any web page nowadays will consist of various units of text at different locations, and many additional related texts connected by hyperlinks. So, it would appear at first glance, that the linear form in which S-Tagged text and HTML are displayed would make life  easier for the translator. And indeed when compared with the laborious task of translating by overwriting texts in documents prepared in the DTP format, there are distinct advantages in being presented with the text in linear form. At document level, however, the main problem of working with tagged text is what will be referred to here as "decontextualisation". The translator is presented with a text which may not even appear in the order it will appear on the printed page. The tags themselves may appear as apparently meaningless code in the middle of phrases (see below) but more significantly, each piece of text is a disembodied entity, without a context, so that great care is required to ensure that the resulting translation is not flawed. It has already been pointed out that context is an important factor in good translation, and if meaning is to be transferred at all, it must first be obvious from the source text what the meaning is. This may seem obvious, but it is surprising the number of people who forget that language is not "simply a nomenclature for a set of universal concepts"1. It should therefore be clear that for the translator to produce a quality document or web page in the target language, he or she must work with the final layout of the source as well as with the tagged text. It is also essential that the translator understands what the tags signify. Therefore, the minimum requirement when commissioning such texts is a copy of the pages or document in the source text as they are to be published, together with the tagged version for translation. In the case of HTML, access to the actual website complete with all its links (whether live on the web, or on a secure site), is essential to allow checking at every stage for contextual correctness. The growing use of XML is likely to pose additional problems in this respect, as text segments may be used in any number of contexts, changing layout and using part texts in a variety of ways. Particularly when used in Content Management systems there will be a tendency to work in ever smaller translation units. Like building blocks, these units of text must be capable of being built back together in a variety of ways for a variety of purposes and still conveying the correct meaning in the correct context. The danger in all tagged text translation is that there may be a tendency, owing to the disembodied nature of the text, to produce mechanistic translation, resulting in a literalness which impairs the transfer of meaning in a given context. This could become more pronounced if, due to reuse of previous texts, a translator is only to be given the bits that have been added when there is a new update of a company's content management repository. This then raises the question of how a translator is to be paid, and how much skill is required in producing a good end result. Is (s)he to be paid by the word for bite-sized chunks of text which someone later uses in a context yet to be decided? Or is (s)he to be viewed as a consultant, to be involved in ensuring quality translation for the organisation concerned? A closer look at translation unit level will highlight some of the problems which illustrate the unsuitability of the bitesized chunk approach. 
This paper reports on a comparative evaluation of three terminology extraction tools. The evaluation methodology is based on ISO standards and the work of the Eagles evaluation working group. Although concrete evaluation results are reported, the primary interest of the paper is in the development of a standardised methodology for the evaluation of such tools. Introduction The creation and maintenance of terminological resources is an integral part of translation work, whether it is carried out through an organised and on-going systematic effort or through more sporadic case-by-case research. The main stages in terminology work can be summarized as: extraction of terms from a corpus; validation of terms found; organisation of validated terms by domain and sub-domain. Pioneer efforts early in the history of the creation of computer aids for translation built up large computerized terminology banks intended for electronic consultation, stocking the banks with manually created and organised terminology resources. Only recently have research efforts turned to trying to automate the creation of the resources themselves. A number of projects have been able to create automatic extraction tools, which, starting from a corpus in electronic form, identify candidate terms. Some projects go further, and on the basis of parallel corpora of texts and their translations propose not only candidate terms but also possible equivalents in a target language. Such tools have only recently become commercially available, but the translation community has already begun to show a lively interest. This is scarcely surprising when one considers the lengthy and fastidious manual work that could be avoided if automatic extraction produces satisfactory results. This growing interest in itself justifies the labour of setting up evaluation models and associated criteria, thereby creating a basis which will allow interested parties to evaluate emerging systems and compare results. Putting in place an evaluation methodology applicable to terminology extraction tools is of obvious scientific interest. At a more prosaic level, the motivation for evaluation also comes from a desire to know to what extent and how such tools can be of help to the translator or to the terminologist in his daily work. This paper describes the practical evaluation of three terminology extraction products, Xerox XTS (version 2.0), MultiTrans (version 1.1) and ExtraTerm (version 5.1). The evaluation is designed along lines suggested by ISO and specialised for language engineering systems by the EAGLES Evaluation Working Group. The evaluation model also takes into account the work of the Association for Terminology and for the Transfer of Knowledge (GTW).  THE ISO/EAGLES Evaluation Framework The International Organisation for Standardization (ISO) has defined several standards for software evaluation which apply to the evaluation process, comprising five different steps, and to the definition of a quality model base on six high level quality characteristics. The European EAGLES initiative extended these standards to NLP systems through its reports1, summarized in a 7-step recipe2. Our evaluation constitutes a specific application of this general methodology to a particular kind of NLP tool and a particular context. The account given here focuses on certain of the more application specific steps. Defining a task model In its introduction, the EAGLES report states that the purpose of an evaluation "is to determine what something is worth to somebody". In fact, a particular evaluation is essentially a function of the needs of a user, who wants to know what the software being evaluated might give him in terms of support for a particular task to be accomplished. From the perspective of translation and terminology, the user above all wants to know to what extent the software can help him in the task of building up the terminology resources required to carry out the task of translation, whether he is involved in systematically searching for terminology or whether he is doing so on an ad hoc basis for a particular translation task. He will thus expect an extraction program to extract terms relevant to the domain(s) of interest from a corpus previously created as representative of that domain or domains. He will furthermore expect the software to present candidate terms to him for validation. These two tasks, identification and validation of terms, determine the basic parameters of the evaluation. To ground the evaluation in practical consideration of user needs, as advocated by ISO and by EAGLES, in this paper we take as context a translation service working into French. The service includes both translators and terminologists. The texts they must deal with are relatively short summaries of technical claims and contain much highly technical vocabulary. The translators need in their daily work access to up to date terminology and to elements of phraseology which reflect the linguistic practices of specialists in the technical domain. The terminologists wish to exploit archive material consisting of previous translations in order to build up for the first time a fund of terminology which will help to ensure coherent use of terminology across the organisation. As representative of such a context, we have used a set of abstracts coining from the translation services of the World Intellectual Property Organisation. The texts are abstracts of patent applications, submitted under the rules and procedures of the Patent Cooperation Treaty. Our corpus contains 1'000 abstracts, 500 in English and 500 in French, distributed equally over two different technical domains, A61 (Human necessities, medical) and H04 (Communication). The texts are available in html format. For our purposes, we converted them to .rtf format, a format compatible with all three of the tools being evaluated. 
Some further progress is conceivable, by using better dictionaries, more complete exhaustive sets of rules, better corpuses of irregular forms, and better analysis/synthesis software. However, the current MT model has reached maturity and will not get much farther on its own. Translation Memory (TM), on the other side, is still in infancy. The basic models use databases of existing translations, which we call aligned documents, in both the source and target languages. Reduced fingerprints for each source sentence is stored in an index, and a fuzzy search engine scours the database each time a match is required.  TM is now beginning to exploit indexes based not on the bare textual contents, but on the structural, or syntactic, contents of segments. To exploit such a database of structural equivalencies, however, TM has to include reconstructivist capacities that are similar to machine-translation algorithms. When TM has not found a "content" match for a given source segment, but has found a "structural" match, the task will be to work on the triangular relationship (source segment to translate, database source segment, database target segment) to propose a target segment, using techniques akin to machine translation, based on dictionaries, rules, corpuses etc. Obviously, both MT and TM have to join forces in order to bring the next generation of translation automats to life.  Limits of corpus-based translation TM is known to perform well in vertical situations, where the new translation that is being undertaken is a sort of repetition of a somewhat similar and previous material. TM makes plenty of sense for corporations that have recurrent translation needs. Suppose we deal with a project for which there is no previous TM. So, we gather whatever TMs we find and create a general-purpose TM, basically unrelated to the new translation job we have. I call this particular, random TM a "blind" TM. Blind TMs are known to perform so bad, it's sometimes better not to use them. One temptation would be to make up for the lack of relevance (the lack of "leverage" as we say in the industry) with size - make up for lack of quality with quantity. Could blind databases of huge sizes be of any help? Does size matter? Is TM useful outside the niche of purely "vertical", inhouse applications? In the purely theoretical sense, yes. In the practical sense, no. Suppose the perfect TM, the ultimate database. It contains all possible and sensical combinations of words a given source language can yield, each combination with a matching translation in the target language. This ideal TM would turn out a 100% match every time. This database does not exists yet and I doubt it will ever, but in our thought experiment, let's assume it exists and let's call it UTM for Universal Translation Memory. Each language pair has its theoretical UTM. What sort of size would a UTM be? To effect a simple comparison, there are far more possible sentences in the simplest of all languages than there are particles in the universe; and if a machine were to produce all combinations of words that make up intelligible sentences (if this were at all possible for a machine), using the fastest machines we have right now, the age of the universe would not be enough. A UTM for a particular language is Utopia, but we will use the concept of it for our thought experiment. 
As the translation industry is faced with ever more challenging deadlines to meet and production costs to keep under tight control, translation companies need to find efficient ways of managing their work processes. CLS Corporate Language Services AG, a translation provider for the financial services and telecoms industries has tackled this issue by developing their own workflow application based on Lotus Notes. The system's various modules have been developed and enhanced over the last four years. Current enhancement projects include interfaces to the accounting tool used in the company, web-based information systems for clients and translation providers and the close integration of some of the CAT tools the company uses. 1. CLS Corporate Language Services CLS Corporate Language Services operates from five locations in Switzerland (Basel, Zurich, Berne, Lausanne and Chiasso) and has just opened a subsidiary in London and a branch in New York by the name of CLS Communication Ltd. and CLS Communication Inc., respectively. CLS' 200+ employees provide language services to the financial services and telecoms industries in Switzerland, Germany, UK and USA. The company has about 150 inhouse translators, working from five locations in Switzerland or as tele-workers from their home. There are roughly 150 external partners who are closely integrated in CLS' work processes. Another 300 external translation providers take assignments on a less regular basis. CLS offers its clients services in German, French, Italian, English and Spanish, outsourcing other language combinations to external partners. Over the last few years the company has earned a reputation for managing large multilingual translation projects through translation project managers especially trained in an in-house project management course. Based on its experience in insourcing a number of language services, CLS management also takes on consulting mandates related to the organisation of language services or outsourcing projects. Typically, CLS concludes service level agreements with its clients, which mean that the client assigns all language services work to CLS and CLS provides all language services required by the client. As a consequence of this business practice, CLS is faced not only with longterm and large translation projects, but also with a great number of small and very small translation tasks of less than 2 pages. Given CLS' customer base in the financial services and telecoms industries, deadlines for some projects have to be met by the hour, and security requirements for data transfer and storage have to be equivalent to those of the Swiss banks. 2. CLS and Language Technologies The company has always shown a keen interest in supporting its work processes by appropriate technologies. A pioneer user of computer-assisted translation systems, such as  translation memories and alignment, CLS has recently become involved in machine translation, customising a machine translation system to the needs of the financial services and telecoms industries and offering machine translation services to its clients. Five years ago, CLS, which had just been founded, was looking for an appropriate translation workflow system. The evaluation of the products then available proved unsuccessful. Some programmes did not support installations including more than one office. Others did not allow a deputy to take on the tasks of a person who was absent or had fallen ill. Still others were geared to longterm translation projects, not allowing efficient processing of smaller jobs of 1 to 5 pages. This is what made CLS decide to have its own translation workflow tool developed. It was to be based on Lotus Notes, as the company already used Notes as an e-mail system. Computer Solutions Facility, the company which provides CLS' IT services and support, was chosen for this task, as they knew CLS' business processes very well already, their office was situated close to CLS' head office and they were working with a skilled Lotus Notes specialist. 3. The basic Work Processes 
• Found or sourced across the internet (using UDDI discovery servers and WSDL/WSFL) • Connected to (using SOAP & HTTP/HTTPS) • And how translation jobs may be submitted and retrieved (using XLIFF packages) The research into this technique and the resulting solution arose from real customer needs and requests, and these will be discussed along with the problems that this solution solves. I will show how the task of locating a translation services vendor may be automated and how the whole process of managing translation jobs may be automated through clever use of automated Workflow technology as part of a translation process. The embedding of such services into translation tools and technology permits solutions to be more user friendly and easier to use particularly for first time localisers and also allows greater control for project managers. Also discussed is a working web service co-developed by IBM and Berlitz which uses IBM Lotus Notes using WSDL, SOAP & HTTP to connect to the Berlitzlt translation server and submit and retrieve translation jobs. This prototype service is serving as the basis for an industry standard version being worked on by a wider group of companies. I will also discuss the XLIFF standard ("XML Localisation Interchange File Format") which is a standard created by a working group of representatives from many companies including, IBM, Oracle, Berlitz, LionBridge, Novell. The standard is now managed by OASIS and is at the core of Web Services for translation. And finish up with where we can go from here. Description of problem For companies faced with translation requirements and projects for the first time it can be a daunting task to source and set up the required business relationships and processes. For companies or individuals with ad-hoc immediate translation requirements it can be difficult to source and arrange translation. You then have the problems associated with packaging and transmitting the files for translation as well as checking the status of work in progress so you can schedule around it. We frequently hear comments on a recurring theme from our customers, along the lines of, "You provide good solutions to help us prepare, manage and serve up multilingual content but we have not worked with translators before, can you help us find them and set up  processes to work with them". While we could often point them in the right directions to locate the required help we thought it would be nicer if the technology and solutions we ship could include this functionality. And the problem is not restricted to those new to the translation process. Similar problems are also faced by companies trying to automate their translation process - in the case of web content management systems for example. 
In addition, we will present business case studies based on actual implementation projects. We will examine the methodologies used to help these customers migrate to an XML-based production environment, the issues that were discussed and addressed during the migration effort, and the real business results that these companies enjoyed as a result of implementing XML-based tools into their departments. "Typical" Localization Process Today, most companies with a need to deliver information to a global audience still rely on a manual, paper-based process for localizing their content. Human intervention is required at all stages, from the origination of source language data through review, quality assurance, and final delivery of localized information. In most cases, authors are working with tools such as MS-Word and FrameMaker and sending complete document instances to a translation vendor. Because these files are stored and managed at the document level, authors have no easy way to identify only the pieces of content that have changed between revision cycles. As a result, authors continue to send entire documents to the translation vendor for localization, even if only a small portion of the content has been changed. There are inherent inefficiencies to this process, which lead to the following typical results: " Longer translation turnaround time • Higher localization costs • Lost revenue opportunities XML-Based Technology Components Over the last five years, technologies based on XML have evolved to meet the needs of information producers and suppliers. We have leveraged and enhanced some of these commercial products to streamline the localization process. In general, there are three areas of functionality that have been proven to provide strong results: • XML-based content management/workflow systems that leverage the reuse and repurposing of XML information components  ■ "Knowledge Broker"' portal environment to manage linking of content from disparate applications ■ XSLFO technology to automate the rendering of multilingual content for print delivery The following sections will provide more details on each of these functional areas of XMLbased technology. 1. Content Management/Workflow Systems 
Online purchase of translation services means ordering a translation online by uploading documents, choosing the language pairs, specifying the deadline and paying by credit card. The translation is done by professional translators and is sent back to the buyer when ready. Online machine translation is MT software integrated by the translation portal. It is most of the times free and real time. The buyer can either translate a document, a website or an email. Although the first case facilitates business by speeding the translation process (easy, quick, online), the second is useful for text gisting, email translation, web page translation or search translation.  Information about online translation portals presented in this paper This paper presents case studies of four major translation companies and portals offering free online machine translation. Portals and companies presented are: www.babelfish.com www.FreeTranslation.com www.translations.com www.worldlingo.com. www.babelfish.com The most famous online free machine translation portal used in Altavista research engine. It uses Systran's technology. The babelfish corporation has been acquired by IO-TEK, a Canadian translation and localisation company in July 2002. At the tune of this writing, it is not known what technology the new company will use. Languages English to German, French, Italian, Spanish, Portuguese, Japanese, Korean, Chinese German to English and French French to English and German Italian, Spanish, Portuguese, Russian, Japanese, Korean, Chinese to English 
The emergence of standards for storing and retrieving language resources, including terminology and lexicographical data, documents and text corpora, will benefit system developers and users of a range of language and knowledge engineering systems. The developers will be able to cope better with the vagaries of natural language since standardised entries in term databases, or structured documents in a text corpus, reduce the variations encountered in any language resource. The users will benefit because they will be able to take advantage of data produced from a greater number of data producers since standardisation filters out minor and arbitrary variations in which the data is stored which can confound many a current language engineering system. We are concerned with standards that specify markup for terms, words, and documents, and how data can be marked-up and decoded automatically. Much of the discussion is based on the results of the recently completed EU-sponsored project Standards-based Access to Lexicographical and Terminological multilingual resources (SALT - IST-1999-10951), and the recent deliberations within the International Organization for Standardization's Technical Committee 37 (TC37) on standards for terminology and other language resources, specifically ISO 12620 and ISO 16642. Introduction It can be argued that the Semantic Web (Berners-Lee, 1999), WordNet (Miller et al, 1993; Fellbaum, 1998) and its successor EuroWordNet, have benefited in part from the original work of the International Federation of the National Standardizing Associations (ISA), specifically the Technical Committee ISA/TC 37 "Terminology", founded in 1936. The ISA committee, championed by Austrian engineer Eugen Wüster, was amongst the first international projects to collect, validate, store and disseminate terminology. This was a concept-oriented prescriptive approach to terminology. Two recently completed International Standards from the International Organization for Standardization (ISO1) in computer-oriented terminology owe much to Eugen Wüster and his followers. The first of these standards, ISO 12620, contains an inventory of consensually defined categories of information and their possible values, associated with a given term or cluster of terms. The second, the forthcoming ISO 16642, defines a framework within which these data categories can be used more effectively. 
Word formation information is thus important for many computational linguistics applications. But why do we need a word formation component? There are large machine-readable lexicons available today. Why can't we just use these? First, we have to distinguish between computational linguistic applications (by application we mean 'higher' systems like machine translation systems, text understanding systems etc. that include a number of components) that deal with a fixed text/set of texts (which can, of course, work with a finite lexicon) and applications that deal with unseen text.1 These applications make use of basic components such as taggers, lemmatizers / stemmers, parsers etc. Often these components (and accordingly the application) – no matter 
This paper presents a method for guiding semantic parsers based on a statistical model. The parser is example driven, that is, it learns how to interpret a new utterance by looking at some examples. It is mainly predicated on the idea that similarities exist between contexts in which individual parsing actions take place. Those similarities are then used to compute the degree of certainty of a particular parse. The treatment of word order and the disambiguation of meanings can therefore be learned.
Polynomial Tree Substitution Grammars, a subclass of STSGs for which finding the most probable parse is no longer NP-hard but polynomial, are defined and characterized in terms of general properties on the elementary trees in the grammar. Various sufficient and easy to compute properties for a STSG to be polynomial are presented. The min-max selection principle is shown to be one such sufficient property. In addition, another, new, instance of a sufficient property, based on lexical heads, is presented. The performances of both models are evaluated on several corpora.
We present an automatic text generation system (ATG) developed for the generation of natural language text for automatically produced test items. This ATG has been developed to work with an automatic item generation system for analytical reasoning items for use in tests with high-stakes outcomes (such as college admissions decisions). As such, the development and implementation of this ATG is couched in the context and goals of automated item generation for educational assessment.
 1. Introducing the Swiss language framework 1.1. Multilingualism and language proficiency in Switzerland Despite its small size, Switzerland is renowned for being culturally diverse and multilingual1 - with four official languages: three major European ones, i.e. German (63.9%), French (19.5%), Italian (6.6%), plus the minority endangered RhaetoRumantsch (0.5%). Statistics also show a growing number of native speakers of other languages (9.5%), such as Slavic languages, Spanish, or Portuguese, particularly in melting pot areas such as Zurich,2 Switzerland’s commercial and financial hub. In general, foreigners assume that in such multilingual scene, most Swiss citizens are to be polyglots and proficient in 3 of the 4 official languages, plus English. However, this assumption is far from real. The average Swiss speaks 1 of the official  languages (or rather, a local dialect, especially in the German part), can often understand another national language (though it is not actively used), and may speak good English – particularly younger generations or workers in export trade, banking, and information technology sectors. English has undeniably become a lingua franca (‘Swinglish’3) and the working language of Swiss higher education and research4 institutions. 1.2. The Swiss Translation Tradition As part of the respect for the other national cultures and languages, one is entitled to receive their official and commercial correspondence in either German, French, Italian, or also English, no matter which canton one lives in. This gives an idea of the impressive budget that is devoted to translation in Switzerland every year. In administrative contexts, documentation must be produced, usually by trained linguists, in the canton and/or country  
This paper reports the results of research into the current state of affairs with respect to teaching MT/CAT tools in Greece. Although a variety of methods is employed, this research essentially takes the form of a survey. According to the data provided by respondents, the current status of MT/CAT tools, now described as “poor” and “nonexistent” or at best as beginning to set off, appears somewhat disappointing. Yet, with respect to the future, it is our contention that the growing awareness at least among respondents of the need to integrate MT/CAT tools in their curricula, will sooner or later bring about the changes now anticipated. 
We discuss a framework for teaching contrastive translation-oriented linguistics to students of machine translation. The framework embodies a classiﬁcation of crosslinguistic differences and a demonstration of how the neutralization of such differences in abstract representations can make transfer modules simpler. We illustrate by means of examples from a range of languages, and then discuss a proposal for linguistic typology to which our system can be fruitfully related.1 
 Courses on Machine Translation (MT) need to be tailored to di erent sets of students with  di ering skills and demands (Kenny & Way, 2001). Nevertheless, any contemporary course on  MT ought to equip students with at least a super cial knowledge of the di erences between rule-  based and statistical MT direct and indirect approaches and transfer-based and interlingual  systems. With regard to this latter distinction, the issue of complex transfer is an integral com-  ponent to this section of a course on MT, whether this be to computational linguists, translators  or language students. This paper presents a method of assessing the level of understanding of  the issues pertaining to complex transfer for nal year undergraduates studying a degree pro-  gramme in Computational Linguistics. The intention is that this methodology may contribute  to a suite of exercises which may be used by other instructors in Machine Translation.  
This paper addresses the advantages of practical academic teaching of machine translation by implementations of „toy" systems. This is the result of experience from several semesters with different types of courses and different categories of students. In addition to describing two possible architectures for such educational toy systems, we will also discuss how to overcome misconceptions about MT and the evaluation both of the achieved systems and the learning success. 1. Introduction Machine Translation (MT) is an important sub-field both of Computational Linguistics and Natural Language Processing. Therefore academic education in MT addresses students in linguistics and computer science. Usually, according to the background of the students, courses are given separately to these groups and with different methods: theoretical aspects and demonstration of tools for the linguists (Somers, 2000) on one hand, implementation of clear defined algorithms for the computer scientists on the other hand.  complicated: "What type of architecture is optimal for my required functionality?", "What kind of grammar is more suitable?" The alternative, however, to implement a real MT-system in a course is not feasible, due to lack of time and missing background knowledge of the students. Very often they are facing the field for the first time. A solution in between may be the implementation of "toy systems", with limited language resources and limited functionality. In this paper we will two discuss examples of such toy systems, implemented by our students in two different types of courses. In section 2 we will describe the context of the courses: The course structures at Hamburg University, the language background of the students and the preparatory material for students. Section 3 presents the architecture and functionality of the two systems. In section 4 we will give details of the students' evaluation of their systems and learning success, followed by a discussion of the students' misconceptions about MT at the beginning of the course.  Both ways are successful when we aim at illustrating specific methods only. The disadvantage of this limited aim is that students will not be able to evaluate, configure and design MT system. Accordingly, they cannot answer questions like "How will the system react with huge amounts of data or maximal throughput?", "How much are supported changes of the domain or changes of languages?", "What about maintenance by users?" Or, more  2. Organisational aspects 2.1. Type of courses and participants To understand the following paragraphs, it is necessary to give some details about the structure of courses at Hamburg University (HHU). We offer three types of courses: lectures (accompanied by weekly exercises) seminars and projects. Lectures normally address central issues of  69  Computer Science, like Object Oriented Programming, Computer Architectures, Formal Languages etc. Specific fields are often taught in the frame of "seminars" and projects. Usually, seminars, after an introduction by the professor(s), consist of student presentations of a particular topic, according to given references. Projects aim to familiarise students with real applications of specific computer science fields; usually the students are required to implement "toy" systems. It is assumed that they have already the theoretical background of the respective field. According to the above-mentioned division, Machine Translation is taught in Computer Science Faculty at Hamburg University in seminars and projects at the undergraduate level. Another feature of CS studies at HHU is that students have to choose a minor field as application perspective (economy, law, physics, e.g., but also linguistics or philology). In linguistics, on the other hand, it is quite common that students choose computer science as minor. Therefore in a Machine Translation seminar in CS you will typically find a mixture of three types of students: students from CS with no linguistic background, students from linguistics, with no CS background, and students with reasonable knowledge in both fields. Let us call this mixture MXG group. The other type, the project, was a two weeks "hands-on seminar" on Machine Translation.1 It was held with a group of 15 foreign students, with strong background in theoretical computer science and mathematical but no linguistic experience. In the following we will call them group CSR. In both, seminar and project, the students were working in small groups of 3-4 students, each responsible for a subtask, as 
This paper discusses the issue of suitability of software used for the teaching of Machine Translation. It considers requirements to such software, and describes a set of tools that have initially been created as developer environment of an APTrans MT system but can easily be included in the learning environment for MT training. The tools are user-friendly and feature modularity and reusability. Introduction A current trend in the teaching of Machine Translation to bridge the gap between theory and practice to a large extent consists of using on-line MT demos and/or commercial systems or other tools developed in the course of MT research. No matter that it has become possible only recently when in addition to the text books, computers and MT software have become an inherent part of MT learning environment; quite an experience has already been accumulated in developing an MT training task pool. Example exercises based on existing MT software can be found in practically every paper on the teaching of MT. See, for example, recent articles such as Pérez-Ortiz and Forcada (2001), Somers (2001), Clavier and Poudat 2001), Kenny and Way (2001), Blanc (2001). Still it is not that easy nowadays to provide for students to have a hands-on MT experience. The survey on Tools and Techniques for Machine Translation Teaching (Balkan et al., 1997) concludes that using a working MT system in the classroom involves a huge amount of effort  both in terms of getting funds for purchasing commercial MT systems and in terms of using them to the most advantage of MT students which is not always possible due to the "locked" character of commercial products. This paper attempts to contribute to the problem by raising the issue of suitability of MT software for an MT student (and instructor). In other words, we believe that the MT learning environment will benefit from a kind of training software that was specially selected or designed for an MT student as the main user. From this perspective MT training software should be treated as any computer application for special purposes and thus should meet quite a number of requirements. A massive collection of papers on humancomputer interaction - see, for example, Dix et al. (1998), Beyer and Holtzblatt (1998), Hackos and Redish (1998) - attempt to bring some structure to the often chaotic application design process. Topics include human limitations, usability principles, interface design, models of the user, task analysis, and practical methods of gathering data about users and tasks. It is stressed that there are no minor issues in the field of interface design and even such common topics as error messages, toolbars, tabbed dialogues, icons and responsiveness should be well thought out. In what follows we first attempt task and user analysis for an MT training tool as a special computer application for a Computational Linguistics student. We then illustrate our considerations with a brief  79  description of an APTrans application (see next paragraph) focusing on its developer environment that might be used as such training tool. At the end we discuss MT training issues that could be covered with the APTrans tool kit. APTrans is an experimental MT system for translating patent claims as described in Sheremetyeva and Nirenburg (1999) that is currently under development. The current version of APTrans is a 32-bit Windows application developed to run in a number of operating environments: Windows 95/98/2000/NT. The facts that APTrans draws on domain knowledge but does not impose a sublanguage approach to MT, and that both the system and developer tools run on a personal computer, offer a wide range of training applications. Desiderata for MT Training Software Task and User analysis. Practically everyone involved in the teaching of MT recognizes what is very clearly formulated in Somers (2001): the teaching of Machine Translation takes different perspectives depending of student profiles, such as Computer Science, Computational Linguistics, Translation or Foreign Language Learning. The first two groups are often put in one category, which may be due to the fact that "historically, MT was probably the first non-numerical use of computers proposed" (Somers, op.cit). From those early days both Computer Science and Computational Linguistics grew into related but well established fields of their own with the focus on different issues. For the Computer Science students the focus is on high level programming skills in the most advanced programming languages, such as C, C+, C++, etc. For the Computational Linguistics students the emphasis is on MT linguistic problems and solutions. That means that if we want to train students in solving MT related linguistic problems it would be reasonable not to augment the difficulty of their tasks with programming matters. As for the content of training that should be covered by an MT training tool (or a tool kit) ideally it should be possible to use it for both  • illustrative purposes to familiarize students with weaknesses of MT software, by presenting them (in a very clear way) the results of every processing step for linguistic error analysis and • "participation" purposes to let a student interfere with MT processing at every possible stage (lexicon and grammar acquisition, disambiguation, etc.) to influence the output of MT so as to immediately see the results of his/her updates. It should also be possible to save traces of students' training activity for further work or classroom discussion. User constraints. An MT training tool should meet general user requirements of making the program as easy as possible to install and to use despite its diverse and complex functionality. We should take into consideration human limitations and desire to automate tedious tasks such as typing, revising texts, propagating changes, etc. It is desirable to visualize the results of every step of the training procedure in the most "human" way. All user-computer communication should be done in the most natural way, i.e. in a natural language. APTrans Overview A prototype of the APTrans application has been described in Sheremetyeva and Nirenburg (1999) so we shall not deal with specifications of the application but rather with the description of the application and its developer environment from the angle of using it as part of MT learning environment. APTrans is an experimental interactive MT system for translating patent claims between any pair of European languages. The model has been initially developed for English and Russian as both SLs and TLs but is readily extensible to other languages, the Danish language is now being added to the system. For better understanding all examples in this paper are in English. 
This article reports on three cases of teaching translation or English as a foreign language using pre-editing tasks with a machine translation system. Trainee translators or English learners were asked to input a Chinese or English paragraph into an MT system, observe the irregula rities in the output, and subsequently edit the source text and input it again in the hope of getting better output from the MT system. Student reports on pre-editing procedures were collected and analysed, as well as thoughts and suggestions about using MT. It is argued that pre-editing MT input can boost student learning in the cognitive and affective domains, apart from training students to use MT systems.  useful engine” in both settings, probably providing a convenient link for the two intellectual activities. Based on the types of input into an MT system, three categories of pre-editing tasks are distinguished in this article. l Pre-editing text written in mother tongue: Input is a Chinese text written by the native speaker. Output is the English text produced by the MT system. Object is to modify the Chinese text until the English translation is in the best condition. l Pre-editing L2 text written by native speaker: Input is an English text written by the native speaker. Output is the Chinese text produced by the MT system. Object is to modify the English text until the Chinese translation is in the best condition.  1. Introduction It has been explained elsewhere by the author (Shei, 2002) that translating into a second language (L2), although not a preferred direction of translation, is nevertheless necessary due to the lack of native speakers in local settings where the service is required. In this context learning an L2 is a subset of learning to translate into the L2. Also, translation is increasingly being reconsidered as a viable means for learning an L2 (Cook, 1998). Thus it can be argued that L2 learning and learning to translate become more and more intertwined. This article explores the use of MT not only in the context of translation teaching but also in the context of L2 learning, showing that MT can be “a  l Pre-editing L2 text written by learner: Input is an English text written by the learner. Output is the Chinese text produced by the MT system. Object is to modify the English text until the Chinese translation is in the best condition. 2. Pre -editing mother language A class session in the General Translation module instructed by the author was devoted to pre-editing MT input. The direction of translation was from Chinese to English. There were two MT systems used (from which the student should choose one) which offered free online translation facility: 1. SYSTRAN at http://babelfish.altavista.com/translate.dyn,  (Babel Fish Translation) 1 and 2. Beijing Golden Bridge Translation Port Network at http://www.netat.net/. Here we will use a simple paragraph to illustrate the pre-editing procedure taught in this class. In (1) we have a randomly chosen Chinese paragraph from a randomly picked Web page: (1) (a) Source text2  Chinese clauses. To rectify this problem, we need to amend the punctuation (i.e. change a Chinese comma to a period where we think an English sentence should be rounded up) and add some subject nouns to (1a). Thus we have the new set of input and output in (2): (2) (a) Source text  (b) Target text in English 14 year old of little surname few heroic women singer grandson swallow posture works as the idol, thinks thin is beautiful, the control diet and the movement reduces weight, reduces from 45 kilograms to 28 kilograms, the outlook turns the skin and bones, has day physical strength does not faint, is delivered the hospital first aid, returned proficiently still continues to reduce weight. As we can see from (1b), the English translation by SYSTRAN based on (1a) has quite a few problems caused by literal translation and mistaken analysis of structure. One of the most conspicuous problems may be the run-on sentence phenomenon and the subject-less sentences which are a peculiar characteristic of 
1. Why teach post-editing? 1.1 Growing Demand The global market for translation was valued at around $13 billion in 2000, and a growth to around $22.7 billion by the end of 2005 has been predicted.1 This increasing demand has led to an increase in the use of translation aids, including terminology management tools, translation memory (TM) and machine translation (MT) technology. At a recent conference on multilingual communication, leading translation companies reported that they are now testing and implementing a hybrid TM-MT technology solution to meet the growing demand for translation.2 
1. Introduction The translation industry has dramatically changed over the last ten years as the gradual process of introducing translation memory (TM) and machine translation (MT) systems in the documentation and translation workflow has been gaining momentum. Also, the burgeoning development of the software localization business in an increasingly globalized world presents a significant challenge to the translation industry by multiplying the number of languages that need to be translated and by expecting shorter turnaround time of translations. Online documentation and web delivery call for a responsive translation industry ready to deliver in a highly competitive environment. In this scenario, the use of MT systems in the industry has spawned two additional fields that require careful  consideration by the professional translator: the use of controlled languages in specific industries (aeronautics, automotive industry, heavy-equipment manufacturers, software companies, etc.) and the post-editing needs of documents translated with MT systems. Future professional translators need to be aware of issues and skills such as what minimum post-editing entails, which standard metrics are starting to be developed to score machine translation output, which controlled languages are used in the industry, how to write documents in controlled language to get better results when using MT systems, etc. They also need to have a thorough exposure to different professional and online MT systems, not only from the user's viewpoint but also from the developer's since only by a good understanding of the complexity of these systems can the future translator judge the quality of the output and foresee what errors will be frequent. 2. Key Elements in Controlled Translation This triad of use of controlled languages, MT systems and post-editing processes coupled with the ever-increasing use of translation memory, CAT tools, terminology management systems, etc. depict a new working environment for the professional translator that we call Controlled Translation. 2.1. Controlled Languages Controlled languages (CLs) have become  107  more prevalent in the industry in recent  years. They are used in technical  documentation systems in large  corporations since they improve the  readability of the documents by imposing  clear and direct writing, they reduce  syntactic and lexical ambiguities by  applying grammatical and lexical  constraints, and they also increase the  translatability of the text, making it  amenable to MT (Mitamura & Nyberg,  1995). The resulting effect is consistency  in the style of the documents, the  reusability of texts, and the corresponding  savings in authoring and translation  processes along with higher customer  satisfaction because of better  documentation and translation.  CLs impose a controlled  vocabulary and a controlled grammar. The  size of the vocabulary and the meanings  per word are restricted to avoid lexical  ambiguities. This is very advantageous  when the translation is performed by MT  systems (Baker et al. 1994). The rules for  controlled grammar are intended to enforce  a clear style where elliptical constructions,  multiple coordinated sentences, syntactic  ambiguity  (prepositional  phrase  attachments, adjectival modifiers,  anaphoric expressions, etc.) are reduced to  a minimum. Usually, checkers are  developed and integrated in the  documentation environment along with  editing software, TM and MT, terminology  management systems and CAT tools.  Mitamura (1999) provides a set of criteria  that must be met in a specific application  domain for a successful deployment of a  CL for MT: the translation should be for  dissemination or outbound translation, the  authors should be highly trained, the CL  should be enforced by a checker, and the  domain should be technical and very  specific.  It is not a coincidence that one of  the first CLs to be developed was AECMA  Simplified English (Farrington, 1996),  which is employed in the aeronautic  industry to tackle the increasing  complexity of the aircraft's technical  documentation. An extension of AECMA  Simplified English is Boeing Technical  English (BTE) which is used at Boeing to  improve the readability and consistency of  the technical documentation (Wojcik, et al.  1998). A checker is used to help authors  write text in BTE and maintain a consistent  style (Wojcik, et al. 1990). The French  aerospace industries association (GIFAS)  has also developed a CL in French ("le  français rationalisé") which allows easy  translation into Simplified English while  improving readability for French speakers  (Barthe, 1998).  Caterpillar  Inc.,  which  manufactures heavy-equipment machinery,  uses Caterpillar Technical English (CTE)  as CL for all English technical  documentation (Kamprath, et al., 1998).  CTE consists of a controlled terminology  inventory (over 70,000 terms) and a  controlled grammar complex enough to  write in a technical domain.  The automotive industry has also  followed the path initiated by the  aeronautic and heavy-equipment industry.  General Motors, for instance, started in  1993 the CASL Project (Controlled  Automotive Service Language) for  technical documentation (Godden, 2000).  CASL is a subset of English that consists  of 62 grammar rules and controlled  terminology. The Swedish manufacturer  Scannia also explored the feasibility of  defining a controlled Swedish for truck  maintenance documentation (Sagvall &  Amqvist, 1996). BMW is considering a  controlled German application developed  by the Institute of Applied Information  Sciences (IAI) from Saarbrücken (Reuther  & Schmidt-Wigger, 2000).  It is expected that the use of CLs  for technical documentation will expand in  years to come beyond the fields described  above to cover other areas such as software  and hardware documentation, software  localization, information technology,  telecommunications, digital technologies,  web content, etc.  108  2.2. Pre-editing Pre-editing technical texts is also part of the working environment of Controlled Translation. Translators need to have the skills to adapt texts to the controlled language guidelines from different industries. These guidelines are employed to achieve both consistency in the authoring of source texts and improvement of the translatability of these texts by making them amenable to MT, as explained above. The guidelines can be often found in the literature on CLs when grammatical and lexical constraints are described. For example, Mitamura (1999) explains that in KANT Controlled English there are lexical constraints (besides the controlled vocabulary that limits lexical ambiguity) such as the following: use of functional words such as determiners should be encouraged, but use of pronouns and conjunctions should be discouraged since they increase the syntactic ambiguity; use of participial forms (-ing and -ed forms) should be restricted when used after conjunctions (* While driving the vehicle...) or in reduced relative clauses (*Directional stability caused by wheel lock-up...). Both examples should therefore be rewritten as follows: While you are driving the vehicle... and The directional stability that is caused by the wheel lock-up. As for grammatical constraints, Mitamura argues that these constraints make sense even when technical texts are not intended for translation, since they improve readability of the source texts and reduce the ambiguity. She distinguishes between phrase-level constraints and sentence-level constraints. In the former category she includes replacing phrasal verbs with single-word verbs (turn on should be rewritten as start) and repeating prepositions in conjoined constructions to avoid ambiguous readings. For instance, this phrase recorded memory of radio and each control unit is ambiguous since it can be construed as {recorded memory of radio} and {each control unit}, or recorded memory {of radio and each  control unit}. Therefore, it needs to be rewritten this way: recorded memory of the radio and of each control unit. As for sentence-level constraints, Mitamura includes coordination of sentences (the two parts of a conjoined sentence should be of the same type), relative clauses (they should always be introduced by a relative pronoun), and elliptical constructions, that should be avoided altogether. Wojcik et al (1998) provide a list of writing rules that are used in Boeing Technical English (BTE). These rules specify, among other aspects, that : (a) determiners such as the a, an, this, these, etc. should be used when appropriate; (b) passive sentences should be avoided in descriptive writing, (c) sentence length should be limited to 25 words, (d) noun clusters with more than three words should be avoided, (e) there should not be more than two adjectives modifying a noun or a noun cluster, (f) -ing forms should be carefully avoided, (g) the word that should not be omitted after verbs, (h) relative pronouns should be used to introduce relative clauses; (i) and parallel constructions are encouraged in coordinated structures. There are also explicit guidelines provided by MT developers for writing content that need to be translated using MT. Notice that some of these guidelines overlap with the controlled language guidelines from KANT Controlled English and BTE. For instance, IBM1 provides these recommendations: use of simple, short sentences (not more than 20 words), avoidance of idiomatic and slang expressions, avoidance of ambiguous words, repetition of nouns and noun phrases instead of using pronouns when possible, use of proper punctuation, use of complete sentences in lists, etc. Therefore, pre-editing texts following the constraints from the controlled language specifications or even writing texts directly in CL become a new set of skills that translators 
Real (i.e., working) machine translation may be presented both as the result of inevitable approximations over an ideal, theoretically motivated model based on the principle of semantic compositionality and as the result of a set of necessary refinements over a very rudimentary word-for-word substitutional system. This paper explores the pedagogical value of presenting real MT as being somewhere in the middle of these two extreme scenarios. I contend that it is possible to reshape the (either optimistic or pessimistic) expectations of students about real MT by showing students, on the one hand, the approximations, compromises, and sacrifices necessary to mechanize efficiently a linguistically accurate model, and, on the other hand, the large amount of work needed to improve a word-for-word model so that it produces reasonable translations. This prepares them to learn and appreciate the strategies used to tackle the problem of machine translation. 
This paper discusses the role that can be played in teaching machine translation (MT) by free on-line MT services, emphasising the advantages that they offer for didactic purposes as well as their drawbacks in comparison with traditional PC-based MT software. Some of the key reasons behind the huge potential for the successful integration of on-line systems into MT teaching activities are explored. The paper argues that exposing students to Internet-based MT can raise their awareness on crucial issues in today’s multilingual communication processes, which are relevant inter alia to web localisation and multilingual on-line content management. 
 machine translation (MT) systems. However,  This paper presents the experiment that was led at  companies usually do not have qualified staff  the University Paris 7 to teach machine translation to translation students. Linking machine translation with translation-training in order to help students acquire the linguistic and technical skills increasingly required on the translation market has become part of a number of translation syllabuses in the last few years. Although, MT knowledge is more and more required for professional translators, most of them are still skeptical, not to say hostile to MT.  which can use or enhance MT systems. Some international organisations, such as the European Community for example, have been using MT for years in their translation tasks. Therefore, linking MT with translation training in order to help students acquire the linguistic and technical skills increasingly required on the translation market has become part of a number  Our objective here is then twofold: showing  of translation syllabuses in the last few years, as  translation trainees why and how MT can be used for professional purposes, from the text to be translated to the final result.  shown for example on the LISA Education Initiative Taskforce website.1 Although, MT knowledge is more and more required for  1. Introduction Nowadays, new needs are arising from the ongoing globalization process. The impact of globalization has been to augment crosscultural communication, hence the need for intercultural language resource management,  professional translators, most of them are still skeptical, not to say hostile to MT. Our objective here is then twofold: showing translation trainees why and how MT can be used for professional purposes, from the text to be translated to the final result.  which is part of content management. Intercultural language resource management is the creation, processing, and management of multilingual and multicultural language resources. Therefore, new skills and competences are required from language professionals on the labour market, especially ICT, technology, and language resource management competences, as identified in the three following surveys: the EU-sponsored SPICE-PREP II report on content localisation  This paper presents the two teaching scenarios that attempted to achieve those objectives and the results obtained. Section 2 deals with the academic context in which the experience was led. In section 3, the projects, data and tools used will be described. Section 4 explains how the projects were led, describing in details the different stages necessary to achieve a translation task, using MT, which will lead to the conclusion.  (updated 2001), the LETRAC survey on the  2. Academic Context  skill and competences needed by industry for translators, and the translators section of the LEIT Industry Needs Survey. Although the LETRAC project was criticised for its emphasis on technology, various authors have identified  The experiment that was led involved post- graduate students following a one-year training program, which is called DESS ILTS2 and divided into two options:  such a need more recently, and, what seems to  be more and more essential, the necessity for “constant adjustments” (Torres del Rey 2000) to constantly developing technology and new business processes. Localization companies,  
This paper presents a generalized description of the characteristics and implications of two processes that enable Fluent Machines{'} machine translation system, called EliMT (a term coined by Dr. Jamie Carbonell after the system{'}s inventor, Eli Abir). These two processes are (1) an automated cross-language database builder and (2) an n-gram connector.
LogoMedia Corporation offers a new multilingual machine translation system {--} LogoMedia Translate {--} based upon smaller applications, called {``}applets{''}, designed to perform a small group of related tasks and to provide services to other applets. Working together, applets provide comprehensive solutions that are more effective, easier to implement, and less costly to maintain. Version 2, released in 2002, provides a single set of cooperating user interfaces and translation engines from 6 vendors for English to and from Chinese (Simplified and Traditional) Japanese, Korean, French, Italian, German, Spanish, Portuguese, Russian, Polish, and Ukrainian.
Any-Language Communications has developed a novel semantics-oriented pre-market prototype system, based on the Theory of Universal Grammar, that uses the innate relationships of the words in a sensible sentence (the natural intelligence) to determine the true contextual meaning of all the words. The system is built on a class/category structure of language concepts and includes a weighted inheritance system, a number language word conversion, and a tailored genetic algorithm to select the best of the possible word meanings. By incorporating all of the language information within the dictionaries, the same semantic processing code is used to interpret any language. This approach is suitable for machine translation (MT), sophisticated text mining, and artificial intelligence applications. An MT system has been tested with English, French, German, Hindi, and Russian. Sentences for each of those languages have been successfully interpreted and proper translations generated.
Pre-market prototype - to be available commercially in the second or third quarter of 2003.
This paper presents a description of the well-known family of machine translation systems, PARS. PARS was developed in the USSR as long ago as in 1989, and, since then, it has passed a difficult way from a mainframe-based, somewhat bulky system to a modern PC-oriented product. At the same time, we understand but well that, as any machine translation software, PARS is not artificial intelligence, and it is only capable of generating what is called {``}draft translation{''}. It is certainly useful, but can by no means be considered a kind of substitution for a human translator whenever high-quality translation is required.
MSR-MT is an advanced research MT prototype that combines rule-based and statistical techniques with example-based transfer. This hybrid, large-scale system is capable of learning all its knowledge of lexical and phrasal translations directly from data. MSR-MT has undergone rigorous evaluation showing that, trained on a corpus of technical data similar to the test corpus, its output surpasses the quality of best-of-breed commercial MT systems.
NESPOLE! is a speech-to-speech machine translation research system designed to provide fully functional speech-to-speech capabilities within real-world settings of common users involved in e-commerce applications. The project is funded jointly by the European Commission and the US NSF. The NESPOLE! system uses a client-server architecture to allow a common user, who is browsing web-pages on the internet, to connect seamlessly in real-time to an agent of the service provider, using a video-conferencing channel and with speech-to-speech translation services mediating the conversation. Shared web pages and annotated images supported via a Whiteboard application are available to enhance the communication.
We will present the KANTOO machine translation environment, a set of software servers and tools for multilingual document production. KANTOO includes modules for source language analysis, target language generation, source terminology management, target terminology management, and knowledge source development (see Figure 1).
The paper discusses a number of important issues in speech-to-speech translation, including the key issue of level of integration of all components of such systems, based on our experience in the field since 1990. Section 1 discusses dimensions of the spoken translation problem, while current and near term approaches to spoken translation are treated in Sections 2 and 3. Section 2 describes our current expectation-based, speaker-independent, two-way translation systems, and Section 3 presents the advanced translation engine under development for handling spontaneous dialogs.
This paper describes the process of implementing a machine translation system (MT system) and the problems and pitfalls encountered within this process at CLS Corporate Language Services AG, a language solutions provider for the Swiss financial services industry, in particular UBS AG and Zurich Financial Services. The implementation was based on the perceived requirements of large organizations, which is why the focus was more on practical rather than academic aspects. The paper can be roughly divided into three parts: (1) definition of the implementation process, co-ordination and execution, (2) implementation plan and customer/user management, (3) monitoring of the MT system and related maintenance after going live.
Most large companies are very good at {``}getting the message out{''} {--}publishing reams of announcements and documentation to their employees and customers. More challenging by far is {``}getting the message in{''} {--} ensuring that these messages are read, understood, and acted upon by the recipients. This paper describes NCR Corporation{'}s experience with the selection and implementation of a machine translation (MT) system in the Global Learning division of Human Resources. The author summarizes NCR{`}s vision for the use of MT, the competitive {``}fly-off{''} evaluation process he conducted in the spring of 2000, the current MT production environment, and the reactions of the MT users. Although the vision is not yet fulfilled, progress is being made. The author describes NCR{'}s plans to extend its current MT architecture to provide real-time translation of web pages and other intranet resources.
For over ten years, Ford Vehicle Operations has utilized an Artificial Intelligence (AI) system to assist in the creation and maintenance of process build instructions for our vehicle assembly plants. This system, known as the Direct Labor Management System, utilizes a restricted subset of English called Standard Language as a tool for the writing of process build instructions for the North American plants. The expansion of DLMS beyond North America as part of the Global Study Process Allocation System (GSPAS) required us to develop a method to translate these build instructions from English to other languages. This Machine Translation process, developed in conjunction with SYSTRAN, has allowed us to develop a system to automatically translate vehicle assembly build instructions for our plants in Europe and South America.
Machine Translation of minority languages presents unique challenges, including the paucity of bilingual training data and the unavailability of linguistically-trained speakers. This paper focuses on a machine learning approach to transfer-based MT, where data in the form of translations and lexical alignments are elicited from bilingual speakers, and a seeded version-space learning algorithm formulates and refines transfer rules. A rule-generalization lattice is defined based on LFG-style f-structures, permitting generalization operators in the search for the most general rules consistent with the elicited data. The paper presents these methods and illustrates examples.
In this paper we present a model for the future use of Machine Translation (MT) and Computer Assisted Translation. In order to accommodate the future needs in middle value translations, we discuss a number of MT techniques and architectures. We anticipate a hybrid environment that integrates data- and rule-driven approaches where translations will be routed through the available translation options and consumers will receive accurate information on the quality, pricing and time implications of their translation choice.
We present a new approach to the problem of aligning English and Chinese sentences in a bilingual corpus based on adaptive learning. While using length information alone produces surprisingly good results for aligning bilingual French and English sentences with success rates well over 95{\%}, it does not fair as well for the alignment of English and Chinese sentences. The crux of the problem lies in greater variability of lengths and match types of the matched sentences. We propose to cope with such variability via a two-pass scheme under which model parameters can be learned from the data at hand. Experiments show that under the approach bilingual English-Chinese texts can be aligned effectively across diverse domains, genres and translation directions with accuracy rates approaching 99{\%}.
The frequent occurrence of divergenceS{---}structural differences between languages{---}presents a great challenge for statistical word-level alignment. In this paper, we introduce DUSTer, a method for systematically identifying common divergence types and transforming an English sentence structure to bear a closer resemblance to that of another language. Our ultimate goal is to enable more accurate alignment and projection of dependency trees in another language without requiring any training on dependency-tree data in that language. We present an empirical analysis comparing the complexities of performing word-level alignments with and without divergence handling. Our results suggest that our approach facilitates word-level alignment, particularly for sentence pairs containing divergences.
Text prediction is a form of interactive machine translation that is well suited to skilled translators. In recent work it has been shown that simple statistical translation models can be applied within a usermodeling framework to improve translator productivity by over 10{\%} in simulated results. For the sake of efficiency in making real-time predictions, these models ignore the alignment relation between source and target texts. In this paper we introduce a new model that captures fuzzy alignments in a very simple way, and show that it gives modest improvements in predictive performance without significantly increasing the time required to generate predictions.
Maximum entropy (ME) models have been successfully applied to many natural language problems. In this paper, we show how to integrate ME models efficiently within a maximum likelihood training scheme of statistical machine translation models. Specifically, we define a set of context-dependent ME lexicon models and we present how to perform an efficient training of these ME models within the conventional expectation-maximization (EM) training of statistical translation models. Experimental results are also given in order to demonstrate how these ME models improve the results obtained with the traditional translation models. The results are presented by means of alignment quality comparing the resulting alignments with manually annotated reference alignments.
In the IBM LMT Machine Translation (MT) system, a built-in strategy provides lexical coverage of a particular subset of words that are not listed in its bilingual lexicons. The recognition and coding of these words and their transfer generation is based on a set of derivational morphological rules. A new utility extends unfound words of this type in an LMT-compatible format in an auxiliary bilingual lexical file to be subsequently merged into the core lexicons. What characterizes this approach is the use of morphological, semantic, and syntactic features for both analysis and transfer. The auxiliary lexical file (ALF) has to be revised before a merge into the core lexicons. This utility integrates a linguistics-based analysis and transfer rules with a corpus-based method of verifying or falsifying linguistic hypotheses against extensive document translation, which in addition yields statistics on frequencies of occurrence as well as local context.
One of the limitations of translation memory systems is that the smallest translation units currently accessible are aligned sentential pairs. We propose an example-based machine translation system which uses a {`}phrasal lexicon{'} in addition to the aligned sentences in its database. These phrases are extracted from the Penn Treebank using the Marker Hypothesis as a constraint on segmentation. They are then translated by three on-line machine translation (MT) systems, and a number of linguistic resources are automatically constructed which are used in the translation of new input. We perform two experiments on testsets of sentences and noun phrases to demonstrate the effectiveness of our system. In so doing, we obtain insights into the strengths and weaknesses of the selected on-line MT systems. Finally, like many example-based machine translation systems, our approach also suffers from the problem of {`}boundary friction{'}. Where the quality of resulting translations is compromised as a result, we use a novel, post hoc validation procedure via the World Wide Web to correct imperfect translations prior to their being output to the user.
This paper describes a novel approach to handling translation divergences in a Generation-Heavy Hybrid Machine Translation (GHMT) system. The translation divergence problem is usually reserved for Transfer and Interlingual MT because it requires a large combination of complex lexical and structural mappings. A major requirement of these approaches is the accessibility of large amounts of explicit symmetric knowledge for both source and target languages. This limitation renders Transfer and Interlingual approaches ineffective in the face of structurally-divergent language pairs with asymmetric resources. GHMT addresses the more common form of this problem, source-poor/targetrich, by fully exploiting symbolic and statistical target-language resources. This non-interlingual non-transfer approach is accomplished by using target-language lexical semantics, categorial variations and subcategorization frames to overgenerate multiple lexico-structural variations from a target-glossed syntactic dependency of the source-language sentence. The symbolic overgeneration, which accounts for different possible translation divergences, is constrained by a statistical target-language model.
This paper describes our ongoing project {``}Korean-Chinese Machine Translation System{''}. The main knowledge of our system is verb patterns. Each verb can have several meanings and each meaning of a verb is represented by a verb pattern. A verb pattern consists of a source language pattern part for the analysis and the corresponding target language pattern part for the generation. Each pattern part, according to the degree of generality, contains lexical or semantic information for the arguments or adjuncts of each verb meaning. In this approach, accurate analysis can directly lead to natural and correct generation. Furthermore as the transfer mainly depends upon verb patterns, the translation rate is expected to go higher, as the size of verb pattern grows larger.
Despite the exciting work accomplished over the past decade in the field of Statistical Machine Translation (SMT), we are still far from the point of being able to say that machine translation fully meets the needs of real-life users. In a previous study [6], we have shown how a SMT engine could benefit from terminological resources, especially when translating texts very different from those used to train the system. In the present paper, we discuss the opening of SMT to examples automatically extracted from a Translation Memory (TM). We report results on a fair-sized translation task using the database of a commercial bilingual concordancer.
We present a classification approach to building a English-Korean machine translation (MT) system. We attempt to build a word-based MT system from scratch using a set of parallel documents, online dictionary queries, and monolingual documents on the web. In our approach, MT problem is decomposed into two sub-problems {---} word selection problem and word ordering problem of the selected words. In this paper, we will focus on the word selection problem and discuss some preliminary results.
One of the problems facing translation systems that automatically extract transfer mappings (rules or examples) from bilingual corpora is the trade-off between contextual specificity and general applicability of the mappings, which typically results in conflicting mappings without distinguishing context. We present a machine-learning approach to choosing between such mappings, using classifiers that, in effect, selectively expand the context for these mappings using features available in a linguistic representation of the source language input. We show that using these classifiers in our machine translation system significantly improves the quality of the translated output. Additionally, the set of distinguishing features selected by the classifiers provides insight into the relative importance of the various linguistic features in choosing the correct contextual translation.
We present a new method for aligning sentences with their translations in a parallel bilingual corpus. Previous approaches have generally been based either on sentence length or word correspondences. Sentence-length-based methods are relatively fast and fairly accurate. Word-correspondence-based methods are generally more accurate but much slower, and usually depend on cognates or a bilingual lexicon. Our method adapts and combines these approaches, achieving high accuracy at a modest computational cost, and requiring no knowledge of the languages or the corpus beyond division into words and sentences.
This paper describes the results of a feasibility study which focused on deriving semantic networks from descriptive texts using controlled language. The KANT system [3,6] was used to analyze input paragraphs, producing sentence-level interlingua representations. The interlinguas were merged to construct a paragraph-level representation, which was used to create a semantic network in Conceptual Graph (CG) [1] format. The interlinguas are also translated (using the KANTOO generator) into OWL statements for entry into the Ontology Works electrical power factbase [9]. The system was extended to allow simple querying in natural language.
The existence of a phrase in a large monolingual corpus is very useful information, and so is its frequency. We introduce an alternative approach to automatic translation of phrases/sentences that operationalizes this observation. We use a statistical machine translation system to produce alternative translations and a large monolingual corpus to (re)rank these translations. Our results show that this combination yields better translations, especially when translating out-of-domain phrases/sentences. Our approach can be also used to automatically construct parallel corpora from monolingual resources.
For the purpose of overcoming resource scarcity bottleneck in corpus-based translation knowledge acquisition research, this paper takes an approach of semi-automatically acquiring domain specific translation knowledge from the collection of bilingual news articles on WWW news sites. This paper presents results of applying standard co-occurrence frequency based techniques of estimating bilingual term correspondences from parallel corpora to relevant article pairs automatically collected from WWW news sites. The experimental evaluation results are very encouraging and it is proved that many useful bilingual term correspondences can be efficiently discovered with little human intervention from relevant article pairs on WWW news sites.
The cumulative effort over the past few decades that have gone into developing linguistic resources for tasks ranging from machine readable dictionaries to translation systems is enormous. Such effort is prohibitively expensive for languages outside the (largely) European family. The possibility of building such resources automatically by accessing electronic corpora of such languages are therefore of great interest to those involved in studying these {`}new{'} - {`}lesser known{'} languages. The main stumbling block to applying these data driven techniques directly is that most of them require large corpora rarely available for such {`}new{'} languages. This paper describes an attempt at setting up a bootstrapping agenda to exploit the scarce corpus resources that may be available at the outset to a researcher concerned with such languages. In particular it reports on results of an experiment to use state-of-the-art data-driven techniques for building linguistic resources for Sinhala - a non-European language with virtually no electronic resources.
