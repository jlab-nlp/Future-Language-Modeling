Keywords: information retrieval, speech recognition, error correction, wisdom of crowds, Web 2.0 
Our naturalistic data show that the word order of two-year-old Chinese children reflects adherence to canonical mapping of thematic roles to structural positions, as well as sensitivity to the unselectivity of subject and object. While sentences of OSV order appeared around two years of age, double nominative structures were virtually absent before three, suggesting that as a typological characteristic, topic-prominence is not acquired early. Our experimental results show that Mandarin-speaking children by three years of age have established SVO solidly as the dominant word order, on both comprehension and production, but still find the topicalized and fronting orders (OSV, SOV) difficult, indicating that the structures of the left periphery may be acquired at a later stage, and at different times. The implications of these acquisition findings for the topic prominence parameter will be explored. 
phenomena. In Esther Goody (ed.) Questions and politeness: strategies in social interaction, 56-289. Cambridge: Cambridge University Press. Condon, William S. (1980) The relation of interactional synchrony to cognitive and emotional process. In Key, M. Ritchie. (ed.) The relationship of verbal and nonverbal communication, 51-56. The Hague: Mouton Publishers. Duranti, Alessandro, and Charles Goodwin. (eds.) (1992) Rethinking Context: Language as an Interactive Phenomenon. Cambridge: Cambridge University Press. Eelen, Gino. (2001) A critique of politeness theories. Manchester, UK: St. Jerome Publishing. 
 Abstract. Language and thought shape each other, or at least give influence each other. Japanese viewpoint of the world is different from that of western countries, and it is reflected in linguistic structures. Japanese uses insects' view in contrast to Western birds' view. Reflecting this, Japanese language, and therefore thought process, suits use of situated expressions. This talk elaborates on the point and tries to formalize dialog processes in a situated manner.  Keywords: language and thought, endo-system and exo-system views, situatedness, case grammar, Japanese language  
Abstract. Communicating agents are commonly thought of as intentionally addressing messages to other agents. A growing body of research exists on the interactive case: natural language dialogue. A somewhat different case, also important in many real life social and work settings, is a person overhearing or intentionally listening in on dialogue among a group of other people. Comparatively little research so far illuminates how, for example, a minute taker for a meeting can comprehend a discussion well enough to accurately record decisions, action items, and other such meeting outcomes, including ones that concern technical matters he does not understand. What prevents the small misunderstandings that frequently creep into discussions, even between active participants, from growing into a gross misunderstanding by the minute taker of the discussion to which he is listening? This talk will present some similarities and differences between participating in a conversation and listening in on one, with emphasis on how overhearers who lack opportunities to contribute to a discussion target their interpretive efforts in productive ways. Progress in creating artificial agents capable of similar listening feats will be surveyed and research directions assessed. Keywords: dialogue, multiple speakers, overhearing, meetings. References 
Abstract. Syntax-based statistical translation model is proved to be better than phrasebased model, especially for language pairs with very different syntax structures, such as Chinese and English. In this talk I will introduce a serial of statistical translation models based on source syntax structure. The tree-based model uses the one best syntax tree for translation. The forest-based model uses a compact forest which encodes exponential number of syntax trees in a polynomial spaces and lead to better performance. The joint parsing and translation model produces source parse trees, using the source side of the translation rules instead of separate parsing rules, and generate translations on the target side simultaneously, which outperforms the forest-based model. Some extensions of these models are introduced also. 
{baixiaopeng, chswh}@nus.edu.sg Abstract. Either word sense classification or word sense clustering can not avoid this problem: what features people can get from word meaning that can be used as the tools employed for producing word sense ontology. In this paper, we present a lexical semantic-syntactic model to define and subcategorize a noun class—attribute noun class, by acquiring syntactic behavior and semantic restrictions from corpus. First we introduce the idea of attribute noun class. Then we describe the model, which is a specific tool providing a practical method to decide whether a word should be located in a specific word sense class or not. Then we apply the model to 15 nouns to show how the model works, and create a sample taxonomy for the 15 nouns. Keywords: attribute nouns, word sense classification, syntagmatic features 1. Attribute Noun and Attribute Noun Class Concept (refers to either entity or abstract thing) has several characteristics viewed from different aspects. All things have properties: physical, social, etc. see the expressions below: a1 The color of the hat is red. a2 这人脾气暴躁。(The guy’s temper is irritable.) “color” in a1 is a physical property of hat. “脾气(temper)” in a2 is a psychological property of people. Attribute noun class (as ANC below) refers to such characteristics/properties of things (like “color” of hat and “脾气(temper)” of “人(man)” above). The noun is called attribute noun if it can be classified into ANC. The “attribute” presents the incomplete information—it is supposed to combine with other words to make an integrated expression, in other words, the attribute noun “looks forward to” collocating with words which can specify its meaning. In a1, “red” specify the meaning of attribute noun “color”; in a2, “暴躁(irritable)” specify the meaning of attribute noun “脾气(temper)” 2. The syntactic and semantic features of attribute noun Syntagmatic mode of word has two sub-modes: syntactic behavior and semantic selectional restrictions. While the lexical meaning of word impacts the syntagmatic mode.  66 Regular Papers The meaning of a word might impact the syntactic behavior of the word. For example, attributive adjectives (also called non-predicative adjectives or as qu bie ci known in Chinese) in Chinese like “雪 白(snowy white)” can not be syntactically used with adverb, so “非常(very)/adv 雪白(snowy white)/adj” is not a legal phrase in Chinese. Because there is a adverbial morpheme “雪(snowy)” in word “雪白(snowy white)” which means “snowy”, the word merges the meaning of adverb into its lexical meaning. Attribute noun can not be attributive in attributive-head relation and rarely be isolate subject in subject-predicate relation which we will show in the following section. The semantic selectional restrictions are semantic frame in which words should appear. The semantic frame is the grammatical relation with semantic constrains. For example, there are two grammatical slots in VO relation: verbal and objective. The meaning of object must semantically match with the meaning of the verb and vice versa, in other word, either verb or object has semantic “preferences” to the collocate words; such “preferences” is “selectional restrictions” in lexical semantics. The selectional restrictions also exist in semantic role frame, but we do not discuss the issue in this paper. 2.1 Syntactic behavior of ANC ANC is one of the classes making the noun ontology. There have to be some formal features to distinguish ANC with other noun classes—we have to tell how ANC different with other noun classes. The syntactic behavior, as we discussed before, is the grammatical relations in which attribute noun could appear. Generally, noun is grammatically united other words with following syntactic relations: a. Subject-predicate (SP) relation: 今天 星期一。(Today is Monday.) Noun can be either subject or predicate in Chinese. b. Verb-object (VO) relation: 今天 下午 打 篮球。(Play basketball this afternoon.) Noun can be object. c. Attributive-head (AH) relation: 皇帝 的 新装。(The king’s new dress.) Noun can be either attributive or head. Chinese is SVO language which is word-order-dependent, so we might acquire the syntactic behavior by studying the grammatical distribution of attribute noun in these grammatical relations. The meaning of attribute noun presents incomplete information—it is supposed to combine with other words to make an integrated expression, in other words, the attribute noun “looks forward to” collocating with other words which can specify its meaning. According to this, attribute noun can not be isolate subject in SP. In sentence “今天 星期一(today is Monday)”, the noun “今天” is an isolate subject in the sentence, it can be subject by itself without combining with other word. Because subject should be definite in Chinese and present integrated information, so attribute noun can not be isolate subject in a sentence. For example, a3 颜色 是 绿色 的 (color is green) the meaning of a3 presents incomplete information. It is supposed to tell the color of what thing or who so that people can understand the expression. Thus the attribute noun “颜色(color)” can not be isolate subject in sentence like noun “今天(today)”. With the same reason, in AH relation, the attribute noun can not be the isolate attributive modifying head. Because the meaning of attribute noun needs to be specified by other words rather than specifying, thus it can be head modified by the attributive in AH. Thus, if we put a word before a3: a4 衣服 的 颜色 是 绿色 的 (color of the clothes is green) the meaning of a4 is complete, we have a AH, in which attribute noun is head, to make a4 understandable.  PACLIC 24 Proceedings 67 Therefore, we have the syntactic behavior of ANC by studying its lexical meaning. F1. The attribute noun can not be the isolate subject in SP. F2. The attribute noun can not be the attributive in AH. We will use F1 and F2 to determine whether a noun is an attribute noun or not in the section below. Worth to mention that: 1) the rules of F1 and F2 is not black or white rule, the syntactic features occur with very big probability; 2) when we test whether a noun is an attribute one, the F1 and F2 should be applied in sentence level, i.e. if a noun is an isolate subject or attributive, the SP or AH must be semantically understandable as a sentence. 2.2 Semantic features of ANC The ANC is a big word sense class and its meaning is general concept, thus we need to divide ANC into small sense classes in terms of more specific meaning. We use the semantic features to subcategorize ANC which are the semantic constrains between attribute noun and the collocate words in SP, VO and AH. The selectional restrictions can be learned by studying the meaning of attribute noun and it’s collocate words in AH, SP and VO. 2.2.1 Attribute Noun Used as Subject in SP Predicate tells something about the subject. In SP, predicate is the value of attribute noun. Attribute nouns with different meanings calls for predicates that with different concept. Thus, we can assign an attribute noun to more specific ANC class in terms of the meanings of predicate collocated with. 2.2.2 Attribute noun as object in VO Attribute noun is object in VO, which is one of the arguments of verb. Object semantically matches with verb as other types of arguments. For example, for the attribute noun “外貌”, it can collocate with verb with the meaning of “watch” like “看” in VO (看/v 他 的 外貌/o, watch his appearance); while for the attribute noun “性格”, the verb should be with the meaning of “mental action” like “了解” (了 解/v 他 性格/o, know his temperament). Different attribute nouns “tend to” collocate with different verbs. 2.2.2 Attribute noun as head in AH The function of attributive in AH is specifying the head. There are two meanings of attribute: 1) the value of attribute (昂贵/a 的 价格/h, high price) which semantically resembles the predicate in 2.2.1; 2) the host which semantically possess the attribute (书/a 的 价格/h, price of the book). Thus the meanings of host of attribute and attribute, as well as attribute value and attribute should match each other. The attribute nouns can be assigned to more specific ANC classes in terms of the meanings of words with which the attribute nouns collocate in SP, VO and AH. Thus the selectional restrictions in three grammatical relations can be used as subcategorization criterion for ANC. 3. The lexical semantic-syntactic model for defining and subcategorizing attribute noun Now we deduce a lexical semantic-syntactic model (LSS model) from above analysis for defining and subcategorizing attribute noun. We use pseudo-code of C to describe the model below:  68 Regular Papers Define AN { syn_behvior; sr_SP; sr_VO; sr_AH; } AttrDef(AN noun) { if(Be_Attr(noun, AH)==false||Be_IsoSub(noun, SP)==false) Subcat_AN(noun); else printf(“the noun is not an attribute noun”); } Subcat_AN(noun) { while(sentence[i]) features=get_syn_features(noun, sentence[i]); noun=SR(noun, features, AH) store the result in AN; noun=SR(noun, features, VO); store the result in AN; noun=SR(noun, features, SP); store the result in AN; } end. The attribute noun is presented in a structure AN, in which the syntactic behavior and selectional restrictions are stored. The function AttrDef decides whether a word is an attribute noun or not, using the syntactic features F1 and F2. Then the word is input in Subcat_AN which assigns the word to a more specific sense class. In Subcat_AN, a noun is semantically analyzed in SR function with 3 syntactic relations: AH, VO and SP. The result of syntactic and semantic analysis will be presented in structure for each input. 4. Experiment In this section, we make an experiment to evaluate the applicability of LSS model. 4.1 Methods First we choose 15 nouns from Singapore Chinese Textbook Corpus. Then we abstract sentences from the first page result that CCL online corpus generated for each noun and acquire the data of syntactic features. The syntactic features are used to determine whether a noun is attribute noun or not. Second, we choose 15 attribute nouns from the same corpus and generate sentences for each attribute noun from CCL online corpus, for abstracting semantic features. The semantic features will be put in structure AN mentioned in section 3 to show how 15 nouns differ in  PACLIC 24 Proceedings 69  semantic selectional restrictions. Such semantic features help us to divide 15 attribute nouns into several subcategories. The features, either of syntactic or semantic, are manually abstracted from sample sentences. We do not employ machine learning software (like Stanford Parser, SRL software) to do the job, because the purpose of this paper is to create a model for attribute noun and evaluate rather than evaluate machine learning algorithm. We want the data to verify the model. In additional, the data set is quite small (15 words). 4.2 Syntactic behavior to determine whether a noun is attribute noun The 15 nouns are: 老 百 姓 (people), 华 裔 (foreign citizen of Chinese origin), 茶 客 (customer), 矿长(mine manager), 珍珠(pearl), 甘泉(spring), 气浪(airsurge), 声音(sound), 乾坤(the universe), 前臂(forearm), 长度(length), 指标(indicator), 外貌(appearance), 性能 (performance), 直径(diameter). The last five nouns are already tagged as attribute noun. We use 4 indicators to show the grammatical distribution: isolate subject in SP, head in AH, Attributive in AH, object in VO. A word is called isolate subject when it is the subject of a sentence all alone, rather than in a noun phrase.  Table 1  Word  Isolate sub in Head in AH Attributive in AH  Object in VO  SP  老百姓/45 13  9  10  13  华裔/50  0  0  50  0  茶客/50  22  18  4  6  矿长/50  2  23  23  2  珍珠/40  4  14  11  11  甘泉/43  6  18  2  17  气浪/  4  37  
2cjing3@student.cityu.edu.hk Abstract. We propose the use of fine-grained part-of-speech (POS) tags as discriminatory attributes for automatic genre classification and report empirical results from an experiment that indicate substantial accuracy gain by such features over the conventional bag-of-words approach through word unigrams. In particular, this paper reports our research to investigate the performance of a fine-grained tag set when tested with the British component of the International Corpus of English. Ten different genre classification tasks were identified and the performance of the tags was evaluated in terms of F-score. Our results show that the use of linguistically fine-grained POS tags produces superior accuracy when compared with word unigrams, particularly for a rich set of 32 different genres with Naïve Bayes Multinominal Classifier. Through a comparison with an impoverished tag set, our results further demonstrate that the superior performance is due to the rich linguistic information embodied in the 400-strong different POS tags. Keywords: automatic genre classification, ICE-GB, fine-grained POS tag, linguistic granularity, AUTASYS. 
Keywords: discourse particles, sentiment corpora, corpus pragmatics, decision theory, Japanese 
Keywords: working memory, discourse comprehension, Rhetorical Structure Theory, pronoun resolution, Russian. 
Abstract. This paper discusses a computational model of language generation, based on work in Phase Theory, that attempts to shed light on how the human mind generates sentences. This model presents explicit algorithms that a) determine selection and merger of Lexical Items, b) determine the labels of Merged elements, c) account for movement of Lexical Items within a derivation, and d) account for when chunks of a sentence are sent to Spell-Out. We demonstrate how this model accounts for generation of a wh-question in Japanese. Keywords: Phase Theory, computational modeling, wh-questions, Japanese 
Abstract. This paper argues for the existence of a deeper and more primitive structural unit of syntax and morphology than the constituent. Data from ellipsis, idiom formation, predicate complexes, bracketing paradoxes, and multiple auxiliary constructions challenge constituency-based analyses. In chain-based dependency grammar, however, constituents are seen as complete components. Components are units that are continuous both in the linear and in the dominance dimension. A unit continuous in the dominance dimension is called a chain. Evidence suggests that chains constitute the fundamental structural relationship between syntactic and morphological units, and that constituents are just a special subset of chains. If these assumptions are correct, linguistic research may need to change direction. Keywords: bracketing paradox, chain, constituent, ellipsis, idiom 
Laboratory of Mathematics University of Savoie, France. mhuma@univ-savoie.fr  Aarne Ranta Department of CS and Engineering Chalmers University of Technology and University of Gothenburg, Sweden. aarne@chalmers.se  Abstract. We describe an implementation of morphology, development of a corpus and building of a lexicon for Punjabi language. Such resources are building blocks for various language technology tasks ranging from part of speech tagging to machine translation. Their importance is further increased by the fact that Punjabi is an under resourced language. We release these resources as open-source. Keywords: Morphology, Corpus building, Lexicon extraction, Punjabi, Shahmukhi. 
 b  Hayeon Jang and Hyopil Shin  Department of Linguistics, Seoul National University, 599 Gwanak-ro, Gwanak-gu, Seoul, Republic of Korea a hyan05@snu.ac.kr b hpshin@snu.ac.kr  Abstract. In this paper, we propose a new linguistic approach for sentiment analysis of Korean. In order to overcome shortcomings of previous works confined to statistical methods, we make effective use of various linguistic features reflecting the nature of Korean such as contextual intensifiers, contextual shifters, modal affixes, and the morphological dependency chunk structures. Moreover, unlike complex statistical formulae which are hard to understand, we use simple mathematical formulae in the process of term weighting. Through experiments of news corpus, we verify an improvement on the results of sentiment analysis of Korean in comparison to the experimental results using TFIDF as popular statistical method employing word frequency. This approach, especially the chunking method, will be beneficial to sentiment analysis of other morphologically rich languages like Japanese and Turkish. Keywords: Sentiment analysis, Linguistic feature, Morphologically rich language 
a School of English, Kyung Hee University Seoul, 130-701, Korea jongbok@khu.ac.kr b Dept. of English Education, Chosun University Kwangju, 501-759, Korea nglee@chosun.ac.kr c Dept. of English Education, Kyungpook University Daegu, 702-701, Korea yaesheik@knu.ac.kr  Abstract. The ﬂexibility of Korean NP structures has been well-observed, but there have been few attempts to provide precise syntactic structures. This paper ﬁrst reviews the basic distributional properties of Korean prenominal expressions as well as constraints in ordering, and then sketches a constraint-based, lexicalist analysis for Korean NP structures. Arguing for surface-based syntactic structures with more ﬂexible subcategorization requirements, the paper shows that this lexicalist-based analysis can in a simple manner capture the ﬂexible orderings of prenominal expressions as well as generate proper and precise NP structures, without resorting to functional projections. Keywords: NP structure, prenominal, determinant, ordering  
 b  c  Gyu-hyung Lee , Ye-seul Park , and Yong-hun Lee  a Department of English Language and Literature, Hannam University, 133 Ojeong-dong, Daedeok-gu, Daejeon 306-791, Korea gyuhyung73@naver.com b Department of English Language and Literature, Hannam University, 133 Ojeong-dong, Daedeok-gu, Daejeon 306-791, Korea pys218218@gmail.com c Department of English Language and Literature, Chungnam National University, 220 Gung-dong, Yuseng-gu, Daejeon 305-764, Korea yleeuiuc@hanmail.net  Abstract. It has been known that the syllable structures in Korean are different from those in English. The goal of this paper is to provide computational implementations for Korean syllable structures in the typed feature structure formalism. The system that we adopted in this paper is the Linguistic Knowledge Building system. We first implemented the type hierarchies and AVMs for segment and suprasegment. The types consonant and vowel were included under the type segment, and the various different types were included under the type suprasegment for syllable structures. Then, we provided the rules for syllable structures. Unlike English syllabification, it has been known that onset and nucleus form a unit in Korean, which is called core. Accordingly, we provided the rules for onset, nucleus, and coda; then, the rules for core and syllable to combine segments into syllable structures. This paper also employed the type nf to solve the ambiguity problems. Keywords: syllable structure, Korean, typed feature formalism, LKB, implementation 
 Department of English Language and Literature, Sungkyunkwan University, 53 Myoungryun-dong, 3 ga, Jongro-ku, Seoul 110-745, Korea {hanjung, choihj}@skku.edu  Abstract. Acceptability of case ellipsis on focused subjects and objects exhibits clear asymmetry which so far has not received a plausible explanation. Case ellipsis on focused direct objects occurs naturally, whereas case ellipsis on focused transitive subjects is unnatural whether the subject is contrastively focused or not. The main purpose of this paper is to provide experimental evidence that the degree of acceptability of case ellipsis on focused argument NPs in Korean is sensitive to the usage probability of their properties. Our experiment shows that the degree of acceptability of case ellipsis on focused argument NPs in Korean and the strength of the influence of focus types on case ellipsis both correlate with the likelihood for the argument’s referent to be new information. We argue that this finding lends support to the view that language users’ intuitions of acceptability in context are probability-sensitive in that their preferences are affected by the usage probability of properties of argument NPs.  Keywords: case ellipsis, case marking, focus types, predictability, usage probability  
 b  b  Kiyong Lee , Jonathan Webster and Alex Chengyu Fang  a Department of Linguistics, Korea University Anam-dong, Seoul, Korea klee@korea.ac.kr b Department of Chinese, Translation and Linguistics, City University of Hong Kong Tat Chee Avenue, Kowloon, Hong Kong SAR {ctjjw,acfang}@cityu.edu.hk  Abstract. This paper proposes eSpaceML as a representation scheme for annotating eventdriven spatial expressions in natural language. It adopts SpatialML (MITRE, 2009) and ISO-Space (ISO, 2010) as a basis for the development of a novel, distributed spatial annotation scheme. SpatialML focuses on the annotation of spatial locations and their topological relations, while both ISO-Space and eSpaceML attempt to extend the scope beyond the treatment of toponyms. ISO-Space and eSpaceML also link space to events in various ways but with considerable differences. Unlike ISO-Space, which attempts to provide a self-contained framework for the various links, eSpaceML treats them in a distributed manner, operating as a pivotal system that refers to several other established annotation schemes such as MAF (ISO, 2008) and ISO-TimeML (ISO, 2009c) for morphosyntactic as well as temporal-eventual annotations. Keywords: semantic annotation, space and event, ISO, language resource management 
 a  a  b  Hao Li , Xiang Li , Heng Ji , Yuval Marton  a Computer Science Department Queens College and Graduate Center City University of New York New York, NY 11367, USA hengji@cs.qc.cuny.edu b Center of Computational Learning Systems Columbia University, New York, NY 10027, USA ymarton@ccls.columbia.edu  Abstract. Information Extraction (IE) is becoming increasingly useful, but it is a costly task to discover and annotate novel events, event arguments, and event types. We exploit both monolingual texts and bilingual sentence-aligned parallel texts to cluster event triggers and discover novel event types. We then generate event argument annotations semiautomatically, framed as a sentence ranking and semantic role labeling task. Experiments on three different corpora -- ACE, OntoNotes and a collection of scientific literature -- have demonstrated that our domain-independent methods can significantly speed up the entire event discovery and annotation process while maintaining high quality. Keywords: Information Extraction, Novel Event Discovery, Domain-Independent, Semantic Role Labeling 
Abstract. This paper describes efforts to develop an online repository of Indonesian corpora –and its associated functions and services– that has been designed to support a wide variety of use cases and applications. Two design considerations are ensuring sustainability and accessibility of the corpora, and enabling open enrichment through annotation. The presented model supports OLAC-compliant metadata, is built atop an OAIS-compliant core repository, and exposes data and functionality via RESTful web services. A prototype implementation is presented, which allows users to upload, browse, and search the collection, whose extensible content model currently supports POS tagging. The future plan is for language-independent aspects of the system to be packaged up and released as an open-source package to aid the development of corpora repositories for other languages. Keywords: Indonesian, corpora, annotation, metadata, digital repositories. 
a Department of Liberal Arts, 5-1, Wakaba-cho, Kure City, Hiroshima, Japan hiroaki-nakamura@ax2.mopera.ne.jp  Abstract. In this paper we will consider how the choice between the topic marker wa and nominative marker for subjects affects truth conditions of sentences. We derive the proper interpretations for sentences with topics and nominative subjects in terms of syntaxsemantics interface. The categorial framework adopted here allows us to account for significant difference in meaning with respect to the choice of markers for subjects while maintaining the principle of strong compositionality.  Keywords: information structure, focus, topic, exhaustiveness, contrastiveness, syntaxsemantics interface.  
{jlread,ctjlw,acfang}@cityu.edu.hk Abstract. The task of text classiﬁcation is the assignment of labels that describe texts’ characteristics, such as topic, genre or sentiment. Supervised machine learning techniques such as Support Vector Machines or the simple but effective Na¨ıve Bayes have been successfully applied to this task. However, it is not always practical to acquire a sufﬁcient corpus of labelled examples to train these methods. For these cases we describe an unsupervised method for text classiﬁcation based on two hypotheses. Firstly, we propose that the class of a document may be determined by calculating its constituent features’ similarity with prototypical examples of each class. Secondly, we note the importance of class priors in Na¨ıve Bayes classiﬁers, and hypothesize that class distributions might be estimated using the relative frequency of prototype words. Performing experiments on a corpus of biomedical abstracts with topic information derived from the Medical Subject Headings (MeSH), we investigate the characteristics of the method when used in conjunction with basic, linguistic and knowledge-based features, and ﬁnd that the performance of the unsupervised method is approximately 80% that of Na¨ıve Bayes. Our research is signiﬁcant in that it highlights a candidate method with good potential for further improvement when training on unlabelled data. Keywords: Pointwise mutual information, Text classiﬁcation, Unsupervised methods 
Abstract. This paper investigates some important constraints on the licensing of nominals in the so-called Multiple Nominative Constructions (MNCs) in Korean from a mereological point of view, proposing a semantic relation hierarchy. The main idea advanced in this paper is that MNCs are cyclically formed only when the relationship between the two consecutive NPs satisﬁes one of the conceptual constraints including inclusion, possession and attribution. The inclusion constraints are further divided into meronymic relations, spatio-temporal relations and classiﬁcational relations. The meronymic relations integrate some essential ideas of the tradition of mereological thoughts. Some appealing consequences of this proposal include a new comprehensive classiﬁcation of MNCs and a straightforward account of some long standing problems such as how the additional nominative NPs are licensed. Keywords: Multiple Nominative Constructions, Korean, Mereology, Part-Whole 
Abstract. In this paper, we propose a method for anaphora resolution in speech understanding for a livelihood support robot. For robust speech recognition, we combine two types of speech recognizers; a large vocabulary continuous speech recognizer (LVCSR) and domain-speciﬁc speech recognizers (DSSR). One problem in the anaphora resolution is lack of the antecedent in the outputs. To solve the problem, we introduce 2 types of DSSRs; one medium-scale DSSR and several small DSSRs. In this paper, we describe the basic idea of our multiple speech recognizer ﬁrst. The selection process in the recognizer is based on the similarity between the LVCSR and each DSSR. Then, by using the outputs from the LVCSR and the medium-scale DSSR, we resolve anaphoric expressions in the current output from a small-scale DSSR. The experimental result shows the effectiveness of our method. Keywords: Anaphora resolution, Multiple speech recognizer, Combination 
41 Kawauchi, Aoba-ku, Sendai, 980-8576, Japan uehara@intcul.tohoku.ac.jp Abstract. This paper shows that there are some syntactic and semantic discrepancies among three seemingly semantically equivalent verbs denoting one of the most basic actions in any language, i.e. the verbs meaning ‘kill’ in English, Chinese and Thai. Specifically, it examines the possibility of these verbs to appear in two syntactic patterns in which English is used as the metalanguage: (A) X kill Y dead, and (ฺB) X kill Y but Y not die. The different syntactic properties among these verbs suggest that the verbs for ‘kill’ in the three languages are not completely semantically equivalent. It is found that the resulting dead event of kill in English is lexically entailed but that of shā in Chinese is merely implied. Thai is a more complicated case. The verbs for ‘kill’ in the three languages are thus classified into different categories based on their syntactic and semantic properties. Keywords: accomplishment verb, activity verb, entailed-result verb, implied-result verb. ∗ This research was supported by a research grant from the National Research Council of Thailand (No.GRB BSS 31532201) and Grant-in-Aid for Scientiﬁc Research from the Japan Society for the Promotion of Science (No. 20520347). Copyright 2010 by Kingkarn Thepkanjana and Satoshi Uehara  292 Regular Papers 
Abstract. Unsupervised sentiment classiﬁcation usually needs a user deﬁned sentiment dictionary. However, the existing dictionaries in Chinese are insufﬁcient, for example, the intersection rate of two popular Chinese sentiment dictionaries HowNet and NTUSD is less than 10%. In this paper, we present a method to help expand the dictionaries with more sentiment words by ranking them through link analysis based on a word graph constructed from a large unlabeled corpus. Meanwhile, our method could compute a sentiment polarity strength for each word in the new dictionaries. Manual evaluation has shown that our method has a high precision to expand the dictionaries. Experiments for sentiment classiﬁcation have shown that the new dictionaries with the polarity strength for each word given by our algorithm are effective to improve the performance. As a byproduct, our algorithm could also discover the errors existing in current dictionaries. 
Keywords: event structure, complex predicate, Japanese, experiencer-agent alternation. 1. Introduction When children acquire the meaning of a verb, they understand the possible state or event it describes. However, many verbs also come with arguments. Do we know what arguments a verb accepts when we understand the verb’s meaning? It is still not clear how arguments are matched with the right kind of verbs; neither are the possible repercussions from theories about argument structured. Clarification of these issues will require thinking about the meanings of verbs in non-traditional ways. For example, verb denotations might not carry all the information about a verb’s argument structure. This study investigated Japanese complex predicates of the kind “V-te-a-ru”, composed of an intentional active verb2 in te-form3 and an existential verb aru, which is taken to be either a 
Abstract. This paper presents the Fault-Tolerant Learning approach for term extraction. The approach extracts terms using automatically generated seeds instead of prior domain knowledge or annotated corpora. Thus it is applicable to any domain specific corpus and it is especially useful for resource-limited domains. Two classifiers are separately trained for prediction and verification to ensure the performance of the proposed approach. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach. Keywords: Fault-Tolerant Learning, term extraction, machine learning. 
Abstract. In this paper we propose an approach for automatic construction of concept hierarchies from the snippets returned by Internet search engines using a number of well known techniques. We use surface lexical patterns to construct a set of candidate hypernyms of a given term and additional ﬁltering that is based on both lexical patterns and distributional analysis. Preliminary experimental results for real life English examples are presented. Keywords: ontology learning, concept hierarchy, hypernyms, lexical patterns. 
Keywords: Kinship terms, Ontologies, Japanese kinship terms, Chinese kinship terms. 
 a  b  b  Mei-hua Chen , Chung-chi Huang , Shih-ting Huang , and Jason S. Chang  a Institute of Information Systems and Applications, National Tsing Hua University, HsinChu, Taiwan 300, R.O.C. {chen.meihua, u901571}@gmail.com b Department of Computer Science, National Tsing Hua University, HsinChu, Taiwan 300, R.O.C. {koromiko1104, jason.jschang}@gmail.com  Abstract. We introduce a method for learning to find the representative syntax-based context of a given collocation/phrase. In our approach, grammatical patterns are extracted for query terms aimed at accelerating lexicographers’ and language learners’ navigation through the word usage and learning process. The method involves automatically lemmatizing, part-of-speech tagging and shallowly parsing the sentences of a large-sized general corpus, and automatically constructing inverted files for quick search. At run-time, contextual grammar patterns are retrieved and presented to users with their corresponding statistical analyses. We present a prototype system, GRASP (grammar- and syntax-based pattern-finder), that applies the method to computer-assisted language learning. Preliminary results show that the extracted patterns not only resemble phrases in grammar books (e.g., make up one’s mind) but help to assist the process of language learning and sentence composition/translation. Keywords: Computer-assisted language learning, collocation, part-of-speech tagging, grammatical patterns, and inverted files. 
Keywords: Example Based Machine Translation, Proportional Analogy, Named Entity Transliteration 
 Abstract. Thirty four Russian speaking adults were tested for their comprehension of four complex adverbial sentence types expressing the temporal order of events. Three hypotheses put forward in the literature and tested in English state respectively that: (i) adverbial clause placement, (ii) conjunction choice, and (iii) order of mention effect on participants’ adverbial sentences comprehension. The results of the study demonstrated that none of the three hypotheses were relevant for Russian material. A new explanation was proposed. Keywords: psycholinguistics, language comprehension, working memory, complex adverbial sentences, Russian.  
slh@ens-lyon.fr Abstract. This paper describes the rationale and design of an XML-TEI encoded corpora compatible analysis platform for text mining called TXM. The design of this platform is based on a synthesis of the best available algorithms in existing textometry software. It also relies on identifying the most relevant open-source technologies for processing textual resources encoded in XML and Unicode, for efficient full-text search on annotated corpora and for statistical data analysis. The architecture is based on a Java toolbox articulating a full-text search engine component with a statistical computing environment and with an original import environment able to process a large variety of data sources, including XML-TEI, and to apply embedded NLP tools to them. The platform is distributed as an open-source Eclipse project for developers and in the form of two demonstrator applications for end users: a standard application to install on a workstation and an online web application framework. Keywords: xml-tei corpora, search engine, statistical analysis, textometry, open-source. 
 b  c  d  Jia-Fei Hong , Sue-Jin Ker , Chu-Ren Huang and Kathleen Ahrens  a Institute of Linguistics, Academia Sinica, Taiwan, No. 128, Section 2, Academia Road 115, Taipei, Taiwan jiafei@gate.sinica.edu.tw b Department of Computer Science and Information Management, Soochow University, Taiwan, No.56, Section 1, Guiyang Street 100, Taipei, Taiwan ksj@cis.scu.edu.tw c Faculty of Humanities, The Hong Kong Polytechnic University, Hong Kong The Hong Kong Polytechnic University, Hong Hum, Hong Kong churenhuang@gmail.com d Language Centre, Hong Kong Baptist University, Hong Kong Hong Kong Baptist University, Kowloon Tong, Kowloon, Hong Kong kathleenahrens@yahoo.com  Abstract. In this study, we propose to use two corpus-based linguistic approaches for a sense prediction study. We will concentrate on the character similarity clustering approach and concept similarity clustering approach to predict the senses of non-assigned words by using corpora and tools, such as Chinese Gigaword Corpus, and HowNet. In this study, we would then like to evaluate their predictions via the sense divisions of Chinese Wordnet and Xiandai Hanyu Cidian. Using these corpora, we will determine the clusters of our four target words ---chi1 “eat”, wan2 “play”, huan4 “change” and shao1 “burn” in order to predict their all possible senses and evaluate them. This requirement will demonstrate the visibility of the corpus-based approaches. Keywords: Lexical ambiguity, sense prediction, corpus-based approach, character similarity clustering, concept similarity clustering, evaluation. 
 Abstract. This paper takes Chinese five-syllable compounds as examples to re-examine the derivation of Chinese synthetic compounds. With phonological evidences which show that Chinese synthetic compounds are left stressed, the paper proves that Chinese synthetic compounds are derived in the interface of lexicon and syntax, based on argument structures of the root verbs. The paper assumes that Chinese synthetic compounds should have the same derivation process, although some seem to have different argument structures from others.  Keywords: synthetic compounds, main stress, argument structure, light verb, movement  
 b  Darren Hsin-hung Lin and Shelley Ching-yu Hsieh  Department of Foreign Languages and Literature, National Cheng Kung University,  No. 1, University Road, Tainan City 701, Taiwan, R.O.C.  a  b  g364375@ms49.hinet.net , shelley@mail.ncku.edu.tw  Abstract. This paper presents an analysis of the language of patents, as a contribution to the field of English for Specific Purposes (ESP). While there work appears to fill a niche in the ESP field (and particularly in the English for Occupational Legal Purposes), the present study insists that statistical approach is necessary for compiling patent technical word list for ESP. Since research studies on word associations of patent lexis have been relatively scarce, this paper reports the technique to select appropriate words from high frequency words is required for modern patent language. The research content and statistical investigations building up a patent technical word list which helps learners of modern patent language expand the vocabulary size for a better understanding of patent writing. Keywords: English for Specific Purposes, intellectual property rights, corpus linguistics, lexical semantics, patent 
Abstract. This paper presents and analyzes the KOLON System, created to facilitate Korean Natural Language Processing (KNLP) and to improve experimental results through the KOLON Ontology. It is currently under development at our Computational Linguistics Laboratory, and is based on previous works, namely the Mikrokosmos Ontology and 21st Century Sejong Project. The KOLON System is also extended with software tools to simplify the handling and visualization of the data, as well as the creation of new programs. The mapping of words onto ontological concepts was performed automatically, with faulty information being corrected manually. In order to examine the effectiveness of using KOLON’s data, we have rerun a previous sentiment analysis (SA) experiment, changing the approach to include data from the ontology. This new experiment obtained improved results, which is a strong indication that the project will be of use after its completion. Keywords: KOLON, ontology, Mikrokosmos Ontology, WordNet, FrameNet 
 b  Wakako Kashino and Manabu Okumura  a Department of Corpus Studies, National Institute for Japanese Language and Linguistics, 10-2 Midoricho, Tachikawa City, Tokyo, 190-8561, Japan waka@ninjal.ac.jp b Precision and Intelligence Laboratory, Tokyo Institute of Technology, 4259 Nagatsuta, Midori-ku, Yokohama, 226-8503, Japan oku@ lr.pi.titech.ac.jp  Abstract. Japanese books are usually classified into ten genres by Nippon Decimal Classification (NDC) based on their subject. However, this classification is sometimes insufficient for corpus studies which describe characteristics of the texts in the book. Here, we propose a method of classifying text samples taken from Japanese books into some registers and text types. Firstly, we discuss useful criteria to describe various characteristics of the texts and propose a two-step approach for stable annotation. We then apply our method to 161 book samples from the prerelease version of the Balanced Corpus of Contemporary Written Japanese (BCCWJ), a balanced Japanese corpus comprising 100 million words developed by National Institute for Japanese Language and Linguistics. Finally, we evaluate our method in terms of stability of annotation using kappa coefficients and correlation coefficients. Keywords: text type, register, corpus, kappa coefficient 
a School of English, Kyung Hee University jongbok@khu.ac.kr b Dept. of Computer Science, Kangnam Univeristy jaehyung@kangnam.ac.kr c Dept. of Linguistics, Univ. of Washington sanghoun@gmail.com  Abstract. The so-called serial verb construction (SVC) is a complex predicate structure consisting of two or more verbal heads but denotes one single event. This paper ﬁrst discusses the grammatical properties of Korean SVCs and provides a lexicalist, constructionbased analysis couched upon a typed-feature structure grammar. We also show the results of implementing the grammar in the LKB (Linguistics Knowledge Building) system couched upon the existing the KRG (Korean Resource Grammar) which has been developed since 2003. The implementation results provides us with a feasible direction of expanding the analysis to cover a wider range of relevant data. Keywords: serial verb construction, Korean Resource Grammar, LKB (Linguistic Knowledge Building)  
 a  b  c  Anup Kumar Kolya , Asif Ekbal , and Sivaji Bandyopadhyay  a, c Department of Computer Science and Engineering, Jadavpur University, Kolkata-700032, India  a  c  anup.kolya@gmail.com , sivaji_cse_ju@yahoo.com  b Department of Information Engineering and Computer Science, University of Trento, Italy  b asif.ekbal@gmail.com  Abstract. Temporal information extraction is an interesting research area in Natural Language Processing (NLP). Here, the main task involves identification of the different relations between various events and time expressions in a document. The relations are then classified into some predefined categories like BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and VAGUE. In this paper, we report our works of temporal information extraction along the lines of TempEval-2007 evaluation challenge. We adapt supervised machine learning approach for solving the problems of all the three tasks, namely A, B and C. Initially, a baseline system is developed by considering the most frequent temporal relation in the corresponding task’s training data. Evaluation results on the TempEval-2007 datasets yield the F-score values of 59.8%, 73.8% and 43.8% for Tasks A, B and C, respectively under the strict evaluation scheme. All these systems show the F-score values of 61.1%, 74.8% and 46.9% for Tasks A, B and C, respectively under the relaxed evaluation scheme. For the sub-ordinate event in Task C, the system shows the F-score values of 55.1% and 56.9% under the strict and relaxed evaluation scheme, respectively. Keywords: Temporal Relation Identification, TimeML, Conditional Random Field, TempEval-2007, Tasks A, B and C. 
Abstract. This paper illustrates the idea of parallel distributed parsing (PDP), which allows us to integrate lexical and sublexical analyses. PDP is proposed for providing a new model of efﬁcient, information-rich parses that can remedy the data sparseness problem. 
 Abstract. Despite the a priori desiratum of assimilating the derivation and interpretation of wh-questions in Japanese to that in English, empirical evidence shows that overt displacement of wh-phrases in Japanese is not the same as overt wh-movement in English. The syntax of wh-phrases in Japanese is essentially the same as that of non-wh-phrases. Failure of scope reconstruction in certain cases is not evidence for a constraint applying specifically to wh-movement but is due to the computation of the available readings, the semantic effect or prosody.  Keywords: wh-movement, scrambling, scope of displaced wh-phrases, reconstruction.  
Abstract. This paper deals with an aspectual classification of Korean and Japanese verbs based on Vendler (1967). Both of Korean and Japanese verbs are classified into 12 distinctive aspectual categories by their restrictions on time adverbials, progressive tenses and logical entailments. A well-established aspectual verb classes provide us not only with a better understanding of both languages, but also with an explicit explanation concerning some grammatical phenomena relevant to time. The aspectual classes of Korean verbs clarify the ambiguous semantic functions of so called Korean tense forms ‘-ess-‘ and ‘-essess-‘, while those of Japanese verbs elucidate the semantic functions of ‘sudeni’, the equivalent of ‘already’ in English, when it occurs with a variety of verbs in sentences. Keywords: aktionsarten, temporal properties, internal temporal structure, telicity, resultative 
Keywords: semantic evaluation, parser output, predicate logic formulas, unrestricted natural language 
Abstract. The aim of this paper is to provide formalization of Japanese sentence-ﬁnal particles in the framework of Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000, Szabolcsi 1987). While certain amount of literature has discussed the descriptive meaning of Japanese sentence-ﬁnal particles (Takubo and Kinsui 1997, Chino 2001), little formal account has been provided except for McCready (2007)’s analysis from the viewpoint of dynamic semantics and relevance theory. I analyze particles such as yo and ne as verum focus operators (Ho¨hle 1992, Romero and Han 2004). Keywords: verum focus, questions, focus semantic value 
School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, Scotland, UK {aotani, steedman}@inf.ed.ac.uk  Abstract. This paper discusses the semantic and pragmatic properties of Japanese benefactives with the main focus on the yaru-construction. The benefactive sentence is judged to be acceptable if the transitive verb complement falls into a certain semantic class in which the meaning of transfer of possession is expressed. Hence, the distribution of the recipient role rather than the beneﬁciary role is crucial for determining the acceptability of the construction. To capture such a multi-dimensional linguistic information, HPSG account will be given.  Keywords: yaru-construction, beneﬁciary, recipient, transfer of possession, multi-dimensional architecture of HPSG  
siripong.potisuk@citadel.edu Abstract. This paper describes a preliminary effort in identifying many different types of relations among words in Thai sentences based on dependency grammar. The relation is represented as a triple containing the pair of words and their relation. So far, the current representation contains 35 grammatical relations. The dependencies are all binary relations. That is, a grammatical relation holds between a governor and a dependent. The analysis makes use of the Thai “Orchid” corpus part-of-speech tags and the Stanford typed dependencies definitions. Keywords: Dependency grammar, Thai syntactic analysis. 
Abstract. Multilingual linguistic resources are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. This article seeks to explore and to exploit the idea of using multilingual web-based encyclopedias such as Wikipedia as comparable corpora for bilingual terminology extraction. We propose an approach to extract terms and their translations from different types of Wikipedia link information and data. The next step will be using a linguistic-based information to re-rank and filter the extracted term candidates in the target language. Preliminary evaluations using the combined statistics-based and linguistic-based approaches were applied on different pairs of languages including Japanese, French and English. These evaluations showed a real open improvement and a good quality of the extracted term candidates for building or enriching multilingual ontology, dictionaries or feeding a cross-language information retrieval system with the related expansion terms of the source query. Keywords: Bilingual terminology, comparable corpora, Wikipedia, multilingual linguistic tool. 
Abstract. Wikipedia is a potentially very useful source of information, but intuitively it is difﬁcult to have conﬁdence in the quality of an encyclopedia that anyone can modify. One aspect of correctness is writing style, which we examine in a computer based study of the full Japanese Wikipedia. This is possible because Japanese is a language with clearly distinct writing styles using e.g., different verb forms. We ﬁnd that the writing style of the Japanese Wikipedia is largely consistent with the style guidelines for the project. Exceptions appear to occur primarily in articles with a small number of changes and editors. Keywords: wikipedia, japanese, nlp 
 a  a  Qi Su , Helen Kai-yun Chen , and Chu-Ren Huang  a Department of Chinese & Bilingual Studies,  The Hong Kong Polytechnic University, Hong Kong, China  sukia@pku.edu.cn  b Key Laboratory of Computational Linguistics,  Peking University, Beijing, China  {helenkychen, churenhuang}@gmail.com  Abstract. In this paper, we focus on the task of identifying the best answer for a usergenerated question in Collaborative Question Answering (CQA) services. Given that most existing research on CQA has focused on non-textual features such as click-through counts which are relatively difficult to access, we examine the effectiveness of diverse content-based features for the task. Specially, we propose to explore how the information of evidentiality can contribute to the task. By the comparison of diverse textual features and their combinations, the current study provides useful insight into the issues of detecting the best answer to a given question in CQA without user features or system specific link structures. Keywords: collaborative question answering, answer assessment, credibility, quality 
Abstract. The present paper describes the development of a query focused multi-document automatic summarization. A graph is constructed, where the nodes are sentences of the documents and edge scores reflect the correlation measure between the nodes. The system clusters similar texts having related topical features from the graph using edge scores. Next, query dependent weights for each sentence are added to the edge score of the sentence and accumulated with the corresponding cluster score. Top ranked sentence of each cluster is identified and compressed using a dependency parser. The compressed sentences are included in the output summary. The inter-document cluster is revisited in order until the length of the summary is less than the maximum limit. The summarizer has been tested on the standard TAC 2008 test data sets of the Update Summarization Track. Evaluation of the summarizer yielded accuracy scores of 0.10317 (ROUGE-2) and 0.13998 (ROUGE–SU-4). Keywords: Multi Document Summarizer, Query Focused, Cluster based approach, Parsed and Compressed Sentences, ROUGE Evaluation. 
Keywords: Second Language Acquisition, Motion Events, Event Conflation, Cognitive Linguistics, Equipollently-framed Language 
Davidsonian arguments both in stage-level and individual-level predicates are able to function as topics, namely, existential stage topics and generic situation topics, respectively, which conforms with the well-known definiteness topic constraint. These two types of topics, however, are available only when the Davidsonian argument is bound via different means: the former gets bound in the existential domain, while the latter is constrained by the generic operator. Concerning the situation variable constrainer in sentences with individual-level predicates, we argue that besides general additional information, like adverbials ‘usually’ in English, ‘tongchang’/’yibande’, ‘yaoshi… jiu’ (if… then), etc, in Mandarin, bare NPs with kinddenoting and predicates denoting inherent property will help accommodate the generic operator, and thus a generic situation topic is available and sentences with indefinite subject NPs and individual-level predicates thus get licensed. As a consequence, we suggest that the necessary condition for the indefinite subject sentence should be that the indefinite subject NP is interpreted as specific or a stage topic/generic situation topic is available. Keywords: stage/individual-level predicates, topics, indefinite subjects  1. Introduction  It has long been noted that the distribution of sentences with indefinite subject NPs is quite  restricted, especially with respect to Mandarin Chinese which does not allow an indefinite NP to  appear in the subject or topic position unless it is interpreted as specific (see Chao 1968, Li and  Thompson 1981, Lee 1986, Tsai 2001, Xu 1996, among many others) or gets bound by an  existential or a generic operator (Pan 2009). Some recent studies, however, claim that it is not  correct to treat sentences with indefinite subjects uniformly without distinguishing between stage-  level and individual-level predicates (Erteschik-Shir1997, de Swart 1999), as shown by sentences  from English (1)-(2) and Mandarin (3)-(4), which clearly shows that both these two languages allow  sentences with the indefinite subject and a stage-level predicate, as in (1a) and (3a), but not  sentences with the indefinite subject and an individual-level predicate, as in (1b) and (3b). However,  when the subject is a bare NP, sentences with individual-level predicates are acceptable, as in (2b)  and (4b), contrasting with the indefinite subjects in (2a) and (4a):  English  (1) a. A man arrived  (stage-level predicate)  b. *A man is smart.  (individual-level predicate)  (2) a.*A dog is intelligent.  (indefinite NP)  b. Dogs are intelligent  (bare NP)  
 b  b  Xing ZHANG , Yan Song , and Alex Chengyu Fang  a Department of Chinese, Translation and Linguistics, City University of Hong Kong Kowloon, Hong Kong SAR zxing2@student.cityu.edu.hk b Department of Chinese, Translation and Linguistics, City University of Hong Kong Kowloon, Hong Kong SAR {yansong, acfang} @cityu.edu.hk  Abstract. In this paper, we describe the construction of a machine learning framework that exploit syntactic information in the recognition of biomedical terms and present the limits of machine learning in generating a novel term candidate list. Conditional random fields (CRF), is used as the basis of this framework. We make an effort to find the appropriate use of syntactic information, including parent nodes, syntactic paths and term ratios under this machine learning framework. The experiment results show that CRF model can achieve good precision in term recognition if trained with known term list. However, with regard to discovering potential novel terms for terminology lexicon editors, CRF model fails to show good performance, if trained with known term list only to predict novel terms in testing corpus. Therefore, this result suggests that more semantic information may be needed to determine a word to be a novel term during a specific period. Keywords: term recognition, novel term recognition, conditional random fields 
Abstract. Clitics in Arabic language can be attached to a stem or to each other without orthographic marks such as an apostrophe. In this paper we present a statistical study of clitics and its effect in Arabic language. We tokenize large Arabic text using white-spaces and an automatic clitics tokenizer (AMIRA 2.0) and compare the unique-word count in both cases with English language. We also show the resulted distribution of clitics in Arabic and examine the performance of the used tokenizer. Using a 600 million words Arabic corpus, we report that the corresponding lexicon size could be reduced by 24.54% when applying clitics tokenization. Keywords: Arabic, clitics, statistics. 
 a  a  Evan Liz C. Buhay , Marie Joy P. Evardone , Hansel B. Nocon ,  a  b  Davis Muhajereen D. Dimalen , Rachel Edita O. Roxas  a Information Technology Department, School of Computer Studies MSU Iligan Institute of Technology, Tibanga, Iligan City, Philippines 9200 ebuhay, mevardone, hnocon@yahoo.com, davis.dimalen@gmail.com b College of Computer Studies, De La Salle University-Manila rachel.roxas@delasalle.ph  Abstract: The aim of this study is to build natural language resources for languages with limited resources or minority languages. Manually building these resources is tedious and costly. These natural language resources such as a language corpora and lexicon will be used for natural language processing research and system development. Tagalog, a minority language was considered in this study as a test bed. This study exploited the use of the WWW to retrieve documents that are written in a minority language. We employed a frequency-based algorithm to build the lexicon. For our evaluation, we considered 260 Tagalog documents extracted from the web as our corpus. From the corpus, the system automatically selected 1,386 candidate unique words based on the threshold (with value of 10) as the lexical entries. Each lexical entry is validated by a language expert. Our evaluation shows an accuracy of 97.84% and only 2.16% error rate. The error was based on incorrectly spelled words or words that are not Tagalog. Keywords: Automatic lexicon builder. 
Abstract. This paper describes the work on automated Information Extraction that accepts arbitrary text and extracts information from the text. A new approach to implement Information Extraction system is proposed in this paper. Firstly, the article will be decomposed according to paragraph, sentence and phrase. Every sentence will be compared with the knowledge node, and then append the information extracted to the knowledge model. Finally, the answers are generated to the questions about the input text. With the experimental corpus the accuracy rate of knowledge matching is 63.5%, and accuracy rate of question answering is 65.0% with the system knowledge model. Keywords: Natural Language Processing, Information Extraction, knowledge model, knowledge representation. 
Keywords: user identiﬁcation, extension of scope of target users, messages in other categories, stylistic feature, community site 
Abstract. We propose a method to detect Japanese nasty comments from posts on bulletin board systems (BBS). Nasty comments can cause many social problem, because they express potentially harmful words and phrases. There are methods to recognize harmful words, but they are insufﬁcient. Therefore, we present a method for detecting such comments on a BBS with many posts using an n-gram model. In addition, we compared our method with a support vector machine (SVM) that is based on nasty words. As a result, we detected nasty comments that are different to those by the SVM. We also observe higher detection accuracy by combining two methods. Keywords: sentence detection, nasty comment, n-gram, SVM, BBS 
aDept. of Media Informatics, Ryukoku University, Seta, Otsu-shi, Shiga, 520-2194, Japan t10m101@mail.ryukoku.ac.jp,watanabe@rins.ryukoku.ac.jp b Dept. of Information and Electronics, Tottori University, Koyama-Minami, Tottori, 680-8550, Japan murata@ike.tottori-u.ac.jp c National Institute of Information and Communications Technology, Hikaridai, Seika-cho, Kyoto, Japan {kazama, kuroda, m-tsuchida, torisawa}@nict.go.jp dFaculty of Systems Information Science, Future University Hakodate, Hakodate, 041-8655, Japan fujita@fun.ac.jp eCenter for Knowledge Structuring, University of Tokyo, Hongo, Bunkyo-ku, Tokyo, 113-0033, Japan eiji.aramaki@gmail.com  Abstract. We proposed a method of using machine learning with various features for the recognition of Japanese notational variants. We increased 0.06 at the F-measure by speciﬁc features using existing dictionaries and character pairs useful for recognizing notational variants and obtained 0.91 at the F-measure for the recognition of notational variants. By using the method, we could extract 160 thousand word pairs with a precision rate of 0.9. We also constructed a method using patterns in addition to machine learning and observed that we could extract 4.2 million notational variant pairs with a precision rate of 0.78. We conﬁrmed that our method was much better than an existing method through experiments.  Keywords: machine learning, Japanese notational variant, various features, variant pair, edit distance.  
 Abstract. Recently, M.-J. Kim (2008) provided a reformulation of the Relevancy Condition on kes-relative clauses (kes-RCs) (Kim, Y.-B. 2002), the so-called internally headed relative clauses in Korean. In her analysis, the bipartite conditions of Kim (2002), one involving simultaneity between the main and the relative clause and the other involving a ‘resultant theme’, are collapsed into one involving a temporal overlap between the main and the relative clauses. In Kim (2008), the kes-RC is assumed to describe a temporary state which overlaps with the main event (i) if the relative clause contains an atelic predicate and the aspect is progressive, or (ii) if the relative clause contains a telic predicate and the aspect is perfect as well as progressive. This paper, however, claims that M.-J. Kim’s analysis still suffers from empirical problems, based on new type of examples; they show that (i) the relative clause can contain an atelic, perfect predicate if it is a stative type and (ii) the relative clause denote the volition or prediction of the speaker. The present paper proposes that kes-RCs denote a stage, which instantiates at the time of the main clause’s event.  Keywords: internally-headed relative clauses (IHRC), stage, the Relevancy Condition, aspect, tense  
Abstract. Sindhi is a morphologically rich language. Morphological construction include inflections and derivations. Sindhi morphology becomes more complex due to primary and secondary word types which are further divided into simple, complex and compound words. Sindhi nouns are marked by number gender and case. Finite state transducers (FSTs) quite reasonably represent the inflectional morphology of Sindhi nouns. The paper investigates Sindhi noun inflection rules and defines equivalent computational rules to be used by FSTs; corresponding FSTs are also given. Keywords. Sindhi, morphology, noun inflections, two-level morphology, finite state morphology. 
 b  c  Ju-yeon Ryu , Kaoru Horie , and Yasuhiro Shirai  a Department of Cross-cultural Education, Tohoku University, 41 Kawauchi, Aoba-ku, Sendai, 980-8576 Japan juyeon1212@mail.tains.tohoku.ac.jp b Department of Applied Linguistics, Nagoya University, Furo-cho, Chikusa-ku, Nagoya-shi, Nagoya, 464-0601 Japan horie@lang.nagoya-u.ac.jp c Department of Linguistics, University of Pittsburgh, 2801 Cathedral of Learning Pittsburgh, PA 15260 yshirai@pitt.edu  Abstract. This paper investigates the developmental process through which L2 learners acquire two „imperfective‟ aspect markers in Korean, -ko iss- (progressive and resultative) and -a iss- (resultative) which attempts to identify language-general and language-specific patterns in the L2 acquisition of the Korean imperfective aspect by Japanese learners by comparing the results with previous research. Study 1 collected cross-sectional data from 55 Japanese learners of Korean as a foreign language and 18 Korean native speakers. The results show that the acquisition order was as follows: the progressive -ko iss- → the resultative -ko iss- → the resultative -a iss-. Study 2 examined the influence of instruction order by testing two groups of learners that were taught aspect markers in different orders. The results show that the order of instruction did not yield significant differences except in the rate of accuracy of the resultative marker -a iss- in the comprehension task. Keywords: Second language acquisition, Korean imperfective markers, L1 transfer, prototype, instruction order, pedagogical conditions 
Keywords: sentiment analysis, multi-aspects review summarization, ratings of aspects, important sentence extraction, opinion integration. 
b Graduate School of Engineering, Tottori University 4-101, Koyama-Minami, Tottori, 680-8550, Japan murata@ike.tottori-u.ac.jp c National Institute of Information and Communications Technology 3-5, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {m-tsuchida,stijn,torisawa}@nict.go.jp Abstract. In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this study, we proposed a new method that generates summaries that can appropriately and adequately express the contents of their respective original documents using word-association knowledge. In this method, we assumed that a good summary comprises words that can express the contents of the original document and does not contain words that are unable to express the contents of the original document. Using statistical tests, we conﬁrmed that the use of elements in our method was beneﬁcial. Our method obtained 0.75 as the ratio where the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 
wang.shan@polyu.edu.hk, churen.huang@inet.polyu.edu.hk Abstract. This paper studies the adjectival modification to nouns in Mandarin Chinese based on selective binding. The main findings include: An adjective can select different types of head nouns as arguments and an adjective may modify an individual or an event. The qualia structure of a noun helps us better understand an adjective’s selectional preference. Meanwhile, an adjective can modify multi-facet or one facet of the qualia role of a noun. The adjacent adjective of a noun is not necessarily modifying the noun. Keywords: qualia structure, selective binding, adjectival modification to nouns 
The Hong Kong Polytechnic University wang.shan@polyu.edu.hk, churen.huang@inet.polyu.edu.hk  Abstract. The compositional operation of a predicate can be very complex. This paper captures a full picture of such an operation through investigating genuine corpus data of Chinese perception verb “ ” (kàn, look at). The study reveals that context and qualia structure can affect the meaning of verb-argument composition of “kàn”. It also shows that the compositional ability of “kàn” varies under different senses, and each sense has its own type selection preference. Keywords: kàn, selection, accommodation, exploitation, introduction  
Abstract. The initiative for this Workshop on Advanced Corpus Solutions has been taken in order to focus on the need for corpora that take into account that many users are linguists and philologists who do not have an interest in technical matters. Keywords: Corpora, user-friendly interfaces 
Abstract. This paper describes and evaluates the automatic grammatical annotation of a chat and an e-mail corpus of together 117 million words, using a modular Constraint Grammar system. We discuss a number of genre-specific issues, such as emoticons and personal pronouns, and offer a linguistic comparison of the two corpora with corresponding annotations of the Europarl corpus and the spoken and written subsections of the BNC corpus, with a focus on orality markers such as linguistic complexity and word class distribution. Keywords: chat corpus, e-mail corpus, orality, parsing, Constraint Grammar, NLP 
Abstract. This paper describes how recently developed techniques for sufﬁx array construction and compression can be expanded to bring a new data structure, called parallel sufﬁx array, into existence, which is suitable as an in-memory representation of large annotated corpora, enabling complex queries and fast extractions of the context of matching substrings. It is also shown how parallel sufﬁx arrays are superior to existing corpus search engines, in particular when sequential queries and corpora that are hard to tokenize are involved. Keywords: Corpus search engine, sufﬁx array, search engine for linguists 
Abstract. For many linguistic investigations, the ﬁrst step is to ﬁnd examples. In the 21st century, they should all be found, not invented. Thus linguists need ﬂexible tools for ﬁnding even quite rare phenomena. To support linguists well, they need to be fast even where corpora are very large and queries are complex. We present extensions to the CQL ’Corpus Query Language’ for intuitive creation of syntactically rich queries, and demonstrate that they can be computed quickly within our tool even on multi-billion word corpora. Keywords: corpus search, large corpora, CQL, syntactic search 
Abstract. This paper describes the Nordic Dialect Corpus, a corpus that consists of transcribed spoken dialects, with sound and video, from five North European languages (Danish, Faroese, Finnish, Icelandic, Norwegian and Swedish). The paper focuses on recent developments that have been added as a result of wishes expressed by the linguist users. These include map views of various selections of search results, English translations of every dialect concordance, and search possibiities and presentation of both orthographic and phonetic transcriptions. Keywords: Nordic languages, corpus, multilingual, multimodal, speech 
 b  Jan Pieter Kunst and Franca Wesseling  a Meertens Institute, Royal Netherlands Academy of Arts and Sciences, Joan Muyskenweg 25, 1090 GG Amsterdam, The Netherlands janpieter.kunst@meertens.knaw.nl b Meertens Institute, Royal Netherlands Academy of Arts and Sciences, Joan Muyskenweg 25, 1090 GG Amsterdam, The Netherlands franca.wesseling@meertens.knaw.nl  Abstract. In this paper we will expand on the creation and structure of the DynaSAND database as a case study of a corpus tool. Furthermore we will focus on its implementation in other search engines, thereby illustrating how the underlying data is decoupled from its original interface and used in new ways. Keywords: Dutch dialects, corpus, user interfaces, multiple corpora search 
 b  c  Mitsuko Yamura-Takei , Miho Fujiwara , and Etsuko Yoshida  a Faculty of Law, Hiroshima Shudo University, 1-1-1 Ozukahigashi, Asaminami-ku, Hiroshima 731-3195, JAPAN takeim@shudo-u.ac.jp b Department of Japanese & Chinese, Willamette University, 900 State St. Salem, Oregon 97301 USA mfujiwar@willamette.edu c Faculty of Humanities and Social Sciences, Mie University, 1577 Kurima-machiya-cho, Tsu, Mie, 514-8507, JAPAN tantan@human.mie-u.ac.jp  Abstract. This paper describes an ongoing collaborative project, between Japanese and U.S. universities, that aims to build, analyze and use comparable learner corpora in an attempt to promote discourse-level proficiency in foreign language learning contexts. The focus is placed on discourse coherence created by reference to nominal and clausal entities. The corpus analysis results, within the framework of Centering Theory (Grosz et al., 1995), will be presented, along with some pedagogical insights that teachers can utilize, back in the EFL/JFL classrooms where the data was initially collected. Keywords: learner corpora, entity coherence, event reference, centering theory 1. Introduction Discourse competence, one of the four principal components of communicative competence put forward by Canale (1983), is defined as the ability to connect sentences in stretches of discourse and to form a meaningful whole out of a series of utterances. Unlike “hard rules” at morphological and syntactic levels, many discourse phenomena are usually governed by “principles” or “preferences.” Therefore, it is hard to provide explicit and systematic instructions. While discourse-level research and practice have been attracting attention (cf. McCarthy, 1991), it is undeniable that a focus on sentence-level form tends to be emphasized in many foreign language teaching contexts. Therefore, it is crucial to develop effective ways for learners to better understand the concept of discourse coherence and to utilize relevant cohesive devices. Corpus-based research in the past two decades has unquestionably had a considerable impact on various linguistic sub-disciplines. Foreign language pedagogy is not an exception. Corpora come in various shapes. For instance, learner corpora are structured collections of language produced by language learners. Comparable corpora are two (or more) sets of corpora in different languages designed along the same lines. Learner corpora can provide information about how learner production differs from a target model and thus can inform the field of second language acquisition (SLA), for which a comparable corpus of native speaker texts is required. In an attempt to integrate these two pedagogical challenges and promises, we have designed the collaborative corpus project between Japanese and U.S. universities. In this paper, we will  The work reported in this paper was partially supported by Hiroshima Shudo University Research Grant (2010). Copyright 2010 by Mitsuko Yamura-Takei, Miho Fujiwara and Etsuko Yoshida.  780 Workshop on Advanced Corpus Solutions  first describe the project and then present our corpus analysis results, with a focus on entitycoherence that is characterized in the centering framework (Grosz et al., 1995). Special attention will be paid to event reference found in the data. 2. Corpus Project Our ongoing collaborative corpus project, between Japanese and U.S. universities, aims to (1) build, (2) analyze and (3) use comparable learner corpora in English/Japanese as a foreign language (EFL/JFL) teaching contexts. The project outline is presented in Figure 1 below.  <JAPAN> EFL Japanese native speakers English learners (3) Use  (1) Build (2) Analyze Learner corpus (L2) (EL / JL) Native speaker corpus (L1) (JNS / ENS)  <U.S.A.> JFL English native speakers Japanese learners (3) Use  Figure 1: Corpus project outline. Learner corpora research, when compared against comparable native-speaker corpora, offers important quantitative and qualitative insights to language pedagogy (Mukherjee, 2006). With the same premise, the project focuses on discourse-level features, rather than on well-studied lexico-grammatical patterns. Also, the project is designed along the lines of what Seidlhofer (2002) terms “local learner corpora.” In other words, the texts that the project participants have themselves produced will be used as a resource for teachers and learners in their own classrooms. Thus, the compiled corpus is not intended for public distribution. 2.1. Corpus Design The authors, in EFL and JFL teaching environments in Japan and the U.S., collect student writing in their first and second languages (L1 and L2). In collecting data, a video episode of Pingu, a Swiss clay animation, is presented to students to prompt production of a written narrative (i.e., synopsis writing). A 5-minute Pingu episode is ideal for this purpose in that it involves a limited number of characters (i.e., discourse entities) whose dialogue is in a make believe “penguin language,” making the story easy to follow regardless of the viewers’ language background. Students in Japanese and American universities are instructed to produce a synopsis of an episode of Pingu in both L1 and L2. To minimize L1 transfer (i.e., linguistic interference from a native language norm), the subjects are instructed to write first in their L2 and then in L1 and not to translate from one writing to the other. This provides us with four subsets of corpora: a Japanese native speaker (JNS) corpus, an English learner (EL) corpus, an English native speaker (ENS) corpus, and a Japanese learner (JL) corpus (see Figure 1 above). These four subsets enable several different dimensions of comparability, including comparison of native/non-native speakers (NS/NNS) discourse in English and Japanese, and comparison of L1 and L2 between individual learners. 2.2. Data The data used for this paper has been collected from 32 Japanese and 27 U.S. university students, each of whom wrote a synopsis of either one or both of two different Pingu episodes. Quantitative information for the data is summarized in Table 1.  PACLIC 24 Proceedings 781  Table 1: Basic counts of the data.  Number of texts Number of sentences Average number of sentences per text  ENS 48 755 15.73  EL 36 492 13.67  JNS 34 473 13.91  JL 48 799 16.65  TOTAL 166 2,519 15.17  For this study, the corpus needs to be local and small enough to be compiled in a specific environment to address questions specific to a particular group of learners (Seidlhofer, 2002).  3. Key Concepts and Theory  3.1. Entity Coherence Our major concern is entity coherence, as opposed to relational coherence that is ensured by rhetorical relations (Mann and Thompson, 1987), of a discourse. Let us first present some key concepts and their definitions that we employ for the discussion here. Entity coherence concerns repeated reference to the same entity in a discourse. By the standard definition, discourse entities can be individuals, objects, sets, events, facts, propositions, etc., and may be evoked or inferred in a discourse model. In many cases, reference is made to concrete individuals or objects that are typically introduced into a discourse in nominal forms, via explicit linguistic mention (e.g., pronouns, definite NPs), as in the example (from our corpus data) below.  (1) Pingu is playing with wooden blocks. He has trouble stacking the blocks so he becomes frustrated.  In this example, reference is made to an individual entity Pingu by a personal pronoun he, and to an object entity wooden blocks by a definite NP the blocks. Reference can also be made to so-called “abstract entities” (Hegarty, 2003), also known as “higher order entities” (Gundel et al, 1999). This type of entity is introduced to a discourse by a clause, a sequence of clauses or even a larger unit such as a whole discourse segment. Event reference is one such instance. An example from Gundel et al. (1999) is presented in (2).  (2) There was a snake on my desk. That scared me; …  In this particular example, the demonstrative pronoun that refers to an “event entity” introduced by a whole previous clause. Discourse entities can be concrete or abstract, as contrasted in examples (1) and (2). We will call the former type “nominal entities” and the latter “clausal entities” in this paper.  3.2. Centering Framework One important work that is concerned with entity-oriented coherence is Centering Theory (Grosz et al., 1995). This theory proposes to model the local mechanisms that create local coherence by operating on the discourse entities in each utterance within a discourse segment. The fundamental assumption of centering is that people continuously update their local attentional focus (called CENTER) as they incrementally process a discourse. Different ways of updating CENTER are formulated as the types of TRANSITION from one utterance to the next. The types are called continuation (CON), retaining (RET) and shifting (SHIFT), in the order of preference. The combination of these three TRANSITION types makes a total of nine TRANSITION sequence patterns: CON-CON, RET- CON, SHIFT-CON, CON- RET, RET-RET, SHIFT-RET, CON-SHIFT, RET-SHIFT, and SHIFT-SHIFT, which characterize CENTER transition environments  782 Workshop on Advanced Corpus Solutions (Yamura-Takei, 2005). A sentence which does not share any discourse entities with an immediately previous one is labeled NULL (or elsewhere called “No CB” condition). It is important to note how centering defines a set of discourse entities called CENTERs. Kameyama (1986) assumes that CENTERs are “(sets of) individuals, objects, states, actions and events” (p. 200). Grosz at al. (1995) also suggests that “events and other entities that are more often directly realized by VPs can also be centers” (p. 209, footnote 6), but leaves this beyond their scope, and so does the majority of succeeding centering work. For the purpose of applying centering analysis to the data, we will follow this line and consider only nominal entities for CENTERs. We will then examine in this analysis result how clausal entities interact. 4. Corpus Analysis Results The purpose of the analysis is to explicate learner-specific and language-specific tendencies that could serve as a pedagogical base to help raise students’ and teachers’ awareness of local referential cohesion back in the EFL/JFL teaching/learning environments where the data was initially collected. 4.1. Centering Analysis In order to characterize (nominal) entity coherence of the data, let us first look at our centering analysis results.1 Figures 2 and 3 present the distribution of centering TRANSITION types in English and Japanese data respectively. Figure 2: Use of four centering TRANSITION types in the ENS and EL writing samples. Figure 3: Use of four centering TRANSITION types in the JNS and JL writing samples. 
Maison de la Recherche, Université de Toulouse-Le Mirail, 5, allées Antonio Machado, F-31058 - Toulouse Cedex 9, France yann.desalle@univ-tlse2.fr cCLLE-ERSS, CNRS, Université de Toulouse, Maison de la Recherche, Université de Toulouse-Le Mirail, 5, allées Antonio Machado, F-31058 - Toulouse Cedex 9, France {duvignau, gaume}@univ-tlse2.fr Abstract. In this methodological investigation, we examined the influence of cultural background on viewers’ interpretations of visual stimuli and verbs elicited by these materials. French and Mandarin native speakers’ interpretations of seventeen short movies, produced by French speakers, depicting various state-changing actions were collected by a 25-item cultural protocol. A slight difference in the familiarity rating of movies is found between French and Mandarin participants. We also found that Mandarin speakers used more general verbs when describing actions depicted by movies with low familiarity rating and children used more conventional forms with movies of higher familiarity. Hierarchical cluster analyses were conducted in selecting movies that were matched in action-interpretations by both language groups. Keywords: cultural interpretation, verb specificity, familiarity rating, Mandarin, French 
Abstract. The discipline where sentiment/opinion/emotion has been identified and classified in human written text is well known as sentiment analysis. A typical computational approach to sentiment analysis starts with prior polarity lexicons where entries are tagged with their prior out of context polarity as human beings perceive using cognitive knowledge. Till date, all research efforts found in sentiment analysis literature deal mostly with English texts. In this article, we propose an interactive gaming (Dr Sentiment) technology to create and validate SentiWordNet in 56 languages by involving Internet population. Dr Sentiment is a fictitious character, interact with players using series of questions and finally reveal the behavioral or sentimental status of any player and store the lexicons as the players polarized during playing. The interactive gaming technology is then compared with other multiple automatic linguistics techniques like, WordNet based, dictionary based, corpus based or generative approaches for generating SentiWordNet(s) for Indian languages and other International languages as well. A number of automatic, semiautomatic and manual validations and evaluation methodologies have been adopted to measure the coverage and credibility of the developed SentiWordNet(s). Keywords: SentiWordNet, Global, Sentiment Analysis. 
b Graduate Institute of Linguistics, National Taiwan University, Taipei, Taiwan {shukai, hintat}@ntu.edu.tw c CLLE-ERSS, CNRS - Universite´ de Toulouse, Toulouse, France gaume@univ-tlse2.fr Abstract. In this paper we deﬁne a lexical metrology in graphs of verbal synonymy to compute the ﬂexsemic score of speakers from their verbal productions in action denomination tasks. This ﬂexsemic score is used to automatically categorize young children versus young adults. We show that this score is effective in French and in Mandarin. Keywords: automatic categorization, Flexsem, language acquisition, Prox 
b Alpage, INRIA Paris-Rocquencourt & Universite´ Paris 7, Paris, France. magistry@gate.sinica.edu.tw c Graduate Institute of Linguistics, National Taiwan University, Taipei, Taiwan shukai@ntu.edu.tw d IRIT, University of Toulouse III, Toulouse, France navarro@irit.fr Abstract In this paper we describe the data that will be used to compare the semantic structures that emerge from synonymy in French and in Mandarin. We aim at studying these semantic structures at both a global, lexicographic level, using lexicons, synonymy and translation dictionaries and at a more localised, experimental level, using data collected in parallel psycholinguistic experiments in French and Mandarin. After presenting our research project, the data we need to carry it out and the available resources, we analyse several linguistic issues arising from the structural differences between the French and Mandarin lexicons. We then explain the construction of the synonymy and translation networks from the available resources and detail speciﬁc choices that will enable us to produce meaningful experimental results based on this prepared data. Two kinds of networks are built: lexicographic networks and smaller movie-based networks extracted from experimental recordings. We conclude by describing how we intend to use this data. Keywords: Synonymy, graphs, lexicon, translation, concept similarity 
Abstract. This study assesses the inﬂuence of semantic space on the acquisition of verbal lexicon. The studied one hundred and ﬁfty action verbs extracted from the experimental data in M3 project are classiﬁed into clusters in terms of meaning speciﬁcity. The semantic space variation of different clusters is examined in the distributional model based on Academia Sinica Balanced Corpus (ASBC) with Latent Semantic Analysis (LSA). With semantic distance measured in the distributional model, this survey captures the homogeneity of verbs within the cluster and reveals the heterogeneity between the clusters. We compare the semantic space variation to the age-related changes in verb style, and explore the potential inﬂuence of word space on verbal lexicon acquisition. Keywords: Mandarin verbs, semantic space, latent semantic analysis 
b Graduate Institute of Linguistics, National Taiwan University, Taiwan chunhanchang@ntu.edu.tw c Octogone, CNRS - Université de Toulouse, France desalle@univ-tlse2.fr Abstract. Going cross-linguistic is a an important but challenging track for validating a computational model of lexical organization. Our starting point is a computational model that has been established and validated on French language and we attempted to apply it on Mandarin language. The main ingredients of this model are computational lexical resources and a psycho-linguistic protocol involving extra-linguistic material (video-clips). At this stage, all the psycho-linguistic experiments have been ran, most of the resources have been built but some comparative analyses are not fully completed. Still the project is advanced enough to report on the issues we had to address while performing this cross-linguistic move concerning the resources, the analysis of the data and the data alignment across languages. Keywords: Computational model, Acquisition, cross-linguistic study, French, Mandarin 
Abstract This paper develops new treatment of the problem of cross-sortal predication and copredication in particular. We argue that the solution to these predicate-argument sort mismatches can be solved by a more ﬂexible treatment of polysemy based on the notion of dependent type and dynamic construction of meaning. Keywords: lexical semantics, formal semantics, type theory, ontology, generative lexicon 
 Abstract. A theory of natural language production (speaker mode) has to answer the following questions: (i) Where does the content serving as input to language production come from? (ii) In which format is this relatively language-independent content stored, processed, and retrieved? and (iii) How is activated content mapped into well-formed surfaces of a certain natural language? After brief answers to (i) and (ii), this paper concentrates on (iii), illustrating the time-linear method of Database Semantics (DBS) on some of the most notorious grammatical constructions of natural language.  Keywords: Natural language production, speaker mode, center-embedded relative clauses, long distance dependencies, gapping constructions  
Keywords: change of location, change of state, creation/removal, telicity, degree, Generative Lexicon Theory 1. Introduction1 This paper discusses parallels between change of location and change of state, involved in locomotive (=motion), change of state, and creation/removal verbs, trying to see how telicity is attained semantically, examining cross-linguistic typological variation in lexical patterning and some syntactic behaviors. First, spatial uses of prepositions or postpositions are closely connected with temporal uses of them, although the latter are more abstract and limited because of directionality and dimensionality. Second, change of state (qualities) is structurally associated with change of location, with its Source and Goal. Change always means a shift from ¬P to P in state as well as in location through the flow of time. But the former is more abstract. Third, when change of state becomes psychological, it becomes even more abstract. As seen in build tension or its Korean equivalent kincangkam-ul coseng-ha-ta, indirect constitutive causation changes to experienced (direct) causation and does not need any part of the object such as ‘material’ (from a default argument). This is ‘derived unaccusativity’ (Pustejovsky 1995) that allows for modification by the degree adverbial maywu ‘very’ in Korean. If Mary’s presence is building tension then Mary’s presence has built tension. In other words, imperfective paradox disappears because there is no telicity involved any more. 1We benefited from our three-year project on Semantic Structures of Predicates in Korean (from 1997 to 2000 with B.Kang and S. Nam), which basically adopted the Generative Lexicon Theory (Pustejovsky 1995) approach. We also thank Taro Kageyama for inviting me to his workshop at LP2002 (Meikai University). This research was supported by the KRF-2009-342A00017 (Excellent Scholar) grant. Copyright 2010 by Chungmin Lee  886 Workshop on Semantics and Logical Flavor  2. Motion Expressions with Goal/Theme 2.1 Typological Variation of Motion Expressions  Typologically, in verb-framed languages (Talmy 2000) such as K/J, Hindi, Romance, Semitics,  Bantu and Polynesian languages, an existential/stative location postposition/preposition such as  –ey (K), -ni (J), à (French), -meN (Hindi) is further used only for basic directed motion verbs  but not for manner verbs, e.g.  (1) a. hakkyo-ey iss-ess-ta  school-LOC be - PAST-DEC  ‘(She) was in the school.’ b. hakkyo-ey ka-ass-ta2  school –PATH/GOAL go-PAST-DEC  ‘(She) went to (the) school.’  (2) * hakkyo-ey(K)/gakko-ni(J) talli –ess-ta  school -PATH/GOAL run-PAST-DEC  ‘(She) ran (the) school-ey(K)/-ni(J).’  (3) *LaD.kaa kamre-meN dauD.aa (Hindi)  boy –NOM room-PATH ran  ‘The boy ran the room-meN.’ (in the sense of ‘to’) (Narasimhan 1999)  (4) *El hombre corrido a la casa (Spanish)  the man ran PATH the house  ‘The man ran a the house.’ (in the sense of ‘to’) hasta ‘up to’ OK  (5) a. The boy went/ran to the park (in an hour). Cf. The boy was in the park.  b. Mary drove home for an hour.  c. (hoswu-esyse) kongwon-kkaci/??-ey il mail-ul tali-ess-ta  lake-from park-as far as /to  
Keywords: Pseudo-comparative sentences, denial, illocutionary force, expletives, profanities 1. Relationship between syntactic negation, pragmatic denial, and Modus Tollens Increasing interest has been drawn to the use of language in context, especially the analysis of illocutionary force. In this paper I hope to draw attention to the use of a range of expressions whose variable syntactic order conveys obvious meaningful differences. Take the following as a conversational exchange between speakers A and B. 1. A: John is rich. * This is a continuation of discussions on some issues first raised in my paper “Linguistic structure beyond grammar: pseudo-conditionals, pseudo-comparisons, speech acts and language teaching” in Working Papers in Language and Linguistics No 1, Department of Applied Linguistics, City Polytechnic of Hong Kong, 1989, pp.26-39. That paper was based on my earlier manuscript entitled “The grammar of profanity and the profanity of grammar” which in turn drew inspiration from James McCawley’s paper: “English sentences without overt grammatical subjects” [under the nom de plume: Quang Phuc Dong from the “South Hanoi Institute of Technology” and dated Feb 1967] and subsequent discussions with him. James McCawley has been a giant in the field of logico-semantics in America and elsewhere, while Professor Akira Ikega has a similar position in Asia and has been a dedicated patron of PACLIC It is only befitting that this feeble attempt on a relevant topic be offered to honor both.  908 Workshop on Semantics and Logical Flavor B could respond in the affirmative or negative: 2. B: (a) Oh yes, John is rich ! (affirmative) (b) Oh no, John is not rich ! (negative) (2b) is B’s simple negation of (1) by pragmatically not accepting the truth value of the proposition contained in (1), or by B disagreeing or denying the truth value of the proposition in (1). In Table 1 below, there are examples of other utterances which carry the illocutionary force of denial in their respective sentences. 3. John is rich, like I am the Queen of Sheba ! 4. John is rich, like I am a monkey’s uncle ! 5. John is rich, like I am the Premier of China ! 6. John is rich, like the sun sets in the East ! 7. John is rich, like hell ! Table 1: Some Pseudo-comparative sentences These sentences generally may have a phonological juncture after rich, especially for emphasis, when just may ironically be inserted after the pause. It is notable that while all sentences in Table 1 have the same speech act function of denial as (2b), yet none of them overtly contains a negative marker. So it would seem that an overt negative marker is not the exclusive means to put forward opposing propositions. What is of special interest is that sentences such as (3) to (7) are grammatically complex comparative sentences wherein attributes of two propositions in the first and second constituent sentences are being compared. However, these are not normal but pseudo-comparative sentences. For the sentences in Table 1 under discussion, we can designate A to represent the proposition “John is rich” and B to represent a range of proposition(s) such as those contained in the second constituent sentences. A two-step logical deduction is called for: The two propositions A and B are being equated with respect to some attribute in the pseudo-comparative sentences. This equated attribute allows for comparison and pragmatic manipulation by means of logical reasoning uniquely incorporated into the human communication chain. Thus assuming B is not true, then A is also not true (Modus Tollens or Modus Tollendo Tollens: the mood that by denying denies). This means, for example, in (3), since there was only one unique Queen of Sheba in Biblical times, the speaker could not conceivably be assumed to be the Queen of Sheba1; so there is inherent falsehood in the second proposition, and hence the first proposition is also false by Modus Tollens. When A is said to be like B, then some attribute of A is expected to be found in B, but the converse case is not necessarily true. In this case, the wealth of A is not being compared with the wealth of the Queen of Sheba (which would be conceivable), but with the speaker’s identity as the Queen of Sheba! 
This tutorial will describe the use of a factored probabilistic sequence model for parsing speech and text using a bounded store of three to four incomplete constituents over time, in line with recent estimates of human shortterm working memory capacity. This formulation uses a grammar transform to minimize memory usage during parsing. Incremental operations on incomplete constituents in this transformed representation then deﬁne an extended domain of locality similar to those deﬁned in mildly context-sensitive grammar formalisms, which can similarly be used to process long-distance and crossed-and-nested dependencies.  1.1 Notation This paper will associate variables for syntactic categories c, trees or subtrees τ , and string yields x¯ with constituents in phrase structure trees, identiﬁed using subscripts that describe the path from the root of the tree containing this constituent to the constituent itself. These paths may consist of left branches (indicated by ‘0’s in the path) and right branches (indicated by ‘1’s), concatenated into sequences η (or ι or κ). Thus, if a path η identiﬁes a constituent, that constituent’s left child would be identiﬁed by η0, and that constituent’s right child would be identiﬁed by η1. The empty path will be sued to identify the root of a tree. The probabilistic parsers deﬁned here will also use an indicator function ⋅ to denote deterministic probabilities: φ = 1 if φ is true, 0 otherwise.  
 as (2)-(5) as non-local RNR.3  In this paper, we show that the analysis of right-node-raising (RNR) in coordinate structures proposed in Sarkar and Joshi (1996) can be extended to non-local RNR if it is augmented with delayed tree locality (Chiang and Schefﬂer, 2008), but not with ﬂexible composition (Joshi et al., 2003). In the proposed delayed tree-local analysis, we deﬁne multicomponent (MC) elementary tree sets with contraction set speciﬁcation. We propose that a member of each of the MC sets participates in forming a derivational unit called contraction path in the derivation structure, and that contraction paths must be derivationally local to each other for the relevant contraction to be licensed. 
This paper presents a uniﬁed account of two well-known conditions on extraction domains: the “adjunct island” effect and “freezing” effects. Descriptively speaking, extraction is problematic out of adjoined constituents and out of constituents that have moved. I introduce a syntactic framework from which it emerges naturally that adjoined constituents are relevantly like constituents that have moved, unifying the two descriptive generalisations noted above. 
This paper considers gapping data through the lens of combinatory categorial grammar (CCG) as developed in Steedman (1990, 2000). It analyzes CCG’s predictive power in managing a wide variety of cross-clausal gapping data. CCG predicts the typing of the rightmost subject in cross-clausal gapping data as an object; evidence from Case supports this hypothesis. Reflexive binding in cross-clausal structures favors the Szabolcsi (1989) binding proposal, in which binding occurs at the level of the surface structure. Additionally, facts from Chinese buttress the CCG analysis, as its NP category-assignment offers a natural explanation for the ungrammaticality of gapping sentences containing non-quantified NP objects: they are unable to undergo type-shifting. 
Well-nested word languages have been advertised as adequate formalizations of the notion of mild context-sensitivity. The main result of this paper is a characterization of well-nested tree languages in terms of simple attributed tree transducers. 
Within generative approaches to grammar, characterizing the complexity of natural language has traditionally been couched in terms of formal language theory. Recently, Kuhlmann (2007) and collaborators have shown how derivations of generative grammars can be alternately represented as dependency graphs. The properties of such structures provide a new perspective of grammar formalisms and different metric of complexity. The question of complexity of natural language can be recast in dependency structure terms. Ill-nested structures have been assigned to some examples in the literature (Boston et al, 2009, Maier and Lichte, 2009), but the availability of well-nested alternatives prevents the use of these examples to claim that illnestedness is an unavoidable linguistic reality. This paper claims that two examples, one German and one Czech, are unavoidably ill-nested, indicating that ill-nestedness is indeed unavoidable in natural language. We conclude that formalisms that generate only well-nested structures, such as TAGs, are not quite powerful enough. However, the tree-local multicomponent extension to TAG does generate illnested structures, providing just the appropriate amount of complexity in dependency structure terms for characterizing natural language. 
This paper shows how domain-speciﬁc grammars can be automatically generated from a declarative model of the lexicon-ontology interface and how those grammars can be used for question answering. We show a speciﬁc implementation of the approach using Lexicalized Tree Adjoining Grammars. The main characteristic of the generated elementary trees is that they constitute domains of locality that span lexicalizations of ontological concepts rather than being based on requirements of single lexical heads. 
This paper provides an account for inverse scope restrictions with nominal quantiﬁers using synchronous tree adjoining grammar. It aims to provide an alternative account to Beghelli & Stowell’s (1997) work on similar data.  analysis of quantiﬁer scope in English. In particular this paper examines the restriction on inverse scope readings in English when the object quantiﬁer is a count quantiﬁer as compared to the the ability to have inverse scope readings when the quantiﬁer is a universal quantiﬁer. Additionally, these same restrictions will be shown to hold in the double object constructions in English.  
This paper provides a new account for why online processing of ﬁller-gap relative clause dependencies is more difﬁcult in cases where ﬁller-gap interacts with object control than in cases involving subject control, as reported by Frazier et al. (1983). Frazier et al. (1983) argued for a Recent Filler heuristic in which the parser expects to discharge the most recent ﬁller at every gap site. We observe that statistical subcategorization preferences on the control verb and the embedded verb ‘sing’ interact, favoring subject control disambiguation. We employ surprisal (Hale, 2001) as a complexity metric on ﬁller-gap structures by construing control as a Movement operation in Minimalist Grammars (Stabler, 1997). We obtain greater surprisals for the Distant Filler condition, deriving the prediction that the Recent Filler heuristic falls out from statistical subcategorization preferences. 
In the face of partial fronting phenomena in German, we introduce spinal TT-MCTAG, a new MCTAG variant that integrates features of LTAG-spinal and TT-MCTAG. Using spinal TT-MCTAG we arrive at ﬂat syntactic structures which make available a consistent account for the data.  VP  V  VP  zu reparieren V  VP  versprach NPacc ↓  VP  NPnom ↓ . . .  
This work provides a TAG account of gapping in English, based on a novel deletion-like operation that is referred to as de-anchoring. Deanchoring applies onto elementary trees, but it is licensed by the derivation tree in two ways. Firstly, de-anchored trees must be linked to the root of the derivation tree by a chain of adjunctions, and the sub-graph of de-anchored nodes in a derivation tree must satisfy certain internal constraints. Secondly, de-anchoring must be licensed by the presence of a homomorphic antecedent derivation tree. 
In this paper1 we present an extension of MCTAGs with Local Shared Derivation (Seddah, 2008) which can handle non local elliptic coordinations. Based on a model for control verbs that makes use of so-called ghost trees, we show how this extension leads to an analysis of argument cluster coordinations that provides an adequate derivation graph. This is made possible by an original interpretation of the MCTAG derivation tree mixing the views of Kallmeyer (2005) and Weir (1988). 
Highly compacted TAGs may be built by allowing subtree factorization operators within the elementary trees. While hand-crafting such trees remains possible, a better option arises from a coupling with meta-grammar descriptions. The approach has been validated by the development of FRMG, a widecoverage French TAG of only 207 trees. 
We present a parser for probabilistic Linear Context-Free Rewriting Systems and use it for constituency and dependency treebank parsing. The choice of LCFRS, a formalism with an extended domain of locality, enables us to model discontinuous constituents and nonprojective dependencies in a straight-forward way. The parsing results show that, ﬁrstly, our parser is efﬁcient enough to be used for datadriven parsing and, secondly, its result quality for constituency parsing is comparable to the output quality of other state-of-the-art results, all while yielding structures that display discontinuous dependencies. 
We present PCRISP, a sentence generation system for probabilistic TAG grammars which performs sentence planning and surface realization in an integrated fashion, in the style of the SPUD system. PCRISP operates by converting the generation problem into a metric planning problem and solving it using an offthe-shelf planner. We evaluate PCRISP on the WSJ corpus and identify trade-offs between coverage, efﬁciency, and accuracy. 
Quantiﬁer scope challenges the mantra of Tree Adjoining Grammar (TAG) that all syntactic dependencies are local once syntactic recursion has been factored out. The reason is that on current TAG analyses, a quantiﬁer and the furthest reaches of its scope domain are in general not part of any (unicomponent) elementary tree. In this paper, I consider a novel basic TAG operation called COSUBSTITUTION. In normal substitution, the root of one tree (the argument) replaces a matching non-terminal on the frontier of another tree (the functor). In cosubstitution, the syntactic result is the same, leaving weak and strong generative capacity unchanged, but the derivational and semantic roles are reversed: the embedded subtree is viewed as the functor, and the embedding matrix is viewed as its semantic argument, i.e., as its nuclear scope. On this view, a quantiﬁer taking scope amounts to entering a derivation at the exact moment that its nuclear scope has been constructed. Thus the relationship of a quantiﬁer and its scope is constrained by DERIVATIONAL LOCALITY rather than by elementary-tree locality. 
This paper presents an analysis of bound variable pronouns in English using Synchronous Tree Adjoining Grammar. Bound variables are represented as multi-component sets, composing in delayed tree-local derivations. We propose that the observed anti-locality restriction on English bound variables can be formalised in terms of a constraint on the delay in the composition of the bound variable multi-component set. While most cases are captured in a derivation making use of two simultaneous delays, maintaining weak equivalence with ﬂexible composition, our analysis is open to derivations with an unlimited number of simultaneous delays. 
All permutations of a two level embedding sentence in Turkish is analyzed, in order to develop an LTAG grammar that can account for Turkish long distance dependencies. The fact that Turkish allows only long distance topicalization and extraposition is shown to be connected to a condition -the coherence condition- that draws the boundary between the acceptable and inacceptable permutations of the ﬁve word sentence under investigation. The LTAG grammar for this fragment of Turkish has two levels: the ﬁrst level assumes lexicalized and linguistically appropriate elementary trees, where as the second level assumes elementary trees that are derived from the elementary trees of the ﬁrst level, and are not lexicalized. 
Recent work has proposed the use of an extracted tree grammar as the basis for treebank analysis, in which queries are stated over the elementary trees, which are small chunks of syntactic structure. In this work we integrate search over the derivation tree with this approach in order to analyze differences between two sets of annotation on the same text, an important problem for parser analysis and evaluation of inter-annotator agreement. 
In this paper, we present a system that automatically extracts lexicalized tree adjoining grammars (LTAG) from treebanks. We first discuss in detail extraction algorithms and compare them to previous works. We then report the first LTAG extraction result for Vietnamese, using a recently released Vietnamese treebank. The implementation of an open source and language independent system for automatic extraction of LTAG grammars is also discussed. 
We present a first step towards a model of speech generation for incremental dialogue systems. The model allows a dialogue system to incrementally interpret spoken input, while simultaneously planning, realising and selfmonitoring the system response. The model has been implemented in a general dialogue system framework. Using this framework, we have implemented a specific application and tested it in a Wizard-of-Oz setting, comparing it with a non-incremental version of the same system. The results show that the incremental version, while producing longer utterances, has a shorter response time and is perceived as more efficient by the users. 
Incremental natural language understanding is the task of assigning semantic representations to successively larger preﬁxes of utterances. We compare two types of statistical models for this task: a) local models, which predict a single class for an input; and b), sequential models, which align a sequence of classes to a sequence of input tokens. We show that, with some modiﬁcations, the ﬁrst type of model can be improved and made to approximate the output of the second, even though the latter is more informative. We show on two different data sets that both types of model achieve comparable performance (signiﬁcantly better than a baseline), with the ﬁrst type requiring simpler training data. Results for the ﬁrst type of model have been reported in the literature; we show that for our kind of data our more sophisticated variant of the model performs better. 
This paper proposes a novel approach for predicting user satisfaction transitions during a dialogue only from the ratings given to entire dialogues, with the aim of reducing the cost of creating reference ratings for utterances/dialogue-acts that have been necessary in conventional approaches. In our approach, we ﬁrst train hidden Markov models (HMMs) of dialogue-act sequences associated with each overall rating. Then, we combine such rating-related HMMs into a single HMM to decode a sequence of dialogueacts into state sequences representing to which overall rating each dialogue-act is most related, which leads to our rating predictions. Experimental results in two dialogue domains show that our approach can make reasonable predictions; it significantly outperforms a baseline and nears the upper bound of a supervised approach in some evaluation criteria. We also show that introducing states that represent dialogue-act sequences that occur commonly in all ratings into an HMM signiﬁcantly improves prediction accuracy. 
Commonly used coreference resolution evaluation metrics can only be applied to key mentions, i.e. already annotated mentions. We here propose two variants of the B3 and CEAF coreference resolution evaluation algorithms which can be applied to coreference resolution systems dealing with system mentions, i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results. 
We introduce a novel approach for robust belief tracking of user intention within a spoken dialog system. The space of user intentions is modeled by a probabilistic extension of the underlying domain ontology called a probabilistic ontology tree (POT). POTs embody a principled approach to leverage the dependencies among domain concepts and incorporate corroborating or conﬂicting dialog observations in the form of interpreted user utterances across dialog turns. We tailor standard inference algorithms to the POT framework to efﬁciently compute the user intentions in terms of m-best most probable explanations. We empirically validate the efﬁcacy of our POT and compare it to a hierarchical frame-based approach in experiments with users of a tourism information system. 
Multimodal conversational dialogue systems consisting of numerous software components create challenges for the underlying software architecture and development practices. Typically, such systems are built on separate, often preexisting components developed by different organizations and integrated in a highly iterative way. The traditional dialogue system pipeline is not flexible enough to address the needs of highly interactive systems, which include parallel processing of multimodal input and output. We present an architectural solution for a multimodal conversational social dialogue system. 
We describe work done at three sites on designing conversational agents capable of incremental processing. We focus on the ‘middleware’ layer in these systems, which takes care of passing around and maintaining incremental information between the modules of such agents. All implementations are based on the abstract model of incremental dialogue processing proposed by Schlangen and Skantze (2009), and the paper shows what different instantiations of the model can look like given speciﬁc requirements and application areas. 
Two of the main corpora available for training discourse relation classiﬁers are the RST Discourse Treebank (RST-DT) and the Penn Discourse Treebank (PDTB), which are both based on the Wall Street Journal corpus. Most recent work using discourse relation classiﬁers have employed fully-supervised methods on these corpora. However, certain discourse relations have little labeled data, causing low classiﬁcation performance for their associated classes. In this paper, we attempt to tackle this problem by employing a semi-supervised method for discourse relation classiﬁcation. The proposed method is based on the analysis of feature cooccurrences in unlabeled data. This information is then used as a basis to extend the feature vectors during training. The proposed method is evaluated on both RST-DT and PDTB, where it signiﬁcantly outperformed baseline classiﬁers. We believe that the proposed method is a ﬁrst step towards improving classiﬁcation performance, particularly for discourse relations lacking annotated data. 
We report results on predicting the sense of implicit discourse relations between adjacent sentences in text. Our investigation concentrates on the association between discourse relations and properties of the referring expressions that appear in the related sentences. The properties of interest include coreference information, grammatical role, information status and syntactic form of referring expressions. Predicting the sense of implicit discourse relations based on these features is considerably better than a random baseline and several of the most discriminative features conform with linguistic intuitions. However, these features do not perform as well as lexical features traditionally used for sense prediction. 
This study investigates the use of Same – a relation that connects the parts of a discontinuous discourse segment – in the Discourse Graphbank (Wolf et al., 2004). Our analysis reveals systematic deviations from the definition of the Same relation and a substantial number of confusions between Same and Elaboration relations. We discuss some methodological and theoretical implications of these findings. 
In this paper we motivate and describe a dialogue manager which is able to infer and negotiate causal implicatures. A causal implicature is a type of Gricean relation implicature, and the ability to infer them is crucial in situated dialogue. Because situated dialogue interleaves conversational acts and physical acts, the dialogue manager needs to have a grasp on causal implicatures in order not only to decide what physical acts to do next but also to generate causally-aware clariﬁcations. 
Van der Sandt’s algorithm for handling presupposition is based on a “presupposition as anaphora” paradigm and is expressed in the realm of Kamp’s DRT. In recent years, we have proposed a typetheoretic rebuilding of DRT that allows Montague’s semantics to be combined with discourse dynamics. Here we explore van der Sandt’s theory along the line of this formal framework. It then results that presupposition handling may be expressed in a purely Montagovian setting, and that presupposition accommodation amounts to exception handling. 
Temporal analysis of events is a central problem in computational models of discourse. However, correctly recognizing temporal aspects of events poses serious challenges. This paper introduces a joint modeling framework and feature set for temporal analysis of events that utilizes Markov Logic. The feature set includes novel features derived from lexical ontologies. An evaluation suggests that introducing lexical relation features improves the overall accuracy of temporal relation models. 
In this paper we present a reference model based on Reference Domain Theory that can work both in interpretation and generation. We introduce a formalization of key concepts of RDT, the interpretation and generation algorithms and show an example of behavior in the dynamic, asymmetric and multimodal GIVE environment. 
 with our recommendation system is shown in Figure 1.  In this paper we present a framework for spoken recommendation systems. To provide reliable recommendations to users, we incorporate a review summarization technique which extracts informative opinion summaries from grass-roots users‘ reviews. The dialogue system then utilizes these review summaries to support both quality-based opinion inquiry and feature-specific entity search. We propose a probabilistic language generation approach to automatically creating recommendations in spoken natural language from the text-based opinion summaries. A user study in the restaurant domain shows that the proposed approaches can effectively generate reliable and helpful recommendations in human-computer conversations.  U: Are there any American restaurants in Brighton? S: There are 61 American restaurants in Brighton. All of the rated restaurants have good or excellent ambiance. Many are on Washington Street, Commonwealth Avenue and Market Street. U: Show me the ones that have nice ambiance. S: I found 4 entries. Devlin's Restaurant and Bar is perfect for romantic date, Stockyard Restaurant has friendly atmosphere, Tasca Tapas Restaurant has excellent ambiance, and Wings Express has friendly vibes. U: Which one has good martinis? S: Devlin's Restaurant and Bar serves American food. It has wonderful martinis, good wine, great fish. It is perfect for romantic date. Great place. Figure 1. A real conversation with our recommendation dialogue system in the restaurant domain (‗U‘ is the user and ‗S‘ is the system).  
This paper introduces a new dialogue management framework for goal-directed conversations. A declarative specification defines the domain-specific elements and guides the dialogue manager, which communicates with the knowledge sources to complete the specified goal. The user is viewed as another knowledge source. The dialogue manager finds the next action by a mixture of rule-based reasoning and a simple statistical model. Implementation in the flight-reservation domain demonstrates that the framework enables the developer to easily build a conversational dialogue system. 
We describe an approach to improving the naturalness of a social dialogue system, Talkie, by adding disﬂuencies and other content-independent enhancements to synthesized conversations. We investigated whether listeners perceive conversations with these improvements as natural (i.e., human-like) as human-human conversations. We also assessed their ability to correctly identify these conversations as between humans or computers. We ﬁnd that these enhancements can improve the perceived naturalness of conversations for observers “overhearing” the dialogues. 
The present study uses the dialogue paradigm to explore route communication. It revolves around the analysis of a corpus of route instructions produced in real-time interaction with the follower. It explores the variation in forming route instructions and the factors that contribute in it. The results show that visual co-presence influences the performance, conversation patterns and configuration of instructions. Most importantly, the results suggest an analogy between the choices of instructiongivers and the communicative actions of their partners. 1.1 Spatial language in dialogue The main question this paper attempts to address is how people produce route instructions in dialogue. The current zeitgeist in language research and dialogue system development seems to be the unified investigation of spatial language and dialogue (Coventry et al., 2009). Indicative of the growing prioritisation of dialogue in the study of spatial language are the on-going research efforts within the MapTask1 project and the GIVE challenge2 . 1.2 A framework for the analysis of route instructions The study uses CORK (Communication of Route Knowledge, (Allen 2000)), a framework which provides a component-based analysis of route instructions. The CORK taxonomy differentiates between instructions that are directives (action statements with verbs of movement) and descriptive statements (with state-of-being verbs, like “be” and “see”). Descriptives present a static pic- 
In this paper we examine the influence of dimensionality on natural language route directions in dialogue. Specifically, we show that giving route instructions in a quasi-3d environment leads to experiential descriptive accounts, as manifested by a higher proportion of location descriptions, lack of chunking, use of 1st person singular personal pronouns, and more frequent use of temporal and spatial deictic terms. 2d scenarios lead to informative instructions, as manifested by a frequent use of motion expressions, chunking of route elements, and use of mainly 2nd person singular personal pronouns. 
Older adults are a challenging user group because their behaviour can be highly variable. To the best of our knowledge, this is the ﬁrst study where dialogue strategies are learned and evaluated with both simulated younger users and simulated older users. The simulated users were derived from a corpus of interactions with a strict system-initiative spoken dialogue system (SDS). Learning from simulated younger users leads to a policy which is close to one of the dialogue strategies of the underlying SDS, while the simulated older users allow us to learn more ﬂexible dialogue strategies that accommodate mixed initiative. We conclude that simulated users are a useful technique for modelling the behaviour of new user groups. 
Spoken dialogue management strategy optimization by means of Reinforcement Learning (RL) is now part of the state of the art. Yet, there is still a clear mismatch between the complexity implied by the required naturalness of dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain which can not ensure convergence of most of RL algorithms. In this paper, we propose to combine a sample-efﬁcient generalization framework for RL with a feature selection algorithm for the learning of an optimal spoken dialogue management strategy. 
This paper presents an agenda-based user simulator which has been extended to be trainable on real data with the aim of more closely modelling the complex rational behaviour exhibited by real users. The trainable part is formed by a set of random decision points that may be encountered during the process of receiving a system act and responding with a user act. A samplebased method is presented for using real user data to estimate the parameters that control these decisions. Evaluation results are given both in terms of statistics of generated user behaviour and the quality of policies trained with different simulators. Compared to a handcrafted simulator, the trained system provides a much better ﬁt to corpus data and evaluations suggest that this better ﬁt should result in improved dialogue performance. 
We present new results from a real-user evaluation of a data-driven approach to learning user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difﬁcult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the system learns REG policies which can adapt to unknown users online. For real users of such a system, we show that in comparison to an adaptive hand-coded baseline policy, the learned policy performs signiﬁcantly better, with a 20.8% average increase in adaptation accuracy, 12.6% decrease in time taken, and a 15.1% increase in task completion rate. The learned policy also has a signiﬁcantly better subjective rating from users. This is because the learned policies adapt online to changing evidence about the user’s domain expertise. We also discuss the issue of evaluation in simulation versus evaluation with real users. 
The paper investigates discourse particles on the example of German doch, assigning to them very speciﬁc semantic interpretations that still cover a wide range of their uses. The analysis highlights the role of discourse particles in managing the common ground and crucially takes into account that discourse particles can refer not only to utterances they are a part of and to previously uttered utterances, but also to felicity conditions of these utterances. 
Implicit discourse relation recognition is difﬁcult due to the absence of explicit discourse connectives between arbitrary spans of text. In this paper, we use language models to predict the discourse connectives between the arguments pair. We present two methods to apply the predicted connectives to implicit discourse relation recognition. One is to use the sense frequency of the speciﬁc connectives in a supervised framework. The other is to directly use the presence of the predicted connectives in an unsupervised way. Results on PDTB2 show that using language model to predict the connectives can achieve comparable F-scores to the previous state-of-art method. Our method is quite promising in that not only it has a very small number of features but also once a language model based on other resources is trained it can be more adaptive to other languages and domains. 
We present analyses aimed at eliciting which speciﬁc aspects of discourse provide the strongest indication for text importance. In the context of content selection for single document summarization of news, we examine the beneﬁts of both the graph structure of text provided by discourse relations and the semantic sense of these relations. We ﬁnd that structure information is the most robust indicator of importance. Semantic sense only provides constraints on content selection but is not indicative of important content by itself. However, sense features complement structure information and lead to improved performance. Further, both types of discourse information prove complementary to non-discourse features. While our results establish the usefulness of discourse features, we also ﬁnd that lexical overlap provides a simple and cheap alternative to discourse for computing text structure with comparable performance for the task of content selection. 
Spoken language interaction between humans and robots in natural environments will necessarily involve communication about space and distance. The current study examines people’s close-range route instructions for robots and how the presentation format (schematic, virtual or natural) and the complexity of the route affect the content of instructions. We find that people have a general preference for providing metric-based instructions. At the same time, presentation format appears to have less impact on the formulation of these instructions. We conclude that understanding of spatial language requires handling both landmark-based and metric-based expressions. 
In spoken communications, correction utterances, which are utterances correcting other participants utterances and behaviors, play crucial roles, and detecting them is one of the key issues. Previously, much work has been done on automatic detection of correction utterances in human-human and human-computer dialogs, but they mostly dealt with the correction of erroneous utterances. However, in many real situations, especially in communications between humans and mobile robots, the misunderstandings manifest themselves not only through utterances but also through physical actions performed by the participants. In this paper, we focus on action corrections and propose a classiﬁcation of such utterances into Omission, Commission, and Degree corrections. We present the results of our analysis of correction utterances in dialogs between two humans who were engaging in a kind of on-line computer game, where one participant plays the role of the remote manager of a convenience store, and the other plays the role of a robot store clerk. We analyze the linguistic content, prosody as well as the timing of correction utterances and found that all features were signiﬁcantly correlated with action corrections. 
We propose a non-humanlike spoken dialogue design, which consists of two elements: non-humanlike turn-taking and non-humanlike acknowledgment. Two experimental studies are reported in this paper. The ﬁrst study shows that the proposed non-humanlike spoken dialogue design is effective for reducing speech collisions. It also presents pieces of evidence that show quick humanlike turn-taking is less important in spoken dialogue system design. The second study supports a hypothesis found in the ﬁrst study that user preference on response timing varies depending on interaction patterns. Upon receiving these results, this paper suggests a practical design guideline for spoken dialogue systems. 
Building an industrial spoken dialogue system (SDS) requires several iterations of design, deployment, test, and evaluation phases. Most industrial SDS developers use a graphical tool to design dialogue strategies. They are critical to get good system performances, but their evaluation is not part of the design phase. We propose integrating dialogue logs into the design tool so that developers can jointly monitor call ﬂows and their associated Key Performance Indicators (KPI). It drastically shortens the complete development cycle, and offers a new design experience. Orange Dialogue Design Studio (ODDS), our design tool, allows developers to design several alternatives and compare their relative performances. It helps the SDS developers to understand and analyse the user behaviour, with the assistance of a reinforcement learning algorithm. The SDS developers can thus confront the different KPI and control the further SDS choices by updating the call ﬂow alternatives. Index Terms : Dialogue Design, Online Learning, Spoken Dialogue Systems, Monitoring Tools 
The purpose of this study is to get a working definition that matches people’s intuitive notion of gossip and is sufficiently precise for computational implementation. We conducted two experiments investigating what type of conversations people intuitively understand and interpret as gossip, and whether they could identify three proposed constituents of gossip conversations: third person focus, pejorative evaluation and substantiating behavior. The results show that (1) conversations are very likely to be considered gossip if all elements are present, no intimate relationships exist between the participants, and the person in focus is unambiguous. (2) Conversations that have at most one gossip element are not considered gossip. (3) Conversations that lack one or two elements or have an ambiguous element lead to inconsistent judgments. 
Modelling dialogue as a Partially Observable Markov Decision Process (POMDP) enables a dialogue policy robust to speech understanding errors to be learnt. However, a major challenge in POMDP policy learning is to maintain tractability, so the use of approximation is inevitable. We propose applying Gaussian Processes in Reinforcement learning of optimal POMDP dialogue policies, in order (1) to make the learning process faster and (2) to obtain an estimate of the uncertainty of the approximation. We ﬁrst demonstrate the idea on a simple voice mail dialogue task and then apply this method to a real-world tourist information dialogue task. 
This paper describes the design of a backchannel feedback corpus and its evaluation, aiming at realizing in-car spoken dialogue systems with high responsiveness. We constructed our corpus by annotating the existing in-car spoken dialogue data with back-channel feedback timing information in an off-line environment. Our corpus can be practically used in developing dialogue systems which can provide verbal back-channel feedbacks. As the results of our evaluation, we conﬁrmed that our proposed design enabled the construction of back-channel feedback corpora with high coherency and naturalness. 
We point out several problems in scalingup statistical approaches to spoken dialogue systems to enable them to deal with complex but natural user goals, such as disjunctive and negated goals and preferences. In particular, we explore restrictions imposed by current independence assumptions in POMDP dialogue models. This position paper proposes the use of Automatic Belief Compression methods to remedy these problems. 
We investigate the clariﬁcation strategies exhibited by a hybrid POMDP dialog manager based on data obtained from a phone-based user study. The dialog manager combines task structures with a number of POMDP policies each optimized for obtaining an individual concept. We investigate the relationship between dialog length and task completion. In order to measure the effectiveness of the clariﬁcation strategies, we compute concept precisions for two different mentions of the concept in the dialog: ﬁrst mentions and ﬁnal values after clariﬁcations and similar strategies, and compare this to a rulebased system on the same task. We observe an improvement in concept precision of 12.1% for the hybrid POMDP compared to 5.2% for the rule-based system. 
Statistical user simulation is a promising methodology to train and evaluate the performance of (spoken) dialog systems. We work with a modular architecture for data-driven simulation where the “intentional” component of user simulation includes a User Model representing userspeciﬁc features. We train a dialog simulator that combines traits of human behavior such as cooperativeness and context with domain-related aspects via the Expectation-Maximization algorithm. We show that cooperativeness provides a ﬁner representation of the dialog context which directly affects task completion rate. 
This paper presents a spoken dialogue framework that helps users in making decisions. Users often do not have a deﬁnite goal or criteria for selecting from a list of alternatives. Thus the system has to bridge this knowledge gap and also provide the users with an appropriate alternative together with the reason for this recommendation through dialogue. We present a dialogue state model for such decision making dialogue. To evaluate this model, we implement a trial sightseeing guidance system and collect dialogue data. Then, we optimize the dialogue strategy based on the state model through reinforcement learning with a natural policy gradient approach using a user simulator trained on the collected dialogue corpus. 
The present study explores the vocal intensity of turn-initial cue phrases in a corpus of dialogues in Swedish. Cue phrases convey relatively little propositional content, but have several important pragmatic functions. The majority of these entities are frequently occurring monosyllabic words such as “eh”, “mm”, “ja”. Prosodic analysis shows that these words are produced with higher intensity than other turn-initial words are. In light of these results, it is suggested that speakers produce these expressions with high intensity in order to claim the floor. It is further shown that the difference in intensity can be measured as a dynamic inter-speaker relation over the course of a dialogue using the end of the interlocutor’s previous turn as a reference point. 
 Dialog modeling in robotics suffers from lack of generalizability, due to the fact that the dialog is heavily inﬂuenced by the tasks the robot is able to perform. We introduce interleaving interaction patterns together with a general protocol for task communication which enables us to systematically specify the relationship between dialog structure and task structure. We argue that this approach meets the requirements of advanced dialog modeling on robots and at the same time exhibits a better scalability than existing concepts. 
When dialogue systems, through the use of incremental processing, are not bounded anymore by strict, nonoverlapping turn-taking, a whole range of additional interactional devices becomes available. We explore the use of one such device, trial intonation. We elaborate our approach to dialogue management in incremental systems, based on the Information-State-Update approach, and discuss an implementation in a microdomain that lends itself to the use of immediate feedback, trial intonations and expansions. In an overhearer evaluation, the incremental system was judged as signiﬁcantly more human-like and reactive than a non-incremental version. 
We build a model for speech disﬂuency detection based on conditional random ﬁelds (CRFs) using the Switchboard corpus. This model is then applied to a new domain without any adaptation. We show that a technique for detecting speech disﬂuencies based on Integer Linear Programming (ILP) (Georgila, 2009) signiﬁcantly outperforms CRFs. In particular, in terms of F-score and NIST Error Rate the absolute improvement of ILP over CRFs exceeds 20% and 25% respectively. We conclude that ILP is an approach with great potential for speech disﬂuency detection when there is a lack or shortage of indomain data for training. 
In this paper we present experiments related to the validation of spoken language understanding capabilities in a language and culture training system. In this application, word-level recognition rates are insufficient to characterize how well the system serves its users. We present the results of an annotation exercise that distinguishes instances of non-recognition due to learner error from instances due to poor system coverage. These statistics give a more accurate and interesting description of system performance, showing how the system could be improved without sacrificing the instructional value of rejecting learner utterances when they are poorly formed. 
We perform a study of existing dialogue corpora to establish the theoretical maximum performance of the selection approach to simulating human dialogue behavior in unseen dialogues. This maximum is the proportion of test utterances for which an exact or approximate match exists in the corresponding training corpus. The results indicate that some domains seem quite suitable for a corpusbased selection approach, with over half of the test utterances having been seen before in the corpus, while other domains show much more novelty compared to previous dialogues. 
Little research has been done to explore differences in the interactional aspects of dialogue between children with Autistic Spectrum Disorder (ASD) and those with typical development (TD). Quantifying the differences could aid in diagnosing ASD, understanding its nature, and better understanding the mechanisms of dialogue processing. In this paper, we report on a study of dialogues with children with ASD and TD. We ﬁnd that the two groups differ substantially in how long they pause before speaking, and their use of ﬁllers, acknowledgments, and discourse markers. 
We use automatically extracted acoustic features to detect speech which is generated under stress, achieving 76.24% accuracy with a binary logistic regression. Our data are task-oriented human-human dialogues in which a time-limit is unexpectedly introduced partway through. Analysis suggests that we can detect approximately when this event occurs. We also consider the importance of normalizing the acoustic features by speaker, and detecting stress in new speakers. 
 .T(arnannoscscrirpiptitoionn)  and of  semantic annotation utterances is crucial  part of speech performance analysis and  tuning of spoken dialog systems and other  natural language processing disciplines.  However, the fact that these are manual  tasks makes them expensive and slow. In  this paper, we will discuss how anno-  scription can be partially automated. We  will show that annoscription can reach a  throughput of 693 thousand utterances per  person month under certain assumptions.  
The Workbench for Intelligent exploraTion of Human ComputeR conversaTions is a new platform-independent open-source workbench designed for the analysis, mining and management of large spoken dialogue system corpora. What makes Witchcraft unique is its ability to visualize the effect of classiﬁcation and prediction models on ongoing system-user interactions. Witchcraft is now able to handle predictions from binary and multi-class discriminative classiﬁers as well as regression models. The new XML interface allows a visualization of predictions stemming from any kind of Machine Learning (ML) framework. We adapted the widespread CMU Let’s Go corpus to demonstrate Witchcraft. 
We present our Multi Point Of vieW Evaluation Reﬁnement Studio (MPOWERS), an application framework for Spoken Dialogue System evaluation that implements design conventions in a user-friendly interface. It ensures that all evaluator-users manipulate a unique shared corpus of data with a shared set of parameters to design and retrieve their evaluations. It therefore answers both the need for convergence among the evaluation practices and the consideration of several analytical points of view addressed by the evaluators involved in Spoken Dialogue System projects. After introducing the system architecture, we argue the solution’s added value in supporting a both data-driven and goal-driven process. We conclude with future works and perspectives of improvement upheld by human processes. 
In this paper we present a proposal for the development of dialog systems that, on the one hand, takes into account the beneﬁts of using standards like VoiceXML, whilst on the other, includes a statistical dialog module to avoid the effort of manually deﬁning the dialog strategy. This module is trained using a labeled dialog corpus, and selects the next system response considering a classiﬁcation process that takes into account the dialog history. Thus, system developers only need to deﬁne a set of VoiceXML ﬁles, each including a system prompt and the associated grammar to recognize the users responses to the prompt. We have applied this technique to develop a dialog system in VoiceXML that provides railway information in Spanish. 
This paper proposes a simple framework for building ’virtual networking agents’; programs that can communicate with users and collect information through the internet. These virtual agents can also communicate with each other to share information that one agent does not have. The framework - ’YouBot’ - provides basic functions such as protocol handling, authentication, and data storage. The behavior of the virtual agents is deﬁned by a task processor (’TP’) which can be written in a lightweight language such as JavaScript. It is very easy to add new functions to a virtual agent. The last part of this paper discusses the micro-blog system ’twitter’ and other web services as information sources that a virtual agent can utilise to make its behavior more suited to the user. 
This paper presents a dialogue system in the form of an ECA that acts as a sociable and emotionally intelligent companion for the user. The system dialogue is not task-driven but is social conversation in which the user talks about his/her day at the office. During conversations the system monitors the emotional state of the user and uses that information to inform its dialogue turns. The system is able to respond to spoken interruptions by the user, for example, the user can interrupt to correct the system. The system is already fully implemented and aspects of actual output will be used to illustrate. 
In this paper we propose a new technique to enhance emotion recognition by combining in different ways what we call emotion predictions. The technique is called F2 as the combination is based on a double fusion process. The input to the first fusion phase is the output of a number of classifiers which deal with different types of information regarding each sentence uttered by the user. The output of this process is the input to the second fusion stage, which provides as output the most likely emotional category. Experiments have been carried out using a previously-developed spoken dialogue system designed for the fast food domain. Results obtained considering three and two emotional categories show that our technique outperforms the standard single fusion technique by 2.25% and 3.35% absolute, respectively. 
We develop a method to detect erroneous interpretation results of user utterances by exploiting utterance histories of individual users in spoken dialogue systems that were deployed for the general public and repeatedly utilized. More speciﬁcally, we classify barge-in utterances into correctly and erroneously interpreted ones by using features of individual users’ utterance histories such as their barge-in rates and estimated automatic speech recognition (ASR) accuracies. Online detection is enabled by making these features obtainable without any manual annotation or labeling. We experimentally compare classiﬁcation accuracies for several cases when an ASR conﬁdence measure is used alone or in combination with the features based on the user’s utterance history. The error reduction rate was 15% when the utterance history was used. 
Classifying the dialogue act of a user utterance is a key functionality of a dialogue management system. This paper presents a data-driven dialogue act classifier that is learned from a corpus of human textual dialogue. The task-oriented domain involves tutoring in computer programming exercises. While engaging in the task, students generate a task event stream that is separate from and in parallel with the dialogue. To deal with this complex task-oriented dialogue, we propose a vector-based representation that encodes features from both the dialogue and the hierarchically structured task for training a maximum likelihood classifier. This classifier also leverages knowledge of the hidden dialogue state as learned separately by an HMM, which in previous work has increased the accuracy of models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 
The second person pronoun you serves different functions in English. Each of these different types often corresponds to a different term when translated into another language. Correctly identifying different types of you can be beneﬁcial to machine translation systems. To address this issue, we investigate disambiguation of different types of you occurrences in multiparty meetings with a new focus on the role of hand gesture. Our empirical results have shown that incorporation of gesture improves performance on differentiating between the generic use of you (e.g., refer to people in general) and the referential use of you (e.g., refer to a speciﬁc person or a group of people). Incorporation of gesture can also compensate for limitations in automated language processing (e.g., reliable recognition of dialogue acts) and achieve comparable results. 
In this paper, which addresses smooth spoken interaction between human users and conversational agents, we present an experimental study that evaluates a method for user-adaptive coordination of agent communicative behavior. Our method adapts the pause duration preceding agent utterances and the agent gaze duration to reduce the discomfort perceived by individual users during interaction. The experimental results showed a statistically signiﬁcant tendency: the duration of the agent pause and the gaze converged during interaction with the method. The method also signiﬁcantly improved the perceived relevance of the agent communicative behavior. 
A central problem in Interactive Question Answering (IQA) is how to answer Follow-Up Questions (FU Qs), possibly by taking advantage of information from the dialogue context. We assume that FU Qs can be classiﬁed into speciﬁc types which determine if and how the correct answer relates to the preceding dialogue. The main goal of this paper is to propose an empirically motivated typology of FU Qs, which we then apply in a practical IQA setting. We adopt a supervised machine learning framework that ranks answer candidates to FU Qs. Both the answer ranking and the classiﬁcation of FU Qs is done in this framework, based on a host of measures that include shallow and deep inter-utterance relations, automatically collected dialogue management meta information, and human annotation. We use Principal Component Analysis (PCA) to integrate these measures. As a result, we conﬁrm earlier ﬁndings about the beneﬁt of distinguishing between topic shift and topic continuation FU Qs. We then present a typology of FU Qs that is more ﬁne-grained, extracted from the PCA and based on real dialogue data. Since all our measures are automatically computable, our results are relevant for IQA systems dealing with naturally occurring FU Qs. 
This paper presents a comparison of two similar dialogue analysis tasks: segmenting real-life medical team meetings into patient case discussions, and segmenting scenario-based meetings into topics. In contrast to other methods which use transcribed content and prosodic features (such as pitch, loudness etc), the method used in this comparison employs only the duration of the prosodic units themselves as the basis for dialogue representation. A concept of Vocalisation Horizon (VH) allows us to treat segmentation as a classiﬁcation task where each instance to be classiﬁed is represented by the duration of a talk spurt, pause or speech overlap event in the dialogue. We report on the results this method yielded in segmentation of medical meetings, and on the implications of the results of further experiments on a larger corpus, the Augmented Multiparty Meeting corpus, to our ongoing efforts to support data collection and information retrieval in medical team meetings. 
Rating-scale evaluations are common in NLP, but are problematic for a range of reasons, e.g. they can be unintuitive for evaluators, inter-evaluator agreement and self-consistency tend to be low, and the parametric statistics commonly applied to the results are not generally considered appropriate for ordinal data. In this paper, we compare rating scales with an alternative evaluation paradigm, preferencestrength judgement experiments (PJEs), where evaluators have the simpler task of deciding which of two texts is better in terms of a given quality criterion. We present three pairs of evaluation experiments assessing text ﬂuency and clarity for different data sets, where one of each pair of experiments is a rating-scale experiment, and the other is a PJE. We ﬁnd the PJE versions of the experiments have better evaluator self-consistency and interevaluator agreement, and a larger proportion of variation accounted for by system differences, resulting in a larger number of signiﬁcant differences being found. 
This paper presents an easy-to-adapt, discourse-aware framework that can be utilized as the content selection component of a generation system whose goal is to deliver descriptive texts in several turns. Our framework involves a novel use of a graph-based ranking algorithm, to iteratively determine what content to convey to a given request while taking into account various considerations such as capturing a priori importance of information, conveying related information, avoiding redundancy, and incorporating the effects of discourse history. We illustrate and evaluate this framework in an accessibility system for sight-impaired individuals. 
In this paper we present a reference generation model based on Reference Domain Theory which gives a dynamic account of reference. This reference model assumes that each referring act both relies and updates the reference context. We present a formal deﬁnition of a reference domain, a generation algorithm and its instantiation in the GIVE challenge.  a reference domain but also a reference domain in a set of reference domains that we call here referential space. Moreover each referring act presupposes a given state of the referential space, and the explicit representation of these presuppositions as constraints on the suitable domain for interpretation or generation allows the implementation of a reversible reference module. We will focus here on generation. Details about the interpretation side of RDT can be found in (Salmon-Alt and Romary, 2001; Denis et al., 2006).  
We present a novel approach to natural language generation (NLG) that applies hierarchical reinforcement learning to text generation in the wayﬁnding domain. Our approach aims to optimise the integration of NLG tasks that are inherently different in nature, such as decisions of content selection, text structure, user modelling, referring expression generation (REG), and surface realisation. It also aims to capture existing interdependencies between these areas. We apply hierarchical reinforcement learning to learn a generation policy that captures these interdependencies, and that can be transferred to other NLG tasks. Our experimental results—in a simulated environment—show that the learnt wayﬁnding policy outperforms a baseline policy that takes reasonable actions but without optimization. 
We describe a method for assigning English tense and aspect in a system that realizes surface text for symbolically encoded narratives. Our testbed is an encoding interface in which propositions that are attached to a timeline must be realized from several temporal viewpoints. This involves a mapping from a semantic encoding of time to a set of tense/aspect permutations. The encoding tool realizes each permutation to give a readable, precise description of the narrative so that users can check whether they have correctly encoded actions and statives in the formal representation. Our method selects tenses and aspects for individual event intervals as well as subintervals (with multiple reference points), quoted and unquoted speech (which reassign the temporal focus), and modal events such as conditionals. 
This paper investigates the relationship between the results of an extrinsic, taskbased evaluation of an NLG system and various metrics measuring both surface and deep semantic textual properties, including relevance. The latter rely heavily on domain knowledge. We show that they correlate systematically with some measures of performance. The core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of a text and task performance. 
We present the situated reference generation module of a hybrid human-robot interaction system that collaborates with a human user in assembling target objects from a wooden toy construction set. The system contains a sub-symbolic goal inference system which is able to detect the goals and errors of humans by analysing their verbal and non-verbal behaviour. The dialogue manager and reference generation components then use situated references to explain the errors to the human users and provide solution strategies. We describe a user study comparing the results from subjects who heard constant references to those who heard references generated by an adaptive process. There was no difference in the objective results across the two groups, but the subjects in the adaptive condition gave higher subjective ratings to the robot’s abilities as a conversational partner. An analysis of the objective and subjective results found that the main predictors of subjective user satisfaction were the user’s performance at the assembly task and the number of times they had to ask for instructions to be repeated. 
In this paper, we propose a general way of constructing an NLG system that permits the systematic exploration of the effects of particular system choices on output quality. We call a system developed according to this model a Programmable Instrumented Generator (PIG). Although a PIG could be designed and implemented from scratch, it is likely that researchers would also want to create PIGs based on existing systems. We therefore propose an approach to “instrumenting” an NLG system so as to make it PIG-like. To experiment with the idea, we have produced code to support the “instrumenting” of any NLG system written in Java. We report on initial experiments with “instrumenting” two existing systems and attempting to “tune” them to produce text satisfying complex stylistic constraints. 
The semantic web is a general vision for supporting knowledge-based processing across the WWW and its successors. As such, semantic web technology has potential to support the exchange and processing of complex NLG data. This paper discusses one particular approach to data sharing and exchange that was developed for NLG – the RAGS framework. This was developed independently of the semantic web. RAGS was relatively complex and involved a number of idiosyncratic features. However, we present a rational reconstruction of RAGS in terms of semantic web concepts, which yields a relatively simple approach that can exploit semantic web technology directly. Given that RAGS was motivated by the concerns of the NLG community, it is perhaps remarkable that its aspirations seem to ﬁt so well with semantic web technology. 
This paper discusses the basic structures necessary for the generation of reference to objects in a visual scene. We construct a study designed to elicit naturalistic referring expressions to relatively complex objects, and ﬁnd aspects of reference that have not been accounted for in work on Referring Expression Generation (REG). This includes reference to object parts, size comparisons without crisp measurements, and the use of analogies. By drawing on research in cognitive science, neurophysiology, and psycholinguistics, we begin developing the input structure and background knowledge necessary for an algorithm capable of generating the kinds of reference we observe. 
In this paper we present a complete system for automatically generating natural language abstracts of meeting conversations. This system is comprised of components relating to interpretation of the meeting documents according to a meeting ontology, transformation or content selection from that source representation to a summary representation, and generation of new summary text. In a formative user study, we compare this approach to gold-standard human abstracts and extracts to gauge the usefulness of the different summary types for browsing meeting conversations. We ﬁnd that our automatically generated summaries are ranked signiﬁcantly higher than human-selected extracts on coherence and usability criteria. More generally, users demonstrate a strong preference for abstract-style summaries over extracts. 
The generation of referring expressions (GRE), an important subtask of Natural Language Generation (NLG) is to generate phrases that uniquely identify domain entities. Until recently, many GRE algorithms were developed using only simple formalisms, which were taylor made for the task. Following the fast development of ontology-based systems, reinterpretations of GRE in terms of description logic (DL) have recently started to be studied. However, the expressive power of these DL-based algorithms is still limited, not exceeding that of older GRE approaches. In this paper, we propose a DL-based approach to GRE that exploits the full power of OWL2. Unlike existing approaches, the potential of reasoning in GRE is explored. 
We present a framework for reformulating sentences by applying transfer rules on a typed dependency representation. We specify a list of operations that the framework needs to support and argue that typed dependency structures are currently the most suitable formalism for complex lexico-syntactic paraphrasing. We demonstrate our approach by reformulating sentences expressing the discourse relation of causation using four lexico-syntactic discourse markers – “cause” as a verb and as a noun, “because” as a conjunction and “because of” as a preposition. 
In the ﬁeld of referring expression generation, while in the static domain both intrinsic and extrinsic evaluations have been considered, extrinsic evaluation in the dynamic domain, such as in a situated collaborative dialog, has not been discussed in depth. In a dynamic domain, a crucial problem is that referring expressions do not make sense without an appropriate preceding dialog context. It is unrealistic for an evaluation to simply show a human evaluator the whole period from the beginning of a dialog up to the time point at which a referring expression is used. Hence, to make evaluation feasible it is indispensable to determine an appropriate shorter context. In order to investigate the context necessary to understand a referring expression in a situated collaborative dialog, we carried out an experiment with 33 evaluators and a Japanese referring expression corpus. The results contribute to ﬁnding the proper contexts for extrinsic evalution in dynamic domains. 
This paper proposes a method for extracting high-level rules for expository dialogue generation. The rules are extracted from dialogues that have been authored by expert dialogue writers. We examine the rules that can be extracted by this method, focusing on whether different dialogues and authors exhibit different dialogue styles. 
Fluency rankers are used in modern sentence generation systems to pick sentences that are not just grammatical, but also ﬂuent. It has been shown that feature-based models, such as maximum entropy models, work well for this task. Since maximum entropy models allow for incorporation of arbitrary real-valued features, it is often attractive to create very general feature templates, that create a huge number of features. To select the most discriminative features, feature selection can be applied. In this paper we compare three feature selection methods: frequency-based selection, a generalization of maximum entropy feature selection for ranking tasks with realvalued features, and a new selection method based on feature value correlation. We show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection, and that models with a few hundred well-picked features are competitive to models with no feature selection applied. In the experiments described in this paper, we compressed a model of approximately 490.000 features to 1.000 features.  is equal to the observed feature value in the training data. In its canonical form, the probability of a certain event (y) occurring in the context (x) is a loglinear combination of features and feature weights, where Z(x) is a normalization over all events in context x (Berger et al., 1996):  
Building NLG systems, in particular statistical ones, requires parallel data (paired inputs and outputs) which do not generally occur naturally. In this paper, we investigate the idea of automatically extracting parallel resources for data-to-text generation from comparable corpora obtained from the Web. We describe our comparable corpus of data and texts relating to British hills and the techniques for extracting paired input/output fragments we have developed so far. 
Critical software most often requires an independent validation and veriﬁcation (IVV). IVV is usually performed by domain experts, who are not familiar with speciﬁc, many times formal, development technologies. In addition, model-based testing (MBT) is a promising testing technique for the veriﬁcation of critical software. Test cases generated by MBT tools are logical descriptions. The problem is, then, to provide natural language (NL) descriptions of these test cases, making them accessible to domain experts. In this paper, we present ongoing research aimed at ﬁnding a suitable method for generating NL descriptions from test cases in a formal speciﬁcation language. A ﬁrst prototype has been developed and applied to a real-world project in the aerospace sector. 
 Today there exist a growing number of framenet-like resources offering semantic and syntactic phrase speciﬁcations that can be exploited by natural language generation systems. In this paper we present on-going work that provides a starting point for exploiting framenet information for multilingual natural language generation. We describe the kind of information offered by modern computational lexical resources and discuss how template-based generation systems can beneﬁt from them.  
We have begun a project to automatically create the lexico-syntactic resources for a microplanner as a side-effect of running a domain-specific language understanding system. The resources are parameterized synchronous TAG Derivation Trees. Since the KB is assembled from the information in the texts that these resources are abstracted from, it will decompose along those same lines when used for generation. As all possible ways of expressing each concept are pre-organized into general patterns known to be linguistically-valid (they were observed in natural text), we obtain an architectural account for expressibility. 1. Expressibility People speak grammatically. They may stutter, restart, or make the occasional speech error, but all in all they are faithful to the grammar of the language dialects they use. One of the ways that a language generation system can account for this is through the use of grammar that defines all of the possible lexico-syntactic elements from which a text can be constructed and defines all their rules of composition, such as lexicalized Tree Adjoining Grammar (TAG). Without the ability to even formulate an ungrammatical text, such a generator provides an account for human grammaticality based on its architecture rather than its programmer. We propose a similar kind of accounting for the problem of expressibility: one based on architecture rather than accident. Expressibility, as defined by Meteer (1992), is an issue for microplanners as they decide on which lexical and syntactic resources to employ. Not all of the options they might want to use are available in the language – they are not expressible. Consider the examples in Figure 1, adapted from Meteer 1992 pg. 50.  Expression  Construction (‘decide’)  “quick decision”  <result> + <quick>  “decide quickly”  <action> + <quick>  “important decision” <result> + <important>  * “decide importantly” <action> + <important>  Figure 1: Constraints on expressibility: To say that there was a decision and it was important, you are forced to use the noun form because there is no adverbial form for important as there is for quick  In this short paper, we discuss our approach to expressibility. We describe in detail our novel method centered on how to use parser observations to guide generator decisions, and we provide a snapshot of the current status of our system implementation.  2. Related Work  Natural language generation (NLG) systems must have some way of making sure that the messages they build are actually expressible. Template-based generators avoid problems with expressibility largely by anticipating all of the wording that will be needed and packaging it in chunks that are guaranteed to compose correctly. Becker (2006), for example, does this via fully lexicalized TAG trees. Among more general-purpose generators, one approach to expressibility is to look ahead into the lexicon, avoiding constructions that are lexically incompatible. Look-ahead is expensive, however, and is only practical at small abstraction distances such as Shaw’s re-writing sentence planner (1998). Meteer’s own approach to expressibility started by interposing another level of representation between the microplanner and the surface realizer, an ‘abstract syntactic representation’ in the sense of RAGS (Cahill et al. 1999), that employed functional relationships (head, argument, matrix, adjunct) over semantically typed,  lexicalized constituents. This blocks *decide importantly because ‘important’ only has a realization as a property and her composition rules prohibit using a property to modify an action (‘decide’). Shifting the perspective from the action to its result allows the composition to go through. We are in sympathy with this approach – a microplanner needs its own representational level to serve as a scratch pad (if using a revision-based approach) or just as a scaffold to hold intermediate results. However, Meteer’s semantic and lexical constraints do require operating with fine-grain details. We believe that we can work with larger chunks that have already been vetted for expressibility because we’ve observed someone use them, either in writing or speech. 3. Method Our approach is similar to that of Zhong & Stent (2005) in that we use the analysis of a corpus as the basis for creating the resources for the realization component. Several differences stand out. For one, we are working in specific domains rather than generic corpora like the WSJ. This enables the biggest difference: our analysis is performed by a completely accurate,1 domainspecific NLU system (‘parser’)2 based on a semantic grammar (McDonald 1993). It is reading for the benefit of a knowledge base, adding specific facts within instances of a highly structured, predefined prototypes. Such instances are used as the starting point for the generation process. On the KB side, our present focus happens to be on hurricanes and the process they go through as they evolve. We have developed a semantic grammar for this domain, and it lets us analyze texts like these:3 (1) “Later that day it made landfall near the Haitian town of Jacmel.” 
In this paper we describe a cross-linguistic experiment in attribute selection for referring expression generation. We used a graph-based attribute selection algorithm that was trained and cross-evaluated on English and Dutch data. The results indicate that attribute selection can be done in a largely language independent way. 
Ontologies and datasets for the Semantic Web are encoded in OWL formalisms that are not easily comprehended by people. To make ontologies accessible to human domain experts, several research groups have developed ontology verbalisers using Natural Language Generation. In practice ontologies are usually composed of simple axioms, so that realising them separately is relatively easy; there remains however the problem of producing texts that are coherent and efﬁcient. We describe in this paper some methods for producing sentences that aggregate over sets of axioms that share the same logical structure. Because these methods are based on logical structure rather than domain-speciﬁc concepts or language-speciﬁc syntax, they are generic both as regards domain and language. 
In this paper we investigate the automatic generation and evaluation of sentential paraphrases. We describe a method for generating sentential paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from Google News and a standard Phrase-Based Machine Translation (PBMT) framework. The output of this system is compared to a word substitution baseline. Human judges prefer the PBMT paraphrasing system over the word substitution system. We demonstrate that BLEU correlates well with human judgements provided that the generated paraphrased sentence is sufﬁciently different from the source sentence. 
The paper presents two models for producing and understanding situationally appropriate referring expressions (REs) during a discourse about large-scale space. The models are evaluated against an empirical production experiment. 
The ﬁrst Question Generation challenge consisted of three tasks: Task A required questions to be generated from paragraphs of texts; Task B required systems to generate questions from sentences, and Task C was an Open Task track in which any QG research involving evaluation could be submitted. At the time of going to press, the QG tasks are still running; this volume contains a preliminary report from the organisers. 
There were three GREC Tasks at Generation Challenges 2010: GREC-NER required participating systems to identify all people references in texts; for GRECNEG, systems selected coreference chains for all people entities in texts; and GRECFull combined the NER and NEG tasks, i.e. systems identiﬁed and, if appropriate, replaced references to people in texts. Five teams submitted 10 systems in total, and we additionally created baseline systems for each task. Systems were evaluated automatically using a range of intrinsic metrics. In addition, systems were assessed by human judges using preference strength judgements. This report presents the evaluation results, along with descriptions of the three GREC tasks, the evaluation methods, and the participating systems. 
The problem of Named Entity Generation is expressed as a conditional probability model over a structured domain. By deﬁning a factor-graph model over the mentions of a text, we obtain a compact parameterization of what is learned using the SampleRank algorithm. 
We describe our contribution to the Generation Challenge 2010 for the tasks of Named Entity Recognition and coreference detection (GREC-NER). To extract the NE and the referring expressions, we employ a combination of a Part of Speech Tagger and the Conditional Random Fields (CRF) learning technique. We ﬁnally experiment an original algorithm to detect co-references. We conclude with discussion about our system performances. 
 This paper presents the experiments carried out at Jadavpur University as part of the participation in the GREC Named Entity Generation Challenge 2010. The Baseline system is based on the SEMCAT, SYNCAT and SYNFUNC features of REF and REG08-TYPE and CASE features of REFEX elements. The discourse level system is based on the additional positional features: paragraph number, sentence number, word position in the sentence and mention number of a particular named entity in the document. The inclusion of discourse level features has improved the performance of the system.  
We present the UMUS (Universite´ du Maine/Universita¨t Stuttgart) submission for the NEG task at GREC’10. We reﬁned and tuned our 2009 system but we still rely on predicting generic labels and then choosing from the list of expressions that match those labels. We handled recursive expressions with care by generating speciﬁc labels for all the possible embeddings. The resulting system performs at a type accuracy of 0.84 an a string accuracy of 0.81 on the development set. 
This report describes the methods and results of a system developed for the GREC Named Entity Challenge 2010. We detail the reﬁnements made to our 2009 submission and present the output of the selfevaluation on the development data set. 
This report describes the methods and results of a system developed for the GREC Named Entity Recognition and GREC Named Entity Regeneration Challenges 2010. We explain our process of automatically annotating surface text, as well as how we use this output to select improved referring expressions for named entities. 
We invite the research community to consider challenges for NLG which arise from uncertainty. NLG systems should be able to adapt to their audience and the generation environment in general, but often the important features for adaptation are not known precisely. We explore generation challenges which could employ simulated environments to study NLG which is adaptive under uncertainty, and suggest possible metrics for such tasks. It would be particularly interesting to explore how different planning approaches to NLG perform in challenges involving uncertainty in the generation environment. 
The authors propose that we need some change for the current technology in Chinese word segmentation. We should have separate and different phases in the so-called segmentation. First of all, we need to limit segmentation only to the segmentation of Chinese characters instead of the so-called Chinese words. In character segmentation, we will extract all the information of each character. Then we start a phase called Chinese morphological processing (CMP). The first step of CMP is to do a combination of the separate characters and is then followed by post-segmentation processing, including all sorts of repetitive structures, Chinese-style abbreviations, recognition of pseudo-OOVs and their processing, etc. The most part of post-segmentation processing may have to be done by some rule-based sub-routines, thus we need change the current corpus-based methodology by merging with rule-based technique. 
Textual emotion recognition has gained a lot of attention recent years; it is however less developed due to the complexity nature of emotion. In this paper, we start with the discussion of a number of fundamental yet unresolved issues concerning emotion, which includes its definition, representation and technology. We then propose an alternative solution for emotion recognition taking into account of emotion causes. Two pilot experiments are done to justify our proposal. The first experiment explores the impact of emotion recognition. It shows that the context contains rich and crucial information that effectively help emotion recognition. The other experiment examines emotion cause events in the context. We find that most emotions are expressed with the presence of causes. The experiments prove that emotion cause serves as an important cue for emotion recognition. We suggest that the combination of both emotion study and event analysis would be a fruitful direction for deep emotion processing. 
Sentence segmentation is a fundamental issue in Classical Chinese language processing. To facilitate reading and processing of the raw Classical Chinese data, we propose a statistical method to split unstructured Classical Chinese text into smaller pieces such as sentences and clauses. The segmenter based on the conditional random field (CRF) model is tested under different tagging schemes and various features including n-gram, jump, word class, and phonetic information. We evaluated our method on four datasets from several eras (i.e., from the 5th century BCE to the 19th century). Our CRF segmenter achieves an F-score of 83.34% and can be applied on a variety of data from different eras. 
* This study is supported by National Natural Science Foundation of China, subject No. 60872121 1 With a purpose to show the structure of PClauses in Chinese, the translation of Chinese works may not appear very standard in English. The same applies hereinafter. 
As the proposition of the next-generation Web – semantic Web, semantic computing has been drawing more and more attention within the circle and the industries. A lot of research has been conducted on the theory and methodology of the subject, and potential applications have also been investigated and proposed in many fields. The progress of semantic computing made so far cannot be detached from its supporting pivot – language resources, for instance, language knowledge bases. This paper proposes three perspectives of semantic computing from a macro view and describes the current status of affairs about the construction of language knowledge bases and the related research and applications that have been carried out on the basis of these resources via a case study in the Institute of Computational Linguistics at Peking University.  
In this paper, we propose a novel similarity measure based on co-occurrence probabilities for inducing semantic classes. Clustering with the new similarity measure outperformed that with the widely used distance measure based on Kullback-Leibler divergence in precision, recall and F1 evaluation. We then use the induced semantic classes and structures by the new similarity measure to generate in-domain data. At last, we use the generated data to do language model adaptation and improve the result of character recognition from 85.2% to 91%. 
The main drawback of previous Chinese character error detection systems is the high false alarm rate. To solve this problem, we propose a system that combines a statistic method and template matching to detect Chinese character errors. Error types include pronunciationrelated errors and form-related errors. Possible errors of a character can be collected to form a confusion set. Our system automatically generates templates with the help of a dictionary and confusion sets. The templates can be used to detect and correct errors in essays. In this paper, we compare three methods proposed in previous works. The experiment results show that our system can reduce the false alarm significantly and give the best performance on fscore. 
This paper gives a new definition of Chinese clause called ”Event Descriptive Clause” and proposes an automatic method to identify these clauses in Chinese sentence. By analyzing the characteristics of the clause, the recognition task is formulated as a classification of Chinese punctuations. The maximum entropy classifier is trained and two kinds of useful features and their combinations are explored in the task. Meanwhile, a simple rule-based post processing phase is also proposed to improve the recognition performance. Ultimately, we obtain 81.32% F-score on the test set. 
This paper presents an unsupervised Chinese Part-of-Speech (POS) tagging model based on the ﬁrst-order HMM. Unlike the conventional HMM, the number of hidden states is not ﬁxed and will be increased to ﬁt the training data. In favor of sparse distribution, the Dirichlet priors are introduced with variational inference method. To reduce the emission variables, words are represented by their contexts and clustered based on the distributional similarities between contexts. Experiment results show the output state sequence of HMM are highly correlated to the latent annotations of gold POS tags, in context of clustering similarity measures. The other experiments on a real application, unsupervised dependency parsing, reveal that the output sequence can replace the manually annotated tags without loss of accuracies. 
In this paper, we demonstrate how to mine large-scale parallel corpora with multilingual patents, which have not been thoroughly explored before. We show how a large-scale English-Chinese parallel corpus containing over 14 million sentence pairs with only 1-5% wrong can be mined from a large amount of English-Chinese bilingual patents. To our knowledge, this is the largest single parallel corpus in terms of sentence pairs. Moreover, we estimate the potential for mining multilingual parallel corpora involving English, Chinese, Japanese, Korean, German, etc., which would to some extent reduce the parallel data acquisition bottleneck in multilingual information processing. 
The study on Automatic Recognizing usages of Modern Chinese Adverbs is one of the important parts of the NLP-oriented research of Chinese Functional words Knowledge Base. To solve the problems of the existing rule-based method of adverbs’ usages recognition based on the previous work, this paper has studied automatically recognizing common Chinese adverbs’ usages using statistical methods. Three statistical models, viz. CRF, ME, and SVM, are used to label several common Chinese adverbs’ usages on the segmentation and part-of-speech tagged corpus of People’s Daily(Jan 1998). The experiment results show that statisticalbased method is effective in automatically recognizing of several common adverbs’ usages and has good application prospects. 
We propose an effective approach to automatically identify predicate heads in Chinese sentences based on statistical pre-processing and rule-based post-processing. In the preprocessing stage, the maximal noun phrases in a sentence are recognized and replaced by “NP” labels to simplify the sentence structure. Then a CRF model is trained to recognize the predicate heads of this simplified sentence. In the post-processing stage, a rule base is built according to the grammatical features of predicate heads. It is then utilized to correct the preliminary recognition results. Experimental results show that our approach is feasible and effective, and its accuracy achieves 89.14% on Tsinghua Chinese Treebank. 
Conditional Random Fields (CRFs) are the state-of-the-art models for sequential labeling problems. A critical step is to select optimal feature template subset before employing CRFs, which is a tedious task. To improve the efficiency of this step, we propose a new method that adopts the maximum entropy (ME) model and maximum entropy Markov models (MEMMs) instead of CRFs considering the homology between ME, MEMMs, and CRFs. Moreover, empirical studies on the efficiency and effectiveness of the method are conducted in the field of Chinese text chunking, whose performance is ranked the first place in task two of CIPS-ParsEval-2009. 
Existing methods for extracting features from Chinese reviews only use simplistic syntactic knowledge, while those for identifying sentiments rely heavily on a semantic dictionary. In this paper, we present a systematic technique for identifying features and sentiments, using both syntactic and statistical analysis. We firstly identify candidate features using a proposed set of common syntactic rules. We then prune irrelevant candidates with topical relevance scores below a cut-off point. We also propose an association analysis method based on likelihood ratio test to infer the polarity of opinion word. The sentiment of a feature is finally adjusted by analyzing the negative modifiers in the local context of the opinion word. Experimental results show that our system performs significantly better than a well-known opinion mining system. 
Relation extraction is a fundamental task in information extraction that identifies the semantic relationships between two entities in the text. In this paper, a novel model based on Deep Belief Network (DBN) is first presented to detect and classify the relations among Chinese entities. The experiments conducted on the Automatic Content Extraction (ACE) 2004 dataset demonstrate that the proposed approach is effective in handling high dimensional feature space including character N-grams, entity types and the position information. It outperforms the stateof-the-art learning models such as SVM or BP neutral network. 
This paper presents a weakly-supervised method for Chinese sentiment analysis by incorporating lexical prior knowledge obtained from English sentiment lexicons through machine translation. A mechanism is introduced to incorporate the prior information about polaritybearing words obtained from existing sentiment lexicons into latent Dirichlet allocation (LDA) where sentiment labels are considered as topics. Experiments on Chinese product reviews on mobile phones, digital cameras, MP3 players, and monitors demonstrate the feasibility and effectiveness of the proposed approach and show that the weakly supervised LDA model performs as well as supervised classiﬁers such as Naive Bayes and Support vector Machines with an average of 83% accuracy achieved over a total of 5484 review documents. Moreover, the LDA model is able to extract highly domain-salient polarity words from text. 
This paper investigates techniques to automatically construct training data from social Q&A collections such as Yahoo! Answer to support a machine learningbased complex QA system1. We extract cue expressions for each type of question from collected training data and build question-type-speciﬁc classiﬁers to improve complex QA system. Experiments on 10 types of complex Chinese questions verify that it is effective to mine knowledge from social Q&A collections for answering complex questions, for instance, the F3 improvement of our system over the baseline and translationbased model reaches 7.9% and 5.1%, respectively. 
This paper reports on a treebanking project where eight different modern Chinese translations of the Bible are syntactically analyzed. The trees are created through dynamic treebanking which uses a parser to produce the trees. The trees have been going through manual checking, but corrections are made not by editing the tree files but by re-generating the trees with an updated grammar and dictionary. The accuracy of the treebank is high due to the fact that the grammar and dictionary are optimized for this specific domain. The tree structures essentially follow the guidelines of the Penn Chinese Treebank. The total number of characters covered by the treebank is 7,872,420 characters. The data has been used in Bible translation and Bible search. It should also prove useful in the computational study of the Chinese language in general. 
An approach to recognizing sentiment polarity in Chinese reviews based on topic sentiment sentences is presented. Considering the features of Chinese reviews, we firstly identify the topic of a review using an n-gram matching approach. To extract candidate topic sentiment sentences, we compute the semantic similarity between a given sentence and the ascertained topic and meanwhile determine whether the sentence is subjective. A certain number of these sentences are then selected as representatives according to their semantic similarity value with relation to the topic. The average value of the representative topic sentiment sentences is calculated and taken as the sentiment polarity of a review. Experiment results show that the proposed method is feasible and can achieve relatively high precision. 
In recent years, more and more CJK (Chinese, Japanese, and Korean) web pages appear in the Internet. The information in the CJK web page also becomes more and more important. Web crawler is a kind of tool to retrieve web pages. Previous researches focused on English web crawlers and the web crawler is always optimized for English web pages. We found that the performance of the web crawler is worse in retrieving CJK web pages. We tried to enhance the performance of the CJK crawler by analyzing the web link structure, anchor text, and host name on the hyperlink and changing the crawling algorithm. We distinguish the top-level domain name and the language of the anchor text on hyperlinks. The method that distinguishes the language of the anchor text on hyperlinks is not used on CJK language specific crawler by other researches. Control experiment is used in this research. According to the experimental results, when the target crawling language is Japanese, the 87% of the crawled web pages are Japanese web pages and improves the efficiency about 0.24% compares to the baseline results. When the target crawling language is Chinese, the 88% of the crawled web pages are Chinese web pages and improves the efficiency about 0.07% compares to the baseline results. When the target crawling language is Korean, the 71% of the crawled web pages are Korean web pages and improves the effi-  ciency about 10% compares to the baseline results. 
Opinion Mining aims to automatically acquire useful opinioned information and knowledge in subjective texts. Research of Chinese Opinioned Mining requires the support of annotated corpus for Chinese opinioned-subjective texts. To facilitate the work of corpus annotators, this paper implements an active learning based annotation tool for Chinese opinioned elements which can identify topic, sentiment, and opinion holder in a sentence automatically. 
Character-based tagging method has achieved great success in Chinese Word Segmentation (CWS). This paper proposes a new approach to improve the CWS tagging accuracy by combining Self-Organizing Map (SOM) with structured support vector machine (SVM) for utilization of enormous unlabeled text corpus. First, character N-grams are clustered and mapped into a low-dimensional space by adopting SOM algorithm. Two different maps are built based on the N-gram’s preceding and succeeding context respectively. Then new features are extracted from these maps and integrated into the structured SVM methods for CWS. Experimental results on Bakeoff-2005 database show that SOM-based features can contribute more than 7% relative error reduction, and the structured SVM method for CWS proposed in this paper also outperforms traditional conditional random field (CRF) method. 
Modern Chinese Mandarin has gone through near a hundred years, it is very important to store its representative sample in digital form permanently. In this paper, we propose a Chinese Mandarin Digital Multi-modal Corpus (CMDMC), which is a digital speech museum with diachronic, opened, crossmedia and sharable features. It has over 3460 hours video and audio files with metadata tagging. The materials, which were generated by the authoritative speakers (e.g. announcers at TV or radio station) with normality, are required samples if we can get them. Based on this resource, we also intend to analyze the syntactic correlations of prosodic phrase in broadcasting news speech, and compare the phonetic and prosodic features in movie dialogues among several same-name movies in different historical eras. 
This paper focuses on the automatic segmentation of inflectional affixes of the Kazakh Language (KL) on the basis of studying the corpus of KL. Kazakh is an agglutinative language with word structures formed by productive affixation of derivational and inflectional suffixes to stems. Based on the analysis of the configuration of inflectional affixes, it firstly constructs the Finite-State Automation and the segmentation of inflectional affixes. Secondly it targets at specially constructing the Finite-State Automations of nouns and verbs, which are the most changeable and complex part of speech of KL. And thirdly it adopts the methods of Bidirectional Omni-Word Segmentation and lexical analysis to achieve the goal of stemming and fine segmentation of inflectional affixes of KL. And finally it gives an additional account of studying the segmentation of ambiguous inflectional affixes. The paper intends to improve the accuracy and the quickness of stemming the inflectional affixes of KL. 
Space characters can have an important role in disambiguating text. However, few, if any, Chinese information extraction systems make full use of space characters. However, it seems that treatment of space characters is necessary, especially in cases of extracting information from semi-structured documents. This investigation aims to address the importance of space characters in Chinese information extraction by parsing some semi-structured documents with two similar grammars - one with treatment for space characters, the other ignoring it. This paper also introduces two post processing ﬁlters to further improve treatment of space characters. Results show that the grammar that takes account of spaces clearly out-performs the one that ignores them, and so concludes that space characters can play a useful role in information extraction. 
The CIPS-SIGHAN CLP 2010 Chinese Word Segmentation Bakeoff was held in the summer of 2010 to evaluate the current state of the art in word segmentation. It focused on the crossdomain performance of Chinese word segmentation algorithms. Eighteen groups submitted 128 results over two tracks (open training and closed training), four domains (literature, computer science, medicine and finance) and two subtasks (simplified Chinese and traditional Chinese). We found that compared with the previous Chinese word segmentation bakeoffs, the performance of cross-domain Chinese word segmentation is not much lower, and the out-of-vocabulary recall is improved. 
State-of-the-art Chinese word segmentation systems have achieved high performance when training data and testing data are from the same domain. However, they suffer from the generalizability problem when applied on test data from different domains. We introduce a multi-layer Chinese word segmentation system which can integrate the outputs from multiple heterogeneous segmentation systems. By training a second layer of large margin classiﬁer on top of the outputs from several Conditional Random Fields classiﬁers, it can utilize a small amount of in-domain training data to improve the performance. Experimental results show consistent improvement on F1 scores and OOV recall rates by applying the approach. 
This paper presents a Chinese word segmentation system for CIPS-SIGHAN 2010 Chinese language processing task. Firstly, based on Conditional Random Field (CRF) model, with local features and global features, the character-based tagging model is designed. Secondly, Hidden Markov Models (HMM) is used to revise the substrings with low marginal probability by CRF. Finally, confidence measure is used to regenerate the result and simple rules to deal with the strings within letters and numbers. As is well known that character-based approach has outstanding capability of discovering out-of-vocabulary (OOV) word, but external information of word lost. HMM makes use of word information to increase in-vocabulary (IV) recall. We participate in the simplified Chinese word segmentation both closed and open test on all four corpora, which belong to different domains. Our system achieves better performance. 
We have participated in the open tracks and closed tracks on four corpora of Chinese word segmentation tasks in CIPSSIGHAN-2010 Bake-offs. In our experiments, we used the Chinese inner phonology information in all tracks. For open tracks, we proposed a double hidden layers’ HMM (DHHMM) in which Chinese inner phonology information was used as one hidden layer and the BIO tags as another hidden layer. N-best results were ﬁrstly generated by using DHHMM, then the best one was selected by using a new lexical statistic measure. For close tracks, we used CRF model in which the Chinese inner phonology information was used as features. 
Character-based tagging method has achieved great success in Chinese Word Segmentation (CWS). This paper proposes a new approach to improve the CWS tagging accuracy by structured support vector machine (SVM) utilization of unlabeled text corpus. First, character N-grams in unlabeled text corpus are mapped into low-dimensional space by adopting SOM algorithm. Then new features extracted from these maps and another kind of feature based on entropy for each N-gram are integrated into the structured SVM methods for CWS. We took part in two tracks of the Word Segmentation for Simplified Chinese Text in bakeoff-2010: Closed track and Open track. The test corpora cover four domains: Literature, Computer Science, Medicine and Finance. Our system achieved good performance, especially in the open track on the domain of medicine, our system got the highest score among 18 systems. 
This paper describes our participation in the Chinese word segmentation task of CIPS-SIGHAN 2010. We implemented an n-gram mutual information (NGMI) based segmentation algorithm with the mixed-up features from unsupervised, supervised and dictionarybased segmentation methods. This algorithm is also combined with a simple strategy for out-of-vocabulary (OOV) word recognition. The evaluation for both open and closed training shows encouraging results of our system. The results for OOV word recognition in closed training evaluation were however found unsatisfactory. 
In this paper, we describe our system1 for CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which focused on the cross-domain performance of Chinese word segmentation algorithms. We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word segmentation. 
This paper presents a Chinese Word Segmentation system for the closed track of CIPS-SIGHAN Word Segmentation Bakeoff 2010. This system adopts a character-based joint approach, which combines a character-based generative model and a character-based discriminative model. To further improve the crossdomain performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems. 
 With development in Chinese words  segmentation, in-vocabulary word  segmentation and named entity  recognition achieves state-of-art  performance. However, new words  become bottleneck to Chinese word  segmentation. This paper presents the  result from Beijing Institute of  Technology (BIT) in the Sixth  International  Chinese  Word  Segmentation Bakeoff in 2010. Firstly,  the author reviewed the problem caused  by the new words in Chinese texts, then  introduced the algorithm of new words  detection. The final section provided  the official evaluation result in this  bakeoff and gave conclusions.  
For the competition of Chinese word segmentation held in the ﬁrst CIPS-SIGHNA joint conference. We applied a subwordbased word segmenter using CRFs and extended the segmenter with OOV words recognized by Accessor Variety. Moreover, we proposed several post-processing rules to improve the performance. Our system achieved promising OOV recall among all the participants. 
We participate in the CIPS-SIGHAN2010 bake-off task of Chinese word segmentation. Unlike the previous bakeoff series, the purpose of the bakeoff 2010 is to test the crossdomain performance of Chinese segmentation model. This paper summarizes our approach and our bakeoff results. We mainly propose to use χ2 statistics to increase the OOV recall and use bootstrapping strategy to increase the overall F score. As the results shows, the approach proposed in the paper does help, both of the OOV recall and the overall F score are improved. 
This paper describes our experiments on the cross-domain Chinese word segmentation task at the first CIPS-SIGHAN Joint Conference on Chinese Language Processing. Our system is based on the Conditional Random Fields (CRFs) model. Considering the particular properties of the out-of-domain data, we propose some novel steps to get some improvements for the special task. 
This paper presents a Chinese word segmentation system submitted to the closed training evaluations of CIPSSIGHAN-2010 bakeoff. The system uses a conditional random field model with one simple feature called term contributed boundaries (TCB) in addition to the “BI” character-based tagging approach. TCB can be extracted from unlabeled corpora automatically, and segmentation variations of different domains are expected to be reflected implicitly. The experiment result shows that TCB does improve “BI” tagging domainindependently about 1% of the F1 measure score. 
This paper describes the Chinese Word Segmenter for our participation in CIPSSIGHAN-2010 bake-off task of Chinese word segmentation. We formalize the tasks as sequence tagging problems, and implemented them using conditional random fields (CRFs) model. The system contains two modules: multiple preprocessor and basic segmenter. The basic segmenter is designed as a problem of character-based tagging, and using named entity recognition and chunk recognition based on boundary to preprocess. We participated in the open training on Simplified Chinese Text and Traditional Chinese Text, and our system achieved one Rank#5 and four Rank#2 best in all four domain corpus. 
We present a Chinese word segmentation system which ran on the closed track of the simplified Chinese Word Segmentation task of CIPS-SIGHAN-CLP 2010 bakeoffs. Our segmenter was built using a HMM. To fulfill the cross-domain segmentation task, we use semi-supervised machine learning method to get the HMM model. Finally we get the mean result of four domains: P=0.719, R=0.72 
In this paper, we present the proposed method of participating SIGHAN-2010 Chinese word segmentation bake-off. In this year, our focus aims to quick train and test the given data. Unlike the most structural learning algorithms, such as conditional random fields, we design an in-house development conditional support vector Markov model (CMM) framework. The method is very quick to train and also show better performance in accuracy than CRF. To give a fair comparison, we compare our method to CRF with three additional tasks, namely, CoNLL-2000 chunking, SIGHAN-3 Chinese word segmentation. The results were encourage and indicated that the proposed CMM produces better not only accuracy but also training time efficiency. The official results in SIGHAN-2010 also demonstrates that our method perform very well in traditional Chinese with fine-tuned features set. 
Chinese word segmentation is the initial step for Chinese information processing. The performance of Chinese word segmentation has been greatly improved by character-based approaches in recent years. This approach treats Chinese word segmentation as a character-wordposition-tagging problem. With the help of powerful sequence tagging model, character-based method quickly rose as a mainstream technique in this field. This paper presents our segmentation system for evaluation of CIPS-SIGHAN 2010 in which method combining character-based and subsequence-based tagging is applied and conditional random fields (CRFs) is taken as sequence tagging model. We evaluated our system in closed and open tracks on four corpuses, namely Literary, Computer science, Medicine and Finance, and reported our evaluation results. 
The paper introduced the task designing ideas, data preparation methods, evaluation metrics and results of the second Chinese syntactic parsing evaluation (CIPS-Bakeoff-ParsEval-2010) jointed with SIGHAN Bakeoff tasks. 
Discriminative parse reranking has been shown to be an effective technique to improve the generative parsing models. In this paper, we present a series of experiments on parsing the Tsinghua Chinese Treebank with hierarchically split-merge grammars and reranked with a perceptronbased discriminative model. In addition to the homogeneous annotation on TCT, we also incorporate the PCTB-based parsing result as heterogeneous annotation into the reranking feature model. The reranking model achieved 1.12% absolute improvement on F1 over the Berkeley parser on a development set. The head labels in Task 2.1 are annotated with a sequence labeling model. The system achieved 80.32 (B+C+H F1) in CIPS-SIGHAN2010 Task 2.1 (Open Track) and 76.11 (Overall F1) in Task 2.2 (Open Track)1. 
This paper presents our work for participation in the 2010 CIPS-SIGHAN evaluation on two tasks which are Event Description Sub-sentence (EDSs) Analysis and Complete Sentence (CS) Parsing in Chinese Parsing. The paper describes the implementation of our system as well as the results we have achieved and the analysis. 
This paper gives an overview of China Center for Information Industry Development(CCID) participating in the 2th Evaluation on Chinese parsing. CCID has taken part in the subtask of the analysis of complete sentences. The system participating in the above Evaluation is a rule-based Chinese parser, and its basic information is described in the paper, and its experimental situation for the Evaluation has been analyzed. 
Chinese parsing has received more and more attention, and in this paper, we use toolkit to perform parsing on the data of Tsinghua Chinese Treebank (TCT) used in CIPS, and we use Conditional Random Fields (CRFs) to train specific model for the head recognition. At last, we compare different results on different POS results. 
In this paper, we propose a novel selftraining strategy for parsing which is based on Treebank conversion (SSPTC). In SSPTC, we make full use of the strong points of Treebank conversion and self-training, and offset their weaknesses with each other. To provide good parse selection strategies which are needed in self-training, we score the automatically generated parse trees with parse trees in source Treebank as a reference. To maintain the constituency between source Treebank and conversion Treebank which is needed in Treebank conversion, we get the conversion trees with the help of self-training. In our experiments, SSPTC strategy is utilized to parse Tsinghua Chinese Treebank with the help of Penn Chinese Treebank. The results significantly outperform the baseline parser. 
We present a new probabilistic model based on the lexical PCFG model, which can easily utilize the Chinese character information to solve the lexical information sparseness in lexical PCFG model. We discuss in particular some important features that can improve the parsing performance, and describe the strategy of modifying original label structure to reduce the label ambiguities. Final experiment demonstrates that the character information and label modiﬁcation improve the parsing performance. 
This paper describes a complete syntactic analysis system based on multi-level chunking. On the basis of the correct sequences of Chinese words provided by CLP2010, the system firstly has a Part-ofspeech (POS) tagging with Conditional Random Fields (CRFs), and then does the base chunking and complex chunking with Maximum Entropy (ME), and finally generates a complete syntactic analysis tree. The system took part in the Complete Sentence Parsing Track of the Task 2 Chinese Parsing in CLP2010, achieved the F-1 measure of 63.25% on the overall analysis, ranked the sixth; POS accuracy rate of 89.62%, ranked the third. 
Personal name disambiguation becomes hot as it provides a way to incorporate semantic understanding into information retrieval. In this campaign, we explore Chinese personal name disambiguation in news. In order to examine how well disambiguation technologies work, we concentrate on news articles, which is well-formatted and whose genre is well-studied. We then design a diagnosis test to explore the impact of Chinese word segmentation to personal name disambiguation. 
This paper presents our systems for the participation of Chinese Personal Name Disambiguation task in the CIPSSIGHAN 2010. We submitted two different systems for this task, and both of them all achieve the best performance. This paper introduces the multi-stage clustering framework and some key techniques used in our systems, and demonstrates experimental results on evaluation data. Finally, we further discuss some interesting issues found during the development of the system. 
This paper presents the HITSZ_CITYU system in the CIPS-SIGHAN bakeoff 2010 Task 3, Chinese person name disambiguation. This system incorporates person name string recognition, person identity string recognition and an agglomerative hierarchical clustering for grouping the documents to each identical person. Firstly, for the given name index string, three segmentors are applied to segment the sentences having the index string into Chinese words, respectively. Their outputs are compared and analyzed. An unsupervised clustering is applied here to help the personal name recognition. The document set is then divided into subsets according to each recognized person name string. Next, the system identifies/extracts the person identity string from the sentences based on lexicon and heuristic rules. By incorporating the recognized person identity string, person name, organization name and contextual content words as features, an agglomerative hierarchical clustering is applied to group the similar documents in the document subsets to obtain the final person name disambiguation results. Evaluations show that the proposed system, which incorporates extraction and clustering technique, achieves encouraging recall and good overall performance.  
In this paper, we describe our system for Chinese personal name disambiguation task in the ﬁrst CIPSSIGHAN joint conference on Chinese Language Processing(CLP2010). We use a pipeline approach, in which preprocessing, unrelated documents discarding, Chinese personal name extension and document clustering are performed separately. Chinese personal name extension is the most important part of the system. It uses two additional dictionaries to extract full personal names in Chinese text. And then document clustering is performed under diﬀerent personal names. Experimental results show that our system can achieve good performances. 
 2 Document preprocessing  This report presents the work of our group in the Chinese personal name disambiguation workshop. We propose a system which uses a HAC algorithm to cluster the mentions referring to the same person with features extracted from the documents. 
 This document presents the bakeoff results of Chinese personal name in the First CIPS-SIGHAN Joint Conference on Chinese Language Processing. The authors introduce the frame of person disambiguation system LJPD, which uses a new person model. LJPD was built in short time, and it is not given enough training and adjustment. Evaluation on LJPD shows that the precision is competitive, but the recall is very low. It has more space for further improvement.  Figure 1 Step of Person Disambiguation As illustrated in figure 1, the whole system addresses four problems: personal name recognition, anaphora resolution of personal name, person model creation and clustering.  
In this paper, we describe a Chinese person name disambiguation system for news articles and report the results obtained on the data set of the CLP 2010 Bakeoff-31. The main task of the Bakeoff is to identify different persons from the news stories that contain the same person-name string. Compared to the traditional methods, two additional features are used in our system: 1) n-grams co-occurred with target name string; 2) Jumping distance among the n-grams. On the basis, we propose a two-stage clustering algorithm to improve the low recall. 
With the rapid development of Internet and many related technology, Web has become the main source of information. For many search engines, there are many different identities in the returned results of character information query. Thus the Research of People disambiguation is important. In this paper we attempt to solve this problem by combing different knowledge. As people usually have different kind of careers, so we first utilize this knowledge to classify people roughly. Then we use social context of people to identify different person. The experimental results show that these knowledge are helpful for people disambiguation.  different people. Generally if two people’s name always occurs in same document or very near context ,they will have close relations, one of them will be helpful for disambiguate the other. In this paper, we first use the domain of character’s document to classify roughly, and then context information using social networking is considered again to disambiguate person’s name again. 2 the principle of our system Fig.1. shows the basic principle of our system. The basic steps are:  
In this paper we describe a person clustering system for a given document set and report the results we have obtained on the test set of Chinese personal name (CPN) disambiguation task of CIPSSIGHAN 2010. This task consists of clustering a set of Xinhua news documents that mention an ambiguous CPN according to named entity in reality. Several features including named entities (NE) and common nouns generated from the documents and a variety of rules are employed in our system. This system achieves F = 86.36% with B_Cubed scoring metrics and F = 90.78% with purity_based metrics.  In this paper we employ a CPN disambiguation system that extracts NE and common nouns from the input corpus as features, and then computes the similarity of each two documents in the corpus based on feature vector. Hierarchical Agglomerative Clustering (HAC) algorithm (AK Jain et al., 1999) is used to implement clustering. After a great deal of analysis of news corpus, we constitute several rules, the experiments show that these rules can improve the result of this task. The remainder of this paper is organized as follows. Section 2 introduces the preprocessing of test corpus, and in section 3 we present the methodology of our system. In section 4 we present the experimental results and give a conclusion in section 5. 2 Preprocessing  
In this paper we describe our participation in the SIGHAN 2010 Task3 (Person Name Disambiguation) and detail our approaches. Person Name Disambiguation is typically viewed as an unsupervised clustering problem where the aim is to partition a name’s contexts into different clusters, each representing a real world people. The key point of Clustering is the similarity measure of context, which depends upon the features selection and representation. Two clustering algorithms, HAC and DBSCAN, are investigated in our system. The experiments show that the topic features learned by LDA outperforms token features and more robust. 
The more Chinese language materials come out, the more we have to focus on the “same personal name” problem. In our personal name disambiguation system, the hierarchical agglomerative clustering is applied, and named entity is used as feature for document similarity calculation. We propose a two-stage strategy in which the first stage involves word segmentation and named entity recognition (NER) for feature extraction, and the second stage focuses on clustering. 
In this paper, we describe the Chinese word sense induction task at CLP2010. Seventeen teams participated in this task and nineteen system results were submitted. All participant systems are evaluated on a dataset containing 100 target words and 5000 instances using the standard cluster evaluation. We will describe the participating systems and the evaluation results, and then find the most suitable method by comparing the different Chinese word sense induction systems. 
Word Sense Induction (WSI) is an important topic in natural langage processing area. For the bakeoff task Chinese Word Sense Induction (CWSI), this paper proposes two systems using basic clustering algorithms, k-means and agglomerative clustering. Experimental results show that k-means achieves a better performance. Based only on the data provided by the task organizers, the two systems get FScores of 0.7812 and 0.7651 respectively. 
This paper describes the implementation of our system at CLP 2010 bakeoﬀ of Chinese word sense induction. We ﬁrst extract the triplets for the target word in each sentence, then use the intersection of all related words of these triplets from the Internet. We use the related word to construct feature vectors for the sentence. At last we discriminate the word senses by clustering the sentences. Our system achieved 77.88% F-score under the ofﬁcial evaluation. 
In this paper, we describe the implementation of an unsupervised learning method for Chinese word sense induction in CIPS-SIGHAN-2010 bakeoff. We present three individual clustering algorithms and the ensemble of them, and discuss in particular different approaches to represent text and select features. Our main system based on cluster ensemble achieves 79.33% in F-score, the best result of this WSI task. Our experiments also demonstrate the versatility and effectiveness of the proposed model on data sparseness problems. 
This paper presents the Chinese word sense Induction system of Leshan Teachers’ College. The system participates in the Chinese word sense Induction of task 4 in Back offs organized by the Chinese Information Processing Society of China (CIPS) and SIGHAN. The system extracts neighbor words and their POSs centered in the target words and selected the best one of four cluster algorithms: Simple KMeans, EM, Farthest First and Hierarchical Cluster based on training data. We obtained the F-Score of 60.5% on the training data otherwise the F-Score is 57.89% on the test data provided by organizers. 1. Introduction Automatically obtain the intended sense of polysemous words according to its context has been shown to improve performance in information retrieval、information extraction and machine translation. There are two ways to resolve this problem in view of machine learning, one is supervised classification and the other is unsupervised classification i.e. clustering. The former is word sense disambiguation (WSD) which relies on large scale, high quality manually annotated sense  corpus, but building a sense-annotated corpus is a time-consuming and expensive project. Even the corpus were constructed, the system trained from this corpus show the low performance on different domain test corpus. The later is word sense induction (WSI) which needs not any training data, and it has become one of the most important topics in current computational linguistics. Chinese Information Processing Society of China (CIPS) and SIGHAN organized a task is intended to promote the research on Chinese WSI. We built a WSI system named LSTC-WSI system for this task. This system tried four cluster algorithms, i.e. Simple KMeans、EM、Farthest First and Hierarchical Cluster implemented by weak 3.7.1 [6], and found Simple KMeans compete the other three ones according to their performances on training data. Finally, the results returned by Simple KMeans were submitted. 2. Features Selection Following the feature selection in word sense disambiguation, we extract neighbor words and their POSs centered in the target words. Word segmented and POS-tag tool adapted Chinese Lexical Analysis System developed by Institute of Computing Technology. No other resource is used in the system. The window size of the context is set to 5 around the ambiguous word. The neighbor words which occur only once  were removed. Each sample is represented as a vector, and feature form is binary: if it occurs in is 1 otherwise is 0. 3. Clusters Algorithms Four cluster algorithms were tried in our system. I will introduce them simply in the next respectively. K-means clustering [1] is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The main idea is to define k centroids, one for each cluster. These centroids should be placed in a cunning way because of different location causes different result. So, the better choice is to place them as much as possible far away from each other. EM algorithm[2] is a method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. EM is an iterative method which alternates between performing an expectation (E) step, which computes the expectation of the log-likelihood evaluated using the current estimate for the latent variables, and maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. The Farthest First algorithm [3] is an implementation of the “Farthest First Traversal Algorithm” by Hochbaum and Shmoys (1985). It finds fast, approximate clusters and may be useful as an initialiser for k-means. A hierarchical clustering [4] is the guarantee that for every k, the induced k clustering has cost at most eight times that of the optimal k-clustering. A hierarchical clustering of n data points is a recursive partitioning of the data into 2, 3, 4, . . . and finally n, clusters. Each intermediate clustering is made more fine-grained by dividing one of its clusters.  4. Development  4.1 Evaluation method  We consider the gold standard as a solution to the clustering problem. All examples tagged with a given sense in the gold standard form a class. For the system output, the clusters are formed by instances assigned to the same sense tag. We will compare clusters output by the system with the classes in the gold standard and compute F-score as usual [5]. F-score is computed with the formula below. Suppose Cr is a class of the gold standard, and Si is a cluster of the system generated, then  F  −  Score(C r  ,  S  ) i  =  2  *  P  *  R  /(P  +  R)  (1)  p = the number of correctlylabeled examplesfor a cluster total cluster size R = the number of correctly labeled examples for a cluster total cluster size  Then for a given class Cr,  F  −  score(C  ) r  =  max  (F  −  score(C  r  ,  S  )) i  Si  ∑ C F − Score = c nr FScore( )  r=1 n  r (2)  where c is total number of classes, nr is the size of class Cr , and n is the total size. Participants  will be required to induce the senses of the  target word using only the dataset provided by  the organizers.  4.2 Data Set The organizers provide 50 Chinese training data of SIGHAN2010-WSI-SampleData. The training data contain 50 Chinese words; each word has 50 example sentences, and gives each word the total number of sense. The total number of sense is ranging from 2 to 21, but more cases are 2. In order to facilitate the team participating in the contest to do experiment, the organizers also provide answer to each  word. In order to evaluating the system’s performance of all participating team, the organizers provide 100 test word and each word have 50 example sentences, the system of each participating team need to run out the results which the organizers need.  4.3 System Setup We developed the LSTC-WSI system based on Weka. Firstly, we implemented the evaluation algorithm described in section 4.1. Then, the instances were represented as vectors according to the feature selection. Thirdly, four cluster algorithms from Weka were tried and set different thresholds for feature frequency. Because of paper length constraints, we could not list all the experience data we get. Table 1 listed system performance when frequency threshold set two and without POS information.  Table 1: The Performance on test data  Target word 暗淡 把握 保安 保管 报销 背离 比重 便宜 标兵 病毒 补贴 哺育 材料 采购 参加 草包  Simple Kmeans 0.618 0.404 0.711 0.626 0.571 0.789 0.704 0.568 0.5679 0.601 0.578 0.621 0.560 0.627 0.610 0.643  EM 0.680 0.365 0.557 0.700 0.555 0.596 0.617 0.495 0.679 0.590 0.554 0.537 0.429 0.537 0.538 0.607  Farthest First 0.538 0.400 0.672 0.536 0.572 0.680 0.704 0.461 0.625 0.648 0.662 0.615 0.466 0.643 0.643 0.648  Hierar chical 0.649 0.327 0.636 0.570 0.573 0.548 0.682 0.583 0.688 0.603 0.616 0.627 0.527 0.603 0.638 0.632  程序 澄清 吃饭 冲洗 冲撞 充电 出口 初二 春秋 戳穿 打 打断 打开 打破 打气 大军 大陆 大气 大人 单纯 导师 东北 东方 东西 动力 杜鹃 断交 断气 扼杀 发动 发展 翻身 反射 调动  0.615 0.621 0.538 0.603 0.653 0.627 0.421 0.609 0.634 0.574 0.462 0.661 0.430 0.596 0.614 0.666 0.638 0.841 0.613 0.635 0.603 0.644 0.599 0.588 0.699 0.585 0.643 0.624 0.632 0.451 0.613 0.601 0.591 0.536  0.545 0.616 0.583 0.540 0.557 0.622 0.438 0.528 0.667 0.546 0.429 0.584 0.501 0.644 0.580 0.600 0.590 0.734 0.562 0.617 0.594 0.635 0.595 0.575 0.723 0.596 0.639 0.537 0.525 0.472 0.625 0.640 0.585 0.505  0.662 0.615 0.569 0.632 0.657 0.652 0.454 0.583 0.486 0.577 0.518 0.584 0.549 0.647 0.672 0.615 0.540 0.662 0.670 0.646 0.615 0.661 0.624 0.587 0.673 0.666 0.666 0.663 0.629 0.490 0.6723 0.646 0.663 0.477  0.603 0.658 0.609 0.569 0.603 0.690 0.453 0.627 0.652 0.584 0.501 0.602 0.418 0.654 0.708 0.595 0.678 0.618 0.568 0.649 0.577 0.560 0.638 0.508 0.643 0.603 0.656 0.608 0.617 0.477 0.625 0.661 0.639 0.532  We tried two ways for feature selection: the frequency of features and neighbor words’ POS were taken into account or not. Table 2 shows the average performance on the test data via varying the parameter setting. Observing the results returned by Hierarchical cluster is very  imbalance, we set the options “-L WARD” in order to balance the number.  Table 2: The Average Performance of 50 Training Data  Features Word, Windows 5 Word, Windows 5, Frequency 1 Word, Windows 5, Frequency 2 Word, Windows 5, Frequency 3 Word+POSs, Windows 5 Word+POSs, Windows 5, Frequency 1 Word+POSs, Windows 5, Frequency 2  Simple Kmeans 0.555 0.583 0.605 0.598 0.562 0.589 0.589  EM 0.566 0.567 0.575 0.590 0.582 0.580 0.580  Farthest First 0.607 0.599 0.605 0.600 0.618 0.610 0.610  Hierar chical 0.558 0.582 0.598 0.599 0.569 0.594 0.594  Compared with the average performance of the 50 test data, we find the performance is best1 when considering word only and setting the frequency is two at the same time simple KMeans was adapted. So, we use the same parameters setting and clustered the test data by simple KMeans. As table 2 shows, the F-Score is 60.5% on training data. But on test data, our system’s F-Score is 57.89% officially evaluated by task organizers.  5. Conclusion and Future Works Four cluster algorithms are tried for Chinese word sense induction: Simple KMeans, EM,  
This paper describes a character-based Chinese word sense induction (WSI) system for the International Chinese Language Processing Bakeoff 2010. By computing the longest common substrings between any two contexts of the ambiguous word, our system extracts collocations as features and does not depend on any extra tools, such as Chinese word segmenters. We also design a constrained clustering algorithm for this task. Experiemental results show that our system could achieve 69.88 scores of FScore on the development data set of SIGHAN Bakeoff 2010. 
This paper presents an unsupervised method for automatic Chinese word sense induction. The algorithm is based on clustering the similar words according to the contexts in which they occur. First, the target word which needs to be disambiguated is represented as the vector of its contexts. Then, reconstruct the matrix constituted by the vectors of target words through singular value decomposition (SVD) method, and use the vectors to cluster the similar words. Our system participants in CLP2010 back off task4-Chinese word sense induction. 
Recent studies on word sense induction (WSI) mainly concentrate on European languages, Chinese word sense induction is becoming popular as it presents a new challenge to WSI. In this paper, we propose a feature-based approach using the spectral clustering algorithm to this problem. We also compare various clustering algorithms and similarity metrics. Experimental results show that our system achieves promising performance in F-score. 
This paper details our experiments carried out at Word Sense Induction task. For the foreign language (especially English), there have been many studies of word sense induction (WSI), and the approaches and the techniques are more and more mature. However, the study of Chinese WSI is just getting started, and there has not been a better way to solve the problems encountered. WSI can be divided into two categories: supervised manner and unsupervised manner. But in the light of the high cost of supervised manner, we introduce novel solutions to automatic and unsupervised WSI. In this paper, we propose two different systems. The first one is called K-means-based Chinese word sense induction in an unsupervised manner while the second one is graph-based Chinese word sense induction. In the experiments, the first system has achieved a 0.7729 Fscore on average while the second one has achieved a 0.6067 Fscore. 
Sense Induction is the process of identifying the word sense given its context, often treated as a clustering task. This paper explores the use of spectral cluster method which incorporates word features and ngram features to determine which cluster the word belongs to, each cluster represents one sense in the given document set. 
Processing content for security becomes more and more important since every local danger can have global consequences. Being able to collect and analyse information in diﬀerent languages is a great issue. This paper addresses multilingual solutions for analysis of press articles for epidemiological surveillance. The system described here relies on pragmatics and stylistics, giving up “bag of sentences” approach in favour of discourse repetition patterns. It only needs light resources (compared to existing systems) in order to process new languages easily. In this paper we present here results in English, French and Chinese, three languages with quite diﬀerent characteristics. These results show that simple rules allow selection of relevant documents in a specialized database improving the reliability of information extraction. 
This paper presents an ongoing work on identifying similarity between documents across News papers in different languages. Our aim is to identify similar documents for a given News or event as a query, across languages and make cross lingual search more accurate and easy. For example given an event or News in English, all the English news documents related to the query are retrieved as well as in other languages such as Hindi, Bengali, Tamil, Telugu, Malayalam, Spanish. We use Vector Space Model, a known method for similarity calculation, but the novelty is in identification of terms for VSM calculation. Here a robust translation system is not used for translating the documents. The system is working with good recall and precision. 
Abstract The automatic generation of dictionaries from raw text has previously been based on parallel or comparable corpora. Here we describe an approach requiring only a single monolingual corpus to generate bilingual dictionaries for several language pairs. A constraint is that all language pairs have their target language in common, which needs to be the language of the underlying corpus. Our approach is based on the observation that monolingual corpora usually contain a considerable number of foreign words. As these are often explained via translations typically occurring close by, we can identify these translations by looking at the contexts of a foreign word and by computing its strongest associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn,  2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are a scarce resource for many language pairs (Rapp & Martín Vide, 2007), which is why methods based on comparable corpora have come into focus. One approach is to extract parallel sentences from comparable corpora (Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach relates co-occurrence patterns between languages. Hereby the underlying assumption is that across languages there is a correlation between the cooccurrences of words which are translations of each other. If, for example, in a text of one language two words A and B co-occur more often than expected by chance, then in a text of another language those words which are the translations of A and B should also co-occur more frequently than expected. However, to exploit this observation some bridge needs to be built between the two languages. This can be done via a basic dictionary comprising some essential vocabulary. To put it simply, this kind of dictionary allows a (partial) word-by-word translation from the source to the target language,1 so that the result can be considered as a pair of monolingual corpora. Deal- 
This paper presents a new word alignment method which incorporates knowledge about Bilingual Multi-Word Expressions (BMWEs). Our method of word alignment ﬁrst extracts such BMWEs in a bidirectional way for a given corpus and then starts conventional word alignment, considering the properties of BMWEs in their grouping as well as their alignment links. We give partial annotation of alignment links as prior knowledge to the word alignment process; by replacing the maximum likelihood estimate in the M-step of the IBM Models with the Maximum A Posteriori (MAP) estimate, prior knowledge about BMWEs is embedded in the prior in this MAP estimate. In our experiments, we saw an improvement of 0.77 Bleu points absolute in JP–EN. Except for one case, our method gave better results than the method using only BMWEs grouping. Even though this paper does not directly address the issues in CrossLingual Information Retrieval (CLIR), it discusses an approach of direct relevance to the ﬁeld. This approach could be viewed as the opposite of current trends in CLIR on semantic space that incorporate a notion of order in the bag-of-words model (e.g. co-occurences). 
This paper presents an iterative algorithm for bilingual lexicon extraction from comparable corpora. It is based on a bagof-words model generated at the level of sentences. We present our results of experimentation on corpora of multiple degrees of comparability derived from the FIRE 2010 dataset. Evaluation results on 100 nouns shows that this method outperforms the standard context-vector based approaches. 
In this paper, we describe a voting mechanism for accurate named entity (NE) translation in English–Chinese question answering (QA). This mechanism involves translations from three different sources: machine translation, online encyclopaedia, and web documents. The translation with the highest number of votes is selected. We evaluated this approach using test collection, topics and assessment results from the NTCIR-8 evaluation forum. This mechanism achieved 95% accuracy in NEs translation and 0.3756 MAP in English–Chinese cross-lingual information retrieval of QA. 
OMNIA is an on-going project that aims to retrieve images accompanied with multilingual texts. In this paper, we propose a generic method (language and domain independent) to extract conceptual information from such texts and spontaneous user requests. First, texts are labelled with interlingual annotation, then a generic extractor taking a domain ontology as a parameter extract relevant conceptual information. Implementation is also presented with a ﬁrst experiment and preliminary results. 
The trend toward the growing multilinguality of the Internet requires text summarization techniques that work equally well in multiple languages. Only some of the automated summarization methods proposed in the literature, however, can be deﬁned as “languageindependent”, as they are not based on any morphological analysis of the summarized text. In this paper, we perform an in-depth comparative analysis of language-independent sentence scoring methods for extractive single-document summarization. We evaluate 15 published summarization methods proposed in the literature and 16 methods introduced in (Litvak et al., 2010). The evaluation is performed on English and Hebrew corpora. The results suggest that the performance ranking of the compared methods is quite similar in both languages. The top ten bilingual scoring methods include six methods introduced in (Litvak et al., 2010). 
Multilingual Pseudo-Relevance Feedback (MultiPRF) is a framework to improve the PRF of a source language by taking the help of another language called assisting language. In this paper, we extend the MultiPRF framework to include multiple assisting languages. We consider three different conﬁgurations to incorporate multiple assisting languages - a) Parallel - all assisting languages combined simultaneously b) Serial - assisting languages combined in sequence one after another and c) Selective - dynamically selecting the best feedback model for each query. We study their effect on MultiPRF performance. Results using multiple assisting languages are mixed and it helps in boosting MultiPRF accuracy only in some cases. We also observe that MultiPRF becomes more robust with increase in number of assisting languages. 
Linguistic porting of content management services processing spontaneous utterances in natural language has become important. In most situations, such utterances are noisy, but are constrained by the situation, thus constituting a restricted sublangage. In previous papers, we have presented three methods to port such systems to other languages. In this paper, we study how to also personalize them by making them capable of automatic perception adaptation, using fuzzy evaluation functions. We have reengineered IMRS, a music retrieval NL-based system, to implement that idea, and ported it to French, English and Arabic using an enhanced version of our external porting method, building a unique content extractor for these three languages. More than 30 persons participated in a preliminary on-line qualitative evaluation of the system. 
Query understanding is an important component of web search, like document understanding, query document matching, ranking, and user understanding. The goal of query understanding is to predict the user’s search intent from the given query. Needless to say, search log mining and statistical learning are fundamental technologies to address the task of query understanding. In this talk, I will ﬁrst introduce a large-scale search log mining platform which we have developed at MSRA. I will then explain our approach to query understanding, as well as document understanding, query document matching, and user understanding. After that, I will describe in details about our methods for query understanding based on statistical learning. They include query reﬁnement using CRF, named entity recognition in query using topic model, context aware query topic prediction using HMM. This is joint work with Gu Xu, Daxin Jiang and other collaborators. 
In this paper, we propose a method that clearly separates terms (words and dependency relations) in a natural language query into important and other terms, and differently handles the terms according to their importance. The proposed method uses three types of term importance: necessary, optional, and unnecessary. The importance are detected using linguistic clues. We evaluated the proposed method using a test collection for Japanese information retrieval. Performance was resultantly improved by differently handling terms according to their importance. 
In this paper, we investigate generating a set of query-focused summaries from search results. Since there may be many topics related to a given query in the search results, in order to summarize these results, they should ﬁrst be classiﬁed into topics, and then each topic should be summarized individually. In this summarization process, two types of redundancies need to be reduced. First, each topic summary should not contain any redundancy (we refer to this problem as redundancy within a summary). Second, a topic summary should not be similar to any other topic summary (we refer to this problem as redundancy between summaries). In this paper, we focus on the document clustering process and the reduction of redundancy between summaries in the summarization process. We also propose a method using PLSI to summarize search results. Evaluation results conﬁrm that our method performs well in classifying search results and reducing the redundancy between summaries. 
Classifying and identifying semantic relations between facts and opinions on the Web is of utmost importance for organizing information on the Web, however, this requires consideration of a broader set of semantic relations than are typically handled in Recognizing Textual Entailment (RTE), Cross-document Structure Theory (CST), and similar tasks. In this paper, we describe the construction and evaluation of a system that identiﬁes and classiﬁes semantic relations in Internet data. Our system targets a set of semantic relations that have been inspired by CST but that have been generalized and broadened to facilitate application to mixed fact and opinion data from the Internet. Our system identiﬁes these semantic relations in Japanese Web texts using a combination of lexical, syntactic, and semantic information and evaluate our system against gold standard data that was manually constructed for this task. We will release all gold standard data used in training and evaluation of our system this summer. 
Extracting knowledge from unstructured text has been a long-standing goal of NLP. The advent of the Web further increases its urgency by making available billions of online documents. To represent the acquired knowledge that is complex and heterogeneous, we need ﬁrst-order logic. To handle the inherent uncertainty and ambiguity in extracting and reasoning with knowledge, we need probability. Combining the two has led to rapid progress in the emerging ﬁeld of statistical relational learning. In this talk, I will show that statistical relational learning offers promising solutions for conquering the knowledge-extraction quest. I will present Markov logic, which is the leading unifying framework for representing and reasoning with complex and uncertain knowledge, and has spawned a number of successful applications for knowledge extraction from the Web. In particular, I will present OntoUSP, an end-to-end knowledge extraction system that can read text and answer questions. OntoUSP is completely unsupervised and beneﬁts from jointly conducting ontology induction, population, and knowledge extraction. Experiments show that OntoUSP extracted ﬁve times as many correct answers compared to state-of-the-art systems, with a precision of 91%. 31 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), page 31, Beijing, August 2010   
This paper presents a new computation of lexical distributional similarity, which is a corpus-based method for computing similarity of any two words. Although the conventional method focuses on emphasizing features with which a given word is associated, we propose that even unassociated features of two input words can further improve the performance in total. We also report in addition that more than 90% of the features has no contribution and thus could be reduced in future. 
We analyzed the details of a Web-derived distributional data of Japanese nominal terms with two aims. One aim is to examine if distributionally similar terms can be in fact equated with “semantically similar” terms, and if so to what extent. The other is to investigate into what kind of semantic relations constitute (strongly) distributionally similar terms. Our results show that over 85% of the pairs of the terms derived from the highly similar terms turned out to be semantically similar in some way. The ratio of “classmate,” synonymous, hypernym-hyponym, and meronymic relations are about 62%, 17%, 8% and 1% of the classiﬁed data, respectively. 
Semantic information is a very important factor in coreference resolution. The combination of large corpora and ‘deep’ analysis procedures has made it possible to acquire a range of semantic information and apply it to this task. In this paper, we generate two statistically-based semantic features from a large corpus and measure their influence on pronoun coreference. One is contextual compatibility, which decides if the antecedent can be used in the anaphor’s context; the other is role pair, which decides if the actions asserted of the antecedent and the anaphor are likely to apply to the same entity. We apply a semantic labeling system and a baseline coreference system to a large corpus to generate semantic patterns and convert them into features in a MaxEnt model. These features produce an absolute gain of 1.5% to 1.7% in resolution accuracy (a 6% reduction in errors). To understand the limitations of these features, we also extract patterns from the test corpus, use these patterns to train a coreference model, and examine some of the cases where coreference still fails. We also compare the performance of patterns extracted from semantic role labeling and syntax. 
In this paper, we address the problem of discovering coreference relations between formulas and the surrounding text. The task is different from traditional coreference resolution because of the unique structure of the formulas. In this paper, we present an approach, which we call ‘CDF (Concept Description Formula)’, for mining coreference relations between formulas and the concepts that refer to them. Using Wikipedia articles as a target corpus, our approach is based on surface level text matching between formulas and text, as well as patterns that represent relationships between them. The results showed the potential of our approach for formulas and text coreference mining. 
The rapid spread of electronic health records raised an interest to large-scale information extraction from clinical texts. Considering such a background, we are developing a method that can extract adverse drug event and effect (adverse–effect) relations from massive clinical records. Adverse–effect relations share some features with relations proposed in previous relation extraction studies, but they also have unique characteristics. Adverse–effect relations are usually uncertain. Not even medical experts can usually determine whether a symptom that arises after a medication represents an adverse– effect relation or not. We propose a method to extract adverse–effect relations using a machine-learning technique with dependency features. We performed experiments to extract adverse–effect relations from 2,577 clinical texts, and obtained F1-score of 37.54 with an optimal parameters and F1-score of 34.90 with automatically  tuned parameters. The results also show that dependency features increase the extraction F1-score by 3.59. 
We address the problem of constructing hybrid translation systems by intersecting a Hiero-style hierarchical system with a phrase-based system and present formal techniques for doing so. We model the phrase-based component by introducing a variant of weighted ﬁnite-state automata, called σ-automata, provide a self-contained description of a general algorithm for intersecting weighted synchronous context-free grammars with ﬁnite-state automata, and extend these constructs to σ-automata. We end by brieﬂy discussing complexity properties of the presented algorithms. 
We present two contributions to grammar driven translation. First, since both Inversion Transduction Grammar and Linear Inversion Transduction Grammars have been shown to produce better alignments then the standard word alignment tool, we investigate how the trade-off between speed and end-to-end translation quality extends to the choice of grammar formalism. Second, we prove that Linear Transduction Grammars (LTGs) generate the same transductions as Linear Inversion Transduction Grammars, and present a scheme for arriving at LTGs by bilingualizing Linear Grammars. We also present a method for obtaining Inversion Transduction Grammars from Linear (Inversion) Transduction Grammars, which can speed up grammar induction from parallel corpora dramatically. 
Inspired by previous source-side syntactic reordering methods for SMT, this paper focuses on using automatically learned syntactic reordering patterns with functional words which indicate structural reorderings between the source and target language. This approach takes advantage of phrase alignments and source-side parse trees for pattern extraction, and then ﬁlters out those patterns without functional words. Word lattices transformed by the generated patterns are fed into PBSMT systems to incorporate potential reorderings from the inputs. Experiments are carried out on a medium-sized corpus for a Chinese–English SMT task. The proposed method outperforms the baseline system by 1.38% relative on a randomly selected testset and 10.45% relative on the NIST 2008 testset in terms of BLEU score. Furthermore, a system with just 61.88% of the patterns ﬁltered by functional words obtains a comparable performance with the unﬁltered one on the randomly selected testset, and achieves 1.74% relative improvements on the NIST 2008 testset. 
A typical phrase-based machine translation (PBMT) system uses phrase pairs extracted from word-aligned parallel corpora. All phrase pairs that are consistent with word alignments are collected. The resulting phrase table is very large and includes many non-syntactic phrases which may not be necessary. We propose to filter the phrase table based on source language syntactic constraints. Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words. Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when compared to a baseline PBMT system with full-size tables. 
In Section 2, we present the technical details of the design of our system, together with motivation for the particular design choices. Section 3 details the experimental setup and the data set used for the evaluation results in Section 4. We present improvements that we plan to investigate in further work in Section 5, and provide concluding remarks in Section 6. 2. System Framework We present a system that uses a TM-match to pre-translate parts of the input sentence and guide an SMT system to the generation of a higher-quality translation.  up parts of the input sentence and then supply this marked-up input to an SMT system. This differs to our system in two ways. First, Smith and Clark use EMBT techniques to obtain partial translations of the input from the complete example base, while we are only looking at the best TM match for the given input. Second, the authors use dependency structures for EMBT matching, while we employ phrase-based structures. 2.2. Translation Memory Backend Although the intention is to use a full-scale TM system as the translation memory backend, to have complete control over the process for this initial research we decided to build a simple prototype TM backend ourselves. We employ a database setup using the PostgreSQL v.8.4.31 relational database management (RDBM) system. The segment pairs from a given TM are stored in this database and assigned unique IDs for further reference. When a new sentence is supplied for translation, the database is searched for (near) matches, using an FMS based on normalised character-level Levenshtein edit distance (Levenshtein, 1965). Thus for each input sentence, from the database we obtain the matching segment with the highest FMS, its translation and the score itself. 2.3. Sub-Tree Alignment Backend  2.1. Related Approaches We are not aware of any published research where TM output is used to improve the performance of an SMT system in a manner similar to the system presented in this paper. Most closely related to our approach are the systems by Biçici and Dymetman (2008) and Simard and Isabelle (2009), where the authors use the TM output to extract new phrase pairs that supplement the SMT phrase table. Such an approach, however, does not guarantee that the SMT system will select the TM-motivated phrases even if a heavy bias is applied to them. Another related system is presented in (Smith and Clark, 2009). Here the authors use a syntaxbased EBMT system to pre-translate and mark-  The system presented in this paper uses phrasebased sub-tree structural alignment (Zhechev, 2010) to discover parts of the input sentence that correspond to parts of the suggested translation extracted from the TM database. We chose this particular tool, because it can produce aligned phrase-based-tree pairs from unannotated (i.e. unparsed) data. It can also function fully automatically without the need for any training data. The only auxiliary requirement it has is for a probabilistic dictionary for the languages that are being aligned. As described later in this section, in our case this is obtained automatically from the TM data during the training of the SMT backend. The matching between the input sentence and the TM-suggested translation is done in a threestep process. First, the plain TM match and its  
*MVWX RKVEQ FEWIH QIXVMGW EWWYQI XLEX qKSSHr XVERWPEXMSRW WLEVI XLI WEQI PI\MGEP GLSMGIW [MXL XLI VIJIVIRGI XVERWPEXMSR ;LMPI &0)9 WGSVI TIV JSVQW [IPP MR GETXYVMRK XLI XVERWPEXMSR xYIRG] 'EPPMWSR&YVGL IX EP  ERH /SILR ERH 1SR^  VITSVX GEWIW [LIVI &0)9 WXVSRKP] HMW EKVIIW [MXL LYQER NYHKQIRX SR XVERWPEXMSR UYEP MX] 8LI YRHIVP]MRK VIEWSR MW XLEX PI\MGEP WMQMPEVMX] HSIW RSX EHIUYEXIP] VIxIGX XLI WMQMPEVMX] MR QIER MRK 7IGSRH NYWX PMOI RKVEQ FEWIH QIXVMGW WYGL EW &0)9 W]RXE\FEWIH QIXVMGW EVI WXMPP QSVI xYIRG]SVMIRXIH XLER EHIUYEG]EGGYVEG] SVMIRXIH ;LMPI 781 EHHVIWWIW XLI JEMPYVI SJ &0)9 MR IZEPYEXMRK XLI XVERWPEXMSR KVEQQEXM GEPMX] E KVEQQEXMGEP XVERWPEXMSR GER RSRIXLIPIWW EGLMIZI E LMKL 781 WGSVI IZIR MJ GSRXEMRW IVVSVW EVMWMRK JVSQ GSRJYWMSR SJ WIQERXMG VSPIW 7]RXEG XMG WXVYGXYVI WMQMPEVMX] WXMPP MREHIUYEXIP] VIxIGXW WMQMPEVMX] SJ QIERMRK %W 18 W]WXIQW MQTVSZI XLI WLSVXGSQMRKW SJ  52 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 52–60, COLING 2010, Beijing, August 2010.  *MKYVI  )\EQTPI SJ WIQERXMG JVEQIW MR 'LMRIWI MRTYX )RKPMWL VIJIVIRGI XVERWPEXMSR ERH 18 SYXTYX  PI\MGEP RKVEQ FEWIH ERH W]RXE\FEWIH IZEPYE XMSR QIXVMGW EVI FIGSQMRK QSVI ETTEVIRX 7XEXI SJXLIEVX 18 W]WXIQW EVI SJXIR EFPI XS SYXTYX XVERWPEXMSRW GSRXEMRMRK VSYKLP] XLI GSVVIGX [SVHW ERH FIMRK EPQSWX KVEQQEXMGEP FYX RSX I\TVIWW MRK QIERMRK XLEX MW GPSWI XS XLI WSYVGI MRTYX ;I EHSTX XLI SYXWIX SJ XLI TVMRGMTPI XLEX E KSSH XVERW PEXMSR MW SRI JVSQ [LMGL LYQER VIEHIVW QE] WYG GIWWJYPP] YRHIVWXERH EX PIEWX XLI FEWMG IZIRX WXVYG XYVI i q[LS HMH [LEX XS [LSQ [LIR [LIVI ERH [L]r 4VEHLER IX EP  [LMGL VITVIWIRXW XLI QSWX MQTSVXERX QIERMRK SJ XLI WSYVGI YXXIVERGIW 3YV SFNIGXMZI MW XS IZEPYEXI LS[ [IPP XLI QSWX IW WIRXMEP WIQERXMG MRJSVQEXMSR MW FIMRK GETXYVIH F] XLI QEGLMRI XVERWPEXMSR W]WXIQW JVSQ XLI YWIVtW TSMRX SJ ZMI[ -R XLMW TETIV [I HIWGVMFI MR HIXEMP XLI QIXLSH SPSK] XLEX YRHIVPMIW XLI RI[ WIQERXMG QEGLMRI XVERWPEXMSR IZEPYEXMSR QIXVMGW [I EVI HIZIPSTMRK ;I TVIWIRX XLI VIWYPXW SJ XLI WXYH] SR IZEPYEXMRK QEGLMRI XVERWPEXMSR YXMPMX] F] QIEWYVMRK XLI EGGY VEG] [MXL [LMGL LYQER VIEHIVW EVI EFPI XS GSQ TPIXI XLI WIQERXMG VSPI ERRSXEXMSR XIQTPEXIW 0EWX FYX RSX XLI PIEWX [I WLS[ XLEX SYV TVSTSWIH IZEP YEXMSR QIXVMG LEW E LMKLIV GSVVIPEXMSR [MXL LYQER NYHKQIRXW SR EHIUYEG] XLER &0)9 ERH 781   6IPEXIH ;SVO  7IQERXMG QSHIPW MR 718 2YQIVSYW VIGIRX [SVOW LEW FIIR HSRI SR ETTP] MRK HMJJIVIRX WIQERXMG QSHIPW XS WXEXMWXMGEP QE GLMRI XVERWPEXMSR ;SVH WIRWI HMWEQFMKYEXMSR ;7( QSHIPW GSQFMRI E [MHI VERKI SJ GSRXI\X JIEXYVIW MRXS E WMRKPI PI\MGEP GLSMGI TVIHMGXMSR EW MR XLI [SVO SJ 'EVTYEX ERH ;Y  
Morphological analysis and disambiguation are crucial stages in a variety of natural language processing applications such as machine translation, especially when languages with complex morphology are concerned such as Arabic. Arabic is a highly flexional language, in that, the same root can lead to various forms according to its context. In this paper, we present a system which disambiguates the output of a morphological analyzer for Arabic. The Arabic morphological analyzer used consists of a set of all possible morphological analyses for each word, with the unique correct syntactic feature. We want to choose the correct features using the features generated by the morphological analyzer for the French language in the other side. To obtain this data, we used the results of the alignment of word trained with GIZA++ (Och and Ney, 2003). 
In this paper, we propose a dependency based statistical system that uses discriminative techniques to train its parameters. We conducted experiments on an EnglishHindi parallel corpora. The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper. 
As interaction between speakers of different languages continues to increase, the everpresent problem of language barriers must be overcome. For the same reason, automatic language translation (Machine Translation) has become an attractive area of research and development. Statistical Machine Translation (SMT) has been used for translation between many language pairs, the results of which have shown considerable success. The focus of this research is on the English/Persian language pair. This paper investigates the development and evaluation of the performance of a statistical machine translation system by building a baseline system using subtitles from Persian films. We present an overview of previous related work in English/Persian machine translation, and examine the available corpora for this language pair. We finally show the results of the experiments of our system using an in-house corpus and compare the results we obtained when building a language model with different sized monolingual corpora. Different automatic evaluation metrics like BLEU, NIST and IBM-BLEU were used to evaluate the performance of the system on half of the corpus built. Finally, we look at future work by outlining ways of getting highly accurate translations as fast as possible. 
The present work reports the development of Manipuri-English bidirectional statistical machine translation systems. In the English-Manipuri statistical machine translation system, the role of the suffixes and dependency relations on the source side and case markers on the target side are identified as important translation factors. A parallel corpus of 10350 sentences from news domain is used for training and the system is tested with 500 sentences. Using the proposed translation factors, the output of the translation quality is improved as indicated by baseline BLEU score of 13.045 and factored BLEU score of 16.873 respectively. Similarly, for the Manipuri English system, the role of case markers and POS tags information at the source side and suffixes and dependency relations at the target side are identified as useful translation factors. The case markers and suffixes are not only responsible to determine the word classes but also to determine the dependency relations. Using these translation factors, the output of the translation quality is improved as indicated by baseline BLEU score of 13.452 and factored BLEU score of 17.573 respectively. Further, the subjective evaluation indicates the improvement in the fluency and adequacy of both the factored SMT outputs over the respective baseline systems.  
A major challenge in statistical machine translation is mitigating the word order differences between source and target strings. While reordering and lexical translation choices are often conducted in tandem, source string permutation prior to translation is attractive for studying reordering using hierarchical and syntactic structure. This work contributes an approach for learning source string permutation via transfer of the source syntax tree. We present a novel discriminative, probabilistic tree transduction model, and contribute a set of empirical upperbounds on translation performance for Englishto-Dutch source string permutation under sequence and parse tree constraints. Finally, the translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system signiﬁcantly. 
In this paper, we extend the HMM wordto-phrase alignment model with syntactic dependency constraints. The syntactic dependencies between multiple words in one language are introduced into the model in a bid to produce coherent alignments. Our experimental results on a variety of Chinese–English data show that our syntactically constrained model can lead to as much as a 3.24% relative improvement in BLEU score over current HMM word-to-phrase alignment models on a Phrase-Based Statistical Machine Translation system when the training data is small, and a comparable performance compared to IBM model 4 on a Hiero-style system with larger training data. An intrinsic alignment quality evaluation shows that our alignment model with dependency constraints leads to improvements in both precision (by 1.74% relative) and recall (by 1.75% relative) over the model without dependency information. 
We propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source-span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a conﬁdence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training time. We further propose different methods of combining hierarchical and syntax-based PSCFG models, by merging the grammars as well as by interpolating the translation models. Finally, we compare syntax-augmented MT, which extracts rules based on targetside syntax, to a corresponding variant based on source-side syntax, and experiment with a model extension that jointly takes source and target syntax into account. 
Hierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difﬁculty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005), for example. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models ﬂuency of words not using their local context in the string, as in traditional language models, but instead using the deeper context of a word. In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model. We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems. 
The morphosyntactic treatment of multiword units is particularly challenging in morphologically rich languages. We present a comparative study of two formalisms meant for lexicalized description of MWUs in Polish. We show their expressive power and describe encoding experiments, involving novice and expert lexicographers, and allowing to evaluate the accuracy and efﬁciency of both implementations. 
Idioms are not only interesting but also distinctive in a language for its continuity and metaphorical meaning in its context. This paper introduces the construction of a Chinese idiom knowledge base by the Institute of Computational Linguistics at Peking University and describes an experiment that aims at the automatic emotion classification of Chinese idioms. In the process, we expect to know more about how the constituents in a fossilized composition like an idiom function so as to affect its semantic or grammatical properties. As an important Chinese language resource, our idiom knowledge base will play a major role in applications such as linguistic research, teaching Chinese as a foreign language and even as a tool for preserving this non-material Chinese cultural and historical heritage. 
In this paper we investigate the automatic acquisition of Arabic Multiword Expressions (MWE). We propose three complementary approaches to extract MWEs from available data resources. The ﬁrst approach relies on the correspondence asymmetries between Arabic Wikipedia titles and titles in 21 different languages. The second approach collects English MWEs from Princeton WordNet 3.0, translates the collection into Arabic using Google Translate, and utilizes different search engines to validate the output. The third uses lexical association measures to extract MWEs from a large unannotated corpus. We experimentally explore the feasibility of each approach and measure the quality and coverage of the output against gold standards. 
Identifying collocations in a sentence, in order to ensure their proper processing in subsequent applications, and performing the syntactic analysis of the sentence are interrelated processes. Syntactic information is crucial for detecting collocations, and vice versa, collocational information is useful for parsing. This article describes an original approach in which collocations are identiﬁed in a sentence as soon as possible during the analysis of that sentence, rather than at the end of the analysis, as in our previous work. In this way, priority is given to parsing alternatives involving collocations, and collocational information guide the parser through the maze of alternatives. This solution was shown to lead to substantial improvements in the performance of both tasks (collocation identiﬁcation and parsing), and in that of a subsequent task (machine translation). 
This paper presents the automatic extraction of Complex Predicates (CPs) in Bengali with a special focus on compound verbs (Verb + Verb) and conjunct verbs (Noun /Adjective + Verb). The lexical patterns of compound and conjunct verbs are extracted based on the information of shallow morphology and available seed lists of verbs. Lexical scopes of compound and conjunct verbs in consecutive sequence of Complex Predicates (CPs) have been identified. The fine-grained error analysis through confusion matrix highlights some insufficiencies of lexical patterns and the impacts of different constraints that are used to identify the Complex Predicates (CPs). System achieves F-Scores of 75.73%, and 77.92% for compound verbs and 89.90% and 89.66% for conjunct verbs respectively on two types of Bengali corpus. 
Data preprocessing plays a crucial role in phrase-based statistical machine translation (PB-SMT). In this paper, we show how single-tokenization of two types of multi-word expressions (MWE), namely named entities (NE) and compound verbs, as well as their prior alignment can boost the performance of PB-SMT. Single-tokenization of compound verbs and named entities (NE) provides significant gains over the baseline PB-SMT system. Automatic alignment of NEs substantially improves the overall MT performance, and thereby the word alignment quality indirectly. For establishing NE alignments, we transliterate source NEs into the target language and then compare them with the target NEs. Target language NEs are first converted into a canonical form before the comparison takes place. Our best system achieves statistically significant improvements (4.59 BLEU points absolute, 52.5% relative improvement) on an English—Bangla translation task. 
Most word segmentation methods employed in Chinese Information Retrieval systems are based on a static dictionary or a model trained against a manually segmented corpus. These general segmentation approaches may not be optimal because they disregard information within semantic units. We propose a novel method for improving word-based Chinese IR, which performs segmentation according to the tightness of phrases. In order to evaluate the effectiveness of our method, we employ a new test collection of 203 queries, which include a broad distribution of phrases with different tightness values. The results of our experiments indicate that our method improves IR performance as compared with a general word segmentation approach. The experiments also demonstrate the need for the development of better evaluation corpora. 
In order to accomplish the deep semantic understanding of a language, it is essential to analyze the meaning of predicate phrases, a content word plus functional expressions. In agglutinating languages such as Japanese, however, sentential predicates are multi-morpheme expressions and all the functional expressions including those unnecessary to the meaning of the predicate are merged into one phrase. This triggers an increase in surface forms, which is problematic for NLP systems. We solve this by introducing simplified surface forms of predicates that retain only the crucial meaning of the functional expressions. We construct paraphrasing rules based on syntactic and semantic theories in linguistics. The results of experiments show that our system achieves the high accuracy of 77% while reducing the differences in surface forms by 44%, which is quite close to the performance of manually simplified predicates. 
In linguistic studies, reduplication generally means the repetition of any linguistic unit such as a phoneme, morpheme, word, phrase, clause or the utterance as a whole. The identification of reduplication is a part of general task of identification of multiword expressions (MWE). In the present work, reduplications have been identified from the Bengali corpus of the articles of Rabindranath Tagore. The present rule-based approach is divided into two phases. In the first phase, identification of reduplications has been done mainly at general expression level and in the second phase, their structural and semantics classifications are analyzed. The system has been evaluated with average Precision, Recall and FScore values of 92.82%, 91.50% and 92.15% respectively. 
In this paper we tackle the challenging task of Multi-word term (MWT) extraction from different types of specialized corpora. Contrastive ﬁltering of previously extracted MWTs results in a considerable increment of acquired domainspeciﬁc terms. 
In this paper we present a hybrid approach for identifying Japanese functional expressions and its application in a Japanese reading assistant. We combine the results of machine learning and patternbased methods to identify several types of functional expressions. We show that by using our approach we can double the coverage of previous approaches and still maintain the high level of performance necessary for our application. 
 The Varro toolkit offers an intuitive mechanism for extracting syntactically motivated multi-word expressions (MWEs) from dependency treebanks by looking for recurring connected subtrees instead of subsequences in strings. This approach can ﬁnd MWEs that are in varying orders and have words inserted into their components. This paper also proposes description length gain as a statistical correlation measure well-suited to tree structures.  
Automatic word segmentation errors, for languages having a writing system without word boundaries, negatively affect the performance of language models. As a solution, the use of multiple, instead of unique, segmentation has recently been proposed. This approach boosts N-gram counts and generates new N-grams. However, it also produces bad N-grams that affect the language models' performance. In this paper, we study more deeply the contribution of our multiple segmentation approach and experiment on an efficient solution to minimize the effect of adding bad N-grams. 
Thai language text presents challenges for integration into large-scale multilanguage statistical machine translation (SMT) systems, largely stemming from the nominal lack of punctuation and inter-word space. For Thai sentence breaking, we describe a monolingual maximum entropy classifier with features that may be applicable to other languages such as Arabic, Khmer and Lao. We apply this sentence breaker to our largevocabulary, general-purpose, bidirectional Thai-English SMT system, and achieve BLEU scores of around 0.20, reaching our threshold of releasing it as a free online service. 
This paper reports about the development of clause identification and classification techniques for Bengali language. A syntactic rule based model has been used to identify the clause boundary. For clause type identification a Conditional random Field (CRF) based statistical model has been used. The clause identification system and clause classification system demonstrated 73% and 78% precision values respectively. 
A morphological analyzer forms the foundation for many NLP applications of Indian Languages. In this paper, we propose and evaluate the morphological analyzer for Marathi, an inflectional language. The morphological analyzer exploits the efficiency and flexibility offered by finite state machines in modeling the morphotactics while using the well devised system of paradigms to handle the stem alternations intelligently by exploiting the regularity in inflectional forms. We plug the morphological analyzer with statistical pos tagger and chunker to see its impact on their performance so as to confirm its usability as a foundation for NLP applications. 
A web based Manipuri corpus is developed for identification of reduplicated multiword expression (MWE) and multiword named entity recognition (NER). Manipuri is one of the rarely investigated language and its resources for natural language processing are not available in the required measure. The web content of Manipuri is also very poor. News corpus from a popular Manipuri news website is collected. Approximately four and a half million Manipuri wordforms have been collected from the web. The mode of corpus collection and the identification of reduplicated MWEs and multiword NE based on support vector machine (SVM) learning technique are reported. The SVM based reduplicated MWE system is evaluated with recall, precision and FScore values of 94.62%, 93.53% and 94.07% respectively outperforming the rule based approach. The recall, precision and F-Score for multiword NE are evaluated as 94.82%, 93.12% and 93.96% respectively. 
In this paper we present a lightweight stemmer for Gujarati using a hybrid approach. Instead of using a completely unsupervised approach, we have harnessed linguistic knowledge in the form of a hand-crafted Gujarati suffix list in order to improve the quality of the stems and suffixes learnt during the training phase. We used the EMILLE corpus for training and evaluating the stemmer’s performance. The use of hand-crafted suffixes boosted the accuracy of our stemmer by about 17% and helped us achieve an accuracy of 67.86 %. 
This paper presents a method for constructing a large-scale Person Ontology with category hierarchy from Wikipedia. We first extract Wikipedia category labels which represent person (hereafter, Wikipedia Person Category, WPC) by using a machine learning classifier. We then construct a WPC hierarchy by detecting is-a relations in the Wikipedia category network. We then extract the titles of Wikipedia articles which represent person (hereafter, Wikipedia person instance, WPI). Experiments show that the accuracy of WPC extraction is 99.3% precision and 98.4% recall, while that of WPI extraction is 98.2% and 98.6%, respectively. The accuracies are significantly higher than the previous methods. 
One of the valuable features of any collaboratively constructed semantic resource (CSR) is its ability to – as a system – continuously correct itself. Wikipedia is an excellent example of such a process, with vandalism and misinformation being removed or reverted in astonishing time by a coalition of human editors and machine bots. However, some errors are harder to spot than others, a problem which can lead to persistent unchecked errors, particularly on more obscure, less viewed article pages. In this paper we discuss the problems of incorrect link targets in Wikipedia, and propose a method of automatically highlighting and correcting them using only the semantic information found in this encyclopaedia’s link structure. 
This paper describes an on-going annotation effort which aims at adding a manual annotation layer connecting an existing annotated corpus such as the English ACE-2005 Corpus to Wikipedia. The annotation layer is intended for the evaluation of accuracy of linking to Wikipedia in the framework of a coreference resolution system. 
In this paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones. The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones. We report empirical evidence that our method successfully expands existing textual entailment corpora. 
Sentiment analysis attempts to extract the author’s sentiments or opinions from unstructured text. Unlike approaches based on rules, a machine learning approach holds the promise of learning robust, highcoverage sentiment classiﬁers from labeled examples. However, people tend to use different ways to express the same sentiment due to the richness of natural language. Therefore, each sentiment expression normally does not have many examples in the training corpus. Furthermore, sentences extracted from unstructured text (e.g., I ﬁlmed my daughter’s ballet recital and could not believe how the auto focus kept blurring then focusing) often contain both informative (e.g., the auto focus kept blurring then focusing) and extraneous non-informative text regarding the author’s sentiment towards a certain topic. When there are few examples of any given sentiment expression, extraneous non-sentiment information cannot be identiﬁed as noise by the learning algorithm and can easily become correlated with the sentiment label, thereby confusing sentiment classiﬁers. In this paper, we present a highly effective procedure for using crowd-sourcing techniques to label informative and non-informative information regarding the sentiment expressed in a sentence. We also show that pruning non-informative information using non-expert annotations during the training phase can result in classiﬁers with  better performance even when the test data includes non-informative information. 
Keyword-matching systems based on simple models of semantic relatedness are inadequate at modelling the ambiguities in natural language text, and cannot reliably address the increasingly complex information needs of users. In this paper we propose novel methods for computing semantic relatedness by spreading activation energy over the hyperlink structure of Wikipedia. We demonstrate that our techniques can approach state-of-the-art performance, while requiring only a fraction of the background data. 
The blogosphere is a huge collaboratively constructed resource containing diverse and rich information. This diversity and richness presents a significant research challenge to the Information Retrieval community. This paper addresses this challenge by proposing a method for identification of “topic clusters” within the blogosphere where topic clusters represent the concept of grouping together blogs sharing a common interest i.e. topic, the algorithm takes into account both the hyperlinked social network of blogs along with the content in the blog posts. Additionally we use various forms and parts-of-speech of the topic to provide a broader coverage of the blogosphere. The next step of the method is to assign topic-specific ranks to each blog in the cluster using a metric called “Topic Discussion Rank,” that helps in identifying the most influential blog for a specific topic. We also perform an experimental evaluation of our method on real blog data and show that the proposed method reaches a high level of accuracy. 
 This paper introduces a website called Minna no Hon’yaku (MNH, “Translation for All”), which hosts online volunteer translators. Its core features are (1) a set of translation aid tools, (2) high quality, comprehensive language resources, and (3) the legal sharing of translations. As of May 2010, there are about 1200 users and 4 groups registered to MNH. The groups using it include such major NGOs as Amnesty International Japan and Democracy Now! Japan.  Figure 1: Screenshot of “Minna no Hon’yaku” site (http://trans-aid.jp)  
SemanticNet is a semantic network of lexicons to hold human pragmatic knowledge. So far Natural Language Processing (NLP) research patronized much of manually augmented lexicon resources such as WordNet. But the small set of semantic relations like Hypernym, Holonym, Meronym and Synonym etc are very narrow to capture the wide variations human cognitive knowledge. But no such information could be retrieved from available lexicon resources. SemanticNet is the attempt to capture wide range of context dependent semantic inference among various themes which human beings perceive in their pragmatic knowledge, learned by day to day cognitive interactions with the surrounding physical world. SemanticNet holds human pragmatics with twenty well established semantic relations for every pair of lexemes. As every pair of relations cannot be defined by fixed number of certain semantic relation labels thus additionally contextual semantic affinity inference in SemanticNet could be calculated by network distance and represented as a probabilistic score. SemanticNet is being presently developed for Bengali language. 
In this paper, we present an on-going project aiming at extending the WordNet lexical database by encoding common sense featural knowledge elicited from language speakers. Such extension of WordNet is required in the framework of the STaRS.sys project, which has the goal of building tools for supporting the speech therapist during the preparation of exercises to be submitted to aphasic patients for rehabilitation purposes. We review some preliminary results and illustrate what extensions of the existing WordNet model are needed to accommodate for the encoding of commonsense (featural) knowledge. 
When two texts have an inclusion relation, the relationship between them is called entailment. The task of mechanically distinguishing such a relation is called recognising textual entailment (RTE), which is basically a kind of semantic analysis. A variety of methods have been proposed for RTE. However, when the previous methods were combined, the performances were not clear. So, we utilized each method as a feature of machine learning, in order to combine methods. We have dealt with the binary classification problem of two texts exhibiting inclusion, and proposed a method that uses machine learning to judge whether the two texts present the same content. We have built a program capable to perform entailment judgment on the basis of word overlap, i.e. the matching rate of the words in the two texts, mutual information, and similarity of the respective syntax trees (Subpath Set). Word overlap was calclated by utilizing BiLingual Evaluation Understudy (BLEU). Mutual information is based on co-occurrence frequency, and the Subpath Set was determined by using the Japanise WordNet. A ConfidenceWeighted Score of 68.6% was obtained in the mutual information experiment on RTE. Mutual information and the use of three methods of SVM were shown to be effective.  
Color affects every aspect of our lives. There has been a considerable interest in the psycholinguistic research area addressing the impact of color on emotions. In the experiments conducted by these studies, subjects have usually been asked to indicate their emotional responses to different colors. On the other side, sensing emotions in text by using NLP techniques has recently become a popular topic in computational linguistics. In this paper, we introduce a semantic similarity mechanism acquired from a large corpus of texts in order to check the similarity of colors and emotions. Then we investigate the correlation of our results with the outcomes of some psycholinguistic experiments. The conclusions are quite interesting. The correlation varies among different colors and globally we achieve very good results. 
Mathieu Roche LIRMM, UMR 5506, CNRS, Univ. Montpellier 2 mroche@lirmm.fr  Abstract This paper presents an approach to enrich conceptual classes based on the Web. To test our approach, we first build conceptual classes using syntactic and semantic information provided by a corpus. The concepts can be the input of a dictionary. Our web-mining approach deals with a cognitive process which simulates human reasoning based on the enumeration principle. The experiments reveal the interest of our approach by adding new relevant terms to existing conceptual classes. 1 Introduction Concepts have several definitions; one of the most general describes a concept ‘as the mind’s representation of a thing or an item’ (DesrosiersSabbath, 1984). In a domain such as ours, i.e. ontology building, semantic webs, and computational linguistics, it seems appropriate to stick to the Aristotelian approach to a concept, and consider it as a set of knowledge (gathered information) on common semantic features. The choice of the features and how the knowledge is gathered depend on criteria we will explain below. In this paper, we deal with the building of conceptual classes, which can be defined as gathering semantically close terms. First, we suggest building specific conceptual classes by focusing on knowledge extracted from corpora. Conceptual classes are shaped by the study of syntactic dependencies between corpus terms (as described in section 2). Dependencies tackle relations such as Verb/Subject, Noun/Noun Phrase Complements, Verb/Object, Verb/Complements,  and sometimes Sentence Head/Complements. In this paper, we focus on the Verb/Object dependency because it is representative of a field. For instance, in computer science, the verb ‘to load’ takes as objects, nouns of the conceptual class software (L’Homme, 1998). This feature also extends to ‘download’ or ‘upload’, which have the same verbal root. Corpora are rich sources of terminological information that can be mined. A terminology extraction of this kind is similar to a Harris-like distributional analysis (Harris, 1968) and many works in the literature have been the subject of distributional analysis to acquire terminological or ontological knowledge from textual data (e.g (Bourigault and Lame, 2002) for law, (Nazarenko et al., 2001; Weeds et al., 2005) for medicine). After building conceptual classes (section 2), we describe an approach to expand concepts by using a Web search engine to discover new terms (section 3). In section 4, experiments conducted on real data enable us to validate our approach. 2 Building Conceptual Classes 2.1 Principle In our approach, a class can be defined as a gathering of terms with a common field. In this paper, we focus on objects of verbs judged to be semantically close by using a measure. These objects are thus considered as instances of conceptual classes. The first step in building conceptual classes consists in extracting Verb/Object syntactic relations as explained in the following section.  33 Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 33–37, Beijing, August 2010  2.2 Mining for Verb/Object relations Our corpora are in French since our team is mostly devoted to French-based NLP applications. However, the following method can be used for any other language, provided a reliable dependency parser is available. In our case, we use the SYGFRAN parser developed by (Chauché, 1984). As an example, in the French sentence “Thierry Dusautoir brandissant le drapeau tricolore sur la pelouse de Cardiff après la victoire.” (translation: ‘Thierry Dusautoir brandishing the three colored flag on Cardiff lawn after the victory’), there is a verb-object syntactic relation: “verb: brandir (to brandish), object: drapeau (flag)”, which is a good candidate for retrieval. The second step of the building process corresponds to the gathering of common objects related to semantically close verbs.  Where logAsium(x) is equal to: • for x = 0, logAsium(x) = 0 • else logAsium(x) = log(x) + 1 Therefore, conceptual classes instances are the common objects of close verbs, according to the ASIUM proximity measure. The following section describes the acquisition of new terms starting with a list of terms/concepts obtained with the global process summarized in this section and detailed in (Béchet et al., 2008).  3 Expanding conceptual classes  Figure 1: Common and complementary objects of the verbs “to consume” and “to eat” Assumption of Semantic Closeness. The underlying linguistic hypothesis is the following: Verbs with a significant number of common objects are semantically close. To measure closeness, the ASIUM score (Faure and Nedellec, 1999; Faure, 2000) is used (see figure 1). This type of work is similar to distributional analysis approaches such as that of (Bourigault and Lame, 2002). As explained in the introduction, the measure considers two verbs to be close if they have a significant number of common features (objects). Let p and q be verbs with their respective p1,...,pn and q1,...,qm objects. NbOCp(qi) is the number of occurrences of qi objects from q that are also objects of p (common objects). NbO(qi) is the number of occurrences of qi objects of q verb. The Asium measure is then:  3.1 Acquisition of candidate terms The aim of this approach is to provide new candidates for a given concept. It is based on enumeration on the Web of terms that are semantically close. For instance, with a query (string) “bicycle, car, and”, we can find other vehicles. We propose to use the Web to acquire new candidates. This kind of method uses information regarding the “popularity” of the web and is independent of a particular corpus. Our method of acquisition is quite similar to that of (Nakov and Hearst, 2008). These authors propose to query the Web using the Google search engine to characterize the semantic relation between a pair of nouns. The Google star operator among others, is used to that end. (Nakov and Hearst, 2008) refer to the study of (Lin and Pantel, 2001) who used a Web mining approach to discover inference rules missed by humans. To apply our method, we first consider the common objects of semantically close verbs, which are instances of reference concepts (e.g. vehicle). Let N concepts Ciϵ{1, N} and their respective instances Ij(Ci). For each concept Ci, we submit to a search engine the following queries: ”IjA(Ci), IjB(Ci), and” and ”IjA(Ci), IjB(Ci), or” with jA and jB ϵ {1, ..., NbInstanceCi} and jA ≠ jB.  34  The search engine returns a set of results from which we extract new candidate instances of a concept. For example, if we consider the query: “bicycle, car, and”, one page returned by a search engine gives the following text: Listen here for the Great Commuter Race (17/11/05) between bicycle, car and bus, as part of... Having identified the relevant features in the result returned (in bold in our example), we add the term “bus” to the initial concept “vehicle”. In this way, we obtain new candidates for our concepts. The process can be repeated. In order to automatically determine which candidates are relevant, the candidates are filtered as shown in the following section. 3.2 Filtering of candidates The quality of the extracted terms can be validated by an expert, or automatically by using the Web to check if the extracted candidates (see section 3.1) are relevant. The principle is to consider a relevant term if it is often present with the terms of the original conceptual class (kernel of words). Thus, our aim is to validate a term “in the context”. From that point of view, our method is close to that of (Turney, 2001), which queries the Web via the AltaVista search engine to determine appropriate synonyms for a given term. Like (Turney, 2001), we consider that information concerning the number of pages returned by the queries can give an indication of the relevance of a term. Thus, we submit to a search engine different strings (using citation marks). A query consists of the new candidate and both terms of the concept. Formally, our approach can be defined as follows. Let N concepts Ci ϵ {1, N}, their respective instances Ij(Ci) and the new candidates for a concept Ci, Nik ϵ {1, NbNI(Ci)}. For each Ci, each new candidate Nik is sent as a query to a Web search engine. In practice the three terms are separated either by a comma or the word “or” or “and”1. For each query, the search engine returns a number of results (i.e. number of web pages). Then, the sum of these results is calculated using all possible combinations of “or”, “and”, or of the three words (words of the kernel plus candidate  word to enrich it). Below is an example with the kernel words “car”, “bicycle” and the candidate “bus” to test (using Yahoo): • “car, bicycle, and bus”: 71 pages returned • “car, bicycle, or bus”: 268 pages returned • “bicycle, bus, and car”: 208 pages returned • and so forth Global result: 71 + 268 + 208... The filtering of candidates consists in selecting the k first candidates by class (i.e. with the highest sum), they are added as new instances of the initial concept. We can reiterate the acquisition approach by including these new terms. The acquisition/filtering process can be repeated several times. In the next section, we present experiments conducted to evaluate the quality of our approach. 4 Experiments 4.1 Evaluation protocol We used a French corpus from the Yahoo site (http://fr.news.yahoo.com/) composed of 8,948 news items (16.5 MB) from newspapers. Experiments were performed on 60,000 syntactic relations (Béchet et al., 2008; Béchet et al., 2009) to build original conceptual classes. We manually selected five concepts (see Figure 2). Instances of these concepts are the common objects of verbs defining the concept (see section 2.2).  
This paper presents a cross-linguistic analysis of the largest dictionaries currently existing for Romanian, French, and German, and a new, robust and portable method for Dictionary Entry Parsing (DEP), based on SegmentationCohesion-Dependency (SCD) configurations. The SCD configurations are applied successively on each dictionary entry to identify its lexicographic segments (the first SCD configuration), to extract its sense tree (the second configuration), and to parse its atomic sense definitions (the third one). Using previous results on DLR (The Romanian Thesaurus – new format), the present paper adapts and applies the SCD-based technology to other four large and complex thesauri: DAR (The Romanian Thesaurus – old format), TLF (Le Trésor de la Langue Française), DWB (Deutsches Wörterbuch – GRIMM), and GWB (GötheWörterbuch). This experiment is illustrated on significantly large parsed entries of these thesauri, and proved the following features: (1) the SCD-based method is a completely formal grammarfree approach for dictionary parsing, with efficient (weeks-time adaptable) modeling through sense hierarchies and parsing portability for a new dictionary. (2) SCDconfigurations separate and run sequentially and independently the processes of lexicographic segment recognition, sense tree extraction, and atomic definition  parsing. (3) The whole DEP process with SCD-configurations is optimal. (4) SCDconfigurations, through sense marker classes and their dependency hypergraphs, offer an unique instrument of lexicon construction comparison, sense concept design and DEP standardization. 
This paper is based on our efforts on automatic multi-word terms extraction and its conceptual structure for multiple languages. At present, we mainly focus on English and the major Romance languages such as French, Spanish, Portuguese, and Italian. This paper is a case study for Italian language. We present how to build automatically conceptual structure of automatically extracted multi-word terms from domain speciﬁc corpora for Italian. We show the experimental results for extracting multi-word terms from two domain corpora (“natural area” and “organic agriculture”). Since this work is still ongoing, we discuss our future direction at the end of the paper. 
 56 Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 56–65, Beijing, August 2010  largely ignored. In recent decades, researchers have started to take a cognitive approach to understand the links between nouns and classifiers and found it necessary to make a distinction between classifiers and measure words. For instance, Tai & Wang (1990:38) state that “A classifier categorizes a class of nouns by picking out some salient perceptual properties, either physically or functionally based, which are permanently associated with entities named by the class of nouns; a measure word does not categorize but denotes the quantity of the entity named by a noun. ” This definition makes a clear distinction between a classifier and a measure word, which is assumed to be helpful for L2 learners to have a better understanding of the cognitive basis of a classifier system. This is because there are no measure words in English or other European languages that can also function as classifiers in the same sense as Chinese classifiers. A recent study done by Gao (2010) has shown that Swedish adult learners of Chinese had a lower proficiency in classifier application than their general Chinese proficiency and that most of them were not aware of the difference between the concept of a classifier and that of a measure word. Other previous studies on classifiers include descriptive and experimental studies of classifier systems of natural languages. For example, some descriptive studies make typological surveys of classifier systems in different languages (e.g. Allan, 1977; Lyons, 1977; Goddard, 1998); others provide semantic analysis of classifiers and their associated nouns (e.g. Downing, 1993; Huang & Ahrens, 2003; Matsumoto, 1993), and some also propose that there is an ontological base on which classifiers and nouns are associated with (Sowa 2000; Philpot et al., 2003; Nichols et al., 2005). Experimental studies using computer technology to apply findings of classifier knowledge to natural language processing (NLP) have provided a new approach for the semantic analysis of classifiers (e.g. Nirenburg & Raskin, 2004; Hwang et al., 2007, Quek, 2010) and for computer-assisted language learning (e.g. Guo & Zhong, 2005). However, no e-learning systems developed so far are found to be able to guide second language learners to use the semantic  properties to understand the links between classifiers and their associated nouns. The emergence of computer-assisted language learning (CALL) provides language learners with a user-friendly and flexible elearning tool. CALL incorporates technology into the language learning process and also applies itself across a broad spectrum of teaching styles, textbooks, and courses (Donaldson & Haggstrom, 2006). Its bidirectional and individualized features makes it possible for learners to use it effectively to improve different aspects of language skills (e.g. Mallon 2006; Chang et al., 2008). My idea of designing the e-dictionary of Chinese classifiers is similar to that of CALL. Empirical studies have shown that classifier learning is a big challenge for L2 learners of Chinese. My argument with regards to Chinese classifier acquisition is that cognitive strategies with a bottom-up approach are the key to the understanding of the complexity of classifier and noun associations. Therefore, the design of the edictionary has a focus on guiding learners to explore the cognitive foundations of classifiernoun relations. The e-learning system implemented in the e-dictionary is designed to promote self-paced accelerated learning. It consists of a database of the decomposed semantic features of classifiers and their associated nouns. These well-defined unique and non-unique features will help learners to take a cognitive approach to explore case by case the matched pairs of classifiers and nouns. Currently the e-dictionary has included 168 noun classifiers and 680 nouns, of which 80 classifiers and 560 nouns have been analysed and entered into the elearning database. My aim is to define and include all Chinese classifiers and their associated nouns1 and eventually link them to the e-learning system. 2 Multi-categorization of Classifiers In cognitive linguistics, categories are defined by groups of features and relationships within a same family. From this viewpoint, the 
“Athena” maralex@ilsp.gr  Marianna Mini Institute for Language and Speech Processing / R.C. “Athena” minimar@ilsp.gr  Abstract We report on a user needs investigation carried out in the framework of the project EKFRASIS1 that developed a platform for supporting authoring work in Modern Greek. The platform had as a backbone a conceptually organised dictionary enhanced with rich lexicographic and morphosyntactic information. Organisation of information and encoding drew on Semantic Web technologies (ontologies). Users were all professional authors (of literature, editors, translators, journalists) working for well-established firms. They were all familiar with printed conceptually organized dictionaries while most of them used a computer. They were asked to specify how the platform would be helpful to them when they searched for a word for which they had only vague or few clues, a situation that was familiar to all of them. Users preferred to have, in a first step, easy access to limited but to-the-point lexical information while access to rich semantic information should be provided at a second step. They were interested in rich lexical material although they were not really able to identify the relations that would help them retrieve it. They strongly preferred an organization of material by concept and PoS and appreci- 
Our work is conﬁned to word access, that is, we present here our ideas of how to improve electronic dictionaries in order to help language producers (speaker/writer) to ﬁnd the word they are looking for. Our approach is based on psychological ﬁndings (representation, storage and access of information in the human mind), observed search strategies and typical navigational behavior. If one agrees with the idea that lexical access (word ﬁnding) is basically a search problem, then one may still want to ﬁnd out where and how to search. While the space, i.e. the semantic map in which search takes place is a resource problem,— any of the following could be used: dictionary, corpus, thesauraus, etc. or a mix of them,— its exploration is typically a search problem. Important as it may be, the building of a high quality resource is not the focus of this work, we rely on an existing one, and while we are concerned with its quality, we will be mostly concerned here with search methods, in order to determine the best. 
This document describes an open text-mining system that was developed for the Asian-European project KYOTO. The KYOTO system uses an open text representation format and a central ontology to enable extraction of knowledge and facts from large volumes of text in many different languages. We implemented a semantic tagging approach that performs off-line reasoning. Mining of facts and knowledge is achieved through a flexible pattern matching module that can work in much the same way for different languages, can handle efficiently large volumes of documents and is not restricted to a specific domain. We applied the system to an English database on estuaries. 
We present a novel method for building a large-scale Japanese ontology from Wikipedia using one of the largest Japanese thesauri, Nihongo Goi-Taikei (referred to hereafter as “Goi-Taikei”) as an upper ontology. First, The leaf categories in the Goi-Taikei hierarchy are semi-automatically aligned with semantically equivalent Wikipedia categories. Then, their subcategories are created automatically by detecting is-a links in the Wikipedia category network below the junction using the knowledge deﬁned in Goi-Taikei above the junction. The resulting ontology has a well-deﬁned taxonomy in the upper level and a ﬁne-grained taxonomy in the lower level with a large number of up-to-date instances. A sample evaluation shows that the precisions of the extracted categories and instances are 92.8% and 98.6%, respectively. 
We are describing the construction process of a specialized multilingual lexical resource dedicated for the archive of the Digital Silk Road DSR. The DSR project creates digital archives of cultural heritage along the historical Silk Road; more than 116 of basic references on Silk Road have been digitized and made available online. These books are written in various languages and attract people from different linguistic background, therefore, we are trying to build a multilingual repository for the terminology of the DSR to help its users, and increase the accessibility of these books. The construction of a terminological database using a classical approach is difficult and expensive. Instead, we are introducing specialized lexical resources that can be constructed by the community and its resources; we call it Multilingual Preterminological Graphs MPGs. We build such graphs by analyzing the access log files of the website of the Digital Silk Road. We aim at making this graph as a seed repository so multilingual volunteers can contribute. We have used the access log files of the DSR since its beginning in 2003,  and obtained an initial graph of around 116,000 terms. As an application, We have used this graph to obtain a preterminological multilingual database that has a number of applications. 
We describe a method for the identiﬁcation of medical term variations using parallel corpora and measures of distributional similarity. Our approach is based on automatic word alignment and standard phrase extraction techniques commonly used in statistical machine translation. Combined with pattern-based ﬁlters we obtain encouraging results compared to related approaches using similar datadriven techniques. 
Current approaches of hypernymy acquisition are mostly based on syntactic or surface representations and extract hypernymy relations between surface word forms and not word readings. In this paper we present a purely semantic approach for hypernymy extraction based on semantic networks (SNs). This approach employs a set of patterns sub0(a1, a2) ← premise where the premise part of a pattern is given by a SN. Furthermore this paper describes how the patterns can be derived by relational statistical learning following the Minimum Description Length principle (MDL). The evaluation demonstrates the usefulness of the learned patterns and also of the entire hypernymy extraction system. 
We propose a novel algorithm to extract taxonomic (or isa/instanceOf ) relations from category structure by classifying each category link. Previous algorithms mainly focus on lexical patterns of category names to classify whether or not a given category link is an isa/instanceOf. In contrast, our algorithm extracts intrinsic properties that represent the deﬁnition of given category name, and uses those properties to classify each category link. Experimental result shows about 5 to 18 % increase in F-Measure, compared to other existing systems. 
Increasing biosurveillance capacity is a public health priority in both the developed and the developing world. Effective syndromic surveillance is especially important if we are to successfully identify and monitor disease outbreaks in their early stages. This paper describes the construction and preliminary evaluation of a syndromic surveillance orientated application ontology designed to facilitate the early identiﬁcation of Inﬂuenza-LikeIllness syndrome from Emergency Room clinical reports using natural language processing. 
 We discuss the possibility to link the lexicon of an NLP system with a formal ontology in an attempt to construct a semantic analyzer of natural language texts. The work is carried out on the material of sports news published in Russian media.  
Opinion mining is a growing research area both at the natural language processing and the information retrieval communities. Companies, politicians, as well as customers need powerful tools to track opinions, sentiments, judgments and beliefs that people may express in blogs, reviews, audios and videos data regarding a product/service/person/organisation/etc. This work describes our contribution to feature based opinion mining where opinions expressed towards each feature of an object or a product are extracted and summarized. The state of the art has shown that the hierarchical organization of features is a key step. In this context, our goal is to study the role of a domain ontology to structure and extract object features as well as to produce a comprehensive summary. This paper presents the developed system and the experiments we carried out on a case study: French restaurant reviews. Our results show that our approach outperforms standard baselines. 
In this paper we propose a framework of verb semantic description in order to organize different granularity of similarity between verbs. Since verb meanings highly depend on their arguments we propose a verb thesaurus on the basis of possible shared meanings with predicate-argument structure. Motivations of this work are to (1) construct a practical lexicon for dealing with alternations, paraphrases and entailment relations between predicates, and (2) provide a basic database for statistical learning system as well as a theoretical lexicon study such as Generative Lexicon and Lexical Conceptual Structure. One of the characteristics of our description is that we assume several granularities of semantic classes to characterize verb meanings. The thesaurus form allows us to provide several granularities of shared meanings; thus, this gives us a further revision for applying more detailed analyses of verb meanings. 
This paper describes collaborative work on developing Indonesian WordNet in the AsianWordNet (AWN). We will describe the method to develop for collaborative editing to review and complete the translation of synset. This paper aims to create linkage among Asian languages by adopting the concept of semantic relations and synset expressed in WordNet. 
This paper presents an automatic mapping method among large-scale heterogeneous language resources: Sejong Semantic Classes (SJSC) and KorLex. KorLex is a large-scale Korean WordNet, but it lacks specific syntactic & semantic information. Sejong Electronic Dictionary (SJD), of which semantic segmentation depends on SJSC, has much lower lexical coverage than KorLex, but shows refined syntactic & semantic information. The goal of this study is to build a rich language resource for improving Korean semantico-syntactic parsing technology. Therefore, we consider integration of them and propose automatic mapping method with three approaches: 1) Information of Monosemy/Polysemy of Word senses (IMPW), 2) Instances between Nouns of SJD and Word senses of KorLex (INW), and 3) Semantically Related words between Nouns of SJD and Synsets of KorLex (SRNS). We obtain good performance using combined three approaches: recall 0.837, precision 0.717, and F1 0.773. 
In this paper, semantic role labeling(SRL) on Chinese FrameNet is divided into the subtasks of boundary identiﬁcation(BI) and semantic role classiﬁcation(SRC). These subtasks are regarded as the sequential tagging problem at the word level, respectively. We use the conditional random ﬁelds(CRFs) model to train and test on a two-fold cross-validation data set. The extracted features include 11 word-level and 15 shallow syntactic features derived from automatic base chunk parsing. We use the orthogonal array of statistics to arrange the experiment so that the best feature template is selected. The experimental results show that given the target word within a sentence, the best F-measures of SRL can achieve 60.42%. For the BI and SRC subtasks, the best Fmeasures are 70.55 and 81%, respectively. The statistical t-test shows that the improvement of our SRL model is not significant after appending the base chunk features. 
We describe a method for augmenting a bilingual lexicon with additional information for selecting an appropriate translation word. For each word in the source language, we calculate a correlation matrix of its association words versus its translation candidates. We estimate the degree of correlation by using comparable corpora based on these assumptions: “parallel word associations” and “one sense per word association.” In our word translation disambiguation experiment, the results show that our method achieved 42% recall and 49% precision for Japanese-English newspaper texts, and 45% recall and 76% precision for Chinese-Japanese technical documents. 
This paper presents on-going work on constructing bilingual multimodal corpora of referring expressions in collaborative problem solving for English and Japanese. The corpora were collected from dialogues in which two participants collaboratively solved Tangram puzzles with a puzzle simulator. Extra-linguistic information such as operations on puzzle pieces, mouse cursor position and piece positions were recorded in synchronisation with utterances. The speech data was transcribed and time-aligned with the extra-linguistic information. Referring expressions in utterances that refer to puzzle pieces were annotated in terms of their spans, their referents and their other attributes. The Japanese corpus has already been completed, but the English counterpart is still undergoing annotation. We have conducted a preliminary comparative analysis of both corpora, mainly with respect to task completion time, task success rates and attributes of referring expressions. These corpora showed significant differences in task completion time and success rate. 
Emotion, the private state of a human entity, is becoming an important topic in Natural Language Processing (NLP) with increasing use of search engines. The present task aims to manually annotate the sentences in a web based Bengali blog corpus with the emotional components such as emotional expression (word/phrase), intensity, associated holder and topic(s). Ekman’s six emotion classes (anger, disgust, fear, happy, sad and surprise) along with three types of intensities (high, general and low) are considered for the sentence level annotation. Presence of discourse markers, punctuation marks, negations, conjuncts, reduplication, rhetoric knowledge and especially emoticons play the contributory roles in the annotation process. Different types of fixed and relaxed strategies have been employed to measure the agreement of the sentential emotions, intensities, emotional holders and topics respectively. Experimental results for each emotion class at word level on a small set of the whole corpus have been found satisfactory. 
The discipline where sentiment/ opinion/ emotion has been identified and classified in human written text is well known as sentiment analysis. A typical computational approach to sentiment analysis starts with prior polarity lexicons where entries are tagged with their prior out of context polarity as human beings perceive using their cognitive knowledge. Till date, all research efforts found in sentiment lexicon literature deal mostly with English texts. In this article, we propose multiple computational techniques like, WordNet based, dictionary based, corpus based or generative approaches for generating SentiWordNet(s) for Indian languages. Currently, SentiWordNet(s) are being developed for three Indian languages: Bengali, Hindi and Telugu. An online intuitive game has been developed to create and validate the developed SentiWordNet(s) by involving Internet population. A number of automatic, semi-automatic and manual validations and evaluation methodologies have been adopted to measure the coverage and credibility of the developed SentiWordNet(s). 
Opinion mining and sentiment analysis has recently gained increasing attention among the NLP community. Opinion mining is considered a domaindependent task. Constructing lexicons for different domains is labor intensive. In this paper, we propose a framework for constructing Thai language resource for feature-based opinion mining. The feature-based opinion mining essentially relies on the use of two main lexicons, features and polar words. Our approach for extracting features and polar words from opinionated texts is based on syntactic pattern analysis. The evaluation is performed with a case study on hotel reviews. The proposed method has shown to be very effective in most cases. However, in some cases, the extraction is not quite straightforward. The reasons are due to, ﬁrstly, the use of conversational language in written opinionated texts and, secondly, the language semantic. We provide discussion with possible solutions on pattern extraction for some of the challenging cases. 
We present a strategy for revealing event schema in Chinese based on the manual annotation of texts. The overall event information is divided into three levels and events are chosen as the elementary units in annotation. Event-level annotation content and the obtaining of events patterns are explored in detail. The discourse-level annotation, annotation of relations between events and annotation of the functional attributes provide a simple way to represent event schema. 
This paper presents the proposed Query Expansion (QE) techniques based on Khmer speciﬁc characteristics to improve the retrieval performance of Khmer Information Retrieval (IR) system. Four types of Khmer speciﬁc characteristics: spelling variants, synonyms, derivative words and reduplicative words have been investigated in this research. In order to evaluate the effectiveness and the efﬁciency of the proposed QE techniques, a prototype of Khmer IR system has been implemented. The system is built on top of the popular open source information retrieval software library Lucene1. The Khmer word segmentation tool (Chea et al., 2007) is also implemented into the system to improve the accuracy of indexing as well as searching. Furthermore, the Google web search engine is also used in the evaluation process. The results show the proposed QE techniques improve the retrieval performance both of the proposed system and the Google web search engine. With the reduplicative word QE technique, an improvement of 17.93% of recall can be achieved to the proposed system. 
This paper presents a technique for word segmentation for the Urdu OCR system. Word segmentation or word tokenization is a preliminary task for Urdu language processing. Several techniques are available for word segmentation in other languages. A methodology is proposed for word segmentation in this paper which determines the boundaries of words given a sequence of ligatures, based on collocation of ligatures and words in the corpus. Using this technique, word identification rate of 96.10% is achieved, using trigram probabilities normalized over the number of ligatures and words in the sequence. 
Dzongkha, the national language of Bhutan, is continuous in written form and it fails to mark the word boundary. Dzongkha word segmentation is one of the fundamental problems and a prerequisite that needs to be solved before more advanced Dzongkha text processing and other natural language processing tools can be developed. This paper presents our initial attempt at segmenting Dzongkha sentences into words. The paper describes the implementation of Maximal Matching (Dictionary based Approach) followed by bigram techniques (Non-dictionary based Approach) in segmenting the Dzongkha scripts. Although the used techniques are basic and naive, it provides a baseline of the Dzongkha word segmentation task. Preliminary experimental results show percentage of segmentation accuracy. However, the segmentation accuracy is dependent on the type of document domain and size and quality of the lexicon and the corpus. Some of the related issues for future directions are also discussed. Keywords: Dzongkha script, word segmentation, maximal matching, bigram technique, smoothing technique. 
This paper describes the application of probabilistic part of speech taggers to the Dzongkha language. A tag set containing 66 tags is designed, which is based on the Penn Treebank1. A training corpus of 40,247 tokens is utilized to train the model. Using the lexicon extracted from the training corpus and lexicon from the available word list, we used two statistical taggers for comparison reasons. The best result achieved was 93.1% accuracy in a 10-fold cross validation on the training set. The winning tagger was thereafter applied to annotate a 570,247 token corpus. 
This study presents a novel computational approach to the analysis of unaccusative/unergative distinction in Turkish by employing feed-forward artificial neural networks with a backpropagation algorithm. The findings of the study reveal correspondences between semantic notions and syntactic manifestations of unaccusative/unergative distinction in this language, thus presenting a computational analysis of the distinction at the syntax/semantics interface. The approach is applicable to other languages, particularly the ones which lack an explicit diagnostic such as auxiliary selection but has a number of diagnostics instead. 
This paper introduces a preliminary work on Hindi causative verbs: their classification, a linguistic model for their classification and their verb frames. The main objective of this work is to come up with a classification of the Hindi causative verbs. In the classification we show how different types of Hindi verbs have different types of causative forms. It will be a linguistic resource for Hindi causative verbs which can be used in various NLP applications. This resource enriches the already available linguistic resource on Hindi verb frames (Begum et al., 2008b). This resource will be helpful in getting proper insight into Hindi verbs. In this paper, we present the morphology, semantics and syntax of the causative verbs. The morphology is captured by the word generation process; semantics is captured by the linguistic model followed for classifying the verbs and the syntax has been captured by the verb frames using relations given by Panini. 
One of the challenging problems in Thai NLP is to manage a problem on a syntactical analysis of a long sentence. This paper applies conditional random field and categorical grammar to develop a chunking method, which can group words into larger unit. Based on the experiment, we found the impressive results. We gain around 74.17% on sentence level chunking. Furthermore we got a more correct parsed tree based on our technique. Around 50% of tree can be added. Finally, we solved the problem on implicit sentential NP which is one of the difficult Thai language processing. 58.65% of sentential NP is correctly detected. 
The Korean Resource Grammar (KRG) is a computational open-source grammar of Korean (Kim and Yang, 2003) that has been constructed within the DELPH-IN consortium since 2003. This paper reports the second phase of the KRG development that moves from a phenomenabased approach to grammar customization using the LinGO Grammar Matrix. This new phase of development not only improves the parsing efﬁciency but also adds generation capacity, which is necessary for many NLP applications. 
 from the dialect of Delhi region called khari boli  (Masica, 1991).  We develop a grammar for Urdu in We develop a grammar for Urdu that addresses  Grammatical Framework (GF). GF is a problems related to automated text translation  programming language for defining using an Interlingua approach and provide a way  multilingual grammar applications. GF to precisely translate text. This is described in  resource grammar library currently Section 2. Then we describe different levels of  supports 16 languages. These grammars grammar development including morphology  follow an Interlingua approach and (Section 3) and syntax (Section 4). In Section 6,  consist of morphology and syntax we discuss an application in which a semantics-  modules that cover a wide range of driven translation system is built upon these  features of a language. In this paper we components.  explore different syntactic features of the  Urdu language, and show how to fit them 2. GF (Grammatical Framework)  in the multilingual framework of GF. We  also discuss how we cover some of the GF (Grammatical Framework, Ranta 2004) is a  distinguishing features of Urdu such as, tool for working with grammars, implementing a  ergativity in verb agreement (see Sec programming language for writing grammars  4.2). The main purpose of GF resource which in term is based on a mathematical theory  grammar library is to provide an easy about languages and grammars1. Many  way to write natural language multilingual dialog and text generation  applications without knowing the details applications have been built using GF. GF  of syntax, morphology and lexicon. To grammars have two levels the abstract and the  demonstrate it, we use Urdu resource concrete syntax2. The abstract syntax is  grammar to add support for Urdu in the language independent and is common to all  work reported in (Angelov and Ranta, languages in GF grammar library. It is based on  2010) which is an implementation of common syntactic or semantic constructions,  Attempto (Attempto 2008) in GF.  which work for all the involved languages on an  appropriate level of abstraction. The concrete  1. Introduction  syntax is language dependent and defines a  mapping from abstract to actual textual  Urdu is an Indo-European language of the Indo- representation in a specific language2. GF uses  Aryan family, widely spoken in south Asia. It is the term ‘category’ to model different parts of  a national language of Pakistan and one of the speech (e.g verbs, nouns adjectives etc.). An  official languages of India. It is written in a abstract syntax defines a set of categories, as  modified Perso-Arabic script from right to left. well as a set of tree building functions. Concrete  As regards vocabulary, it has a strong influence syntax contains rules telling how these trees are  of Arabic and Persian along with some linearized. Separating the tree building rules  borrowing from Turkish and English. Urdu is an (abstract syntax) from linearization rules  SOV language having fairly free word order. It (concrete syntax) makes it possible to have  __is__c_l_o_s_e_ly___re_l_a_t_e_d__to___H_i_n_d_i__a_s__b_oth originated multiple concrete syntaxes for one abstract. This 
This paper presents a current status of Thai resources and tools for CG development. We also proposed a Thai categorial dependency grammar (CDG), an extended version of CG which includes dependency analysis into CG notation. Beside, an idea of how to group a word that has the same functions are presented to gain a certain type of category per word. We also discuss about a difficulty of building treebank and mention a toolkit for assisting on a Thai CGs tree building and a tree format representations. In this paper, we also give a summary of applications related to Thai CGs. 
As the smallest meaning-bearing elements of the languages which have rich morphology information, morphemes are often integrated into state-of-the-art statistical machine translation to improve translation quality. The paper proposes an approach which novelly uses morphemes as pivot language in a chained machine translation system. A machine translation based method is used therein to ﬁnd the mapping relations between morphemes and words. Experiments show the effectiveness of our approach, achieving 18.6 percent increase in BLEU score over the baseline phrase-based machine translation system. 
In view of the increasing need to facilitate processing the content of scientiﬁc papers, we present an annotation scheme for annotating full papers with zones of conceptualisation, reﬂecting the information structure and knowledge types which constitute a scientiﬁc investigation. The latter are the Core Scientiﬁc Concepts (CoreSCs) and include Hypothesis, Motivation, Goal, Object, Background, Method, Experiment, Model, Observation, Result and Conclusion. The CoreSC scheme has been used to annotate a corpus of 265 full papers in physical chemistry and biochemistry and we are currently automating the recognition of CoreSCs in papers. We discuss how the CoreSC scheme relates to other views of scientiﬁc papers and indeed how the former could be used to help identify negation and speculation in scientiﬁc texts. 
In this paper we describe the creation of a consensus corpus that was obtained through combining three individual annotations of the same clinical corpus in Swedish. We used a few basic rules that were executed automatically to create the consensus. The corpus contains negation words, speculative words, uncertain expressions and certain expressions. We evaluated the consensus using it for negation and speculation cue detection. We used Stanford NER, which is based on the machine learning algorithm Conditional Random Fields for the training and detection. For comparison we also used the clinical part of the BioScope Corpus and trained it with Stanford NER. For our clinical consensus corpus in Swedish we obtained a precision of 87.9 percent and a recall of 91.7 percent for negation cues, and for English with the Bioscope Corpus we obtained a precision of 97.6 percent and a recall of 96.7 percent for negation cues. 
Electronic Health Records (EHRs) contain a large amount of free text documentation which is potentially very useful for Information Retrieval and Text Mining applications. We have, in an initial annotation trial, annotated 6 739 sentences randomly extracted from a corpus of Swedish EHRs for sentence level (un)certainty, and token level speculative keywords and negations. This set is split into different clinical practices and analyzed by means of descriptive statistics and pairwise Inter-Annotator Agreement (IAA) measured by F1-score. We identify geriatrics as a clinical practice with a low average amount of uncertain sentences and a high average IAA, and neurology with a high average amount of uncertain sentences. Speculative words are often n-grams, and uncertain sentences longer than average. The results of this analysis is to be used in the creation of a new annotated corpus where we will reﬁne and further develop the initial annotation guidelines and introduce more levels of dimensionality. Once we have ﬁnalized our guidelines and reﬁned the annotations we plan to release the corpus for further research, after ensuring that no identiﬁable information is included. 
We explore the role negation and speculation identification plays in the multi-label document-level classification of medical reports for diseases. We identify the polarity of assertions made on noun phrases which reference diseases in the medical reports. We experiment with two machine learning classifiers: one based upon Lucene and the other based upon BoosTexter. We find the performance of these systems on document-level classification of medical reports for diseases fails to show improvement when their input is enhanced by the polarity of assertions made on noun phrases. We conclude that due to the nature of our machine learning classifiers, information on the polarity of phrase-level assertions does not improve performance on our data in a multilabel document-level classification task. 
One emergent field in text mining tools applied to biological texts is the automatic detection of speculative sentences. In this paper, we test on a large scale BioExcom, a rule-based system which annotates and categorizes automatically speculative sentences (“prior” and “new”). This work enables us to highlight a more restrictive way to consider speculations, viewed as a source of knowledge, and to discuss the criteria used to determine if a sentence is speculative or not. By doing so, we demonstrate the efficiency of BioExcom to extract these types of speculations and we argue the importance of this tool for biologists, who are also interested in finding hypotheses. 
In this initial annotation study, we suggest an appropriate approach for determining the level of certainty in text, including classiﬁcation into multiple levels of certainty, types of statement and indicators of ampliﬁed certainty. A primary evaluation, based on pairwise inter-annotator agreement (IAA) using F1-score, is performed on a small corpus comprising documents from the World Bank. While IAA results are low, the analysis will allow further reﬁnement of the created guidelines. 
A general characteristic of most biomedical disciplines is their primarily experimental character. Discoveries are obtained through molecular biology and biochemical techniques that allow understanding of biological processes at the molecular level. To qualify biological events, it is of practical signiﬁcance to detect speciﬁc types of negations that can imply either that a given event is not observed under speciﬁc conditions or even the opposite, that a given event is true by altering the bio-entities studied (e.g. introducing speciﬁc modiﬁcations like mutations). Of special interest is also to determine if a detected assertion is linked to experimental support provided by the authors. Finding experimental qualiﬁer cues and detecting experimental technique mentions is of great interest to the biological community in general and particularly for annotation databases. A short overview of different types of negations and biological qualiﬁers of practical relevance will be provided. 
In Natural Language Processing, negation and modality have mostly been handled using the older, pre-statistical methodologies of formal representations subject to rule-based processing. This ﬁts the traditional treatment of negation and modality in logic-based knowledge representation and linguistics. However, in modern-day statistics-based NLP, how exactly negation and modality should be taken into account, and what role these phenomena play overall, is much less clear. The closest statistics-based NLP gets to semantics at this time is lexical-based word distributions (such as used in word sense disambiguation) and topic models (such as produced by Latent Dirichlet Allocation). What exactly in such representations should a negation or a modality actually apply to? What would, or should, the resulting effects be? The traditional approaches are of little or no help. In this talk I argue that neither model is adequate, and that one needs a different model of semantics to be able to accommodate negation and modality. The traditional formalisms are impoverished in their absence of an explicit representation of the denotations of each symbol, and the statistics-based word distributions do not support the compositionality required of semantics since it is unclear how to link together two separate word distributions in a semantically meaningful way. A kind of hybrid, which one could call Distributional Semantics, should be formulated to include the necessary aspects of both: the ability to carry explicit word associations that are still partitioned so as to allow negation and modality to affect the representations in intuitively plausible ways is what is required. 
Automatic detection of linguistic negation in free text is a critical need for many text processing applications, including sentiment analysis. This paper presents a negation detection system based on a conditional random ﬁeld modeled using features from an English dependency parser. The scope of negation detection is limited to explicit rather than implied negations within single sentences. A new negation corpus is presented that was constructed for the domain of English product reviews obtained from the open web, and the proposed negation extraction system is evaluated against the reviews corpus as well as the standard BioScope negation corpus, achieving 80.0% and 75.5% F1 scores, respectively. The impact of accurate negation detection on a state-of-the-art sentiment analysis system is also reported. 
This paper presents a survey on the role of negation in sentiment analysis. Negation is a very common linguistic construction that affects polarity and, therefore, needs to be taken into consideration in sentiment analysis. We will present various computational approaches modeling negation in sentiment analysis. We will, in particular, focus on aspects, such as level of representation used for sentiment analysis, negation word detection and scope of negation. We will also discuss limits and challenges of negation modeling on that task. 
The correct interpretation of biomedical texts by text mining systems requires the recognition of a range of types of high-level information (or meta-knowledge) about the text. Examples include expressions of negation and speculation, as well as pragmatic/rhetorical intent (e.g. whether the information expressed represents a hypothesis, generally accepted knowledge, new experimental knowledge, etc.) Although such types of information have previously been annotated at the text-span level (most commonly sentences), annotation at the level of the event is currently quite sparse. In this paper, we focus on the evaluation of the multi-dimensional annotation scheme that we have developed specifically for enriching bio-events with meta-knowledge information. Our annotation scheme is intended to be general enough to allow integration with different types of bio-event annotation, whilst being detailed enough to capture important subtleties in the nature of the metaknowledge expressed in the text. To our knowledge, our scheme is unique within the field with regards to the diversity of metaknowledge aspects annotated for each event, whilst the evaluation results have confirmed its feasibility and soundness. 
In this paper we explore the identification of negated molecular events (e.g. protein binding, gene expressions, regulation, etc.) in biomedical research abstracts. We construe the problem as a classification task and apply a machine learning (ML) approach that uses lexical, syntactic, and semantic features associated with sentences that represent events. Lexical features include negation cues, whereas syntactic features are engineered from constituency parse trees and the command relation between constituents. Semantic features include event type and participants. We also consider a rule-based approach that uses only the command relation. On a test dataset, the ML approach showed significantly better results (51% F-measure) compared to the commandbased rules (35-42% F-measure). Training a separate classifier for each event class proved to be useful, as the micro-averaged F-score improved to 63% (with 88% precision), demonstrating the potential of task-specific ML approaches to negation detection. 
In this paper we investigate the relation between positive and negative pairs in Textual Entailment (TE), in order to highlight the role of contradiction in TE datasets. We base our analysis on the decomposition of Text-Hypothesis pairs into monothematic pairs, i.e. pairs where only one linguistic phenomenon at a time is responsible for entailment judgment and we argue that such a deeper inspection of the linguistic phenomena behind textual entailment is necessary in order to highlight the role of contradiction. We support our analysis with a number of empirical experiments, which use current available TE systems. 
Hedge cues were detected using a supervised Conditional Random Field (CRF) classiﬁer exploiting features from the RASP parser. The CRF’s predictions were ﬁltered using known cues and unseen instances were removed, increasing precision while retaining recall. Rules for scope detection, based on the grammatical relations of the sentence and the part-ofspeech tag of the cue, were manuallydeveloped. However, another supervised CRF classiﬁer was used to reﬁne these predictions. As a ﬁnal step, scopes were constructed from the classiﬁer output using a small set of post-processing rules. Development of the system revealed a number of issues with the annotation scheme adopted by the organisers. 
Detecting speculative assertions is essential to distinguish the facts from uncertain information for biomedical text. This paper describes a system to detect hedge cues and their scope using CRF model. HCDic feature is presented to improve the system performance of detecting hedge cues on BioScope corpus. The feature can make use of crossdomain resources. 
With the dramatic growth of scientiﬁc publishing, Information Extraction (IE) systems are becoming an increasingly important tool for large scale data analysis. Hedge detection and uncertainty classiﬁcation are important components of a high precision IE system. This paper describes a two part supervised system which classiﬁes words as hedge or nonhedged and sentences as certain or uncertain in biomedical and Wikipedia data. In the ﬁrst stage, our system trains a logistic regression classiﬁer to detect hedges based on lexical and Part-of-Speech collocation features. In the second stage, we use the output of the hedge classiﬁer to generate sentence level features based on the number of hedge cues, the identity of hedge cues, and a Bag-of-Words feature vector to train a logistic regression classiﬁer for sentence level uncertainty. With the resulting classiﬁcation, an IE system can then discard facts and relations extracted from these sentences or treat them as appropriately doubtful. We present results for in domain training and testing and cross domain training and testing based on a simple union of training sets. 
Our CoNLL-2010 speculative sentence detector disambiguates putative keywords based on the following considerations: a speculative keyword may be composed of one or more word tokens; a speculative sentence may have one or more speculative keywords; and if a sentence contains at least one real speculative keyword, it is deemed speculative. A tree kernel classiﬁer is used to assess whether a potential speculative keyword conveys speculation. We exploit information implicit in tree structures. For prediction efﬁciency, only a segment of the whole tree around a speculation keyword is considered, along with morphological features inside the segment and information about the containing document. A maximum entropy classiﬁer is used for sentences not covered by the tree kernel classiﬁer. Experiments on the Wikipedia data set show that our system achieves 0.55 F-measure (in-domain). 
In this work, we explore the use of SVMs and CRFs in the problem of predicting certainty in sentences. We consider this as a task of tagging uncertainty cues in context, for which we used lexical, wordlist-based and deep-syntactic features. Results show that the syntactic context of the tokens in conjunction with the wordlist-based features turned out to be useful in predicting uncertainty cues. 
This paper presents an algorithm for unsupervised co-occurrence based parsing that improves and extends existing approaches. The proposed algorithm induces a contextfree grammar of the language in question in an iterative manner. The resulting structure of a sentence will be given as a hierarchical arrangement of constituents. Although this algorithm does not use any a priori knowledge about the language, it is able to detect heads, modiﬁers and a phrase type’s different compound composition possibilities. For evaluation purposes, the algorithm is applied to manually annotated part-of-speech tags (POS tags) as well as to word classes induced by an unsupervised part-of-speech tagger. 
We show that Viterbi (or “hard”) EM is well-suited to unsupervised grammar induction. It is more accurate than standard inside-outside re-estimation (classic EM), signiﬁcantly faster, and simpler. Our experiments with Klein and Manning’s Dependency Model with Valence (DMV) attain state-of-the-art performance — 44.8% accuracy on Section 23 (all sentences) of the Wall Street Journal corpus — without clever initialization; with a good initializer, Viterbi training improves to 47.9%. This generalizes to the Brown corpus, our held-out set, where accuracy reaches 50.8% — a 7.5% gain over previous best results. We ﬁnd that classic EM learns better from short sentences but cannot cope with longer ones, where Viterbi thrives. However, we explain that both algorithms optimize the wrong objectives and prove that there are fundamental disconnects between the likelihoods of sentences, best parses, and true parses, beyond the wellestablished discrepancies between likelihood, accuracy and extrinsic performance. 
Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. 
A central problem for NLP is grammar induction: the development of unsupervised learning algorithms for syntax. In this paper we present a lattice-theoretic representation for natural language syntax, called Distributional Lattice Grammars. These representations are objective or empiricist, based on a generalisation of distributional learning, and are capable of representing all regular languages, some but not all context-free languages and some noncontext-free languages. We present a simple algorithm for learning these grammars together with a complete self-contained proof of the correctness and efﬁciency of the algorithm. 
This paper describes a new method for unsupervised grammar induction based on the automatic extraction of certain patterns in the texts. Our starting hypothesis is that there exist some classes of words that function as separators, marking the beginning or the end of new constituents. Among these separators we distinguish those which trigger new levels in the parse tree. If we are able to detect these separators we can follow a very simple procedure to identify the constituents of a sentence by taking the classes of words between separators. This paper is devoted to describe the process that we have followed to automatically identify the set of separators from a corpus only annotated with Part-of-Speech (POS) tags. The proposed approach has allowed us to improve the results of previous proposals when parsing sentences from the Wall Street Journal corpus. 
This work shows how to improve state-of-the-art monolingual natural language processing models using unannotated bilingual text. We build a multiview learning objective that enforces agreement between monolingual and bilingual models. In our method the ﬁrst, monolingual view consists of supervised predictors learned separately for each language. The second, bilingual view consists of log-linear predictors learned over both languages on bilingual text. Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model, and we show how to combine the two models to account for dependence between views. For the task of named entity recognition, using bilingual predictors increases F1 by 16.1% absolute over a supervised monolingual model, and retraining on bilingual predictions increases monolingual model F1 by 14.6%. For syntactic parsing, our bilingual predictor increases F1 by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%. 
Modern unsupervised POS taggers usually apply an optimization procedure to a nonconvex function, and tend to converge to local maxima that are sensitive to starting conditions. The quality of the tagging induced by such algorithms is thus highly variable, and researchers report average results over several random initializations. Consequently, applications are not guaranteed to use an induced tagging of the quality reported for the algorithm. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We run a base tagger with different random initializations, and select the best tagging using the quality test. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipﬁan, allowing us to utilize a perplexity-based quality test. We show that the correlation between our quality test and gold standard-based tagging quality measures is high. Our results are better in most evaluation measures than all results reported in the literature for this task, and are always better than the Clark average results. 
We demonstrate that relational features derived from dependency-syntactic and semantic role structures are useful for the task of detecting opinionated expressions in natural-language text, signiﬁcantly improving over conventional models based on sequence labeling with local features. These features allow us to model the way opinionated expressions interact in a sentence over arbitrary distances. While the relational features make the prediction task more computationally expensive, we show that it can be tackled effectively by using a reranker. We evaluate a number of machine learning approaches for the reranker, and the best model results in a 10-point absolute improvement in soft recall on the MPQA corpus, while decreasing precision only slightly. 
Clustering is a central technique in NLP. Consequently, clustering evaluation is of great importance. Many clustering algorithms are evaluated by their success in tagging corpus tokens. In this paper we discuss type level evaluation, which reﬂects class membership only and is independent of the token statistics of a particular reference corpus. Type level evaluation casts light on the merits of algorithms, and for some applications is a more natural measure of the algorithm’s quality. We propose new type level evaluation measures that, contrary to existing measures, are applicable when items are polysemous, the common case in NLP. We demonstrate the beneﬁts of our measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 
In this paper we present a cognitively plausible approach to word segmentation that segments in an online fashion using only local information and a lexicon of previously segmented words. Unlike popular statistical optimization techniques, the learner uses structural information of the input syllables rather than distributional cues to segment words. We develop a memory model for the learner that like a child learner does not recall previously hypothesized words perfectly. The learner attains an F-score of 86.69% in ideal conditions and 85.05% when word recall is unreliable and stress in the input is reduced. These results demonstrate the power that a simple learner can have when paired with appropriate structural constraints on its hypotheses. 
Prior work on training the IBM-3 translation model is based on suboptimal methods for computing Viterbi alignments. In this paper, we present the ﬁrst method guaranteed to produce globally optimal alignments. This not only results in improved alignments, it also gives us the opportunity to evaluate the quality of standard hillclimbing methods. Indeed, hillclimbing works reasonably well in practice but still fails to ﬁnd the global optimum for between 2% and 12% of all sentence pairs and the probabilities can be several tens of orders of magnitude away from the Viterbi alignment. By reformulating the alignment problem as an Integer Linear Program, we can use standard machinery from global optimization theory to compute the solutions. We use the well-known branch-and-cut method, but also show how it can be customized to the speciﬁc problem discussed in this paper. In fact, a large number of alignments can be excluded from the start without losing global optimality. 
Sarcasm is a form of speech act in which the speakers convey their message in an implicit way. The inherently ambiguous nature of sarcasm sometimes makes it hard even for humans to decide whether an utterance is sarcastic or not. Recognition of sarcasm can beneﬁt many sentiment analysis NLP applications, such as review summarization, dialogue systems and review ranking systems. In this paper we experiment with semisupervised sarcasm identiﬁcation on two very different data sets: a collection of 5.9 million tweets collected from Twitter, and a collection of 66000 product reviews from Amazon. Using the Mechanical Turk we created a gold standard sample in which each sentence was tagged by 3 annotators, obtaining F-scores of 0.78 on the product reviews dataset and 0.83 on the Twitter dataset. We discuss the differences between the datasets and how the algorithm uses them (e.g., for the Amazon dataset the algorithm makes use of structured information). We also discuss the utility of Twitter #sarcasm hashtags for the task. 
Probabilistic phrase-based synchronous grammars are now considered promising devices for statistical machine translation because they can express reordering phenomena between pairs of languages. Learning these hierarchical, probabilistic devices from parallel corpora constitutes a major challenge, because of multiple latent model variables as well as the risk of data overﬁtting. This paper presents an effective method for learning a family of particular interest to MT, binary Synchronous Context-Free Grammars with inverted/monotone orientation (a.k.a. Binary ITG). A second contribution concerns devising a lexicalized phrase reordering mechanism that has complimentary strengths to Chiang’s model. The latter conditions reordering decisions on the surrounding lexical context of phrases, whereas our mechanism works with the lexical content of phrase pairs (akin to standard phrase-based systems). Surprisingly, our experiments on French-English data show that our learning method applied to far simpler models exhibits performance indistinguishable from the Hiero system. 
The availability of substantial, in-domain parallel corpora is critical for the development of high-performance statistical machine translation (SMT) systems. Such corpora, however, are expensive to produce due to the labor intensive nature of manual translation. We propose to alleviate this problem with a novel, semisupervised, batch-mode active learning strategy that attempts to maximize indomain coverage by selecting sentences, which represent a balance between domain match, translation difﬁculty, and batch diversity. Simulation experiments on an English-to-Pashto translation task show that the proposed strategy not only outperforms the random selection baseline, but also traditional active learning techniques based on dissimilarity to existing training data. Our approach achieves a relative improvement of 45.9% in BLEU over the seed baseline, while the closest competitor gained only 24.8% with the same number of selected sentences. 
Supervised learning has been recently used to improve the performance of word alignment. However, due to the limited amount of labeled data, the performance of ”pure” supervised learning, which only used labeled data, is limited. As a result, many existing methods employ features learnt from a large amount of unlabeled data to assist the task. In this paper, we propose a semi-supervised ensemble method to better incorporate both labeled and unlabeled data during learning. Firstly, we employ an ensemble learning framework, which effectively uses alignment results from different unsupervised alignment models. We then propose to use a semi-supervised learning method, namely Tri-training, to train classiﬁers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The ﬁnal translation quality of a phrase-based translation system is slightly improved, as well. 
This paper presents a comparative study of three closely related Bayesian models for unsupervised document level sentiment classiﬁcation, namely, the latent sentiment model (LSM), the joint sentimenttopic (JST) model, and the Reverse-JST model. Extensive experiments have been conducted on two corpora, the movie review dataset and the multi-domain sentiment dataset. It has been found that while all the three models achieve either better or comparable performance on these two corpora when compared to the existing unsupervised sentiment classiﬁcation approaches, both JST and Reverse-JST are able to extract sentiment-oriented topics. In addition, Reverse-JST always performs worse than JST suggesting that the JST model is more appropriate for joint sentiment topic detection. 
In this paper, the authors present a new approach to sentence level sentiment analysis. The aim is to determine whether a sentence expresses a positive, negative or neutral sentiment, as well as its intensity. The method performs WSD over the words in the sentence in order to work with concepts rather than terms, and makes use of the knowledge in an affective lexicon to label these concepts with emotional categories. It also deals with the effect of negations and quantifiers on polarity and intensity analysis. An extensive evaluation in two different domains is performed in order to determine how the method behaves in 2classes (positive and negative), 3-classes (positive, negative and neutral) and 5-classes (strongly negative, weakly negative, neutral, weakly positive and strongly positive) classification tasks. The results obtained compare favorably with those achieved by other systems addressing similar evaluations. 
Recent work in computer vision has aimed to associate image regions with keywords describing the depicted entities, but actual image ‘understanding’ would also require identifying their attributes, relations and activities. Since this information cannot be conveyed by simple keywords, we have collected a corpus of “action” photos each associated with ﬁve descriptive captions. In order to obtain a consistent semantic representation for each image, we need to ﬁrst identify which NPs refer to the same entities. We present three hierarchical Bayesian models for cross-caption coreference resolution. We have also created a simple ontology of entity classes that appear in images and evaluate how well these can be recovered. 
We present a simple technique for learning better SVMs using fewer training examples. Rather than using the standard SVM regularization, we regularize toward low weight-variance. Our new SVM objective remains a convex quadratic function of the weights, and is therefore computationally no harder to optimize than a standard SVM. Variance regularization is shown to enable dramatic improvements in the learning rates of SVMs on three lexical disambiguation tasks. 
Children learn a robust representation of lexical categories at a young age. We propose an incremental model of this process which efﬁciently groups words into lexical categories based on their local context using an information-theoretic criterion. We train our model on a corpus of childdirected speech from CHILDES and show that the model learns a ﬁne-grained set of intuitive word categories. Furthermore, we propose a novel evaluation approach by comparing the efﬁciency of our induced categories against other category sets (including traditional part of speech tags) in a variety of language tasks. We show the categories induced by our model typically outperform the other category sets. 
We propose a method for annotating postto-post discourse structure in online user forum data, in the hopes of improving troubleshooting-oriented information access. We introduce the tasks of: (1) post classiﬁcation, based on a novel dialogue act tag set; and (2) link classiﬁcation. We also introduce three feature sets (structural features, post context features and semantic features) and experiment with three discriminative learners (maximum entropy, SVM-HMM and CRF). We achieve abovebaseline results for both dialogue act and link classiﬁcation, with interesting divergences in which feature sets perform well over the two sub-tasks, and go on to perform preliminary investigation of the interaction between post tagging and linking. 
Both entity and relation extraction can beneﬁt from being performed jointly, allowing each task to correct the errors of the other. We present a new method for joint entity and relation extraction using a graph we call a “card-pyramid.” This graph compactly encodes all possible entities and relations in a sentence, reducing the task of their joint extraction to jointly labeling its nodes. We give an efﬁcient labeling algorithm that is analogous to parsing using dynamic programming. Experimental results show improved results for our joint extraction method compared to a pipelined approach. 
Recent speed-ups for training large-scale models like those found in statistical NLP exploit distributed computing (either on multicore or “cloud” architectures) and rapidly converging online learning algorithms. Here we aim to combine the two. We focus on distributed, “mini-batch” learners that make frequent updates asynchronously (Nedic et al., 2001; Langford et al., 2009). We generalize existing asynchronous algorithms and experiment extensively with structured prediction problems from NLP, including discriminative, unsupervised, and non-convex learning scenarios. Our results show asynchronous learning can provide substantial speedups compared to distributed and singleprocessor mini-batch algorithms with no signs of error arising from the approximate nature of the technique. 
In this paper, we provide a theoretical framework for feature selection in tree kernel spaces based on gradient-vector components of kernel-based machines. We show that a huge number of features can be discarded without a signiﬁcant decrease in accuracy. Our selection algorithm is as accurate as and much more efﬁcient than those proposed in previous work. Comparative experiments on three interesting and very diverse classiﬁcation tasks, i.e. Question Classiﬁcation, Relation Extraction and Semantic Role Labeling, support our theoretical ﬁndings and demonstrate the algorithm performance. 
We propose the notion of a structural bias inherent in a parsing system with respect to the language it is aiming to parse. This structural bias characterizes the behaviour of a parsing system in terms of structures it tends to under- and over- produce. We propose a Boosting-based method for uncovering some of the structural bias inherent in parsing systems. We then apply our method to four English dependency parsers (an Arc-Eager and Arc-Standard transition-based parsers, and ﬁrst- and second-order graph-based parsers). We show that all four parsers are biased with respect to the kind of annotation they are trained to parse. We present a detailed analysis of the biases that highlights speciﬁc differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses. 
Dimensionality reduction has been shown to improve processing and information extraction from high dimensional data. Word space algorithms typically employ linear reduction techniques that assume the space is Euclidean. We investigate the effects of extracting nonlinear structure in the word space using Locality Preserving Projections, a reduction algorithm that performs manifold learning. We apply this reduction to two common word space models and show improved performance over the original models on benchmarks. 
This work focuses on the empirical investigation of distributional models for the automatic acquisition of frame inspired predicate words. While several semantic spaces, both word-based and syntaxbased, are employed, the impact of geometric representation based on dimensionality reduction techniques is investigated. Data statistics are accordingly studied along two orthogonal perspectives: Latent Semantic Analysis exploits global properties while Locality Preserving Projection emphasizes the role of local regularities. This latter is employed by embedding prior FrameNet-derived knowledge in the corresponding non-euclidean transformation. The empirical investigation here reported sheds some light on the role played by these spaces as complex kernels for supervised (i.e. Support Vector Machine) algorithms: their use conﬁgures, as a novel way to semi-supervised lexical learning, a highly appealing research direction for knowledge rich scenarios like FrameNet-based semantic parsing. 
In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation. More speciﬁcally, we argue for a more ﬂexible representation of word meaning than the assignment of a single best-ﬁtting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees. Or move away from dictionary senses completely, and only model similarities between individual word usages. We argue that distributional models provide a ﬂexible framework for experimenting with alternative models of word meanings, and discuss example models. 
In this paper we investigate methods for computing similarity of two phrases based on their relatedness scores across all ranks k in a SVD approximation of a phrase/term co-occurrence matrix. We conﬁrm the major observations made in previous work and our preliminary experiments indicate that these methods can lead to reliable similarity scores which in turn can be used for the task of paraphrasing. 
In this paper we explore the computational modelling of compositionality in distributional models of semantics. In particular, we model the semantic composition of pairs of adjacent English Adjectives and Nouns from the British National Corpus. We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens. We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model. We propose two evaluation methods for the implemented models. Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research. 
We describe an algebraic approach for computing with vector based semantics. The tensor product has been proposed as a method of composition, but has the undesirable property that strings of different length are incomparable. We consider how a quotient algebra of the tensor algebra can allow such comparisons to be made, offering the possibility of data-driven models of semantic composition. 
In this paper, we propose a memory, space, and time efﬁcient framework to scale distributional similarity to the web. We exploit sketch techniques, especially the Count-Min sketch, which approximates the frequency of an item in the corpus without explicitly storing the item itself. These methods use hashing to deal with massive amounts of the streaming text. We store all item counts computed from 90 GB of web data in just 2 billion counters (8 GB main memory) of CM sketch. Our method returns semantic similarity between word pairs in O(K) time and can compute similarity between any word pairs that are stored in the sketch. In our experiments, we show that our framework is as effective as using the exact counts. 
Recent work applied Dirichlet Process Mixture Models to the task of verb clustering, incorporating supervision in the form of must-links and cannot-links constraints between instances. In this work, we introduce an active learning approach for constraint selection employing uncertaintybased sampling. We achieve substantial improvements over random selection on two datasets. 
We present an episodic memory component for enhancing the dialogue of artiﬁcial companions with the capability to refer to, take up and comment on past interactions with the user, and to take into account in the dialogue long-term user preferences and interests. The proposed episodic memory is based on RDF representations of the agent’s experiences and is linked to the agent’s semantic memory containing the agent’s knowledge base of ontological data and information about the user’s interests. 
We present a family of Embodied Conversational Agents (ECAs) using Talking Head technology, along with a program of associated research and user trials. Whilst antecedents of our current ECAs include “chatbots” desgined to pass the Turing Test (TT) or win a Loebner Prize (LP), our current agents are task-oriented Teaching Agents and Social Companions. The current focus for our research includes the role of emotion, expression and gesture in our agents/companions, the explicit teaching of such social skills as recognizing and displaying appropriate expressions/gestures, and the integration of template/database-based dialogue managers with more conversational TT/LP systems as well as with audio-visual speech/gesture recognition/synthesis technologies. 
In line with the growing interest in conversational agents as companions, we are developing a toy companion for children that is capable of engaging interactions and of developing a long-term relationship with them, and is extensible so as to evolve with them. In this paper, we investigate the importance of personalising interaction both for engagement and for long-term relationship development. In particular, we propose a framework for representing, gathering and using personal knowledge about the child during dialogue interaction. 1 
 A technology demonstrator is one thing but having people use a technology is another, and the result reported here is that people often ignore our lovingly crafted handiwork. The SERA project - Social Engagement with Robots and Agents was set up to look explicitly at what happens when a robot companion is put in someone’s home. Even if things worked perfectly, there are times when a companion’s human is simply not engaged. As a result we have separated our “dialog manager” into two parts: the dialog manager itself that determines what to say next, and an “interaction manager” that determines when to say it. This paper details the design of this SALT-E architecture. 
An enduring challenge in humancomputer interaction (HCI) research is the creation of natural and intuitive interfaces. Besides the obvious requirement that such interfaces communicate over modalities such as natural language (especially spoken) and gesturing that are more natural for humans, exhibiting affect and adaptivity have also been identiﬁed as important factors to the interface’s acceptance by the user. In the work presented here, we propose a novel architecture for affective and multimodal dialogue systems that allows explicit control over the personality traits that we want the system to exhibit. More speciﬁcally, we approach personality as a means of synthesising different, and possibly conﬂicting, adaptivity models into an overall model to be used to drive the interaction components of the system. Furthermore, this synthesis is performed in the presence of domain knowledge, so that domain structure and relations inﬂuence the results of the calculation. 
 2 From Dialogue to Conversation  We describe a ‘How was your day?’ (HWYD) Companion whose purpose is to establish a comforting and supportive relationship with a user via a conversation on a variety of work-related topics. The system has several fairly novel features aimed at increasing the naturalness of the interaction: a rapid ‘short loop’ response primed by the results of acoustic emotion analysis, and an ‘interruption manager’, enabling the user to interrupt lengthy or apparently inappropriate system responses, prompting a replanning of behaviour on the part of the system. The ‘long loop’ also takes into account the emotional state of the user, but using more conventional dialogue management and planning techniques. We describe the architecture and components of the implemented prototype HWYD system. 
The purpose of this research was to advance the understanding of the behavior of small groups in online chat rooms. The research was conducted using Internet chat data collected through planned exercises with recruited participants. Analysis of the collected data led to construction of preliminary models of social behavior in online discourse. Some of these models, e.g., how to effectively change the topic of conversation, were subsequently implemented into an automated Virtual Chat Agent (VCA) prototype. VCA has been demonstrated to perform effectively and convincingly in Internet conversation in multiparty chat environments. 
We discuss the problem of model adaptation for the task of named entity recognition with respect to the variation of label distributions in data from different domains. We investigate an adaptive extension of the sequence perceptron, where the adaptive component includes parameters estimated from unlabelled data in combination with background knowledge in the form of gazetteers. We apply this idea empirically on adaptation experiments involving two newswire datasets from different domains and compare with other popular methods such as self training and structural correspondence learning. 
We report results from a domain adaptation task for statistical machine translation (SMT) using cache-based adaptive language and translation models. We apply an exponential decay factor and integrate the cache models in a standard phrasebased SMT decoder. Without the need for any domain-speciﬁc resources we obtain a 2.6% relative improvement on average in BLEU scores using our dynamic adaptation procedure. 
We are interested in improving the summarization of conversations by using domain adaptation. Since very few email corpora have been annotated for summarization purposes, we attempt to leverage the labeled data available in the multiparty meetings domain for the summarization of email threads. In this paper, we compare several approaches to supervised domain adaptation using out-ofdomain labeled data, and also try to use unlabeled data in the target domain through semi-supervised domain adaptation. From the results of our experiments, we conclude that with some in-domain labeled data, training in-domain with no adaptation is most effective, but that when there is no labeled in-domain data, domain adaptation algorithms such as structural correspondence learning can improve summarization. 
Most supervised language processing systems show a signiﬁcant drop-off in performance when they are tested on text that comes from a domain signiﬁcantly different from the domain of the training data. Sequence labeling systems like partof-speech taggers are typically trained on newswire text, and in tests their error rate on, for example, biomedical data can triple, or worse. We investigate techniques for building open-domain sequence labeling systems that approach the ideal of a system whose accuracy is high and constant across domains. In particular, we investigate unsupervised techniques for representation learning that provide new features which are stable across domains, in that they are predictive in both the training and out-of-domain test data. In experiments, our novel techniques reduce error by as much as 29% relative to the previous state of the art on out-of-domain text. 
Many natural language processing (NLP) tools exhibit a decrease in performance when they are applied to data that is linguistically different from the corpus used during development. This makes it hard to develop NLP tools for domains for which annotated corpora are not available. This paper explores a number of metrics that attempt to predict the cross-domain performance of an NLP tool through statistical inference. We apply different similarity metrics to compare different domains and investigate the correlation between similarity and accuracy loss of NLP tool. We ﬁnd that the correlation between the performance of the tool and the similarity metric is linear and that the latter can therefore be used to predict the performance of an NLP tool on out-of-domain data. The approach also provides a way to quantify the difference between domains. 
We compare self-training with and without reranking for parser domain adaptation, and examine the impact of syntactic parser adaptation on a semantic role labeling system. Although self-training without reranking has been found not to improve in-domain accuracy for parsers trained on the WSJ Penn Treebank, we show that it is surprisingly effective for parser domain adaptation. We also show that simple self-training of a syntactic parser improves out-of-domain accuracy of a semantic role labeler. 
We investigate the classiﬁcation of utterances into high-level dialog act categories using word-based features, under conditions where the train and test data differ by genre and/or language. We handle the cross-language cases with machine translation of the test utterances. We analyze and compare two featurebased approaches to using unlabeled data in adaptation: restriction to a shared feature set, and an implementation of Blitzer et al.’s Structural Correspondence Learning. Both methods lead to increased detection of backchannels in the cross-language cases by utilizing correlations between backchannel words and utterance length. 
In this work, we propose a semisupervised extension to a well-known supervised domain adaptation approach (EA) (Daume´ III, 2007). Our proposed approach (EA++) builds on the notion of augmented space (introduced in EA) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target. This semisupervised approach to domain adaptation is extremely simple to implement, and can be applied as a pre-processing step to any supervised learner. Experimental results on sequential labeling tasks demonstrate the efﬁcacy of the proposed method. 
We consider synchronous tree substitution grammars (STSG). With the help of a characterization of the expressive power of STSG in terms of weighted tree bimorphisms, we show that both the forward and the backward application of an STSG preserve recognizability of weighted tree languages in all reasonable cases. As a consequence, both the domain and the range of an STSG without chain rules are recognizable weighted tree languages. 
Synchronous tree insertion grammars (STIG) are formal models for syntaxbased machine translation. We formalize a decoder for probabilistic STIG; the decoder transforms every source-language string into a target-language tree and calculates the probability of this transformation. 
This paper proposes a uniform framework for the development of parsing and translation algorithms for weighted extended (top-down) tree transducers and input strings. The asymptotic time complexity of these algorithms can be improved in practice by exploiting an algorithm for rule factorization in the above transducers. 
We introduce Millstream systems, a formal model consisting of modules and an interface, where the modules formalise different aspects of language, and the interface links these aspects with each other. 
We investigate the problem of structurally changing lexica, while preserving the information. We present a type of lexicon transformation that is complete on an interesting class of lexica. Our work is motivated by the problem of merging one or more lexica into one lexicon. Lexica, lexicon schemas, and lexicon transformations are all seen as particular kinds of trees. 
We derive and implement an algorithm similar to (Huang and Chiang, 2005) for ﬁnding the n best derivations in a weighted hypergraph. We prove the correctness and termination of the algorithm and we show experimental results concerning its runtime. Our work is different from the aforementioned one in the following respects: we consider labeled hypergraphs, allowing for tree-based language models (Maletti and Satta, 2009); we speciﬁcally handle the case of cyclic hypergraphs; we admit structured weight domains, allowing for multiple features to be processed; we use the paradigm of functional programming together with lazy evaluation, achieving concise algorithmic descriptions. 
We present DIRECTL+: an online discriminative sequence prediction model based on many-to-many alignments, which is further augmented by the incorporation of joint n-gram features. Experimental results show improvement over the results achieved by DIRECTL in 2009. We also explore a number of diverse resource-free and language-independent approaches to transliteration mining, which range from simple to sophisticated. 
The system presented in this paper uses a combination of two techniques to directly transliterate from grapheme to grapheme. The technique makes no language specific assumptions, uses no dictionaries or explicit phonetic information; the process transforms sequences of tokens in the source language directly into to sequences of tokens in the target. All the language pairs in our experiments were transliterated by applying this technique in a single unified manner. The approach we take is that of hypothesis rescoring to integrate the models of two stateof-the-art techniques: phrase-based statistical machine translation (SMT), and a joint multigram model. The joint multigram model was used to generate an n-best list of transliteration hypotheses that were re-scored using the models of the phrase-based SMT system. The both of the models’ scores for each hypothesis were linearly interpolated to produce a final hypothesis score that was used to re-rank the hypotheses. In our experiments on development data, the combined system was able to outperform both of its component systems substantially. 
This paper presents transliteration mining on the ACL 2010 NEWS workshop shared transliteration mining task data. Transliteration mining was done using a generative transliteration model applied on the source language and whose output was constrained on the words in the target language. A total of 30 runs were performed on 5 language pairs, with 6 runs for each language pair. In the presence of limited resources, the runs explored the use of phonetic conflation and iterative training of the transliteration model to improve recall. Using letter conflation improved recall by as much as 48%, with improvements in recall dwarfing drops in precision. Using iterative training improved recall, but often at the cost of significant drops in precision. The best runs typically used both letter conflation and iterative learning. 
Effective transliteration of proper names via grapheme conversion needs to ﬁnd transliteration patterns in training data, and then generate optimized candidates for testing samples accordingly. However, the top-1 accuracy for the generated candidates cannot be good if the right one is not ranked at the top. To tackle this issue, we propose to rerank the output candidates for a better order using the averaged perceptron with multiple features. This paper describes our recent work in this direction for our participation in NEWS2010 transliteration evaluation. The ofﬁcial results conﬁrm its effectiveness in English-Chinese bidirectional transliteration. 
This article describes the first trial on bidirectional Thai-English machine transliteration applied on the NEWS 2010 transliteration corpus. The system relies on segmenting sourcelanguage words into syllable-like units, finding unit's pronunciations, consulting a syllable transliteration table to form target-language word hypotheses, and ranking the hypotheses by using syllable n-gram. The approach yields 84.2% and 70.4% mean F-scores on Englishto-Thai and Thai-to-English transliteration. Discussion on existing problems and future solutions are addressed. 
This paper describes the use of a pair Hidden Markov Model (pair HMM) system in mining transliteration pairs from noisy Wikipedia data. A pair HMM variant that uses nine transition parameters, and emission parameters associated with single character mappings between source and target language alphabets is identiﬁed and used in estimating transliteration similarity. The system resulted in a precision of 78% and recall of 83% when evaluated on a random selection of English-Russian Wikipedia topics. 
This paper presents modeling of transliteration as a phrase-based machine translation system. We used a popular phrasebased machine translation system for English-Hindi machine transliteration. We have achieved an accuracy of 38.1% on the test set. We used some basic rules to modulate the existing phrased-based transliteration system. Our experiments show that phrase-based machine translation systems can be adopted by modulating the system to ﬁt the transliteration problem. 
In this paper, a method is presented to recognize multilingual Wikipedia named entity articles. This method classifies multilingual Wikipedia articles using a variety of structured and unstructured features and is aided by cross-language links and features in Wikipedia. Adding multilingual features helps boost classification accuracy and is shown to effectively classify multilingual pages in a language independent way. Classification is done using Support Vectors Machine (SVM) classifier at first, and then the threshold of SVM is adjusted in order to improve the recall scores of classification. Threshold adjustment is performed using beta-gamma threshold adjustment algorithm which is a post learning step that shifts the hyperplane of SVM. This approach boosted recall with minimal effect on precision. 
Named Entity Recognition and Classiﬁcation (NERC) is a well-studied NLP task typically focused on coarse-grained named entity (NE) classes. NERC for more ﬁne-grained semantic NE classes has not been systematically studied. This paper quantiﬁes the difﬁculty of ﬁne-grained NERC (FG-NERC) when performed at large scale on the people domain. We apply unsupervised acquisition methods to construct a gold standard dataset for FG-NERC. This dataset is used to benchmark methods for classifying NEs at various levels of ﬁne-grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches. Our results indicate high difﬁculty of the task and provide a ‘strong’ baseline for future research. 
Identifying named entities is essential in understanding plain texts. Moreover, the categories of the named entities are indicative of their roles in the texts. In this paper, we propose a novel approach, Deep Belief Nets (DBN), for the Chinese entity mention categorization problem. DBN has very strong representation power and it is able to elaborately self-train for discovering complicated feature combinations. The experiments conducted on the Automatic Context Extraction (ACE) 2004 data set demonstrate the effectiveness of DBN. It outperforms the state-of-the-art learning models such as SVM or BP neural network. 
This paper introduces simplified yet effective features that can robustly identify named entities in Arabic text without the need for morphological or syntactic analysis or gazetteers. A CRF sequence labeling model is trained on features that primarily use character n-gram of leading and trailing letters in words and word n-grams. The proposed features help overcome some of the morphological and orthographic complexities of Arabic. In comparing to results in the literature using Arabic specific features such POS tags on the same dataset and same CRF implementation, the results in this paper are lower by 2 F-measure points for locations, but are better by 8 points for organizations and 9 points for persons. 
In this paper, we present a novel approach for Hindi Named Entity Identiﬁcation (NEI) in a large corpus. The key idea is to harness the global distributional characteristics of the words in the corpus. We show that combining the global distributional characteristics along with the local context information improves the NEI performance over statistical baseline systems that employ only local context. The improvement is very signiﬁcant (about 10%) in scenarios where the test and train corpus belong to different genres. We also propose a novel measure for NEI based on term informativeness and show that it is competitive with the best measure and better than other well known information measures. 
Named Entity Recognition or Extraction (NER) is an important task for automated text processing for industries and academia engaged in the field of language processing, intelligence gathering and Bioinformatics. In this paper we discuss the general problem of Named Entity Recognition, more specifically the challenges in NER in languages that do not have language resources e.g. large annotated corpora. We specifically address the challenges for Urdu NER and differentiate it from other South Asian (Indic) languages. We discuss the differences between Hindi and Urdu and conclude that the NER computational models for Hindi cannot be applied to Urdu. A rule-based Urdu NER algorithm is presented that outperforms the models that use statistical learning. 1. Introduction Text processing applications, such as machine translation, information extraction, information retrieval or natural language understanding systems need to recognize multiple word expressions that refer to people names, organizational names, geographical locations, and other named entities. Proper Names play a crucial role in information management, both in specific applications and in underlying technologies that drive the application. Name Recognition becomes important in situations when the person or the organization is more important than the action it performed, for example, bankruptcy of the corner shop John & Sons is not as interesting as the bankruptcy of General Motors, an American car manufacturer. In this particular example, latter event will be of much interest for the financial markets and investors to track. 
Human annotation for Co-reference Resolution (CRR) is labor intensive and costly, and only a handful of annotated corpora are currently available. However, corpora with Named Entity (NE) annotations are widely available. Also, unlike current CRR systems, state-of-the-art NER systems have very high accuracy and can generate NE labels that are very close to the gold standard for unlabeled corpora. We propose a new set of metrics collectively called CONE for Named Entity Coreference Resolution (NE-CRR) that use a subset of gold standard annotations, with the advantage that this subset can be easily approximated using NE labels when gold standard CRR annotations are absent. We define CONE B3 and CONE CEAF metrics based on the traditional B3 and CEAF metrics and show that CONE B3 and CONE CEAF scores of any CRR system on any dataset are highly correlated with its B3 and CEAF scores respectively. We obtain correlation factors greater than 0.6 for all CRR systems across all datasets, and a best-case correlation factor of 0.8. We also present a baseline method to estimate the gold standard required by CONE metrics, and show that CONE B3 and CONE CEAF scores using this estimated gold standard are also correlated with B3 and CEAF scores respectively. We thus demonstrate the suitability of CONE B3and CONE CEAF for automatic evaluation of NE-CRR. 
In this survey we overview graph-based clustering and its applications in computational linguistics. We summarize graph-based clustering as a five-part story: hypothesis, modeling, measure, algorithm and evaluation. We then survey three typical NLP problems in which graph-based clustering approaches have been successfully applied. Finally, we comment on the strengths and weaknesses of graph-based clustering and envision that graph-based clustering is a promising solution for some emerging NLP problems. 
The work described here aims to create a wordnet automatically from a semantic network based on terms. So, a clustering procedure is ran over a synonymy network, in order to obtain synsets. Then, the term arguments of each relational triple are assigned to the latter, originating a wordnet. Experiments towards our goal are reported and their results validated. 
 materials and methods employed. Section 4 presents the results and section 5 concludes.  This work extends the study of Germann et al. (2010) in investigating the lexical organization of verbs. Particularly, we look at the influence of frequency on the process of lexical acquis ition and use. We e xamine data obtained fro m psycholinguistic action naming tasks performed by children and adults (speakers of Brazilian Portuguese), and analyze some characteristics of the verbs used by each group in terms of similarity of content, using Jaccard‟s coefficient, and of topology, using graph theory. The experiments suggest that younger children tend to use more frequent verbs than adults to describe events in the world. 
Graph-based methods that are en vogue in the social network analysis area, such as centrality models, have been recently applied to linguistic knowledge bases, including unsupervised Word Sense Disambiguation. Although the achievable accuracy is rather high, the main drawback of these methods is the high computational demanding whenever applied to the large scale sense repositories. In this paper an adaptation of the PageRank algorithm recently proposed for Word Sense Disambiguation is presented that preserves the reachable accuracy while signiﬁcantly reducing the requested processing time. Experimental analysis over well-known benchmarks will be presented in the paper and the results conﬁrm our hypothesis. 
In this study we apply hierarchical spectral partitioning of bipartite graphs to a Dutch dialect dataset to cluster dialect varieties and determine the concomitant sound correspondences. An important advantage of this clustering method over other dialectometric methods is that the linguistic basis is simultaneously determined, bridging the gap between traditional and quantitative dialectology. Besides showing that the results of the hierarchical clustering improve over the ﬂat spectral clustering method used in an earlier study (Wieling and Nerbonne, 2009), the values of the second singular vector used to generate the two-way clustering can be used to identify the most important sound correspondences for each cluster. This is an important advantage of the hierarchical method as it obviates the need for external methods to determine the most important sound correspondences for a geographical cluster. 
Linguists use phylogenetic methods to build evolutionary trees of languages given lexical, phonological, and morphological data. Perfect phylogeny is too restrictive to explain most data sets. Conservative Dollo phylogeny is more permissive, and has been used in biological applications. We propose the use of conservative Dollo phylogeny as an alternative or complementary approach for linguistic phylogenetics. We test this approach on an Indo-European dataset. 
The talk will commence by discussing some of the problems that arise when machine learning is applied to graph structures. A taxonomy of different methods organised around a) clustering b) characterisation and c) constructing generative models in the graph domain will be introduced. With this taxonomy in hand, Dr. Hancock will then describe a number of graph-spectral algorithms that can be applied to solve the many different problems inherent to graphs, drawing examples from computer vision research. 
As an initial effort to identify universal and language-speciﬁc factors that inﬂuence the behavior of distributional models, we have formulated a distributionally determined word similarity network model, implemented it for eleven different languages, and compared the resulting networks. In the model, vertices constitute words and two words are linked if they occur in similar contexts. The model is found to capture clear isomorphisms across languages in terms of syntactic and semantic classes, as well as functional categories of abstract discourse markers. Language speciﬁc morphology is found to be a dominating factor for the accuracy of the model. 
This paper examines the inﬂuence of features based on clusters of co-occurrences for supervised Word Sense Disambiguation and Lexical Substitution. Cooccurrence cluster features are derived from clustering the local neighborhood of a target word in a co-occurrence graph based on a corpus in a completely unsupervised fashion. Clusters can be assigned in context and are used as features in a supervised WSD system. Experiments ﬁtting a strong baseline system with these additional features are conducted on two datasets, showing improvements. Cooccurrence features are a simple way to mimic Topic Signatures (Mart´ınez et al., 2008) without needing to construct resources manually. Further, a system is described that produces lexical substitutions in context with very high precision. 
We present a representation of documents as directed, weighted graphs, modeling the range of influence of terms within the document as well as contextually determined semantic relatedness among terms. We then show the usefulness of this kind of representation in topic segmentation. Our boundary detection algorithm uses this graph to determine topical coherence and potential topic shifts, and does not require labeled data or training of parameters. We show that this method yields improved results on both concatenated pseudo-documents and on closed-captions for television programs. 
MuLLinG is a model for knowledge extraction (especially lexical extraction from corpora), based on multilevel graphs. Its aim is to allow large-scale data acquisition, by making it easy to realize automatically, and simple to configure by linguists with limited knowledge in computer programming. In MuLLinG, each new level represents the information in a different manner (more and more abstract). We also introduce several associated operators, written to be as generic as possible. They are independent of what nodes and edges represent, and of the task to achieve. Consequently, they allow the description of a complex extraction process as a succession of simple graph manipulations. Finally, we present an experiment of collocation extraction using MuLLinG model. 
Recently, with the huge amount of growing information in the web and the little available time to read and process all this information, automatic summaries have become very important resources. In this work, we evaluate deep content selection methods for multidocument summarization based on the CST model (Cross-document Structure Theory). Our methods consider summarization preferences and focus on the overall main problems of multidocument treatment: redundancy, complementarity, and contradiction among different information sources. We also evaluate the impact of the CST model over superficial summarization systems. Our results show that the use of CST model helps to improve informativeness and quality in automatic summaries. 
Topological and dynamic features of complex networks have proven to be suitable for capturing text characteristics in recent years, with various applications in natural language processing. In this article we show that texts with positive and negative opinions can be distinguished from each other when represented as complex networks. The distinction was possible by obtaining several metrics of the networks, including the in-degree, out-degree, shortest paths, clustering coefﬁcient, betweenness and global efﬁciency. For visualization, the obtained multidimensional dataset was projected into a 2-dimensional space with the canonical variable analysis. The distinction was quantiﬁed using machine learning algorithms, which allowed an recall of 70% in the automatic discrimination for the negative opinions, even without attempts to optimize the pattern recognition process. 
We present a brief overview of the way in which image analysis, coupled with associated collateral text, is being used for auto-annotation and sentiment analysis. In particular, we describe our approach to auto-annotation using the graphtheoretic dominant set clustering algorithm and the annotation of images with sentiment scores from SentiWordNet. Preliminary results are given for both, and our planned work aims to explore synergies between the two approaches. 
Understanding, as opposed to reading is vital for the extraction of opinions out of a text. This is especially true, as an author’s opinion is not always clearly marked. Finding the overall opinion in a text can be challenging to both human readers and computers alike. Media Content Analysis is a popular method of extracting information out of a text, by means of human coders. We describe the difﬁculties humans have and the process they use to extract opinions and offer a formalization that could help to automate opinion extraction within the Media Content Analysis framework. 
This paper focuses on redundancy, overlapping information in multi-documents, and presents a method for detecting salient, key sentences from documents that discuss the same event. To eliminate redundancy, we used spectral clustering and classiﬁed each sentence into groups, each of which consists of semantically related sentences. Then, we applied link analysis, the Markov Random Walk (MRW) Model to deciding the importance of a sentence within documents. The method was tested on the NTCIR evaluation data, and the result shows the effectiveness of the method. 
Using the technique of ”semantic mirroring” a graph is obtained that represents words and their translations from a parallel corpus or a bilingual lexicon. The connectedness of the graph holds information about the different meanings of words that occur in the translations. Spectral graph theory is used to partition the graph, which leads to a grouping of the words according to different senses. We also report results from an evaluation using a small sample of seed words from a lexicon of Swedish and English adjectives. 
I present LIBPHON, a nonparametric regression-based model of phonological acquisition that induces a generalised and productive pattern of vowel harmony—including opaque and transparent neutrality—on the basis of simpliﬁed formant data. The model quickly learns to generate harmonically correct morphologically complex forms to which it has not been exposed. 
This paper applies finite state technologies to verify the typological validity of Turbid Spreading, a theory of vowel harmony in Optimality Theory (OT) (Prince & Smolensky, 1993/2004). Previous analyses of vowel harmony in OT have been prone to typological inconsistencies, predicting grammars that do not occur in natural language (Wilson, 2003). However, attempts to eliminate typological pathologies relying on hand-made inputs and candidate sets have been shown to be highly prone to error (Wilson, 2005). Using a modified version of the Contenders Algorithm (Riggle, 2004b), we verify that Turbid Spreading makes typologically valid predictions about the types of harmony processes that may appear in natural language. This modification of the Contenders Algorithm to include complex spreading interactions and intermediate representations demonstrates the utility of computational methods for verifying the typological predictions of complex phonological theories. 
The problem of the acquisition of Phonotactics in OT is shown to be not tractable in its strong formulation, whereby constraints and generating function vary arbitrarily as inputs of the problem. Tesar and Smolensky (1998) consider the basic ranking problem in Optimality Theory (OT). According to this problem, the learner needs to ﬁnd a ranking consistent with a given set of data. They show that this problem is solvable even in its strong formulation, namely without any assumptions on the generating function or the constraint set. Yet, this basic ranking problem is too simple to realistically model any actual aspect of language acquisition. To make the problem more realistic, we might want, for instance, to require the learner to ﬁnd not just any ranking consistent with the data, rather one that furthermore generates a smallest language (w.r.t. set inclusion). Prince and Tesar (2004) and Hayes (2004) note that this computational problem models the task of the acquisition of phonotactics within OT. This paper shows that, contrary to the basic ranking problem considered by Tesar and Smolensky, this more realistic problem of the acquisition of phonotactics is not solvable, at least not in its strong formulation. I conjecture that this complexity result has nothing to do with the choice of the OT framework, namely that an analogous result holds for the corresponding problem within alternative frameworks, such as Harmonic Grammar (Legendre et al., 1990b; Legendre et al., 1990a). Furthermore, I conjecture that the culprit lies with the fact that generating function and constraint set are completely unconstrained. From this perspective, this paper motivates the following research question: to ﬁnd phonologically plausible assumptions on generating function and constraint set that make the problem of the acquisition of phonotactics tractable.  
Motivated by recent work in phonotactic learning (Hayes and Wilson 2008, Albright 2009), this paper shows how to deﬁne feature-based probability distributions whose parameters can be provably efﬁciently estimated. The main idea is that these distributions are deﬁned as a product of simpler distributions (cf. Ghahramani and Jordan 1997). One advantage of this framework is it draws attention to what is minimally necessary to describe and learn phonological feature interactions in phonotactic patterns. The “bottom-up” approach adopted here is contrasted with the “top-down” approach in Hayes and Wilson (2008), and it is argued that the bottom-up approach is more analytically transparent. 
A novel method is presented for compiling two-level rules which have multiple context parts. The same method can also be applied to the resolution of so-called right-arrow rule conflicts. The method makes use of the fact that one can efficiently compose sets of twolevel rules with a lexicon transducer. By introducing variant characters and using simple pre-processing of multi-context rules, all rules can be reduced into single-context rules. After the modified rules have been combined with the lexicon transducer, the variant characters may be reverted back to the original surface characters. The proposed method appears to be efficient but only partial evidence is presented yet. 
In this paper we apply the multi-way decomposition method PARAFAC in order to detect the most prominent sound changes in dialect variation. We investigate various phonetic patterns, both in stressed and unstressed syllables. We proceed from regular sound correspondences which are automatically extracted from the aligned transcriptions and analyzed using PARAFAC. This enables us to analyze simultaneously the co-occurrence patterns of all sound correspondences found in the data set and determine the most important factors of the variation. The ﬁrst ten dimensions are examined in more detail by recovering the geographical distribution of the extracted correspondences. We also compare dialect divisions based on the extracted correspondences to the divisions based on the whole data set and to the traditional scholarship as well. The results show that PARAFAC can be successfully used to detect the linguistic basis of the automatically obtained dialect divisions. 
This paper develops computational tools for evaluating competing syllabic parses of a phonological string on the basis of temporal patterns in speech production data. This is done by constructing models linking syllable parses to patterns of coordination between articulatory events. Data simulated from different syllabic parses are evaluated against experimental data from American English and Moroccan Arabic, two languages claimed to parse similar strings of segments into different syllabic structures. Results implicate a tautosyllabic parse of initial consonant clusters in English and a heterosyllabic parse of initial clusters in Arabic, in accordance with theoretical work on the syllable structure of these languages. It is further demonstrated that the model can correctly diagnose syllable structure even when previously proposed phonetic heuristics for such structure do not clearly point to the correct diagnosis.  data are available, languages that allow complex onsets exhibit patterns of temporal stability that differ from languages that allow only syllables with simplex syllable onsets. These observed temporal differences have been quantified in terms of the relative stability of intervals as calculated across words beginning with one, two and three initial consonants (Browman & Goldstein, 1988; Byrd, 1995; Honorof & Browman, 1995; Shaw, Gafos, Hoole, & Zeroual, 2009). Figure 1 schematizes temporal differences between simplex and complex onsets. The figure shows three temporal intervals left-delimited by landmarks in the consonant cluster, the left edge of the cluster, the center of the cluster and the right edge of the cluster, and right-delimited by a common anchor point.  
Unsupervised algorithms for the induction of linguistic knowledge should at best require as few basic assumptions as possible and at the same time in principle yield good results for any language. However, most of the time such algorithms are only tested on a few (closely related) languages. In this paper, an approach is presented that takes into account typological knowledge in order to induce syllabic divisions in a fully automatic manner based on reasonably-sized written texts. Our approach is able to account for syllable structures of languages where other approaches would fail, thereby raising the question whether computational methods can really be claimed to be language-universal when they are not tested on the variety of structures that are found in the languages of the world. 
Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difﬁculties for any system requiring reference to a static lexicon accessed by orthographic form. In this paper, we present three methods for associating unknown historical word forms with synchronically active canonical cognates and evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse. 
We consider morphology learning in a semi-supervised setting, where a small set of linguistic gold standard analyses is available. We extend Morfessor Baseline, which is a method for unsupervised morphological segmentation, to this task. We show that known linguistic segmentations can be exploited by adding them into the data likelihood function and optimizing separate weights for unlabeled and labeled data. Experiments on English and Finnish are presented with varying amount of labeled data. Results of the linguistic evaluation of Morpho Challenge improve rapidly already with small amounts of labeled data, surpassing the state-ofthe-art unsupervised methods at 1000 labeled words for English and at 100 labeled words for Finnish. 
Morpho Challenge is an annual evaluation campaign for unsupervised morpheme analysis. In morpheme analysis, words are segmented into smaller meaningful units. This is an essential part in processing complex word forms in many large-scale natural language processing applications, such as speech recognition, information retrieval, and machine translation. The discovery of morphemes is particularly important for morphologically rich languages where inﬂection, derivation and composition can produce a huge amount of different word forms. Morpho Challenge aims at language-independent unsupervised learning algorithms that can discover useful morpheme-like units from raw text material. In this paper we deﬁne the challenge, review proposed algorithms, evaluations and results so far, and point out the questions that are still open. 
This paper describes work being done on the modeling and encoding of a legacy resource, the traditional descriptive wordlist, in ways that make its data accessible to NLP applications. We describe an abstract model for traditional wordlist entries and then provide an instantiation of the model in RDF/XML which makes clear the relationship between our wordlist database and interlingua approaches aimed towards machine translation, and which also allows for straightforward interoperation with data from full lexicons. 
Evidentiality is the linguistic representation of the nature of evidence for a statement. In other words, it is the linguistically encoded evidence for the trustworthiness of a statement. In this paper, we aim to explore how linguistically encoded information of evidentiality can contribute to the prediction of trustworthiness in natural language processing (NLP). We propose to incorporate evidentiality into a framework of machine learning based text classification. We first construct a taxonomy of evidentials. Then experiments involving collaborative question answering (CQA) are designed and implemented using this taxonomy. The experimental results confirm that evidentiality is an important clue for text trustworthiness detection. With the binarized vector setting, evidential based text representation model has considerably performaned better than both the bag-of-word model and the content word based model. Most crucially, we show that the best trustworthiness detection result is achieved when evidentiality is incorporated in a linguistically sophisticated model where their meanings are interpreted in both semantic and pragmatic terms. 
This paper summarizes some of the applications of NLP techniques in various linguistic sub-ﬁelds, and presents a few examples that call for a deeper engagement between the two ﬁelds. 
While some linguistic questions pose challenges that could be met by developing and applying NLP techniques, other problems can best be approached with a blend of old-fashioned linguistic investigation and the use of simple, well-established NLP tools. Unfortunately, this means that the NLP component is too simple to be of interest to the computationally-minded, while existing tools are often difﬁcult for the programming novice to use. For NLP to come to the aid of research in theoretical linguistics, a continuing investment of effort is required to bridge the gap. This investment can be made from both sides. 
In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portability to new domains: their performance might decrease substantially as the distance between test and training domain increases. Yet, to which degree do they suffer from this problem, i.e. which kind of parsing system is more affected by domain shifts? Intuitively, grammar-driven systems should be less affected by domain changes. To investigate this hypothesis, an empirical investigation on Dutch is carried out. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques. 
We provide a detailed comparison of strategies for implementing medium-tolow frequency phenomena such as German adverbial participles in a broadcoverage, rule-based parsing system. We show that allowing for general adverb conversion of participles in the German LFG grammar seriously affects its overall performance, due to increased spurious ambiguity. As a solution, we present a corpus-based cross-lingual induction technique that detects adverbially used participles in parallel text. In a grammarbased evaluation, we show that the automatically induced resource appropriately restricts the adverb conversion to a limited class of participles, and improves parsing quantitatively as well as qualitatively. 
As a consequence of the established practice to prefer training data obtained from written sources, NLP tools encounter problems in handling data from the spoken domain. However, accurate models of spoken data are increasingly in demand for naturalistic speech generation and machine translations in speech-like contexts (such as chat windows and SMS). There is a widely held assumption in the linguistic ﬁeld that spoken language is an impoverished form of written language. However, we show that spoken data is not unpredictably irregular and that language models can beneﬁt from detailed consideration of spoken language features. This paper considers one speciﬁc construction which is largely restricted to the spoken domain - the ZERO AUXILIARY and makes a predictive model of that construction for native speakers of British English. The model can predict zero auxiliary occurrence in the BNC with 96.9% accuracy. We will demonstrate how this model can be integrated into existing parsing tools, increasing the number of successful parses for this zero auxiliary construction by around 30%, and thus improving the performance of NLP applications which rely on parsing. 
Cross-lingual parallelism and small-scale language variation have recently become subject of research in both computational and theoretical linguistics. In this article, we use a parallel corpus and an automatic aligner to study English light verb constructions and their German translations. We show that parallel corpus data can provide new empirical evidence for better understanding the properties of light verbs. We also study the inﬂuence that the identiﬁed properties of light verb constructions have on the quality of their automatic alignment in a parallel corpus. We show that, even though characterised by limited compositionality, these constructions can be aligned better than fully compositional phrases, due to an interaction between the type of light verb construction and its frequency. 
We consider sentences of the form No X is too Y to Z, in which X is a noun phrase, Y is an adjective phrase, and Z is a verb phrase. Such constructions are ambiguous, with two possible (and opposite!) interpretations, roughly meaning either that “Every X Zs”, or that “No X Zs”. The interpretations have been noted to depend on semantic and pragmatic factors. We show here that automatic disambiguation of this pragmatically complex construction can be largely achieved by using features of the lexical semantic properties of the verb (i.e., Z) participating in the construction. We discuss our experimental ﬁndings in the context of construction grammar, which suggests a possible account of this phenomenon. 
In this paper, we explore the phenomenon of Similar Place Avoidance (SPA), according to which successive consonants within stems sharing the same place of articulation are avoided. This principle has recently been hypothesized as a universal tendency although evidence from only a few languages scattered across the world has been considered. Using methods taken from the ﬁeld of Visual Analytics, which have demonstrably been shown to help with understanding complex interactions across large data sets, we investigated a large crosslinguistic lexical database (comprising data on more than 4,500 languages) and found that a universal tendency can indeed be maintained. 
When we grow tired of embarrassing ourselves, what should we do? Fortunately, injecting some linguistic (and other) sophistication into our work is not that complicated. The key is annotation: by using a theoretically informed set of choices rather than a bottom-up naive one, we can have annotators tag corpora with labels that reﬂect some underlying theories. While the large-C contingent of our community will not care, researchers interested in investigating language rather than processing will be able to ﬁnd new ways to connect with Corpus Linguists, Psycholinguists, and even Ontologists. 
In this paper we investigate a new source of information for syntactic category acquisition: sentence type (question, declarative, imperative). Sentence type correlates strongly with intonation patterns in most languages; we hypothesize that these intonation patterns are a valuable signal to a language learner, indicating different syntactic patterns. To test this hypothesis, we train a Bayesian Hidden Markov Model (and variants) on child-directed speech. We ﬁrst show that simply training a separate model for each sentence type decreases performance due to sparse data. As an alternative, we propose two new models based on the BHMM in which sentence type is an observed variable which inﬂuences either emission or transition probabilities. Both models outperform a standard BHMM on data from English, Cantonese, and Dutch. This suggests that sentence type information available from intonational cues may be helpful for syntactic acquisition crosslinguistically. 
Natural language as well as other communication forms are constrained by cognitive function and evolved through a social process. Here, we examine whether human memory may be uniquely adapted to the social structures prevalent in groups, speciﬁcally small-world networks. The emergence of domain languages is simulated using an empirically evaluated ACTR-based cognitive model of agents in a naming game played within communities. Several community structures are examined (grids, trees, random graphs and small-world networks). We present preliminary results from small-scale simulations, showing relative robustness of cognitive models to network structure. 
In this paper we investigate the manner in which the human language comprehension system adapts to shifts in probability distributions over syntactic structures, given experimentally controlled experience with those structures. We replicate a classic reading experiment, and present a model of the behavioral data that implements a form of Bayesian belief update over the course of the experiment. 
Hierarchical Hidden Markov Model (HHMM) parsers have been proposed as psycholinguistic models due to their broad coverage within human-like working memory limits (Schuler et al., 2008) and ability to model human reading time behavior according to various complexity metrics (Wu et al., 2010). But HHMMs have been evaluated previously only with very wide beams of several thousand parallel hypotheses, weakening claims to the model’s efﬁciency and psychological relevance. This paper examines the effects of varying beam width on parsing accuracy and speed in this model, showing that parsing accuracy degrades gracefully as beam width decreases dramatically (to 2% of the width used to achieve previous top results), without sacriﬁcing gains over a baseline CKY parser. 
This paper examines how grammatical and memory constraints explain gradience in superiority violation acceptability. A computational model encoding both categories of constraints is compared to experimental evidence. By formalizing memory capacity as beam-search in the parser, the model predicts gradience evident in human data. To predict attachment behavior, the parser must be sensitive to the types of nominal intervenors that occur between a wh-ﬁller and its head. The results suggest memory is more informative for modeling violation gradience patterns than grammatical constraints. 
We formally derive a mathematical model for evaluating the effect of context relevance in language production. The model is based on the principle that distant contextual cues tend to gradually lose their relevance for predicting upcoming linguistic signals. We evaluate our model against a hypothesis of efﬁcient communication (Genzel and Charniak’s Constant Entropy Rate hypothesis). We show that the development of entropy throughout discourses is described signiﬁcantly better by a model with cue relevance decay than by previous models that do not consider context effects. 
When subjects describe concepts in terms of their characteristic properties, they often produce composite properties, e. g., rabbits are said to have long ears, not just ears. We present a set of simple methods to extract the modiﬁers of composite properties (in particular: parts) from corpora. We achieve our best performance by combining evidence about the association between the modiﬁer and the part both within the context of the target concept and independently of it. We show that this performance is relatively stable across languages (Italian and German) and for production vs. perception of properties. 
This paper presents a data-driven model of eye movement control in reading that builds on earlier work using machine learning methods to model saccade behavior. We extend previous work by modeling the time course of eye movements, in addition to where the eyes move. In this model, the initiation of eye movements is delayed as a function of on-line processing difﬁculty, and the decision of where to move the eyes is guided by past reading experience, approximated using machine learning methods. In benchmarking the model against held-out previously unseen data, we show that it can predict gaze durations and skipping probabilities with good accuracy. 
This paper investigates whether surprisal theory can account for differential processing difﬁculty in the NP-/S-coordination ambiguity in Dutch. Surprisal is estimated using a Probabilistic Context-Free Grammar (PCFG), which is induced from an automatically annotated corpus. We ﬁnd that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by Frazier (1987). We argue that syntactic and lexical probabilities, as speciﬁed in a PCFG, are sufﬁcient to account for what is commonly referred to as an NP-coordination preference. 
The amount of cognitive effort required to process a word has been argued to depend on the word’s effect on the uncertainty about the incoming sentence, as quantiﬁed by the entropy over sentence probabilities. The current paper tests this hypothesis more thoroughly than has been done before by using recurrent neural networks for entropy-reduction estimation. A comparison between these estimates and wordreading times shows that entropy reduction is positively related to processing effort, conﬁrming the entropy-reduction hypothesis. This effect is independent from the effect of surprisal. 
Linear-chain Conditional Random Fields (CRF) has been applied to perform the Named Entity Recognition (NER) task in many biomedical text mining and information extraction systems. However, the linear-chain CRF cannot capture long distance dependency, which is very common in the biomedical literature. In this paper, we propose a novel study of capturing such long distance dependency by deﬁning two principles of constructing skipedges for a skip-chain CRF: linking similar words and linking words having typed dependencies. The approach is applied to recognize gene/protein mentions in the literature. When tested on the BioCreAtIvE II Gene Mention dataset and GENIA corpus, the approach contributes signiﬁcant improvements over the linear-chain CRF. We also present in-depth error analysis on inconsistent labeling and study the inﬂuence of the quality of skip edges on the labeling performance. 
We present the ﬁrst full-scale event extraction experiment covering the titles and abstracts of all PubMed citations. Extraction is performed using a pipeline composed of state-of-the-art methods: the BANNER named entity recognizer, the McCloskyCharniak domain-adapted parser, and the Turku Event Extraction System. We analyze the statistical properties of the resulting dataset and present evaluations of the core event extraction as well as negation and speculation detection components of the system. Further, we study in detail the set of extracted events relevant to the apoptosis pathway to gain insight into the biological relevance of the result. The dataset, consisting of 19.2 million occurrences of 4.5 million unique events, is freely available for use in research at http://bionlp.utu.fi/. 
Based on linguistic generalizations, we enhanced an existing semantic processor, SemRep, for effective interpretation of a wide range of patterns used to express arguments of nominalization in clinically oriented biomedical text. Nominalizations are pervasive in the scientific literature, yet few text mining systems adequately address them, thus missing a wealth of information. We evaluated the system by assessing the algorithm independently and by determining its contribution to SemRep generally. The first evaluation demonstrated the strength of the method through an F-score of 0.646 (P=0.743, R=0.569), which is more than 20 points higher than the baseline. The second evaluation showed that overall SemRep results were increased to F-score 0.689 (P=0.745, R=0.640), approximately 25 points better than processing without nominalizations. 
We describe a concept-based summarization system for biomedical documents and show that its performance can be improved using Word Sense Disambiguation. The system represents the documents as graphs formed from concepts and relations from the UMLS. A degree-based clustering algorithm is applied to these graphs to discover different themes or topics within the document. To create the graphs, the MetaMap program is used to map the text onto concepts in the UMLS Metathesaurus. This paper shows that applying a graph-based Word Sense Disambiguation algorithm to the output of MetaMap improves the quality of the summaries that are generated. 
Forums and mailing lists dedicated to particular diseases are increasingly popular online. Automatically inferring the health status of a patient can be useful for both forum users and health researchers who study patients’ online behaviors. In this paper, we focus on breast cancer forums and present a method to predict the stage of patients’ cancers from their online discourse. We show that what the patients talk about (content-based features) and whom they interact with (social networkbased features) provide complementary cues to predicting cancer stage and can be leveraged for better prediction. Our methods are extendable and can be applied to other tasks of acquiring contextual information about online health forum participants. 
Here we explore mining data on gene expression from the biomedical literature and present Gene Expression Text Miner (GETM), a tool for extraction of information about the expression of genes and their anatomical locations from text. Provided with recognized gene mentions, GETM identifies mentions of anatomical locations and cell lines, and extracts text passages where authors discuss the expression of a particular gene in specific anatomical locations or cell lines. This enables the automatic construction of expression profiles for both genes and anatomical locations. Evaluated against a manually extended version of the BioNLP '09 corpus, GETM achieved precision and recall levels of 58.8% and 23.8%, respectively. Application of GETM to MEDLINE and PubMed Central yielded over 700,000 gene expression mentions. This data set may be queried through a web interface, and should prove useful not only for researchers who are interested in the developmental regulation of specific genes of interest, but also for database curators aiming to create structured repositories of gene expression information. The compiled tool, its source code, the manually annotated evaluation corpus and a search query interface to the data set extracted from MEDLINE and PubMed Central is available at http://getmproject.sourceforge.net/. 
We investigate the automatic identiﬁcation of negated and speculative statements in biomedical texts, focusing on the clinical domain. Our goal is to evaluate the performance of simple, Regex-based algorithms that have the advantage of low computational cost, simple implementation, and do not rely on the accurate computation of deep linguistic features of idiosyncratic clinical texts. The performance of the NegEx algorithm with an additional set of Regex-based rules reveals promising results (evaluated on the BioScope corpus). Current and future work focuses on a bootstrapping algorithm for the discovery of new rules from unannotated clinical texts. 
Despite an increasing amount of research on biomedical named entity recognition, there has been not enough work done on disease mention recognition. Difﬁculty of obtaining adequate corpora is one of the key reasons which hindered this particular research. Previous studies argue that correct identiﬁcation of disease mentions is the key issue for further improvement of the disease-centric knowledge extraction tasks. In this paper, we present a machine learning based approach that uses a feature set tailored for disease mention recognition and outperforms the state-ofthe-art results. The paper also discusses why a feature set for the well studied gene/protein mention recognition task is not necessarily equally effective for other biomedical semantic types such as diseases. 
This paper describes our study on identifying semantic relations that exist between diseases and treatments in biomedical sentences. We focus on three semantic relations: Cure, Prevent, and Side Effect. The contributions of this paper consists in the fact that better results are obtained compared to previous studies and the fact that our research settings allow the integration of biomedical and medical knowledge. We obtain 98.55% F-measure for the Cure relation, 100% F-measure for the Prevent relation, and 88.89% F-measure for the Side Effect relation. 
Many practical tasks require accessing speciﬁc types of information in scientiﬁc literature; e.g. information about the objective, methods, results or conclusions of the study in question. Several schemes have been developed to characterize such information in full journal papers. Yet many tasks focus on abstracts instead. We take three schemes of different type and granularity (those based on section names, argumentative zones and conceptual structure of documents) and investigate their applicability to biomedical abstracts. We show that even for the ﬁnest-grained of these schemes, the majority of categories appear in abstracts and can be identiﬁed relatively reliably using machine learning. We discuss the impact of our results and the need for subsequent task-based evaluation of the schemes. 
Adverse reactions to drugs are among the most common causes of death in industrialized nations. Expensive clinical trials are not sufﬁcient to uncover all of the adverse reactions a drug may cause, necessitating systems for post-marketing surveillance, or pharmacovigilance. These systems have typically relied on voluntary reporting by health care professionals. However, self-reported patient data has become an increasingly important resource, with efforts such as MedWatch from the FDA allowing reports directly from the consumer. In this paper, we propose mining the relationships between drugs and adverse reactions as reported by the patients themselves in user comments to health-related websites. We evaluate our system on a manually annotated set of user comments, with promising performance. We also report encouraging correlations between the frequency of adverse drug reactions found by our system in unlabeled data and the frequency of documented adverse drug reactions. We conclude that user comments pose a signiﬁcant natural language processing challenge, but do contain useful extractable information which merits further exploration. 
This abstract describes work in progress on semantic role labeling of gene regulation events. We present preliminary results of a supervised semantic role labeler that has been trained and tested on the GREC corpus. 
Early recognition of distinguishing patterns of a novel pandemic disease is important. We introduce a methodological approach based on popular data mining techniques to extract key features and temporal patterns of swine (h1n1) flu that is discriminated from swine flu like symptoms. 
Event extraction approaches based on expressive structured representations of extracted information have been a signiﬁcant focus of research in recent biomedical natural language processing studies. However, event extraction efforts have so far been limited to publication abstracts, with most studies further considering only the speciﬁc transcription factor-related subdomain of molecular biology of the GENIA corpus. To establish the broader relevance of the event extraction approach and proposed methods, it is necessary to expand on these constraints. In this study, we propose an adaptation of the event extraction approach to a subdomain related to infectious diseases and present analysis and initial experiments on the feasibility of event extraction from domain full text publications. 
We present a preliminary attempt to apply the TARSQI Toolkit to the medical domain, speciﬁcally electronic health records, for use in answering temporally motivated questions. 
As research on biomedical text mining is shifting focus from simple binary relations to more expressive event representations, extraction performance drops due to the increase in complexity. Recently introduced data sets speciﬁcally targeting static relations between named entities and domain terms have been suggested to enable a better representation of the biological processes underlying annotated events and opportunities for addressing their complexity. In this paper, we present the ﬁrst study of integrating these static relations with event data with the aim of enhancing event extraction performance. While obtaining promising results, we will argue that an event extraction framework will beneﬁt most from this new data when taking intrinsic differences between various event types into account. 
The paper describes a learner corpus of Czech, currently under development. The corpus captures Czech as used by nonnative speakers. We discuss its structure, the layered annotation of errors and the annotation process. 
We are interested in extracting social networks from text. We present a novel annotation scheme for a new type of event, called social event, in which two people participate such that at least one of them is cognizant of the other. We compare our scheme in detail to the ACE scheme. We perform a detailed analysis of interannotator agreement, which shows that our annotations are reliable. 
 2 Background and Related Work  This paper describes work testing agile data annotation by moving away from the traditional, linear phases of corpus creation towards iterative ones and by recognizing the potential for sources of error occurring throughout the annotation process. 
This paper explores ways to detect errors in aligned corpora, using very little technology. In the ﬁrst method, applicable to any aligned corpus, we consider alignment as a string-to-string mapping. Treating the target string as a label, we examine each source string to ﬁnd inconsistencies in alignment. Despite setting up the problem on a par with grammatical annotation, we demonstrate crucial differences in sorting errors from legitimate variations. The second method examines phrase nodes which are predicted to be aligned, based on the alignment of their yields. Both methods are effective in complementary ways. 
Manual annotation of natural language to capture linguistic information is essential for NLP tasks involving supervised machine learning of semantic knowledge. Judgements of meaning can be more or less subjective, in which case instead of a single correct label, the labels assigned might vary among annotators based on the annotators’ knowledge, age, gender, intuitions, background, and so on. We introduce a framework ”Anveshan,” where we investigate annotator behavior to ﬁnd outliers, cluster annotators by behavior, and identify confusable labels. We also investigate the effectiveness of using trained annotators versus a larger number of untrained annotators on a word sense annotation task. The annotation data comes from a word sense disambiguation task for polysemous words, annotated by both trained annotators and untrained annotators from Amazon’s Mechanical turk. Our results show that Anveshan is effective in uncovering patterns in annotator behavior, and we also show that trained annotators are superior to a larger number of untrained annotators for this task. 
This article details a series of carefully designed experiments aiming at evaluating the inﬂuence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a speciﬁc attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments conﬁrm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They ﬁnally demonstrate that even a not so accurate tagger can help improving annotation speed. 
The common accepted wisdom is that blind double annotation followed by adjudication of disagreements is necessary to create training and test corpora that result in the best possible performance. We provide evidence that this is unlikely to be the case. Rather, the greatest value for your annotation dollar lies in single annotating more data. 
Many noun phrases in text are ambiguously quantiﬁed: syntax doesn’t explicitly tell us whether they refer to a single entity or to several, and what portion of the set denoted by the Nbar actually takes part in the event expressed by the verb. We describe this ambiguity phenomenon in terms of underspeciﬁcation, or rather underquantiﬁcation. We attempt to validate the underquantiﬁcation hypothesis by producing and testing an annotation scheme for quantiﬁcation resolution, the aim of which is to associate a single quantiﬁer with each noun phrase in our corpus. 
In this paper, we have addressed the task of PropBank annotation of light verb constructions, which like multi-word expressions pose special problems. To arrive at a solution, we have evaluated 3 different possible methods of annotation. The final method involves three passes: (1) manual identification of a light verb construction, (2) annotation based on the light verb construction‟s Frame File, and (3) a deterministic merging of the first two passes. We also discuss how in various languages the light verb constructions are identified and can be distinguished from the non-light verb word groupings. 
This paper describes the retrieval of correct semantic boundaries for predicateargument structures annotated by dependency structure. Unlike phrase structure, in which arguments are annotated at the phrase level, dependency structure does not have phrases so the argument labels are associated with head words instead: the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. However, at least in English, retrieving such subtrees does not always guarantee retrieval of the correct phrase boundaries. In this paper, we present heuristics that retrieve correct phrase boundaries for semantic arguments, called semantic boundaries, from dependency trees. By applying heuristics, we achieved an F1-score of 99.54% for correct representation of semantic boundaries. Furthermore, error analysis showed that some of the errors could also be considered correct, depending on the interpretation of the annotation. 
We present an annotation scheme for the annotation of complex predicates, understood as constructions with more than one lexical unit, each contributing part of the information normally associated with a single predicate. We discuss our annotation guidelines of four types of complex predicates, and the treatment of several difﬁcult cases, related to ambiguity, overlap and coordination. We then discuss the process of marking up the Portuguese CINTIL corpus of 1M tokens (written and spoken) with a new layer of information regarding complex predicates. We also present the outcomes of the annotation work and statistics on the types of CPs that we found in the corpus. 
Language documentation is important as a tool for preservation of endangered languages and making data available to speakers and researchers of a language. A data base such as TypeCraft is important for typology studies both for well documented languages as well as little documented languages and is a valid tool for comparison of languages. This requires that linguistic elements must be coded in a manner that allows comparability across widely varying language data. In this paper, I discuss how I have used the coding system in TypeCraft for the documentation of data from Èdó language, a language belonging to the Edoid group of the Benue-Congo subfamily of the Volta-Congo language family and spoken in Mid-Western Nigeria, West Africa. The study shows how syntactic, semantic and morphological properties of multi-verb constructions in Èdó (Benue-Congo) can be represented in a relational database. 1. Introduction In this paper1, I show some ways in which I am using a shared methodology in my research on multi-verb constructions. My research is centered around the language Èdó, spoken in Mid-Western Nigeria, Ga and Akan (kwa), and the tool is the system TypeCraft, which has been developed in the ISK department, NTNU and first documented in Beermann and Prange (2006). Èdó language belongs to the NigerCongo, Atlantic-Congo, Volta-Congo, BenueCongo-Edoid language family. The Ediod 
Methods that re-use existing mono-lingual semantic annotation resources to annotate a new language rely on the hypothesis that the semantic annotation scheme used is cross-lingually valid. We test this hypothesis in an annotation agreement study. We show that the annotation scheme can be applied cross-lingually. 
The purpose of this paper is to present an unusual English dataset for affect exploration in text. It describes a corpus of fairy tales from three sources that have been annotated for affect at the sentence level. Special attention is given to data marked by high annotator agreement. A qualitative analysis of characteristics of high agreement sentences from H. C. Andersen reveals several interesting trends, illustrated by examples. 
In this paper, we introduce our recent work on re-annotating the deep information, which includes both the grammatical functional tags and the traces, in a Chinese scientific treebank. The issues with regard to re-annotation and its corresponding solutions are discussed. Furthermore, the process of the re-annotation work is described. 
We propose a unified model of syntax and discourse in which text structure is viewed as a tree structure augmented with anaphoric relations and other secondary relations. We describe how the model accounts for discourse connectives and the syntax-discourse-semantics interface. Our model is dependency-based, ie, words are the basic building blocks in our analyses. The analyses have been applied cross-linguistically in the Copenhagen Dependency Treebanks, a set of parallel treebanks for Danish, English, German, Italian, and Spanish which are currently being annotated with respect to discourse, anaphora, syntax, morphology, and translational equivalence. 
This paper reports on a pilot study where two Models of argument were applied to the Discussion sections of a corpus of biomedical research articles. The goal was to identify sources of systematic inter-annotator variation as diagnostics for improving the Models. In addition to showing a need to revise both Models, the results identified problems resulting from limitations in annotator expertise. In future work two types of annotators are required: those with biomedical domain expertise and those with an understanding of rhetorical structure. 
In this paper, we present a PropBank of clinical Finnish, an annotated corpus of verbal propositions and arguments. The clinical PropBank is created on top of a previously existing dependency treebank annotated in the Stanford Dependency (SD) scheme and covers 90% of all verb occurrences in the treebank. We establish that the PropBank scheme is applicable to clinical Finnish as well as compatible with the SD scheme, with an overwhelming proportion of arguments being governed by the verb. This allows argument candidates to be restricted to direct verb dependents, substantially simplifying the PropBank construction. The clinical Finnish PropBank is freely available at the address http://bionlp.utu.fi. 
In this paper, we introduce the NotaBene RDF Annotation Tool free software used to build the Syntactic Reference Corpus of Medieval French. It relies on a dependency-based model to manually annotate Old French texts from the Base de Français Médiéval and the Nouveau Corpus d’Amsterdam. NotaBene uses OWL ontologies to frame the terminology used in the annotation, which is displayed in a tree-like view of the annotation. This tree widget allows easy grouping and tagging of words and structures. To increase the quality of the annotation, two annotators work independently on the same texts at the same time and NotaBene can also generate automatic comparisons between both analyses. The RDF format can be used to export the data to several other formats: namely, TigerXML (for querying the data and extracting structures) and graphviz dot format (for quoting syntactic description in research papers). First, we will present the Syntactic Reference Corpus of Medieval French project (SRCMF) (1). Then, we will show how the NotaBene RDF Annotation Tool software is used within the project (2). In our conclusion, we will stress further developments of the tool (3). 
This paper describes a CoNLL-style chunk representation for the Tu¨bingen Treebank of Written German, which assumes a ﬂat chunk structure so that each word belongs to at most one chunk. For German, such a chunk deﬁnition causes problems in cases of complex prenominal modiﬁcation. We introduce a ﬂat annotation that can handle these structures via a stranded noun chunk. 
We present a proposal for the annotation of multi-word expressions in a 1M corpus of contemporary portuguese. Our aim is to create a resource that allows us to study multi-word expressions (MWEs) in their context. The corpus will be a valuable additional resource next to the already existing MWE lexicon that was based on a much larger corpus of 50M words. In this paper we discuss the problematic cases for annotation and proposed solutions, focusing on the variational properties of MWEs. 
We propose a feature type classification thought to be used in a therapeutic context. Such a scenario lays behind our need for a easily usable and cognitively plausible classification. Nevertheless, our proposal has both a practical and a theoretical outcome, and its applications range from computational linguistics to psycholinguistics. An evaluation through inter-coder agreement has been performed to highlight the strength of our proposal and to conceive some improvements for the future. 
This paper presents preliminary work on a corpus-based study of Korean demonstratives. Through the development of an annotation scheme and the use of spoken and written corpora, we aim to determine different functions of demonstratives and to examine their distributional properties. Our corpus study adopts similar features of annotation used in Botley and McEnery (2001) and provides some linguistic hypotheses on grammatical functions of Korean demonstratives to be further explored. 
This paper describes the creation of a resource of German sentences with multiple automatically created alternative syntactic analyses (parses) for the same text, and how qualitative and quantitative investigations of this resource can be performed using ANNIS, a tool for corpus querying and visualization. Using the example of PP attachment, we show how parsing can beneﬁt from the use of such a resource. 
The paper presents an architecture for connecting annotated linguistic data with a computational grammar system. Pivotal to the architecture is an annotational interlingua – called the Construction Labeling system (CL) - which is notationally very simple, descriptively finegrained, cross-typologically applicable, and formally well-defined enough to map to a state-of-the-art computational model of grammar. In the present instantiation of the architecture, the computational grammar is an HPSG-based system called TypeGram. Underlying the architecture is a research program of enhancing the interconnectivity between linguistic analytic subsystems such as grammar formalisms and text annotation systems.  keeping other parts constant. At the present point, however, this is a demonstration tied to unique choices for each module in the architecture. It serves as a feasibility demonstration of the design as such, and equally much to motivate the specific annotation code presented, which is pivotal to the system as a whole. This paper has two parts. The first part presents the sentence-level annotation code. It consists of strings of labels (connected by hyphens) where each label represents a possible property of a sentential sign, such as, e.g., ‘has Argument structure X’, ‘has Aspect Y’, ‘has a Subject with properties Z’, ‘expresses situation type S’, etc. The construction type specification in (1) is a first illustration of the code:  
Prepositions are highly polysemous. Yet, little effort has been spent to develop languagespecific annotation schemata for preposition senses to systematically represent and analyze the polysemy of prepositions in large corpora. In this paper, we present an annotation schema for preposition senses in German. The annotation schema includes a hierarchical taxonomy and also allows multiple annotations for individual tokens. It is based on an analysis of usage-based dictionaries and grammars and has been evaluated in an inter-annotatoragreement study. 
This paper presents OTTO, a transcription tool designed for diplomatic transcription of historical language data. The tool supports easy and fast typing and instant rendering of transcription in order to gain a look as close to the original manuscript as possible. In addition, the tool provides support for the management of transcription projects which involve distributed, collaborative working of multiple parties on collections of documents. 
We propose in this paper a broad-coverage approach for multimodal annotation of conversational data. Large annotation projects addressing the question of multimodal annotation bring together many different kinds of information from different domains, with different levels of granularity. We present in this paper the ﬁrst results of the OTIM project aiming at developing conventions and tools for multimodal annotation. 
This paper describes a new kind of semantic annotation in parallel treebanks. We build French-German parallel treebanks of mountaineering reports, a text genre that abounds with geographical names which we classify and ground with reference to a large gazetteer of Swiss toponyms. We discuss the challenges in obtaining a high recall and precision in automatic grounding, and sketch how we represent the grounding information in our treebank. 
We describe the challenges of resource creation for a resource-light system for morphological tagging of fusional languages (Feldman and Hana, 2010). The constraints on resources (time, expertise, and money) introduce challenges that are not present in development of morphological tools and corpora in the usual, resource intensive way. 
In this paper, we describe an annotation environment developed for the marking of discourse structures in Turkish, and the kinds of discourse relation conﬁgurations that led to its design. 
We present our concept-annotation guidelines for an large multi-institutional effort to create a gold-standard manually annotated corpus of full-text biomedical journal articles. We are semantically annotating these documents with the full term sets of eight large biomedical ontologies and controlled terminologies ranging from approximately 1,000 to millions of terms, and, using these guidelines, we have been able to perform this extremely challenging task with a high degree of interannotator agreement. The guidelines have been designed to be able to be used with any terminology employed to semantically annotate concept mentions in text and are available for external use. 
In this paper, we argue for and demonstrate the use of Prolog as a tool to query annotated corpora. We present a case study based on the German TüBa-D/Z Treebank to show that ﬂexible and efﬁcient corpus querying can be started with a minimal amount of effort. We end this paper with a brief discussion of performance, that suggests that the approach is both fast enough and scalable. 
E-Dictor is a tool for encoding, applying levels of editions, and assigning part-ofspeech tags to ancient texts. In short, it works as a WYSIWYG interface to encode text in XML format. It comes from the experience during the building of the Tycho Brahe Parsed Corpus of Historical Portuguese and from consortium activities with other research groups. Preliminary results show a decrease of at least 50% on the overall time taken on the editing process. 
The revised Arabic PropBank (APB) reflects a number of changes to the data and the process of PropBanking. Several changes stem from Treebank revisions. An automatic process was put in place to map existing annotation to the new trees. We have revised the original 493 Frame Files from the Pilot APB and added 1462 new files for a total of 1955 Frame Files with 2446 framesets. In addition to a heightened attention to sense distinctions this cycle includes a greater attempt to address complicated predicates such as light verb constructions and multi-word expressions. New tools facilitate the data tagging and also simplify frame creation. 
Vinay Kumar NC State University 890 Oval Drive Raleigh, NC 27695  Nagiza F. Samatova Oak Ridge National Lab 
Among the many proposals to promote alternatives to costly to create gold standards, just recently the idea of a fully automatically, and thus cheaply, to set up silver standard has been launched. However, the current construction policy for such a silver standard requires crucial parameters (such as similarity thresholds and agreement cut-offs) to be set a priori, based on extensive testing though, at corpus compile time. Accordingly, such a corpus is static, once it is released. We here propose an alternative policy where silver standards can be dynamically optimized and customized on demand (given a speciﬁc goal function) using a gold standard as an oracle. 
In this paper, we present a two-phase, hybrid model for generating training data for Named Entity Recognition systems. In the first phase, a trained annotator labels all named entities in a text irrespective of type. In the second phase, naïve crowdsourcing workers complete binary judgment tasks to indicate the type(s) of each entity. Decomposing the data generation task in this way results in a flexible, reusable corpus that accommodates changes to entity type taxonomies. In addition, it makes efficient use of precious trained annotator resources by leveraging highly available and cost effective crowdsourcing worker pools in a way that does not sacrifice quality. Keywords: annotation scheme design, annotation tools and systems, corpus annotation, annotation for machine learning 
In this paper, we apply the annotation scheme design methodology deﬁned in (Bunt, 2010) and demonstrate its use for generating a mapping from an existing annotation scheme to a representation in GrAF format. The most important features of this methodology are (1) the distinction of the abstract and concrete syntax of an annotation language; (2) the speciﬁcation of a formal semantics for the abstract syntax; and (3) the formalization of the relation between abstract and concrete syntax, which guarantees that any concrete syntax inherits the semantics of the abstract syntax, and thus guarantees meaning-preserving mappings between representation formats. By way of illustration, we apply this mapping strategy to annotations from ISOTimeML, PropBank, and FrameNet. 
In conversational language, references to people (especially to the conversation participants, e.g., I, you, and we) are an essential part of many expressed meanings. In most conversational settings, however, many such expressions have numerous potential meanings, are frequently vague, and are highly dependent on social and situational context. This is a signiﬁcant challenge to conversational language understanding systems — one which has seen little attention in annotation studies. In this paper, we present a method for annotating verbal reference to people in conversational speech, with a focus on reference to conversation participants. Our goal is to provide a resource that tackles the issues of vagueness, ambiguity, and contextual dependency in a nuanced yet reliable way, with the ultimate aim of supporting work on summarization and information extraction for conversation. 
We present a syntactic annotation scheme for spoken French that is currently used in the Rhapsodie project. This annotation is dependency-based and includes coordination and disfluency as analogously encoded types of paradigmatic phenomena. Furthermore, we attempt a thorough definition of the discourse units required by the systematic annotation of other phenomena beyond usual sentence boundaries, which are typical for spoken language. This includes so called “macrosyntactic” phenomena such as dislocation, parataxis, insertions, grafts, and epexegesis. 
In this paper, we report on the annotation procedures we developed for annotating the Turkish Discourse Bank (TDB), an effort that extends the Penn Discourse Tree Bank (PDTB) annotation style by using it for annotating Turkish discourse. After a brief introduction to the TDB, we describe the annotation cycle and the annotation scheme we developed, defining which parts of the scheme are an extension of the PDTB and which parts are different. We provide inter-coder reliability calculations on the first and second arguments of some connectives and discuss the most important sources of disagreement among annotators. 
We present a word alignment framework that can incorporate partial manual alignments. The core of the approach is a novel semi-supervised algorithm extending the widely used IBM Models with a constrained EM algorithm. The partial manual alignments can be obtained by human labelling or automatically by high-precision-low-recall heuristics. We demonstrate the usages of both methods by selecting alignment links from manually aligned corpus and apply links generated from bilingual dictionary on unlabelled data. For the ﬁrst method, we conduct controlled experiments on ChineseEnglish and Arabic-English translation tasks to compare the quality of word alignment, and to measure effects of two different methods in selecting alignment links from manually aligned corpus. For the second method, we experimented with moderate-scale Chinese-English translation task. The experiment results show an average improvement of 0.33 BLEU point across 8 test sets. 
This paper presents a fast consensus hypothesis regeneration approach for machine translation. It combines the advantages of feature-based fast consensus decoding and hypothesis regeneration. Our approach is more efficient than previous work on hypothesis regeneration, and it explores a wider search space than consensus decoding, resulting in improved performance. Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 
This paper describes our Statistical Machine Translation systems for the WMT10 evaluation, where LIMSI participated for two language pairs (French-English and German-English, in both directions). For German-English, we concentrated on normalizing the German side through a proper preprocessing, aimed at reducing the lexical redundancy and at splitting complex compounds. For French-English, we studied two extensions of our in-house N -code decoder: ﬁrstly, the effect of integrating a new bilingual reordering model; second, the use of adaptation techniques for the translation model. For both set of experiments, we report the improvements obtained on the development and test data. 
The paper describes our experiments with English-Czech machine translation for WMT101 in 2010. Focusing primarily on the translation to Czech, our additions to the standard Moses phrase-based MT pipeline include two-step translation to overcome target-side data sparseness and optimization towards SemPOS, a metric better suited for evaluating Czech. Unfortunately, none of the approaches bring a signiﬁcant improvement over our standard setup. 
We present the Carnegie Mellon University Stat-XFER group submission to the WMT 2010 shared translation task. Updates to our syntax-based SMT system mainly fell in the areas of new feature formulations in the translation model and improved ﬁltering of SCFG rules. Compared to our WMT 2009 submission, we report a gain of 1.73 BLEU by using the new features and decoding environment, and a gain of up to 0.52 BLEU from improved grammar selection. 
In this paper we describe the statistical machine translation system of the RWTH Aachen University developed for the translation task of the Fifth Workshop on Statistical Machine Translation. Stateof-the-art phrase-based and hierarchical statistical MT systems are augmented with appropriate morpho-syntactic enhancements, as well as alternative phrase training methods and extended lexicon models. For some tasks, a system combination of the best systems was used to generate a ﬁnal hypothesis. We participated in the constrained condition of GermanEnglish and French-English in each translation direction. 
 2 Phrase-based SMT  This paper describes the 2010 phrase-based statistical machine translation system developed at the TALP Research Center of the UPC1 in cooperation with BMIC2 and VMU3. In phrase-based SMT, the phrase table is the main tool in translation. It is created extracting phrases from an aligned parallel corpus and then computing translation model scores with them. Performing a collocation segmentation over the source and target corpus before the alignment causes that dierent and larger phrases are extracted from the same original documents. We performed this segmentation and used the union of this phrase set with the phrase set extracted from the nonsegmented corpus to compute the phrase table. We present the congurations considered and also report results obtained with internal and ocial test sets. 
We describe our system for the translation task of WMT 2010. This system, developed for the English-French and FrenchEnglish directions, is based on Moses and was trained using only the resources supplied for the workshop. We report experiments to enhance it with out-of-domain parallel corpora sub-sampling, N-best list post-processing and a French grammatical checker. 
In this paper, we describe Exodus, a joint pilot project of the European Commission’s Directorate-General for Translation (DGT) and the European Parliament’s DirectorateGeneral for Translation (DG TRAD) which explores the potential of deploying new approaches to machine translation in European institutions. We have participated in the English-to-French track of this year’s WMT10 shared translation task using a system trained on data previously extracted from large inhouse translation memories. 
We report on efforts to build large-scale translation systems for eight European language pairs. We achieve most gains from the use of larger training corpora and basic modeling, but also show promising results from integrating more linguistic annotation. 
NRC’s Portage system participated in the English-French (E-F) and French-English (F-E) translation tasks of the ACL WMT 2010 evaluation. The most notable improvement over earlier versions of Portage is an efficient implementation of lattice MERT. While Portage has typically performed well in Chinese to English MT evaluations, most recently in the NIST09 evaluation, our participation in WMT 2010 revealed some interesting differences between Chinese-English and E-F/F-E translation, and alerted us to certain weak spots in our system. Most of this paper discusses the problems we found in our system and ways of fixing them. We learned several lessons that we think will be of general interest. 
We describe the progress we have made in the past year on Joshua (Li et al., 2009a), an open source toolkit for parsing based machine translation. The new functionality includes: support for translation grammars with a rich set of syntactic nonterminals, the ability for external modules to posit constraints on how spans in the input sentence should be translated, lattice parsing for dealing with input uncertainty, a semiring framework that provides a uniﬁed way of doing various dynamic programming calculations, variational decoding for approximating the intractable MAP decoding, hypergraph-based discriminative training for better feature engineering, a parallelized MERT module, documentlevel and tail-based MERT, visualization of the derivation trees, and a cleaner pipeline for MT experiments. 
 2 Baseline System  This paper describes our phrase-based Statistical Machine Translation (SMT) system for the WMT10 Translation Task. We submitted translations for the German to English and English to German translation tasks. Compared to state-of-the-art phrase-based systems we preformed additional preprocessing and used a discriminative word alignment approach. The word reordering was modeled using POS information and we extended the translation model with additional features. 
This paper describes the Cunei Machine Translation Platform and how it was used in the WMT ’10 German to English and Czech to English translation tasks. 
This paper describes the Cambridge University Engineering Department submission to the Fifth Workshop on Statistical Machine Translation. We report results for the French-English and Spanish-English shared translation tasks in both directions. The CUED system is based on HiFST, a hierarchical phrase-based decoder implemented using weighted ﬁnite-state transducers. In the French-English task, we investigate the use of context-dependent alignment models. We also show that lattice minimum Bayes-risk decoding is an effective framework for multi-source translation, leading to large gains in BLEU score. 
This paper describes the system submitted by the Laboratory of Informatics of Grenoble (LIG) for the ﬁfth Workshop on Statistical Machine Translation. We participated to the news shared translation task for the French-English language pair. We investigated differents techniques to simply deal with Out-Of-Vocabulary words in a statistical phrase-based machine translation system and analyze their impact on translation quality. The ﬁnal submission is a combination between a standard phrase-based system using the Moses decoder, with appropriate setups and pre-processing, and a lemmatized system to deal with Out-Of-Vocabulary conjugated verbs. 
We explore the possibility of using Stochastic Bracketing Linear Inversion Transduction Grammars for a full-scale German–English translation task, both on their own and in conjunction with alignments induced with GIZA++. The rationale for transduction grammars, the details of the system and some results are presented. 
We present the Johns Hopkins University submission to the 2010 WMT shared translation task. We describe processing steps using open data and open source software used in our submission, and provide the scripts and conﬁgurations required to train, tune, and test our machine translation system. 
We report results of our submissions to the WMT 2010 shared translation task in which we applied a system that includes adaptive language and translation models. Adaptation is implemented using exponentially decaying caches storing previous translations as the history for new predictions. Evidence from the cache is then mixed with the global background model. The main problem in this setup is error propagation and our submissions essentially failed to improve over the competitive baseline. There are slight improvements in lexical choice but the global performance decreases in terms of BLEU scores. 
This paper describes the Aalto submission for the German-to-English and the Czechto-English translation tasks of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR. Statistical machine translation has focused on using words, and longer phrases constructed from words, as tokens in the system. In contrast, we apply different morphological decompositions of words using the unsupervised Morfessor algorithms. While translation models trained using the morphological decompositions did not improve the BLEU scores, we show that the Minimum Bayes Risk combination with a word-based translation model produces signiﬁcant improvements for the Germanto-English translation. However, we did not see improvements for the Czech-toEnglish translations. 
 Z(x) is the normalizing factor  Maximum Entropy Principle has been used successfully in various NLP tasks. In this paper we propose a forward translation model consisting of a set of maximum entropy classiﬁers: a separate classiﬁer is trained for each (sufﬁciently frequent) source-side lemma. In this way the estimates of translation probabilities can be sensitive to a large number of features derived from the source sentence (including non-local features, features making use of sentence syntactic structure, etc.). When integrated into English-toCzech dependency-based translation scenario implemented in the TectoMT framework, the new translation model significantly outperforms the baseline model (MLE) in terms of BLEU. The performance is further boosted in a conﬁguration inspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model.  
 This paper describes the system developed in collabaration between UCH and UPV for the 2010 WMT. For this year’s workshop, we present a system for EnglishSpanish translation. Output N -best lists were rescored via a target Neural Network Language Model, yielding improvements in the ﬁnal translation quality as measured by BLEU and TER.  
In this paper we focus on the incremental decoding for a statistical phrase-based machine translation system. In incremental decoding, translations are generated incrementally for every word typed by a user, instead of waiting for the entire sentence as input. We introduce a novel modiﬁcation to the beam-search decoding algorithm for phrase-based MT to address this issue, aimed at efﬁcient computation of future costs and avoiding search errors. Our objective is to do a faster translation during incremental decoding without signiﬁcant reduction in the translation quality. 
Compound splitting is an important problem in many NLP applications which must be solved in order to address issues of data sparsity. Previous work has shown that linguistic approaches for German compound splitting produce a correct splitting more often, but corpus-driven approaches work best for phrase-based statistical machine translation from German to English, a worrisome contradiction. We address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance. 
In Arabic-to-English phrase-based statistical machine translation, a large number of syntactic disﬂuencies are due to wrong long-range reordering of the verb in VSO sentences, where the verb is anticipated with respect to the English word order. In this paper, we propose a chunk-based reordering technique to automatically detect and displace clause-initial verbs in the Arabic side of a word-aligned parallel corpus. This method is applied to preprocess the training data, and to collect statistics about verb movements. From this analysis, speciﬁc verb reordering lattices are then built on the test sentences before decoding them. The application of our reordering methods on the training and test sets results in consistent BLEU score improvements on the NIST-MT 2009 ArabicEnglish benchmark. 
English is a typical SVO (Subject-VerbObject) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently, a few groups have proposed rulebased preprocessing methods to mitigate this problem (Xu et al., 2009; Hong et al., 2009). These methods rewrite SVO sentences to derive more SOV-like sentences by using a set of handcrafted rules. In this paper, we propose an alternative single reordering rule: Head Finalization. This is a syntax-based preprocessing approach that offers the advantage of simplicity. We do not have to be concerned about partof-speech tags or rule weights because the powerful Enju parser allows us to implement the rule at a general level. Our experiments show that its result, Head Final English (HFE), follows almost the same order as Japanese. We also show that this rule improves automatic evaluation scores. 
We propose a method to improve the translation of pronouns by resolving their coreference to prior mentions. We report results using two different co-reference resolution methods and point to remaining challenges. 
We present Jane, RWTH’s hierarchical phrase-based translation system, which has been open sourced for the scientiﬁc community. This system has been in development at RWTH for the last two years and has been successfully applied in different machine translation evaluations. It includes extensions to the hierarchical approach developed by RWTH as well as other research institutions. In this paper we give an overview of its main features. We also introduce a novel reordering model for the hierarchical phrase-based approach which further enhances translation performance, and analyze the effect some recent extended lexicon models have on the performance of the system. 
LIUM participated in the System Combination task of the Fifth Workshop on Statistical Machine Translation (WMT 2010). Hypotheses from 5 French/English MT systems were combined with MANY, an open source system combination software based on confusion networks currently developed at LIUM. The system combination yielded signiﬁcant improvements in BLEU score when applied on WMT’09 data. The same behavior has been observed when tuning is performed on development data of this year evaluation. 
We analyze adaptive model weighting techniques for reranking using instance scores obtained by L1 regularized transductive regression. Competitive statistical machine translation is an on-line learning technique for sequential translation tasks where we try to select the best among competing statistical machine translators. The competitive predictor assigns a probability per model weighted by the sequential performance. We deﬁne additive, multiplicative, and lossbased weight updates with exponential loss functions for competitive statistical machine translation. Without any pre-knowledge of the performance of the translation models, we succeed in achieving the performance of the best model in all systems and surpass their performance in most of the language pairs we considered. 
We use L1 regularized transductive regression to learn mappings between source and target features of the training sets derived for each test sentence and use these mappings to rerank translation outputs. We compare the effectiveness of L1 regularization techniques for regression to learn mappings between features given in a sparse feature matrix. The results show the effectiveness of using L1 regularization versus L2 used in ridge regression. We show that regression mapping is effective in reranking translation outputs and in selecting the best system combinations with encouraging results on different language pairs. 
 translation hypotheses and generate a new target  This paper describes the augmented threepass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combination task. The basic three-pass framework includes building individual confusion networks (CNs), a super network, and a modiﬁed Minimum Bayes-risk (mConMBR) decoder. The augmented parts for WMT2010 tasks include 1) a rescoring component which is used to re-rank the N -best lists generated from the individual CNs and the super network, 2) a new hypothesis alignment metric – TERp – that is used to carry out English-targeted hypothesis alignment, and 3) more different backbone-based CNs which are employed to increase the diversity of the mConMBR decoding phase. We took part in the combination tasks of Englishto-Czech and French-to-English. Experimental results show that our proposed combination framework achieved 2.17 absolute points (13.36 relative points) and 1.52 absolute points (5.37 relative points) in terms of BLEU score on English-toCzech and French-to-English tasks respectively than the best single system. We also achieved better performance on human evaluation.  sentence. A CN is essentially a directed acyclic graph built from a set of translation hypotheses against a reference or “backbone”. Each arc between two nodes in the CN denotes a word or token, possibly a null item, with an associated posterior probability. Typically, the dominant CN is constructed at the word level by a state-of-the-art framework: ﬁrstly, a minimum Bayes-risk (MBR) decoder (Kumar and Byrne, 2004) is utilised to choose the backbone from a merged set of hypotheses, and then the remaining hypotheses are aligned against the backbone by a speciﬁc alignment approach. Currently, most research in system combination has focused on hypothesis alignment due to its significant inﬂuence on combination quality. A multiple CN or “super-network” framework was ﬁrstly proposed in Rosti et al. (2007) who used each of all individual system results as the backbone to build CNs based on the same alignment metric, TER (Snover et al., 2006). A consensus network MBR (ConMBR) approach was presented in (Sim et al., 2007), where MBR decoding is employed to select the best hypothesis with the minimum cost from the original single system outputs compared to the consensus output. Du and Way (2009) proposed a combination strategy that employs MBR, super network, and a modiﬁed ConMBR (mConMBR) approach to construct a three-pass system combination framework which can effectively combine different hy-  
 computes the optimal sequence of edit operations  UPV-PRHLT participated in the System Combination task of the Fifth Workshop on Statistical Machine Translation (WMT 2010). On each translation direction, all the submitted systems were combined into a consensus translation. These consensus translations always improve translation quality of the best individual system.  (insertions, deletions and substitutions of words) needed to transform one string into the other. The main problem with the ED is its dependence on the length of the compared strings. This fact led to the deﬁnition of a new distance whose value is independent from the length of the strings compared. This normalised edit distance (NED) (Vidal et al., 1995) is computed by averaging the number of edit operations by the length of the edit path. The ex-  
This paper describes our submission, cmu-heafield-combo, to the WMT 2010 machine translation system combination task. Using constrained resources, we participated in all nine language pairs, namely translating English to and from Czech, French, German, and Spanish as well as combining English translations from multiple languages. Combination proceeds by aligning all pairs of system outputs then navigating the aligned outputs from left to right where each path is a candidate combination. Candidate combinations are scored by their length, agreement with the underlying systems, and a language model. On tuning data, improvement in BLEU over the best system depends on the language pair and ranges from 0.89% to 5.57% with mean 2.37%. 
This paper describes the JHU system combination scheme that was used in the WMT 2010 submission. The incremental alignment scheme of (Karakos et.al, 2008) was used for confusion network generation. The system order in the alignment of each sentence was learned using SVMs, following the work of (Karakos et.al, 2010). Additionally, web-scale n-grams from the Google corpus were used to build language models that improved the quality of the combination output. Experiments in SpanishEnglish, French-English, German-English and Czech-English language pairs were conducted, and the results show approximately 1 BLEU point and 2 TER points improvement over the best individual system. 
RWTH participated in the System Combination task of the Fifth Workshop on Statistical Machine Translation (WMT 2010). For 7 of the 8 language pairs, we combine 5 to 13 systems into a single consensus translation, using additional n-best reranking techniques in two of these language pairs. Depending on the language pair, improvements versus the best single system are in the range of +0.5 and +1.7 on BLEU, and between −0.4 and −2.3 on TER. Novel techniques compared with RWTH’s submission to WMT 2009 include the utilization of n-best reranking techniques, a consensus true casing approach, a different tuning algorithm, and the separate selection of input systems for CN construction, primary/skeleton hypotheses, HypLM, and true casing. 
BBN submitted system combination outputs for Czech-English, German-English, Spanish-English, French-English, and AllEnglish language pairs. All combinations were based on confusion network decoding. An incremental hypothesis alignment algorithm with ﬂexible matching was used to build the networks. The bi-gram decoding weights for the single source language translations were tuned directly to maximize the BLEU score of the decoding output. Approximate expected BLEU was used as the objective function in gradient based optimization of the combination weights for a 44 system multi-source language combination (All-English). The system combination gained around 0.42.0 BLEU points over the best individual systems on the single source conditions. On the multi-source condition, the system combination gained 6.6 BLEU points. 
The ability to measure the quality of word order in translations is an important goal for research in machine translation. Current machine translation metrics do not adequately measure the reordering performance of translation systems. We present a novel metric, the LRscore, which directly measures reordering success. The reordering component is balanced by a lexical metric. Capturing the two most important elements of translation success in a simple combined metric with only one parameter results in an intuitive, shallow, language independent metric. 
This paper describes the joint submission of Universitat Polite`cnica de Catalunya and Universitat de Barcelona to the Metrics MaTr 2010 evaluation challenge, in collaboration with ELDA/ELRA. Our work is aimed at widening the scope of current automatic evaluation measures from sentence to document level. Preliminary experiments, based on an extension of the metrics by Gime´nez and Ma`rquez (2009) operating over discourse representations, are presented. 
This paper describes our submission to the WMT10 Shared Evaluation Task and MetricsMATR10. We present a version of the METEOR-NEXT metric with paraphrase tables for ﬁve target languages. We describe the creation of these paraphrase tables and conduct a tuning experiment that demonstrates consistent improvement across all languages over baseline versions of the metric without paraphrase resources. 
We describe DCU’s LFG dependencybased metric submitted to the shared evaluation task of WMT-MetricsMATR 2010. The metric is built on the LFG F-structurebased approach presented in (Owczarzak et al., 2007). We explore the following improvements on the original metric: 1) we replace the in-house LFG parser with an open source dependency parser that directly parses strings into LFG dependencies; 2) we add a stemming module and unigram paraphrases to strengthen the aligner; 3) we introduce a chunk penalty following the practice of METEOR to reward continuous matches; and 4) we introduce and tune parameters to maximize the correlation with human judgement. Experiments show that these enhancements improve the dependency-based metric’s correlation with human judgement. 
We present TESLA-M and TESLA, two novel automatic machine translation evaluation metrics with state-of-the-art performances. TESLA-M builds on the success of METEOR and MaxSim, but employs a more expressive linear programming framework. TESLA further exploits parallel texts to build a shallow semantic representation. We evaluate both on the WMT 2009 shared evaluation task and show that they outperform all participating systems in most tasks. 
This paper describes the latest version of the ATEC metric for automatic MT evaluation, with parameters optimized for word choice and word order, the two fundamental features of language that the metric relies on. The former is assessed by matching at various linguistic levels and weighting the informativeness of both matched and unmatched words. The latter is quantified in term of word position and information flow. We also discuss those aspects of language not yet covered by other existing evaluation metrics but carefully considered in the formulation of our metric. 
We present a uniﬁed approach to performing minimum risk training and minimum Bayes risk (MBR) decoding with BLEU in a phrase-based model. Key to our approach is the use of a Gibbs sampler that allows us to explore the entire probability distribution and maintain a strict probabilistic formulation across the pipeline. We also describe a new sampling algorithm called corpus sampling which allows us at training time to use BLEU instead of an approximation thereof. Our approach is theoretically sound and gives better (up to +0.6%BLEU) and more stable results than the standard MERT optimization algorithm. By comparing our approach to lattice MBR, we are also able to gain crucial insights about both methods. 
We propose a new framework for N-best reranking on sparse feature sets. The idea is to reformulate the reranking problem as a Multitask Learning problem, where each N-best list corresponds to a distinct task. This is motivated by the observation that N-best lists often show signiﬁcant differences in feature distributions. Training a single reranker directly on this heterogenous data can be difﬁcult. Our proposed meta-algorithm solves this challenge by using multitask learning (such as ℓ1/ℓ2 regularization) to discover common feature representations across Nbest lists. This meta-algorithm is simple to implement, and its modular approach allows one to plug-in different learning algorithms from existing literature. As a proof of concept, we show statistically signiﬁcant improvements on a machine translation system involving millions of features. 
Structured perceptrons are attractive due to their simplicity and speed, and have been used successfully for tuning the weights of binary features in a machine translation system. In attempting to apply them to tuning the weights of real-valued features with highly skewed distributions, we found that they did not work well. This paper describes a modiﬁcation to the update step and compares the performance of the resulting algorithm to standard minimum error-rate training (MERT). In addition, preliminary results for combining MERT or structured-perceptron tuning of the log-linear feature weights with coordinate ascent of other translation system parameters are presented. 
The translation model of statistical machine translation systems is trained on parallel data coming from various sources and domains. These corpora are usually concatenated, word alignments are calculated and phrases are extracted. This means that the corpora are not weighted according to their importance to the domain of the translation task. This is in contrast to the training of the language model for which well known techniques are used to weight the various sources of texts. On a smaller granularity, the automatic calculated word alignments differ in quality. This is usually not considered when extracting phrases either. In this paper we propose a method to automatically weight the different corpora and alignments. This is achieved with a resampling technique. We report experimental results for a small (IWSLT) and large (NIST) Arabic/English translation tasks. In both cases, signiﬁcant improvements in the BLEU score were observed. 
This paper proposes an unsupervised word segmentation algorithm that identiﬁes word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches. The method can be applied to any language pair where the source language is unsegmented and the target language segmentation is known. First, an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext. In the second step, multiple segmentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models. Experimental results translating ﬁve Asian languages into English revealed that the method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available state-ofthe-art monolingually-built segmentation tools. 
We present a new translation model that include undecorated hierarchical-style phrase rules, decorated source-syntax rules, and partially decorated rules. Results show an increase in translation performance of up to 0.8% BLEU for German–English translation when trained on the news-commentary corpus, using syntactic annotation from a source language parser. We also experimented with annotation from shallow taggers and found this increased performance by 0.5% BLEU. 
This paper proposes a novel method for long distance, clause-level reordering in statistical machine translation (SMT). The proposed method separately translates clauses in the source sentence and reconstructs the target sentence using the clause translations with non-terminals. The nonterminals are placeholders of embedded clauses, by which we reduce complicated clause-level reordering into simple wordlevel reordering. Its translation model is trained using a bilingual corpus with clause-level alignment, which can be automatically annotated by our alignment algorithm with a syntactic parser in the source language. We achieved signiﬁcant improvements of 1.4% in BLEU and 1.3% in TER by using Moses, and 2.2% in BLEU and 3.5% in TER by using our hierarchical phrase-based SMT, for the English-to-Japanese translation of research paper abstracts in the medical domain. 
We present a method for incorporating arbitrary context-informed word attributes into statistical machine translation by clustering attribute-qualied source words, and smoothing their word translation probabilities using binary decision trees. We describe two ways in which the decision trees are used in machine translation: by using the attribute-qualied source word clusters directly, or by using attributedependent lexical translation probabilities that are obtained from the trees, as a lexical smoothing feature in the decoder model. We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-ofspeech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system. 
In this paper we give an overview of Computational Linguistics / Natural Language Processing in Brazil, describing the general research scenario, the main research groups, existing events and journals, and the perceived challenges, among other relevant information. We also identify opportunities for collaboration. 
This paper provides a survey of some ongoing research projects in computational linguistics within the group of Natural Language Processing at the University of Co´rdoba, Argentina. We outline our future plans and spotlight some opportunities for collaboration. 
Variable-Length Markov Chains (VLMCs) offer a way of modeling contexts longer than trigrams without suffering from data sparsity and state space complexity. However, in Historical Portuguese, two words show a high degree of ambiguity: que and a. The number of errors tagging these words corresponds to a quarter of the total errors made by a VLMCbased tagger. Moreover, these words seem to show two different types of ambiguity: one depending on non-local context and another on right context. We searched ways of expanding the VLMC-based tagger with a number of different models and methods in order to tackle these issues. The methods showed variable degrees of success, with one particular method solving much of the ambiguity of a. We explore reasons why this happened, and how everything we tried fails to improve the precision of que. 
This paper reports an ongoing work in applying Common Sense knowledge to Machine Translation aiming at generating more culturally contextualized translations. Common Sense can be deﬁned as the knowledge shared by a group of people in a given time, space and culture; and this knowledge, here, is represented by a semantic network called ConceptNet. Machine Translation, in turn, is the automatic process of generating an equivalent translated version of a source sentence. In this work we intend to use the knowledge represented in two ConceptNets, one in Brazilian Portuguese and another in English, to ﬁx/ﬁlter translations built automatically. So, this paper presents the initial ideas of our work, the steps taken so far as well as some opportunities for collaboration. 
We present work in progress in the application of Natural Language Processing (NLP) technology to the analysis of textual transcriptions of psychotherapy sessions in the Spanish Language. We are developing a set of NLP tools as well as adapting an existing dictionary for the analysis of interviews framed on a psychoanalytic theory. We investigate the application of NLP techniques, including dictionarybased interpretation, and speech act identiﬁcation and classiﬁcation for the (semi) automatic identiﬁcation in text of a set of psychoanalytical variables. The objective of the work is to provide a set of tools and resources to assist therapist during discourse analysis. 
This paper aims to bring a general overview on the situation of Computational Linguistics in Costa Rica, particularly in the academic world. 
In this paper we present the PorSimples project, whose aim is to develop text adaptations tools for Brazilian Portuguese. The tools developed cater for both people at poor literacy levels and authors that want to produce texts for this audience. Here we describe the tools and resources developed over two years of this project and point directions for future work and collaboration. Since Portuguese and Spanish have many aspects in common, we believe our main point for collaboration lies in transferring our knowledge and experience to researches willing to developed simplification and elaboration tools for Spanish. 
We present our work on the identification of opinions and its components: the source, the topic and the message. We describe a rule-based system for which we achieved a recall of 74% and a precision of 94%. Experimentation with machine-learning techniques for the same task is currently underway. 
This paper presents a system that uses machine learning algorithms for the task of recognizing textual entailment in Spanish language. The datasets used include SPARTE Corpus and a translated version to Spanish of RTE3, RTE4 and RTE5 datasets. The features chosen quantify lexical, syntactic and semantic level matching between text and hypothesis sentences. We analyze how the different sizes of datasets and classifiers could impact on the final overall performance of the RTE classification of two-way task in Spanish. The RTE system yields 60.83% of accuracy and a competitive result of 66.50% of accuracy is reported by train and test set taken from SPARTE Corpus with 70% split. 
The evolution of literary styles in the western tradition has been the subject of extended research that arguably has spanned centuries. In particular, previous work has conjectured the existence of a gradual yet persistent increase of the degree of self-awareness or introspection, i.e. that capacity to expound on one’s own thought processes and behaviors, reﬂected in the chronology of the classical literary texts. This type of question has been traditionally addressed by qualitative studies in philology and literary theory. In this paper, we describe preliminary results based on the application of computational linguistics techniques to quantitatively analyze this hypothesis. We evaluate the appearance of introspection in texts by searching words related to it, and focus on simple studies on the Bible. This preliminary results are highly positive, indicating that it is indeed possible to statistically discriminate between texts based on a semantic core centered around introspection, chronologically and culturally belonging to different phases. In our opinion, the rigurous extension of our analysis can provide not only a stricter statistical measure of the evolution of introspection, but also means to investigate subtle differences in aesthetic styles and cognitive structures across cultures, authors and literary forms.  
In this paper, an overview of an approach for cross-language image indexing and multilingual terminology alignment is presented. Content-Based Image Retrieval (CBIR) is proposed as a means to find similar images in target language documents in the web and natural language processing is used to reduce the search space and find the image index. As the experiments are carried out in specialized domains, a systematic and recursive use of the approach is used to align multilingual terminology by creating repositories of images with their respective cross-language indices. 
IRASubcat is a language-independent tool to acquire information about the subcategorization of verbs from corpus. The tool can extract information from corpora annotated at various levels, including almost raw text, where only verbs are identiﬁed. It can also aggregate information from a pre-existing lexicon with verbal subcategorization information. The system is highly customizable, and works with XML as input and output format. IRASubcat identiﬁes patterns of constituents in the corpus, and associates patterns with verbs if their association strength is over a frequency threshold and passes the likelihood ratio hypothesis test. It also implements a procedure to identify verbal constituents that could be playing the role of an adjunct in a pattern. Thresholds controlling frequency and identiﬁcation of adjuncts can be customized by the user, or else they are given a default value. 
Linguistic resources with domain-specific coverage are crucial for the development of concrete Natural Language Processing (NLP) systems. In this paper we give a global introduction to the ongoing (since 2009) TermiNet project, whose aims are to instantiate a generic NLP methodology for the development of terminological wordnets and to apply the instantiated methodology for building a terminological wordnet in Brazilian Portuguese. 
Most embedded systems for the avionics industry are considered safety critical systems; as a result, strict software development standards exist to ensure critical software is built with the highest quality possible. One of such standards, DO-178B, establishes a number of properties that software requirements must satisfy including: accuracy, non-ambiguity and veriﬁability. From a language perspective, it is possible to automate the analysis of software requirements to determine whether or not they satisfy some quality properties. This work suggests a bounded deﬁnition for three properties (accuracy, non-ambiguity and veriﬁability) considering the main characteristics that software requirements must exhibit to satisfy those objectives. A software prototype that combines natural language processing (NLP) techniques and specialized dictionaries was built to examine software requirements written in English with the goal of identifying whether or not they satisfy the desired properties. Preliminary results are presented showing how the tool effectively identiﬁes critical issues that are normally ignored by human reviewers. 
In this paper we propose a method to exploit analytical definitions extracted from Spanish corpora, in order to build a lexical network based on the hyponymy/hyperonymy, part/whole and attribution relations. Our method considers the following steps: (a) the recognition and extraction of definitional contexts from specialized documents, (b) the identification of analytical definitions on these definitional contexts, using verbal predications, (c) the syntactic and probabilistic analysis of the association observed between verbal predication and analytical definitions, (d) the identification of the hyponymy/hyperonymy, part/whole and attribution relations based on the lexical information that lies between predications and definitions and other types of phrases, in particular prepositional phrases mapped by the preposition de (Eng. of/from). 
Requirements elicitation is one of the first processes of software development and it is intended to be hand-made by means of analyst-stakeholder interviews. As a naturallanguage-based activity, requirements elicitation can take advantages of Computational Linguistics techniques, in order to achieve better results looking for automation in this field. In this paper we survey some of the work related to software development automation, guided by Computational Linguistics techniques, and performed by the Computational Language Research Group from the Universidad Nacional de Colombia. We aim the definition of future trans-national effort to be made in this research line. 
Despite the growing interest in NLP focused on the Brazilian Portuguese language in recent years, its obvious counterpart – Natural Language Generation (NLG) – remains in that case a little-explored research field. In this paper we describe preliminary results of a first project of this kind, addressing the issue of surface realization for Brazilian Portuguese. Our approach, which may be particularly suitable to simpler NLG applications in which a domain corpus of the most likely output sentences happens to be available, is in principle adaptable to many closely-related languages, and paves the way to further NLG research focused on Romance languages in general. 
We present an on-going research project carried out at the Universidad Nacional de Co´rdoba in Argentina. This project investigates theoretical and practical research questions related to the development of a dialogue system situated in a virtual environment. We describe the PLN research group in which this project is being developed and, in particular, we spell out the areas of expertise of the authors. Moreover, we discuss relevant past, current and future collaborations of the research group. 
In this paper we introduce the ﬁrst version of noWaC, a large web-based corpus of Bokmål Norwegian currently containing about 700 million tokens. The corpus has been built by crawling, downloading and processing web documents in the .no top-level internet domain. The procedure used to collect the noWaC corpus is largely based on the techniques described by Ferraresi et al. (2008). In brief, ﬁrst a set of “seed” URLs containing documents in the target language is collected by sending queries to commercial search engines (Google and Yahoo). The obtained seeds (overall 6900 URLs) are then used to start a crawling job using the Heritrix web-crawler limited to the .no domain. The downloaded documents are then processed in various ways in order to build a linguistic corpus (e.g. ﬁltering by document size, language identiﬁcation, duplicate and near duplicate detection, etc.). 
Post-positional particles are a signiﬁcant source of errors for learners of Korean. Following methodology that has proven effective in handling English preposition errors, we are beginning the process of building a machine learner for particle error detection in L2 Korean writing. As a ﬁrst step, however, we must acquire data, and thus we present a methodology for constructing large-scale corpora of Korean from the Web, exploring the feasibility of building corpora appropriate for a given topic and grammatical construction. 
In this paper, we address the challenges posed by large amounts of text data by exploiting the power of hashing in the context of streaming data. We explore sketch techniques, especially the CountMin Sketch, which approximates the frequency of a word pair in the corpus without explicitly storing the word pairs themselves. We use the idea of a conservative update with the Count-Min Sketch to reduce the average relative error of its approximate counts by a factor of two. We show that it is possible to store all words and word pairs counts computed from 37 GB of web data in just 2 billion counters (8 GB RAM). The number of these counters is up to 30 times less than the stream size which is a big memory and space gain. In Semantic Orientation experiments, the PMI scores computed from 2 billion counters are as effective as exact PMI scores. 
A procedure is described to gather corpora of academic writing from the web using BootCaT. The procedure uses terms distinctive of different registers and disciplines in COCA to locate and gather web pages containing them. 
This paper introduces Web1T5-Easy, a simple indexing solution that allows interactive searches of the Web 1T 5-gram database and a derived database of quasi-collocations. The latter is validated against co-occurrence data from the BNC and ukWaC on the automatic identiﬁcation of non-compositional VPC. 
The term Morphologically Rich Languages (MRLs) refers to languages in which significant information concerning syntactic units and relations is expressed at word-level. There is ample evidence that the application of readily available statistical parsing models to such languages is susceptible to serious performance degradation. The ﬁrst workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite languagespeciﬁc idiosyncrasies, the problems associated with parsing MRLs cut across languages and parsing frameworks. In this paper we review the current state-of-affairs with respect to parsing MRLs and point out central challenges. We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations. 
We explore the contribution of different lexical and inﬂectional morphological features to dependency parsing of Arabic, a morphologically rich language. We experiment with all leading POS tagsets for Arabic, and introduce a few new sets. We show that training the parser using a simple regular expressive extension of an impoverished POS tagset with high prediction accuracy does better than using a highly informative POS tagset with only medium prediction accuracy, although the latter performs best on gold input. Using controlled experiments, we ﬁnd that deﬁniteness (or determiner presence), the so-called phifeatures (person, number, gender), and undiacritzed lemma are most helpful for Arabic parsing on predicted input, while case and state are most helpful on gold. 
In this paper we explore two strategies to incorporate local morphosyntactic features in Hindi dependency parsing. These features are obtained using a shallow parser. We first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type, head/non-head information, chunk boundary information, distance to the end of the chunk and suffix concatenation are very crucial in Hindi dependency parsing. We then investigate the best way to incorporate this information during dependency parsing. Further, we compare the results of various experiments based on various criterions and do some error analysis. All the experiments were done with two data-driven parsers, MaltParser and MSTParser, on a part of multi-layered and multi-representational Hindi Treebank which is under development. This paper is also the first attempt at complete sentence level parsing for Hindi. 
We present a set of experiments on dependency parsing of the Basque Dependency Treebank (BDT). The present work has examined several directions that try to explore the rich set of morphosyntactic features in the BDT: i) experimenting the impact of morphological features, ii) application of dependency tree transformations, iii) application of a two-stage parsing scheme (stacking), and iv) combinations of the individual experiments. All the tests were conducted using MaltParser (Nivre et al., 2007a), a freely available and state of the art dependency parser generator. 
We show that na¨ıve modeling of morphosyntactic agreement in a Constituency-Based (CB) statistical parsing model is worse than none, whereas a linguistically adequate way of modeling inﬂectional morphology in CB parsing leads to improved performance. In particular, we show that an extension of the Relational-Realizational (RR) model that incorporates agreement features is superior to CB models that treat morphosyntax as statesplits (SP), and that the RR model beneﬁts more from inﬂectional features. We focus on parsing Hebrew and report the best result to date, F184.13 for parsing off of gold-tagged text, 5% error reduction from previous results. 
We investigate parsing accuracy on the Korean Treebank 2.0 with a number of different grammars. Comparisons among these grammars and to their English counterparts suggest different aspects of Korean that contribute to parsing difﬁculty. Our results indicate that the coarseness of the Treebank’s nonterminal set is a even greater problem than in the English Treebank. We also ﬁnd that Korean’s relatively free word order does not impact parsing results as much as one might expect, but in fact the prevalence of zero pronouns accounts for a large portion of the difference between Korean and English parsing scores. 
Discontinuities occur especially frequently in languages with a relatively free word order, such as German. Generally, due to the longdistance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser. In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks. In both treebanks, discontinuities are annotated with crossing branches. Based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems.  In most constituency treebanks, sentence annotation is restricted to having the shape of trees without crossing branches, and the non-local dependencies induced by the discontinuities are modeled by an additional mechanism. In the Penn Treebank (PTB) (Marcus et al., 1994), e.g., this mechanism is a combination of special labels and empty nodes, establishing implicit additional edges. In the German Tu¨Ba-D/Z (Telljohann et al., 2006), additional edges are established by a combination of topological ﬁeld annotation and special edge labels. As an example, Fig. 1 shows a tree from Tu¨Ba-D/Z with the annotation of (1). Note here the edge label ONMOD on the relative clause which indicates that the subject of the sentence (alle Attribute) is modiﬁed.  
This paper presents a study of the impact of using simple and complex morphological clues to improve the classiﬁcation of rare and unknown words for parsing. We compare this approach to a language-independent technique often used in parsers which is based solely on word frequencies. This study is applied to three languages that exhibit different levels of morphological expressiveness: Arabic, French and English. We integrate information about Arabic afﬁxes and morphotactics into a PCFG-LA parser and obtain stateof-the-art accuracy. We also show that these morphological clues can be learnt automatically from an annotated corpus. 
We present and discuss experiments in statistical parsing of French, where terminal forms used during training and parsing are replaced by more general symbols, particularly clusters of words obtained through unsupervised linear clustering. We build on the work of Candito and Crabbé (2009) who proposed to use clusters built over slightly coarsened French inﬂected forms. We investigate the alternative method of building clusters over lemma/part-of-speech pairs, using a raw corpus automatically tagged and lemmatized. We ﬁnd that both methods lead to comparable improvement over the baseline (we obtain F1=86.20% and F1=86.21% respectively, compared to a baseline of F1=84.10%). Yet, when we replace gold lemma/POS pairs with their corresponding cluster, we obtain an upper bound (F1=87.80) that suggests room for improvement for this technique, should tagging/lemmatisation performance increase for French. We also analyze the improvement in performance for both techniques with respect to word frequency. We ﬁnd that replacing word forms with clusters improves attachment performance for words that are originally either unknown or low-frequency, since these words are replaced by cluster symbols that tend to have higher frequencies. Furthermore, clustering also helps signiﬁcantly for medium to high frequency words, suggesting that training on word clusters leads to better probability estimates for these words. 
This paper shows that training a lexicalized parser on a lemmatized morphologically-rich treebank such as the French Treebank slightly improves parsing results. We also show that lemmatizing a similar in size subset of the English Penn Treebank has almost no effect on parsing performance with gold lemmas and leads to a small drop of performance when automatically assigned lemmas and POS tags are used. This highlights two facts: (i) lemmatization helps to reduce lexicon data-sparseness issues for French, (ii) it also makes the parsing process sensitive to correct assignment of POS tags to unknown words. 
This paper analyzes the relative importance of different linguistic features for data-driven dependency parsing of Hindi, using a feature pool derived from two state-of-the-art parsers. The analysis shows that the greatest gain in accuracy comes from the addition of morphosyntactic features related to case, tense, aspect and modality. Combining features from the two parsers, we achieve a labeled attachment score of 76.5%, which is 2 percentage points better than the previous state of the art. We finally provide a detailed error analysis and suggest possible improvements to the parsing scheme. 
We investigate the performance of an easyﬁrst, non-directional dependency parser on the Hebrew Dependency treebank. We show that with a basic feature set the greedy parser’s accuracy is on a par with that of a ﬁrst-order globally optimized MST parser. The addition of morphological-agreement feature improves the parsing accuracy, making it on-par with a second-order globally optimized MST parser. The improvement due to the morphological agreement information is persistent both when gold-standard and automatically-induced morphological information is used. 
We are building a tool that helps children with Complex Communication Needs1 (CCN) to create stories about their day at school. The tool uses Natural Language Generation (NLG) technology to create a draft story based on sensor data of the child’s activities, which the child can edit. This work is still in its early stages, but we believe it has great potential to support interactive personal narrative which is not well supported by current Augmentative and Alternative Communication (AAC) tools. 
We detail the design, development and evaluation of Augmentative and Alternative Communication (AAC) software which encourages rapid conversational interaction. The system uses Natural Language Generation (NLG) technology to automatically generate conversational utterances from a domain knowledge base modelled from content suggested by a small AAC user group. Findings from this work are presented along with a discussion about how NLG might be successfully applied to conversational AAC systems in the future.  munication rates. However, this approach suffers from several drawbacks which may have affected its more general adoption. Furthermore, Natural Language Processing (NLP) technology has proven to be a fruitful line of inquiry within the field. It has offered a powerful means to improve system productivity and usability. We are currently investigating how Natural Language Generation (NLG) might be applied in a useful way within an AAC device geared towards fast-paced and rewarding social interactions. It is hoped that the linguistic control and automaticity offered by NLG may go some way towards addressing the previous criticisms of pre-stored material regarding its inflexibility and cost in effort. 2 Background  
Utterance-based AAC systems have the potential to significantly speed communication rate for someone who relies on a speech generating device for communication. At the same time, such systems pose interesting challenges including anticipating text needs, remembering what text is stored, and accessing desired text when needed. Moreover, using such systems has profound pragmatic implications as a prestored message may or may not capture exactly what the user wishes to say in a particular discourse situation. In this paper we describe a prototype of an utterance-based AAC system whose design choices are driven by findings from theoretically driven studies concerning pragmatic choices with which the user of such a system is faced. These findings are coupled with cognitive theories to make choices for system design. 
We present preliminary experiments of a binary-switch, static-grid typing interface making use of varying language model contributions. Our motivation is to quantify the degree to which language models can make the simplest scanning interfaces – such as showing one symbol at a time rather than a scanning a grid – competitive in terms of typing speed. We present a grid scanning method making use of optimal Huffman binary codes, and demonstrate the impact of higher order language models on its performance. We also investigate the scanning methods of highlighting just one cell in a grid at any given time or showing one symbol at a time without a grid, and show that they yield commensurate performance when using higher order n-gram models, mainly due to lower error rate and a lower rate of missed targets. 
The use of speech production data has been limited by a steep learning curve and the need for laborious hand measurement. We are building a tool set that provides summary statistics for measures designed by clinicians to screen, diagnose or provide training to assistive technology users. This will be achieved by extending an existing shareware software platform with “plug-ins” that perform specific measures and report results to the user. The common underlying basis for this tool set is a Stevens’ paradigm of landmarks, points in an utterance around which information about articulatory events can be extracted.  We are building a tool set to provide summary statistics for measures designed by clinicians to screen, diagnose or provide training to patients. This will be achieved by extending an existing shareware software platform with “plug-ins” that perform specific measures and report results to the user. At present, our goal is to use the existing shareware software tool Wavesurfer (Wavesurfer, 2005). The new modules will be set up to report data from a single audio file, or groups of audio files in a standard table format, for easy input to statistical or other analysis software. For example, the data may be imported into a program that correlates speech data with scalp electrode and medication data.  
Today, the technology used for voting does not fully address the issues that disabled voters are confronted with during elections. Voters, including those with most disabilities, should be able to vote and verify his or her ballot during elections without the assistance of others. In order for this to happen, a universal design should be incorporated into the development of all voting systems. The research presented here embraces the needs of those who are disabled. The primary objective of this research was to develop a system in which a person, can efficiently, anonymously, and independently write-in a candidate’s name during an election. The method presented here uses speech interaction and name prediction to allow voters to privately spell the name of the candidate they intend to write-in. A study was performed to determine the effectiveness and efficiency of the system. The results of the study showed that spelling a name using the predictive method developed is an effective and efficient solution to the aforementioned issues. 
Existing Augmentative and Alternative Communication vocabularies assign multimodal stimuli to words with multiple meanings. The ambiguity hampers the vocabulary effectiveness when used by people with language disabilities. For example, the noun “a missing letter” may refer to a character or a written message, and each corresponds to a different picture. A vocabulary with images and sounds unambiguously linked to words can better eliminate misunderstanding and assist communication for people with language disorders. We explore a new approach of creating such a vocabulary via automatically assigning semantically unambiguous groups of synonyms to sound and image labels. We propose an unsupervised word sense disambiguation (WSD) voting algorithm, which combines different semantic relatedness measures. Our voting algorithm achieved over 80% accuracy with a sound label dataset, which significantly outperforms WSD with individual measures. We also explore the use of human judgments of evocation between members of concept pairs, in the label disambiguation task. Results show that evocation achieves similar performance to most of the existing relatedness measures. 
This paper describes the results of our experiments in building speaker-adaptive recognizers for talkers with spastic dysarthria. We study two modiﬁcations – (a) MAP adaptation of speaker-independent systems trained on normal speech and, (b) using a transition probability matrix that is a linear interpolation between fully ergodic and (exclusively) leftto-right structures, for both speaker-dependent and speaker-adapted systems. The experiments indicate that (1) for speaker-dependent systems, left-to-right HMMs have lower word error rate than transition-interpolated HMMs, (2) adapting all parameters other than transition probabilities results in the highest recognition accuracy compared to adapting any subset of these parameters or adapting all parameters including transition probabilities, (3) performing both transition-interpolation and adaptation gives higher word error rate than performing adaptation alone and, (4) dysarthria severity is not a sufﬁcient indicator of the relative performance of speakerdependent and speaker-adapted systems.  Automatic speech recognition (ASR) systems generally assume that the speech signal is a realisation of some message encoded as a sequence of one or more symbols. To effect the reverse operation of recognising the underlying symbol sequence given a spoken utterance, the continuous speech waveform is ﬁrst converted to a sequence of equally spaced discrete parameter vectors. The role of the recogniser is to effect a mapping between sequences of speech vectors and the wanted underlying symbol sequences. Most speech recognizers today are based on the hidden Markov model (HMM) paradigm: it is assumed that the sequence of observed speech vectors is generated by a Markov model as shown in Fig. 1. A Markov model is a ﬁnite state machine which changes state once every time unit and each time t that a state j is entered, a speech vector ot is generated from the probability density bj(ot) which  Hidden  Markov  a22  a33  a44  Model  a12  a23  a34  a45  
Modern automatic speech recognition is ineffective at understanding relatively unintelligible speech caused by neuro-motor disabilities collectively called dysarthria. Since dysarthria is primarily an articulatory phenomenon, we are collecting a database of vocal tract measurements during speech of individuals with cerebral palsy. In this paper, we demonstrate that articulatory knowledge can remove ambiguities in the acoustics of dysarthric speakers by reducing entropy relatively by 18.3%, on average. Furthermore, we demonstrate that dysarthric speech is more precisely portrayed as a noisy-channel distortion of an abstract representation of articulatory goals, rather than as a distortion of non-dysarthric speech. We discuss what implications these results have for our ongoing development of speech systems for dysarthric speakers. 
American Sign Language (ASL) generation software can improve the accessibility of information and services for deaf individuals with low English literacy. The understandability of current ASL systems is limited; they have been constructed without the benefit of annotated ASL corpora that encode detailed human movement. We discuss how linguistic challenges in ASL generation can be addressed in a data-driven manner, and we describe our current work on collecting a motion-capture corpus. To evaluate the quality of our motion-capture configuration, calibration, and recording protocol, we conducted an evaluation study with native ASL signers. 
This paper presents factors in designing a system for automatically skimming text documents in response to a question. The system will take a potentially complex question and a single document and return a Web page containing links to text related to the question. The goal is that these text areas be those that visual readers would spend the most time on when skimming for the answer to a question. To identify these areas, we had visual readers skim for an answer to a complex question while being tracked by an eye-tracking system. Analysis of these results indicates that text with semantic connections to the question are of interest, but these connections are much looser than can be identified with traditional Question-Answering or Information Retrieval techniques. Instead, we are expanding traditional semantic treatments by using a Web search. The goal of this system is to give nonvisual readers information similar to what visual readers get when skimming through a document in response to a question. 
 or entailment) (Harabagiu and Hickl, 2006) trans-  We present an exploration of generative modeling for the question answering (QA) task to rank candidate passages. We investigate Latent Dirichlet Allocation (LDA) models to obtain ranking scores based on a novel similarity measure between a natural language question posed by the user and a candidate passage. We construct two models each one introducing deeper evaluations on latent characteristics of passages together with given question. With the new representation of topical structures on QA datasets, using a limited amount of world knowledge, we show improvements on performance of a QA ranking system.  lation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised, e.g., they require hand engineered features; or cas-  
In this paper, we propose a multiwordenhanced author topic model that clusters authors with similar interests and expertise, and apply it to an information retrieval system that returns a ranked list of authors related to a keyword. For example, we can retrieve Eugene Charniak via search for statistical parsing. The existing works on author topic modeling assume a “bag-of-words” representation. However, many semantic atomic concepts are represented by multiwords in text documents. This paper presents a pre-computation step as a way to discover these multiwords in the corpus automatically and tags them in the termdocument matrix. The key advantage of this method is that it retains the simplicity and the computational efﬁciency of the unigram model. In addition to a qualitative evaluation, we evaluate the results by using the topic models as a component in a search engine. We exhibit improved retrieval scores when the documents are represented via sets of latent topics and authors. 
 Empirical research has demonstrated the ambiva-  Text normalization transforms words into a base form so that terms from common equivalent classes match. Traditionally, information retrieval systems employ stemming techniques to remove derivational afﬁxes. Depluralization, the transformation of plurals into singular forms, is also used as a low-level text normalization technique to preserve more precise lexical semantics of text. Experiment results suggest that the choice of text normalization technique should be made individually on each topic to enhance information retrieval accuracy. This paper proposes a hybrid approach, constructing a query-based selection model to select the appropriate text normalization technique (stemming, depluralization, or not doing any text normalization). The selection model utilized ambiguity properties extracted from queries to train a composite of Support Vector Regression (SVR) models to predict a text normalization technique that yields the highest Mean Average Precision (MAP). Based on our study, such a selection model holds promise in improving retrieval accuracy.  lent effect of stemming on text retrieval performance. Hull (1996) conducted a comprehensive case study on the effects of four stemmer techniques and the removal of plural “s” 1 on retrieval performance. Hull suggested that the adoption of stemming is beneﬁcial but plural removal is as well competitive when the size of documents is small. Prior research (Manning and Schtze, 1999; McNamee et al., 2008) indicated that traditional stemming, though still beneﬁting some queries, would not necessarily enhance the average retrieval performance. In addition, stemming was considered one of the technique failures undermining retrieval performance in the TREC 2004 Robust Track (Voorhees, 2006). Prior research also noted the semantic differences between plurals and singulars. Riloff (1995) indicated that plural and singular nouns are distinct because plural nouns usually pertain to the “general types of incidents,” while singular nouns often pertain to “a speciﬁc incident.” Nevertheless, prior research has not closely examined the effect of the change of the semantics caused by different level of text normalization tech-  niques. In our work, we conducted extensive exper-  
 W hat f ilm introduced Jar Jar Binks?  We investigate a graph-based semi-supervised learning approach for labeling semantic components of questions such as topic, focus, event, etc., for question understanding task. We focus on graph construction to handle learning with dense/sparse graphs and present Relaxed Linear Neighborhoods method, in which each node is linearly constructed from varying sizes of its neighbors based on the density/sparsity of its surrounding. With the new graph representation, we show performance improvements on syntactic and real datasets, primarily due to the use of unlabeled data and relaxed graph construction.  other f ocus event  topic  Semantic Components & Named-Entitiy Types  topic: ’Jar’ (Begin-Topic); ’Jar’ (In-Topic) ;  ’Binks’ (In-Topic)(HUMAN:Individual)  focus: ’ﬁlm’ (Begin-Focus) (DESCRIPTION:Deﬁnition)  action / event: ’introduced’ (Begin-Event)  expected answer-type: ENTITY:creative  Table 1: Question Analysis - Semantic Components of a sample question from TREC QA task.  that instances connected by large weights are given similar labels. Such methods can perform well when  
Bloggers, professional reviewers, and consumers continuously create opinion–rich web reviews about products and services, with the result that textual reviews are now abundant on the web and often convey a useful overall rating (number of stars). However, an overall rating cannot express the multiple or conﬂicting opinions that might be contained in the text, or explicitly rate the different aspects of the evaluated entity. This work addresses the task of automatically predicting ratings, for given aspects of a textual review, by assigning a numerical score to each evaluated aspect in the reviews. We handle this task as both a regression and a classiﬁcation modeling problem and explore several combinations of syntactic and semantic features. Our results suggest that classiﬁcation techniques perform better than ranking modeling when handling evaluative text. 
As the web evolves, increasing quantities of structured information is embedded in web pages in disparate formats. For example, a digital camera’s description may include its price and megapixels whereas a professor’s description may include her name, university, and research interests. Both types of pages may include additional ambiguous information. General search engines (GSEs) do not support queries over these types of data because they ignore the web document semantics. Conversely, describing requisite semantics through structured queries into databases populated by information extraction (IE) techniques are expensive and not easily adaptable to new domains. This paper describes a methodology for rapidly developing search engines capable of answering structured queries over unstructured corpora by utilizing machine learning to avoid explicit IE. We empirically show that with minimum additional human effort, our system outperforms a GSE with respect to structured queries with clear object semantics. 
MedEval is a Swedish medical test collection where assessments have been made, not only for topical relevance, but also for target reader group: Doctors or Patients. The user of the test collection can choose if s/he wishes to search in the Doctors or the Patients scenarios where the topical relevance assessments have been adjusted with consideration to user group, or to search in a scenario which regards only topical relevance. MedEval makes it possible to compare the effectiveness of search terms when it comes to retrieving documents aimed at the different user groups. MedEval is also the ﬁrst medical Swedish test collection. 
Achieving guideline-based targets in patients with diabetes is crucial for improving clinical outcomes and preventing long-term complications. Using electronic heath records (EHRs) to identify high-risk patients for further intervention by screening large populations is limited because many EHRs store clinical information as dictated and transcribed free text notes that are not amenable to statistical analysis. This paper presents the process of extracting elements needed for generating a diabetes report card from free text notes written in English. Numerical measurements, representing lab values and physical examinations results are extracted from free text documents and then stored in a structured database. Extracting diagnosis information and medication lists are work in progress. The complete dataset for this project is comprised of 81,932 documents from 30,459 patients collected over a period of 5 years. The patient population is considered high risk for diabetes as they have existing cardiovascular complications. Experimental results validate our method, demonstrating high precision (88.8-100%).  
NegEx, a rule-based algorithm that detects negations in English clinical text, was translated into Swedish and evaluated on clinical text written in Swedish. The NegEx algorithm detects negations through the use of trigger phrases, which indicate that a preceding or following concept is negated. A list of English trigger phrases was translated into Swedish, taking grammatical differences between the two languages into account. This translation was evaluated on a set of 436 manually classiﬁed sentences from Swedish health records. The results showed a precision of 70% and a recall of 81% for sentences containing the trigger phrases and a negative predictive value of 96% for sentences not containing any trigger phrases. The precision was signiﬁcantly lower for the Swedish adaptation than published results on the English version, but since many negated propositions were identiﬁed through a limited set of trigger phrases, it could nevertheless be concluded that the same trigger phrase approach is possible in a Swedish context, even though it needs to be further developed. 
We present an approach to analysing automatic speech recognition (ASR) hypotheses for dictated medical reports based on background knowledge. Our application area is prescriptions of medications, which are a frequent source of misrecognitions: In a sample report corpus, we found that about 40% of the active substances or trade names and dosages were recognized incorrectly. In about 25% of these errors, the correct string of words was contained in the word graph. We have built a knowledge base of medications based on information contained in the Uniﬁed Medical Language System (UMLS), consisting of trade names, active substances, strengths and dosages. From this, we generate a variety of linguistic realizations for prescriptions. Whenever an inconsistency in a prescription is encountered on the best path of the word graph, the system searches for alternative paths which contain valid linguistic realizations of prescriptions consistent with the knowledge base. If such a path exists, a new concept edge with a better score is added to the word graph, resulting in a higher plausibility for this reading. The concept edge can be used for rescoring the word graph to obtain a new best path. A preliminary evaluation led to encouraging results: in nearly half of the cases where the word graph contained the correct variant, the correction was successful. 
This paper presents ongoing work on application of Information Extraction (IE) technology to domain of Public Health, in a real-world scenario. A central issue in IE is the quality of the results. We present two novel points. First, we distinguish the criteria for quality: the objective criteria that measure correctness of the system’s analysis in traditional terms (F-measure, recall and precision), and, on the other hand, subjective criteria that measure the utility of the results to the end-user. Second, to obtain measures of utility, we build an environment that allows users to interact with the system by rating the analyzed content. We then build and compare several classiﬁers that learn from the user’s responses to predict the relevance scores for new events. We conduct experiments with learning to predict relevance, and discuss the results and their implications for text mining in the domain of Public Health. 
In this paper we present a detailed scheme for annotating medical web pages designed for health care consumers. The annotation is along two axes: first, by reliability or the extent to which the medical information on the page can be trusted, second, by the type of page (patient leaflet, commercial, link, medical article, testimonial, or support). We analyze interrater agreement among three judges for each category. Inter-rater agreement was moderate (0.77 accuracy, 0.62 F-measure, 0.49 Kappa) on the reliability axis and good (0.81 accuracy, 0.72 F-measure, 0.73 Kappa) along the type axis. 
Acronyms are increasingly prevalent in biomedical text, and the task of acronym disambiguation is fundamentally important for biomedical natural language processing systems. Several groups have generated sense inventories of acronym long form expansions from the biomedical literature. Long form sense inventories, however, may contain conceptually redundant expansions that negatively affect their quality. Our approach to improving sense inventories consists of mapping long form expansions to concepts in the Unified Medical Language System (UMLS) with subsequent application of a semantic similarity algorithm based upon conceptual overlap. We evaluated this approach on a reference standard developed for ten acronyms. A total of 119 of 155 (78%) long forms mapped to concepts in the UMLS. Our approach identified synonymous long forms with a sensitivity of 70.2% and a positive predictive value of 96.3%. Although further refinements are needed, this study demonstrates the potential value of using automated techniques to merge synonymous biomedical acronym long forms to improve the quality of biomedical acronym sense inventories. 
We present a comparative study of Finnish and Swedish free-text nursing narratives from intensive care. Although the two languages are linguistically very dissimilar, our hypothesis is that there are similarities that are important and interesting from a language technology point of view. This may have implications when building tools to support producing and using health care documentation. We perform a comparative qualitative analysis based on structure and content, as well as a comparative quantitative analysis on Finnish and Swedish Intensive Care Unit (ICU) nursing narratives. Our findings are that ICU nursing narratives in Finland and Sweden have many properties in common, but that many of these are challenging when it comes to developing language technology tools. 
Extracting medication information from clinical records has many potential applications and was the focus of the i2b2 challenge in 2009. We present a hybrid system, comprised of machine learning and rule-based modules, for medication information extraction. With only a handful of template-filling rules, the system’s core is a cascade of statistical classifiers for field detection. It achieved good performance that was comparable to the top systems in the i2b2 challenge, demonstrating that a heavily statistical approach can perform as well or better than systems with many sophisticated rules. The system can easily incorporate additional resources such as medication name lists to further improve performance.  (https://www.i2b2.org/NLP/Medication/), a task we refer to as the i2b2 challenge in this paper. In the past decade, there has been extensive research on information extraction in both the general and biomedical domains (Wellner et al., 2004; Grenager et al., 2005; Poon and Domingos, 2007; Meystre et al, 2008; Rozenfeld and Feldman, 2008). Interestingly, despite the recent prevalence of statistical approaches in most NLP tasks (including information extraction), most of the systems developed for the i2b2 challenge were rulebased. In this paper we present our hybrid system, whose core is a cascade of statistical classifiers that identify medication fields such as medication names and dosages. The fields are then assembled to form medication entries. While our system did not participate in the i2b2 challenge (as we were part of the organizing team), it achieved good results that matched the top i2b2 systems. 2 The i2b2 Challenge  
In this pilot study we define and apply a methodology for building an event extraction system for the Swedish scientific medical and clinical language. Our aim is to find and describe linguistic expressions which refer to medical events, such as events related to diseases, symptoms and drug effects. In order to achieve this goal we have initiated actions that aim to extend and refine parts of the ongoing compilation of the Swedish FrameNet++ (SFN++). SFN++, as its English original predecessor, is grounded in Frame Semantics which provides a sound theoretical ground for modeling and linking linguistic structures encountered in general language and in specific domains (after specialization). Using such resource we have started to manually annotate domain texts for enriching SFN++ with authentic samples and for providing training data for automated event extraction techniques. 
Current metrics for de-identification are based on information extraction metrics, and do not address the real-world questions “how good are current systems”, and “how good do they need to be”. Metrics are needed that quantify both the risk of re-identification and information preservation. We review the challenges in de-identifying clinical texts and the current metrics for assessing clinical de-identification systems. We then introduce three areas to explore that can lead to metrics that quantify reidentification risk and information preservation. 
 tems implemented are proprietary and highly customized when used by larger care institutions.  An overview is provided of six information visualization systems designed specifically for gaining an overview of electronic health records (EHR). The systems discussed all make use of timelines: Lifelines, Lifelines2, KNAVE II, CLEF Visual Navigator, Timeline, and AsbruView. With the exception of Lifelines2, the main user groups targeted are physicians involved in direct patient care. Little attention has been paid towards supporting true secondary use of EHR contents, for activities such as assessing quality of care, patient health and safety monitoring, and clinical trial recruitment. Future work on such systems needs to address the complexity of EHR data, missing and incomplete information, and difficulties in displaying data with differing levels of granularity. 
This paper describes the architecture of an encoding system which aim is to be implemented as a coding help at the Cliniques universtaires Saint-Luc, a hospital in Brussels. This paper focuses on machine learning methods, more specifically, on the appropriate set of attributes to be chosen in order to optimize the results of these methods. A series of four experiments was conducted on a baseline method: Naïve Bayes with varying sets of attributes. These experiments showed that a first step consisting in the extraction of information to be coded (such as diseases, procedures, aggravating factors, etc.) is essential. It also demonstrated the importance of stemming features. Restraining the classes to categories resulted in a recall of 81.1 %. 
We explore the use of conditional random ﬁelds (CRFs) to automatically extract important metadata from clinical research articles. These metadata ﬁelds include formulaic metadata about the authors, extracted from the title page, as well as free text ﬁelds concerning the study’s critical parameters, such as longitudinal variables and medical intervention methods, extracted from the body text of the article. Extracting such information can help both readers conduct deep semantic search of articles and policy makers and sociologists track macro level trends in research. Preliminary results show an acceptable level of performance for formulaic metadata and a high precision for those found in the free text. 
We describe a readability assessment approach to support the process of text simplification for poor literacy readers. Given an input text, the goal is to predict its readability level, which corresponds to the literacy level that is expected from the target reader: rudimentary, basic or advanced. We complement features traditionally used for readability assessment with a number of new features, and experiment with alternative ways to model this problem using machine learning methods, namely classification, regression and ranking. The best resulting model is embedded in an authoring tool for Text Simplification. 
In this paper, we present a corrected and errortagged corpus of essays written by non-native speakers of English. The corpus contains 63000 words and includes data by learners of English of nine ﬁrst language backgrounds. The annotation was performed at the sentence level and involved correcting all errors in the sentence. Error classiﬁcation includes mistakes in preposition and article usage, errors in grammar, word order, and word choice. We show an analysis of errors in the annotated corpus by error categories and ﬁrst language backgrounds, as well as inter-annotator agreement on the task. We also describe a computer program that was developed to facilitate and standardize the annotation procedure for the task. The program allows for the annotation of various types of mistakes and was used in the annotation of the corpus. 
We investigate the use of web search queries for detecting errors in non-native writing. Distinguishing a correct sequence of words from a sequence with a learner error is a baseline task that any error detection and correction system needs to address. Using a large corpus of error-annotated learner data, we investigate whether web search result counts can be used to distinguish correct from incorrect usage. In this investigation, we compare a variety of query formulation strategies and a number of web resources, including two major search engine APIs and a large web-based n-gram corpus. 
 especially problematic for this ﬁeld of research since  some ESL errors, such as preposition usage, occur at  In this paper we present results from two pilot studies which show that using the Amazon Mechanical Turk for preposition error annotation is as effective as using trained raters, but at a fraction of the time and cost. Based on these results, we propose a new evaluation method which makes it feasible to compare two error detection systems tested on different  error rates as low as 10%. This means that to collect a corpus of 1,000 preposition errors, an annotator would have to check over 10,000 prepositions.1 (Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. For example, trained  learner data sets.  raters typically annotate preposition errors with a  kappa around 0.60. This low rater reliability has  
Computer generation of cloze tasks still falls short of full automation; most current systems are used by teachers as authoring aids. Improved methods to estimate cloze quality are needed for full automation. We investigated lexical reading difficulty as a novel automatic estimator of cloze quality, to which cooccurrence frequency of words was compared as an alternate estimator. Rather than relying on expert evaluation of cloze quality, we submitted open cloze tasks to workers on Amazon Mechanical Turk (AMT) and discuss ways to measure of the results of these tasks. Results show one statistically significant correlation between the above measures and estimators, which was lexical co-occurrence and Cloze Easiness. Reading difficulty was not found to correlate significantly. We gave subsets of cloze sentences to an English teacher as a gold standard. Sentences selected by co-occurrence and Cloze Easiness were ranked most highly, corroborating the evidence from AMT. 
 automatically by computer, and often they are—  perhaps even right after each student uploads their  We describe Prograder, a software package for automatic checking of requirements for programming homework assignments. Prograder lets instructors specify requirements in natural language as well as explains grading results to students in natural language. It does so using a grammar that generates as well as parses to translate between a small fragment of English and a ﬁrst-order logical speciﬁcation language that can be executed directly in Python. This execution embodies multiple semantics—both to check the requirement and to search for evidence that proves or disproves the requirement. Such a checker needs to interpret and generate sentences containing quantiﬁers and  work so that the student can revise their work using the immediate feedback. This common workﬂow is wanting in two aspects. First, the requirements are both speciﬁed to the students in English and coded into a testing harness by the course staff. Keeping the two versions is a hassle that involves much boilerplate text as well as boilerplate code. Second, students rightfully demand comprehensible explanations when their work is rejected by the requirement tester. It is tricky to code up a tester that produces error messages neither too terse nor too verbose, when one student might forget to comment just one ﬁle and another might not know  negation. To handle quantiﬁer and negation scope, we systematically simulate continuation grammars using record structures in the Grammatical Framework.  the lexical syntax of comments at all. A natural approach to improve this workﬂow, then, is to specify the requirements in a formal language and to implement an interpreter for the lan-  guage that produces explanations. Because many  
A central challenge for tutorial dialogue systems is selecting an appropriate move given the dialogue context. Corpus-based approaches to creating tutorial dialogue management models may facilitate more flexible and rapid development of tutorial dialogue systems and may increase the effectiveness of these systems by allowing data-driven adaptation to learning contexts and to individual learners. This paper presents a family of models, including first-order Markov, hidden Markov, and hierarchical hidden Markov models, for predicting tutor dialogue acts within a corpus. This work takes a step toward fully data-driven tutorial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling. 
 tendorf et al., 2008). The detected structural events  We investigated using structural events, e.g., clause and disﬂuency structure, from transcriptions of spontaneous non-native speech, to compute features for measuring speaking proﬁciency. Using a set of transcribed audio ﬁles collected from the TOEFL Practice Test Online (TPO), we conducted a sophisticated annotation of structural events, including clause boundaries and types, as well as disﬂuencies. Based on words and the annotated structural events, we extracted features related to syntactic complexity, e.g., the mean length of clause (MLC) and dependent clause frequency (DEPC), and a feature related to disﬂuencies, the interruption point frequency per clause (IPC). Among these features, the IPC shows the highest correlation with holistic scores (r = −0.344). Furthermore, we increased the correlation with human scores by normalizing IPC by (1) MLC (r = −0.386), (2) DEPC (r = −0.429), and (3) both (r = −0.462). In this research, the features derived from structural events of speech transcriptions are found to predict holistic scores measuring speaking proﬁciency. This suggests that structural events estimated on speech word strings provide a potential way for assessing nonnative speech.  have been successfully used in many natural language processing (NLP) applications (Ostendorf et al., 2008). However, the structural events in speech data haven’t been largely utilized by the research on using automatic speech recognition (ASR) technology to assess speech proﬁciency (Neumeyer et al., 2000; Zechner et al., 2007), which mainly used cues derived at the word level, such as timing information of spoken words. The information beyond the word level, e.g., clause/sentence structure of utterances and disﬂuency structure, has not been or is poorly represented. For example, in Zechner et al. (2007), only special words for ﬁlled pauses such as um and uh were obtained from ASR results to represent disﬂuencies. Given the successful usage of structural events on a wide range of NLP applications and the fact that the usage of these events is missing in the automatic speech assessment research, a research question emerges: Can we use structural events of spontaneous speech to assess non-native speech proﬁciency? We will address this question in this paper. The paper is organized as follows: Section 2 reviews  
This paper explores an issue of redundant errors reported while automatically scoring English learners’ sentences. We use a human-computer collaboration approach to eliminate redundant errors. The first step is to automatically select candidate redundant errors using PMI and RFC. Since those errors are detected with different IDs although they represent the same error, the candidacy cannot be confirmed automatically. The errors are then handed over to human experts to determine the candidacy. The final candidates are provided to the system and trained with a decision tree. With those redundant errors eliminated, the system accuracy has been improved. 
Automatic tools for analyzing student online discussions are highly desirable for providing better assistance and promoting discussion participation. This paper presents an approach for identifying student discussions with unresolved issues or unanswered questions. In order to handle highly incoherent data, we perform several data processing steps. We then apply a two-phase classification algorithm. First, we classify “speech acts” of individual messages to identify the roles that the messages play, such as question, issue raising, and answers. We then use the resulting speech acts as features for classifying discussion threads with unanswered questions or unresolved issues. We performed a preliminary analysis of the classifiers and the system shows an average F score of 0.76 in discussion thread classification. 
Our work addresses the problem of predicting whether an essay is off-topic to a given prompt or question without any previouslyseen essays as training data. Prior work has used similarity between essay vocabulary and prompt words to estimate the degree of ontopic content. In our corpus of opinion essays, prompts are very short, and using similarity with such prompts to detect off-topic essays yields error rates of about 10%. We propose two methods to enable better comparison of prompt and essay text. We automatically expand short prompts before comparison, with words likely to appear in an essay to that prompt. We also apply spelling correction to the essay texts. Both methods reduce the error rates during off-topic essay detection and turn out to be complementary, leading to even better performance when used in unison. 
This paper explores the close relationship between question answering and machine reading, and how the active use of reasoning to answer (and in the process, disambiguate) questions can also be applied to reading declarative texts, where a substantial proportion of the text’s contents is already known to (represented in) the system. In question answering, a question may be ambiguous, and it may only be in the process of trying to answer it that the "right" way to disambiguate it becomes apparent. Similarly in machine reading, a text may be ambiguous, and may require some process to relate it to what is already known. Our conjecture in this paper is that these two processes are similar, and that we can modify a question answering tool to help "read" new text that augments existing system knowledge. Specifically, interpreting a new text T can be recast as trying to answer, or partially answer, the question "Is it true that T?", resulting in both appropriate disambiguation and connection of T to existing knowledge. Some preliminary investigation suggests this might be useful for proposing knowledge base extensions, extracted from text, to a knowledge engineer. 
 multiple alternatives, thereby creating multiple in-  terpretation paths (see (b) in ﬁg 1). Then, the system  We previously proposed a packed graphical representation to succinctly represent a huge number of alternative semantic representations of a given sentence. We also showed that this representation could improve text interpretation accuracy considerably because the system could postpone resolving ambiguity until more evidence accumulates. This paper discusses our plan to build an end-to-end text reading system based on our packed representation.  might choose the best interpretation at the last step of the pipeline. However, this approach is intractable due to the combinatorial explosion in the number of interpretation paths. In previous work (Kim et al., 2010), we proposed an alternative approach in which each component passes forward multiple interpretations which are compressed into an intensional representation that we call a packed graphical (PG) representation (see (c) in ﬁg. 1). Our experiment showed that the  approach could improve the interpretation accuracy  
Texts are replete with gaps, information omitted since authors assume a certain amount of background knowledge. We describe the kind of information (the formalism and methods to derive the content) useful for automated filling of such gaps. We describe a stepwise procedure with a detailed example. 
 a way that integrates background knowledge and in-  ference, and thus are doing the relation detection  We present a technique for reading sentences and producing sets of hypothetical relations that the sentence may be expressing. The technique uses large amounts of instance-level background knowledge about the relations in order to gather statistics on the various ways the relation may be expressed in language, and was inspired by the observation that half of the linguistic forms used to express relations occur very infrequently and are simply not considered by systems that use too few seed examples. Some very early experiments are presented that show promising results.  to better integrate text with pre-existing knowledge, however that should not (and does not) prevent us from using what knowledge we have to inﬂuence that integration along the way. 2 Background The most obvious points of interaction between NLP and KR systems are named entity tagging and other forms of type instance extraction. The second major point of interaction is relation extraction, and while there are many kinds of relations that may be detected (e.g. syntactic relations such as modi-  ﬁers and verb subject/object, equivalence relations  
This paper presents preliminary work to extract script-like structures, called events and event sets, from collections of web documents. Our approach, contrary to existing methods, is topic-driven in the sense that event sets are extracted for a speciﬁed topic. We introduce an iterative system architecture and present methods to reduce noise problems with web corpora. Preliminary results show that LSA-based event relatedness yields better event sets from web corpora than previous methods.  chains (Chambers and Jurafsky, 2008), describe typical sequences of events in a particular context. Given the number of potential scripts, their development by hand becomes a resource intensive process. In the past, some work has been devoted to automatically construct script-like structures from compiled corpora (Fujiki et al., 2003) (Chambers and Jurafsky, 2008). Such approaches, however, only produce scripts that are directly related to the topics represented in such corpora. Therefore, newspaper corpora (e.g., the Reuters Corpus) are likely to contain scripts relating to government, crime and ﬁnancials, but neglect other subject areas. We present a system  that extracts scripts from the web and removes the  
 be supported by knowledge mined from a large cor-  We present a method of extracting opendomain commonsense knowledge by applying discourse parsing to a large corpus of personal stories written by Internet authors. We demonstrate the use of a linear-time, joint syntax/discourse dependency parser for this purpose, and we show how the extracted discourse relations can be used to generate opendomain textual inferences. Our evaluations of the discourse parser and inference models show some success, but also identify a number of interesting directions for future work.  pus of personal stories written by Internet weblog authors.1 Gordon and Swanson (2008) identiﬁed three primary obstacles to such an approach. First, stories must be distinguished from other weblog content (e.g., lists, recipes, and reviews). Second, stories must be analyzed in order to extract the implicit commonsense knowledge that they contain. Third, inference mechanisms must be developed that use the extracted knowledge to perform the core envisionment tasks listed above. In the current paper, we present an approach to open-domain commonsense inference that addresses  
 We are interested in the best possible technique  Open Information Extraction is a recent paradigm for machine reading from arbitrary text. In contrast to existing techniques, which have used only shallow syntactic features, we investigate the use of semantic features (semantic roles) for the task of Open IE. We compare TEXTRUNNER (Banko et al., 2007), a state of the art open extractor, with our novel extractor SRL-IE, which is based on UIUC’s SRL system (Punyakanok et al., 2008). We ﬁnd that SRL-IE is robust to noisy heterogeneous Web data and outperforms TEXTRUNNER on extraction quality. On the other hand, TEXTRUNNER performs over 2 orders of magnitude faster and achieves good precision in high locality and high redundancy extractions. These observations enable the construction of hybrid extractors that output higher quality results than TEXTRUNNER and similar quality as SRL-IE in much less time.  for Open IE. The TEXTRUNNER Open IE system (Banko and Etzioni, 2008) employs only shallow syntactic features in the extraction process. Avoiding the expensive processing of deep syntactic analysis allowed TEXTRUNNER to process at Web scale. In this paper, we explore the beneﬁts of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. SRL is a popular NLP task that has seen signiﬁcant progress over the last few years. The advent of hand-constructed semantic resources such as Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) have resulted in semantic role labelers achieving high in-domain precisions. Our ﬁrst observation is that semantically labeled arguments in a sentence almost always correspond to the arguments in Open IE extractions. Similarly, the verbs often match up with Open IE relations.  
In this paper, we present empirical results on the challenge of learning to read. That is, given a handful of examples of the concepts and relations in an ontology and a large corpus, the system should learn to map from text to the concepts/relations of the ontology. In this paper, we report contrastive experiments on the recall, precision, and F-measure (F) of the mapping in the following conditions: (1) employing word-based patterns, employing semantic structure, and combining the two; and (2) fully automatic learning versus allowing minimal questions of a human informant. 
In this paper, we consider the problem of inductively learning rules from speciﬁc facts extracted from texts. This problem is challenging due to two reasons. First, natural texts are radically incomplete since there are always too many facts to mention. Second, natural texts are systematically biased towards novelty and surprise, which presents an unrepresentative sample to the learner. Our solutions to these two problems are based on building a generative observation model of what is mentioned and what is extracted given what is true. We ﬁrst present a Multiple-predicate Bootstrapping approach that consists of iteratively learning if-then rules based on an implicit observation model and then imputing new facts implied by the learned rules. Second, we present an iterative ensemble colearning approach, where multiple decisiontrees are learned from bootstrap samples of the incomplete training data, and facts are imputed based on weighted majority.  which is on the web accessible to automatic processing. There are at least three different ways in which this can be done. First, factual knowledge on the web can be extracted as formal relations or tuples of a data base. A number of information extraction systems, starting from the WebKb project (Craven et al., 2000), to Whirl (Cohen, 2000) to the TextRunner (Etzioni et al., 2008) project are of this kind. They typically learn patterns or rules that can be applied to text to extract instances of relations. A second possibility is to learn general knowledge, rules, or general processes and procedures by reading natural language descriptions of them, for example, extracting formal descriptions of the rules of the United States Senate or a recipe to make a dessert. A third instance of machine reading is to generalize the facts extracted from the text to learn more general knowledge. For example, one might learn by generalizing from reading the obituaries that most people live less than 90 years, or people tend to live and die in the countries they were born in. In this paper, we consider the problem of learning such general rules by reading about speciﬁc facts.  
We present an unsupervised and unrestricted approach to discovering an infobox like ontology by exploiting the inter-article links within Wikipedia. It discovers new slots and fillers that may not be available in the Wikipedia infoboxes. Our results demonstrate that there are certain types of properties that are evident in the link structure of resources like Wikipedia that can be predicted with high accuracy using little or no linguistic analysis. The discovered properties can be further used to discover a class hierarchy. Our experiments have focused on analyzing people in Wikipedia, but the techniques can be directly applied to other types of entities in text resources that are rich with hyperlinks. 
 machine learning approaches (e.g., components in  Machine reading is a long-standing goal of AI and NLP. In recent years, tremendous progress has been made in developing machine learning approaches for many of its subtasks such as parsing, information extraction, and question answering. However, existing end-to-end solutions typically require substantial amount of human efforts (e.g., labeled data and/or manual engineering), and are not well poised for Web-scale knowledge acquisition. In this paper, we propose a unifying approach for machine reading by bootstrapping from the easiest extractable knowledge and conquering the long tail via a self-supervised learning process. This self-supervision is powered by joint inference based on Markov logic, and is made scalable by leveraging hierarchical structures and coarse-to-ﬁne inference. Researchers at the University of Washington have taken the ﬁrst steps in this direction. Our existing work explores the wide spectrum of this vision and shows its promise.  the traditional NLP pipeline such as POS tagging and syntactic parsing). However, end-to-end solutions are still rare, and existing systems typically require substantial amount of human effort in manual engineering and/or labeling examples. As a result, they often target restricted domains and only extract limited types of knowledge (e.g., a pre-speciﬁed relation). Moreover, many machine reading systems train their knowledge extractors once and do not leverage further learning opportunities such as additional text and interaction with end users. Ideally, a machine reading system should strive to satisfy the following desiderata: End-to-end: the system should input raw text, extract knowledge, and be able to answer questions and support other end tasks; High quality: the system should extract knowledge with high accuracy; Large-scale: the system should acquire knowledge at Web-scale and be open to arbitrary domains,  
Analogy is heavily used in written explanations, particularly in instructional texts. We introduce the concept of analogical dialogue acts (ADAs) which represent the roles utterances play in instructional analogies. We describe a catalog of such acts, based on ideas from structure-mapping theory. We focus on the operations that these acts lead to while understanding instructional texts, using the Structure-Mapping Engine (SME) and dynamic case construction in a computational model. We test this model on a small corpus of instructional analogies, expressed in simplified English, which were understood via a semiautomatic natural language system using analogical dialogue acts. The model enabled a system to answer questions after understanding the analogies that it was not able to answer without them. 
This paper describes a hybrid approach for unsupervised and unrestricted relation discovery between entities using output from linguistic analysis and semantic typing information from a knowledge base. We use Factz (encoded as subject, predicate and object triples) produced by Powerset as a result of linguistic analysis. A particular relation may be expressed in a variety of ways in text and hence have multiple facts associated with it. We present an unsupervised approach for collapsing multiple facts which represent the same kind of semantic relation between entities. Then a label is selected for the relation based on the input facts and entropy based label ranking of context words. Finally, we demonstrate relation discovery between entities at different levels of abstraction by leveraging semantic typing information from a knowledge base. 
The pervasive ambiguity of language allows sentences that diﬀer in just one lexical item to have rather diﬀerent inference patterns. This would be no problem if the diﬀerent lexical items fell into clearly deﬁnable and easy to represent classes. But this is not the case. To draw the correct inferences we need to look how the referents of the lexical items in the sentence (or broader context) interact in the described situation. Given that the knowledge our systems have of the represented situation will typically be incomplete, the classiﬁcations we come up with can only be probabilistic. We illustrate this problem with an investigation of various inference patterns associated with predications of the form ‘Verb from X to Y’, especially ‘go from X to Y’. We characterize the various readings and make an initial proposal about how to create the lexical classes that will allow us to draw the correct inferences in the diﬀerent cases. 
One of the main bottlenecks in natural language processing is the lack of a comprehensive lexicalized relation resource that contains ﬁne grained knowledge on predicates. In this paper, we present PRISMATIC, a large scale lexicalized relation resource that is automatically created over 30 gb of text. Speciﬁcally, we describe what kind of information is collected in PRISMATIC and how it compares with existing lexical resources. Our main focus has been on building the infrastructure and gathering the data. Although we are still in the early stages of applying PRISMATIC to a wide variety of applications, we believe the resource will be of tremendous value for AI researchers, and we discuss some of potential applications in this paper. 
In Construction Grammar, structurally patterned units called constructions are assigned meaning in the same way that words are – via convention rather than composition. That is, rather than piecing semantics together from individual lexical items, Construction Grammar proposes that semantics can be assigned at the construction level. In this paper, we investigate whether a classifier can be taught to identify these constructions and consider the hypothesis that identifying construction types can improve the semantic interpretation of previously unseen predicate uses. Our results show that not only can the constructions be automatically identified with high accuracy, but the classifier also performs just as well with out-of-vocabulary predicates. 
This paper outlines and pilots our approach towards developing an inventory of verb-argument constructions based upon English form, function, and usage. We search a tagged and dependencyparsed BNC (a 100-million word corpus of English) for Verb-Argument Constructions (VACs) including those previously identified in the pattern grammar resulting from the COBUILD project. This generates (1) a list of verb types that occupy each construction. We next tally the frequency profiles of these verbs to produce (2) a frequency ranked type-token distribution for these verbs, and we determine the degree to which this is Zipfian. Since some verbs are faithful to one construction while others are more promiscuous, we next produce (3) a contingency-weighted list reflecting their statistical association. To test whether each of these measures is a step towards increasing the learnability of VACs as categories, following principles of associative learning, we examine 20 verbs from each distribution. Here we explore whether there is an increase in the semantic cohesion of the verbs occupying each construction using semantic similarity measures. From inspection, this seems to be so. We are developing measures of this using network measures of clustering in the verb-space defined by WordNet and Roget’s Thesaurus. 
In this paper we investigate the Presentational Relative Clause (PRC) construction. In both the linguistic and NLP literature, relative clauses have been considered to contain background information that is not directly relevant or highly useful in semantic analysis. In text summarization in particular, the information contained in the relative clauses is often removed, being viewed as non-central content to the topic or discourse. We discuss the importance of distinguishing the PRC construction from other relative clause types. We show that in the PRC, the relative clause, rather than the main clause, contains the assertion of the utterance. Based on linguistic analysis, we suggest informative features that may be used in automatic extraction of PRC constructions. We believe that identifying this construction will be useful in discriminating central information from peripheral. 
We describe and motivate the design of a lexico-grammatical knowledgebase called StringNet and illustrate its significance for research into constructional phenomena in English. StringNet consists of a massive archive of what we call hybrid n-grams. Unlike traditional n-grams, hybrid n-grams can consist of any co-occurring combination of POS tags, lexemes, and specific word forms. Further, we detect and represent superordinate and subordinate relations among hybrid n-grams by cross-indexing, allowing the navigation of StringNet through these hierarchies, from specific fixed expressions (“It’s the thought that counts”) up to their hosting protoconstructions (e.g. the It Cleft construction: “it’s the [noun] that [verb]”). StringNet supports discovery of grammatical dependencies (e.g., subject-verb agreement) in noncanonical configurations as well as lexical dependencies (e.g., adjective/noun collocations specific to families of constructions). 
This paper illustrates a way of using paraphrasal interpretation of English nominal compound for translating them into Hindi. Input Nominal compound is ﬁrst paraphrased automatically with the 8 prepositions as proposed by Lauer (1995) for the task. English prepositions have one-to-one mapping to post-position in Hindi. The English paraphrases are then translated into Hindi using the mapping schema. We have got an accuracy of 71% over a set of gold data of 250 Nominal Compound. The translation-strategy is motivated by the following observation: It is only 50% of the cases that English nominal compound is translated into nominal compound in Hindi. In other cases, they are translated into varied syntactic constructs. Among them the most frequent construction type is “Modiﬁer + Postposition + Head”. The translation module also attempts to determine when a compound is translated using paraphrase and when it is translated into a Nominal compound. 
In this research, we use machine learning techniques to provide solutions for descriptive linguists in the domain of language standardization. With regard to the personal name construction in Afrikaans, we perform function learning from word pairs using the Default&Refine algorithm. We demonstrate how the extracted rules can be used to identify irregularities in previously standardized constructions and to predict new forms of unseen words. In addition, we define a generic, automated process that allows us to extract constructional schemas and present these visually as categorization networks, similar to what is often being used in Cognitive Grammar. We conclude that computational modeling of constructions can contribute to new descriptive linguistic insights, and to practical language solutions. 
This paper explores the task of building an accurate prepositional phrase attachment corpus for new genres while avoiding a large investment in terms of time and money by crowdsourcing judgments. We develop and present a system to extract prepositional phrases and their potential attachments from ungrammatical and informal sentences and pose the subsequent disambiguation tasks as multiple choice questions to workers from Amazon’s Mechanical Turk service. Our analysis shows that this two-step approach is capable of producing reliable annotations on informal and potentially noisy blog text, and this semi-automated strategy holds promise for similar annotation projects in new genres. 
Vocabulary tutors need word sense disambiguation (WSD) in order to provide exercises and assessments that match the sense of words being taught. Using expert annotators to build a WSD training set for all the words supported would be too expensive. Crowdsourcing that task seems to be a good solution. However, a first required step is to define what the possible sense labels to assign to word occurrence are. This can be viewed as a clustering task on dictionary definitions. This paper evaluates the possibility of using Amazon Mechanical Turk (MTurk) to carry out that prerequisite step to WSD. We propose two different approaches to using a crowd to accomplish clustering: one where the worker has a global view of the task, and one where only a local view is available. We discuss how we can aggregate multiple workers‟ clusters together, as well as pros and cons of our two approaches. We show that either approach has an interannotator agreement with experts that corresponds to the agreement between experts, and so using MTurk to cluster dictionary definitions appears to be a reliable approach. 
Word alignment is an important preprocessing step for machine translation. The project aims at incorporating manual alignments from Amazon Mechanical Turk (MTurk) to help improve word alignment quality. As a global crowdsourcing service, MTurk can provide ﬂexible and abundant labor force and therefore reduce the cost of obtaining labels. An easyto-use interface is developed to simplify the labeling process. We compare the alignment results by Turkers to that by experts, and incorporate the alignments in a semi-supervised word alignment tool to improve the quality of the labels. We also compared two pricing strategies for word alignment task. Experimental results show high precision of the alignments provided by Turkers and the semi-supervised approach achieved 0.5% absolute reduction on alignment error rate. 
We use Amazon Mechanical Turk to rate computer-generated reading comprehension questions about Wikipedia articles. Such application-speciﬁc ratings can be used to train statistical rankers to improve systems’ ﬁnal output, or to evaluate technologies that generate natural language. We discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through Amazon Mechanical Turk, and show evidence that these ratings can be used to improve question generation. 
 lel text translations (Callison-Burch, 2009) demonstrated that Turkers can provide useful free-form an-  Mechanical Turk is useful for generating complex speech resources like conversational speech transcription. In this work, we explore the next step of eliciting narrations of Wikipedia articles to improve accessibility for low-literacy users. This task proves a useful test-bed to implement qualitative vetting of workers based on difﬁcult to deﬁne metrics like narrative quality. Working with the Mechanical Turk API, we collected sample narrations, had other Turkers rate these samples and then granted access to full narration HITs depending on aggregate quality. While narrating full articles proved too onerous a task to be viable, using other Turkers to perform vetting was very successful. Elicitation is possible on Mechanical Turk, but it should conform to suggested best practices of simple tasks that can be completed in a streamlined workﬂow.  notation. In this paper, we extend open ended collection even further by eliciting narrations of English Wikipedia articles. To vet prospective narrators, we use qualitative qualiﬁcations by aggregating the opinions of other Turkers on narrative style, thus avoiding quantiﬁcation of qualitative tasks. The Spoken Wikipedia Project1 aims to increase the accessibility of Wikipedia by recording articles for use by blind or illiterate users. Since 2008, over 1600 English articles covering topics from art to technology have been narrated by volunteers. The charitable nature of this work should provide additional incentive for Turkers to complete this task. We use Wikipedia narrations as an initial proof-ofconcept for other more challenging elicitation tasks such as spontaneous or conversational speech.  While previous work used other Turkers in  
This study investigates the use of Amazon Mechanical Turk for the transcription of nonnative speech. Multiple transcriptions were obtained from several distinct MTurk workers and were combined to produce merged transcriptions that had higher levels of agreement with a gold standard transcription than the individual transcriptions. Three different methods for merging transcriptions were compared across two types of responses (spontaneous and read-aloud). The results show that the merged MTurk transcriptions are as accurate as an individual expert transcriber for the readaloud responses, and are only slightly less accurate for the spontaneous responses. 
This paper discusses a machine translation evaluation task conducted using Amazon Mechanical Turk. We present a translation adequacy assessment task for untrained Arabicspeaking annotators and discuss several techniques for normalizing the resulting data. We present a novel 2-stage normalization technique shown to have the best performance on this task and further discuss the results of all techniques and the usability of the resulting adequacy scores.  data normalization techniques must be employed to ensure that the data collected is usable. This paper describes a MT evaluation task for translations of English into Arabic conducted using MTurk and compares several data normalization techniques. A novel 2-stage normalization technique is demonstrated to produce the highest agreement between Turkers and experts while retaining enough judgments to provide a robust tuning set for automatic evaluation metrics. 2 Data Set  
Corpus based approaches to machine translation (MT) rely on the availability of parallel corpora. In this paper we explore the effectiveness of Mechanical Turk for creating parallel corpora. We explore the task of sentence translation, both into and out of a language. We also perform preliminary experiments for the task of phrase translation, where ambiguous phrases are provided to the turker for translation in isolation and in the context of the sentence it originated from. 
This paper describes a semi-automatic paraphrasing task for English-Arabic machine translation conducted using Amazon Mechanical Turk. The method for automatically extracting paraphrases is described, as are several human judgment tasks completed by Turkers. An ideal task type, revised specifically to address feedback from Turkers, is shown to be sophisticated enough to identify and ﬁlter problem Turkers while remaining simple enough for non-experts to complete. The results of this task are discussed along with the viability of using this data to combat data sparsity in MT. 
Amazon's Mechanical Turk service has been successfully applied to many natural language processing tasks. However, the task of named entity recognition presents unique challenges. In a large annotation task involving over 20,000 emails, we demonstrate that a compet­ itive bonus system and inter­annotator agree­ ment can be used to improve the quality of named entity annotations from Mechanical Turk. We also build several statistical named entity recognition models trained with these annotations, which compare favorably to sim­ ilar models trained on expert annotations. 
We describe our experience using both Amazon Mechanical Turk (MTurk) and CrowdFlower to collect simple named entity annotations for Twitter status updates. Unlike most genres that have traditionally been the focus of named entity experiments, Twitter is far more informal and abbreviated. The collected annotations and annotation techniques will provide a ﬁrst step towards the full study of named entity recognition in domains like Facebook and Twitter. We also brieﬂy describe how to use MTurk to collect judgements on the quality of “word clouds.” 
This paper presents findings on using crowdsourcing via Amazon Mechanical Turk (MTurk) to obtain Arabic nicknames as a contribution to exiting Named Entity (NE) lexicons. It demonstrates a strategy for increasing MTurk participation from Arab countries. The researchers validate the nicknames using experts, MTurk workers, and Google search and then compare them against the Database of Arabic Names (DAN). Additionally, the experiment looks at the effect of pay rate on speed of nickname collection and documents an advertising effect where MTurk workers respond to existing work batches, called Human Intelligence Tasks (HITs), more quickly once similar higher paying HITs are posted. 
We propose a framework for improving output quality of machine translation systems, by operating on the level of grammar rule features. Our framework aims to give a boost to grammar rules that appear in the derivations of translation candidates that are deemed to be of good quality, hence making those rules more preferable by the system. To that end, we ask human annotators on Amazon Mechanical Turk to compare translation candidates, and then interpret their preferences of one candidate over another as an implicit preference for one derivation over another, and therefore as an implicit preference for one or more grammar rules. Our framework also allows us to generalize these preferences to grammar rules corresponding to a previously unseen test set, namely rules for which no candidates have been judged. 
Due to its complexity, meeting speech provides a challenge for both transcription and annotation. While Amazon’s Mechanical Turk (MTurk) has been shown to produce good results for some types of speech, its suitability for transcription and annotation of spontaneous speech has not been established. We find that MTurk can be used to produce highquality transcription and describe two techniques for doing so (voting and corrective). We also show that using a similar approach, high quality annotations useful for summarization systems can also be produced. In both cases, accuracy is comparable to that obtained using trained personnel. 
In this work we present results from using Amazon’s Mechanical Turk (MTurk) to annotate translation lexicons between English and a large set of less commonly used languages. We generate candidate translations for 100 English words in each of 42 foreign languages using Wikipedia and a lexicon induction framework. We evaluate the MTurk annotations by using positive and negative control candidate translations. Additionally, we evaluate the annotations by adding pairs to our seed dictionaries, providing a feedback loop into the induction system. MTurk workers are more successful in annotating some languages than others and are not evenly distributed around the world or among the world’s languages. However, in general, we ﬁnd that MTurk is a valuable resource for gathering cheap and simple annotations for most of the languages that we explored, and these annotations provide useful feedback in building a larger, more accurate lexicon. 
One of the major bottlenecks in the development of data-driven AI Systems is the cost of reliable human annotations. The recent advent of several crowdsourcing platforms such as Amazon’s Mechanical Turk, allowing requesters the access to affordable and rapid results of a global workforce, greatly facilitates the creation of massive training data. Most of the available studies on the effectiveness of crowdsourcing report on English data. We use Mechanical Turk annotations to train an Opinion Mining System to classify Spanish consumer comments. We design three different Human Intelligence Task (HIT) strategies and report high inter-annotator agreement between non-experts and expert annotators. We evaluate the advantages/drawbacks of each HIT design and show that, in our case, the use of non-expert annotations is a viable and costeffective alternative to expert annotations. 
We present a compendium of recent and current projects that utilize crowdsourcing technologies for language studies, ﬁnding that the quality is comparable to controlled laboratory experiments, and in some cases superior. While crowdsourcing has primarily been used for annotation in recent language studies, the results here demonstrate that far richer data may be generated in a range of linguistic disciplines from semantics to psycholinguistics. For these, we report a number of successful methods for evaluating data quality in the absence of a ‘correct’ response for any given data point. 
Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Recent studies have found that while there are suggestive connections between topic models and the way humans interpret data, these two often disagree. In this paper, we explore this disagreement from the perspective of the learning process rather than the output. We present a novel task, tag-and-cluster, which asks subjects to simultaneously annotate documents and cluster those annotations. We use these annotations as a novel approach for constructing a topic model, grounded in human interpretations of documents. We demonstrate that these topic models have features which distinguish them from traditional topic models. 
Crowd-sourcing approaches such as Amazon’s Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of linguistic data at a relatively low cost and high speed. However, MTurk offers only limited control over who is allowed to particpate in a particular task. This is particularly problematic for tasks requiring free-form text entry. Unlike multiple-choice tasks there is no correct answer, and therefore control items for which the correct answer is known cannot be used. Furthermore, MTurk has no effective built-in mechanism to guarantee workers are proﬁcient English writers. We describe our experience in creating corpora of images annotated with multiple one-sentence descriptions on MTurk and explore the effectiveness of different quality control strategies for collecting linguistic data using Mechanical MTurk. We ﬁnd that the use of a qualiﬁcation test provides the highest improvement of quality, whereas reﬁning the annotations through follow-up tasks works rather poorly. Using our best setup, we construct two image corpora, totaling more than 40,000 descriptive captions for 9000 images. 
We provide evidence that intrinsic evaluation of summaries using Amazon’s Mechanical Turk is quite difﬁcult. Experiments mirroring evaluation at the Text Analysis Conference’s summarization track show that nonexpert judges are not able to recover system rankings derived from experts. 
This paper considers the linguistic indicators of bias in political text. We used Amazon Mechanical Turk judgments about sentences from American political blogs, asking annotators to indicate whether a sentence showed bias, and if so, in which political direction and through which word tokens. We also asked annotators questions about their own political views. We conducted a preliminary analysis of the data, exploring how different groups perceive bias in different blogs, and showing some lexical indicators strongly associated with perceived bias. 
Efforts to automatically acquire world knowledge from text suffer from the lack of an easy means of evaluating the resulting knowledge. We describe initial experiments using Mechanical Turk to crowdsource evaluation to nonexperts for little cost, resulting in a collection of factoids with associated quality judgements. We describe the method of acquiring usable judgements from the public and the impact of such large-scale evaluation on the task of knowledge acquisition. 
This paper describes our experiments of using Amazon’s Mechanical Turk to generate (counter-)facts from texts for certain namedentities. We give the human annotators a paragraph of text and a highlighted named-entity. They will write down several (counter-)facts about this named-entity in that context. The analysis of the results is performed by comparing the acquired data with the recognizing textual entailment (RTE) challenge dataset. 
Human listeners can almost instantaneously judge whether or not another speaker is part of their speech community. The basis of this judgment is the speaker’s accent. Even though humans judge speech accents with ease, it has been tremendously difficult to automatically evaluate and rate accents in any consistent manner. This paper describes an experiment using the Amazon Mechanical Turk to develop an automatic speech accent rating dataset. 
We investigate human factors involved in designing effective Human Intelligence Tasks (HITs) for Amazon’s Mechanical Turk1. In particular, we assess document relevance to search queries via MTurk in order to evaluate search engine accuracy. Our study varies four human factors and measures resulting experimental outcomes of cost, time, and accuracy of the assessments. While results are largely inconclusive, we identify important obstacles encountered, lessons learned, related work, and interesting ideas for future investigation. Experimental data is also made publicly available for further study by the community2. 
Amazon’s Mechanical Turk (MTurk) service is becoming increasingly popular in Natural Language Processing (NLP) research. In this paper, we report our findings in using MTurk to annotate medical text extracted from clinical trial descriptions with three entity types: medical condition, medication, and laboratory test. We compared MTurk annotations with a gold standard manually created by a domain expert. Based on the good performance results, we conclude that MTurk is a very promising tool for annotating large-scale corpora for biomedical NLP tasks. 
To rapidly port speech applications to new languages one of the most difficult tasks is the initial collection of sufficient speech corpora. State-of-the-art automatic speech recognition systems are typical trained on hundreds of hours of speech data. While pre-existing corpora do exist for major languages, a sufficient amount of quality speech data is not available for most world languages. While previous works have focused on the collection of translations and the transcription of audio via Mechanical-Turk mechanisms, in this paper we introduce two tools which enable the collection of speech data remotely. We then compare the quality of audio collected from paid part-time staff and unsupervised volunteers, and determine that basic user training is critical to obtain usable data. 
Hopper and Thompson (1980) deﬁned a multi-axis theory of transitivity that goes beyond simple syntactic transitivity and captures how much “action” takes place in a sentence. Detecting these features requires a deep understanding of lexical semantics and real-world pragmatics. We propose two general approaches for creating a corpus of sentences labeled with respect to the Hopper-Thompson transitivity schema using Amazon Mechanical Turk. Both approaches assume no existing resources and incorporate all necessary annotation into a single system; this is done to allow for future generalization to other languages. The ﬁrst task attempts to use languageneutral videos to elicit human-composed sentences with speciﬁed transitivity attributes. The second task uses an iterative process to ﬁrst label the actors and objects in sentences and then annotate the sentences’ transitivity. We examine the success of these techniques and perform a preliminary classiﬁcation of the transitivity of held-out data. Hopper and Thompson (1980) created a multi-axis theory of Transitivity1 that describes the volition of the subject, the affectedness of the object, and the duration of the action. In short, this theory goes beyond the simple grammatical notion of transitivity (whether verbs take objects — transitive — or not — intransitive) and captures how much “action” takes place in a sentence. Such notions of Transitivity are not apparent from surface features alone; identical syntactic constructions can have vastly different Transitivity. This well-established linguistic theory, however, is not useful for real-world applications without a Transitivity-annotated corpus. Given such a substantive corpus, conventional machine learning techniques could help determine the Transitivity of verbs within sentences. Transitivity has been found to play a role in what is called “syntactic framing,” which expresses implicit sentiment (Greene and Resnik, 2009). 1We use capital “T” to differentiate from conventional syntactic transitivity throughout the paper.  In these contexts, the perspective or sentiment of the writer is reﬂected in the constructions used to express ideas. For example, a less Transitive construction might be used to deﬂect responsibility (e.g. “John was killed” vs. “Benjamin killed John”). In the rest of this paper, we review the HopperThompson transitivity schema and propose two relatively language-neutral methods to collect Transitivity ratings. The ﬁrst asks humans to generate sentences with desired Transitivity characteristics. The second asks humans to rate sentences on dimensions from the HopperThompson schema. We then discuss the difﬁculties of collecting such linguistically deep data and analyze the available results. We then pilot an initial classiﬁer on the Hopper-Thompson dimensions. 
Amazon Mechanical Turk (MTurk) is a marketplace for so-called “human intelligence tasks” (HITs), or tasks that are easy for humans but currently difﬁcult for automated processes. Providers upload tasks to MTurk which workers then complete. Natural language annotation is one such human intelligence task. In this paper, we investigate using MTurk to collect annotations for Subjectivity Word Sense Disambiguation (SWSD), a coarse-grained word sense disambiguation task. We investigate whether we can use MTurk to acquire good annotations with respect to gold-standard data, whether we can ﬁlter out low-quality workers (spammers), and whether there is a learning effect associated with repeatedly completing the same kind of task. While our results with respect to spammers are inconclusive, we are able to obtain high-quality annotations for the SWSD task. These results suggest a greater role for MTurk with respect to constructing a large scale SWSD system in the future, promising substantial improvement in subjectivity and sentiment analysis. 
We explore a new way to collect human annotated relations in text using Amazon Mechanical Turk. Given a knowledge base of relations and a corpus, we identify sentences which mention both an entity and an attribute that have some relation in the knowledge base. Each noisy sentence/relation pair is presented to multiple turkers, who are asked whether the sentence expresses the relation. We describe a design which encourages user efﬁciency and aids discovery of cheating. We also present results on inter-annotator agreement. 
Building machine translation (MT) test sets is a relatively expensive task. As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case. In this paper, we investigate using Amazon’s Mechanical Turk (MTurk) to make MT test sets cheaply. We ﬁnd that MTurk can be used to make test sets much cheaper than professionally-produced test sets. More importantly, in experiments with multiple MT systems, we ﬁnd that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield. 
This paper reports on experiments in the creation of a bi-lingual Textual Entailment corpus, using non-experts’ workforce under strict cost and time limitations ($100, 10 days). To this aim workers have been hired for translation and validation tasks, through the CrowdFlower channel to Amazon Mechanical Turk. As a result, an accurate and reliable corpus of 426 English/Spanish entailment pairs has been produced in a more cost-effective way compared to other methods for the acquisition of translations based on crowdsourcing. Focusing on two orthogonal dimensions (i.e. reliability of annotations made by non experts, and overall corpus creation costs), we summarize the methodology we adopted, the achieved results, the main problems encountered, and the lessons learned. 
The source text provided to a machine translation system is typically only one of many ways the input sentence could have been expressed, and alternative forms of expression can often produce a better translation. We introduce here error driven paraphrasing of source sentences: instead of paraphrasing a source sentence exhaustively, we obtain paraphrases for only the parts that are predicted to be problematic for the translation system. We report on an Amazon Mechanical Turk study that explores this idea, and establishes via an oracle evaluation that it holds the potential to substantially improve translation quality. 
(Mitchell et al., 2008) showed that it was possible to use a text corpus to learn the value of hypothesized semantic features characterizing the meaning of a concrete noun. The authors also demonstrated that those features could be used to decompose the spatial pattern of fMRI-measured brain activation in response to a stimulus containing that noun and a picture of it. In this paper we introduce a method for learning such semantic features automatically from a text corpus, without needing to hypothesize them or provide any proxies for their presence on the text. We show that those features are effective in a more demanding classiﬁcation task than that in (Mitchell et al., 2008) and describe their qualitative relationship to the features proposed in that paper. 
Multivariate analysis allows decoding of single trial data in individual subjects. Since different models are obtained for each subject it becomes hard to perform an analysis on the group level. We introduce a new algorithm for Bayesian multi-task learning which imposes a coupling between single-subject models. Using the CMU fMRI dataset it is shown that the algorithm can be used for concept classiﬁcation based on the average activation of regions in the AAL atlas. Concepts which were most easily classiﬁed correspond to the categories shelter, manipulation and eating, which is in accordance with the literature. The multi-task learning algorithm is shown to ﬁnd regions of interest that are common to all subjects which therefore facilitates interpretation of the obtained models. 
Different studies have been conducted for predicting human brain activity associated with the semantics of nouns. Corpus based approaches have been used for deriving feature vectors of concrete nouns, to model the brain activity associated with that noun. In this paper a computational model is proposed in which, the feature vectors for each concrete noun is computed by the WordNet similarity of that noun with the 25 sensory-motor verbs suggested by psychologists. The feature vectors are used for training a linear model to predict functional MRI images of the brain associated with nouns. The WordNet extracted features are also combined with corpus based semantic features of the nouns. The combined features give better results in predicting human brain activity related to concrete nouns. 
Korean Word Associations (KorWA) were collected to build a semantic network for the Korean language. A graphic representation approach of applying coefficients to complex networks allows us to discern the semantic structures within words. A semantic network of the KorWA was found to exhibit the scalefree property in its degree distribution. The growth of the network around hub words was also confirmed through two experimental phases. As an issue for further research, we suggest that the present results may yield insights for computational neurolinguistics, as a semantic network of word association norms can bridge the gap between information about lexical co-occurrences derived from a corpora and anatomical networks as a basis for mapping out neural activations. 
Electroencephalography (EEG) and magnetoencephalography (MEG) are closely related neuroimaging technologies that both measure summed electrical activity of synchronous sources of neural activity. However they differ in the portions of the brain to which they are more sensitive, in the frequency bands they can detect, and to the amount of noise to which they are subject. Since semantic representations are thought to be widely distributed in the brain, this preliminary study considered if the broader coverage offered by simultaneous EEG/MEG recordings would increase sensitivity to these cognitive states. The results showed that MEG data allowed stimuli in two semantic categories (mammals and tools) to be distinguished more accurately, despite some experimental settings that were optimised for EEG. The addition of EEG data did not prove informative, indicating that it may be redundant relative to MEG, even when using dimensionality reduction techniques to combat overﬁtting. 
Through the application of Chinese WordNet, the current study used the manipulation of visual field and the number of senses of the first character in Chinese disyllabic compounds to investigate the representation and the hemispheric processing of related senses in nouns and verbs. In the previous study, Huang et al. (2009) have found the ERP evidence to indicate single entry representation for Chinese polysemy in the left hemisphere; however, in the right hemisphere, they found sense inhibition which may be due to (1) the nature of hemispheric processing in dealing with semantic ambiguity or (2) the semantic activation from the separate-entry representation for senses. To clarify these possibilities, the study used the word class judgment task with the attempt to push subjects in a deeper level of lexical processing. The results revealed sense facilitation effect in the RH and suggested that in a deeper level, the RH had more possibility to observe the sense facilitation due to different efficiency of cerebral hemispheres. 
This work investigates lexical organizat ion of verbs looking at the influence of some linguistic factors on the process of lexical acquisition and use. Among the factors that may play a role in acquisition, in this paper we investigate the influence of polysemy. We examine data obtained from psycholinguistic action naming tasks performed by children and adults (speakers of Brazilian Portuguese), and analyze so me characteristics of the verbs used by each group in terms of similarity of content, using Jaccard‟s coefficient, and of topology, using graph theory. The experiments suggest that younger children tend to use more polysemic verbs than adults to describe events in the world. 
The automatic acquisition of feature-based conceptual representations from text corpora can be challenging, given the unconstrained nature of human-generated features. We examine large-scale extraction of conceptrelation-feature triples and the utility of syntactic, semantic, and encyclopedic information in guiding this complex task. Methods traditionally employed do not investigate the full range of triples occurring in human-generated norms (e.g. ﬂute produce sound), rather targeting concept-feature pairs (e.g. ﬂute – sound) or triples involving speciﬁc relations (e.g. is-a, part-of ). We introduce a novel method that extracts candidate triples (e.g. deer have antlers, ﬂute produce sound) from parsed data and re-ranks them using semantic information. We apply this technique to Wikipedia and the British National Corpus and assess its accuracy in a variety of ways. Our work demonstrates the utility of external knowledge in guiding feature extraction, and suggests a number of avenues for future work.  sources of such knowledge are property-norming studies, where a large number of participants write down lists of features for concepts. For example, McRae et al. (2005) collected a set of norms listing features for 541 concrete concepts. In that study, the features listed by different participants were normalised by mapping different feature descriptions with identical meanings to the same feature label.1 Table 1 gives the ten most frequent normed features for two concepts in the norms.  elephant  banana  Relation is has is is lives has has has has has  Feature large a trunk an animal grey in Africa ears tusks legs four legs large ears  Relation is is is is grows on eaten by eaten by is tastes  Feature yellow a fruit edible soft trees peeling grows monkeys long good  Table 1: Sample triples from McRae Norms  
We present a series of methods for deriving conceptual representations from corpora and investigate the usefulness of the fMRI data and machine learning methodology of Mitchell et al. (2008) as a basis for evaluating the different models. Within this framework, the quality of a semantic model is quantiﬁed by its ability to predict the fMRI activation associated with conceptual stimuli. Mitchell et al. used a manually-acquired set of verbs as the basis for their semantic model; in this paper, we also consider automatically acquired feature-norm-like semantic representations. These models make different assumptions about the kinds of information available in corpora that is relevant to representing conceptual knowledge. Our results indicate that automatically-acquired representations can make equally powerful predictions about the brain activity associated with the stimuli. 
In this paper we study what effects sentiment have on the temporal dynamics of user interaction and content generation in a knowledge sharing setting. We try to identify how sentiment inﬂuences interaction dynamics in terms of answer arrival, user ratings arrival, community agreement and content popularity. Our study suggests that “Negativity Bias” triggers more community attention and consequently more content contribution. Our ﬁndings provide insight into how users interact in online knowledge sharing communities, and helpful for improving existing systems. 
Article terms can move stock prices. By analyzing verbs in financial news articles and coupling their usage with a discrete machine learning algorithm tied to stock price movement, we can build a model of price movement based upon the verbs used, to not only identify those terms that can move a stock price the most, but also whether they move the predicted price up or down. 
Social Network Service (SNS) and personal blogs have become the most popular platform for online communication and sharing information. However because most modern computer keyboards are Latin-based, Asian language speakers (such as Chinese) has to rely on a input system which accepts Romanisation of the characters and convert them into characters or words in that language. In Chinese this form of Romanisation (usually called Pinyin) is highly ambiguous, word misuses often occur because the user choose a wrong candidate or deliverately substitute the word with another character string that has the identical Romanisation to convey certain semantics, or to achieve a sarcasm effect. In this paper we aim to develop a system that can automatically identify such word misuse, and suggest the correct word to be used. 
In this paper we propose the IR-LM (Information Retrieval Language Model) which is an approach to carrying out language modeling based on large volumes of constantly changing data as is the case of social media data. Our approach addresses specific characteristics of social data: large volume of constantly generated content as well as the need to frequently integrating and removing data from the model. 
Many questions submitted to Collaborative Question Answering (CQA) sites have been answered before. We propose an approach to automatically generating an answer to such questions based on automatically learning to identify “equivalent” questions. Our main contribution is an unsupervised method for automatically learning question equivalence patterns from CQA archive data. These patterns can be used to match new questions to their equivalents that have been answered before, and thereby help suggest answers automatically. We experimented with our method approach over a large collection of more than 200,000 real questions drawn from the Yahoo! Answers archive, automatically acquiring over 300 groups of question equivalence patterns. These patterns allow our method to obtain over 66% precision on automatically suggesting answers to new questions, significantly outperforming conventional baseline approaches to question matching. 
We are modeling roles of individual messages and participants in Q&A discussion forums. In this paper, we present a mixed network model that represents message exchanges and message influences within a discussion thread. We first model individual message roles and thread-level user roles using discussion content features. We then combine the resulting message roles and the user roles to generate the overall influence network. Message influences and their aggregation over the network are analyzed using B-centrality measures. We use the results in identifying the most influential message in answering the initial question of the thread.  eling the thread-level influence of the messages, besides the sink/source role of the individual messages, we take into account the roles of the message posters within the thread. In Q&A discussion threads, since the roles of the posters as an information provider or an information seeker often do not change within the same thread, such information can help us identify the true roles or influence of the messages. We combine the message roles and the user roles with a network model. Message influences and their aggregation over the network are analyzed using B-centrality measures. We use the resulting influence scores in identifying the most influential message in answering the initial question of the thread.  
• Capturing dynamics of social interactions: the sequence of communication or who is responding to whom is important in understanding the nature of interactions. • Relating social interactions to content analysis: the content can give hint on the nature of the interaction and vice versa (e.g., users with more social interactions are more likely to have common interests).  To address the above issues, one needs to go beyond the static analysis approach, and develop dynamical models that will explicitly account for the interplay between the content of communication (topics) and the structure of communications (social networks). Such framework and corresponding algorithmic base will allow us to infer “polarizing” topics discussed in forums, identify evolving communities of interests, and examine the link between social and content dynamics. To illustrate the advantages and the need for more ﬁne–grained analysis, we now turn to a concrete example. Figure 1(a) provides a sample of discussion co-participation network from an online discussion forum. Each oval node represents a user and each square shows a discussion thread, while each arrow represents users participating in the thread. The numbers on the arrow represent the number of messages contributed to the thread. Ten discussion threads with 127 messages from 43 users are captured. Based on this network, we can identify users that have similar interests, cluster topics and/or users according to similarities, and so on. However, this network is too coarse–grained to get additional information about the social interactions. For instance, it does not say anything whether co–participating users have similar or conﬂicting views. 
We propose an alternative to conventional information retrieval over Linux forum data, based on thread-, post- and user-level analysis, interfaced with an information retrieval engine via reranking.  Mining) project. Linux users and developers rely particularly heavily on web user forums and mailing lists, due to the nature of the community, which is highly decentralised — with massive proliferation of packages and distributions — and notoriously bad at maintaining up-to-date documentation at a level suitable for newbie and even intermediate users.  
 Philips Research Europe  ISLA, University of Amsterdam  High Tech Campus 34  Science Park 107  5656 AE Eindhoven, The Netherlands  1098 XG Amsterdam, The Netherlands  paul.ackermans@philips.com  jijkoun,m.derijke,w.weerkamp@uva.nl gijs.geleijnse@philips.com  
This paper describes a technique for automatically tagging political blog posts using SVM’s and named entity recognition. We compare the quality of the tags detected by this approach to earlier approaches in other domains, observing effects from the political domain and benefits from NLP techniques complementary to the core SVM method. 
 3 The Annotation Process  We detail methods for entity span identification and entity class annotation of Twitter communications that take place during times of mass emergency. We present our motivation, method and preliminary results. 
We describe the ﬁrst release of our corpus of 97 million Twitter posts. We believe that this data will prove valuable to researchers working in social media, natural language processing, large-scale data processing, and similar areas.  Table 1: N-gram statistics.  N-grams  tokens  unique  Unigrams Bigrams Trigrams 4-grams  2,263,886,631 2,167,567,986 2,072,595,131 1,980,386,036  31,883,775 174,785,693 948,850,470 1,095,417,876  
This paper describes work in progress on labelling and spatio-temporal grounding of news events as part of a news analysis system that is under development. 
 2 Preliminary Results  Tracking information ﬂow (IFLOW) is crucial to understanding the evolution of news stories. We present analysis and experiments for IFLOW between company announcements and newswire. Error analysis shows that many FPs are annotation errors and many FNs are due to coarse-grained document-level modelling. Experiments show that document meta-data features (e.g., category, length, timing) improve f-scores relative to upper bound by 23%. 
from Wikipedia pages of people included in the  Wikipedia controversial topic list.  
In Technical Documentation, Authoring Tools are used to maintain a consistent text quality—especially with regard to the often followed translation of the original documents into several languages using a Translation Memory System. Hitherto these tools have often been used separately one after the other. Additionally Authoring tools often have no linguistic intelligence and thus the quality level of the automated checks is very poor. In this paper I will describe the integration of a linguistically intelligent Authoring Tool into a Translation Memory System, thereby combining linguistic intelligence with the advantages of both systems in a single environment. The system allows you not only the use of common authoring aids (spell, grammar and style checker) in source and target language—by using a single environment the terminology database of the Translation Memory System can be used by the authoring aid to control terminology both in the source and target document. Moreover, the linguistically intelligent Authoring Tool enables automatic extraction of term candidates from existing documents directly to the terminology database of the Translation Memory System. 
Scientiﬁc authors urgently need help in managing the fast increasing number of publications. We describe and demonstrate a tool that supports authors in browsing graphically through electronically available publications, thus allowing them to quickly adapt to new domains and publish faster. Navigation is assisted by means of typed citation graphs, i.e. we use methods and resources from computational linguistics to compute the kind of citation that is made from one paper to another (refutation, use, conﬁrmation etc.). To verify the computed citation type, the user can inspect the highlighted citation sentence in the original PDF document. While our classiﬁcation methods used to generate a realistic test data set are relatively simple and could be combined with other proposed approaches, we put a strong focus on usability and quick navigation in the potentially huge graphs. In the outlook, we argue that our tool could be made part of a community approach to overcome the sparseness and correctness dilemma in citation classiﬁcation. 
In this paper, we present findings from a human judgement task we conducted on the effectiveness of syntax filtering in a word completion task. Human participants were asked to review a series of incomplete sentences and identify which words from accompanying lists extend the expressions in a grammatically appropriate way. The accompanying word lists were generated by two word completion systems (our own plus a third-party commercial system) where the ungrammatical items were filtered out. Overall, participants agreed more, to a statistically significant degree, with the syntax-filtered systems than with baseline. However, further analysis suggests that syntax filtering alone does not necessarily improve the overall acceptability and usability of the word completion output. Given that word completion is typically employed in applications to aid writing, unlike other NLP tasks, accounting for the role of writer vs. reader becomes critical. Evaluating word completion and, more generally, applications for alternative and augmentative communication (AAC) will be discussed. 
Web applications have the opportunity to check spelling, style, and grammar using a software service architecture. A software service authoring aid can offer contextual spell checking, detect real word errors, and avoid poor grammar checker suggestions through the use of large language models. Here we present After the Deadline, an open source authoring aid, used in production on WordPress.com, a blogging platform with over ten million writers. We discuss the benefits of the software service environment and how it affected our choice of algorithms. We summarize our design principles as speed over accuracy, simplicity over complexity, and do what works.  software service architecture. In this paper we discuss how this system works, the trade-offs of the software service environment, and the benefits. We conclude with a discussion of our design principles: speed over accuracy, simplicity over complexity, and do what works. 1.1 What is a Software Service? A software service (Turner et al., 2003) is an application that runs on a server. Client applications post the expected inputs to the server and receive the output as XML. Our software service checks spelling, style, and grammar. A client connects to our server, posts the text, and receives the errors and suggestions as XML. Figure 1 shows this process. It is the client’s responsibility to display the errors and present the suggestions to the user.  
This paper describes a resource-rich toolkit that assists EFL writers take a discoverybased approach to writing accurate and fluent English. The system helps learners identify lexico-grammatical errors by matching patterns gleaned from a very large corpus of learners’ texts. Users are guided to appropriate language patterns as they write and revise through online declarative and procedural resources. Even as more robust and fully automatic feedback technologies evolve, comprehensive resource-rich support will remain necessary for second-language (L2) writers who must develop practical life-long language learning strategies. To assist language tutors support novice L2 writers, we have also produced tools that help tutors reinforce their students’ independent writing and proofreading strategies. The operation and rationale of this approach have been implemented and evaluated in several Hong Kong universities and secondary schools. 
Text simpliﬁcation is the process of changing vocabulary and grammatical structure to create a more accessible version of the text while maintaining the underlying information and content. Automated tools for text simpliﬁcation are a practical way to make large corpora of text accessible to a wider audience lacking high levels of ﬂuency in the corpus language. In this work, we investigate the potential of Simple Wikipedia to assist automatic text simpliﬁcation by building a statistical classiﬁcation system that discriminates simple English from ordinary English. Most text simpliﬁcation systems are based on hand-written rules (e.g., PEST (Carroll et al., 1999) and its module SYSTAR (Canning et al., 2000)), and therefore face limitations scaling and transferring across domains. The potential for using Simple Wikipedia for text simpliﬁcation is signiﬁcant; it contains nearly 60,000 articles with revision histories and aligned articles to ordinary English Wikipedia. Using articles from Simple Wikipedia and ordinary Wikipedia, we evaluated different classiﬁers and feature sets to identify the most discriminative features of simple English for use across domains. These ﬁndings help further understanding of what makes text simple and can be applied as a tool to help writers craft simple text. 
Rather than explain research that has already been carried out, this paper describes a specific context of writing instruction and poses questions about how research on writing and computational linguistics might be brought together to address three pressing issues: the validity of Directed SelfPlacement; the relationship between confidence and competence in student writing; and strategies to help English Language Learners, especially those in the category of Generation 1.5, improve their writing. 
Novice writers face significant challenges as they learn to master the broad range of skills that contribute to composition. Novice and expert writers differ considerably, and devising effective composition support tools for novice writers requires a clear understanding of the process and products of writing. This paper reports on a study conducted with more than one hundred middle grade students interacting with a narrative composition support environment. The texts are found to pose important challenges for state-of-the-art natural language processing techniques. Furthermore, the study investigates the language usage of middle grade students, the cohesion and coherence of the resulting texts, and the relationship between students’ language arts skills and their writing processes. The findings suggest that composition support environments require robust NLP tools that can account for the variations in students’ writing in order to effectively support each phase of the writing process. 
In this paper, we present a new approach to writing tools that extends beyond the rudimentary spelling and grammar checking to the content of the writing itself. Linguistic methods have long been used to detect familiar lexical patterns in the text to aid automatic summarization and translation of documents. We apply these methods to determine the quality of the text and implement new techniques for measuring readability and providing feedback to authors on how to improve the quality of their documents. We take an extended view of readability that considers text cohesion, propositional density, and word familiarity. We provide simple feedback to the user detailing the most and least readable sentences, the sentences most densely packed with information and the most cohesive words in their document. Commonly used verbose words and phrases in the text, as identified by The Plain English Campaign, can be replaced with user-selected replacements. Our techniques were implemented as a free download extension to the Open Office word processor generating 6,500 downloads to date. 
In this paper, a novel system for the automatic identiﬁcation and conjugation of Spanish verb neologisms is presented. The paper describes a rule-based algorithm consisting of six steps which are taken to determine whether a new verb is regular or not, and to establish the rules that the verb should follow in its conjugation. The method was evaluated on 4,307 new verbs and its performance found to be satisfactory both for irregular and regular neologisms. The algorithm also contains extra rules to cater for verb neologisms in Spanish that do not exist as yet, but are inferred to be possible in light of existing cases of new verb creation in Spanish. 
The exponential growth of the Persian blogosphere and the increased number of neologisms create a major challenge in NLP applications of Persian blogs. This paper describes a method for extracting and classifying newly constructed words and borrowings from Persian blog posts. The analysis of the occurrence of neologisms across five distinct topic categories points to a correspondence between the topic domain and the type of neologism that is most commonly encountered. The results suggest that different approaches should be implemented for the automatic detection and processing of neologisms depending on the domain of application. 
Most computational approaches to metaphor have focused on discerning between metaphorical and literal text. Recent work on computational metaphor identiﬁcation (CMI) instead seeks to identify overarching conceptual metaphors by mapping selectional preferences between source and target corpora. This paper explores using semantic role labeling (SRL) in CMI. Its goals are two-fold: ﬁrst, to demonstrate that semantic roles can effectively be used to identify conceptual metaphors, and second, to compare SRL to the current use of typed dependency parsing in CMI. The results show that SRL can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that SRL introduces its own set of complications. The paper concludes by suggesting future directions, both for evaluating the use of SRL in CMI, and for fostering critical and creative thinking about metaphors. 
Man achieved ﬂight by studying how birds ﬂy, and yet the solution that engineers came up with (jet planes) is very different from the one birds apply. In this paper I review a number of efforts in automated story telling and poetry generation, identifying which human abilities are being modelled in each case. In an analogy to the classic example of bird-ﬂight and jet planes, I explore how the computational models relate to (the little we know about) human performance, what the similarities are between the case for linguistic creativity and the case for ﬂight, and what the analogy might have to say about artiﬁcial linguistic creativity if it were valid.  In this paper I review a number of such research and development efforts that I have been involved in or studied in detail, paying particular attention to identifying which traits of human activity are being modelled in each case. In an analogy to the classic example of bird-ﬂight and jet planes, I explore how the computational models of linguistic creativity relate to (the little we know about) human performance, what the similarities are between the case for linguistic creativity and the case for ﬂight, and what the analogy might have to say about artiﬁcial linguistic creativity if it were valid. 2 Creativity at Different Levels of Linguistic Decision  
This paper presents our on-going work to improve the lyric generation component of the Automatic Lyric Generation system for the Tamil Language. An earlier version of the system used an n-gram based model to generate lyrics that match the given melody. This paper identifies some of the deficiencies in the melody analysis and text generation components of the earlier system and explains the new approach used to tackle those drawbacks. The two central approaches discussed in this paper are: (1) An improved mapping scheme for matching melody with words and (2) Knowledge-based Text Generation algorithm based on an existing Ontology and Tamil Morphology Generator. 
Automatic story generation systems require a body of commonsense knowledge about the basic relationships between concepts we find everyday in our world in order to produce interesting narratives that describe human actions and world events. This paper presents an ongoing work that investigates the use of Suggested Upper Merged Ontology (SUMO) to represent storytelling knowledge and its inference engine Sigma to query actions and events that may take place in the story to be generated. The resulting story plan (fabula) is also represented in SUMO, allowing for a single story representation to be realized in various human languages. 
This paper describes two natural language processing systems designed to assist songwriters in obtaining and developing ideas for their craft. Titular is a text synthesis algorithm for automatically generating novel song titles, which lyricists can use to back-form concepts and narrative story arcs. LyriCloud is a word-level language “browser” or “explorer,” which allows users to interactively select words and receive lyrical suggestions in return. Two criteria for creativity tools are also presented along with examples of how they guided the development of these systems, which were used by musicians during an international songwriting contest. 
Though data-driven in nature, emotion analysis based on latent semantic analysis still relies on some measure of expert knowledge in order to isolate the emotional keywords or keysets necessary to the construction of affective categories. This makes it vulnerable to any discrepancy between the ensuing taxonomy of affective states and the underlying domain of discourse. This paper proposes a more general strategy which leverages two distincts semantic levels, one that encapsulates the foundations of the domain considered, and one that speciﬁcally accounts for the overall affective fabric of the language. Exposing the emergent relationship between these two levels advantageously informs the emotion classiﬁcation process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text.  interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of course, it plays a critical role in the generation of expressive synthetic speech (Schro¨der, 2006). Emphasis has traditionally been placed on the set of six “universal” emotions (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, and SURPRISE (Alm et al., 2005; Liu et al., 2003; Subasic and Huettner, 2001). Emotion analysis is typically carried out using a simpliﬁed description of emotional states in a low-dimensional space, which normally comprises dimensions such as valence (positive/negative evalution), activation (stimulation of activity), and/or control (dominant/submissive power) (Mehrabian, 1995; Russell, 1980; Strapparava and Mihalcea, 2008). Classiﬁcation proceeds based on an underlying emotional knowledge base, which strives to pro-  
 channel by automatically routing the emails involv-  ing critical issues to specialized representatives. Be-  Prompt and knowledgeable responses to customers’ emails are critical in maximizing customer satisfaction. Such emails often contain complaints about unfair treatment due to negligence, incompetence, rigid protocols, unfriendly systems, and unresponsive personnel. In this paper, we refer to these emails as emotional emails. They provide valuable feedback to improve contact center processes and customer care, as well as, to enhance customer retention. This paper describes a method for extracting salient features and identifying emotional emails in customer care. Salient features reﬂect customer frustration, dissatisfaction with the business, and threats to either leave, take legal action and/or report to authorities. Compared to a baseline system using word ngrams, our proposed approach with salient features resulted in a 20% absolute Fmeasure improvement.  sides concerns related to products and services, businesses ensure that emails complaining about unfair treatment due to negligence, incompetence, rigid protocols and unfriendly systems, are always handled with care. Such emails, referred to as emotional emails, are critical to reduce the churn i.e., retaining customers who otherwise would have taken their business elsewhere, and, at the same time, they are a valuable source of information for improving business processes. In recurring service oriented businesses, a large number of customer emails may contain routine complaints. While such complaints are important and are addressed by customer service representatives, our purpose here is to identify emotional emails where severity of the complaints and customer dissatisfaction are relatively high. Emotional emails may contain abusive and probably emotion-  ally charged language, but we are mainly interested  
 analysis. Plot units are complex structures that in-  clude affect states, causal links, and cross-character  We present a system called AESOP that automatically produces affect states associated with characters in a story. This research represents a ﬁrst step toward the automatic generation of plot unit structures from text. AESOP incorporates several existing sentiment analysis tools and lexicons to evaluate the effectiveness of current sentiment technology on this  links, and generating complete plot unit structures is beyond the scope of this work. As an initial step toward the long-term goal of automatically generating plot units, we began by creating a system to automatically identify the affect states associated with characters. An affect state represents the emotional state of a character, based on their perspective of events  task. AESOP also includes two novel components: a method for acquiring patient polarity verbs, which impart negative affect on their patients, and affect projection rules to propagate affect tags from surrounding words onto the characters in the story. We evaluate AESOP on a small collection of fables.  in the story. Plots units include three types of affect states: positive (+) states, negative (-) states, and mental (M) states that have neutral emotion (these are often associated with plans and goals). Our system, called AESOP, pulls together a variety of existing technologies in sentiment analysis to automatically identify words and phrases that  
 shout and boiling are indicative of anger, and so on.  Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk. In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech. We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand.  Therefore an emotion lexicon—a list of emotions and words that are indicative of each emotion—is likely to be useful in identifying emotions in text. Words may evoke different emotions in different contexts, and the emotion evoked by a phrase or a sentence is not simply the sum of emotions conveyed by the words in it, but the emotion lexicon will be a useful component for any sophisticated emotion detecting algorithm. The lexicon will also be useful for evaluating automatic methods that identify the emotions evoked by a word. Such algorithms may then be used to automatically generate emotion lexicons in languages where no such lexicons exist. As of now, high-quality high-coverage emotion lexicons do not exist for any language, although there are a few limited-coverage lexicons for a handful of languages, for example, the WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) for six basic emotions and the General Inquirer (GI) (Stone et al., 1966), which categorizes words into a number of  
 speciﬁc applications, or they use lexical resources  Since paraphrasing is one of the crucial tasks in natural language understanding and generation, this paper introduces a novel technique to extract paraphrases for emotion terms, from non-parallel corpora. We present a bootstrapping technique for identifying paraphrases, starting with a small number of seeds. WordNet Affect emotion words are used as seeds. The bootstrapping approach learns extraction patterns for six classes of emotions. We use annotated blogs and other datasets as texts from which to extract paraphrases, based on the highest-scoring extraction patterns. The results include lexical and morpho-syntactic paraphrases, that we evaluate with human judges.  such as WordNet (Miller et al., 1993) to identify paraphrases. This paper introduces a novel method for extracting paraphrases for emotions from texts. We focus on the six basic emotions proposed by Ekman (1992): happiness, sadness, anger, disgust, surprise, and fear. We describe the construction of the paraphrases extractor. We also propose a k-window algorithm for selecting contexts that are used in the paraphrase extraction method. We automatically learn patterns that are able to extract the emotion paraphrases from corpora, starting with a set of seed words. We use data sets such as blogs and other annotated cor-  pora, in which the emotions are marked. We use  
Emotion cause detection is a new research area in emotion processing even though most theories of emotion treat recognition of a triggering cause event as an integral part of emotion. As a first step towards fully automatic inference of cause-emotion correlation, we propose a textdriven, rule-based approach to emotion cause detection in this paper. First of all, a Chinese emotion cause annotated corpus is constructed based on our proposed annotation scheme. By analyzing the corpus data, we identify seven groups of linguistic cues and generalize two sets of linguistic rules for detection of emotion causes. With the linguistic rules, we then develop a rule-based system for emotion cause detection. In addition, we propose an evaluation scheme with two phases for performance assessment. Experiments show that our system achieves a promising performance for cause occurrence detection as well as cause event detection. The current study should lay the ground for future research on the inferences of implicit information and the discovery of new information based on cause-event relation. 
This paper describes methods aimed at solving the novel problem of automatically discovering ‘wishes’ from (English) documents such as reviews or customer surveys. These wishes are sentences in which authors make suggestions (especially for improvements) about a product or service or show intentions to purchase a product or service. Such ‘wishes’ are of great use to product managers and sales personnel, and supplement the area of sentiment analysis by providing insights into the minds of consumers. We describe rules that can help detect these ‘wishes’ from text. We evaluate these methods on texts from the electronic and banking industries. 
 in detecting emotion (e.g., Subasic and Huettner  Subtle social information is available in text such as a speaker’s emotional state, intentions, and attitude, but current information extraction systems are unable to extract this information at the level that humans can. We describe a methodology for creating databases of messages annotated with social information based on interactive games between humans trying to generate and interpret messages for a number of different social information types. We then present some classiﬁcation results achieved by using a small-scale database created with this methodology.  2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn 2006, Zhou and Sung 2008). This latter kind of social information is useful for identifying the “tone” of a message, i.e., for understanding the underlying intention behind a message’s creation, and also for predicting how this message will be interpreted by humans reading it. A technical barrier to extracting this kind of social information is that there are currently no large-scale text databases that are annotated with social information from which to learn the relevant linguistic  
The automatic analysis and classification of text using fine-grained attitude labels is the main task we address in our research. The developed @AM system relies on compositionality principle and a novel approach based on the rules elaborated for semantically distinct verb classes. The evaluation of our method on 1000 sentences, that describe personal experiences, showed promising results: average accuracy on fine-grained level was 62%, on middle level – 71%, and on top level – 88%. 
 of lexical alternatives, customized to the application  domain. We are interested in approaches that will  We investigate techniques for generating alternative output sentences with varying sentiment, using (an approximation to) the Valentino method, based on SentiWordNet, of Guerini et al. We extend this method by ﬁltering out unacceptable candidate sentences, using bigrams sourced from different corpora to determine whether lexical substitutions are appropriate in the given context. We also compare the generated candidates against human judgements of whether the desired sentiment shift has occurred: our results suggest limitations with the overall knowledge-based approach, and we propose potential directions for improvement.  scale, and can be applied domain-independently. While our ultimate aim is generation of language that relects emotional state, in this work we investigate the automatic generation of varying “sentiment” in output utterances; we focus on sentiment mainly due to the recent development of useful resources for this task. (Guerini et al., 2008)’s Valentino system is an approach to automatically generating candidate output utterances with different sentiment from an original; the authors suggest ECAs as a possible application scenario for their techniques. We explore this suggestion, implementing a lexical substitution (McCarthy and Navigli, 2007) approach to dialogue generation with sen-  
 set than typically used in EA, and assign them to an  unrestricted range of text.  Emotion analysis (EA) is a rapidly developing area in computational linguistics. An EA system can be extremely useful in ﬁelds such as information retrieval and emotion-driven computer animation. For most EA systems, the number of emotion classes is very limited and the text units the classes are assigned to are discrete and predeﬁned. The question we address in this paper is whether the set of emotion categories can be enriched and whether the units to which the categories are assigned can be more ﬂexibly deﬁned. We present an experiment showing how an annotation task can be set up so that untrained participants can perform emotion analysis with high agreement even when not restricted to a predetermined annotation unit and using a rich set of emotion categories. As such it sets the stage for the development of more complex EA systems which are closer to the actual human emotional perception of text.  To explore whether human annotators can reliably perform a task, inter-annotator agreement (IAA) (Artstein and Poesio, 2008) is the relevant measure. This measure can be calculated between every two individual annotations in order to ﬁnd pairs or even teams of annotators whose strategies seem to be consistent and coherent enough so that they can be used further as the gold-standard annotation suited to train a machine learning approach for automatic EA analysis. A resulting EA system, capable of simulating human emotional perception of text, would be useful for information retrieval and many other ﬁelds. There are two main aspects of the resulting annotations to be researched. First, how consistently can people perceive and locate the emotional aspect of fairy tale texts? Second, how do they express their perception of text by means of annotation strategies? In the next sections, we address these questions and  provide details of an experiment we conducted to  
We investigate the effect of text summarisation in the problem of rating-inference – the task of associating a ﬁne-grained numerical rating to an opinionated document. We set-up a comparison framework to study the effect of different summarisation algorithms of various compression rates in this task and compare the classiﬁcation accuracy of summaries and documents for associating documents to classes. We make use of SVM algorithms to associate numerical ratings to opinionated documents. The algorithms are informed by linguistic and sentiment-based features computed from full documents and summaries. Preliminary results show that some types of summaries could be as effective or better as full documents in this problem. 
This work explores the utility of sentiment and arguing opinions for classifying stances in ideological debates. In order to capture arguing opinions in ideological stance taking, we construct an arguing lexicon automatically from a manually annotated corpus. We build supervised systems employing sentiment and arguing opinions and their targets as features. Our systems perform substantially better than a distribution-based baseline. Additionally, by employing both types of opinion features, we are able to perform better than a unigrambased system. 
 provide any new information to the model. For ex-  In this work, we propose a novel representation of text based on patterns derived from linguistic annotation graphs. We use a subgraph mining algorithm to automatically derive features as frequent subgraphs from the annotation graph. This process generates a very large number of features, many of which are highly correlated. We propose a genetic programming based approach to feature construction which creates a ﬁxed number of strong classiﬁcation predictors from these subgraphs. We evaluate the beneﬁt gained from evolved structured features, when used in addition to the bag-of-words features, for a sentiment classiﬁcation task.  ample, a feature of type unigram POS (e.g. “camera NN”) doesn’t provide any additional information beyond the unigram feature (e.g. “camera”), for words that are often used with the same part of speech. However, alongside several redundant features, there are also features that provide new information. It is these features that we aim to capture. In this work, we propose an evolutionary approach that constructs complex features from subgraphs extracted from an annotation graph. A constant number of these features are added to the unigram feature space, adding much of the representational beneﬁts without the computational cost of a drastic increase in feature space size. In the remainder of the paper, we review prior  
We explore the task of automatic classification of texts by the emotions expressed. Our novel method arranges neutrality, polarity and emotions hierarchically. We test the method on two datasets and show that it outperforms the corresponding “flat” approach, which does not take into account the hierarchical information. The highly imbalanced structure of most of the datasets in this area, particularly the two datasets with which we worked, has a dramatic effect on the performance of classification. The hierarchical approach helps alleviate the effect. 
Active learning is a promising method to reduce human’s effort for data annotation in different NLP applications. Since it is an iterative task, it should be stopped at some point which is optimum or near-optimum. In this paper we propose a novel stopping criterion for active learning of frame assignment based on the variability of the classiﬁer’s conﬁdence score on the unlabeled data. The important advantage of this criterion is that we rely only on the unlabeled data to stop the data annotation process; as a result there are no requirements for the gold standard data and testing the classiﬁer’s performance in each iteration. Our experiments show that the proposed method achieves 93.67% of the classiﬁer maximum performance. 
Word alignment models form an important part of building statistical machine translation systems. Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial alignments acquired from humans. Such dedicated elicitation effort is often expensive and depends on availability of bilingual speakers for the language-pair. In this paper we study active learning query strategies to carefully identify highly uncertain or most informative alignment links that are proposed under an unsupervised word alignment model. Manual correction of such informative links can then be applied to create a labeled dataset used by a semi-supervised word alignment model. Our experiments show that using active learning leads to maximal reduction of alignment error rates with reduced human effort. 
In some classiﬁcation tasks, such as those related to the automatic building and maintenance of text corpora, it is expensive to obtain labeled examples to train a classiﬁer. In such circumstances it is common to have massive corpora where a few examples are labeled (typically a minority) while others are not. Semi-supervised learning techniques try to leverage the intrinsic information in unlabeled examples to improve classiﬁcation models. However, these techniques assume that the labeled examples cover all the classes to learn which might not stand. In the presence of an imbalanced class distribution getting labeled examples from minority classes might be very costly if queries are randomly selected. Active learning allows asking an oracle to label new examples, that are criteriously selected, and does not assume a previous knowledge of all classes. D-Conﬁdence is an active learning approach that is effective when in presence of imbalanced training sets. In this paper we discuss the performance of dConﬁdence over text corpora. We show empirically that d-Conﬁdence reduces the number of queries required to identify examples from all classes to learn when compared to conﬁdence, a common active learning criterion. 
In this work, we show how active learning in some (target) domain can leverage information from a different but related (source) domain. We present an algorithm that harnesses the source domain data to learn the best possible initializer hypothesis for doing active learning in the target domain, resulting in improved label complexity. We also present a variant of this algorithm which additionally uses the domain divergence information to selectively query the most informative points in the target domain, leading to further reductions in label complexity. Experimental results on a variety of datasets establish the efﬁcacy of the proposed methods. 
A practical concern for Active Learning (AL) is the amount of time human experts must wait for the next instance to label. We propose a method for eliminating this wait time independent of speciﬁc learning and scoring algorithms by making scores always available for all instances, using old (stale) scores when necessary. The time during which the expert is annotating is used to train models and score instances–in parallel–to maximize the recency of the scores. Our method can be seen as a parameterless, dynamic batch AL algorithm. We analyze the amount of staleness introduced by various AL schemes and then examine the effect of the staleness on performance on a part-of-speech tagging task on the Wall Street Journal. Empirically, the parallel AL algorithm effectively has a batch size of one and a large candidate set size but eliminates the time an annotator would have to wait for a similarly parameterized batch scheme to select instances. The exact performance of our method on other tasks will depend on the relative ratios of time spent annotating, training, and scoring, but in general we expect our parameterless method to perform favorably compared to batch when accounting for wait time. 
We assume that in order to properly capture opinion and sentiment expressed in a text or dialog any system needs a deep text processing approach. In particular, the idea that the task may be solved by the use of Information Retrieval tools like Bag of Words Approaches (BOWs) is totally ﬂawed. BOWs approaches are sometimes also camouﬂaged by a keyword based Ontology matching and Concept search, based on such lexica as SentiWordNet, by simply stemming a text and using content words to match its entries and produce some result. Any search based on keywords and BOWs is fatally ﬂawed by the impossibility to cope with such fundamental issues as the following ones: • presence of negation at diﬀerent levels of syntactic constituency; • presence of lexicalized negation in the verb or in adverbs; • presence of conditional, counterfactual subordinators; • double negations with copulative verbs; • presence of modals and other modality operators. In order to cope with these linguistic elements we propose to build a Flat Logical Form (FLF) directly from a Dependency Structure representation augmented by indices and where anaphora resolution has operated pronoun-antecedent substitutions. We implemented these additions our the system called venses that we will show. The output of the system is an xml representation where each sentence of a text or dialog is a list of attribute-value pairs, like polarity, attitute and factuality. In order to produce this output, the system makes use of FLF and a vector of semantic attributes associated to the verb at propositional level and then memorized. Important notions required by the computation of opinion and sentiment are also the distinction of the semantic content of each proposition into two separate categories: • Objective vs Subjective This distinction is obtained by searching for factivity markers again at propositional level. In particular we take into account: • tense; • voice; • mood; • modality operators; • modiﬁers and attributes adjuncts at sentence level; • lexical type of the verb (in Levin’s classes and also using WordNet classiﬁcation). Rodolfo Delmonte. 2010. Opinion Mining, Subjectivity and Factuality. In Proceedings of Australasian Language Technology Association Workshop, page 2 
Computers that can listen and talk? Helpers that read your books and answer complex questions? Natural language processing has always made big promises, and has left many people with a healthy dose of skepticism. The truth is that language technology has had a bigger impact than you may think, by working behind the scenes. In this talk, I will discuss the changing role of NLP in applications, and the trends I have observed over the last 5 years with my work at Google. I will be focusing on the practicalities of applying NLP techniques to real-world problems, avoiding common pitfalls, and learning from our most valuable resource: our users. Casey Whitelaw. 2010. Language Technology: A View From The Trenches. In Proceedings of Australasian Language Technology Association Workshop, page 3  
This study is a linguistic study on idiosyncrasy using speaker classiﬁcation technique as an analytical tool. The goals of this study are to ﬁnd out 1) to what extent Japanese ﬁller words (e.g. um, you know in English) carry individual idiosyncratic information; 2) if there are any differences in the degree/nature of idiosyncrasy between the sexes; and 3) what contributes to the identiﬁed gender differences, if there are any. Based purely on the individual selection of ﬁllers, we report in this study that 1) speaker discrimination performance was better in the male (ca. 85% accuracy) than the female (ca. 75% accuracy) speakers by approximately 10%, and 2) the poorer performance of the female speakers was due to the larger within– speaker differences in the female speakers than the male speakers. That is, the selection of ﬁllers by female speakers is more variable, speech by speech, than that by male speakers, even under similar conditions (e.g. same type of audience and the same degree of formality). We also discuss that the ﬁndings of the current study agree with the previously–reported differences between the sexes in language use. 
Written contracts are a fundamental framework for commercial and cooperative transactions and relationships. Limited research has been published on the application of machine learning and natural language processing (NLP) to contracts. In this paper we report the classiﬁcation of components of contract texts using machine learning and hand-coded methods. Authors studying a range of domains have found that combining machine learning and rule based approaches increases accuracy of machine learning. We ﬁnd similar results which suggest the utility of considering leveraging hand coded classiﬁcation rules for machine learning. We attained an average accuracy of 83.48% on a multiclass labelling task on 20 contracts combining machine learning and rule based approaches, increasing performance over machine learning alone. 
This research focuses on improving information access over troubleshootingoriented technical user forums via threadlevel analysis. We describe a modular task formulation and novel dataset, and go on to describe a series of preliminary classiﬁcation experiments over the data. We ﬁnd that a class composition strategy achieves the best results, surpassing multiclass classiﬁcation approaches. 
Preceding a phrase-based statistical machine translation (PSMT) system by a syntactically-informed reordering preprocessing step has been shown to improve overall translation performance compared to a baseline PSMT system. However, the improvement is not seen for every sentence. We use a lattice input to a PSMT system in order to translate simultaneously across both original and reordered versions of a sentence, and include a number of conﬁdence features to support the system in choosing on a sentence-by-sentence basis whether to use the reordering process. In German-to-English translation, our best system achieves a BLEU score of 21.39, an improvement of 0.62. 
Pathology reports are used to store information about cells and tissues of a patient, and they are crucial to monitor the health of individuals and population groups. In this work we present an evaluation of supervised text classiﬁcation models for the prediction of relevant categories in pathology reports. Our aim is to integrate automatic classiﬁers to improve the current workﬂow of medical experts, and we implement and evaluate different machine learning approaches for a large number of categories. Our results show that we are able to predict nominal categories with high average f-score (81.3%), and we can improve over the majority class baseline by relying on Naive Bayes and feature selection. We also ﬁnd that the classiﬁcation of numeric categories is harder, and deeper analysis would be required to predict these labels. 
This paper introduces a novel user classiﬁcation task in the context of web user forums. We present a deﬁnition of four basic user characteristics and an annotated dataset. We outline a series of approaches for predicting user characteristics, utilising aggregated post features and user/thread network analysis in a supervised learning context. Using the proposed feature sets, we achieve results above both a naive baseline and a bag-ofwords approach, for all four of our basic user characteristics. In all cases, our bestperforming classiﬁer is statistically indistinct from an upper bound based on the inter-annotator agreement for the task. 
Early use of corpora for language learning has included analysis of word usage via concordancing. In addition, some attempts have been made to use readability criteria for recommending reading to learners. In this paper we discuss various tools and approaches for enhanced language learning support, including different methods of ﬁltering text based on vocabulary and grammatical criteria. We demonstrate the effects of various criteria on the retrieval of text, assuming the user is English-speaking and learning French. Filtering text based on a small vocabulary of frequently occurring words, a set of English-French cognates and named entities, and high coverage criteria, results in the retrieval of short readable extracts from French literature. We expect that text available from the web may yield many more documents of appropriate readability. 
Automatically judging sentences for their grammaticality is potentially useful for several purposes — evaluating language technology systems, assessing language competence of second or foreign language learners, and so on. Previous work has examined parser ‘byproducts’, in particular parse probabilities, to distinguish grammatical sentences from ungrammatical ones. The aim of the present paper is to examine whether the primary output of a parser, which we characterise via CFG production rules embodied in a parse, contains useful information for sentence grammaticality classiﬁcation; and also to examine which feature selection metrics are most useful in this task. Our results show that using gold standard production rules alone can improve over using parse probabilities alone. Combining parser-produced production rules with parse probabilities further produces an improvement of 1.6% on average in the overall classiﬁcation accuracy. 
In this paper we motivate the need for a corpus for the development and testing of summarisation systems for evidencebased medicine. We describe the corpus which we are currently creating, and show its applicability by evaluating several simple query-based summarisation techniques using a small fragment of the corpus. 
In this paper we describe machine learning experiments that aim to characterise the content selection process for distinguishing descriptions. Our experiments are based on two large corpora of humanproduced descriptions of objects in relatively small visual scenes; the referring expressions are annotated with their semantic content. The visual context of reference is widely considered to be a primary determinant of content in referring expression generation, so we explore whether a model can be trained to predict the collection of descriptive attributes that should be used in a given situation. Our experiments demonstrate that speaker-speciﬁc preferences play a much more important role than existing approaches to referring expression generation acknowledge. 
n-best parse reranking is an important technique for improving the accuracy of statistical parsers. Reranking is not constrained by the dynamic programming required for tractable parsing, so arbitrary features of each parse may be considered. We adapt the reranking features and methodology used by Charniak and Johnson (2005) for the C&C Combinatory Categorial Grammar parser, and develop new features based on the richer formalism. The reranker achieves a labeled dependency F-score of 87.59%, which is a signiﬁcant improvement over prior results. 
Unrehearsed spoken language often contains many disﬂuencies. If we want to correctly interpret the content of spoken language, we need to be able to detect these disﬂuencies and deal with them appropriately. In the work described here, we use a statistical noisy channel model to detect disﬂuencies in transcripts of spoken language. Like all statistical approaches, this is naturally very data-hungry; however, corpora containing transcripts of unrehearsed spoken language with disﬂuencies annotated are a scarce resource, which makes training diﬃcult. We address this issue in the following ways: First, since written textual corpora are much more abundant than speech corpora, we see whether using a large text corpus to increase the data available to our language model component delivers an improvement. Second, given that most spoken language corpora are not annotated with disﬂuencies, we explore the use of Expectation Maximisation to mark the disﬂuencies in such corpora, so as to increase the data availability for our complete model. In neither case do we see an improvement in our results. We discuss these results and the possible reasons for the negative outcome. 
We report here our work on English French Cross-lingual Word Sense Disambiguation where the task is to ﬁnd the best French translation for a target English word depending on the context in which it is used. Our approach relies on identifying the nearest neighbors of the test sentence from the training data using a pairwise similarity measure. The proposed measure ﬁnds the afﬁnity between two sentences by calculating a weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard Wordnet Similarity measures. Once the nearest neighbors have been identiﬁed, the best translation is found by taking a majority vote over the French translations of the nearest neighbors. 
This paper describes the design of a system for extracting keyphrases from a single document The principle of the algorithm is to cluster sentences of the documents in order to highlight parts of text that are semantically related. The clusters of sentences, that reﬂect the themes of the document, are then analyzed to ﬁnd the main topics of the text. Finally, the most important words, or groups of words, from these topics are proposed as keyphrases. 
In this paper we present a chunk based keyphrase extraction method for scientiﬁc articles. Different from most previous systems, supervised machine learning algorithms are not used in our system. Instead, document structure information is used to remove unimportant contents; Chunk extraction and ﬁltering is used to reduce the quantity of candidates; Keywords are used to ﬁlter the candidates before generating ﬁnal keyphrases. Our experimental results on test data show that the method works better than the baseline systems and is comparable with other known algorithms. 
Likey is an unsupervised statistical approach for keyphrase extraction. The method is language-independent and the only language-dependent component is the reference corpus with which the documents to be analyzed are compared. In this study, we have also used another language-dependent component: an English-speciﬁc Porter stemmer as a preprocessing step. In our experiments of keyphrase extraction from scientiﬁc articles, the Likey method outperforms both supervised and unsupervised baseline methods. 
In this paper, it is presented an unsupervised approach to automatically discover the latent keyphrases contained in scientiﬁc articles. The proposed technique is constructed on the basis of the combination of two techniques: maximal frequent sequences and pageranking. We evaluated the obtained results by using micro-averaged precision, recall and Fscores with respect to two different gold standards: 1) reader’s keyphrases, and 2) a combined set of author’s and reader’s keyphrases. The obtained results were also compared against three different baselines: one unsupervised (TF-IDF based) and two supervised (Na¨ıve Bayes and Maximum Entropy). 
The UvT system is based on a hybrid, linguistic and statistical approach, originally proposed for the recognition of multiword terminological phrases, the C-value method (Frantzi et al., 2000). In the UvT implementation, we use an extended noun phrase rule set and take into consideration orthographic and morphological variation, term abbreviations and acronyms, and basic document structure information. 
This paper addresses the problem of ranking a list of paraphrases associated with a noun-noun compound as closely as possible to human raters (Butnariu et al., 2010). UCD-Goggle tackles this task using semantic knowledge learnt from the Google n-grams together with human-preferences for paraphrases mined from training data. Empirical evaluation shows that UCDGoggle achieves 0.432 Spearman correlation with human judgments. 
We describe a system which ranks humanprovided paraphrases of noun compounds, where the frequency with which a given paraphrase was provided by human volunteers is the gold standard for ranking. Our system assigns a score to a paraphrase of a given compound according to the number of times it has co-occurred with other paraphrases in the rest of the dataset. We use these co-occurrence statistics to compute conditional probabilities to estimate a sub-typing or Is-A relation between paraphrases. This method clusters together paraphrases which have similar meanings and also favours frequent, general paraphrases rather than infrequent paraphrases with more speciﬁc meanings. 
Extracting temporal information from raw text is fundamental for deep language understanding, and key to many applications like question answering, information extraction, and document summarization. In this paper, we describe two systems we submitted to the TempEval 2 challenge, for extracting temporal information from raw text. The systems use a combination of deep semantic parsing, Markov Logic Networks and Conditional Random Field classifiers. Our two submitted systems, TRIPS and TRIOS, approached all tasks and outperformed all teams in two tasks. Furthermore, TRIOS mostly had second-best performances in other tasks. TRIOS also outperformed the other teams that attempted all the tasks. Our system is notable in that for tasks C – F, they operated on raw text while all other systems used tagged events and temporal expressions in the corpus as input. 
This paper presents TIPSem, a system to extract temporal information from natural language texts for English and Spanish. TIPSem, learns CRF models from training data. Although the used features include different language analysis levels, the approach is focused on semantic information. For Spanish, TIPSem achieved the best F1 score in all the tasks. For English, it obtained the best F1 in tasks B (events) and D (event-dct links); and was among the best systems in the rest. 
The system to spot INIs, DNIs and their antecedents is an adaptation of VENSES, a system for semantic evaluation that has been used for RTE challenges in the last 6 years. In the following we will briefly describe the system and then the additions we made to cope with the new task. In particular, we will discuss how we mapped the VENSES analysis to the representation of frame information in order to identify null instantiations in the text. 
 methods for recognition and normalization tasks  The system described in this paper has participated in the Tempeval 2 competition, speciﬁcally in the Task A, which aim is to determine the extent of the time expressions in a text as deﬁned by the TimeML TIMEX3 tag, and the value of the features type and val. For this purpose, a combination of TERSEO system and the T2T3 Transducer was used. TERSEO system is able to annotate text with TIDES TIMEX2 tags, and T2T3 transducer performs the translation from this TIMEX2 tags to TIMEX3 tags.  (Ahn et al., 2005) (Ahn, 2006); CU-TMP (University of Colorado): uses machine learning for automatic annotation (Bethard and Martin, 2007), and c) mixed combination of rules and ML approaches, such as, TempEx (MITRE Corporation): combines hand-coded patterns with machine learning rules to tag documents (TempEx, 2008) (Mani and Wilson, 2000); TARSQI (Brandeis University): currently uses GUTime (2008) for temporal expression annotation, which extends the capabilities of the TempEx tagger while generating TIMEX3 annotations (Verhagen et al., 2005). However, whatever the approach, the output of these systems is a standardized annotation scheme.  
In this paper, we describe HeidelTime, a system for the extraction and normalization of temporal expressions. HeidelTime is a rule-based system mainly using regular expression patterns for the extraction of temporal expressions and knowledge resources as well as linguistic clues for their normalization. In the TempEval-2 challenge, HeidelTime achieved the highest FScore (86%) for the extraction and the best results in assigning the correct value attribute, i.e., in understanding the semantics of the temporal expressions. 
In this paper we describe a system for the recognition and normalization of temporal expressions (Task 13: TempEval-2, Task A). The recognition task is approached as a classification problem of sentence constituents and the normalization is implemented in a rule-based manner. One of the system features is extending positive annotations in the corpus by semantically similar words automatically obtained from a large unannotated textual corpus. The best results obtained by the system are 0.85 and 0.84 for precision and recall respectively for recognition of temporal expressions; the accuracy values of 0.91 and 0.55 were obtained for the feature values TYPE and VAL respectively. 
This paper describes the participation of Universidad Carlos III de Madrid in Task A of the TempEval-2 evaluation. The UC3M system was originally developed for the temporal expressions recognition and normalization (TERN task) in Spanish texts, according to the TIDES standard. Current version supposes an almost-total refactoring of the earliest system. Additionally, it has been adapted to the TimeML annotation schema and a considerable effort has been done with the aim of increasing its coverage. It takes a rule-based design both in the identification and the resolution phases. It adopts an inductive approach based on the empirical study of frequency of temporal expressions in Spanish corpora. Detecting the extent of the temporal expressions the system achieved a Precision/Recall of 0.90/0.87 whereas, in determining the TYPE and VALUE of those expressions, system results were 0.91 and 0.83, respectively. 
We describe the Edinburgh information extraction system which we are currently adapting for analysis of newspaper text as part of the SYNC3 project. Our most recent focus is geospatial and temporal grounding of entities and it has been useful to participate in TempEval-2 to measure the performance of our system and to guide further development. We took part in Tasks A and B for English. 
We describe the University of Shefﬁeld system used in the TempEval-2 challenge, USFD2. The challenge requires the automatic identiﬁcation of temporal entities and relations in text. USFD2 identiﬁes and anchors temporal expressions, and also attempts two of the four temporal relation assignment tasks. A rule-based system picks out and anchors temporal expressions, and a maximum entropy classiﬁer assigns temporal link labels, based on features that include descriptions of associated temporal signal words. USFD2 identiﬁed temporal expressions successfully, and correctly classiﬁed their type in 90% of cases. Determining the relation between an event and time expression in the same sentence was performed at 63% accuracy, the second highest score in this part of the challenge. 
As a participant in TempEval-2, we address the temporal relations task consisting of four related subtasks. We take a supervised machine-learning technique using Markov Logic in combination with rich lexical relations beyond basic and syntactic features. One of our two submitted systems achieved the highest score for the Task F (66% precision), untied, and the second highest score (63% precision) for the Task C, which tied with three other systems. 
Word sense induction and discrimination (WSID) identiﬁes the senses of an ambiguous word and assigns instances of this word to one of these senses. We have build a WSID system that exploits syntactic and semantic features based on the results of a natural language parser component. To achieve high robustness and good generalization capabilities, we designed our system to work on a restricted, but grammatically rich set of features. Based on the results of the evaluations our system provides a promising performance and robustness. 
This paper studies the application of the Web Selectors word sense disambiguation system on a speciﬁc domain. The system was primarily applied without any domain tuning, but the incorporation of domain predominant sense information was explored. Results indicated that the system performs relatively the same with domain predominant sense information as without, scoring well above a random baseline, but still 5 percentage points below results of using the ﬁrst sense. 
We describe two approaches for All-words Word Sense Disambiguation on a Speciﬁc Domain. The ﬁrst approach is a knowledge based approach which extracts domain-speciﬁc largest connected components from the Wordnet graph by exploiting the semantic relations between all candidate synsets appearing in a domainspeciﬁc untagged corpus. Given a test word, disambiguation is performed by considering only those candidate synsets that belong to the top-k largest connected components. The second approach is a weakly supervised approach which relies on the “One Sense Per Domain” heuristic and uses a few hand labeled examples for the most frequently appearing words in the target domain. Once the most frequent words have been disambiguated they can provide strong clues for disambiguating other words in the sentence using an iterative disambiguation algorithm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. 
This demonstration presents the LinGO Grammar Matrix grammar customization system: a repository of distilled linguistic knowledge and a web-based service which elicits a typological description of a language from the user and yields a customized grammar fragment ready for sustained development into a broad-coverage grammar. We describe the implementation of this repository with an emphasis on how the information is made available to users, including in-browser testing capabilities. 
We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single uniﬁed internal representation for translation forests, the decoder strictly separates model-speciﬁc translation logic from general rescoring, pruning, and inference algorithms. From this uniﬁed representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efﬁcient C++ implementation means that memory use and runtime performance are signiﬁcantly better than comparable decoders. 
We present BEETLE II, a tutorial dialogue system designed to accept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies. Our ﬁrst system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 
GernEdiT (short for: GermaNet Editing Tool) offers a graphical interface for the lexicographers and developers of GermaNet to access and modify the underlying GermaNet resource. GermaNet is a lexical-semantic wordnet that is modeled after the Princeton WordNet for English. The traditional lexicographic development of GermaNet was error prone and time-consuming, mainly due to a complex underlying data format and no opportunity of automatic consistency checks. GernEdiT replaces the earlier development by a more userfriendly tool, which facilitates automatic checking of internal consistency and correctness of the linguistic resource. This paper presents all these core functionalities of GernEdiT along with details about its usage and usability. 
This software demonstration presents WebLicht (short for: Web-Based Linguistic Chaining Tool), a webbased service environment for the integration and use of language resources and tools (LRT). WebLicht is being developed as part of the D-SPIN project1. WebLicht is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of LRT services. WebLicht allows the integration and use of distributed web services with standardized APIs. The nature of these open and standardized APIs makes it possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.) Additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service.  nizer) and at the Seminar für Sprachwissenschaft/Computerlinguistik at the University of Tübingen (conversion of plain text to D-Spin format, GermaNet, Open Thesaurus synonym service, and Treebank browser). They cover a wide range of linguistic applications, like tokenization, co-occurrence extraction, POS Tagging, lexical and semantic analysis, and several laguages (currently German, English, Italian, French, Romanian, Spanish and Finnish). For some of these tasks, more than one web service is available. As a first external partner, the University of Helsinki in Finnland contributed a set of web services to create morphological annotated text corpora in the Finnish language. With the help of the webbased user interface, these individual web services can be combined into a chain of linguistic applications. 2 Service Oriented Architecture  
We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efﬁciency of the reference implementations and also provide their results on six benchmarks. 
This paper describes the KomParse system, a natural-language dialog system in the three-dimensional virtual world Twinity. In order to fulﬁll the various communication demands between nonplayer characters (NPCs) and users in such an online virtual world, the system realizes a ﬂexible and hybrid approach combining knowledge-intensive domainspeciﬁc question answering, task-speciﬁc and domain-speciﬁc dialog with robust chatbot-like chitchat. 
This paper presents a general-purpose open source package for recognizing Textual Entailment. The system implements a collection of algorithms, providing a conﬁgurable framework to quickly set up a working environment to experiment with the RTE task. Fast prototyping of new solutions is also allowed by the possibility to extend its modular architecture. We present the tool as a useful resource to approach the Textual Entailment problem, as an instrument for didactic purposes, and as an opportunity to create a collaborative environment to promote research in the ﬁeld. 
In the business world, analyzing and dealing with risk permeates all decisions and actions. However, to date, risk identiﬁcation, the ﬁrst step in the risk management cycle, has always been a manual activity with little to no intelligent software tool support. In addition, although companies are required to list risks to their business in their annual SEC ﬁlings in the USA, these descriptions are often very highlevel and vague. In this paper, we introduce Risk Mining, which is the task of identifying a set of risks pertaining to a business area or entity. We argue that by combining Web mining and Information Extraction (IE) techniques, risks can be detected automatically before they materialize, thus providing valuable business intelligence. We describe a system that induces a risk taxonomy with concrete risks (e.g., interest rate changes) at its leaves and more abstract risks (e.g., ﬁnancial risks) closer to its root node. The taxonomy is induced via a bootstrapping algorithms starting with a few seeds. The risk taxonomy is used by the system as input to a risk monitor that matches risk mentions in ﬁnancial documents to the abstract risk types, thus bridging a lexical gap. Our system is able to automatically generate company speciﬁc “risk maps”, which we demonstrate for a corpus of earnings report conference calls. 
The Deep Web is the collection of information repositories that are not indexed by search engines. These repositories are typically accessible through web forms and contain dynamically changing information. In this paper, we present a system that allows users to access such rich repositories of information on mobile devices using spoken language. 
This is a system demo for a set of tools for translating texts between multiple languages in real time with high quality. The translation works on restricted languages, and is based on semantic interlinguas. The underlying model is GF (Grammatical Framework), which is an open-source toolkit for multilingual grammar implementations. The demo will cover up to 20 parallel languages. Two related sets of tools are presented: grammarian’s tools helping to build translators for new domains and languages, and translator’s tools helping to translate documents. The grammarian’s tools are designed to make it easy to port the technique to new applications. The translator’s tools are essential in the restricted language context, enabling the author to remain in the fragments recognized by the system. The tools that are demonstrated will be applied and developed further in the European project MOLTO (Multilingual On-Line Translation) which has started in March 2010 and runs for three years. 
This paper presents ongoing research on computational models for non-cooperative dialogue. We start by analysing different levels of cooperation in conversation. Then, inspired by ﬁndings from an empirical study, we propose a technique for measuring non-cooperation in political interviews. Finally, we describe a research programme towards obtaining a suitable model and discuss previous accounts for conﬂictive dialogue, identifying the differences with our work. 
Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to ﬁll this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The ﬁrst-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efﬁciently abstract over large regions of the state and action spaces. 
This work models Word Sense Disambiguation (WSD) problem as a Distributed Constraint Optimization Problem (DCOP). To model WSD as a DCOP, we view information from various knowledge sources as constraints. DCOP algorithms have the remarkable property to jointly maximize over a wide range of utility functions associated with these constraints. We show how utility functions can be designed for various knowledge sources. For the purpose of evaluation, we modelled all words WSD as a simple DCOP problem. The results are competitive with state-of-art knowledge based systems. 
We present a probabilistic model extension to the Tesnie`re Dependency Structure (TDS) framework formulated in (Sangati and Mazza, 2009). This representation incorporates aspects from both constituency and dependency theory. In addition, it makes use of junction structures to handle coordination constructions. We test our model on parsing the English Penn WSJ treebank using a re-ranking framework. This technique allows us to efﬁciently test our model without needing a specialized parser, and to use the standard evaluation metric on the original Phrase Structure version of the treebank. We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking. 
 2 Related Work  The translation of sentiment information is a task from which sentiment analysis systems can beneﬁt. We present a novel, graph-based approach using SimRank, a well-established vertex similarity algorithm to transfer sentiment information between a source language and a target language graph. We evaluate this method in comparison with SO-PMI. 
We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efﬁcient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and conﬁrms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modiﬁcation to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU. 
The growing availability of spoken language corpora presents new opportunities for enriching the methodologies of speech and language therapy. In this paper, we present a novel approach for constructing speech motor exercises, based on linguistic knowledge extracted from spoken language corpora. In our study with the Dutch Spoken Corpus, syllabic inventories were obtained by means of automatic syllabiﬁcation of the spoken language data. Our experimental syllabiﬁcation method exhibited a reliable performance, and allowed for the acquisition of syllabic tokens from the corpus. Consequently, the syllabic tokens were integrated in a tool for clinicians, a result which holds the potential of contributing to the current state of speech motor training methodologies. 
The emergence of social media brings chances, but also challenges, to linguistic analysis. In this paper we investigate a novel problem of discovering patterns based on emotion and the association of moods and affective lexicon usage in blogosphere, a representative for social media. We propose the use of normative emotional scores for English words in combination with a psychological model of emotion measurement and a nonparametric clustering process for inferring meaningful emotion patterns automatically from data. Our results on a dataset consisting of more than 17 million mood-groundtruthed blogposts have shown interesting evidence of the emotion patterns automatically discovered that match well with the coreaffect emotion model theorized by psychologists. We then present a method based on information theory to discover the association of moods and affective lexicon usage in the new media. 
Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. First, we utilize user behaviors to generate candidate words. Then, we utilize search engine to enrich candidate words with adequate semantic features. Finally, we reorder candidate words according to their semantic relatedness to the seed word. Experimental results on a Chinese input method dataset show that our method gains better performance. 
We show that using conﬁdence-weighted classiﬁcation in transition-based parsing gives results comparable to using SVMs with faster training and parsing time. We also compare with other online learning algorithms and investigate the effect of pruning features when using conﬁdenceweighted classiﬁcation. 
A robust dictionary of semantic frames is an essential element of natural language understanding systems that use ontologies. However, creating lexical resources that accurately capture semantic representations en masse is a persistent problem. Where the sheer amount of content makes hand creation inefficient, computerized approaches often suffer from over generality and difficulty with sense disambiguation. This paper describes a semi-automatic method to create verb semantic frames in the Cyc ontology by converting the information contained in VerbNet into a Cyc usable format. This method captures the differences in meaning between types of verbs, and uses existing connections between WordNet, VerbNet, and Cyc to specify distinctions between individual verbs when available. This method provides 27,909 frames to OpenCyc which currently has none and can be used to extend ResearchCyc as well. We show that these frames lead to a 20% increase in sample sentences parsed over the Research Cyc verb lexicon. 
Various text mining algorithms require the process of feature selection. High-level semantically rich features, such as ﬁgurative language uses, speech errors etc., are very promising for such problems as e.g. writing style detection, but automatic extraction of such features is a big challenge. In this paper, we propose a framework for ﬁgurative language use detection. This framework is based on the idea of sense differentiation. We describe two algorithms illustrating the mentioned idea. We show then how these algorithms work by applying them to Russian language data. 
We present a system that automatically induces Selectional Preferences (SPs) for Latin verbs from two treebanks by using Latin WordNet. Our method overcomes some of the problems connected with data sparseness and the small size of the input corpora. We also suggest a way to evaluate the acquired SPs on unseen events extracted from other Latin corpora. 
In this paper, we propose a novel method for automatic segmentation of a Sanskrit string into different words. The input for our segmentizer is a Sanskrit string either encoded as a Unicode string or as a Roman transliterated string and the output is a set of possible splits with weights associated with each of them. We followed two different approaches to segment a Sanskrit text using sandhi1 rules extracted from a parallel corpus of manually sandhi split text. While the ﬁrst approach augments the ﬁnite state transducer used to analyze Sanskrit morphology and traverse it to segment a word, the second approach generates all possible segmentations and validates each constituent using a morph analyzer. 
Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 
Presupposition relations between verbs are not very well covered in existing lexical semantic resources. We propose a weakly supervised algorithm for learning presupposition relations between verbs that distinguishes ﬁve semantic relations: presupposition, entailment, temporal inclusion, antonymy and other/no relation. We start with a number of seed verb pairs selected manually for each semantic relation and classify unseen verb pairs. Our algorithm achieves an overall accuracy of 36% for type-based classiﬁcation. 
Statistical systems with high accuracy are very useful in real-world applications. If these systems can capture basic linguistic information, then the usefulness of these statistical systems improve a lot. This paper is an attempt at incorporating linguistic constraints in statistical dependency parsing. We consider a simple linguistic constraint that a verb should not have multiple subjects/objects as its children in the dependency tree. We first describe the importance of this constraint considering Machine Translation systems which use dependency parser output, as an example application. We then show how the current state-ofthe-art dependency parsers violate this constraint. We present two new methods to handle this constraint. We evaluate our methods on the state-of-the-art dependency parsers for Hindi and Czech. 
The aim of this work is to present some preliminary results of an investigation in course on the typology of the morphology of the native South American languages from the point of view of the formal language theory. With this object, we give two contrasting examples of descriptions of two Aboriginal languages ﬁnite verb forms morphology: Argentinean Quechua (quichua santiaguen˜o) and Toba. The description of the morphology of the ﬁnite verb forms of Argentinean quechua, uses ﬁnite automata and ﬁnite transducers. In this case the construction is straightforward using two level morphology and then, describes in a very natural way the Argentinean Quechua morphology using a regular language. On the contrary, the Toba verbs morphology, with a system that simultaneously uses preﬁxes and sufﬁxes, has not a natural description as regular language. Toba has a complex system of causative sufﬁxes, whose successive applications determinate the use of preﬁxes belonging different person marking preﬁx sets. We adopt the solution of Creider et al. (1995) to naturally deal with this and other similar morphological processes which involve interactions between preﬁxes and sufﬁxes and then we describe the toba morphology using linear context-free languages.1 . 
Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained signiﬁcant gains in BLEU scores for IWSLT and Europarl datasets. 
In hierarchical phrase-based SMT systems, statistical models are integrated to guide the hierarchical rule selection for better translation performance. Previous work mainly focused on the selection of either the source side of a hierarchical rule or the target side of a hierarchical rule rather than considering both of them simultaneously. This paper presents a joint model to predict the selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield signiﬁcant improvements in performance. 
 Lexicalized reordering models play a crucial role in phrase-based translation systems. They are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efﬁciently. Experimental results on the NIST Chinese-English test sets show that our approach signiﬁcantly outperforms the baseline method. 
Source language parse trees offer very useful but imperfect reordering constraints for statistical machine translation. A lot of effort has been made for soft applications of syntactic constraints. We alternatively propose the selective use of syntactic constraints. A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system. 
We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversiﬁed alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 
This paper presents an efﬁcient implementation of linearised lattice minimum Bayes-risk decoding using weighted ﬁnite state transducers. We introduce transducers to efﬁciently count lattice paths containing n-grams and use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices. 
We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision. 
In this paper, we present a novel approach for authorship attribution, the task of identifying the author of a document, using probabilistic context-free grammars. Our approach involves building a probabilistic context-free grammar for each author and using this grammar as a language model for classiﬁcation. We evaluate the performance of our method on a wide range of datasets to demonstrate its efﬁcacy. 
Supporting natural language input may improve learning in intelligent tutoring systems. However, interpretation errors are unavoidable and require an effective recovery policy. We describe an evaluation of an error recovery policy in the BEETLE II tutorial dialogue system and discuss how different types of interpretation problems affect learning gain and user satisfaction. In particular, the problems arising from student use of non-standard terminology appear to have negative consequences. We argue that existing strategies for dealing with terminology problems are insufﬁcient and that improving such strategies is important in future ITS research. 
Generating referring expressions is a key step in Natural Language Generation. Researchers have focused almost exclusively on generating distinctive referring expressions, that is, referring expressions that uniquely identify their intended referent. While undoubtedly one of their most important functions, referring expressions can be more than distinctive. In particular, descriptive referring expressions – those that provide additional information not required for distinction – are critical to ﬂuent, efﬁcient, well-written text. We present a corpus analysis in which approximately one-ﬁfth of 7,207 referring expressions in 24,422 words of news and narrative are descriptive. These data show that if we are ever to fully master natural language generation, especially for the genres of news and narrative, researchers will need to devote more attention to understanding how to generate descriptive, and not just distinctive, referring expressions. 
Current Referring Expression Generation algorithms rely on domain dependent preferences for both content selection and linguistic realization. We present two experiments showing that human speakers may opt for dispreferred properties and dispreferred modiﬁer orderings when these were salient in a preceding interaction (without speakers being consciously aware of this). We discuss the impact of these ﬁndings for current generation algorithms. 
We pose the development of cognitively plausible models of human language processing as a challenge for computational linguistics. Existing models can only deal with isolated phenomena (e.g., garden paths) on small, speciﬁcally selected data sets. The challenge is to build models that integrate multiple aspects of human language processing at the syntactic, semantic, and discourse level. Like human language processing, these models should be incremental, predictive, broad coverage, and robust to noise. This challenge can only be met if standardized data sets and evaluation measures are developed. 
The Manually Annotated Sub-Corpus (MASC) project provides data and annotations to serve as the base for a communitywide annotation effort of a subset of the American National Corpus. The MASC infrastructure enables the incorporation of contributed annotations into a single, usable format that can then be analyzed as it is or ported to any of a variety of other formats. MASC includes data from a much wider variety of genres than existing multiply-annotated corpora of English, and the project is committed to a fully open model of distribution, without restriction, for all data and annotations produced or contributed. As such, MASC is the ﬁrst large-scale, open, communitybased effort to create much needed language resources for NLP. This paper describes the MASC project, its corpus and annotations, and serves as a call for contributions of data and annotations from the language processing community. 
This paper proposes a method of correcting annotation errors in a treebank. By using a synchronous grammar, the method transforms parse trees containing annotation errors into the ones whose errors are corrected. The synchronous grammar is automatically induced from the treebank. We report an experimental result of applying our method to the Penn Treebank. The result demonstrates that our method corrects syntactic annotation errors with high precision. 
This paper introduces mNCD, a method for automatic evaluation of machine translations. The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and ﬂexible word matching provided by stemming and synonyms. The mNCD measure outperforms NCD in system-level correlation to human judgments in English. 
We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well. 
This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we ﬁnd that a simple exemplar model outperforms more complex state-of-the-art models. 
In predicate-argument structure analysis, it is important to capture non-local dependencies among arguments and interdependencies between the sense of a predicate and the semantic roles of its arguments. However, no existing approach explicitly handles both non-local dependencies and semantic dependencies between predicates and arguments. In this paper we propose a structured model that overcomes the limitation of existing approaches; the model captures both types of dependencies simultaneously by introducing four types of factors including a global factor type capturing non-local dependencies among arguments and a pairwise factor type capturing local dependencies between a predicate and an argument. In experiments the proposed model achieved competitive results compared to the stateof-the-art systems without applying any feature selection procedure. 
One deﬁciency of current shallow parsing based Semantic Role Labeling (SRL) methods is that syntactic chunks are too small to effectively group words. To partially resolve this problem, we propose semantics-driven shallow parsing, which takes into account both syntactic structures and predicate-argument structures. We also introduce several new “path” features to improve shallow parsing based SRL method. Experiments indicate that our new method obtains a signiﬁcant improvement over the best reported Chinese SRL result. 
 In this paper we start to explore two-part collocation extraction association measures that do not estimate expected probabilities on the basis of the independence assumption. We propose two new measures based upon the well-known measures of mutual information and pointwise mutual information. Expected probabilities are derived from automatically trained Aggregate Markov Models. On three collocation gold standards, we ﬁnd the new association measures vary in their effectiveness.  
Bag-of-words approaches to information retrieval (IR) are effective but assume independence between words. The Hyperspace Analogue to Language (HAL) is a cognitively motivated and validated semantic space model that captures statistical dependencies between words by considering their co-occurrences in a surrounding window of text. HAL has been successfully applied to query expansion in IR, but has several limitations, including high processing cost and use of distributional statistics that do not exploit syntax. In this paper, we pursue two methods for incorporating syntactic-semantic information from textual ‘events’ into HAL. We build the HAL space directly from events to investigate whether processing costs can be reduced through more careful deﬁnition of word co-occurrence, and improve the quality of the pseudo-relevance feedback by applying event information as a constraint during HAL construction. Both methods signiﬁcantly improve performance results in comparison with original HAL, and interpolation of HAL and relevance model expansion outperforms either method alone. 
We propose a novel method to automatically acquire a term-frequency-based taxonomy from a corpus using an unsupervised method. A term-frequency-based taxonomy is useful for application domains where the frequency with which terms occur on their own and in combination with other terms imposes a natural term hierarchy. We highlight an application for our approach and demonstrate its effectiveness and robustness in extracting knowledge from real-world data. 
We describe the strategy currently pursued for verbalising OWL ontologies by sentences in Controlled Natural Language (i.e., combining generic rules for realising logical patterns with ontology-speciﬁc lexicons for realising atomic terms for individuals, classes, and properties) and argue that its success depends on assumptions about the complexity of terms and axioms in the ontology. We then show, through analysis of a corpus of ontologies, that although these assumptions could in principle be violated, they are overwhelmingly respected in practice by ontology developers. 
We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. We design a generative model for word alignment that uses synonym information as a regularization term. The experimental results show that our proposed method signiﬁcantly improves word alignment quality. 
This paper presents a novel ﬁltration criterion to restrict the rule extraction for the hierarchical phrase-based translation model, where a bilingual but relaxed wellformed dependency restriction is used to ﬁlter out bad rules. Furthermore, a new feature which describes the regularity that the source/target dependency edge triggers the target/source word is also proposed. Experimental results show that, the new criteria weeds out about 40% rules while with translation performance improvement, and the new feature brings another improvement to the baseline system, especially on larger corpus. 
Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 
Documents often have inherently parallel structure: they may consist of a text and commentaries, or an abstract and a body, or parts presenting alternative views on the same problem. Revealing relations between the parts by jointly segmenting and predicting links between the segments, would help to visualize such documents and construct friendlier user interfaces. To address this problem, we propose an unsupervised Bayesian model for joint discourse segmentation and alignment. We apply our method to the “English as a second language” podcast dataset where each episode is composed of two parallel parts: a story and an explanatory lecture. The predicted topical links uncover hidden relations between the stories and the lectures. In this domain, our method achieves competitive results, rivaling those of a previously proposed supervised technique. 
Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difﬁcult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference resolution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets. 
Maintaining high annotation consistency in large corpora is crucial for statistical learning; however, such work is hard, especially for tasks containing semantic elements. This paper describes predicate argument structure analysis using ɹ transformation-based learning. An advantage of transformation-based learning is the readability of learned rules. A disadvantage is that the rule extraction procedure is time-consuming. We present incremental-based, transformation-based learning for semantic processing tasks. As an example, we deal with Japanese predicate argument analysis and show some tendencies of annotators for constructing a corpus with our method. 
Developing features has been shown crucial to advancing the state-of-the-art in Semantic Role Labeling (SRL). To improve Chinese SRL, we propose a set of additional features, some of which are designed to better capture structural information. Our system achieves 93.49 Fmeasure, a signiﬁcant improvement over the best reported performance 92.0. We are further concerned with the effect of parsing in Chinese SRL. We empirically analyze the two-fold effect, grouping words into constituents and providing syntactic information. We also give some preliminary linguistic explanations. 
 This work deals with the application of conﬁdence measures within an interactivepredictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efﬁciency, user effort can be saved by interactively translating only those initial translations which the conﬁdence measure classiﬁes as incorrect. We apply conﬁdence estimation as a way to achieve a balance between user effort savings and ﬁnal translation error. Empirical results show that our proposal allows to obtain almost perfect translations while signiﬁcantly reducing user effort.  
We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difﬁcult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy signiﬁcantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses. 
We propose a corpus-based probabilistic framework to extract hidden common syntax across languages from non-parallel multilingual corpora in an unsupervised fashion. For this purpose, we assume a generative model for multilingual corpora, where each sentence is generated from a language dependent probabilistic contextfree grammar (PCFG), and these PCFGs are generated from a prior grammar that is common across languages. We also develop a variational method for efﬁcient inference. Experiments on a non-parallel multilingual corpus of eleven languages demonstrate the feasibility of the proposed method. 
Nivre’s method was improved by enhancing deterministic dependency parsing through application of a tree-based model. The model considers all words necessary for selection of parsing actions by including words in the form of trees. It chooses the most probable head candidate from among the trees and uses this candidate to select a parsing action. In an evaluation experiment using the Penn Treebank (WSJ section), the proposed model achieved higher accuracy than did previous deterministic models. Although the proposed model’s worst-case time complexity is O(n2), the experimental results demonstrated an average parsing time not much slower than O(n). 
A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Speciﬁcally, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In experiments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques. 
We propose a top-down algorithm for extracting k-best lists from a parser. Our algorithm, TKA∗ is a variant of the kbest A∗ (KA∗) algorithm of Pauls and Klein (2009). In contrast to KA∗, which performs an inside and outside pass before performing k-best extraction bottom up, TKA∗ performs only the inside pass before extracting k-best lists top down. TKA∗ maintains the same optimality and efﬁciency guarantees of KA∗, but is simpler to both specify and implement. 
Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classiﬁcation task. This simpliﬁes semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004). 
The Minimum Description Length (MDL) principle is a method for model selection that trades oﬀ between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efﬁcient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets. 
We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 
We address the problem of selecting nondomain-speciﬁc language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domainspeciﬁc and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods. 
Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently converges in less time. A core component of the algorithm is a grammar transformation which represents an inﬁnite tree substitution grammar in a ﬁnite context free grammar. This enables efﬁcient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy. 
Motivated by the recent interest in streaming algorithms for processing large text collections, we revisit the work of Ravichandran et al. (2005) on using the Locality Sensitive Hash (LSH) method of Charikar (2002) to enable fast, approximate comparisons of vector cosine similarity. For the common case of feature updates being additive over a data stream, we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than when using the standard ofﬂine technique. 
In this paper we demonstrate that there is a strong correlation between the Question Answering (QA) accuracy and the log-likelihood of the answer typing component of our statistical QA model. We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the log-likelihood of a set of question-and-answer pairs. Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters. 
Many NLP tasks need accurate knowledge for semantic inference. To this end, mostly WordNet is utilized. Yet WordNet is limited, especially for inference between predicates. To help ﬁlling this gap, we present an algorithm that generates inference rules between predicates from FrameNet. Our experiment shows that the novel resource is effective and complements WordNet in terms of rule coverage. 
 argument of an upward-entailing operator by a su-  Researchers in textual entailment have begun to consider inferences involving downward-entailing operators, an interesting and important class of lexical items that change the way inferences are made. Recent work proposed a method for learning English downward-entailing operators that requires access to a high-quality col-  perset (a more general version); in our case, the set ‘opium use’ was replaced by the superset ‘narcotic use’. Downward-entailing (DE) (also known as downward monotonic or monotone decreasing) operators violate this default inference rule: with DE operators, reasoning instead goes from “sets to subsets”. An example is the word ‘bans’:  lection of negative polarity items (NPIs).  ‘The law bans opium use’  However, English is one of the very few  ⇒ (⇐)  languages for which such a list exists. We propose the ﬁrst approach that can be ap-  ‘The law bans narcotic use’.  plied to the many languages for which there is no pre-existing high-precision  Although DE behavior represents an exception to the default, DE operators are as a class rather com-  database of NPIs. As a case study, we  mon. They are also quite diverse in sense and  apply our method to Romanian and show that our method yields good results. Also,  even part of speech. Some are simple negations, such as ‘not’, but some other English DE opera-  we perform a cross-linguistic analysis that suggests interesting connections to some ﬁndings in linguistic typology.  tors are ‘without’, ‘reluctant to’, ‘to doubt’, and ‘to allow’.1 This variety makes them hard to ex- tract automatically.  
We establish the following characteristics of the task of perspective classiﬁcation: (a) using term frequencies in a document does not improve classiﬁcation achieved with absence/presence features; (b) for datasets allowing the relevant comparisons, a small number of top features is found to be as effective as the full feature set and indispensable for the best achieved performance, testifying to the existence of perspective-speciﬁc keywords. We relate our ﬁndings to research on word frequency distributions and to discourse analytic studies of perspective. 
In this paper, we study the problem of using an annotated corpus in English for the same natural language processing task in another language. While various machine translation systems are available, automated translation is still far from perfect. To minimize the noise introduced by translations, we propose to use only key ‘reliable” parts from the translations and apply structural correspondence learning (SCL) to ﬁnd a low dimensional representation shared by the two languages. We perform experiments on an EnglishChinese sentiment classiﬁcation task and compare our results with a previous cotraining approach. To alleviate the problem of data sparseness, we create extra pseudo-examples for SCL by making queries to a search engine. Experiments on real-world on-line review data demonstrate the two techniques can effectively improve the performance compared to previous work. 
Current work on automatic opinion mining has ignored opinion targets expressed by anaphorical pronouns, thereby missing a signiﬁcant number of opinion targets. In this paper we empirically evaluate whether using an off-the-shelf anaphora resolution algorithm can improve the performance of a baseline opinion mining system. We present an analysis based on two different anaphora resolution systems. Our experiments on a movie review corpus demonstrate, that an unsupervised anaphora resolution algorithm signiﬁcantly improves the opinion target extraction. We furthermore suggest domain and task speciﬁc extensions to an off-the-shelf algorithm which in turn yield signiﬁcant improvements. 
Automatic opinion recognition involves a number of related tasks, such as identifying the boundaries of opinion expression, determining their polarity, and determining their intensity. Although much progress has been made in this area, existing research typically treats each of the above tasks in isolation. In this paper, we apply a hierarchical parameter sharing technique using Conditional Random Fields for ﬁne-grained opinion analysis, jointly detecting the boundaries of opinion expressions as well as determining two of their key attributes — polarity and intensity. Our experimental results show that our proposed approach improves the performance over a baseline that does not exploit hierarchical structure among the classes. In addition, we ﬁnd that the joint approach outperforms a baseline that is based on cascading two separate components. 
This paper presents a joint optimization method of a two-step conditional random ﬁeld (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the ﬁrst CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outperforms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time signiﬁcantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy. 
Building an accurate Named Entity Recognition (NER) system for languages with complex morphology is a challenging task. In this paper, we present research that explores the feature space using both gold and bootstrapped noisy features to build an improved highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute). 
Classical Information Extraction (IE) systems ﬁll slots in domain-speciﬁc frames. This paper reports on SEQ, a novel open IE system that leverages a domainindependent frame to extract ordered sequences such as presidents of the United States or the most common causes of death in the U.S. SEQ leverages regularities about sequences to extract a coherent set of sequences from Web text. SEQ nearly doubles the area under the precision-recall curve compared to an extractor that does not exploit these regularities. 
We present a generative model of template-ﬁlling in which coreference resolution and role assignment are jointly determined. Underlying template roles ﬁrst generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%. 
It is a fundamental and important task to extract key phrases from documents. Generally, phrases in a document are not independent in delivering the content of the document. In order to capture and make better use of their relationships in key phrase extraction, we suggest exploring the Wikipedia knowledge to model a document as a semantic network, where both n-ary and binary relationships among phrases are formulated. Based on a commonly accepted assumption that the title of a document is always elaborated to reflect the content of a document and consequently key phrases tend to have close semantics to the title, we propose a novel semi-supervised key phrase extraction approach in this paper by computing the phrase importance in the semantic network, through which the influence of title phrases is propagated to the other phrases iteratively. Experimental results demonstrate the remarkable performance of this approach. 
We investigate a recently proposed Bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition, given a large corpus of written language data and a small corpus of speech transcripts. Experiments show that the method consistently outperforms linear interpolation which is typically used in such cases. 
We investigate hierarchical graphical models (HGMs) for automatically detecting decisions in multi-party discussions. Several types of dialogue act (DA) are distinguished on the basis of their roles in formulating decisions. HGMs enable us to model dependencies between observed features of discussions, decision DAs, and subdialogues that result in a decision. For the task of detecting decision regions, an HGM classiﬁer was found to outperform non-hierarchical graphical models and support vector machines, raising the F1-score to 0.80 from 0.55. 
Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using dictation than voice search. 
This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures signiﬁcantly improve automatic classiﬁcation of dialogue act labels for linguistic expressions of feedback. 
We propose a novel algorithm for sentiment summarization that takes account of informativeness and readability, simultaneously. Our algorithm generates a summary by selecting and ordering sentences taken from multiple review texts according to two scores that represent the informativeness and readability of the sentence order. The informativeness score is deﬁned by the number of sentiment expressions and the readability score is learned from the target corpus. We evaluate our method by summarizing reviews on restaurants. Our method outperforms an existing algorithm as indicated by its ROUGE score and human readability experiments. 
Two psycholinguistic and psychophysical experiments show that in order to efficiently extract polarity of written texts such as customerreviews on the Internet, one should concentrate computational efforts on messages in the final position of the text. 
One of the central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classiﬁcation. Rather unexpectedly, we ﬁnd the automatically generated rationales just as helpful as human rationales. 
We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. By encoding some basic linguistic information, the machine learning task is simpliﬁed, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difﬁculties. 
Hierarchical A∗ (HA∗) uses of a hierarchy of coarse grammars to speed up parsing without sacriﬁcing optimality. HA∗ prioritizes search in reﬁned grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A∗ (BHA∗), a modiﬁed Hierarchial A∗ algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix ﬁner outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA∗ substantially outperforms HA∗ when the hierarchy contains only very coarse grammars, while achieving comparable performance on more reﬁned hierarchies. 
We evaluate the effect of adding parse features to a leading model of preposition usage. Results show a signiﬁcant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an ESL error detection task. Analysis of the parser output indicates that it is robust enough in the face of noisy non-native writing to extract useful information. 
Distributional similarity is a classic technique for entity set expansion, where the system is given a set of seed entities of a particular class, and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds. This paper shows that a machine learning model called positive and unlabeled learning (PU learning) can model the set expansion problem better. Based on the test results of 10 corpora, we show that a PU learning technique outperformed distributional similarity significantly. 
Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner. 
We present a novel system that helps nonexperts ﬁnd sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classiﬁer based on a ﬁxed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%. 
We initiate a study comparing effectiveness of the transformed spaces learned by recently proposed supervised, and semisupervised metric learning algorithms to those generated by previously proposed unsupervised dimensionality reduction methods (e.g., PCA). Through a variety of experiments on different realworld datasets, we ﬁnd IDML-IT, a semisupervised metric learning algorithm to be the most effective. 
The main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries. This is motivated by empirical evidence from TAC 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries. We report encouraging preliminary results comparable to those attained by participating systems at TAC 2009. 
We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efﬁcient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. 
In this paper we describe an intuitionistic method for dependency parsing, where a classiﬁer is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classiﬁcation instances rather than a complete tree. Experiments show that, the classiﬁer trained on the projected classiﬁcation instances signiﬁcantly outperforms previous projected dependency parsers. More importantly, when this classiﬁer is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 
This paper proposes a dependency parsing method that uses bilingual constraints to improve the accuracy of parsing bilingual texts (bitexts). In our method, a targetside tree fragment that corresponds to a source-side tree fragment is identiﬁed via word alignment and mapping rules that are automatically learned. Then it is veriﬁed by checking the subtree list that is collected from large scale automatically parsed data on the target side. Our method, thus, requires gold standard trees only on the source side of a bilingual corpus in the training phase, unlike the joint parsing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English. 
We present an efﬁcient algorithm for computing the weakest readings of semantically ambiguous sentences. A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars. 
This paper presents a supervised approach for identifying generic noun phrases in context. Generic statements express rulelike knowledge about kinds or events. Therefore, their identiﬁcation is important for the automatic construction of knowledge bases. In particular, the distinction between generic and non-generic statements is crucial for the correct encoding of generic and instance-level information. Generic expressions have been studied extensively in formal semantics. Building on this work, we explore a corpus-based learning approach for identifying generic NPs, using selections of linguistically motivated features. Our results perform well above the baseline and existing prior work. 
Name ambiguity problem has raised urgent demands for efficient, high-quality named entity disambiguation methods. In recent years, the increasing availability of large-scale, rich semantic knowledge sources (such as Wikipedia and WordNet) creates new opportunities to enhance the named entity disambiguation by developing algorithms which can exploit these knowledge sources at best. The problem is that these knowledge sources are heterogeneous and most of the semantic knowledge within them is embedded in complex structures, such as graphs and networks. This paper proposes a knowledge-based method, called Structural Semantic Relatedness (SSR), which can enhance the named entity disambiguation by capturing and leveraging the structural semantic knowledge in multiple knowledge sources. Empirical results show that, in comparison with the classical BOW based methods and social network based methods, our method can significantly improve the disambiguation performance by respectively 8.7% and 14.7%. 
We introduce a novel mechanism for incorporating articulatory dynamics into speech recognition with the theory of task dynamics. This system reranks sentencelevel hypotheses by the likelihoods of their hypothetical articulatory realizations which are derived from relationships learned with aligned acoustic/articulatory data. Experiments compare this with two baseline systems, namely an acoustic hidden Markov model and a dynamic Bayes network augmented with discretized representations of the vocal tract. Our system based on task dynamics reduces worderror rates signiﬁcantly by 10.2% relative to the best baseline models. 
We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difﬁcult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the system learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs signiﬁcantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise. 
In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches. 
We present a grand challenge to build a corpus that will include all of the world’s languages, in a consistent structure that permits large-scale cross-linguistic processing, enabling the study of universal linguistics. The focal data types, bilingual texts and lexicons, relate each language to one of a set of reference languages. We propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language. We call on the computational linguistics community to begin work on this Universal Corpus, pursuing the many strands of activity described here, as their contribution to the global effort to document the world’s linguistic heritage before more languages fall silent. 
Bilingual lexicons are fundamental resources. Modern automated lexicon generation methods usually require parallel corpora, which are not available for most language pairs. Lexicons can be generated using non-parallel corpora or a pivot language, but such lexicons are noisy. We present an algorithm for generating a high quality lexicon from a noisy one, which only requires an independent corpus for each language. Our algorithm introduces non-aligned signatures (NAS), a cross-lingual word context similarity score that avoids the over-constrained and inefﬁcient nature of alignment-based methods. We use NAS to eliminate incorrect translations from the generated lexicon. We evaluate our method by improving the quality of noisy Spanish-Hebrew lexicons generated from two pivot English lexicons. Our algorithm substantially outperforms other lexicon generation methods. 
As described in this paper, we propose a new automatic evaluation method for machine translation using noun-phrase chunking. Our method correctly determines the matching words between two sentences using corresponding noun phrases. Moreover, our method determines the similarity between two sentences in terms of the noun-phrase order of appearance. Evaluation experiments were conducted to calculate the correlation among human judgments, along with the scores produced using automatic evaluation methods for MT outputs obtained from the 12 machine translation systems in NTCIR7. Experimental results show that our method obtained the highest correlations among the methods in both sentence-level adequacy and ﬂuency. 
Information-extraction (IE) systems seek to distill semantic relations from naturallanguage text, but most systems use supervised learning of relation-speciﬁc examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? This paper presents WOE, an open IE system which improves dramatically on TextRunner’s precision and recall. The key to WOE’s performance is a novel form of self-supervised learning for open extractors — using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE’s extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher. 
As information extraction (IE) becomes more central to enterprise applications, rule-based IE engines have become increasingly important. In this paper, we describe SystemT, a rule-based IE system whose basic design removes the expressivity and performance limitations of current systems based on cascading grammars. SystemT uses a declarative rule language, AQL, and an optimizer that generates high-performance algebraic execution plans for AQL rules. We compare SystemT’s approach against cascading grammars, both theoretically and with a thorough experimental evaluation. Our results show that SystemT can deliver result quality comparable to the state-of-theart and an order of magnitude higher annotation throughput. 
We present a method for extracting social networks from literature, namely, nineteenth-century British novels and serials. We derive the networks from dialogue interactions, and thus our method depends on the ability to determine when two characters are in conversation. Our approach involves character name chunking, quoted speech attribution and conversation detection given the set of quotes. We extract features from the social networks and examine their correlation with one another, as well as with metadata such as the novel’s setting. Our results provide evidence that the majority of novels in this time period do not ﬁt two characterizations provided by literacy scholars. Instead, our results suggest an alternative explanation for differences in social networks. 
The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus. But word appears to be too fine-grained in some cases such as non-compositional phrasal equivalences, where no clear word alignments exist. Using words as inputs to PBSMT pipeline has inborn deficiency. This paper proposes pseudo-word as a new start point for PB-SMT pipeline. Pseudo-word is a kind of basic multi-word expression that characterizes minimal sequence of consecutive words in sense of translation. By casting pseudo-word searching problem into a parsing framework, we search for pseudo-words in a monolingual way and a bilingual synchronous way. Experiments show that pseudo-word significantly outperforms word for PB-SMT model in both travel translation domain and news translation domain. 
 We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can eﬃciently extract a ranked k-best list. We score a given alignment within the forest with a ﬂexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system. 
Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modiﬁers. We show how to ground scalar modiﬁer meaning based on data collected from the Web. We learn scales between modiﬁers and infer the extent to which a given answer conveys ‘yes’ or ‘no’. To evaluate the methods, we collected examples of question–answer pairs involving scalar modiﬁers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’. Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference. 
Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefﬁcient dialogues. In this paper we present a model we refer to as Importance-Driven Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We ﬁnd that Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artiﬁcial collaborative slot-ﬁlling domain. The negotiative nature of this model creates efﬁcient dialogues, and supports the improvement of mixed-initiative interaction. 
One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological ﬁelds, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological ﬁeld information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological ﬁelds into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically signiﬁcantly. 
The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model. 
Once released, treebanks tend to remain unchanged despite any shortcomings in their depth of linguistic analysis or coverage of speciﬁc phenomena. Instead, separate resources are created to address such problems. In this paper we show how to improve the quality of a treebank, by integrating resources and implementing improved analyses for speciﬁc constructions. We demonstrate this rebanking process by creating an updated version of CCGbank that includes the predicate-argument structure of both verbs and nouns, baseNP brackets, verb-particle constructions, and restrictive and non-restrictive nominal modiﬁers; and evaluate the impact of these changes on a statistical parser. 
In this paper we present BabelNet – a very large, wide-coverage multilingual semantic network. The resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition Machine Translation is also applied to enrich the resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource. 
The core-adjunct argument distinction is a basic one in the theory of argument structure. The task of distinguishing between the two has strong relations to various basic NLP tasks such as syntactic parsing, semantic role labeling and subcategorization acquisition. This paper presents a novel unsupervised algorithm for the task that uses no supervised models, utilizing instead state-of-the-art syntactic induction algorithms. This is the ﬁrst work to tackle this task in a fully unsupervised scenario. 
Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classiﬁcation model promotes a simpler feature space that limits the potential overﬁtting effects. The large scale empirical study here discussed conﬁrms that state-of-art accuracy can be obtained for out-of-domain evaluations. 
Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words’ context proﬁles obtained from a limited amount of data. This paper proposes a Bayesian method for robust distributional word similarities. The method uses a distribution of context proﬁles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution. When the context proﬁles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefﬁcient, we can derive an analytical form that allows efﬁcient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures. 
The variety of engaging interactions among users in social medial distinguishes it from traditional Web media. Such a feature should be utilized while attempting to provide intelligent services to social media participants. In this article, we present a framework to recommend relevant information in Internet forums and blogs using user comments, one of the most representative of user behaviors in online discussion. When incorporating user comments, we consider structural, semantic, and authority information carried by them. One of the most important observation from this work is that semantic contents of user comments can play a fairly different role in a different form of social media. When designing a recommendation system for this purpose, such a difference must be considered with caution. 
This paper explores the use of clickthrough data for query spelling correction. First, large amounts of query-correction pairs are derived by analyzing users' query reformulation behavior encoded in the clickthrough data. Then, a phrase-based error model that accounts for the transformation probability between multi-term phrases is trained and integrated into a query speller system. Experiments are carried out on a human-labeled data set. Results show that the system using the phrase-based error model outperforms significantly its baseline systems. 
This research explores the idea of inducing domain-speciﬁc semantic class taggers using only a domain-speciﬁc text collection and seed words. The learning process begins by inducing a classiﬁer that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classiﬁer then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classiﬁers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classiﬁers to dynamically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts. 
Many researchers are trying to use information extraction (IE) to create large-scale knowledge bases from natural language text on the Web. However, the primary approach (supervised learning of relation-speciﬁc extractors) requires manually-labeled training data for each relation and doesn’t scale to the thousands of relations encoded in Web text. This paper presents LUCHS, a self-supervised, relation-speciﬁc IE system which learns 5025 relations — more than an order of magnitude greater than any previous approach — with an average F1 score of 61%. Crucial to LUCHS’s performance is an automated system for dynamic lexicon learning, which allows it to learn accurately from heuristically-generated training data, which is often noisy and sparse. 
Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate OntoUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches. 
While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 
Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining ﬁne-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a ﬁne-grained description of the syntactic property and a semantic representation of a sentence. We extract ﬁne-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English translations testiﬁed the effectiveness of our approach. 
The deﬁnition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the deﬁnitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy. 
We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highestscoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining signiﬁcant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text. 
We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning. 
Finding allowable places in words to insert hyphens is an important practical problem. The algorithm that is used most often nowadays has remained essentially unchanged for 25 years. This method is the TEX hyphenation algorithm of Knuth and Liang. We present here a hyphenation method that is clearly more accurate. The new method is an application of conditional random ﬁelds. We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch. Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages. 
This paper demonstrates that the use of ensemble methods and carefully calibrating the decision threshold can signiﬁcantly improve the performance of machine learning methods for morphological word decomposition. We employ two algorithms which come from a family of generative probabilistic models. The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. The advantage of this model family is that it can learn from small datasets and easily generalises to larger datasets. The ﬁrst algorithm PROMODES, which participated in the Morpho Challenge 2009 (an international competition for unsupervised morphological analysis) employs a lower order model whereas the second algorithm PROMODES-H is a novel development of the ﬁrst using a higher order model. We present the mathematical description for both algorithms, conduct experiments on the morphologically rich language Zulu and compare characteristics of both algorithms based on the experimental results. 
If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and ﬁnd that each of the three word representations improves the accuracy of these baselines. We ﬁnd further improvements by combining diﬀerent word representations. You can download our word features, for oﬀ-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/ 
Automatically identifying the polarity of words is a very important task in Natural Language Processing. It has applications in text classiﬁcation, text ﬁltering, analysis of product review, analysis of responses to surveys, and mining online discussions. We propose a method for identifying the polarity of words. We apply a Markov random walk model to a large word relatedness graph, producing a polarity estimate for any given word. A key advantage of the model is its ability to accurately and quickly assign a polarity sign and magnitude to any word. The method could be used both in a semi-supervised setting where a training set of labeled words is used, and in an unsupervised setting where a handful of seeds is used to deﬁne the two polarity classes. The method is experimentally tested using a manually labeled set of positive and negative words. It outperforms the state of the art methods in the semi-supervised setting. The results in the unsupervised setting is comparable to the best reported values. However, the proposed method is faster and does not need a large corpus. 
Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product’s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a deﬁned Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HLSOT approach is easily generalized to labeling a mix of reviews of more than one products. 
In this paper, we adopt two views, personal and impersonal views, and systematically employ them in both supervised and semi-supervised sentiment classification. Here, personal views consist of those sentences which directly express speaker’s feeling and preference towards a target object while impersonal views focus on statements towards a target object for evaluation. To obtain them, an unsupervised mining approach is proposed. On this basis, an ensemble method and a co-training algorithm are explored to employ the two views in supervised and semi-supervised sentiment classification respectively. Experimental results across eight domains demonstrate the effectiveness of our proposed approach. 
The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the beneﬁts of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. We compare LDA-SP to several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP’s effectiveness at ﬁltering improper applications of inference rules, where we show substantial improvement over Pantel et al.’s system (Pantel et al., 2007). 
This paper describes the application of so-called topic models to selectional preference induction. Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word cooccurrences, are presented and evaluated on datasets of human plausibility judgements. Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data. 
This paper improves the use of pseudowords as an evaluation framework for selectional preferences. While pseudowords originally evaluated word sense disambiguation, they are now commonly used to evaluate selectional preferences. A selectional preference model ranks a set of possible arguments for a verb by their semantic ﬁt to the verb. Pseudo-words serve as a proxy evaluation for these decisions. The evaluation takes an argument of a verb like drive (e.g. car), pairs it with an alternative word (e.g. car/rock), and asks a model to identify the original. This paper studies two main aspects of pseudoword creation that affect performance results. (1) Pseudo-word evaluations often evaluate only a subset of the words. We show that selectional preferences should instead be evaluated on the data in its entirety. (2) Different approaches to selecting partner words can produce overly optimistic evaluations. We offer suggestions to address these factors and present a simple baseline that outperforms the state-ofthe-art by 13% absolute on a newspaper domain. 
We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but ﬁnd that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets. 
We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation. We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem. Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words. We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts. We obtain ﬁnal BLEU scores of 19.35 (conditional probability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system. This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu. 
Several attempts have been made to learn phrase translation probabilities for phrasebased statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with overﬁtting. We describe a novel leavingone-out approach to prevent over-ﬁtting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering models in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%. 
The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efﬁciently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, CARPEDIEM (Esposito and Radicioni, 2009). 
We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efﬁciently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization. 
Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efﬁciency stems here from the sparsity induced by the use of a 1 penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efﬁciently and that very large models are able to improve the accuracy, while delivering compact parameter sets. 
 Dominance links were introduced in grammars to model long distance scrambling phenomena, motivating the deﬁnition of multiset-valued linear indexed grammars (MLIGs) by Rambow (1994b), and inspiring quite a few recent formalisms. It turns out that MLIGs have since been rediscovered and reused in a variety of contexts, and that the complexity of their emptiness problem has become the key to several open questions in computer science. We survey complexity results and open issues on MLIGs and related formalisms, and provide new complexity bounds for some linguistically motivated restrictions.  
Linear Context-Free Rewriting Systems (LCFRSs) are a grammar formalism capable of modeling discontinuous phrases. Many parsing applications use LCFRSs where the fan-out (a measure of the discontinuity of phrases) does not exceed 2. We present an efﬁcient algorithm for optimal reduction of the length of production right-hand side in LCFRSs with fan-out at most 2. This results in asymptotical running time improvement for known parsing algorithms for this class. 
Combinatory Categorial Grammar (CCG) is generally construed as a fully lexicalized formalism, where all grammars use one and the same universal set of rules, and crosslinguistic variation is isolated in the lexicon. In this paper, we show that the weak generative capacity of this ‘pure’ form of CCG is strictly smaller than that of CCG with grammar-speciﬁc rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 
To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the ﬁrst systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference information, and summarization speciﬁc features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a speciﬁc input. 
Identifying background (context) information in scientiﬁc articles can help scholars understand major contributions in their research area more easily. In this paper, we propose a general framework based on probabilistic inference to extract such context information from scientiﬁc papers. We model the sentences in an article and their lexical similarities as a Markov Random Field tuned to detect the patterns that context data create, and employ a Belief Propagation mechanism to detect likely context sentences. We also address the problem of generating surveys of scientiﬁc papers. Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone. 
In this paper we present a joint content selection and compression model for single-document summarization. The model operates over a phrase-based representation of the source document which we obtain by merging information from PCFG parse trees and dependency graphs. Using an integer linear programming formulation, the model learns to select and combine phrases subject to length, coverage and grammar constraints. We evaluate the approach on the task of generating “story highlights”—a small number of brief, self-contained sentences that allow readers to quickly gather information on news stories. Experimental results show that the model’s output is comparable to human-written highlights in terms of both grammaticality and content. 
In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the ﬁrst stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 
We present a method for automatically generating focused and accurate topicspeciﬁc subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the ﬁeld of media analysis, describe a bootstrapping method for generating a topic-speciﬁc lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opinion retrieval system. 
Subjectivity analysis is a rapidly growing ﬁeld of study. Along with its applications to various NLP tasks, much work have put efforts into multilingual subjectivity learning from existing resources. Multilingual subjectivity analysis requires language-independent criteria for comparable outcomes across languages. This paper proposes to measure the multilanguage-comparability of subjectivity analysis tools, and provides meaningful comparisons of multilingual subjectivity analysis from various points of view. 
The adoption of Machine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs. 
We propose a translation recommendation framework to integrate Statistical Machine Translation (SMT) output with Translation Memory (TM) systems. The framework recommends SMT outputs to a TM user when it predicts that SMT outputs are more suitable for post-editing than the hits provided by the TM. We describe an implementation of this framework using an SVM binary classiﬁer. We exploit methods to ﬁne-tune the classiﬁer and investigate a variety of features of different types. We rely on automatic MT evaluation metrics to approximate human judgements in our experiments. Experimental results show that our system can achieve 0.85 precision at 0.89 recall, excluding exact matches. Furthermore, it is possible for the end-user to achieve a desired balance between precision and recall by adjusting conﬁdence levels. 
We observe that (1) how a given named entity (NE) is translated (i.e., either semantically or phonetically) depends greatly on its associated entity type, and (2) entities within an aligned pair should share the same type. Also, (3) those initially detected NEs are anchors, whose information should be used to give certainty scores when selecting candidates. From this basis, an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between Chinese and English. It adopts a new mapping type ratio feature (which is the proportion of NE internal tokens that are semantically translated), enforces an entity type consistency constraint, and utilizes additional monolingual candidate certainty factors (based on those NE anchors). The experiments show that this novel approach has substantially raised the type-sensitive F-score of identified NE-pairs from 68.4% to 81.7% (42.1% F-score imperfection reduction) in our Chinese-English NE alignment task. 
In this paper, we propose a novel approach to automatic generation of summary templates from given collections of summary articles. This kind of summary templates can be useful in various applications. We ﬁrst develop an entity-aspect LDA model to simultaneously cluster both sentences and words into aspects. We then apply frequent subtree pattern mining on the dependency parse trees of the clustered and labeled sentences to discover sentence patterns that well represent the aspects. Key features of our method include automatic grouping of semantically related sentence patterns and automatic identiﬁcation of template slots that need to be ﬁlled in. We apply our method on ﬁve Wikipedia entity categories and compare our method with two baseline methods. Both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantages of our method. 
Comparing one thing with another is a typical part of human decision making process. However, it is not always easy to know what to compare and what are the alternatives. To address this difficulty, we present a novel way to automatically mine comparable entities from comparative questions that users posted online. To ensure high precision and high recall, we develop a weakly-supervised bootstrapping method for comparative question identification and comparable entity extraction by leveraging a large online question archive. The experimental results show our method achieves F1measure of 82.5% in comparative question identification and 83.3% in comparable entity extraction. Both significantly outperform an existing state-of-the-art method. 
This paper describes a series of experiments to test the hypothesis that the parallel application of multiple NLP tools and the integration of their results improves the correctness and robustness of the resulting analysis. It is shown how annotations created by seven NLP tools are mapped onto toolindependent descriptions that are deﬁned with reference to an ontology of linguistic annotations, and how a majority vote and ontological consistency constraints can be used to integrate multiple alternative analyses of the same token in a consistent way. For morphosyntactic (parts of speech) and morphological annotations of three German corpora, the resulting merged sets of ontological descriptions are evaluated in comparison to (ontological representation of) existing reference annotations. 
We describe the semi-automatic adaptation of a TimeML annotated corpus from English to Portuguese, a language for which TimeML annotated data was not available yet. In order to validate this adaptation, we use the obtained data to replicate some results in the literature that used the original English data. The fact that comparable results are obtained indicates that our approach can be used successfully to rapidly create semantically annotated resources for new languages. 
The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difﬁcult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classiﬁcation method for automatic noun compound interpretation. 
Automatic processing of metaphor can be clearly divided into two subtasks: metaphor recognition (distinguishing between literal and metaphorical language in a text) and metaphor interpretation (identifying the intended literal meaning of a metaphorical expression). Both of them have been repeatedly addressed in NLP. This paper is the ﬁrst comprehensive and systematic review of the existing computational models of metaphor, the issues of metaphor annotation in corpora and the available resources. 
We present a game-theoretic model of bargaining over a metaphor in the context of political communication, ﬁnd its equilibrium, and use it to rationalize observed linguistic behavior. We argue that game theory is well suited for modeling discourse as a dynamic resulting from a number of conﬂicting pressures, and suggest applications of interest to computational linguists. 
One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data. In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model. Experiments on joint parsing and named entity recognition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data. 
We outline different methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their representation in the training data and ﬂagging anomalous ones. By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process. 
In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 
Prior use of machine learning in genre classiﬁcation used a list of labels as classiﬁcation categories. However, genre classes are often organised into hierarchies, e.g., covering the subgenres of ﬁction. In this paper we present a method of using the hierarchy of labels to improve the classiﬁcation accuracy. As a testbed for this approach we use the Brown Corpus as well as a range of other corpora, including the BNC, HGC and Syracuse. The results are not encouraging: apart from the Brown corpus, the improvements of our structural classiﬁer over the ﬂat one are not statistically signiﬁcant. We discuss the relation between structural learning performance and the visual and distributional balance of the label hierarchy, suggesting that only balanced hierarchies might proﬁt from structural learning. 
This paper presents a framework for automatically processing information coming from community Question Answering (cQA) portals with the purpose of generating a trustful, complete, relevant and succinct summary in response to a question. We exploit the metadata intrinsically present in User Generated Content (UGC) to bias automatic multi-document summarization techniques toward high quality information. We adopt a representation of concepts alternative to n-grams and propose two concept-scoring functions based on semantic overlap. Experimental results on data drawn from Yahoo! Answers demonstrate the effectiveness of our method in terms of ROUGE scores. We show that the information contained in the best answers voted by users of cQA portals can be successfully complemented by our method. 
In recent years, research in natural language processing has increasingly focused on normalizing SMS messages. Different well-deﬁned approaches have been proposed, but the problem remains far from being solved: best systems achieve a 11% Word Error Rate. This paper presents a method that shares similarities with both spell checking and machine translation approaches. The normalization part of the system is entirely based on models trained from a corpus. Evaluated in French by 10-fold-cross validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score. 
Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of reﬁning the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets. 
Event extraction is a particularly challenging type of information extraction (IE). Most current event extraction systems rely on local information at the phrase or sentence level. However, this local context may be insufficient to resolve ambiguities in identifying particular types of events; information from a wider scope can serve to resolve some of these ambiguities. In this paper, we use document level information to improve the performance of ACE event extraction. In contrast to previous work, we do not limit ourselves to information about events of the same type, but rather use information about other types of events to make predictions or resolve ambiguities regarding a given event. We learn such relationships from the training corpus and use them to help predict the occurrence of events and event arguments in a text. Experiments show that we can get 9.0% (absolute) gain in trigger (event) classification, and more than 8% gain for argument (role) classification in ACE event extraction. 
In-vehicle dialogue systems often contain more than one application, e.g. a navigation and a telephone application. This means that the user might, for example, interrupt the interaction with the telephone application to ask for directions from the navigation application, and then resume the dialogue with the telephone application. In this paper we present an analysis of interruption and resumption behaviour in human-human in-vehicle dialogues and also propose some implications for resumption strategies in an in-vehicle dialogue system. 
 We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difﬁcult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths. 
Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less redundant and more coherent based upon manual quality evaluations. 
This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 
Wikipedia articles in different languages are connected by interwiki links that are increasingly being recognized as a valuable source of cross-lingual information. Unfortunately, large numbers of links are imprecise or simply wrong. In this paper, techniques to detect such problems are identiﬁed. We formalize their removal as an optimization task based on graph repair operations. We then present an algorithm with provable properties that uses linear programming and a region growing technique to tackle this challenge. This allows us to transform Wikipedia into a much more consistent multilingual register of the world’s entities and concepts. 
We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and ﬁnd that we get an order of magnitude increase in performance rates of improvement. 
In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classiﬁers. We compare classiﬁers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance. 
This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 
Strictly Piecewise (SP) languages are a subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we deﬁne SP distributions and show that they can be efﬁciently estimated from positive data. 
This paper provides a uniﬁed, learningtheoretic analysis of several learnable classes of languages discussed previously in the literature. The analysis shows that for these classes an incremental, globally consistent, locally conservative, set-driven learner always exists. Additionally, the analysis provides a recipe for constructing new learnable classes. Potential applications include learnable models for aspects of natural language and cognition. 
We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms. 
Cross-language document summarization is a task of producing a summary in one language for a document set in a different language. Existing methods simply use machine translation for document translation or summary translation. However, current machine translation services are far from satisfactory, which results in that the quality of the cross-language summary is usually very poor, both in readability and content. In this paper, we propose to consider the translation quality of each sentence in the English-to-Chinese cross-language summarization process. First, the translation quality of each English sentence in the document set is predicted with the SVM regression method, and then the quality score of each sentence is incorporated into the summarization process. Finally, the English sentences with high translation quality and high informativeness are selected and translated to form the Chinese summary. Experimental results demonstrate the effectiveness and usefulness of the proposed approach. 
Automated summarization methods can be deﬁned as “language-independent,” if they are not based on any languagespeciﬁc knowledge. Such methods can be used for multilingual summarization deﬁned by Mani (2001) as “processing several languages, with summary in the same language as input.” In this paper, we introduce MUSE, a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. We tested our methodology on two languages—English and Hebrew—and evaluated its performance with ROUGE-1 Recall vs. stateof-the-art extractive summarization approaches. Our results show that MUSE performs better than the best known multilingual approach (TextRank1) in both languages. Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages. 
We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a ﬁxed grammar. 
We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of ﬁrst- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the ﬁrst time that an unsupervised method has been applied to this task. 
We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations. A simple and efﬁcient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts. We consider the generative semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts. 
Most supervised language processing systems show a signiﬁcant drop-off in performance when they are tested on text that comes from a domain signiﬁcantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on ﬁction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text. 
We describe a novel approach to unsupervised learning of the events that make up a script, along with constraints on their temporal ordering. We collect naturallanguage descriptions of script-speciﬁc event sequences from volunteers over the Internet. Then we compute a graph representation of the script’s temporal structure using a multiple sequence alignment algorithm. The evaluation of our system shows that we outperform two informed baselines. 
A fundamental step in sentence comprehension involves assigning semantic roles to sentence constituents. To accomplish this, the listener must parse the sentence, ﬁnd constituents that are candidate arguments, and assign semantic roles to those constituents. Each step depends on prior lexical and syntactic knowledge. Where do children learning their ﬁrst languages begin in solving this problem? In this paper we focus on the parsing and argumentidentiﬁcation steps that precede Semantic Role Labeling (SRL) training. We combine a simpliﬁed SRL with an unsupervised HMM part of speech tagger, and experiment with psycholinguisticallymotivated ways to label clusters resulting from the HMM so that they can be used to parse input for the SRL system. The results show that proposed shallow representations of sentence structure are robust to reductions in parsing accuracy, and that the contribution of alternative representations of sentence structure to successful semantic role labeling varies with the integrity of the parsing and argumentidentiﬁcation stages. 
Substantial research effort has been invested in recent decades into the computational study and automatic processing of multi-party conversation. While most aspects of conversational speech have beneﬁted from a wide availability of analytic, computationally tractable techniques, only qualitative assessments are available for characterizing multi-party turn-taking. The current paper attempts to address this deﬁciency by ﬁrst proposing a framework for computing turn-taking model perplexity, and then by evaluating several multi-participant modeling approaches. Experiments show that direct multi-participant models do not generalize to held out data, and likely never will, for practical reasons. In contrast, the Extended-Degree-of-Overlap model represents a suitable candidate for future work in this area, and is shown to successfully predict the distribution of speech in time and across participants in previously unseen conversations. 
We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies signiﬁcantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are signiﬁcantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the ﬁrst to show that adding predictive “lower level” features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS. 
English noun/verb (N/V) pairs (contract, cement) have undergone complex patterns of change between 3 stress patterns for several centuries. We describe a longitudinal dataset of N/V pair pronunciations, leading to a set of properties to be accounted for by any computational model. We analyze the dynamics of 5 dynamical systems models of linguistic populations, each derived from a model of learning by individuals. We compare each model’s dynamics to a set of properties observed in the N/V data, and reason about how assumptions about individual learning affect population-level dynamics. 
A central problem in historical linguistics is the identiﬁcation of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model signiﬁcantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words. 
Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and ﬁnd that our algorithm has the potential to be a viable, practical method for efﬁciently solving decipherment problems. 
In this paper we propose a method for the automatic decipherment of lost languages. Given a non-parallel corpus in a known related language, our model produces both alphabetic mappings and translations of words into their corresponding cognates. We employ a non-parametric Bayesian framework to simultaneously capture both low-level character mappings and highlevel morphemic correspondences. This formulation enables us to encode some of the linguistic intuitions that have guided human decipherers. When applied to the ancient Semitic language Ugaritic, the model correctly maps 29 of 30 letters to their Hebrew counterparts, and deduces the correct Hebrew cognate for 60% of the Ugaritic words which have cognates in Hebrew. 
Weighted tree transducers have been proposed as useful formal models for representing syntactic natural language processing applications, but there has been little description of inference algorithms for these automata beyond formal foundations. We give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-ﬂy variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). 
A characterization of the expressive power of synchronous tree-adjoining grammars (STAGs) in terms of tree transducers (or equivalently, synchronous tree substitution grammars) is developed. Essentially, a STAG corresponds to an extended tree transducer that uses explicit substitution in both the input and output. This characterization allows the easy integration of STAG into toolkits for extended tree transducers. Moreover, the applicability of the characterization to several representational and algorithmic problems is demonstrated. 
Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efﬁciency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empirically, our algorithm yields up to a ﬁve-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our ﬁnal parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster. 
For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable. Unfortunately, most statistical classiﬁers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause. We augment a statistical classiﬁer with an integer linear program imposing hard linguistic constraints on the solution space output by the classiﬁer, capturing global distributional restrictions. We show that this improves labelling quality, in particular for argument grammatical functions, in an intrinsic evaluation, and, importantly, grammar coverage for treebankbased (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation. 
We present a simple but accurate parser which exploits both large tree fragments and symbol reﬁnement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and treesubstitution grammar learning. We require only simple, deterministic grammar symbol reﬁnement, in contrast to recent work on latent symbol reﬁnement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional speciﬁc contributions center on making implicit all-fragments parsing efﬁcient, including a coarse-to-ﬁne inference scheme and a new graph encoding. 
This paper explores joint syntactic and semantic parsing of Chinese to further improve the performance of both syntactic and semantic parsing, in particular the performance of semantic parsing (in this paper, semantic role labeling). This is done from two levels. Firstly, an integrated parsing approach is proposed to integrate semantic parsing into the syntactic parsing process. Secondly, semantic information generated by semantic parsing is incorporated into the syntactic parsing model to better capture semantic information in syntactic parsing. Evaluation on Chinese TreeBank, Chinese PropBank, and Chinese NomBank shows that our integrated parsing approach outperforms the pipeline parsing approach on n-best parse trees, a natural extension of the widely used pipeline parsing approach on the top-best parse tree. Moreover, it shows that incorporating semantic role-related information into the syntactic parsing model significantly improves the performance of both syntactic parsing and semantic parsing. To our best knowledge, this is the first research on exploring syntactic parsing and semantic role labeling for both verbal and nominal predicates in an integrated way. 
We present a new approach to crosslanguage text classiﬁcation that builds on structural correspondence learning, a recently proposed theory for domain adaptation. The approach uses unlabeled documents, along with a simple word translation oracle, in order to induce taskspeciﬁc, cross-lingual word correspondences. We report on analyses that reveal quantitative insights about the use of unlabeled data and the complexity of interlanguage correspondence modeling. We conduct experiments in the ﬁeld of cross-language sentiment classiﬁcation, employing English as source language, and German, French, and Japanese as target languages. The results are convincing; they demonstrate both the robustness and the competitiveness of the presented ideas. 
Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way. One common deﬁciency of existing topic models, though, is that they would not work well for extracting cross-lingual latent topics simply because words in different languages generally do not co-occur with each other. In this paper, we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages. Speciﬁcally, we propose a new topic model called Probabilistic Cross-Lingual Latent Semantic Analysis (PCLSA) which extends the Probabilistic Latent Semantic Analysis (PLSA) model by regularizing its likelihood function with soft constraints deﬁned based on a bilingual dictionary. Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract cross-lingual latent topics from multilingual text data. 
This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, ﬁne-grained word sense disambiguation, and detection of literal vs. nonliteral usages of potentially idiomatic expressions. In all three cases, we outperform state-of-the-art systems either quantitatively or statistically signiﬁcantly. 
This paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as “topic models” to produce a lowdimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) deﬁne distributions over trees. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The ﬁrst replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the ﬁrst one to learn aspects of the internal structure of proper names. 
We report on an experiment to track complex decision points in linguistic metadata annotation where the decision behavior of annotators is observed with an eyetracking device. As experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics. Our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and, more interestingly, indicates that fullscale context is mostly negligible – with the exception of semantic high-complexity cases. We then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models. Our data reveals that the cognitively founded model explains annotation costs (expressed in annotation time) more adequately than non-cognitive ones. 
A number of results in the study of realtime sentence comprehension have been explained by computational models as resulting from the rational use of probabilistic linguistic information. Many times, these hypotheses have been tested in reading by linking predictions about relative word difﬁculty to word-aggregated eye tracking measures such as go-past time. In this paper, we extend these results by asking to what extent reading is well-modeled as rational behavior at a ﬁner level of analysis, predicting not aggregate measures, but the duration and location of each ﬁxation. We present a new rational model of eye movement control in reading, the central assumption of which is that eye movement decisions are made to obtain noisy visual information as the reader performs Bayesian inference on the identities of the words in the sentence. As a case study, we present two simulations demonstrating that the model gives a rational explanation for between-word regressions. 
Probabilistic models of sentence comprehension are increasingly relevant to questions concerning human language processing. However, such models are often limited to syntactic factors. This paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic-based model of coreference resolution, which allows us to simulate how context interacts with syntax in a reading task. Our simulations show that a Weakly Interactive cognitive architecture can explain data which had been provided as evidence for the Strongly Interactive hypothesis. 
Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difﬁculty in understanding a sentence. Besides deﬁning standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a signiﬁcant, independent contribution. 
We challenge the NLP community to participate in a large-scale, distributed effort to design and build resources for developing and evaluating solutions to new and existing NLP tasks in the context of Recognizing Textual Entailment. We argue that the single global label with which RTE examples are annotated is insufﬁcient to effectively evaluate RTE system performance; to promote research on smaller, related NLP tasks, we believe more detailed annotation and evaluation are needed, and that this effort will beneﬁt not just RTE researchers, but the NLP community as a whole. We use insights from successful RTE systems to propose a model for identifying and annotating textual inference phenomena in textual entailment examples, and we present the results of a pilot annotation study that show this model is feasible and the results immediately useful. 
Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal. 
We propose a global algorithm for learning entailment relations between predicates. We deﬁne a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms. 
Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. In this paper, a deep belief network is proposed to model the semantic relevance for question-answer pairs. Observing the textual similarity between the community-driven questionanswering (cQA) dataset and the forum dataset, we present a novel learning strategy to promote the performance of our method on the social community datasets without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora. 
In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model deﬁned over phrases is superior to extractive methods. 
This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple webdocuments that contain information related to an image’s location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to signiﬁcantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns. 
This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on reference resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task. 
In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that ﬁlls in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efﬁcient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1 
We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we reﬁne approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manning’s Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-theart by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not beneﬁt from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus — nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP. 
We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift. Our method does not require any translated texts or token-level alignments. Instead, the phylogenetic prior couples languages at a parameter level. Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%. 
We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm ﬁrst identiﬁes landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task. 
We present a novel framework for automated extraction and approximation of numerical object attributes such as height and weight from the Web. Given an object-attribute pair, we discover and analyze attribute information for a set of comparable objects in order to infer the desired value. This allows us to approximate the desired numerical values even when no exact values can be found in the text. Our framework makes use of relation deﬁning patterns and WordNet similarity information. First, we obtain from the Web and WordNet a list of terms similar to the given object. Then we retrieve attribute values for each term in this list, and information that allows us to compare different objects in the list and to infer the attribute value range. Finally, we combine the retrieved data for all terms from the list to select or approximate the requested value. We evaluate our method using automated question answering, WordNet enrichment, and comparison with answers given in Wikipedia and by leading search engines. In all of these, our framework provides a signiﬁcant improvement. 
Deﬁnition extraction is the task of automatically identifying deﬁnitional sentences within texts. The task has proven useful in many research areas including ontology learning, relation extraction and question answering. However, current approaches – mostly focused on lexicosyntactic patterns – suffer from both low recall and precision, as deﬁnitional sentences occur in highly variable syntactic structures. In this paper, we propose WordClass Lattices (WCLs), a generalization of word lattices that we use to model textual deﬁnitions. Lattices are learned from a dataset of deﬁnitions from Wikipedia. Our method is applied to the task of definition and hypernym extraction and compares favorably to other pattern generalization methods proposed in the literature. 
An important relation in information extraction is the part-whole relation. Ontological studies mention several types of this relation. In this paper, we show that the traditional practice of initializing minimally-supervised algorithms with a single set that mixes seeds of different types fails to capture the wide variety of part-whole patterns and tuples. The results obtained with mixed seeds ultimately converge to one of the part-whole relation types. We also demonstrate that all the different types of part-whole relations can still be discovered, regardless of the type characterized by the initializing seeds. We performed our experiments with a state-ofthe-art information extraction algorithm. 
Determining the semantic intent of web queries not only involves identifying their semantic class, which is a primary focus of previous works, but also understanding their semantic structure. In this work, we formally deﬁne the semantic structure of noun phrase queries as comprised of intent heads and intent modiﬁers. We present methods that automatically identify these constituents as well as their semantic roles based on Markov and semi-Markov conditional random ﬁelds. We show that the use of semantic features and syntactic features signiﬁcantly contribute to improving the understanding performance. 
In a previous work of ours Chinnakotla et al. (2010) we introduced a novel framework for Pseudo-Relevance Feedback (PRF) called MultiPRF. Given a query in one language called Source, we used English as the Assisting Language to improve the performance of PRF for the source language. MulitiPRF showed remarkable improvement over plain Model Based Feedback (MBF) uniformly for 4 languages, viz., French, German, Hungarian and Finnish with English as the assisting language. This fact inspired us to study the effect of any source-assistant pair on MultiPRF performance from out of a set of languages with widely different characteristics, viz., Dutch, English, Finnish, French, German and Spanish. Carrying this further, we looked into the effect of using two assisting languages together on PRF. The present paper is a report of these investigations, their results and conclusions drawn therefrom. While performance improvement on MultiPRF is observed whatever the assisting language and whatever the source, observations are mixed when two assisting languages are used simultaneously. Interestingly, the performance improvement is more pronounced when the source and assisting languages are closely related, e.g., French and Spanish. 
Is it possible to use sense inventories to improve Web search results diversity for one word queries? To answer this question, we focus on two broad-coverage lexical resources of a different nature: WordNet, as a de-facto standard used in Word Sense Disambiguation experiments; and Wikipedia, as a large coverage, updated encyclopaedic resource which may have a better coverage of relevant senses in Web pages. Our results indicate that (i) Wikipedia has a much better coverage of search results, (ii) the distribution of senses in search results can be estimated using the internal graph structure of the Wikipedia and the relative number of visits received by each sense in Wikipedia, and (iii) associating Web pages to Wikipedia senses with simple and efﬁcient algorithms, we can produce modiﬁed rankings that cover 70% more Wikipedia senses than the original search engine rankings. 
There is a growing research interest in opinion retrieval as on-line users’ opinions are becoming more and more popular in business, social networks, etc. Practically speaking, the goal of opinion retrieval is to retrieve documents, which entail opinions or comments, relevant to a target subject specified by the user’s query. A fundamental challenge in opinion retrieval is information representation. Existing research focuses on document-based approaches and documents are represented by bag-of-word. However, due to loss of contextual information, this representation fails to capture the associative information between an opinion and its corresponding target. It cannot distinguish different degrees of a sentiment word when associated with different targets. This in turn seriously affects opinion retrieval performance. In this paper, we propose a sentence-based approach based on a new information representation, namely topic-sentiment word pair, to capture intra-sentence contextual information between an opinion and its target. Additionally, we consider inter-sentence information to capture the relationships among the opinions on the same topic. Finally, the two types of information are combined in a unified graph-based model, which can effectively rank the documents. Compared with existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement. 
Music Recommendation Systems often recommend individual songs, as opposed to entire albums. The challenge is to generate reviews for each song, since only full album reviews are available on-line. We developed a summarizer that combines information extraction and generation techniques to produce summaries of reviews of individual songs. We present an intrinsic evaluation of the extraction components, and of the informativeness of the summaries; and a user study of the impact of the song review summaries on users’ decision making processes. Users were able to make quicker and more informed decisions when presented with the summary as compared to the full album review. 
Most sentiment analysis approaches use as baseline a support vector machines (SVM) classiﬁer with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classiﬁcation accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide signiﬁcant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge. 
The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception ﬁfteen years ago. 
This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially inﬁnite number of features and categorical outcomes. The evaluation performed for solving both within- and cross-document event coreference shows signiﬁcant improvements of the models when compared against two baselines for this task. 
This paper explores the effect that different corpus conﬁgurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task deﬁnition, coding schemes, and features. They also expose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement. 
Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efﬁciency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically signiﬁcant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the ﬁrst time that a treeto-tree model can surpass tree-to-string counterparts. 
Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for signiﬁcant improvements in translation accuracy. 
We present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments. 
Weblogs are a source of human activity knowledge comprising valuable information such as facts, opinions and personal experiences. In this paper, we propose a method for mining personal experiences from a large set of weblogs. We define experience as knowledge embedded in a collection of activities or events which an individual or group has actually undergone. Based on an observation that experience-revealing sentences have a certain linguistic style, we formulate the problem of detecting experience as a classification task using various features including tense, mood, aspect, modality, experiencer, and verb classes. We also present an activity verb lexicon construction method based on theories of lexical semantics. Our results demonstrate that the activity verb lexicon plays a pivotal role among selected features in the classification performance and shows that our proposed method outperforms the baseline significantly. 
Graph-based semi-supervised learning (SSL) algorithms have been successfully used to extract class-instance pairs from large unstructured and structured text collections. However, a careful comparison of different graph-based SSL algorithms on that task has been lacking. We compare three graph-based SSL algorithms for class-instance acquisition on a variety of graphs constructed from different domains. We ﬁnd that the recently proposed MAD algorithm is the most effective. We also show that class-instance extraction can be signiﬁcantly improved by adding semantic information in the form of instance-attribute edges derived from an independently developed knowledge base. All of our code and data will be made publicly available to encourage reproducible research in this area. 
A challenging problem in open information extraction and text mining is the learning of the selectional restrictions of semantic relations. We propose a minimally supervised bootstrapping algorithm that uses a single seed and a recursive lexico-syntactic pattern to learn the arguments and the supertypes of a diverse set of semantic relations from the Web. We evaluate the performance of our algorithm on multiple semantic relations expressed using “verb”, “noun”, and “verb prep” lexico-syntactic patterns. Humanbased evaluation shows that the accuracy of the harvested information is about 90%. We also compare our results with existing knowledge base to outline the similarities and differences of the granularity and diversity of the harvested knowledge. 
We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 
Constructing an encoding of a concept lattice using short bit vectors allows for efﬁcient computation of join operations on the lattice. Join is the central operation any uniﬁcation-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a ﬁxed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to ﬁnd near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector techniques. 
One of the main obstacles to highperformance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-speciﬁc datasets. 
In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal. Many supervised WSD systems have been built, but the effort of creating the training corpus - annotated sense marked corpora - has always been a matter of concern. Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora. However such approaches have not proved effective, since they typically do not better Wordnet ﬁrst sense baseline accuracy. Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation. We show that if we have ANY sense marked corpora, be it from mixed domain or a speciﬁc domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain. We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus. Accuracy ﬁgures close to self domain training lend credence to the viability of our approach. Our contribution thus lies in ﬁnding a convenient middle ground between pure supervised and pure unsupervised WSD. Finally, our approach is not restricted to any speciﬁc set of target words, a departure from a commonly observed practice in domain speciﬁc WSD. 
Word Sense Disambiguation remains one of the most complex problems facing computational linguists to date. In this paper we present a system that combines evidence from a monolingual WSD system together with that from a multilingual WSD system to yield state of the art performance on standard All-Words data sets. The monolingual system is based on a modiﬁcation of the graph based state of the art algorithm In-Degree. The multilingual system is an improvement over an AllWords unsupervised approach, SALAAM. SALAAM exploits multilingual evidence as a means of disambiguation. In this paper, we present modiﬁcations to both of the original approaches and then their combination. We ﬁnally report the highest results obtained to date on the SENSEVAL 2 standard data set using an unsupervised method, we achieve an overall F measure of 64.58 using a voting scheme. 
Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents BAGEL, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that BAGEL can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved signiﬁcantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data. 
In this paper we develop a story generator that leverages knowledge inherent in corpora without requiring extensive manual involvement. A key feature in our approach is the reliance on a story planner which we acquire automatically by recording events, their participants, and their precedence relationships in a training corpus. Contrary to previous work our system does not follow a generate-and-rank architecture. Instead, we employ evolutionary search techniques to explore the space of possible stories which we argue are well suited to the story generation task. Experiments on generating simple children’s stories show that our system outperforms previous data-driven approaches. 
We present a natural language generation approach which models, exploits, and manipulates the non-linguistic context in situated communication, using techniques from AI planning. We show how to generate instructions which deliberately guide the hearer to a location that is convenient for the generation of simple referring expressions, and how to generate referring expressions with context-dependent adjectives. We implement and evaluate our approach in the framework of the Challenge on Generating Instructions in Virtual Environments, ﬁnding that it performs well even under the constraints of realtime generation. 
Despite its substantial coverage, NomBank does not account for all withinsentence arguments and ignores extrasentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially beneﬁt many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classiﬁcation model. Our results and analyses provide a baseline for future work on this emerging task. 
In many Chinese text processing tasks, Chinese word segmentation is a vital and required step. Various methods have been proposed to address this problem using machine learning algorithm in previous studies. In order to achieve high performance, many studies used external resources and combined with various machine learning algorithms to help segmentation. The goal of this paper is to construct a simple and effective Chinese word segmentation tool without external resources, that is, a closed test for Chinese word segmentation. We use training data to construct a vocabulary to combine maximum matching word segmentation results with sequence labeling methods including hidden Markov model (HMM) and conditional random fields (CRF). The major idea is to provide machine learning algorithm with ambiguity information via forward and backward maximum matching as well as unknown word information via vocabulary masking. The experimental results show that maximum matching and vocabulary masking can significantly improve the performance of HMM segmentation (F-measure: 0.812 → 0.948 → 0.953). Meanwhile, combining maximum matching with CRF achieves a performance with 0.953 and is improved to 0.963 via vocabulary masking. Keywords: Chinese Word Segmentation, Maximal Matching, Hidden Markov Model, Conditional Random Field, Vocabulary Masking 1. 序論 中文斷詞在中文的自然語言處理上，是非常重要的前置處理工作。許多中文的自然語言 相關的領域，例如：問答系統、自動摘要、文件檢索、機器翻譯、語音辨識…等，都需 要先處理中文斷詞，可見中文斷詞是個相當基礎且非常重要的工作。 所謂的「中文斷詞」就是將一連串的中文「字串」轉換成「詞串」的組合。例如： 「我昨天去台北」這個中文句子，透過中文斷詞的處理後變成「我／昨天／去／台北」， 也就是將｛我、昨、天、去、台、北｝字串轉成｛我、昨天、去、台北｝的詞串組合。 傳統上，處理中文斷詞會遇到的問題，大致可歸納為兩點，一是「歧義性」（ambiguity） 問題，二是「未知詞」（unknown word）問題。歧義性問題即是同一個中文字串，於不 同的文章當中，存在不同的斷詞結果，因此容易造成斷詞上的錯誤。歧義型態大致上可 以分為兩類：  交集型歧義（overlapping ambiguity） 令x, y, z 代表中文字元所組成的字串，若x、z、xy 與yz 皆為辭典中的詞，則xyz 的組合，於不同的文章中，可能會被斷詞成xy/z 或x/yz 等兩種不同的結果，則xyz 稱為「交集型歧義字串」。例如：「不可以」三個中文字元所組成的字串，辭典  結合長詞優先與序列標記之中文斷詞研究  163  中的詞含有「不、不可、可以」，「不可以」所組成的字串，在下列句子中，因 其上下文的不同而產生不同的斷詞結果：「不／可以／忘記」、「不可／以／營 利／為／目的」。  組合型歧義（covering ambiguity） 令x, y 代表中文字元所組成的字串，若x、y、xy 都是辭典中的詞，xy 的組合中， 可在不同的文章中，分別被斷詞成xy 或x/y，因為詞xy 是由x 與y等兩個不同的 詞所組成，因此xy 稱為「組合型歧義字串」。例如：「才能」二個字所組成的字 串，辭典中的詞有「才、能、才能」，在下列句子中「才能」組成的字串，將產 生不同的斷詞結果：「他／才能／非凡」、「只有／他／才／能／勝任」。 另外，「未知詞」則指辭典中未收錄的詞，包含了人名、地名、組織名、人名地名 組織名之縮寫、衍生詞、複合詞、數字型態等，由於人類所使用的語言會隨著社會不斷 改變，而持續地創造出新的用語，並且詞的衍生現象也非常地普遍，因此新詞會不斷的 出現，辭典永遠無法因應新詞產生的速度，所以會出現未知詞問題，斷詞系統必須能夠 處理未知詞，才可提高斷詞的正確性。 近年來的斷詞系統傾向於機器學習式（machine learning-based）演算法來解決中文 斷詞的問題，例如應用最大熵分類 Maximum Entropy (MaxEnt) (Xue, 2003)、向量支持機 Support Vector Machine (SVM) (Asahara, et al., 2003; Goh, et al., 2005) 、 Transformation-Based Learning Algorithm （TBL） (Lu, 2005)等分類演算法，另外以隱藏 式馬可夫模型 Hidden Markov Model （HMM） (Asahara, et al., 2005; Lu, 2005; Xue & Shen, 2003; Zhang, et al., 2003)的序列標記演算法等等，並且顯示了使用機器學習式演算 法做中文斷詞，確實可以達到很高的斷詞準確率。 本研究使用隱藏式馬可夫模型來解決中文斷詞的問題。雖然已有數篇研究同樣使用 隱藏式馬可夫模型來處理斷詞問題 (Asahara, et al.,2003; Lu, 2005; Xue & Shen, 2003; Zhang, et al., 2003)，但使用傳統的作法，隱藏式馬可夫模型在解決中文斷詞的問題上， 無法達到較好的斷詞效能（F-measure 約 80%），因此這些研究便結合了其他機器學習 演算法，以增加斷詞的效能。 我們的研究目的是希望只使用隱藏式馬可夫模型當成主要的演算法，並且應用「特 製化」（Specialization）的概念來提升隱藏式馬可夫模型的準確率。我們的作法是給予 隱藏式馬可夫模型更多的資訊，在完全不修改模型之訓練及測試過程的前提下，透過兩 階段特製化的方式，分別為擴充「觀測符號」，以及擴充「狀態符號」的方式，大大地 改善了隱藏式馬可夫模型的斷詞準確性。 於第一階段中，為了擴充觀測符號，我們使用最簡單也最常被使用的辭典比對式斷 詞演算法－「長詞優先法」（Maximum Matching Algorithm），來增加額外的資訊於隱 藏式馬可夫模型中，使得模型擁有更多的斷詞資訊做學習。第二階段擴充狀態符號的方 式，我們則使用詞彙式隱藏式馬可夫模型（Lexicalized HMM）的概念，也就是只根據某 些特製詞來做特製化，將狀態做延伸，來提升系統斷詞的效能。  164  林千翔 等  2. 相關研究 中文斷詞的研究已有相當歷史，但在近幾年仍陸續新的方法提出，底下我們分別就解決 歧義性及未知詞兩個問題分別做文獻回顧。 首先就斷詞歧義性問題，M.Li 等人 (Li, et al., 2003) 於 2003 年的研究中，提出一 種非監督式（unsupervised）訓練的方法，藉由訓練 Naïve Bayes 分類器，來解決中文斷 詞的交集型歧義問題，實驗結果可達到 94.13% 的準確率。另一方面，解決組合型歧義 比解決交集型歧義更加困難，主要的原因是，要解決組合型歧義則需要依賴更多的內文 資訊，如句法分析（syntactic）、語意分析（semantic） 以及前因後果的資訊（pragmatic information）等，才能正確的解決這類的歧義問題。1999 年 J. H. Zheng 等人 (Zheng & Wu, 1999) 使用規則式（rule-based method）的作法來處理組合型歧義，並達到 85 % 的準確 率。而 2002 年 X. Luo 等人 (Luo, et al., 2002) 的研究，則是使用類似於自然語言處理 領域中解決「詞義消歧」（word sense disambiguation）的問題，來解決組合型歧義問題， 該篇研究使用 TF.IDF 權重計算的公式，重新定義新的 TF 與 IDF 的公式，以此方式來 解決組合型歧義問題，達到 96.58 % 的準確率。 解決未知詞問題是做中文斷詞的另一個重要步驟。中研院陳克健博士等人於 1997 年開始，提出了三篇關於解決未知詞問題的研究 (Chen & Bai, 1997; Chen & Ma, 2002; Ma & Chen, 2003) ，最早於 1997 的研究 (Chen & Bai, 1997)，透過統計斷詞語料庫，產 生所有單一字元之已知詞的偵測規則。此階段的研究只能偵測出所有的單一字元的結果， 並未真正將未知詞擷取出來。2002 年的研究 (Chen & Ma, 2002) ，則是使用人工加上一 些統計的方法來建立擷取規則，將所有被偵測出屬於未知詞部分的單一字詞，透過擷取 規則以合併這些單一字詞而成為未知詞。實驗中測試 1,160 個未知詞，結果達到 89 % 的 擷取準確率。另外於 2003 年的研究 (Ma & Chen, 2003) 中，該研究將所有種類的未知 詞的構詞方式以 context free grammar 表示出來，並搭配 bottom-up merging algorithm 來 解決大部分統計特性低的未知詞擷取問題。實驗效能達到 75 % 的擷取準確率。其他解 決未知詞問題的研究，如 Zhang 等人 (Zhang, et al., 2002) 於 2002 年的研究，則使用類 似詞性標示（part-of-speech tagging）的作法，稱為「角色標示」（roles tagging），角色 指的是在未知詞的組成成分、上下文以及句子中的其他部分，並且依據句子的角色序列 來辨識出未知詞。實驗部分針對中國人名以及外國翻譯名等未知詞做測試，並且達到不 錯的準確率以及召回率。 近年來的研究主要趨向於機器學習式的方法來處理中文斷詞，例如最大熵分類法 Maximum Entropy（ME）(Xue, 2003) 是將斷詞轉成字元分類問題（character classification）， 並且使用了數種類似的特徵，如目前字元、加上前後各一字元、加上前後各兩字元等， 來當作模型的屬性。而 C. L. Goh 等人則使用支持向量機 Support Vector Machin（e SVM） (Goh, et al., 2005) 來解決中文斷詞的問題，該篇研究結合辭典比對式方法－長詞優先法， 利用長詞優先法的歧義性以及未知詞的資訊，來加強 SVM 的特徵屬性以改善斷詞效能。 另外也有使用感知機（Perceptron）(Li, et al., 2005) 的方法做斷詞，該篇研究認為  結合長詞優先與序列標記之中文斷詞研究  165  Perceptron 方法雖然與 SVM 類似，不過效能卻較 SVM 差一些，但由於其訓練的速度 非常快，因此他們系統提出的主要貢獻就是一個速度快且效能不至於差太多的斷詞方 法。 另外一類則是以序列標記（sequence labeling）問題來處理中文斷詞，尤其以隱藏式 馬可夫模型為主。不過單獨使用 HMM 本身的效能並不高（約 81%）(Asahara, et al., 2003; Xue & Shen, 2003)，因此這兩篇研究將隱藏式馬可夫模型的斷詞結果當成是一個屬性， 並分別使用 SVM (Asahara, et al., 2003)以及 TBL (Lu, 2005)來當成主要的演算法做斷詞， 以達到較佳的斷詞結果。另外，條件隨機域 Conditional Random Fields（CRF）則是 2003 年後廣為使用的資訊擷取（information extraction）及斷詞（segmentation）方法，如 Massechusset Amherst 大學 A. McCallum, F. Peng, F. Fang 等人在 Rocling 2004 的論文，運 用了 24 個中文詞素如姓氐、國名、職稱字首、職稱字尾、地名、日期、單位、動詞、名 詞、形容詞等，以及大量的辭典（Vocubulary），得以將中研院平衡語料庫(AS)的 closed test 達到 0.956；而 H. H. Tseng 等人在 Sighan Backoff 2005 的論文，則藉由罕見字的字首 及字尾表所建構的特色，解決未知詞的問題，在 2,558,840 的特徵函數下，對中研院平衡 語料庫的 closed set 可達到 0.97。 3. 系統架構 我們提出的系統架構如圖 1 所示，主要想法是利用訓練資料中已斷詞的文件（Segmented Texts），建立一個辭典（Vocabulary Construction），再利用長詞優先比對(Maximum Matching)提供正向及反向標記資訊，讓學習模組（Learning Module）得以學習最佳參數； 實際斷詞時，即將末斷詞之文章（Unsegmented Texts），同樣利用長詞優先比對，產生 與訓練資料相同的測試資料，藉由以訓練好的模型（Model），標記文件並得到斷詞結 果（Segmented Texts）。我們使用兩種學習模組，一者為隱藏馬可夫模型，一者為條件 機率域來解決中文斷詞的問題。  Segmented Texts  WordList Construction  Maximum Training Matching  Learning  WordList Unsegmented Texts  Model  Maximum Testing Matching  Testing  圖 1. 系統學習架構  Segmented Data  166  林千翔 等  3.1 BIES分類與序列標記問題  利用機器學習式演算法來解中文斷詞的問題時，一般的作法是將中文斷詞問題轉換成分 類 的 問 題 ， 而 最 常 被 使 用 的 方 法 就 是 轉 換 成 字 元 分 類 問 題 （ character classification problem），將每個字元都給予其對應的類別，透過字元類別來做分類，這些字元的類別 由出現在中文詞當中的特定位置來決定，一個字元的位置可以分為位於詞的開始 （beginning）、位於詞的中間 （intermediate）、位於詞的結尾（end）以及由單一字元 組成的詞（single-character）等四種類別，因此也稱為「BIES 分類問題」。  理論上中文字元可以存在於中文詞的任何位置上，例如表 1 的例子，字元「中」 可以存在於詞的開始（B）、詞的中間（I）、詞的結尾（E）、以及單一字元的詞（S）。 所以 BIES 分類所要解決的問題也就是決定每個字元的正確類別。在中文斷詞的問題上， 一旦將欲斷詞字串中的所有字元都已分類完成，則也表示已經斷詞完成，例如：「今天 是重要的日子」這個中文字串，利用分類問題將找出每個字元所對應的 BIES 標籤，在 此例子中，也就是「BESBESBE」，則相當於是已經斷詞出｛今天、是、重要、的、日 子｝等詞出來了，因此原來的中文字串便可以轉換成「今天／是／重要／的／日子」的 斷詞結果。  表 1. 字元「中」可出現在詞的任何位置  B 中醫  I  國民中學  E 集中  S 在 資料庫 中  在 BIES 分類問題中，由於一個字元可出現在詞的不同位置，而導至所對應的 BIES 標籤不只一個，一旦類別標示錯誤，連帶會使得斷詞結果錯誤。但此種斷詞歧義性在 HMM 模式下，並無特殊處理方式。由於正向長詞優先與反向長詞優先在做斷詞時，遇 到歧義性的句子會產生不同的斷詞結果，因此如能將正向長詞優先與反向長詞優先的資 訊同時加入 HMM 模型中，相當於提供歧義性的資訊，並且長詞優先法屬於辭典比對式 斷詞法，雖無法直接提供未知詞的資訊，但可間接的調整辭典大小來反應未知詞多寡。 這也是我們之所以採用長詞優先比對提供 BIES 分類額外資訊的原因。 另外，BIES 字元分類雖然可以藉由前後幾個字的資訊，來完成斷詞，然而決定每個 字元是類別是獨立的，因此並無法兼顧前後字元的標記，例如一個字元若被標記為 B， 則其後字元理應被標記為 I 或 E，而不是 S 和 B，這也是序列標記問題希望能解決的問 題。本篇論文採用分別採用隱藏馬可夫模型及線性條件隨機域模型做為序列標記的演算 法。  結合長詞優先與序列標記之中文斷詞研究  167  3.2 長詞優先法 長詞優先法（Maximum Matching Algorithm, MM）是最簡單也最為廣泛使用的辭典比對 式的斷詞方法，其斷詞的策略是由句子的一端開始，試著比對出在辭典中最長的詞，當 作斷詞結果，接著去除此詞後，剩下的部分繼續做長詞優先法斷詞，直到句子的另一端 結束為止。一般來說，如果所使用的辭典夠大，長詞優先法斷詞可達到超過 90%以上的 斷詞準確率。  長詞優先法依照比對方向的不同又可分為兩種不同的變形，第一種是「正向長詞優 先法」（Forward Maximum Matching, FMM），即由句子開頭的第一個字元開始，由左 而右逐一掃瞄，比對出在辭典中最長的詞，以當作斷詞的結果，並直到句子的結尾而結 束。相反地，另一種長詞優先法的變形則是「反向長詞優先法」（Backward Maximum Matching, BMM），由句子的最後一個字元開始掃瞄，從右至左依序比對辭典中的詞， 比對到最長的詞當成反向長詞優先法的斷詞結果，並直到句子的開頭而結束。  此兩種不同的長詞優先斷詞法，當斷詞的結果不同時，則表示發生交集型歧義，如 表 2 中的第二個例子：「即將來臨時」字串，因為「將」可與「即」和「來」結合成｛即 將、將來｝等不同的詞，因此屬於交集型歧義字串，正向長詞優先法會斷詞成「即將／ 來臨／時」，而反向長詞優先則斷詞成「即／將來／臨時」。 表 2. 長詞優先法的不同變形  例句  正向長詞優先  反向長詞優先  即將畢業  即將／畢業  即將／畢業  即將來臨時  即將／來臨／時  即／將來／臨時  另外，由於長詞優先法屬於辭典比對式斷詞方法，只有在辭典中的詞才有可能正確 斷出，所以無法解決未知詞問題。當遇到未知詞時，正向長詞優先與反向長詞優先都將 斷詞成單一中文字元。例如：「鴻海董事長郭台銘」字串，由於辭典中未收錄｛鴻海、 郭台銘｝等詞，因此正向長詞優先法與反向長詞優先法都同樣會斷詞成「鴻／海／董事 長／郭／台／銘」。  3.3 隱藏式馬可夫模型 隱 藏 式 馬 可 夫 模 型 可 以 視 為 一 個 雙 層 的 隨 機 序 列 ， 包 含 了 隱 藏 層 的 狀 態 序 列 （ state sequence）和可觀察層的觀測序列（observation sequence）。隱藏層是無法直接觀察得到 的，但可以從另一個可觀察的觀測序列之隨機過程的集合觀察得出。因此，隱藏式馬可 夫模型是一個馬可夫鏈的機率函數，無法直接觀察的隱藏層就是一個有限狀態的馬可夫 鏈，其初始的狀態機率分佈以及狀態之間的轉移機率由狀態初始機率向量Π和狀態轉移 機率矩陣 A 來決定，另外還需定義觀測符號機率矩陣 B ，儲存各個觀測符號在不同的 狀態下的機率值。令 S 表示所有狀態的集合，S={s1,s2,…,sN}，N 表示模型中所有狀態的 個數，K 表示所有觀測符號的集合，K={k1,k2,…, kM}，M 表示模型中所有觀測符號的數  168  林千翔 等  目，則隱藏式馬可夫模型可由三個機率分佈Π, A, B 來描述：   Π=(πi)代表狀態初始的機率向量，πi=P(q1 = si)，1≤i≤N，表示在 t=1 時，狀態為 si 的 機率，且需滿足Σπi=1 的條件。  A=[aij] 代表狀態轉移機率矩陣，aij=P(qt+1 = sj | qt= si)，1≤i,j≤N，表示從狀態 si 到狀態 sj 的機率，且滿足 aij≥0 和Σjaij=1。  B=[bi(k)] 代表觀測符號矩陣，bi(k)=P(ot = vk | qt= si)，1≤i≤N 和 1≤k≤M，表示在狀態 為 si 時，觀測符號為 vk 的機率，且滿足Σkbi(k)=1。 給定輸入之觀察序列 O = o1o2 "on = ( ot 表示在時間 t 所對應的觀測符號，且滿足 ot ∈ K )。隱藏式馬可夫模型的目的就是要選出一個對應於觀測序列之最佳的狀態序列= Q = q1q2 "qn ( qt 表示在時間 t 所對應的狀態，且滿足 qt ∈ S )，也就是找出 P(Q1n | O1n ) 為最大機率值時的狀態序列。  由於在馬可夫基本假設下，第 t +1 的時間狀態只和第 t 的時間狀態有關，與其他任  何以前的時間狀態無關，即 P{qt+1 = sk | q1, q2 ,..., qt } = P{qt+1 = sk | qt } ，且隨機過程中的 機率轉移不隨時間改變，因此 P(Q1n | O1n ) 的計算可簡化成：  P(Q1n  |  O1n )  =  n ∏  P  (qt  t =1  qt −1 ) P(ot  n−1  n  qt  )  =  π  q1  ∏ t =1  Aqt  ,qt+1  ∏ t =1  Bqt  (ot  )  (1)  而取得此最大值的狀態序列 Q1n ，則是使用維特比（Viterbi）演算法計算得到。  隱藏式馬可夫模型當初所提出來的方法 (Rabiner, 1989) 是使用非監督式的學習方  法（unsupervised approach）做訓練，也就是從未標示狀態的文件中做訓練（因而稱之為  「隱藏式」），訓練的方法則是使用 Baum-Welch 演算法做參數的更新。而近年來許多  領域都已發展出大量已標示的語料庫可供訓練，隱藏式馬可夫模型同樣可以在已標示狀  態的文件中來做監督式（supervised approach）訓練 (Manning & Schutze, 1999) ，訓練過  程則直接利用最大概似估計法（maximum likelihood estimation）計算出模型參數則此模  型，又可稱為「可見式馬可夫模型」（Visible Markov Model, VMM）或「語言模型」  （Language Model）等，但絕大部分的研究仍然稱「隱藏式」馬可夫模型。於我們的系  統中，我們使用監督式的方法來訓練模型，在本論文中也直接以「隱藏式馬可夫模型」  稱之。  3.3.1 觀測符號的擴充（FB+HMM） 隱藏式馬可夫模型原本的設計是只有單一個觀測符號，將長詞優先比對資訊加入隱藏式 馬可夫模型，直接面臨的問題是如何計算多個觀測符號的機率。方法一是分別計算各個 符號出現的機率再以觀測符號彼此獨立的假設來計算多個觀測符號的聯合機率；方法二 則是直接記錄多個觀測符號的聯合機率；前者節省空間，後者機率估計較準。因此我們 採用第二種方式，將正向長詞優先（FMM）與反向長詞優先（BMM）之斷詞結果(即所 得的 BIES 標籤)，與原來的「字元」組成的新的觀測符號，延伸為「字元-FMM-BMM」  結合長詞優先與序列標記之中文斷詞研究  169  等三個資訊結合而成的觀測序列。表 3 中以一個例子來針對 FB+HMM 訓練以及測試過 程做個說明，在訓練階段中，原始的觀測符號序列為「研、究、生、命、起、源」，加 入了長詞優先法的資訊後，新的觀測符號序列便被轉換成「研-B-B、究-I-E、生-E-B、命 -S-E、起-B-B、源-E-E」。這些中文字元旁的 B、I、E、S 標籤即是由正向長詞優先與 反向長詞優先法所標示的，因此新的觀測符號種類相當於增加了 16 倍，在此狀態種類 並未做改變。 表 3. FB+HMM 的例子  訓練過程  測試過程  原始句子 研究／生命／起源 結合成分子  觀測序列 狀態 觀測序列 狀態  研-B-B B 結-B-S  HMM 訓練測試資料  究-I-E 生-E-B 命-S-E  E B E  合-I-B 成-E-E 分-B-B  ? ? ? ?  起-B-B B 子-E-E  ?  源-E-E E  3.3.2 特製隱藏式馬可夫模型  隱藏式馬可夫模型的特製化（specialization）概念，最早是由 J. D. Kim 等人於 1999 年 與 2000 年等兩篇研究 (Kim, et al., 1999; Lee, et al., 2000) 所提出來的，之後於 2001 年 到 2004 年間，A. Molina 及 F. Pla 等兩位學者，更是將此概念成功的應用到許多不同 的領域上，如詞性標示（part-of-speech tagging）(Pla & Molina, 2001; 2004)、淺層分析 （shallow parsing）(Molina & Pla, 2002)、詞義消歧（word sense disambiguation）(Molina, et al., 2002) 等問題上。  特製化的過程是指在不修改隱藏式馬可夫模型的訓練以及測試過程的前提下，透過 狀態的延伸使得模型增加更多資訊，以提升模型準確率。其主要的作法就是給予一個特 製化函式（specialization function），將原來的狀態符號產生出新的狀態符號，特製化的 過程以底下式子來說明：  f ( oi , qi ) = oi , qi .oi  (2)  <oi, qi>代表觀測序列中的某個觀測符號以及其對應的狀態，新的狀態符號經過特製化的 過程中，由此觀測符號加上原來狀態來產生新的狀態，經過特製化過程的隱藏式馬可夫 模型又稱為「特製隱藏式馬可夫模型」（Specialized HMM）。而如果不將所有的觀測符 號所對應的狀態都做進行特製化，而是只針對某些較容易分類錯誤的觀測符號才做特製 化，此過程則稱之為「詞彙式的隱藏式馬可夫模型」（Lexicalized HMM），此過程是屬  170  林千翔 等  於特製化過程的一種特例，也被稱為詞彙化（lexicalization），正式說來：  f(  oi , qi  )  =  ⎪⎧ ⎨  ⎪⎩  oi , qi .oi oi , qi  if oi ∈W if oi ∉W  (3)  其中 W 為特製詞（specialized words），只有屬於特製詞的觀測符號才會做特製化處理， 而特製詞的選擇又有許多不同的準則來選取。  表 4. 特製詞集合｛生-E-B, 起-B-B｝做詞彙化產生新的狀態  觀測符號 原來的狀態 新的狀態  研-B-B 究-I-E 生-E-B 命-S-E 起-B-B 源-E-E  B  B  E  E  B  B-生-E-B  E  E  B  B-起-B-B  E  E  在本篇論文中，透過第一階段將所有的觀測符號做延伸之後，我們進一步的以這些 新的觀測符號來做詞彙化，也就是取某些特定的觀測符號來當成特製詞，將其對應的狀 態做延伸的過程。舉例來說，如表 4 所示，假設觀測符號「生-E-B」、「起-B-B」屬於 特製詞，則經過詞彙化的過程之後，觀測符號「生-E-B」以及「起-B-B」所對應的狀態 就被轉換成「B-生-E-B」及「B-起-B-B」。也是多了兩個新的狀態：一個是由觀測符號 「生-E-B」所屬的新狀態「B-生-E-B」，以及由觀測符號「起-B-B」所屬的新狀態「B起-B-B」。因此在新的訓練資料中，狀態符號被延伸了。 此特製化過程也將牽扯到一個問題：由於隱藏式馬可夫模型的三個主要參數都與 「狀態符號」有關，因此這階段的特製化過程，將增加隱藏式馬可夫模型的參數大小， 因此計算量也就會跟著增加，而且過多的特製詞不見得能一直提升準確率。所以我們必 須根據訓練資料來決定特製詞的大小。特製詞的選擇方式，我們是使用兩種不同的準則 （criteria）來選取，此兩種不同的準則分別說明如下：  SWF: (the Words with High Frequency) 取在訓練資料中屬於最高頻率的觀測符號，當成特製詞。  SEF: (the Words with Tagging Error Frequency) 取具有高測試錯誤率 (或稱標示錯誤率) 的詞，當成特製詞。 不論是使用 SWF 或是 SEF 準則來選取特製詞，都需要決定一個門檻值（threshold）， 此門檻值是決定最合適的特製詞數量，我們會於實驗四中找出最佳斷詞效能的門檻值。  結合長詞優先與序列標記之中文斷詞研究  171  3.4 條件式隨機域 條件隨機域為一種無向圖(undirected graphical)模型，可被用來估算給予一觀測序列,得到 相對應的狀態序列的條件機率分佈。相對於 HMM 以生成模型（generative model）描述 觀察序列如何經由狀態轉移及符號產生的過程，CRF 專注於狀態序列在給定觀測序列下 的條件機率分佈，屬於一種鑑別式機率模型（discriminative model）。原則上，條件隨機 場的圖模型佈局是可以任意給定的，一般常用的佈局是鏈結式的架構，鏈結式架構不論 在訓練、推論、或是解碼上，都存在有效率的演算法可供演算。 鏈結式條件隨機域模型（如圖 2 所示）可以定義為如下的問題：給定一組訓練資料 樣本{X1, X2, …}，以及其相對的序列標資料{Y1, Y2, …}，監督式學習的目標是找到最 佳的潛藏函數，使得最大似然度估計（likelihood）Πi P(Yi|Xi)最大化。  圖 2. 鏈結式條件隨機域模型  令 Xn 為一長度為 n 的觀測序列，則狀態序列 Y=y1, y2, …, yn 的條件機率以隨機域表 示如下：  P( y1,..., yn  | X)=  
We investigated the problem of classifying short essays used in comprehension tests for senior high school students in Taiwan. The tests were for first and second year students, so the answers included only four categories, each for one semester of the first two years. A random-guess approach would achieve only 25% in accuracy for our problem. We analyzed three publicly available scores for readability, but did not find them directly applicable. By considering a wide array of features at the levels of word, sentence, and essay, we gradually improved the F measure achieved by our classifiers from 0.381 to 0.536. Keywords: Computer-assisted Language Learning, Readability Analysis, Document Classification, Short Essays for Reading Comprehension. 1. Introduction Reading is a key competence for language learners. For learners of English as a Second Language (ESL), reading provides a crucial channel for learners to integrate and exercise the knowledge of previously learned vocabulary and grammar. If we could provide appropriate material to ESL learners, they would receive individualized stimulus, maintain the motivation to learn, and benefit more from reading activities. Hence, researchers have been investigating the readability of articles and books for a long time (Flesch, 1948). In recent decades, research about readability has not been confined to just classifying the readability of articles. In large-scale language tests that include a writing assessment, grading the writing of a large number of test takers is very time consuming. Moreover, maintaining a consistent grading standard over the group of graders is also a challenge. Hence, techniques 
This paper describes a framework that extracts effective correction rules from a sentence-aligned corpus and shows a practical application: auto-editing using the discovered rules. The framework exploits the methodology of finding the Levenshtein distance between sentences to identify the key parts of the rules and uses the editing corpus to filter, condense, and refine the rules. We have produced the rule candidates of such form, A Æ B, where A stands for the erroneous pattern and B for the correct pattern. The developed framework is language independent; therefore, it can be applied to other languages. The evaluation of the discovered rules reveals that 67.2% of the top 1500 ranked rules are annotated as correct or mostly correct by experts. Based on the rules, we have developed an online auto-editing system for demonstration at http://ppt.cc/02yY. Keywords: Edit Distance, Erroneous Pattern, Correction Rrules, Auto Editing 1. Introduction Nowadays, people write blogs, diaries, and reports not only in their native language but sometimes in a language they are not that familiar with. During the process of writing, second/foreign language learners might make some errors, such as in spelling, grammar, and lexical usage. Therefore, how to provide editorial assistance automatically and effectively has become an important and practical research issue for NLP (Natural Language Processing) researchers. For second/foreign language learners, providing instant responses to their writing, indicating which part might be incorrect, and offering auto-editing suggestions for them to choose from would be beneficial for the improvement of their writing and other aspects of language development. 
 Kuang-hua Chen 摘要 網際網路已成為學術訊息傳播的主要管道，本研究關注擷取網際網路上學術研 究人員關心的學術會議訊息，提供會議主題、時間、空間等訊息，企望減輕研 究人員蒐集與管理會議資訊的負擔，進而提升學術研究出版的效率。本研究首 先提出一套學術會議資訊檢索與擷取的自動程序，並藉由實驗確認其可行性， 實驗結果顯示文件分類績效 F1 measure 超過 80%；具名實體擷取績效 Recall 超過 86%，F1 measure 超過 70%。繼而實際開發學術會議檢索與擷取系統平台， 提供文件檢索、資訊擷取、分類瀏覽、行事曆等功能，整合研究人員的學術活 動與日常行程安排，展示前述學術會議資訊檢索與擷取程序的實用性。 關鍵詞：學術資訊、資訊擷取、資訊檢索、具名實體 Abstract Internet has become a major channel for academic information dissemination in recent years. As a matter of fact, academic information, e.g., “call for papers”, “call for proposals”, “advances of research”, etc., is crucial for researchers, since they have to publish research outputs and capture new research trends. This study focuses on extraction of academic conference information including topics, temporal information, spatial information, etc. Hope to reduce overhead of searching and managing conference information for researchers and improve ∗國立臺灣大學圖書資訊學系 Department of Library and Information Science, National Taiwan University E-mail: khchen@ntu.edu.tw  238  陳光華  efficiency of publication of research outputs. An automatic procedure for conference information retrieval and extraction is proposed firstly. A sequence of experiments is carried out. The experimental results show the feasibility of the proposed procedure. The F1 measure for text classification is over 80%; F1 measure and Recall for extraction of named entities are over 86% and 70%, respectively. A system platform for academic conference information retrieval and extraction is implemented to demonstrate the practicality. This system features functionalities of document retrieval, named entities extraction, faceted browsing, and calendar with a fusion of academic activities and daily life for researchers. Keywords: Academic Information, Information Extraction, Information Retrieval, Named Entities 1. 緒論 在全球化的趨勢之下，大學的學術評價更加受到前所未有的重視，有各式各樣以全球大 學為標的之學術評鑑報告陸續公告周知，如上海交通大學（ARWU, 2010）與英國 Quacquarelli Symonds （QS, 2010）所做的世界大學排名。此外，Thomson Reuters 公司的 SCI、SSCI、A&HCI 等資料庫，以及 Journal Citation Report（JCR），提供的統計數據， 往往成為各國評鑑國內大學學術成果的計量指標。在這種激烈的學術競爭環境之下，且 學術競爭力被視為國家競爭力的一環，大學教授莫不兢兢業業地、努力地從事學術研究。 學術研究人員掌握學術會議資訊的即時性與確實性，對於其研究工作的進展與研究成果 的發表，是非常重要的。本研究在這樣的背景下，研發學術會議資訊檢索與擷取系統， 希望能夠有效地由充斥浮濫資訊的網際網路，擷取相關的學術會議資訊。 學術研究人員的學術活動是非常多元的，學術資源服務的類型眾多，本研究將著重 於以資訊擷取為基礎的學術會議資訊的檢索與擷取。研究人員的學術活動中很重要的一 項便是「學術研究的出版」，學術的出版有兩個主要的方向，一個是學術會議，另一則 是學術期刊。會議的 Call For Paper 有時間的期限，而期刊 Special Issue 的 Call For Submission 也有時間的期限，協助研究人員掌握這些重要的訊息，自動地由網路擷取學 術會議的時間訊息、空間訊息、與主題訊息，協助研究人員管理時間與空間訊息，將有 很大的助益。若能進一步搭配「行事曆（calendar）」的功能，對於研究人員而言更是事 半功倍的。換言之，一般行事曆功能僅提供使用者新增資訊、更新資訊、刪除資訊，為 了搭配學術研究的出版，行事曆必須有更進階的功能，能夠依據使用者的 profile 搜尋 Call For Paper 與 Call For Submission，填入行事曆，並依據使用者的設定，提供警示（alert） 的服務。 研討會通知或會議論文投稿須知，一般是透過既有的郵寄目錄發送，或是以網頁文 件的形式發佈，也因此訊息傳播的目標通常局限於特定族群及研究機構。即使使用者自 行利用網頁搜尋工具在網際網路上查找，所取得的資訊可能不完整，或是已錯過參與的 時機。若要提供即時的且整合的研討會相關資訊，蒐集網際網路上與研討會通知相關網  學術會議資訊之擷取及其應用  239  頁的自動機制，是重要的一環。 一般在網路上大量蒐集網頁的方式，通常利用網頁擷取機器人（web crawler）到處 拜訪網站並擷取所有網頁內容。由於 Web Crawler 的建置困難度較高，維護與效能控管 也較為複雜，不當的設計常會佔據網路頻寬資源，或導致被網站封鎖而無法擷取內容。 因此另有一種方式，並不採用傳統的 web crawler 而是修改網頁擷取機制，以適當的關鍵 字與網頁搜尋引擎的整合來蒐集網頁。 目標式網頁擷取（focused crawling）是一種蒐集研討會通知資訊的方式。有別於一 般 Web Crawler 漫無目的地抓取所有的網頁，Focused Crawling 會先過濾與主題無關的內 容，也就是會應用一組特定主題的關鍵詞，用以訓練並建立文件分類機制，再由此分類 機制引導 crawler 擷取與主題相關的網頁。（Chakrabarti, van den Berg, & Dom, 1999）另 外還可以將 Focused Crawling 稍加變化，依據一組系統已經記載的研討會議網站清單， 反向地蒐集相關網頁文件，這種網頁資料蒐集的替代方案被稱為反向式網頁擷取 （backward crawling）。（Brennhaug, 2005）這種網頁蒐集機制首先以主題關鍵字，透過 搜尋引擎取得相關網頁的網址及網頁內容，以建構候選相關文件集。再接續利用搜尋引 擎的反向連結查詢功能（back link query），一併蒐集連結到候選文件的網頁。又考量到 這種由反向連查詢所得的網頁也有可能再連結到其他研討會議網頁，所以再繼續以正向 連結（forward crawling）擷取該網頁中的其他 URL，以發掘潛在的相關網頁。此程序將 會一直重覆執行直到重覆的次數達到預設的門檻。 若以蒐集研討會議徵稿通告的相關資訊來檢視網頁自動擷取機制，無論是正向或反 向擷取，都會面臨下列兩項議題：(1)網路上傳播的研討會會議資訊經常更新，例如投稿 截止日期的延期、會議地點資訊的更新、或是新加入的 workshop 議程等等，而所蒐集的 研討會會議資訊必需能夠即時反應各項更新資訊。(2)目前雖然將「研討會議通知資訊」 定義為與研討會議相關的訊息通知網頁，但網頁內容通常包含許多與研討會無關的各種 式樣各種規格的其他資訊，例如文字或影音廣告，網站目錄選項，或其他網站連結等， 這也造成在擷取網頁機制建置時，文件相關程度判斷的問題。 本研究基於前述的背景，運用網頁搜尋技術，以及資訊檢索與擷取技術，發展一套 學術會議資訊檢索與擷取的自動程序，並實際建構系統平台，以服務學術研究人員。本 文的結構如下：文獻探討一節將說明資訊擷取的技術，運用於學術會議檢索的情形，相 關資訊服務系統的現況；學術會議資訊蒐集一節討論由網際網路蒐集學術會議資訊的方 法，以及過濾不相關資訊與雜訊的作法；資訊擷取模型之訓練與建置一節探討學術會議 資訊擷取模型的訓練與建立；系統實作與功能一節討論系統實作的方法，以及各項功能； 最後則是簡短的結論。 2. 文獻探討 學術會議資訊之檢索屬於資訊檢索的應用研究，其中牽涉的研究議題眾多，至少有具名 實體的辨識（named entities identification）、分群歸類（clustering and classification）、  240  陳光華  文件檢索（text retrieval）。然而，若要建置完整的應用系統，則牽涉更多的技術，如時 間與空間資訊的搭配，各種 API 應用元件的整合。本研究嘗試建構學術會議資訊檢索與 擷取系統，首先探討資訊檢索與擷取技術的現況，以及現有檢索系統的發展。限於篇幅， 本文並不嘗試進行全面而完整的相關文獻的探討。 學術會議資訊文件含有許多具名實體，包括會議名稱、會議時間、會議地點、會議 主題、截稿日期等等，已有許多學術論文探討這個研究課題，訊息理解會議（Message Understanding Conference，簡稱 MUC）是第一個將具名實體的辨識視為一項檢索研究的 評量項目，企圖推動資訊檢索研究社群，投注研究能量，發展更新的技術，提昇具名實 體辨識的績效。（MUC, 2001）訊息理解會議認為不僅僅需要辨識重要的實體，還必須 確認實體之間的關係（relationship），MUC-6 則明確地規範三個層次的資訊擷取的研究 議題：具名實體之辨識、照應詞之解析、樣版資訊之建構。照應詞之解析是串連具名實 體及其對應的照應詞（如代名詞）；腳本樣版則是依照預先訂定的樣版，由文件中擷取 相關的資訊填入樣版的欄位。（Grishman & Sundheim, 1996） 雖然具名實體辨識的研究很早就開始了，但是學術會議資訊擷取的研究則是比較不 受到許多研究者的關注。Lazarinis（1998）提出應該應用資訊擷取技術進行論文徵稿通 告（call for paper，簡稱 CFP）的檢索，有別於傳統上僅以文件檢索技術檢索 CFP。Lazarinis 發現這種作法在固定 Recall 的情形下，可以提昇 45%-60%的 Precision，這項研究確認應 將學術會議資訊的檢索，視為資訊擷取的問題，而非單純的文件檢索的問題。 Schneider（2005）應用 Conditional Random Fields（CRF）模型，擷取 CFP 的重要 訊息，Schneider 特別關注文件版面特徵（layout features）的貢獻，發現版面特徵可以提 昇約 30%的 F1 分數（F1 measure）。因為，Schneider 的研究關注於各項特徵的效益，使 用的測試資料僅有 263 篇乾淨無雜訊的 CFP，而避開真實文件各種複雜的情況，因此很 難建構一個實際可行的資訊服務系統。 目前亦有許多學術組織，建構了 Conference Calendar 的相關網頁，希望有利於會議 資訊的流通，但是這種資訊彙整形式的網頁，僅提供瀏覽的功能，沒有進階檢索功能， 使用者仍須耗費相當的精力，才能瀏覽相關的會議資訊。另外，尚有功能比較好的類似 系統，例如 WikiCFP 與 EventSeer 等 CFP 資訊共享服務系統，但是提供的多為電腦科學 相關學術領域的學術會議資訊。WikiCFP（http://www.wikicfp.com/）是使用 Wiki 建構的 CFP 共享系統，資訊來源是依賴使用者提供相關會議資訊；EventSee（r http://eventseer.net/） 是一個 Web 2.0 的網站，企圖建構一個電腦科學研究的社群網站，除了允許登錄使用者 自由發佈學術資訊外，另外運用 Robot 主動搜集網際網路上的 CPF 資訊。 Takada（2008）建構的 ConfShare 資訊服務系統，透過瀏覽器提供學術會議資訊檢 索的服務。Takada 認為研究者為了參加學術會議學習最新的研究成果，或發表本身的研 究成果，都需要蒐集學術會議的相關資訊。蒐集資訊的工作是參加會議不可缺乏的，但 也造成研究者不小的負擔。ConfShare 以使用者（亦即研究者）的角度，提供與學術會議 相關資訊的各種服務，希望能夠減輕前述研究者的額外負擔。  學術會議資訊之擷取及其應用  241  Xin, Li, Tang, and Luo（2008）使用 Constrained Hierarchical CRF（CHCRF）標註學 術會議官方網站的網頁以及屬性，企圖建構一個學術會議的行事曆系統。Xin 等人關注 的是學術會議的官方網站而非 CFP，然而官方網站成立的時間通常都很晚，不像 CFP 的 快速與即時，而且，官方網站的資料是透過下達會議名稱與時間，由 Google 檢索而得， 這樣的假設並非很合理，因為，類似的系統應該是藉由學術研究的主題取得學術會議資 訊，而非藉由特定的會議名稱或是舉辦時間。 本研究企圖建構的學術會議資訊檢索與擷取系統（Academic Conference Information Retrieval and Extraction System，ACIRES），較接近於 Takada（2008）的 ConfShare 系統， 但是在功能面仍有差異，使用的技術亦不相同，涵蓋的學科主題範疇亦有很大的差異。 下文將說明本研究的資訊的蒐集、處理、模型的訓練、以及系統的實作 。 3. 學術會議資訊蒐集 學術會議資訊的檢索與擷取，當然需要被檢索的標的物，必須有一套機制蒐集網路上的 論文徵稿通告，作為系統開發前，資訊擷取模型訓練之用；系統開發完成，正式運轉時， 亦需要這套機制持續蒐集論文徵稿通告，以服務學術研究人員以及一般的使用者。 為了有效地蒐集相關的學術論文徵稿通告，本研究採用目標式網頁擷取（focused crawling）的概念，先以學門分類表做為各學科主題的查詢關鍵字，利用網頁搜尋引擎蒐 集所需之論文徵稿通告。我們採用澳洲與紐西蘭標準研究分類表（Australian and New Zealand Standard Research Classification，簡稱 ANZSRC）為主（Pink & Bascand, 2008）， 再整合 Wikipedia 提供的學術領域列表以補充新興學科。由於論文徵稿通告不一定會標 示所屬學科領域，以學門分類名稱為查詢關鍵詞所蒐集的論文徵稿通告，可能無法涵蓋 各學科領域所有重要的研討會資訊。因此，可再進一步分析第一批搜集的論文徵稿通告 的研究議題相關詞彙，整合到學科主題關鍵詞列表，形成所謂的 bootstrapped crawling， 讓學術會議資訊的蒐集更為廣泛且完整。表 1 依字母順序，簡要列出部分之主題關鍵詞。 利用前述的主題關鍵詞，透過 Google 搜尋引擎，分別取得查詢結果前五十筆最相關 的網頁，再接續依相關網頁的內容執行一次正向連結查詢（forward link query），一併收 錄該五十筆網頁中超連結所指到的網頁。透過網頁搜尋引擎，可一次性地蒐集大量的相 關網頁，但無法掌控網頁提供的會議資訊是否已過期。再考量研討會資訊的提供，必須 符合即時性與時效性，因此再進一步利用網頁快訊服務（Google Alert），補充最新的研 討會資訊。 網頁快訊服務就是當新的網頁發佈於網際網路時，網頁搜尋引擎比較該新網頁與使 用者預設的 profile 的相關度，若是在搜尋結果的前 20 名內，就會立即以電子郵件通知 快訊訂閱客戶。利用此服務特性，將前述的學科主題關鍵詞，做為取得快訊的搜尋詞彙， 即時取得最新發佈的網頁文件。對於以網頁快訊服務取得的相關網頁，本研究也會進一 步執行一次正向連結查詢。  242  陳光華  無論是從網頁搜尋引擎或是網頁快訊服務蒐集而得的網路資訊，必定會有重覆的情 形，因此在蒐集網頁時，必須初步過濾重覆的網頁。以網頁搜尋引擎取得的相關網頁， 由於是同一時間取得的網頁內容，因此不需考量網頁更新的因素，直接比對網址過濾重 覆者。以網頁快訊服務取得的新網頁，若網址與現有文件相同，則必須考量網頁更新因 素，先比對兩筆網頁的上次更新時間，再保留更新時間較近的網頁。若無法取得網頁的 上次更新時間，則保留由網頁快訊服務取得的網頁。 由於從網頁搜尋引擎及網頁快訊服務廣泛蒐集的網頁數量龐大，大量的文件中可能 包含與研討會論文徵稿通告無關的網頁，為了提升學術會議資訊自動標註的準確度，必 須篩選無關的網頁文件。本研究運用文件自動分類技術，可以迅速處理大量文件，避免 繁瑣且冗長的人工分類作業，我們採用開放程式碼 Rainbow Classifier 自動過濾非會議徵 稿通告的網頁文件。（McCallum, 1996）由於 Rainbow Classifier 需要一組已分類的文件 做為分類模型所需的訓練文件，此訓練文件將利用人工分類的方式產生，該人工分類的 作業一併整合至人工標註輔助系統，讓標註人員可同時並行訓練文件分類與文件內容標 註工作。 表 1. 部分主題關鍵詞 abnormal psychology accompanying accounting scholarship acoustic engineering acoustics acting actuarial science adapted physical education admiralty law advertising aerobiology aeronautical engineering aerospace engineering aesthetics affine geometry african studies agricultural economics agricultural education agricultural engineering agrology agronomy air force studies algebraic computation algebraic geometry algebraic number theory algebraic topology american history american politics american studies analytical chemistry ancient egyptian religion ancient history animal communications animal science animation anthropology of technology apiculture appalachian studies applied psychology approximation theory aquaculture architectural engineering archival science art education art history artillery arts administration asian american studies asian studies associative algebra astrobiology astronomy astrophysics atheism and humanism atomic, molecular, and optical physics australian literature automotive systems engineering beekeeping behavioral geography behavioural economics behavioural science bilingual education biochemistry bioeconomics biogeography bioinformatics biological psychology biology biomechanical engineering biomedical engineering biophysics black studies or african american studies botany business administration business english business ethics calligraphy campaigning canadian literature canadian studies canon law cardiology cardiothoracic surgery cartography category theory cell biology celtic studies chamber music chemical engineering cheminformatics chemistry education chicano studies child welfare children geographies chinese history chinese studies or sinology choreography christianity chronobiology church music civics civil procedure classical archaeology classics climatology coastal geography cognitive behavioral therapy cognitive psychology cognitive science collective behavior combat engineering communication design communication engineering  學術會議資訊之擷取及其應用  243  4. 資訊擷取模型之訓練與建置 學術會議的論文徵稿通告主要包含會議名稱、會議地點、會議時間、會議主題、會議官 方網站、以及各項截止日期或公佈日期等。論文徵稿通告與一般文件最大的差異在於其 重要資訊不一定是以完整的語意文句組成，可能利用內容配置及排版以突顯各項資訊。 例如，一份論文徵稿通告的會議名稱通常單行置中且前後各有空行，研討會議題以項目 符號逐項表列，各項重要期限或公佈日期通常利用表格呈現。除了排版上的特色之外， 還可利用特定詞彙判斷是否為重要通知資訊，例如會議名稱通常會出現 conference、 international、annual 等詞彙，submission、notification、deadline 等詞彙則經常伴隨日期 出現，另外也可以利用完整的地名詞典擷取會議舉行地點。雖然可利用排版及詞彙兩種 特性設計論文徵稿通告的資訊自動擷取機制，但是網路上或電子郵件提供的論文徵稿通 告，並沒有一致的文件格式，通知項目也沒有統一的名稱，這都增加資訊判斷的困難度。 本研究應用 Conditional Random Field（CRF）建立自動擷取會議資訊的模組，從會 議通告網頁文件，擷取重要的會議資訊欄位（如會議名稱，會議日期，會議地點等）。 CRF 為機器學習式（machine learning-based）演算法，需設定數種資料特徵以訓練模型， 因此以學術會議徵稿通告必備的重要資訊項目，作為資料特徵欄位（如表 2 所示），再 使用一部分學術研討會徵稿通告，做為訓練文件集，先以人工的方式標註特徵欄位，並 利用特殊詞典或地名資料庫標示特定詞彙（例如地名、會議專有名詞等），建立 CRF 學 習樣版，再經由 CRF 自動學習與測試，調整資訊辨識的準確度，以建置資訊擷取的自動 機制。 CRF 是在機率演算的架構之下，針對某種結構組成的文字資料進行分段（segment） 或是標註（label）的工作，其文字資料結構包含序列式或是矩陣式等。某些機器學習的 演算法必須假設每一個序列資訊都是相互獨立，例如 Hidden Markov Model（HMM）， 但是真實世界的序列資料並不是由一連串獨立的資訊組成的。CRF 不同於其他機器學習 演算法，會考量隨機序列資訊的關聯性，以求整體序列的聯合條件機率，以避免詞彙標 註的偏置（bias）問題（Wallach, 2004）。本文並不試圖詳細描述 CRF 的理論與技術， 相關說明請參考（Sutton, Rohanimanesh, & McCallum, 2004; Lafferty, McCallum, & Pereira, 2001）。  244  陳光華  表 2. 徵稿通告之特徵及對應之標籤  中文名稱  英文名稱  HTML 標籤  標籤範例  會議全名 Conference Name  confname  <confname> Multimedia in Ubiquitous Computing and Security Services</confname>  會議名稱 Abbreviation of 縮寫 Conference Name  confabbr <confabbr> MUCASS 2008 </confabbr>  會議地點 Conference Location  confloc <confloc> Hobart, Australia </confloc>  會議日期 Conference Date  confdate <confdate> October 14-16, 2008 </confdate>  會議網址 Conference Website  <confwebsite> confwebsite http://www.sersc.org/MUCASS2008 </confwebsite>  會議主題 Conference Topic  conftopic  <conftopic> Real-time and interactive multimedia applications </conftopic>  報名截止 日期 Registration Deadline  registdue  <registdue> Registration - 15th October, 2007 </registdue>  摘要提交 截止日期 Abstract Submission Due  abstractdue  <abstractdue> Deadline for abstract 11 June 2008 </abstractdue>  摘要錄取 通知日期 Abstract Notification  abstractnotify  <abstractnotify> Acceptance of papers - August 30, 2009 </abstractnotify>  論文提交 Paper Submission 截止日期 Deadline  submissiondue  <submissiondue>February 15 23, 2009 - Paper submission</submissiondue>  論文錄取 通知日期 Author Notification  authornotify  <authornotify> March 23, 2009 - Author notification </authornotify>  論文定稿 截止日期 Final Paper Due  finalpaperdue  <finalpaperdue> Camera-ready copies: April 7, 2009 </finalpaperdue>  海報論文 截止日期 Poster Paper Due  posterdue  <posterdue> Poster Paper Submission Deadline May 15, 2008 </posterdue>  專題提案 截止日期 Workshop Proposals Due  workshopdue  <workshopdue> workshop submissions due : Sunday, 2 Mar 2008 </workshopdue>  教學提案 截止日期 Tutorial Proposals Due  tutorialdue  <tutorialdue> Tutorial Proposals: June 30, 2003 </tutorialdue>  博士生論 壇投稿截 Doctoral Consortium Due 止日期  doctoraldue  <doctoraldue> Doctoral consortium submissions due: 6 Apr 2008 </doctoraldue>  整體工作流程如圖 1 所示，包含文件前置處理、分類模型的訓練、CRF 模型的訓練 三項工作。文件前置處理包含去除文件雜訊、標註學術會議資訊、Tokenization 與詞彙特 性標示。  學術會議資訊之擷取及其應用  245  前置處理 去除雜訊  文件集 (CRF 格式)  CRF Training  調整參數  Google Search Google Alert  文件集 原始網頁  人工標註 會議特徵 Tokenization 詞彙特性 標示  文件集(已 分類網頁)  CRF Model  CRF Testing  Classifier Training  調整參數  人工分類  Classifier Model  Classifier Testing  圖 1. 學術會議資訊檢索與擷取自動模型之建置流程  4.1 文件前置處理  4.1.1 去除文件雜訊 由於由網際網路蒐集的文件，通常為 html 的網頁，包含許多各式各樣的資訊，除了該網 頁的主要內容之外，尚有網頁相互連結的資訊，以及網站外部的延伸資訊。有些網頁的 作者為讓網頁更吸引使用者瀏覽，採用了動態網頁或是多媒體的呈現模式，增加處理網 頁內容工作的複雜度。無論在資訊擷取的訓練階段或是正式的應用上，過多與會議資料 無關的雜訊將會影響資訊欄位判斷的精確度，因此必須先去除與網頁內容主體無關的雜 訊，包含廣告，圖片，網站目錄，視覺特效相關程式段落等等。  4.1.2 標註學術會議資訊 建構自動文件分類機制以及自動資訊擷取模型，需要大量的訓練資料，本研究另外建置 類別標註系統（Genre Annotating System，GAS），整合內容標註與文件分類二大功能， 以求內容特徵標註與文件分類標註的一致性與效率。GAS 以瀏覽器為系統平台，為典型 的 Web-Based Application，主要功能分成三部分：候選文件瀏覽、文件分類標註，以及 內容特徵標註。圖 2 為本研究建構之類別標註系統的操作畫面。  246  陳光華  1. 候選文件瀏覽區 圖 2 右上方的功能區塊為候選文件瀏覽區。如前文所述，候選文件是以學門分類表的 學科名稱為關鍵字，經由 Google Search 及 Google Alert 於網路上蒐集與會議論文徵稿 通告相關的網頁文件集合，經由去除雜訊處理之後，自動載入 GAS 系統。標註人員登 入 GAS 後，系統會於候選文件瀏覽區展示由該人員負責標註之文件清單，標註人員也 可以利用左方的查詢功能篩選網頁文件，清單上同時標示每份候選文件的標註狀態及 記錄。  圖 2. GAS -功能畫面  學術會議資訊之擷取及其應用  247  2. 文件分類標註區 文件分類標註區位於圖 2 系統功能畫面中間的狹長矩形區塊。候選網頁文件主要分成 相關與不相關兩類，所謂的相關與不相關，是以該網頁文件是否與會議論文徵稿通告 相關與否，作為判斷的依據。但是，考量有些網頁文件內容資訊太複雜而無法斷定， 也可以暫時不將該網頁歸類，且可以註記無法歸類的原因，作為後續文件分類例外處 理的參考，如圖 3 所示。標註人員從內容特徵標註區可檢視網頁文件，判斷該文件內 容是否是會議論文徵稿通告，若確定是會議論文徵稿通告，才需要進一步針對文件內 容標註各項會議資訊。 3. 內容特徵標註區 內容特徵標註區位於圖 2 的 GAS 系統功能畫面的下方功能區塊。選取候選文件瀏覽區 的任一筆資料，系統會將該網頁文件全文載入內容特徵標註區，內容特徵標註區係以 HTML 模式呈現網頁文件內容。內容特徵標註區上方的功能列，除了提供「復原動作」、 「重覆動作」、「去除 HTML 標籤」、及「字串查詢」等功能按鈕之外，最重要的功能是 「樣式」的下拉式選單，此樣式選單列出所有本研究採用的會議資訊特徵，標註人員 於網頁內容中框選特徵資訊後，再選取對應的會議資訊特徵樣式，標註之後，所選取 的特徵資訊會以特定的 HTML 標籤標示。例如會議名稱在 HTML 原始碼中標示為 <confname>會議名稱</confname>，本研究考量的會議資訊特徵與對應的 HTML 標籤請 再次參見表 2。  圖 3. GAS -文件分類標註區 4. Tokenization 與詞彙特性標示 CRF 需切割序列性資料為一連串 Token 後，並賦予各 Token 適當的詞性標示，再依每 個 Token 的特徵向量，計算各 Token 之間的條件機率，以做為建構詞彙辨識模型的依 據。因此去除雜訊後的網頁內容，要再抽取非 HTML 標籤的字串，將字串以單一詞彙 或標點符號為單位，切割成更小的片段為 Token，針對每一個 Token，進一步做一般詞 性標示及專門詞性標示。一般詞性標示包含標點符號，大小寫，數字，日期型態等識 別。專門詞性則包括地名，會議資訊經常使用專門詞彙，例如 conference、congress、  248  陳光華  association、annual、national 等，本研究採用 GeoNames 地名資料庫為地名辨視依據， 並整理會議資訊經常使用的專門詞彙，用以比對並標示相關詞彙，如表 3 所示。  表 3. 會議資訊使用之專門詞彙列表  專門詞彙類別  詞彙項目  機構名稱  Center, centre, college, department, institute, school, univ., university  組織名稱  Association, consortium, council, group, society  事件名稱 時間屬性名稱  Colloquium, conf., conference, congress, convention, forum, meeting, round, roundtable, seminar, summit, symposium, table, track, workshop Annual, autumn, biannual, biennial, European, fall, int., interdisciplinary, international, joint, national, special, spring, summer, winter  4.2 分類模型的訓練 文件分類的目的是為了預先過濾並非論文徵稿通告的文件，以降低內容自動標註時的負 擔。當系統運轉後，大量的網路文件進入系統時，必須先判斷是否為論文徵稿通告的相 關文件，然後再透過內容特徵擷取功能，擷取所需要的會議資訊。由於目前有許多的開 放程式碼可供使用，以開發文件分類的功能模組，本研究使用 McCallum（1996）的 Bow Library，開發統計學習為本的文件自動分類功能模組，用以過濾由網路取得的會議通告 文 件 ， Rainbow 則 是 基 於 Bow 的 應 用 程 式 ， 可 由 http://www.cs.cmu.edu/~mccallum/bow/rainbow/取得。基本上，Rainbow 是利用已知類別 的文件，統計分析各文件特徵並建立分類模型，再依此分類模型對新文件進行自動分類。 在人工標註輔助系統所產生的相關文件集與不相關文件集，是收錄原始網頁文件，而不 是已被人工標註特徵項目的新網頁內容，因為本研究的會議資訊自動擷取系統，是先過 濾非會議通告網頁，才進行資訊擷取程序，因此文件自動分類功能模組，是以原始網頁 做為訓練文件。我們進行大量的訓練與測試，使用 k-Nearest Neighbo（r kNN）、Naive Bayes （NB）、Support Vector Machine（SVM）三種分類模式，隨機抽取文件進行 20 次的實 驗，使用訓練文件與測試文件比例分別為（7:3）、（5:5）、（3:7），觀察分類績效的 變動情形，以決定系統使用的分類模型。分類結果的優劣是以 Recal（l 求全率）與 Precision （求準率）評量，可以進一步將兩項指標結合為單一的 F1 指標，計算方式說明如下。每 一篇文件皆已有正確的分類標記，在每一次的分類實驗，分類模型會為每一篇自動賦予 其分類標記，可能與正確的分類標記一樣，或是不一樣，因此有四種可能性，如表 3 所 示。 依據表 4 可以計算 Recall (R)、Precision (P)、以及 F1 Measure。  TP  P  ,  TP FP  TP  R  ,  TP FN  2P R F1 PR  學術會議資訊之擷取及其應用  249  表 4. 分類結果列聯表  Category i  Expert Assignment TRUE FALSE  System  TRUE  TPi  FPi  Judgment FALSE  FNi  TNi  因為進行了 20 次實驗，可以計算 Micro Recall、Micro Precision、Marco Recall、Macro Precision，以及對應的 Micro F1 Measure 與 Macro F1 Measure，以觀察每次實驗的變異情 形，計算方式如下所示，其中 n 代表實驗次數。  ∑ TP  ∑ TP  P  ∑  , TP FP  R  ∑ TP FN  
Transliteration is the general choice for handling named entities and out of vocabulary words in any MT application, particularly in machine translation. Transliteration (or forward transliteration) is the process of mapping source language phonemes or graphemes into target language approximations; the reverse process is called back transliteration. This paper presents a novel approach to improve Punjabi to Hindi transliteration by combining a basic character to character mapping approach with rule based and Soundex based enhancements. Experimental results show that our approach effectively improves the word accuracy rate and average Levenshtein distance of the various categories by a large margin. Keywords: Transliteration, Punjabi, Hindi, Soundex Approach, Rule based Approach, Word Accuracy Rate. 1. Introduction Every machine translation system has to deal with out-of-vocabulary words, like technical terms and proper names of person, places, objects, etc. Machine transliteration is an obvious choice for such words. When words cannot be found in translation resources, such as a bilingual dictionary, transliteration - the process of converting characters in one alphabet into another alphabet - is used. Transliteration is a process wherein an input string in some alphabet is converted to a string in another alphabet, usually based on the phonetics of the original word. If the target language contains all the phonemes used in the source language, the transliteration is straightforward, e.g. the Hindi transliteration of Punjabi word “ਕਮਰਾ” [kamarā] (room)1 is “कमरा” [kamarā], which is essentially pronounced in the same way. Nevertheless, if some of the sounds are missing or are extra in the target language, they are generally mapped to the most phonetically similar letter, e.g., in Hindi we have syllables (consonant clusters) that are written as a single atomic grapheme and have a double sound ∗ Department of Computer Science, Punjabi University Patiala, Punjab, India Email: josangurpreet@rediffmail.com; gslehal@gmail.com The author for correspondence is Gurpreet Singh Josan. 
It is shown that the enormous improvement in the size of disk storage space in recent years can be used to build individual word-domain statistical language models, one for each significant word of a language that contributes to the context of the text. Each of these word-domain language models is a precise domain model for the relevant significant word; when combined appropriately, they provide a highly specific domain language model for the language following a cache, even a short cache. Our individual word probability and frequency models have been constructed and tested in the Vietnamese and English languages. For English, we employed the Wall Street Journal corpus of 40 million English word tokens; for Vietnamese, we used the QUB corpus of 6.5 million tokens. Our testing methods used a priori and a posteriori approaches. Finally, we explain adjustment of a previously exaggerated prediction of the potential power of a posteriori models. Accurate improvements in perplexity for 14 kinds of individual word language models have been obtained in tests, (i) between 33.9% and 53.34% for Vietnamese and (ii) between 30.78% and 44.5% for English, over a baseline global tri-gram weighted average model. For both languages, the best a posteriori model is the a posteriori weighted frequency model of 44.5% English perplexity improvement and 53.34% Vietnamese perplexity improvement. In addition, five Vietnamese a posteriori models were tested to obtain from 9.9% to 16.8% word-error-rate (WER) reduction over a Katz trigram model by the same Vietnamese speech decoder. ∗ Faculty of Information Technology, Hochiminh City University of Industry, Ministry of Industry and Trade, 12 Nguyen Van Bao, Ward 4, Go Vap District, Hochiminh City, Vietnam E-mail: lequanha@ hui.edu.vn; NLp.Sr@ Shaw.ca; letrongngoc@ hui.edu.vn + Faculty of Information Technology, Nong Lam University of Hochiminh City, Block 6, Linh Trung Ward, Thu Duc District, Hochiminh City, Vietnam E-mail: {06130155, 06130204, 06130194}@ st.hcmuaf.edu.vn  104  Le Quan Ha et al.  Keywords: A Posteriori, Stop Words, Individual Word Language Models, Frequency Models. 1. Introduction A human is able to work out the precise domain of a spoken sentence after hearing only a few words. The clear identification of this domain makes it possible for a human to anticipate the following words and combination of words, thus, recognizing speech even in a very noisy environment. This ability to anticipate still cannot be replicated by statistical language models. In this paper, we suggest one way that significant improvement in language modeling performance can be achieved by building domain models for significant words in a language. The word-domain language model extends the idea of cache models (Kuhn & De Mori, 1990) and trigger models (Lau, Rosenfeld, & Roukos, 1993) by triggering a separate n-gram language model for each significant word in a cache and combining them to produce a combined model. The word-domain models are built by collecting a training corpus for each significant word. This is done by amalgamating the text fragments where the word appears in a large global training corpus. In this paper, the text fragments are the sentences containing the significant words. Sicilia-Garcia, Ming, and Smith (2001, 2002) have shown that larger fragments are not needed. We define a significant word as any word that significantly contributes to the context of the text or any word that is (i) not a stop word, i.e. not an article, conjunction, or preposition; (ii) not among the most frequently used words in the language, such as “will”; and (iii) not a common adverb or adjective, “now,” “very,” “some,” etc. All other words are considered significant, and a corpus is built for each. A statistical language model is then calculated from this corpus, i.e. from all of the sentences containing the word. Therefore, the model should be able to represent the domain of that word. This approach entails a very large number of individual word language models being created, which requires a correspondingly large amount of disk storage; previous experiments by Sicilia-Garcia, Ming, and Smith (2005) were done on twenty thousand individual word language models, which occupied approximately 180 GigaBytes. Thus, this tactic is feasible only given the relatively recent increase in affordable hard disk space. These word models gradually developed from the PhD work of Sicilia-Garcia 1999-2005. Almost at the same time as her research, similar research work was done by Blei, Ng, and Jordan (2003); they originally started from the ideas of Hofmann, Puzicha, and Buhmann (1998) and from Hofmann (1999). The remaining sections are organized in the following way. First, we discuss the weighted average model our developed approach lies upon. Then, we discuss both the probability models - including linear interpolation and the exponential decay model - and the  A Posteriori Individual Word Language Models for Vietnamese Language  105  weighted models, such as the weighted probability model, weighted exponential model, and linear interpolation exponential model with weights. Then, we discuss their corresponding frequency models and we discuss a priori and a posteriori testing methods. Following this, the corpus, experiments, and results are shown for the probability models, for the frequency models, and for a posteriori models. Finally, we provide some conclusions.  2. The Language Models  Experiments had shown that we needed to combine the global language model with the  individual word-domain models in order to obtain good results. (This may be due to the  limited size of the global corpus in our tests, which was 40 million tokens.) So, we first built a  language model for the whole global corpus. Frequencies of words and phrases derived from  the corpus and the conditional probability of a word given a sequence of preceding words  were calculated. The conditional probabilities were approximated by the maximum  likelihoods:  ( ) (( )) PML  wi w1i−1  f w1i = f w1i−1  =  f ( w1...wi−1wi ) f ( w1...wi−1 )  (1)  ( ) where f w1n is the frequency of the phrase w1n = w1...wn−1wn in the text. These probabilities were smoothed by one of the well-known methods, such as Turing-Good  estimation (Good, 1953) or the Katz back-off method (Katz, 1987). Although any of these  could be used in our experiment to demonstrate the principle of our multiple word-domain  models, it was convenient to use the empirical weighted average (WA) linear interpolation  n-gram model (O’Boyle, Owens & Smith, 1994) because of its simplicity. It gives results  comparable to the Katz back-off method but is much quicker to use. The weighted average  probability of a word w given the preceding words w1…wm-1wm is:  ( ) μ0  PML  (w)  +  m ∑  μi  PML  w wmm+1−i  ( ) PWA w w1m =  i=1 m  (2)  ∑ μi  i=0  where the weighted functions (in the simplest case) are given by  ( ( )) μ0 = Ln (T ) and μi = Ln f wmm+1−i .2i  (3)  ( ) where T is the number of tokens in the corpus and f wmm+1−i is the frequency of the  sentence wm+1-i…wm in the text. The unigram maximum likelihood probability of a word is:  PML (w) =  f (w) T  (4)  106  Le Quan Ha et al.  The language model defined by Equations (2) and (4) is called the global language model when trained on the global corpus. The creation of a language model for each significant word is formed in the same manner as the global language model.  3. Probability Models  We need to combine the probabilities obtained from each word-domain language model and  from the global language model in order to obtain a combined probability for a word, given a  sequence of words. One simple way to do this is a mathematical combination of the global  language model and the word language models in a linear interpolated expression as:  ( ) ( ) ( ) P w w1n  = λG PG  w w1n  m + ∑ λi Pi  w w1n  (5)  i=1  ( ) where  m λG + ∑ λi = 1  and  PG  w w1n  is the conditional probability of the word w following  a phrase w1...w1 n-1,wn in the global language model, Pi is the conditional probability in the word  language model for the significant word wi, λi is the correspondent weight, and m is the  number of word models that are included. Ideally, the λi parameters would be optimized using  a held-out training corpus; however, this is not practical as we do not know which  combination of words wi will arise in the cache. So, a simpler approach is needed.  3.1 Linear Interpolation  A simple method of choosing the λ-values is to give the same weight to all of the word  language models but a different weight to the global language model and to put a restriction  on the number of word language models to be included. This weighted model is defined as  ( ) ( ) ( ) P w w1n  = λ.PG  w w1n  +  (1− λ m  )  ⎡ ⎢  m ∑  Pi  ⎣i=1  w w1n  ⎤ ⎥  ⎦  (6)  and λ and m are parameters that are chosen to optimize the model.  3.2 Exponential Decay Model  A method was developed based on an exponential decay of the word model probabilities with  distance since a word appearing several words before the target word will generally be less  relevant than more recent words. Given a sequence of Vietnamese words, for example,  “  ” (meaning “I had friendly relations with HUI”) in Table 1,  where 5, 4, 3, 2, 1 represent the distance of the word from the target word “HUI”. The words  “tri” (relation) and “giao” (friendly) are significant words for which we have individual word  language models.  A Posteriori Individual Word Language Models for Vietnamese Language  107  Table 1. An explanation of distance of words.  tri  giao  5  4  3  2  
In this paper, we propose a system that automatically generates templates for detecting Chinese character errors. We first collect the confusion sets for each high-frequency Chinese character. Error types include pronunciation-related errors and radical-related errors. With the help of the confusion sets, our system generates possible error patterns in context, which will be used as detection templates. Combined with a word segmentation module, our system generates more accurate templates. The experimental results show the precision of performance approaches 95%. Such a system should not only help teachers grade and check student essays, but also effectively help students learn how to write. Keywords: Template Generation, Template Mining, Chinese Character Error. 1. Introduction In essays written in Chinese by students, incorrect Chinese characters are quite common. Since incorrect characters are a negative factor in essay scoring, students should avoid such errors in their essays. Our research goal is to build a computer tool that can detect incorrect Chinese characters in student essays and correct them, so that teachers and students can learn faster with help from the computer system. Compared with the detection of spelling errors in English, the detection of incorrect Chinese characters is much more difficult. In English, a word consists of a series of letters while a meaningful Chinese word usually consists of 2 to 4 Chinese characters. The difficulty lies partly in the fact that there are more than 5,000 high-frequency characters. In previous works on Chinese character error detection systems (Zhang, Huang, Zhou, &  ∗ Department of Computer Science and Information Engineering, Chaoyang University of Technology E-mail: {9727602, shwu}@cyut.edu.tw The author for correspondence is Shih-Hung Wu. + Institute for Information Industry E-mail: {maciaclark, cujing}@iii.org.tw  128  Yong-Zhi Chen et al.  Pan, 2000) (Ren, Shi, & Zhou, 1994), a confusion set for each character is built and is used to detect the character error with the help of a language model. The confusion set is based on a Chinese input method. The characters that have similar input sequences probably belong to the same confusion set. For example, the Wubizixing input method (Wubi), which is a Chinese character input method primarily for inputting both simplified and traditional Chinese text in a computer, is used in (Zhang, Huang, Zhou, & Pan, 2000). The Wubi method is based on the structure of the characters rather than on the pronunciation. It encodes every character in four keystrokes at the most. Therefore, if one keystroke is changed, another character similar to the correct one will show up. Once a student chooses the similar character instead of the accurate one, a character error is established, and a confusion set is automatically generated by the character error. Another approach is to manually edit the confusion set. Common Errors in Chinese Writings gives 1477 common errors (National Languages Committee, 1996). Nevertheless, this amount is not sufficient to build a system. Hung manually compiled 6701 common errors from different sources (Hung & Wu, 2008). These common errors were compiled from essays of junior high school students and were used in Chinese character error detection and correction. Since the cost of manual compilation is high, Chen et al. proposed an automatic method that can collect these common errors from a corpus (Chen, Wu, Lu, & Ku, 2009). The idea is similar to template generation, which builds a question-answer system (Ravichandran & Hovy, 2001) (Sung, Lee, Yen, & Hsu, 2008). The template generation method investigates a large corpus and mines possible question-answer pairs. Templates for Chinese character error detection can be generated and tested by the chi-square test on the basis of a large corpus. In this paper, we will further improve the methods for building confusion sets and automatically generating a template. According to recent studies(Liu, Tien, Lai, Chuang, & Wu, 2009a; 2009b), character errors in student essays are of four major types: errors in which characters have similar shapes (30.7%), errors in which characters have similar pronunciation (79.9%), errors in which the two previous types are combined (20.9%), and other errors (2.4%). Therefore, an ideal system should be able to deal with these errors, especially those resulting from similar pronunciation and similar character shapes. The confusion set for similar pronunciation is relatively easy to build, whereas the confusion set for similar shapes is more difficult. In addition to the Wubi input method, the Cangjie input method is also used to compile confusion sets (Liu & Lin, 2008). The paper is organized as follows. In Section 2, we introduce the system design and related works. In Section 3, we describe a new process of template generation. Section 4 describes the experimental procedure and the data. Finally, in Section 5, we give the conclusion and propose our future research.  Improving the Template Generation for  129  Chinese Character Error Detection with Confusion Sets  2. System Design  2.1 Chinese Character Error Detection and Correction System The system that can detect and correct Chinese character errors works as follows. First, it needs a student to input an essay. The system then reports the errors in the essay and gives suggestions on correction, as shown in Figure 1. Such a system uses templates that can detect whether common errors have occurred. A template consists of a pair of words, a correct one and an error one, such as “辯論會”-“辨論會”. For example, if the error template “辨論會” is matched in an essay, our system can conclude that there is an error and make a suggestion on correction to “辯論會”.  Figure 1. System function of Chinese character error detection in an essay In previous works, these templates were compiled manually (Liu, Tien, Lai, Chuang, & Wu, 2009b). The quality of the manually-edited templates is high. Nevertheless, the method is time-consuming and costs too much manpower. Therefore, an automatic template generation method based on the context of errors was proposed in 2009 (Chen, Wu, Lu, & Ku, 2009), several examples of automatically generated tri-gram and four-gram templates are shown in Figure 2. The automatic template generation method is less costly; however, it does not accommodate conventional vocabulary. The template generation method has a serious drawback. In Figure 2, we find that several templates contain unrecognizable words, such as “辯護律,” “視辯論,” and “電視辯,” which are trigrams of Chinese characters that do not have  130  Yong-Zhi Chen et al.  any meaning. These templates can be used to detect character errors, but are not suitable for suggesting corrections.  In the following subsections, we will propose a new method to avoid this drawback.  Templates  Templates  Correct 會首長  Error 會首常  Correct 清潔隊長  Error 清潔隊常  會給予  會給于  交通隊長  交通隊常  辯論會  辨論會  辯護律師  辨護律師  辯護律  辨護律  視辯論會  視辨論會  的辯論  的辨論  政策辯論  政策辨論  視辯論  視辨論  電視辯論  電視辨論  電視辯  電視辨  公開辯論  公開辨論  半世紀  辦世紀  半個世紀  辦個世紀  半以上  辦以上  一年半的  一年辦的  半個小  辦個小  的另一半  的另一辦  Figure 2. The templates for error detection and correction in (Chen, Wu, Lu, & Ku, 2009)  2.2 Confusion Set The first step in template generation is to replace one character in a word with a character in the corresponding confusion set. For example, by replacing one character in the correct word “芭蕉,” we get a wrong word “笆蕉”. Such a correct-wrong word pair is used as the template for error detection and correction suggestion. According to Liu et al. (Liu, Tien, Lai, Chuang, & Wu, 2009a; 2009b), the most common error types are characters with similar shapes and characters with similar pronunciation. The percentage of these two types of errors combined is 89.7% of all errors. Therefore, the confusion set should deal with characters with similar pronunciation and shapes. We first compile all of the characters that have the same pronunciation from a dictionary and make them the elements of a confusion set. For example, “八(ba1)” and “巴(ba1)” have the same pronunciation. Therefore, they belong to the same confusion set. To reduce the size of the confusion set, we treat characters with different tones as belonging to different sets, even though they sound similar. For example, “罷(ba4)” is not in the confusion set of “八 (ba1)”. We formed 1,351 sets with a total of 15,160 characters, as shown in Figure 3. In this paper, we use a simple rule to compile characters with similar shapes. In the first book on Chinese characters, known as Shuowen Jiezi (說文解字) (Xu, 2009), in the second  Improving the Template Generation for  131  Chinese Character Error Detection with Confusion Sets  century, radicals (部首) were used to categorize characters. We use the key component of a character, its radical, as the basic shape of the character to find the characters with the same radicals. There are 214 radicals in Chinese, according to the Kangxi Dictionary (康熙字典) (Zhang, 1999). Therefore, we compile 214 confusion sets with a total of 9,752 different characters. Figure 4 shows some examples.  After constructing the confusion sets, our system can find characters with the same pronunciation and characters with similar shapes for any character that is input. For example, given a character “兇,” the system can find characters with the same pronunciation “凶兄匈洶 恟胸,” and characters with similar shapes “兄光兆先兌克免,” as shown in Figure 5. This is a crucial step of our new template generation.  Zhuyin ㄅㄚ  Pinyin ba1  Characters 蚆扒八巴仈叭朳芭疤捌笆粑豝鈀吧  ㄅㄚˊ  ba2  鈸茇拔胈跋菝詙軷魃鼥犮  ㄅㄚˇ  ba3  鈀把靶  ㄅㄚˋ  ba4  伯罷霸猈弝爸壩灞把耙  ㄅㄛ  bo1  剝嚗波袚玻柭砵缽啵菠碆撥嶓蹳鱍岥播襏  ㄅㄛˊ  bo2  爆伯犮襏挀蔔柏瓝薄泊謈濼鋍帛勃胉挬浡  ㄅㄛˇ  bo3  簸跛蚾  ㄅㄛˋ  bo4  播檗蘗亳擘譒北挀薜簸繴  Figure 3. Examples of characters in confusion sets  Radicals  Characters  一  一丂丁七三下丈上万丌丑丐不丏丙世丕且  丶  丸凡丹主  丿  乂乃久么之尹乍乏乎乒乓乑乖乘  乙  乙九乜也乞乣乩乳乾亂  亅  了予事  二  二于云井互五亓亙些亞亟  亠  亡亢交亦亥亨享京亭亮亳亶亹  人  人仁什仃仆仇仍今介仄仂仉以付仔仕他仗  儿  兀元允充兄光兇兆先兌克免兕兔兒兗党兜  入  入內全兩  Figure 4. Examples of characters in confusion sets  132  Yong-Zhi Chen et al.  Figure 5. Combination of the two confusion sets for a given character 2.3 Automatic Template Generation Figure 6 shows the flowchart of our automatic template generation process. The basic assumption is that the corpus might contain more correct words than wrong ones. Therefore, our system first replaces one character in the correct words to form the corresponding wrong words. Then, our system checks the frequency of the words in the corpus. If the replacement creates a word with a relatively high frequency, we do not treat it as a wrong word.  Figure 6. The flowchart of the automatic template generation process As we mentioned in Section 2.1, the automatically-generated templates might not be suitable for suggesting corrections. To overcome this drawback, we use existing vocabulary, instead of n-gram character sequences, as the candidate for a template. There are 145,608 words in the MOE dictionary (Ministry of Education, 2007). We treat them as the seeds of the templates. In our experiment, we focus on 4,998 high-frequency characters that were compiled on the basis of a 1998 survey (National Languages Committee, 1998). Our system generates templates by checking each high-frequency character and finding all of the words that contain the character. Then, the system replaces the character in each  Improving the Template Generation for  133  Chinese Character Error Detection with Confusion Sets  word with a character in the corresponding confusion set. The correct-wrong word pair undergoes a simple statistical test. If it passes the test, it will be kept as a template; otherwise, it will be discarded. The statistical test is based on the frequency of each word in the pairs appearing in a large corpus. To prevent the process from generating controversial templates, our system also conducts a close test. The close test checks whether the new template will cause a false alarm on our old test data. The template that generates conflicting templates will also be discarded. The close test threshold is set to 0, which means any template that might cause a false alarm will not be used. A template generation example is shown in Figure 7.  Figure 7. A template generation example, where two templates are generated for an input character “官”.  The statistical test in our system is not a rigid test. We tune the threshold of relatively high frequency based on two formulae. One is adopted from the chi-square test, and the other one is from our observation. The first test is a simplified (n=1) chi-square test used in a previous work (Hung & Wu, 2008):  X 2 = (O − E)2 ,  (1)  E  where E is the frequency of a correct word and O is the frequency of a wrong word. To avoid  further disputation, we assume that E>O in our study. The chi-square test provides a threshold  mechanism to decide whether a correct-wrong pair is a proper template or not.  In this study, we suggest the test should be like Equations (2) and (3).  Cfreq > Wfreq , Cfreq > AverageFreq  (2)  n ∑ Cvocabulary(i)  Threshold = i=1  ,  (3)  n  134  Yong-Zhi Chen et al.  where Cfeq is the frequency of the correct word, Wfeq is the frequency of the wrong word, and AverageFreq is the average of the frequencies of all correct words. If the frequency of the correct word is higher than the threshold and if the square root of the frequency of the correct word is higher than the frequency of the wrong word, then the pair passes the test. We have found that the templates that do not pass the test are also the ones that will cause false alarms; for example, the pairs “未來”-“為來,” “已經”-“以經,” and “但是”-“但事”. When the context is different, these templates do not always give correct detection results and cause false alarms. 2.4 Word Segmentation As in the examples above, short templates with only two characters could cause false alarms. The reason is that, when we treat words as bi-gram character sequences, many word boundaries may be unclear. For example, as shown in Figure 8, the template “擁有”-“雍有” can be used to detect and correct the first sentence, “一個人可雍有很多快樂”, in which one of the word pair appears, but the template “擁有”-“以有” cause a false alarm in the second sentence, “一個人可以有很多快樂”. We find that this failure can be avoided by using correct word segmentation. The character “以” should be a part of the previous word “可以”. If we have enough confidence in the word segmentation, then the characters in a segmented word should not be candidates for character error detection.  Figure 8. A false alarm in the second sentence for a short template “擁有”-“雍有” and “擁有”-“以有” We assume that a word segmentation tool can give the correct results for normal input sentences and does not segment sentences with wrong character sequences into words. Figure 9 shows the segmentation results of the two sentences shown in Figure 8. In our experiment, we used the segmentation tool provided by CKIP, Academia Sinica1. With the help of this segmentation tool, our system can compile more accurate short templates. Some short templates are shown in Figure 10. 
This paper focuses on tourism-related opinion mining, including tourism-related opinion detection and tourist-attraction target identification. The experimental data are blog articles labeled as being in the domestic tourism category in a blogspace. Annotators were asked to annotate the opinion polarity and the opinion target for every sentence. Different strategies and features have been proposed to identify opinion targets, including tourist attraction keywords, coreferential expressions, tourism-related opinion words, and a 2-level classifier. We used machine learning methods to train classifiers for tourism-related opinion mining. A retraining mechanism is proposed to obtain the system decisions of preceding sentences. The precision and recall scores of tourism-related opinion detection were 55.98% and 59.30%, respectively, and the scores of tourist attraction target identification among known tourism-related opinionated sentences were 90.06% and 89.91%, respectively. The overall precision and recall scores were 51.30% and 54.21%, respectively. Keywords: Tourism-Related Opinion Mining, Tourist Attraction Target Identification, Opinion Analysis. 1. Introduction The blogspace is a large resource for opinion mining. Opinion extraction methods are valuable for a wide range of applications. Our initial interest is to extract opinions related to tourist attractions from blog articles because it is helpful to see other people’s opinions about tourist attractions when planning a tour. Nevertheless, two issues arise when trying to apply published methods to retrieve opinions of tourist attractions: ∗ Department of Computer Science and Engineering, National Taiwan Ocean University No 2, Pei-Ning Road, Keelung, 20224 Taiwan E-mail: {cjlin, M96570038}@ntou.edu.tw  38  Chuan-Jie Lin and Pin-Hsien Chao  (1) Sentence-level or document-level: A travel article is often multi-topic because a travel route often includes several tourist attractions. Therefore, the opinion analysis for a specific tourist attraction should be carried out at sentence level, not document level. (2) Opinion topic or opinion target: Tourist attractions may be treated as topics (queries in IR) or as targets of opinions. Consider the following two sentences selected and adapted from our dataset: The Dream Lake is a beautiful place. The water is green and clear. Both sentences are considered tourism-related opinions by us. Their opinion targets, however, are not the same. The opinion target of the first sentence is “the Dream Lake” itself, while the target of the second sentence is “the water” (in the Dream Lake). Both sentences are related to the same topic, “the Dream Lake,” but the second sentence does not contain its topic words. We find difficulty in applying the previously developed methods due to these reasons. Opinion mining and analysis have been widely studied in several topics, including opinion detection and polarity classification (Wiebe et al., 2001; Pang et al., 2002; Alm et al., 2005; Ghose et al., 2007), opinion holder finding (Choi et al., 2005; Kim & Hovy, 2005; Breck et al., 2007), and opinion summarization (Ku et al., 2005). Some well-known large-scale opinion mining benchmarks have also been created, such as the NTCIR MOAT datasets (Seki et al., 2010) which are constructed on four languages, including Traditional Chinese. Opinion retrieval is one of the research topics relevant to our work. Godbole et al. (2007) estimated the polarity scores for a large set of named entities. Nevertheless, the opinionated sentences that did not contain named entities were skipped because they measured the scores by the co-occurrences of named entities and opinion words. Ku et al. (2005) retrieved documents containing relevant opinions relating to TREC-like topics. Zhang et al. (2008) accepted short queries (titles only) and expanded the queries by web resources and relevance feedback. The units of their retrieval work, however, were documents, not sentence-level. Okamoto et al. (2009) extracted relevant opinionated sentences by language model. Unfortunately, a large-scale training set is required to build a reliable probabilistic model, which is labor-consuming to prepare in the tourism domain. Opinion target identification is another research topic that is relevant to our work. Many researchers have focused on learning features of pre-defined types of products from reviews (Hu & Liu, 2004; Ghani et al., 2006; Xia et al., 2009). Nevertheless, the question remains whether the features of all kinds of tourist attractions are common. Moreover, in the  Tourism-Related Opinion Detection and Tourist-Attraction Target Identification 39 conventional definition, an opinion target in a tourism-related opinion is not always the name of the tourist attraction. Therefore, we define tourism-related opinion mining as a new topic and propose several approaches to solve the problem, including rule-based approaches and machine learning approaches. Although the experimental data used in this paper are written in Chinese, many of the rules and features are not language-dependent or can be easily adopted if necessary resources are available. We also hope that the experience gained from these experiments can be applied to other domains where articles are often multi-topic, such as baseball game critics. The structure of this paper is as follows. Section 2 presents the main ideas of tourism-related opinion identification and introduces the resources prepared for the work. Section 3 describes the design of a rule-based opinion identification system. Section 4 defines the features for training classifiers to build an opinion identification system. Section 5 discusses the experimental results, and Section 6 concludes this paper. 2. Tourism-Related Opinion Analysis 2.1 Problem Definition Opinionated sentences related to tourist attractions are the main interest of this paper. We call such an opinionated sentence a tourism-related opinion (hereafter “TR-opinion”) and its targeted tourist attraction a tourist attraction target (hereafter “TA-target”). The main goal of this paper is to retrieve TR-opinions and determine their TA-targets. That is, given an opinionated sentence, determine whether it is tourism-related or not, and decide which tourist attraction is the focus of this opinion. Our experiments were performed based on two assumptions: (1) sentences have been correctly tagged as ‘opinionated’ or not; (2) tourist attraction names appearing in a document have been correctly recognized. Hence, we have not integrated an opinion detection module and a tourist-attraction recognition module into our system yet. Opinion identification is not the main focus of this paper. There has been a lot of research on this topic. In the future, we would like to perform well-developed methods to do opinion detection in order to build a full system. In this paper, though, the input sentences are those sentences correctly labeled as opinions. Tourist attraction name recognition also is not a focus of this paper. It requires a named entity recognition system specifically designed for tourist attraction names, but we cannot find one. Although some of the tourist attractions are locations or organizations, such as parks or museums, there are various types of names, such as monuments or scenic spots that would need to be learned. In this paper, we simply prepare a list of tourist attraction names and manually check the correctness of the occurrences of the attraction names in the articles.  40  Chuan-Jie Lin and Pin-Hsien Chao  Tourist attraction name recognition will be studied in the future. The main ideas in accomplishing the tasks are: (1) Some opinion words strongly hint that a sentence is tourism-related. (2) The frequency of use of a tourist attraction and its distance to an opinionated sentence can be useful information. (3) A tourist attraction can be expressed in several ways in an article. This is the well-known coreference problem. (4) A sentence may target a tourist attraction if its preceding sentence also focuses on a tourist attraction. Before designing rules or features according to these ideas, some resources were prepared beforehand, as described in the following subsections. 2.2 Experimental Dataset Preparation The best known benchmarks for opinion mining are the NTCIR MOAT datasets (Seki et al., 2010). There was one pilot task in NTCIR-6 and were two formal tasks in NTCIR-7 and NTCIR-8. There are a total of 70 topics in Traditional Chinese. Nevertheless, none of their information need is about tourism attraction opinions. Although some topics may bring in tourism-related documents, such as the terrorist bombing on Bali Island and the tsunami in Sumatra, the number of topics is too small, and we still have to find TR-opinions among the opinionated sentences. For these reasons, we decided to build a new experimental dataset in the tourism domain. 200 travel articles were collected from a blog site called Wretch1 (無名小站). These articles were categorized as “domestic travel” on the blog site. We chose the most recommended articles by the readers in order to assure that the articles were truly about travel. Three annotators were asked to annotate the data. Each sentence was labeled as opinionated or not, its opinion polarity was assigned, and its TA-target was found if the annotator considered it a TR-opinion. The guidelines of TA-target decision for the annotators are as follows. Given a document, a list of tourist attractions mentioned in the document is shown to the annotators. A TA-target must be one of the tourist attractions on the list. If an opinion is made on a part of a tourist attraction (e.g. the souvenir shop in an amusement park), its TA-target is set to be the tourist attraction. If an opinionated sentence mentions a tourist attraction together with the city it belongs to, its TA-target is set to be the tourist attraction only. A city can be chosen as a TA-target only when the blogger directly expresses his or her feeling about the city. Note that, 
This paper presents a decision tree pruning method for the model clustering of HMM-based parametric speech synthesis by cross-validation (CV) under the minimum generation error (MGE) criterion. Decision-tree-based model clustering is an important component in the training process of an HMM based speech synthesis system. Conventionally, the maximum likelihood (ML) criterion is employed to choose the optimal contextual question from the question set for each tree node split and the minimum description length (MDL) principle is introduced as the stopping criterion to prevent building overly large tree models. Nevertheless, the MDL criterion is derived based on an asymptotic assumption and is problematic in theory when the size of the training data set is not large enough. Besides, inconsistency exists between the MDL criterion and the aim of speech synthesis. Therefore, a minimum cross generation error (MCGE) based decision tree pruning method for HMM-based speech synthesis is proposed in this paper. The initial decision tree is trained by MDL clustering with a factor estimated using the MCGE criterion by cross-validation. Then the decision tree size is tuned by backing-off or splitting each leaf node iteratively to minimize a cross generation error, which is defined to present the sum of generation errors calculated for all training sentences using cross-validation. Objective and subjective evaluation results show that the proposed method outperforms the conventional MDL-based model clustering method significantly. Keywords: Speech Synthesis, Hidden Markov Model, Decision Tree Pruning, Cross-validation, Minimum Generation Error. ∗ University of Science and Technology of China, No. 96, Jinzhai Road, Hefei, Anhui, China Tel: (+86) 13721053256; fax: (+86) 551 5331801. E-mail: luhenglh@mail.ustc.edu.cn; zhling@ustc.edu; lrdai@ustc.edu.cn; rhw@ustc.edu.cn The author for correspondence is Heng Lu.  62  Heng Lu et al.  1. Introduction Currently, there are two main speech synthesis methods. One is unit-selection speech synthesis (Hunt & Black, 1996) (Ling & Wang, 2007) and the other is the hidden Markov model (HMM) based parametric speech synthesis (Black, Zen, & Tokuda, 2007). The unit-selection approach concatenates the natural speech segments selected from a recorded database to produce synthetic speech. It can generate highly natural speech often, but its performance may degrade severely when the contexts for synthesis are not included in the database. In HMM-based parametric speech synthesis, speech waveforms are parameterized and modeled by HMMs in model training (Yoshimura, Tokuda, Masuko, Kobayashi, & Kitamura, 1999). During synthesis, speech parameters are generated from the trained models (Tokuda, Yoshimura, Masuko, Kobayashi, & Kitamura, 2000) and sent to a parametric synthesizer to reconstruct speech waveforms. Although the quality of synthetic speech still needs improvement, HMM-based parametric synthesis has several important advantages, including high flexibility of the statistical models, a comparatively small database necessary for system construction and robust performance of the synthetic speech -- it never makes the serious errors that unit-selection speech synthesis may make sometimes. In HMM-based parametric speech synthesis, binary decision tree based context-dependent model clustering is a necessary step in dealing with data-sparsity problems and predicting model parameters for the contextual features of synthetic speech that do not occur in the training set. In the conventional model clustering process, the maximum likelihood (ML) criterion is utilized to choose the optimal question from the question set for each tree node split and the minimum description length (MDL) criterion (Shinoda & Watanabe, 2000) is used as the stopping criterion to control the size of trained decision trees, which affects the performance of synthetic speech significantly, e.g., a large decision tree may alleviate the over-smoothing effects in generated speech parameters but may also lead to over-fitting problems. Nevertheless, the MDL criterion is derived based on an asymptotic assumption and the assumption that fails when there is not enough training data (Rissanen, 1980). Therefore, it may not work successfully in HMM-based speech synthesis, where the amount of training data is much smaller than that in speech recognition. Some research work has been done to improve the MDL criterion for the decision tree construction of HMM-based speech synthesis. A decision tree backing-off method was proposed in (Kataoka, Mizutani, Tokuda & Kitamura, 2004). In this method, a decision tree was first built using ML criterion without pruning. During synthesis, the tree nodes that generated the observations with maximum likelihood were chosen by a process of backing-off from the leaf node that was decided by the contextual information of each state for synthesis to the root node. Nevertheless, there still exist two issues in this method. One is the one-dimensional optimization algorithm adopted in (Kataoka, Mizutani, Tokuda, & Kitamura,  Cross-Validation and Minimum Generation Error based  63  Decision Tree Pruning for HMM-based Speech Synthesis  2004) to reduce the computational complexity, which means the decision tree backing-off is conducted simultaneously for all states instead of processing each state separately. The other is the inconsistency between the ML criterion and the aim of speech synthesis, which is to generate speech (acoustic parameters) as close to natural speech as possible. The minimum generation error (MGE) criterion has been proposed to solve the second issue. It optimized the model parameters by minimizing the distortion between the generated speech parameters and the natural ones for the sentences in the training set. The MGE criterion has been applied not only to the clustered model training (Wu & Wang, 2006b) but also to the decision tree based model clustering of context-dependent models (Wu, Guo & Wang, 2006) and positive results have been achieved in improving the naturalness of synthetic speech. In (Wu, Guo & Wang, 2006), MGE was adopted to replace the ML criterion to select the optimal question at each tree node split. Since increasing the size of the decision tree always leads to the reduction of the generation error on the training set, MGE cannot be used directly as a stopping criterion in decision tree building. Thus, the size of the decision tree trained in (Wu, Guo & Wang, 2006) was tuned manually to compare the results with the MDL clustering that had almost equivalent numbers of leaf nodes. On the other hand, cross-validation (CV) is a well-known technique to deal with the over-training and under-training problems without requiring extra development data. It estimates the accuracy of performance of a predictive model by partitioning the data set into complementary subsets and uses different subsets for training and validation (Bishop. 2006). In (Hashimoto, Zen, Nankaku, Masuko & Tokuda, 2009), a CV based method of setting hyper-parameters for HMM-based speech synthesis under the Bayesian criterion was proposed and positive results were reported. In this paper, we integrate the minimum “cross” generation error criterion to optimize the size of the model clustering decision tree automatically for HMM-based speech synthesis. Different from (Wu, Guo & Wang, 2006), the ML criterion is still adopted to select the optimal question at each tree node split. A “cross” generation error is defined to calculate the sum of generation errors for all training sentences by cross-validation using the models clustered with a given decision tree. The size of the decision tree is optimized to minimize the cross generation error in two steps. First, an initial decision tree is obtained through model clustering with the MDL factor tuned with MCGE criterion. Then, the decision tree is finely modified by backing-off or splitting each leaf node iteratively to minimize the cross generation error. Objective and subjective evaluation results show that this proposed method outperforms the conventional MDL based HMM model clustering method significantly. This paper is organized as follows: Section 2 describes the HMM-based speech synthesis method with conventional MDL clustering. In Section 3, the proposed MCGE based decision tree pruning method is introduced. Objective and subjective experimental results are discussed  64  Heng Lu et al.  in Section 4. Finally, conclusions are given in Section 5. 2. HMM-based Parametric Speech Synthesis 2.1 The Framework of HMM-based Speech Synthesis As shown in Figure 1, a typical HMM-based parametric speech synthesis system consists of two parts: the model training part and the speech synthesis part. In the model training part, spectrum, F0 and state duration are modeled simultaneously in a unified HMM framework. For each HMM state, the spectral features are modeled by a continuous probability distribution and F0 features are modeled using a multi-space probability distribution (MSD) (Tokuda, Masuko, Miyazaki & Kobayashi, 1999). In the synthesis step, speech parameters are generated from the trained models using maximum likelihood parameter generation (MLPG) algorithm (Tokuda, Yoshimura, Masuko, Kobayashi & Kitamura, 2000) and a parametric synthesizer is employed to reconstruct speech waveforms from the generated parameters.  Figure 1. Flowchart of a conventional HMM-based parametric speech synthesis system. 2.2 MDL-based Model Clustering In the training stage, decision-tree-based model clustering is conducted after training for full context-dependent HMMs to avoid data-sparsity problems and to predict model parameters for the context features that do not occur in the training set. A question set containing  Cross-Validation and Minimum Generation Error based  65  Decision Tree Pruning for HMM-based Speech Synthesis  language-dependent contextual questions is used. In the top-down decision tree building process, the ML criterion is commonly adopted to choose the optimal question and leaf node for splitting that lead to the greatest likelihood of growth. Further, the MDL principle is employed as a stopping criterion for decision tree pruning (Shinoda & Watanabe, 2000). The description length (DL) is defined as  I (λ) ≡ − log P(o | λ) + 1 D(λ) log N + C  (1)  2  where λ denotes the clustered models; o = [o1T , o2T ,..., oNT ]T is the training feature sequence, (⋅)T means the matrix transpose and N is the total frames of training data; log P(o | λ) is  the log likelihood function of λ on the training set; D(λ) is the dimensionality of the model  parameters; and C is a constant. The decision tree stops growth if the optimal leaf node  splitting determined by the ML criterion can no longer reduce the DL.  If a single-Gaussian distribution with diagonal covariance matrix is used as the output  probability distribution function (PDF) of each HMM state, Eq. (1) can be calculated as  Equation (2) in (Shinoda & Watanabe, 2000)  I (λ)  =  M ∑ m=1  
This paper describes a framework to extract the effective correction rules from the sentence-aligned corpus and show a practical application: auto-editing using the found rules. The framework exploits the methodology of finding Levenshtein distance between sentences to identify the key parts of the rules and then use the editing corpus to filter, condense and refine the rules. We produce the rule candidates of such form, A => B, where A stands for the erroneous pattern and B is the correct pattern. Our framework is language independent, therefore can be applied to other languages easily. The evaluation of the discovered rules reveals that 67.2% of the top 1500 ranked rules are annotated as correct or mostly correct by experts. Based on the rules, we create an online auto-editing system for demo on http://mslab.csie.ntu.edu.tw/~kw/new_demo.html. Keywords: edit distance, erroneous pattern, correction rules, auto editing 251 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 251-265, Puli, Nantou,Taiwan, September 2010.  
The English articles, the, indefinite a/an, and zero can be troublesome for English language learners. Thomas [1] demonstrated that English second language (L2) learners from first languages (L1) that do not have the equivalent of an article system encounter problems using articles. Ionin and Wexler [2] found that such learners fluctuate between definiteness and specificity. This study examined English L2 article use with Taiwanese English learners to determine the potential factors influencing English article substitution and error patterns in their academic writing. The corpus-based analysis used natural data collected for the Academic Writing Textual Analysis (AWTA) corpus [3]. A detailed online corpus tagging system was developed to examine article use, covering semantic (specific and hearer knowledge) as well as the other features of the English article. The results indicated that learners overused both the definite and indefinite articles but underused the zero article. The definite article was substituted for the indefinite article in specific environments. Although no significant difference existed between specific and non-specific semantic environments in zero article errors, a significant difference emerged between plural and mass/non-count nouns. These results suggest that, in regard to writing, learners need to focus on the semantic/pragmatic relationships of specificity and hearer (or reader) knowledge. Keywords: definite article, indefinite article, zero article, hearer knowledge. 1. Introduction The use of cohesive devices in writing is a well-researched topic in second language acquisition research, taking on a greater significance in recent years as increasingly more students are being asked to present their work in English, thus pointing to the need for greater accuracy and cohesion in students’ writing. Errors within the article system (i.e., a, an, the, 266 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 266-280, Puli, Nantou,Taiwan, September 2010.  and the zero article) have been noted in studies examining L2 learners’ writing, and such errors can be found in advanced learners’ texts as well [4]. To put this issue into perspective, a corpus study of 668 TOEFL essays from Chinese, Japanese, and Russian students found that 13% of sentences—or 1 in every 8 noun phrases—had article errors [5]. 
The present study examines English patent documents extracted from LexisNexis. We compiled a reference corpus of independent claim texts and lay the focus specifically on their collocation features. The findings suggest the functional development of independent claim involves verb-noun collocation and semantic prosody. Verb-noun collocations happen to function as semantic trigger affected by semantic prosody. In particular, clausal nominalization ([13]) is observed in that of verbal clauses. Based on discourse thematic referentiality ([2]), independent claim entails how clausal-specific units constructed the patent setting. The result is significant because discourse thematic referentiality which addresses how lexical units build up modern patent language providing empirical evidence for the overall characterization of independent claim. Besides, rhetorical structure and lexical meaning of independent claim can be derived from components of clausal types as they occur collocationally, referentially and dependently. Mutual information is attainable with the help of selectional collocation features that specific clausal types represented in natural language processing of modern patent language. It is suggested that the development of independent claim as a primer for Patent English. Keywords: intellectual property rights, patent, corpus, collocation, functional grammar 1. Introduction In the knowledge economy age, the intellectual property rights (IPR) become the important assets to human beings. Especially to the knowledge industry, the IPR is the key measure of a company competing with others. As globalization has resulted in greater economic growth rapidly, inevitably the challenges of interdisciplinary communication that concerned with intellectual property and other significant sector encounters has increased. This recognition of the importance has brought intellectual property to the limelight. Resulting from such recognition, the recent emphasis that has been placed on using English as the lingua franca to apply patents on an international level and how to write professional patent documents for successful patent application becomes a significant research topic in applied linguistic research. 296 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 296-310, Puli, Nantou,Taiwan, September 2010.  1.1 English for Specific Purposes (ESP) ESP is now well established as an important and distinct part of English Teaching ([3]). As English has acquired the status of lingua franca in almost any field of research, the teaching of ESP has generally been seen as a separate activity within English language teaching, and ESP research as an identifiable component of applied linguistic research ([7]). Basically, the origins of ESP can be traced back to the 1960s when there is a growing need for the technological and business industries ([24]). ESP, the prime realization of applied discourse analysis, was later evolved for every specialized area needs appropriate teaching materials. Recently, ESP is utilized as an umbrella term with multitudinous acronyms standing for the various sub-fields ([7]). Under ESP framework, there are two major sub-fields, English for Academic Purposes (EAP) and English for Occupational Purposes (EOP) which are distinguished by their research nature and pedagogical tradition ([7], [20]). EAP concerning students’ needs to learn academic language constitutes the majority of ESP, whereas EOP comprises of professional purposes in administration, medicine, law and business, and vocational purposes for non-professionals in work or pre-work situations ([7]). In EOP, there has been little investigation into interdisciplinary needs of patent over workplace settings which motivate the present research. 1.2 Technical vocabulary Writing for specific purposes requires familiarity with not only knowledge of the content but knowledge of the language. Unfamiliarity with vocabulary in writing is perceived to be a challenging task for language learners. As the importance of teaching vocabulary has been gained recognition, Coxhead and Nation (2001) [6] categorize vocabulary into four groups: high frequency words, academic vocabulary, technical vocabulary, and low frequency vocabulary. Nation (2001) [19] defines those words in the use of writing. High-frequency words refer to the most frequently used 2000 words of English that were used in all types of writing. Low-frequency words are the rarely used terms and covered only 5% of all words. Academic words, namely semi-technical or sub-technical vocabulary, were for academic purposes. This vocabulary is common to a wide range of academic fields but is not what is known as high frequency vocabulary and is not technical in that it is not typically associated with just one field ([5]). In contrast, technical words are the ones used in a specialized field, which are considerably different from subject to subject. As Chung and Nation (2003) [5] point to, technical vocabulary is largely of interest and used to people working in a specialized field. In the genre of law, Mellinkoff (1963) [18] suggests legal vocabulary are those of common words with uncommon meanings. For example, merger and acquisition bear the same literal meaning as ‘combination’ in general English. However, of economic and financial law, merger depicts the acquisition of one company by another. The combination into a single legal entity will increase the benefits to each other is semantically positive. As to acquisition, the combination often bears unequal treatments is often negative. 297  2. Methodology 2.1 Independent claim As patent law 35U.S.C.§112 paragraph 1 reads, “patent claim” is viewed as the specification containing a written description of the invention, and of manner and process of making and using it, in such full, clear, concise, and exact terms as to enable any person skilled in the art to which it pertains, or with which it is most nearly connected, to make and use the same, and shall set forth the best mode contemplated by the inventor of carrying out his invention. That is to say, patent claims of a published patent inform the public the scope of rights that distinguished the invention. As it is technically dealt with specific terms used, it allows the users to familiarize with the invention an applicant owns. Based on technical vocabulary suggested from the USPTO (United States Patent and Trademark Office) Glossary, the frequency of each has been listed according to the occurrence in the USPTO Patent Full-Text and Image Database (PatFT), and the distribution is presented in Table 1([16]).  Table 1. Frequency of the patent technical word list ([16])  Topic  Technical Words Total Frequency Percentage  Patent Activity  99  6,622,873  28  Patent Claim  17  12,695,484  54  Patent Community  23  1,455,693  6  People of the Patent  18  1,468,215  6  Patent Description  30  1,060,782  4.5  Patent Aid  25  342,988  1.5  Total  212  23,646,035  100  Rank 2 1 3 3 5 6  As Table 1 shows, “patent claim” which has high priority (54%) is valuable for a corpus-based research. According to the definition, “patent claim” is the precise legal definition of the invention, identifying the specific elements of the invention for which the inventor is claiming rights and seeking protection. Besides, of patent claims, “independent claim” which describes the invention in adding the essential features will provide a comprehensive view of patent claim ([16]). Technically, an “independent claim” is a proper noun in terms of patent which formally describes the invention in adding the essential features. In the patent application for a pencil, for example, the independent claim might begin with “a device comprising a cylindrical piece of wood with a piece of lead inserted into the center of the wood.” In such case, a pencil was distinguished with regard to the shape (cylindrical) and the materials they were made of (wood and lead). For the same pencil with the opposite shape, it will not be taken into consideration for such invention.  2.2 Compilation of the reference corpus of independent claim texts Since more efforts have to be made to explore the possibilities of modern patent language in applied linguistic research, we compiled a reference corpus made up of  298  independent claim texts, over a period of time 2000 to 2009, retrieved from LexisNexis, a database of multitudinous information for professionals in legal fields. Corpus of the present research contained 98 English patent documents with independent claim texts retrieved, and is made up of 4,887,084 word tokens. Although LexisNexis does not have a build-in patent claim subcorpus, the self-compiled reference corpus of independent claim texts adds significant strength to the development of claim language. Although an available specialized corpus contains an infinite amount data, constructing a small scale one would be needed for a profound linguistic study ([10]). 2.3 WordSmith tools 5.0 Owing to the size of text collection, the quantitatively analysis was computer-assisted, using WordSmith Tools 5.0 ([21]) to search for the word item as a string of letters to ascertain the absolute and relative frequency. Concordancer-tagged function of WordSmith 5.0 allowed us to calculate collocations and clusters around the search or node word. With the help of such tools, we can find more discriminative linguistics patterns and structures in the patents. The researchers search for instances of independent claim in the corpus resulted in a concordance containing 249 citations. This is a list of the 249 examples of independent claim with the words that preceded and followed. Figure 1 shows part of the concordance. Figure 1. Concordance of independent claim Out of the 249 examples of independent claim, 5 were irrelevant to the researchers’ analysis because independent claim was being mentioned, rather than used. Those irrelevant examples were those of the same pattern without subject in present progressive 299  tense—identifying at least one independent claim of the patent. Of the remaining 244 examples, all concordances for each were stored. Then, the concordance lines for each were manually analyzed one by one for further investigation.  2.4 Functional grammar We analyzed the collected data by Halliday’s (2004) [8] functional grammar. In Halliday’s (2004) [8] study, he distinguished six central processes which elicit the transitivity that describes a whole clause, rather than the verb and its object. The total set of functions used in interpreting the clause as representation, with criteria for recognizing the various types of process is illustrated in Table 2.  Process type Material action event Mental perception cognition desideration emotion Relational attribution identification Verbal Behavioural Existential  Table 2. Process types, their meanings and participants ([8])  Category meaning  Participants, directly Participants, obliquely  involved  involved  doing  Actor, Goal  Recipient, Client; Scope;  doing  Initiator; Attribute  happening  sensing  Senser, Phenomenon  seeing  thinking  wanting  feeling  being  attributing  Carrier, Attribute  Attributor, Beneficiary  identifying  Identified, Identifier;  Assigner  Token Value  saying  Sayer, Target  Receiver; Verbiage  behaving  Behaver  Behaviour  existing  Existent  As for the present research, functional grammar is applied as the 244 citations of independent claim were examined. The researchers first singled out each citation as a constructed clause. In this regard, the researchers conducted analysis at the clausal level to better reflect the actual process an independent claim was associated with. In this manner, the researchers elicited the verbs that distinguished each process type. For verification, the researchers derived nominals that represent participants in each clause. The researchers give an instance in (1).  (1) The processing computer can store the independent claim text information  Actor  Process  Goal  300  As shown in (1), store outlines a material process in which ‘processing computer’ (Actor) accumulates ‘independent claim text information’ (Goal). In such case, processing computer which occurs with store might provide selectional features (Chomsky, 1965:111) of the knowledge of independent claim. It is noted that verb-noun collocation ‘store + independent claim’ followed by processing computer is a subtle distinctive feature of independent claim which is expected to be known for such grammatical pattern making up knowledge of the grammar of patents in use. The investigation of such collocationally fixed relationship will, in turn give insights to learners how independent claim is used on a lexical level and further, prepare them for the actual business world they may need to work in, or give them the information about patents that they already work for. 3. Results Since independent claim describes the invention in adding the essential features, in this section, independent claim is annotated by three primary clauses of the total four clausal types found in the data. They were material, relational, and verbal clauses. The concept of clause as representation ([8]) is applied to remind language users where to locate independent claim to produce correct sentences. 3.1 Clausal types of independent claim texts There are a total of four clause types found in the data (see Table 3).  Table 3. Clauses types of independent claim  Clause Type  Total Frequency  Material Clauses  127  Relational Clause  65  Verbal Clause  48  Existential Clause  4  Total  244  Percentage 52.0 26.6 19.7 1.7 100  As Table 3 shows, material clauses have the largest proportion among the total, which account for 52%, with relational clauses coming next at 26.6%, followed by verbal clauses, making up 19.7%, and finally come existential clauses at 1.7%. However, behavioral clauses were not found as legal discourse of the Republic of China also addresses such phenomena. Tsai (2006) [25] explains law is essential in that it elaborates the obligation of human beings. Behaviors such as dream, cough, and cry, however, were basic instincts that human beings embraced. There is less importance to further develop such behaviors in the discourse of law. Though patent language and legislative language differ in their rationale, declarative sentences were favored in that of the examined clauses of the present research is in accordance with Tsai’s (2006) [25] research on legislative language.  301  It can be concluded from Table 3 that material clauses are the most commonly experience that independent claim embraced, while existential clauses are the least. These clauses of independent claim entail the directions for the novice. They should learn material clauses first.  3.2 Verb-Noun collocations of independent claim texts Frequently used verbs in patents can be seen as concepts which carry meanings to specify the clauses for communication. In total 244 examined clauses, the researchers found 23 verb-noun collocations from the data. Meanings of each collocating verb from the verb-noun collocations were carefully analyzed. Table 4 illustrates the results.  Verb identify direct contain be correspond infringe analyze isolate perform generate process store regard exist break up formulate permit fall illustrate provide utilize associate exhibit Total  Table 4. Collocating verbs of independent claim  Verb Meaning  Total Frequency Percentage  to extract, recognize, discover, or find  61  25.00  to request or enjoin with authority  51  20.90  to have within  42  17.20  state of having existence  20  8.19  to be in conformity or agreement  11  4.50  to encroach upon in a way that violates law or the rights of  7  2.90  another  to determine the nature and relationship of the parts of by analysis  6  2.50  to set apart from others  6  2.50  carry out an action or pattern of behavior  6  2.50  to bring into existence  5  2.00  a series of actions or operations conducing to an end  4  1.64  to place or leave in a location  4  1.64  an aspect to be taken into consideration  4  1.64  to have the functions of vitality  4  1.64  to do away with  2  0.80  to develop a formula for the preparation  2  0.80  to consent to expressly or formally  2  0.80  to come within the limits  2  0.80  to make clear  
With the evolution of human lives and the accelerated spread of information, new things and concepts are generated quickly, and new words emerge every day. It is therefore important for natural language processing systems to identify new words. This paper used the scheme for Chinese word extraction based on machine learning approaches to combining various statistical features. Due to the broad areas for the natural language applications, however, it is quite probable that the mismatch of statistical characteristics between the training and the testing domains occurs, which degrades the performance for word extraction inevitably. This paper proposes the scheme of utilizing the histogram equalization for feature normalization in statistical approaches. Through this scheme, the mismatch of the feature distributions for the training set and the testing set, with different sizes or in different domains, can be compensated. This makes the statistical approaches of unknown word extraction more robust for novel domains. This scheme was tested on the corpora provided by SIGHAN2. The best results, 68.43% and 71.40% of F-Measure for the CKIP corpus and the HKCU corpus respectively, can be achieved with four features with normalization and histogram equalization. When applied to unknown word extraction in an novel domain, it can be found that this scheme is capable of identifying such pronouns as “Cape No. 7”(海角七號), “Financial Tsunami”(金融海 嘯) and so on, which are not easy to be extracted by those approaches based on semantic characteristics. This scheme appears not good enough for extracting such new terms as the names of humans, places and organizations, in which the semantic structures are prominent. When compared with the results of unknown word extraction for two Chinese word segmentation systems, it can be observed that this scheme exhibits to be complementary with other approaches, and it is promising to combine approaches with different capabilities. 關鍵字〆未知詞萃取、機器學習、多層次類神經網路、中文詞彙萃取、直方圖均化。 Keywords: Unknown Word Extraction, Machine Learning, Multilayer Perceptrons, Chinese Word Extraction, Histogram Equalization. 一、緒論 隨著人類生活方式的演變和資訊普及的加速，新的詞彙在網路與媒體上不斷快速地增 加。這使得自然語言處理系統必須具有學習新詞的能力，才能與時俱進。例如，在中 文斷詞系統中通常都會用到詞典々系統雖然可以盡可能地增加詞典中的詞彙數 量，但是無論詞彙量有多大，都不可能含括所有可能用到的詞彙。這是因為，自然語 言處理的應用領域非常廣泛，自然語言本身就是隨著時間演進，在各種知識領域中都有 獨特的、不斷新增的關鍵字詞或專有名詞等，這些不是系統設計者可以預先知道的。因 此，斷詞系統的詞典內包含的詞彙不能一成不變，應該隨著處理的文章或相關領域 做更新。如果自然語言處理系統能針對各種新領域自動萃取出未知的詞彙，對於系 統的應用範圍或新領域的探索會有相當大的幫助。 一般萃取未知詞的方法主要有統計式與法則式兩大類。法則式的萃取方式，主要依據未 知詞的種類，考慮詞彙的語意，而訂定特定的萃取規則。在文章中，常出現具有規則的 365  未知詞，包含了人名、專名詞、複合詞、數值等。例如〆Sun 等人針對中文人名做辨識 [1]。在統計式的萃詞方法中，運用人們的習慣，詞彙組成的特徵資訊，利用統計的方 式計算出數值，判斷是詞彙的可能性，例如〆利用字元組在文章中出現的次數，Lu 等 人提出運用字元組出現頻率[2]，並且判斷字元組被另一個字元組包含時，兩字元組出 現頻率是否相同，做為刪除候選詞的決策條件。但是很難藉由某一種特徵值，就能完全 的模擬詞彙的特性。在中文語言的處理系統中，萃取新詞，常運用在改善斷詞的準確性。 例如 Hai zhao 利用常用的統計特徵[3]，計算字元組的特徵值，篩選出未知詞，適當的 刪減詞量，運用在斷詞上，針對特徵值的數值，實驗出一個門檻值，做為篩選未知詞的 依據。但這樣的做法，經過多次的實驗，得到一個最佳的門檻值，會增加不確定因素，而 且過度依賴特定語料庫的特性。此外，除了單純利用統計方式外，還有結合訂定法則的 方式。例如〆Ken-Jiann 等人在大量的語料庫中[4]，以字元為觀點，找出規則的模組，標 示未知詞的可能位置，針對被標示未知詞的部分，分別利用統計和法則，對於不同的種 類型態的未知詞，用不同的規則篩選，其特點在於可以找出低詞頻的未知詞。 詞彙的組成方式非常的複雜，不論是統計式、法則式或是結合兩種特性，都必須建立許 多的規則或是找出許多的特徵，也因為使用不同的語料庫，所以找出來的規則，或是統 計出來的特徵值，無法適用於其他的語料庫。因此有學者提出運用分類器的方式，萃取 新詞，例如〆梁婷等人利用構詞學的原理[5]，及非詞彙的篩選法則，將三音新詞篩選 出來並且過濾掉大部分非詞彙的新詞，針對這些詞統計特徵，利用統計特徵值結合類神 經網路來萃取新詞。GOH 等人針對字元在構成詞彙的特性[6]，例如字元的詞性，字元 位置詞彙的位置。結合了鄰近字元的特性和支持向量機（Support Vector Machines , SVM） 訓練一個分辨字元在詞彙中的位置，進而萃取出新詞。 為了降低對於特定訓練語料庫的依賴性，並且能針對各種不同領域，萃取出詞彙。因 此，本篇論文的研究，主要利用統計式的方法，探討如何結合不同特性的統計特徵，應 用機器學習方法來萃取詞彙。針對統計特徵值的分佈進行直方圖均化（ Histogram Equalization），使得測詴特徵值分佈能與訓練特徵值分佈能互相匹配，解決語料庫大小 或領域不同所造成特徵值範圍變動及分佈差異的問題，不必因為領域的差異而重新訓練 詞彙萃取模型，使得本論文的詞彙萃取方法更具一般性。 我們使用 SIGHAN2 的繁體語料庫進行詞彙萃取的測詴，在結合 DLG、AV、Link、PreC 四種特徵時，並且利用直方圖均化的方法對於 DLG 特徵分佈進行正規化，可以使得 F-Measure 上升 8%左右。對於中研院資訊所詞庫小組及香港城市大學所提供的語料 庫，F-Measure 分別可以達到 68.43%和 71.40%。最後將本論文詞彙萃取方法應用於新 穎領域的資料，從新穎領域資料萃取出未知詞，並與中央研究院資訊所詞庫小組和中國 科學院計算技術所提供的斷詞系統抽取的未知詞進行分析比較，我們發現本論文方法與 其兩套斷詞系統具有互補的特性，可以萃取出具有強烈的統計詞彙特性且難以透過語意 的方式萃取出來的未知詞。但是對於人名或地方名稱的未知詞萃取，則本論文方式萃取 能力較不足。 366  二、統計特徵的計算 在計算統計特徵之前，我們先統計字元 n 連次數(character n-gram)，初步篩選出現次數 大於等於 5 且長度小於等於 7 的字元組，作為候選的未知詞，稱為候選詞。所有候選詞 所形成的集合，稱之候選詞集。針對這些候選詞，可計算下列各種不同特性的統計特徵〆 1. 字元 n 連次數對數值（Logarithm of Character N-Gram, LogC） （公式 ） Ti : 第 i 個候選詞 C(Ti) : 候選詞 Ti 在所有文件中出現的總次數 詞彙本身就具有重複出現的特性，因此，若候選詞出現的次數愈高，愈有可能是詞彙。 2. 描述長度增益（Description Length Gain, DLG） （公式 ）  X : 語料庫中的所有文句 |X| : 語料庫中所有文句的字元總數 V : 語料庫中所有字元所構成的集合 L(〄) : 語料庫的資訊量（亂度） X[ @ → Ti] : 語料庫所有文句中，將候選詞 Ti 取代成"@" 描述長度增益特徵是由 Kit 等人所提出來的統計特徵[7]，主要概念是利用資料壓縮的程 度來評估字元組是一個詞彙的可能性。公式 2.2 中的Ｌ(X)為語料庫含有 Ti 的資訊 量，L(X[@→Ti])則是將語料庫中所有出現的候選詞 Ti 取代為"＠"之後的資訊量。因 此，DLG(Ti)表示 Ti 所產生的語料庫資訊量增益，可以反應出該候選詞對於整個語料庫 資訊量的貢獻度。對語料庫資訊量貢獻度愈高的候選詞，愈可能是個詞彙。  3. 介接變異度（Accessor Variety, AV）  AV(Ti)= min{ LAV(Ti), RAV(Ti)} LAV(Ti) : 候選詞左邊相鄰不同字元的個數 RAV(Ti) : 候選詞右邊相鄰不同字元的個數  （公式 2.3）  介接變異度是由 Feng 等人提出[8]，用來衡量一個字元組獨立出現的程度。其主要想法 是，若字元組前後可鄰接的不同字元數愈高，則該字元組愈可能是一個詞彙。反之，若 字元組可鄰接字元數低的時候，顯示該字元組並不常被單獨使用，而是須伴隨其他特定 字元一起被使用。因此，該字元組較可能只是一個詞彙的一部份，本身並非一個詞彙。  367  所以 AV(Ti)利用的是候選詞的上下文資訊，來反應該候選詞獨立出現的程度，獨立性愈 高，表示愈像是一個詞彙。 4. 鏈結強度對數值（Logarithm of Total Links, Link） 在 LogC 特徵中只考慮了候選詞出現的次數，並未考慮候選詞的子結構（子字元組）對 該候選詞是否為詞彙的支持強度，我們因此提出了鏈結強度特徵。鏈結強度不僅考慮候 選詞本身出現的次數，也考慮其內部結構所蘊含的支持強度。計算的方式是累計該候選 詞內部所有可能子字元組的ｎ連次數，如下列公式。  ）  （公式 ）  S(Ti;k,l)〆從候選詞 Ti 中取出位置 k 到 l 的字元組。 以字元組「行政院長」為例，其包含的子字串除了“行政院長”外還有“行政”、“行政 院”、“政院”、等，累加所有子字串的出現次數，即可得到鏈結強度。 5. 字首分離度（PreC） 我們利用與字元組擁有相同的字首的其他字元組與其子字元組的資訊，加強此特徵值 的可靠度；也就是透過具有相同字首的字元組，一起計算出字首的分離程度。如果字 首的分離度愈大，則候選的字元組較可能不是詞彙。其計算公式如下〆  （公式 ）  F〆Ti 的字首 S(F)〆以 F 字元為字首的字元組所組成的集合，且字元組長度需大於 2 |S(F)|〆S 集合的字元組總數 〆不包含字首的 x 子字元組 x[1:L] 我們針對個別字首，先取出長度為 3 到 7 的字元組，計算其移除字首後子字元組出現次 數的平均，作為該字首的分離度，若候選詞長度大於 2，輸出其字首分離度，小於等於 2 時，則以候選詞出現次數替代。例如〆「在台北」，與其相同字首的字元組有「在拍攝」、 「在學校」等，則分別統計子字元組「台北」、「拍攝」、「學校」的出現次數，計算其平 均值，即可取得字首分離度。 由於前面所述的特徵都是由語料庫統計得到，這些特徵的數值會受語料庫大小的影 響，而落在不同的範圍々如果測詴和訓練的語料庫大小有明顯的差異時，則訓練和測詴 的統計特徵值將落在不同的動態範圍，這使得訓練出來的分類器無法對測詴的資料做可 368  靠地分類。為了解決這個問題，我們把上述的各個統計特徵以線性方式正規化到0至1 之間，公式如下〆 （公式 ） v : 輸入的特徵數值 : 特徵值的種類 Min( ) : 特徵值中最小的數值 Max( ) : 特徵值中最大的數值 F(v) : 特徵值 v 經過正規化後的輸出值 三、詞彙萃取的方法 (一)、統計特徵分佈的問題 由於本論文使用的分類特徵都是由語料庫統計得到，特徵值分佈容易因為語料庫的不同 而有所差異。雖然利用第二章節中公式(2.6)，可以將數值範圍正規化到 0 至 1 之間，但 是當分類器應用於跨領域的分類資料時，有些統計特徵值可能會因語料庫領域或大小有 明顯的差異。這會造成訓練集與測詴集的特徵分佈不匹配，而影響到分類的正確率。所 以我們提出改進正規化的方法，將在下一節介紹。 我們以在 SIGHAN2 競賽中由中央研究院資訊科學研究所詞庫小組（Chinese Knowledge Information Processing Group, Institute of Information Science, Academia Sinica, 簡稱 CKIP）以及香港城市大學（HKCU）所提供的繁體語料庫為例。首先，將 CKIP 提供的 語料庫，隨機且平均地分成兩份語料庫，一份當作訓練語料庫，稱為 CKIP_Train，另 一份當作同領域的測詴語料庫，稱為 CKIP_Test。HKCU 的語料庫簡稱為 HKCU_Test。 因為 CKIP_Train 語料與 CKIP_Test 語料是由 CKIP 提供的語料庫隨機等分的語料庫，為 相同領域的語料庫。HKCU_Test 語料相對於 CKIP_Train 語料，則屬於跨領域的語料庫。 我們分別統計這三種語料庫中特徵值分佈，比較相同領域與跨領域的特徵值分佈。圖 3.1 是 DLG 統計特徵值的分佈，圖中的(a)顯示相同領域的語料庫 CKIP_Train 和 CKIP_Test 中的 DLG 分佈差異，圖(b)則顯示跨領域語料庫 CKIP_Train 和 HKCU_Test 中的 DLG 分佈差異。分別以語料庫的名稱命名分佈曲線，例如〆圖(a)中的 CKIP_Train 曲線，表示從 CKIP_Train 語料庫中統計得到的 DLG 特徵值分佈。由圖 3.1(a)可以看出，從 相同領域且資料量相近的語料庫，統計出來的 DLG 特徵值分佈，彼此只有些微差異，但 是在圖 3.1(b)中，因為不同領域的語料庫、資料量明顯的差異等原因，使得 DLG 特徵 值分佈有明顯的差異。因此，使用 CKIP_Train 訓練出來的分類器，尚可被應用於分類 CKIP_Test 的語料々但是應用於跨領域的資料時，其統計分佈有明顯的差異，導致此分 類器無法可靠地分類。所以我們針對 DLG 特徵值進一步做正規化，使其訓練資料與測 詴資料中的統計分佈，可以互相匹配。 369  DLG特徵值分佈  60%  資 料  40%  量 20%  0%  CKIP_Train CKIP_Test  60%  資 料  40%  量 20%  0%  DLG特徵值分佈 CKIP_Train HKCU_Test  0.04 0.049 0.058 0.067 0.076 0.085 0.094 0.02 0.032 0.044 0.056 0.068 0.08 0.092  (a) 相同領域分佈圖  (b) 跨領域分佈圖  圖 3.1 不同語料庫之 DLG 特徵值分佈比較圖  (二)、分佈正規化的法方介紹 訓練與測詴特徵值分佈有明顯差異時，訓練出來的分類器就無法可靠地分類測詴領域的 資料。由於我們希望詞彙萃取技術可以用來探索未知的新領域，在新領域中特徵的統計 分佈很可能不同於訓練領域，因此必須克服此問題。本論文分別為使用標準差倍數法與 直方圖均化法進一步正規化 DLG 數值。首先介紹標準差倍數正規化方法（Mean Standard Deviation Weight, 簡稱 MSW），公式如下〆  ｄ  （公式  d : destination 目標領域 S : source 來源領域 Md〆目標領域(訓練資料)特徵分佈中的平均數 MS〆來源領域(測詴資料)特徵分佈中的平均數 σd〆目標領域特徵分佈中的標準差 σS〆來源領域特徵分佈中的標準差 XS〆來源領域的特徵值 Xd〆轉換後的目標領域特徵值 標準差倍數正規化是一種線性調整的方法。轉換方式是以來源領域（測詴資料）標準差 為衡量基準單位，計算特徵數值與其分佈平均值之間的正規化距離為多少倍標準差，再 換算成目標領域（訓練資料）的值。 接著介紹直方圖均化法（Histogram Equalization, 簡稱 HEQ）[9]，其轉換函式如下〆  Xd ＝ P(XS)〃( Xmax - Xmin ) + Xmin XS〆特徵值 Xd〆均化後的數值 P(XS)：特徵值之累積分佈函數(CDF)  （公式 3.2）  370  PEQ(X)〆均化分佈之累積分佈函數（CDF） Xmax〆特徵的最大值 Xmin〆特徵的最小值 圖 3.2 是 HEQ 正規化方法的示意圖，縱軸為特徵 X 的累積分佈函數（Cumulative Distribution Function, CDF），橫軸為特徵 X 的值。此轉換方式是利用累積分佈函數，將 XS 轉換到均化分佈 PEQ(X)上具有相同 CDF 值的特徵 Xd，也就是將 X 特徵的估測 CDF 分佈映射到線性 CDF 分佈的空間中。而線性 CDF 所對應的機率密度函數值（Probability Density Function, PDF)是一均勻分佈（uniform distribution），故稱為「均化」。直方圖均 化是一種單調（monotonic）的轉換方式，根據特徵值的資料量做非線性調整，調整後 的數值能平均分佈於相同動態範圍中（Xmin 到 Xmax 之間）。使用 HEQ 正規化方法時，必 須將訓練特徵和測詴特徵都必須使用公式 3.2 進行均化，使統計分佈同時轉換至線性 CDF 分佈空間，讓彼此可以互相匹配，解決統計分佈差異造成分類器無法可靠地分類 問題。 
In English and Chinese noun-noun compounds, the most common construction is modifier-head with the head on the right. For example, in bookstore, the modifier is book and the head is store; in shā-táng (sand-sugar) 'sugar', the modifier is shā and the head noun is táng. However, such analysis is derived on the basis of syntax rather than semantics. Since this present work focuses on the meaning relationship between the components of a given compound, we may encounter noun-noun compounds in which the head noun can be both the word on the left and the word on the right. When choosing examples, we eliminate compound-like words which are composed of the construction of a word and an affix. For instance, some Chinese noun-noun compounds may be composed of a noun and an affix, such as zhuō-jiǎo (table-leg) 'table leg', in which zhuō is an affix rather than a word. This sort of compound is not taken into consideration due to the fact that an affix, which cannot stand alone as an individual word, is not a noun. Moreover, in other languages, especially modern Romance languages, we may also find different constructions of noun-noun compounds from those found in English and Chinese. For example, in Italian, a noun-noun compound is composed as headpreposition-modifier, such as succo di limone (juice-preposition-lemon) „lemon juice‟; in French, café au lait (coffee-preposition-milk) „white coffee‟ is also composed as headpreposition-modifier; Spanish has cuchillo de cocina (knife-preposition-kitchen) „kitchen knife‟. In the Romance languages mentioned above, the semantic head noun is usually located before the preposition, which means that the preposition in a noun-noun compound can function as an indicator for the head noun, while Chinese does not have this linguistic property. In the semantic analysis of nominal compounds, Gagne and Shoben [1] first proposed a set of „thematic relations‟, which are claimed to cover the majority of semantic relations between modifier and head in English noun-noun compounds: Table 1: Thematic relations (Gagne et al. 1997) 380  However, upon close scrutinization some deviance and discrepancies can be found. The enumerative approach to the compositionality of compounds would easily lose the power in facing with usage in the novel context. As an alternative approach, in the next section, we will introduce a formally elaborated lexical semantic theory of a generative approach to compound meaning. 3. Qualia Structure in the Generative Lexicon Theory 3.1 Overview The Generative Lexicon Theory (GL Theory) gives a new interpretation of the traditional qualia structure mentioned in the previous section. As pointed out by Pustejovsky [4], the qualia structure of lexical items can be explained as follows: a. CONSTITUTIVE: the relation between an object and its constituents, or proper parts i. Material ii. Weight iii. Parts and component elements b. FORMAL: that which distinguishes the object within a larger domain i. Orientation ii. Magnitude iii. Shape iv. Dimensionality v. Color vi. Position c. TELIC: purpose and function of the object i. Purpose that an agent has in performing an act ii. Built-in function or aim which specifies certain activities d. AGENTIVE: factors involved in the origin or “bringing about” of an object i. Creator ii. Artifact iii. Natural Kind iv. Causal Chain Let us examine some English examples that can demonstrate the qualia structure in nounnoun compounds, which in English is composed in the modifier-head structure. In glass door, chocolate cake, and oil painting, the modifier is a sort of material of the head. Here, the examples demonstrate the CONSTITUTIVE quale, while in history book, history distinguishes book from other sorts of books, such as math books or chemistry books; in horror movie, horror distinguishes movie form other sorts of movies, such as action movies; in noun phrase, noun distinguishes phrase from other sorts of phrases, such as verb phrases; in college student, college distinguishes student from other sorts of students, such as high school students. Here, the examples demonstrate the FORMAL quale. 381  In jewelry box, jewelry indicates the function of box, which means that the box is used to contain jewelry; in bookstore, the function of store is to sell books; in operation knife, knife is with the function of surgical operation; in drinking water, water is with the function of being safely drunk by people. Here, the examples demonstrate the TELIC quale. In adenovirus pneumonia, adenovirus is a sort of virus that causes pneumonia; in steamboat, steam is the power for boat; in turtle egg, turtle is the producer of egg. Here, the examples demonstrate the AGENTIVE quale. 3.2 The Base Modes of the Qualia Structure In this research, we use a lexical entry of the GL Theory in order to represent the structure of noun-noun compounds as follows: (α as the term itself; TYPESTR as type structure; ARG as argument; ARGSTR as argument structure; EVENTSTR as event structure) Figure 1. The Base Mode of the Qualia Structure The representation of the CONSTITUTIVE quale is as the following: Figure 2 The Mode of the CONSTITUTIVE Quale For the FORMAL quale, Pustejovsky [4] interprets the FORMAL quale with the GL Theory into two structures: (a). Simple Typing: value of FORMAL role is identical to sortal typing of the argument. (b). Complex Typing: value of FORMAL role defines the relation between the arguments of different types. The base mode of the FORMAL quale is as the following form: Figure 3. Types of the FORMAL Quale 382  The GL Theory interprets the TELIC quale into two base modes as described below: Figure 4. Direct TELIC: something which one acts on directly Figure 5. Purpose TELIC: something which is used for facilitating a particular activity Also, the AGENTIVE quale can be interpreted by the GL Theory with the base mode as the following: Figure 6. Types of the FORMAL quale The next section is going to discuss the modification of the qualia structure in the GL Theory with cross-language data for comparison. 4. Qualia Modification In previous sections, we discuss the morphology of Chinese compounds and the GL Theory. In this section, we are going to discuss the mapping of qualia in Chinese compounds. The proposed qualia structure in GL theory can be used to provide the lexical connection which binds semantic contributions of modifying nouns and the head noun in the compound [5]. In the Headedness Principle Packard [3] noted that there are two kinds of headedness, one is structurally (syntactically), the other is semantically. For example, gāng-qín-jiànpán, which means piano keyboard, structurally the head noun must be jiàn-pán, and thus the qualia structure is FORMAL. On the contrary, if we analyze the compound semantically, both gāng-qín and jiàn-pán can be the head noun. While we consider gāng-qín the head noun, the qualia structure is CONSTITUTIVE; however, if we consider jiàn-pán the head noun, the qualia structure is FORMAL. In short, the form of the semantic relation between the head and the modifier is not as specific as viewed from a syntactic perspective. 383  Therefore, we choose the word that serves the semantic content of the compound to be the head noun. 4.1 TELIC Qualia Modification To illustrate qualia modification, we first discuss the TELIC role in which the modifying noun describes the purpose of the head noun. In the Chinese compound cài-dāo „cleaver‟, cài modifies dāo‟s purpose, which is to cut vegetable.  Figure 7. The TELIC Mode  N1 is served as N2‟s function:  cài-dāo  菜刀  shuǐ-guǒ-dāo  水果刀  fàn-wǎn  飯碗  yóu-jǐng  油井  yǎn-jìng-hé  眼鏡盒  Table 2 vegetable-knife fruit-knife rice-bowl oil-well eyeglasses-box  „cleaver‟ „fruit knife‟ „rice bowl‟ „oil well‟ „glasses case‟  The TELIC quales of the compounds are also shown in other languages:  ITALIAN JAPANESE FRENCH  Table 3  coltello da pane bicchiere da vino うつるえかん 映畫館 でんわちょう 電話帳 ぶんぐてん 文具店 たくたまだい 桌球台 どうろひょうしき 道路標識 サングラス ほんばこ 本箱  knife-bread glass-wine movie-building telephonenotebook stationery-store ping-pong-table road-sign sun-glasses book-box  couteau de cuisine knife-kitchen  boîte à bijoux  box-jewelry  salle de bain  room-bath  „bread knife‟ „wine glass‟ „cinema‟ „telephone book‟ „stationery‟ „pingpong table‟ „road sign‟ „sunglasses‟ „book box‟ „kitchen knife‟ „jewelry box‟ „bathroom‟  384  GERMAN SPANISH  Lebensmittelgeschäft foodstuff-store  Briefmarke  letter-mark  Buchhandlung  book-action 
This paper focuses on the tourism-related opinion mining, including tourism-related opinion detection and tourist attraction target identification. The experimental data are blog articles labeled as in the domestic tourism category in a blogspace. Annotators were asked to annotate the opinion polarity and the opinion target for every sentence. Different strategies and features have been proposed to identify opinion targets, including tourist attraction keywords, coreferential expressions, tourism-related opinion words, a 2-level classifier, and so on. We used machine learning methods to train classifiers for tourism-related opinion mining. A retraining mechanism was proposed to obtain the system decisions of preceding sentences as a new feature. The precision and recall scores of tourism-related opinion detection were 55.98% and 59.30%, respectively, and the scores of tourist attraction target identification among known tourism-related opinionated sentences were 90.06% and 89.91%, respectively. The overall precision and recall scores were 51.30% and 54.21%, respectively. Keywords: tourism-related opinion mining, tourist attraction target identification, opinion analysis 
Short query terms often result in irrelevant search results due to the lack of appropriate contexts to disambiguate real user intention and thus introduce search errors. Without high precision raw search results, post re-ranking modules may not really help since garbage input only results in garbage output. Automatic query formulation, which supplies appropriate left and right contexts to the query terms, is therefore an important pre-processing technique for acquiring highly relevant documents and submitting them for post re-ranking. A systematic approach for augmenting short query terms with the best contextual text patterns is proposed in this paper for matching answers of some well-defined questions such as “the birthday of Bill Gates”(and most factoid questions). The augmentation patterns are learned to directly maximize the top-1 accuracy rate for searching relevant documents. In comparison with the basic two-term query form, which submits a key entity query term (‘Bill Gates’) plus an intended attribute (‘birthday’) to be answered, the augmented patterns achieve 31% top-1 accuracy rate, in contrast to the extremely low, 4%, accuracy achieved by two-term query; the top-10 performance, which is about 54%, is also significantly better than the 21% accuracy with two-term query. This also implies that about 57% of the top-10 results have their correct answer given at the first place. By using appropriate augmented query terms, the correct search results can thus often be ranked at the first few places, and very likely at rank-1. Experiments show that the augmented query patterns significantly boost the top-1 performance for answering well-defined questions. By applying such techniques to queries, it is likely to improve the search precision significantly, sometimes even without the help of post re-ranking. Keywords: query formation, query augmentation, augmented query term, user intention, search engines, well-defined questions, question answering. 
The modulation spectra of speech features are often distorted due to environmental interferences. In order to reduce the distortion, in this paper we apply the minimum variance (MV) criterion to obtain the optimal frequency response of the temporal filter, and then two approaches, least-squares spectral fitting (LSSF) and magnitude spectrum interpolation (MSI) are used to obtain the filtered feature sequence. Accordingly, two new temporal processing approaches are proposed, which are named MV-LSSF and MV-MSI, respectively. In the Aurora-2 clean-condition training task, we show that the new MV-LSSF and MV-MSI give more than 50% relative error rate reduction over the baseline, and provide relative error rate reductions of 8.18% and 2.73% over the conventional LSSF and MSI, respectively. 86 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 86-97, Puli, Nantou,Taiwan, September 2010.  These results reveal that the proposed methods significantly enhance the robustness of speech features in noise-corrupted environments. 關鍵詞：自動語音辨識、最小變異數、調變頻譜、強健性語音特徵 Keywords: speech recognition, minimum variance, modulation spectra, robust speech features 一、簡介 縱使語音科技日新月異，自動語音辨識(automatic speech recognition, ASR)[1]依舊是眾多 專家、學者研究開發的標的。主要原因在於實際生活環境中存在著多方面的變異性 (variation)影響辨識效果，這當中影響語音辨識的變異性包含了訓練環境與應用環境之 間的環境不匹配(environmental mismatch)、訓練語者與應用語者之間的語者差異性 (speaker variation)及不同語者或同一語者在發音上的變異性(pronunciation variation)等許 多因素，這些因素都會明顯影響語音辨識系統的效能。因此在近幾十年來，有許許多多 的學者持續不斷朝著努力改善以上幾種的語音差異性，進而使語音辨識系統能更有效地 運用於真實的生活環境中。 針對環境不匹配的狀況發展出許多強健性方法，綜觀而言，大致包含特徵補償[2]與模 型補償[3]兩大類型，而特徵補償方法當中，有一類型是針對語音辨識所用的特徵參數 之統計量作正規化處理，這些處理方式通常是作在特徵之時間序列域(temporal domain) 上，目的是將強調特徵中語音的成分，將雜訊的成分壓抑下來，或是使不同環境下的語 音特徵之統計量都能趨於一致，藉此提高語音辨識率，這些方法例如倒頻譜平均值正規 化法(cepstral mean normalization, CMN)[4]、倒頻譜平均值與變異數正規化法(cepstral mean and variance normalization, CMVN)[5]、相對頻譜法(RelAtive SpecTra, RASTA)[6]、 倒頻譜平均值與變異數正規化結合自回歸動態平均濾波器法(cepstral mean and variance normalization plus auto-regressive-moving average filtering, MVA)[7]、與統計圖等化法 (histogram equalization, HEQ)[8]、時間序列結構正規化法(temporal structure normalization, TSN)[9]、等漣波時間序列濾波器法(equi-ripple temporal filter, ERTF)[10]、最小平方頻譜 擬合法(least squares spectrum fitting, LSSF) [10]、強度頻譜內插法(magnitude spectrum interpolation, MSI) [10]等。而特別一提的是，2009 年時有學者提出了最小變異數調變 濾波器設計法 (minimum variance modulation filter, MVMF)[11]，其主要是根據最小化雜 訊 變 異 數 的 最 佳 化 目 標 ， 進 而 推 得 特 徵 之 時 間 序 列 域 上 的 濾 波 器 脈 衝 響 應 (impulse response)，藉由對語音特徵濾波處理，而改善語音特徵的雜訊強健性。 以上各種技術主要是直接或間接執行在語音特徵的時間序列域上，但在其效能的分析 上，我們通常會去探討雜訊及通道效應對於原始特徵之調變頻譜的失真，及這些方法對 於此失真的改善程度， 因此在本篇論文中，我們參考了 MVMF 法[11]的構想，使用變 異數最小化之最佳準則來處理語音特徵，但我們所發展的方法與原始 MVMF 法不同點 在於，它們是求得最佳的濾波器之頻率響應(frequency response)，即調變頻域上的最佳 化，再經由前述之 LSSF 與 MSI 法，求得濾波器處理後的語音特徵，而並非如 MVMF 法直接在特徵之時間序列域的最佳化求得濾波器的脈衝響應。在實驗結果發現，我們所 新提出的 MV-LSSF 法與 MV-MSI 法，所對應之辨識效能優於原始 LSSF 法與 MSI 87  法，且 MV-LSSF 法優於 MVMF 法，而 MV-MSI 法效果則與 MVMF 法十分相近。 本論文其他章節概要如下：在第二章中介紹本論文所提出之新方法，即 MV-LSSF 與 MV-MSI 法；第三章將呈現本論文所提出的新方法之辨識實驗結果與討論，第四章為 結論與未來展望。 二、基於最小變異數之調變頻譜正規化法  在本章中，我們首先簡略介紹前學者所提出之最小變異數調變濾波器法[11]，接著，我 們介紹本論文所提出之兩個新方法，即是將最小變異數調變濾波器設計的目標函數，應 用至求取濾波器之頻率響應上，進而延伸出兩種特徵時間序列處理演算法，分別為基於 最小變異數之最小平方濾波器法(MV-LSSF)與及基於最小變異數之強度頻譜內插法 (MV-MSI)，這兩種新方法的詳細步驟將於本章詳述。  (1)最小變異數調變濾波器法(minimum variance modulation filter, MVMF)  一設計得當的特徵時間序列濾波器，可以凸顯特徵中的語音成分並抑制雜訊成分，進而 提升語音特徵的強健性。而最小變異數濾波器(MVMF)設計法[11]，主要是根據三個方 向來設計特徵的時間序列濾波器：  1. 濾波器本身可以隨著不同語句（可能對應不同的雜訊干擾環境）而作動態調整。  2. 定義一個『環境失真』的目標函數，經由調變濾波器的設計，使處理後的環境失真 目標函數值能趨於最小。  3. 濾波器的設計，除了考量到降低雜訊成分外，也同時考慮到原始語音成分儘量不受 到影響與更動。  附帶一提的是，在文獻[11]中的最小變異數調變濾波器為第一類線性相位濾波器(type I linear-phase filter)，即濾波器長度為奇數且前後對稱，如我們所知，線性相位濾波器只 改變輸入訊號的頻譜強度及造成固定的時間延遲，並不會造成訊號時間延遲上的失真， 而第一類線性相位濾波器額外優點則是可以不受限地近似各種型態的濾波器（如低通、 高通、帶通與帶拒等濾波器等）。  以下為 MVMF 法中設計濾波器係數的步驟：  1. 對於任一語句的某一維特徵時間序列，定義其『環境失真』(environmental mismatch) 如下式：  a  =  òl p -p  H  (w)  2  PN  (w)d  w  +  ò p -p  
This paper proposes a novel feature for conditional random field (CRF) model in Chinese word segmentation system. The system uses a conditional random field as machine learning model with one simple feature called term contributed boundaries (TCB) in addition to the “BIEO” character-based label scheme. TCB can be extracted from unlabeled corpora automatically, and segmentation variations of different domains are expected to be reflected implicitly. The dataset used in this paper is the closed training task in CIPS-SIGHAN-2010 bakeoff, including simplified and traditional Chinese texts. The experiment result shows that TCB does improve “BIEO” tagging domain-independently about 1% of the F1 measure score. Keywords: Term contributed boundary, Conditional Random fields, Chinese word segmentation. 1. Introduction Word segmentation is a trivial problem for most Western language, since there are clear delimiters (e.g. spaces) for individual words. However, for some Asia languages such as Chinese, Japanese and other language do not have word delimiters, word segmentation problem will be encountered if we want to do some further language processing, e.g. information retrieval, summarization and so on. Thus, the Chinese word segmentation could be viewed as a fundamental problem for natural language processing. Chinese word segmentation is still a challenging issue, and there is contest held in SIGHAN community [1]. The CIPS-SIGHAN-2010 bakeoff task of Chinese word segmentation is focused on cross-domain texts [2]. The design of data set is challenging particularly. The domain-specific training corpora remain unlabeled, and two of the test corpora keep domains unknown before releasing, therefore it is not easy to apply ordinary machine learning approaches, especially for the closed training evaluations. 143 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 143-156, Puli, Nantou,Taiwan, September 2010.  Traditional approach for Chinese word segmentation problem is adopted dictionary along with lots of rules to segment the unlabelled texts [3]. Recent years, the statistical machine learning models, such as Hidden Markov Model (HMM) [4], Maximum Entropy Markov Model (MEMM) [5] and Conditional Random Field (CRF) [6], show the moderate performance for sequential labeling problem, especially CRF achieves better outcome. In this paper we propose a novel feature named term contributed boundary (TCB) for CRF model training. Since term contributed boundary extraction [10] is unsupervised, it is suitable for closed training task that any external resource or extra knowledge is not allowed. Without proper knowledge, the closed task of word segmentation can be hard when out-of-vocabulary (OOV) sequences occurred, where TCB extracted from test data directly may help. We also compare different character based label scheme “BI”, “BIO” and “BIEO” for model training. “B,” “I,” “E” and “O” mean the beginning of word, the internal of word, the end of word and the single character word, respectively. The character-based “BIO” tagging of Conditional Random Field has been widely used in Chinese word segmentation recently [11, 12, 13]. From the experiments, “BIEO” labeling shows the better performance than “BI” and “BIO”. The layout of this paper is as follows. We briefly introduce of CRF in Section 2. The novel feature term contributed boundary will be given in Section 3. Section 4 describes the data set and experimental results with error analysis. The conclusion is in Section 5.  2. Conditional Random Fields  Conditional random fields (CRF) are undirected graphical models trained to maximize a conditional probability of random variables X and Y , and the concept is well established for sequential labeling problem [6]. Given an input sequence (or observation sequence) X = x1 KxT and label sequence Y = y1 K yT , a conditional probability of linear-chain CRF with parameters Λ = {λ1,..., λn} can be defined as:  ∑ ∑ Pλ  (Y  |  X)  =  
This paper proposes a Japanese opinion word translation method based on unsupervised word sense disambiguation. The method comprises the corpus preparation, opinion word dictionary construction, and weighting method. Different from the machine translation, our method does not need parallel corpora, tagged corpora or parsing tree banks. Our method is low-cost but effective, and requires a well-made bilingual dictionary only. Besides, our method can extract key information from the opinions to help users understand the opinions. We construct four configurations and evaluate our method on four Japanese opinion words with high frequency. The evaluation result shows that the dependency grammar and opinion word dictionary is effective on opinion word translation. Our method can deal with the translation disambiguation problem and improve the translation precision to help user realize Japanese opinions. 關鍵詞：詞義消歧，意見詞，字詞翻譯，非監督式 Keywords: Word Sense Disambiguation, Opinion Word, Word Translation, Unsupervised 一、緒論 隨著網路的蓬勃發展，越來越多的使用者會在網路上發表對產品的評論，或者對於美食 157 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 157-171, Puli, Nantou,Taiwan, September 2010.  的食記，對於旅遊的遊記，對於住宿旅館的評價，除了分享與發表之外，也透過閱讀評 論，來作為是否把該目的物加入考量的一個方式。像是目前在多國規模領先的搜尋引擎 Google 也漸漸重視到這塊，在搜尋選項中可以選擇「討論」這個項目。 且受限於語言的隔閡，多數消費者無法理解以外語撰寫的評論。除了較普遍的第二外語 英文之外，大多數的華人無法了解檢索出來的外語網頁。以旅遊住宿資訊為例，台灣有 背包客網站（http://www.backpackers.com.tw/），但是如果今天有一家日本新開的旅館， 而在台灣的背包客網站也許要過好幾個月後才會有人分享，甚至若一直沒有台灣人去住 過，也許永遠看不到那家旅館的評論資訊。另外，對於小眾的旅行者而言，如：走訪當 地的鄉土民情、登山、釣魚等，當地語言的相關資訊或者評論也較完整。 而在意見探勘（Opinion Mining）的研究領域上，大致可以分成幾項任務來處理[1]，分 別是：  從整篇評論的觀點：  判斷整篇評論的情緒傾向  從句子的觀點：  判斷句有主觀性/意見性的句子  判斷句子的情緒傾向  從特徵的觀點：  判斷由什麼提出的評論（例如：評論者）  判斷意見詞的情緒傾向  聚集分類特徵 其中在從特徵的觀點上，還有判斷意見詞以及判斷特徵詞的問題存在。而本研究挑選意 見詞來判斷詞義主要基於在評論句當中最重要的資訊就是特徵詞以及意見詞兩者，當使 用者知道句子中的特徵詞與意見詞即可以做決策是否要購買或者使用等等。且在意見詞 方面，大多數是形容詞居多，形容詞在經過線上字典查詢過後，翻譯結果的差異性比較 大。再加上意見詞用詞程度也會影響使用者的決策，因為每個人的選擇不同，可以接受 好壞程度的範圍不同，有些人也許只去評論最好的地方，有些人則是覺得普通就可以接 受了。因此，在翻譯評論句中的意見詞詞義上更顯得重要，也是最容易造成使用者誤解 的部分。 本論文有別於現今在自然語言處理的問題之機器翻譯技術，機器翻譯技術可初略的分為 統計式（Statistical Machine Translation）與規則式（Rule Base）兩類，早期的研究者較 注重規則式的翻譯方法，但是由於網際網路上的語料越來越多，目前統計式的方法比較 受到重視。 以往，採用規則比對方式是採用人為給定規則權重的方式，以進行剖析或詞性標記等動 作。由於權重是人為給定的，這使得每條規則的影響力落入主觀的判斷之中，若在語料 庫內沒有包含此規則的現象，也很難去完全的包含取到所有的規則。而統計式的翻譯所 依靠的主要是語料庫，而非語法規則，機器翻譯系統的語料庫通常稱為平行語料庫 （Parallel Corpus），甚至，有些語料庫還記載著詞性標記的資訊的標記語料庫（Tagged Corpus），或記載著剖析樹的結構資訊為樹庫（Tree Bank）。然而，建立與取得這些語料 庫都相當花費人工與時間，準確率也並不一定較佳。 因此，就在意見分析上的重點與特色以及機器翻譯系統所需的建構成本較大。本論文著 重在非監督式的方式，利用一個線上辭典以及網路上的語料，再經由共現的情況與其他 158  特徵資訊來判斷評論句中最關鍵的意見詞的詞義。 二、文獻探討 
In this paper, we present a simple but efficient approach for the automatic mood classification of microblogging messages from Plurk platform. In contrast with Twitter, Plurk has become the most popular microblogging service in Taiwan and other countries1; however, no previous research has been done for the emotion and mood recognition, nor the Chinese affective terms or corpus available. Following the line of mashup programming, we thus construct a dynamic plurk corpus by pipelining Plurk APIs, Yahoo! Chinese segmentation APIs, etc to preprocess and annotate the corpus data. Based on the corpus, we conduct experiments by way of combining textual statistics and emoticons data, and our method yield the results with high performance. This work can be further extended to combine with affective ontology designed with emotion theory of appraisal. Keyword: mood classification, plurks, keyness, emotion paradox 
schen@mail.nctu.edu.tw 222 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 222-235, Puli, Nantou,Taiwan, September 2010.  摘要 本論文描述以隱藏式馬可夫模型為基礎發展出之「可變速中文文字轉語音系統」，訓練 語料為三種不同語速之平行語料，分別對三種語速訓練文脈相關隱藏式馬可夫模型，並 利用給予不同語速模型權重值來內插調整語速。另外，加入靜音停頓預測是相當重要 的，從語料庫觀察可以發現到慢速語音之靜音停頓較多而快速語音較少，傳統以標點符 號位置決定靜音停頓的簡單方法，在用於可變速語音合成是不適當的，因此本研究加入 預估靜音停頓之機制，對於不同語速分別訓練靜音停頓預估決策樹，再利用調整權重值 內插不同語速決策樹機率的方法，達到不同語速下靜音停頓的預估。為了評估本系統之 效能，我們對系統進行客觀測試及主觀測試；在客觀測試中，評量靜音停頓預估之效能 及量測合成語音和目標語音的誤差值；在主觀測試中，特別針對隱藏式馬可夫模型權 重、靜音停頓決策樹權重以上兩組權重值的組合比較合成語音自然度，實驗結果顯示兩 組權重值必頇匹配才可合成出較自然的語音。期望以本論文提出方法建構之系統，較傳 統單一語速之文字轉語音系統，更適合用於人機互動之中。 關鍵詞：文字轉語音系統、中文韻律、語速、停頓預估 一、緒論 （一）研究背景、動機 文字轉語音技術在人機界面裡扮演著重要的角色，隨著大型語料庫(corpus-based)以及隱 藏式馬可夫模型(HMM-based)為基礎的文字轉語音技術興起，語音合成的品質較以往進 步許多，在許多人機介面應用中已有不錯的表現，然而在不同的應用上會有不同說話速 度語音的需求，以達到更有效的溝通。拿電話語音訂票系統為例，對本國籍的互動者來 說，一般速度的合成語音可能過慢而浪費時間，這時快速語音就很適合此情形，但對於 老人或外國人士來說，提供比一般速度稍慢的電話語音，才能讓他們有足夠的反應時間 聽懂內容，在一些特定的人機互動下，傳統只做單一語速之語音合成系統便顯得不夠實 用。更進一步地，若應用語音合成於家用機器人上，希望機器人講話的方式如人類一般， 考慮在對於不同環境或不同對象下，為達成溝通，講話方式有所改變，比如說想跟遠方 的人溝通，語速會相對比較慢、且音高提高，跟較近的人溝通時，語速會比較快、且音 高較低，因此開發不同語速的語音合成系統是一個值得深入探討的議題。 （二）相關研究 1. 語音合成方法 近期語音合成系統廣為使用的合成方式主要有兩種，分別是單元選取(Unit selection approach) [1]及隱藏式馬可夫模型(HMM-based approach)[2]的語音合成方法；單元選取 合成由錄製好的語料庫中，挑選適當的語音信號片段串接合成，因此可原音重現，有極 佳的合成音質，但是如果要合成出不同特性的語音，如不同講話速度及多種情緒等應 用，則頇錄製大量的語料作為挑選單元的基礎，然欲收集不同特性之語料並不容易，因 此，對於合成不同特性語音的應用，單元選取並不是一個適合的方法。 223  基於隱藏式馬可夫模型語音合成器是一種統計式參數語音合方法，是目前最為廣泛 採用的合成方法，它以文脈相關隱藏式馬可夫模型(Context-dependent HMMs, CDHMMs) 來模擬不同語言參數或韻律架構下的聲學信號，從語料庫中的自然語音訓練得到頻譜模 型 (spectral parameter model) 、 基 頻 模 型 (F0 parameter model) 及 音 長 模 型 (duration model)。欲合成語音時，利用上述訓練好的三種模型，依據輸入文本的語言參數或預估 之韻律標記找到適當 CDHMM 模型並串接之，再以特殊的演算法由串接之 CDHMM 參 數產生 frame spectral 及 frame F0 參數，最後將 spectral 和 f0 參數輸入 MLSA 濾波器(Mel Log Spectrum Approximation filter) [3]輸出合成出語音訊號。 當想要利用現有的模型去合成出不同特性的語音訊號，可以利用調整參數的方式去 達到目的，如內插(interpolation methods) [4]、調適(adaptation methods) [5]，跟單元挑選 相反的，使用隱藏式馬可夫模型合成器，不需要大量目標的語料，只需要足夠的語料就 能利用現有隱藏式馬可夫模型去合成出不同特性的語音信號。 2. 不同語速韻律之研究 研究語音韻律的文獻雖然很多，但是有在討論相異語速的文獻很少，在陶[6]中，作者 一開始先利用對話語音的說話速度較快，以及音高軌跡範圍較朗讀式語音為窄的特性， 將朗讀式語音利用 linear regression 的方式轉換成對話語音，另外，對話語音由於說話 速度較快，音節的音高軌跡可能會因為發音不完全導致軌跡不完整，相較於在朗讀式語 音完整發音如呈現拋物線的音高軌跡，對話語音變成近似直線，利用相對於朗讀式語音 音高軌跡不完整的特性，將朗讀式語音韻律轉換為對話式語音。 在李[7]中，作者採用階層式韻律架構的觀念，採用三種不同語速之平行語料庫做 分析，實驗對於語速的測量分為兩類，一為 speech rate (SR)，定義為每秒鐘包含停頓時 長(pause duration)的發音的音節個數；另一為 articulation rate (AR)，定義為每秒鐘的音 節個數，但不包含 pause duration。實驗語料庫為四個漢語文字段落，音節數分別為 134、 123、151 和 34，實驗語料有快、中、慢速的區別，分析了在不同語速下，不同韻律單 元的 AR、SR 變化，發現改變說話速度對各韻律階層邊界的 silent pause 是非線性的， 語速的快慢會影響基頻軌跡(F0)的平均，發現的現象是快速語料的音高比較高而慢速語 料的音高比較低，且其音高軌跡的 dynamic range 比慢速語料小。此篇提出一些不錯的 觀點，但是語料庫的資料量不夠大，導致其分析結果不夠一般性是比較可惜的地方。 在鄭[8]中，根據其所提出的階層式多短語韻律句群架構，使用線性回歸統計中的 逐步回歸技術(step-wise regression technique)來估算語料，分析出三種不同中文語速之韻 律詞、韻律短語和呼吸組層次的時長和音強 pattern，解析出不同語速下，各個層次韻律 單元於時長和音強的貢獻，此實驗中平行語料庫之快速語料為一位台灣男性播音員所發 音，中速語料是由一位台灣女性播音員發音，而慢速語料則由北京女性播音員發音。此 篇研究提出了不少新的發現，但因其語料庫不是由同一人發音，會導致有些影響實驗結 果的因素沒考慮到。 上述這些文獻雖有探討到相異語速的韻律變化，但仍有幾項需要克服的因素，陶[6] 的方法提供了 bottom-up 的方式分析，僅從音節層次討論音高軌跡會忽略到韻律結構上 層的影響；至於李[7]和鄭[8]的階層式韻律架構則提供一個 top-down 的分析方式，對於 底層之音節層次分析較缺乏，此外，傳統韻律階層的研究都需要人工事先標記韻律邊 224  界，因此，江[9]同時提供 bottom-up 和 top-down 的分析方式，盡可能從各個不同面向 討論相異語速語音之韻律變化，採用的自動標記分析方法可以省時省力，同時還可兼顧 採用大量語料做研究，分別對不同語速語料的訓練得到韻律模型，藉由分析不同語速之 韻律模型參數，探討了不同語速的韻律特性，包含：（1）不同語速音節基頻軌跡之比較、 （2）不同語速之 prosodic phrasing、（3）上層韻律單元的 patterns、以及（4）break 和 語言參數的關係。此研究是近期對於不同語速韻律較大規模的研究，對於建構可變速語 音合成提供了許多實用的資訊。 （三）系統概述及研究方向 本研究是以 HMM-based 語音合成器為基礎之「可變速中文文字轉語音系統」，訓練語 料為一位女專業播音員所錄製的快、中、慢三種語速之平行語料庫，其文本為中研院 Treebank 3.0[10]選出之 348 篇短文。本研究先以這三種語速的語音資料庫，各自訓練出 不同語速的 HMM-based 語音合成器(包含頻譜及音高 CDHMM 模型及 state duration 模 型)，另外，為了由輸入的文字或語言參數決定音節之間靜音停頓的存在與否，我們分 別對三種語速的語音，以決策樹的方法由語言參數預估靜音停頓的插入。為了達到可變 速的語音合成，本研究以調整不同語速之靜音停頓決策樹模型以及 HMM-based 語音合 成器參數之權重，可內插出不同語速之合成語音，探討不同語速之靜音停頓決策樹權重 和 HMM-based 語音合成器權重關係，找到影響合成可變速語音品質的重要因素。  MEDIAN SPEECH DATABASE  Speech signal  Training part Synthesis part  訓練決策樹決 定是否要加入sp Training of decision tree for sp Word, POS and segmentation  Excitation parameter extraction  Spectral parameter extraction  Excitation parameter  Spectral parameter  Label  Training of context dependent HMM  Decision tree for sp (fast)  Decision tree for sp (median)  Decision tree for sp (slow)  Context dependent HMMs (fast)  Context dependent HMMs (median)  Context dependent HMMs (slow)  SPEECH RATE  Weight interpolation  利用決策樹決在 Label加入sp  Weight interpolation  Text analysis  Make label  Label  Parameter generation from HMM  Excitation parameter  Spectral parameter  TEXT  Excitation generation  Synthesis Filter  圖一、多語速文字轉語音系統之訓練及合成部份  SYNTHESIZED SPEECH  225  （四）漢語多語速語料簡介 本研究所採用的實驗語料庫，是由一位專業的女性播音員讀稿之快速、中速及慢速 之文本平行語料庫，此平行語料庫含有 348 個音檔，共有 48035 個音節，其語速及音高 統計資訊如表一所示，其中 AR 與 SR 的定義同二、1 節。語料庫的錄製順序是在第一 梯次先錄中速語速，接下來才將其他兩種速度錄製完成，音檔均為 20kHz 的取樣頻率 及 16-bit 之 PCM 格式，語料庫的錄製文字為 Sinica Treebank 語料庫中選出的短篇文章， 主要內容大多摘錄自新聞、網路文章、國小教科書等，由數個句子所組成的段落。所有 音 節 的 切 割 標 記 和 基 頻 軌 跡 (F0) 的 偵 測 均 先 自 動 由 Hidden Markov Model Tool Kit(HTK)[11]和 WaveSurfer[12]完成，明顯的參數錯誤再以人工修正，平均每個語句 (utterance)音節數為 138，每個句子 10.37 個字，最短及最長分別為 80 與 272 個音節。  表一、平行語料庫的平均音長、SRs 和 ARs  語料庫類型 每字平均音長(秒) SR(syllables/sec) SR 的變異數 AR(syllables/sec) AR 的變異數 F0 的平均值(Hz) F0 的變異數  Fast 0.183 4.48 0.082 5.56 0.144 201.38 2489.27  Median 0.241 3.01 0.040 4.19 0.070 195.88 2559.20  Slow 0.267 2.47 0.044 3.79 0.065 195.594 2773.37  二、文字轉語音系統之訓練 本系統是由三種語速的 CDHMM 和靜音停頓決策樹共同加權合成出可變速之語音，我 們分別對三種不同語速各自訓練出其 CDHMM 和靜音停頓決策樹，詳細方法如下。 （一）基於隱藏馬可夫模型之語音合成 (HMM-based Speech Synthesis) 我們將中文聲母、韻母、長靜音（SIL）以及短靜音( SP )模擬成五個狀態的 HMM 模型， 也就是將他們模擬成最小的 HMM 訓練單元，對於每個最小單元給予文本標示紀錄其文 脈相關資訊，利用由語料求取好的語音聲學參數和文本標示，訓練出文脈相關的頻譜及 音高 CDHMM 模型及 state duration 模型。  1. 聲學參數 (Spectral and excitation parameter extraction) 本研究中CDHMM模擬的聲學參數為廣義梅爾倒頻譜係數(Mel-generalized cepstrum, MGC)及基頻(F0)，由SPTK[13]工具抽取24階廣義梅爾倒頻譜係數，音檔取樣頻率為 20kHz，所使用的分析音框為25ms（500個資料點）的漢明窗（Hamming window），音 226  框位移為5ms（100個資料點），而抽取基頻參數則使用SNACK工具中的ESPS方法求取。  2. 文本標示 (label) 文本標示提供訓練 CDHMM 及 state duration 的文脈相關語言參數，用以在合成時挑選 適當的 CDHMM 及 state duration 模型，訓練 CDHMM 時依照文本標示提供的文脈相關 資訊對聲學參數作訓練，文本標示的文脈相關參數會影響 HMM 單元本身的頻譜及韻律 變化，也會影響 HMM 單元之間連接的狀況，如連音現象、詞首詞尾和句首句尾明顯的 音高差異及音節伸長縮短。本系統使用的文脈資訊如表二：  表二、文脈相關語言參數  pn1, pn , pn1 STn1, STn , STn1 PW1 / PW2 PS1 / PS2 PM  Previous(PRE)/current(CUR)/following(FOL) Initial/Final/SP Lexical tones of PRE/CUR/FOL syllable Syllable position in a lexical word (LW) (forward/backward) Syllable position in a sentence (forward/backward) Punctuation mark after the current syllable  Lengths of previous-previous/PRE/CUR/FOL/following-following WLn2 ,WLn1,WLn ,WLn1,WLn2 LWs in syllable  WPn2,WPn1,WPn ,WPn1,WPn2 POSs of previous-previous/PRE/CUR/FOL/following-following LWs  SLn1, SLn , SLn1  Lengths of PRE/CUR/FOL sentences in syllable  由於我們將長靜音以及短靜音視為 HMM 的訓練單元，長靜音就是在音檔開始和結 束的靜音部份，而短靜音則定義為語句中音節間大於 25ms 靜音停頓，所以在文本標示 中，對於短靜音也給予文脈相關資訊，在訓練時也會學習到不同文脈相關資訊下的停頓 長度。  3. 隱藏馬可夫模型之訓練 文本標示的文脈相關資訊組合相當多，每一種組合都是個別的 CDHMM，在訓練語料 不夠充足的情況下，多數組合的 CDHMM 訓練資料量過少，使得訓練出來的模型會不 夠準確造成過度訓練（overfitting），因此本研究使用標準的 Tree-based CDHMM 訓練方 法，以決策樹搭配適當的問題集來分群作訓練，以語言學的知識為基礎設定出合理的問 題集，對於某些資料量較少的模型可以合併在一起訓練以增加訓練的資料，可訓練出較 強健的模型。在合成時，輸入文本標示依據決策樹上每個節點的問題，可找出適當的 CDHMM 串接，進而產生聲學以及韻律參數。以下為問題集的概述：  依據前一個、現在、後一個聲母或韻母的發音方法、發音位置、送氣不送氣以及清 音濁音設定問題集。  依據前一個、現在、後一個音節聲調的調值特性作分類，設定問題集，如一聲和二  227  聲以高調值（H）為結尾、一聲和四聲以高調值為開始、二聲和三聲以中（M）或 低調( L )值開始。  考慮現在音節所在的詞長和詞的位置，將主要會影響韻律特性的位置和詞長合併， 設定為問題集，如現在音節是否在詞首或詞尾、詞長是否大於四字詞等。  考慮前後及現在詞的詞類，將中研院 46 類詞類依實詞虛詞、八大詞類及其他特殊 詞類集合合併，產生問題集。  考慮現在音節所在的句長和句子的位置，將主要會影響韻律特性的位置和句長合 併，設定為問題集，如現在音節是否在句首或句尾、句長是否大於十個字等。 由上列問題集概述的考量，本研究所設定的問題集共約 2100 個左右。 （二）基於決策樹之停頓預估 由於在訓練時把靜音停頓也視為一個 CDHMM 來作訓練，其存在與否可由語音切割資 訊來決定（短靜音定義為語句中音節間大於 25ms 靜音停頓），靜音停頓的長度（state duration）可由標準的 Tree-based CDHMM 訓練後得到的決策樹依輸入的文本標示決 定。但在合成時，靜音停頓存在與否，只能由文本標示的文脈相關語言參數資訊去預估。 本研究分別對於不同語速的語料獨自訓練其靜音停頓決策樹模型，目標為預估音節間是 否有靜音停頓。由不同語速語料靜音停頓的觀察，發現快速語料的靜音停頓較中速少， 而中速語料又比慢速少，利用這種語速語靜音停頓多寡的的關係，在合成時將決策樹對 於每個音節間是否有靜音停頓求出機率值，即為有靜音停頓的機率和沒有靜音停頓的機 率，分別利用快中慢的決策樹預估出三組機率，再利用權重值乘以相對應的機率值相 加，以達到不同語速下預估靜音停頓的目的。 本研究只考慮詞和詞之間的靜音停頓，假設詞內音節間無靜音停頓，所以預估處理 的單元為詞邊界，所使用的文脈相關資訊如訓練 CDHMM 的文本標示一樣，問題集只 考慮詞以上的語言參數，而決策樹的分裂條件為 maximum information gain。 三、多語速文字轉語音系統 （一）Text analysis: 文字分析(Text analysis)是文字轉語音系統的第一級，傳統的國語斷詞器使用的是長 詞優先及構詞規則，最著名的是中央研究院的中文斷詞系統[14]。但自 2000 年起，由 於 conditional random field (CRF)方法[15]被提出，並有效的使用在自然語言處理中的各 個問題，都被證實較傳統規則法或其他統計式方法為佳。因此，本系統的 Text analysis 的斷詞、base-phrase chunker 及詞類標記部分，便是採用 CRF 的方法做為核心，其系統 架構如下圖二所示，其中包含了(1)symbol normalization、(2) word segmentation、(2) POS(part-of-speech) tagger、(4)Word construction、(5)base-phrase chunker 及(6)grapheme to phone 六部分。 228  Character sequence Symbol Normalization Character sequence Word Segmentation  System Lexicon  Top-N Candidates of word sequence  POS Tagger Words/POSs sequence  Word construction  Words/POSs sequence  Word/POS Sequence with base-phrase tagging  Base-Phrase Chunker Grapheme to phone  Words/POSs User Lexicon Word construction Rules Pronunciation dictionary  Word/POS Sequence with base-phrase tagging, and phonetic/phonological label 圖二、Text analysis 之系統方塊圖。  1. Symbol normalization 在此級中將輸入的字串如有 ASCII 的部分，要轉換為 BIG5，另外，有很多標點符 號是屬於同一種標點符號類別，我們將這些同義異形的標點符號正規劃為其中一種作為 代表。 2. Word segmentation 由於中文文章沒有標示詞的邊界，我們必頇將詞的邊界識別出來以得到語音合成需 要的語言參數，本系統是以 CRF 以每一個中文字做為 input feature，要預估的目標為每 個中文字後的標示：{ B, B1, B2, M, E}，另外我們也可以使用 user define 的外掛字典輔 助斷詞。 3. POS tagger 利用 CRF 以詞、詞對應可能的 POS 為 input feature，預估每個詞對應到的 POS。 4. Word construction 在這一級我們以規則法，將符合構詞規則的詞由前級斷詞和標示 POS 的結果來構 成更具語法和語義的詞，這些詞包括定量複合詞、重複詞等等。 5. Base-Phrase chunker 在這一級我們利用斷出的詞和詞類，以 CRF 將一些基本語法片語標記出來，這些 基本語法片語包含 VP：述詞詞組、NP：名詞詞組、GP：方位詞詞、PP：介詞詞組、 AP/ADVP：形容詞詞組及副詞詞組。  229  6. Grapheme to phone 此級為文字分析器的最後一級，將前級所斷出的詞以一個十二萬詞的發音字典標 記上發音和聲調，另外我們也以規則法處理了一些常見的破音字，使其發音和聲調正 確。 表三為 word segmentation、POS tagging 以及 base-phrase chunker 效能的評估，其實 驗語料的設定皆為十分之九的訓練及十分之一的測試。由表所示的數據顯示，本文字分 析的效能十分優良。  實驗 Word segmentation POS tagging Base-phrase chunker  表三、文字分析器效能評估  實驗語料  accuracy precision  Bakeoff-2004 中央研究院漢語 平衡語料庫 中研院 sinica treebank3.0  98.30 94.73 93.16  95.95 94.73 92.18  recall FB1 96.79 96.37 94.73 94.73 92.27 92.22  （二）Weight interpolation:  為了達成多語速合成，本系統具有兩組權重值，一組權重值為調整預測靜音停頓決 策樹的比重，調整此權重影響最大的是 SR，當權重值調成接近慢速，預估的靜音停頓 會越來越多，利用決策樹對於每個音節間是否有靜音停頓求出機率值，即為有靜音停頓 的機率和沒有靜音停頓的機率，分別利用快中慢的決策樹預估出三組機率，在利用權重 值乘以相對應的機率值相加，以達到調整權重決定靜音停頓的目的，如下式：  3   spn*    arg max spn  i1  wi  Pi (spn  |  Ln )  (1)  其中 i 為決策樹的 index(i=1：慢，i=2：中，i=3：快)；wi 為第 i 個決策樹模型的權重值； spn {靜音停頓, 非靜音停頓} 為第 n 個詞後面的靜音停頓與否； Ln 為文脈語言資訊。  而第二組權重值影響著頻譜、音長及基頻，在語料分析中發現語速越快不僅音長變 短，基頻也會隨著拉高，直接影響到隱藏式馬可夫模型的參數，很直觀地，當調整權重 值越靠近快速語速語音之隱藏式馬可夫模型時，相對於僅使用慢速語音之隱藏式馬可夫 模型，每個音節的音長會變短且音頻會提高。以不同權重值內差三種語速之模型參數方 法如下式[5]：  3   μ  ai μi  (2)  i 1  3   U  ai2 Ui  (3)  i 1  230  其中 i 為 CDHMM 模型的 index(i=1：慢，i=2：中，i=3：快)； ai 為第 i 個 CDHMM 模 型的權重值， μi 及 Ui 分別為 CDHMM state 之 mean vector 及 covariance matrix。 第一組權重值影響靜音停頓的變化，而第二組權重值影響了音長、頻譜及基頻，在 自然的語音訊號中，慢速語料靜音停頓較多，音節音長也會拉長，快速語料則相反。在 給定權重值也需要按照語速的規則，當想要合成語速較快的語音訊號時，增加快速語速 之靜音停頓決策樹的比重，使靜音停頓預估出的數量較少，只在適合的位置給定靜音停 頓，同時，我們也調整隱藏式馬可夫模型權重，增加快速語料的比重，而可以產生出較 短的音長及較高的基頻，這兩組權重值需要有正相關才會匹配，兩組不匹配的權重值會 合成出不自然的語音訊號，因此在不同語速下兩組權重值的匹配是相當重要的，在之後 的實驗會對這兩組權重值匹配作主觀測試的實驗。 （三）Make label: 欲合成的文字經由文本分析後，可得到對應文脈相關的語言參數資訊，使用之前以內插 靜音停頓決策樹所預估之靜音停頓，放入文本標示(label)，最終產生的 label 具有欲合成 文本中每個聲母、韻母及靜音停頓的文脈相關語言參數。 （四）Parameter generation from HMM: 在 make label 步驟後產生文本標示(label)，依據文本標示使用三種語速之 CDHMM 模 型、state duration 模型及 CDHMM 模型參數權重，由文本相關決策樹找到適當的模型， 首先預估出每個聲母、韻母或靜音停頓的長度，再利用 maximum likelihood 法[2]產生每 個音框的 logF0 及 MGC 頻譜參數。 （五）Excitation generation and Synthesis Filter: 將上一步得到的每個音框之 logF0 和 MGC 頻譜參數輸入至 MSLA filter (Mel-Log Spectrum Approximation filter) [2]產生合成語音。 四、實驗結果及討論 實驗語料已於一、(四)中介紹，對於每種語速取其約 328 個語句為訓練語料，另 20 句 為測試語料，為了評估本可變速漢語語音合成系統的效能，我們分別對合成語音進行客 觀及主觀的測試。在客觀測試方面，我們量測了靜音停頓決策樹的預估正確性，另外， 也量測了整個系統合成語音和目標語音的量化誤差。而在主觀測試方面，對系統兩組權 重值匹配的狀況作主觀測試的實驗，合成音檔的展示請連結 http://140.113.144.71。 （一）客觀測試 在第一個實驗中，我們對靜音停頓決策樹的效能進行評估，計算合成音檔和目標語句的 靜音停頓預估的正確率及混淆程度，因為只有單純三種語速的目標語句，沒有實際介於 這三種語速的目標語句，所以只有對於三種不同語速目標語句的預估結果作觀察，以合 成快速語音為例，當測試快速語料時，我們調整快速的靜音停頓預估決策樹權重值為 1， 其他語速之權重為 0，中慢速測試亦同。表四為預估靜音停頓對於快中慢語速的結果。 231  由實驗結果發現，對於快速合成語音預估靜音停頓的結果是最差的，錯誤大多是在 預測目標語句有靜音停頓的部份，主要原因可能是因為快速語料裡音節間的靜音停頓較 少，所以造成了決策樹學習到音節間無靜音停頓的機率較大，在預測結果也是偏向沒有 靜音停頓，另外可能的原因，是考慮到快速語料不論是 AR 和 SR 變化都是最大的，語 句和語句間語速有較大的差異，因為語速和靜音停頓的多寡有關係，語速的差異潛在會 造成快速語料靜音停頓預估上的困難。在慢速語料上雖然在非靜音停頓預測上略輸快速 語料，但在有靜音停頓預測上比快速語料準得多，可能是因為語者於朗讀慢速語料時， 會將詞或韻律詞的結構清楚念出，所以在語料上產生較一致性的靜音停頓，可以從語言 參數學到規則，因此準確度比快速要高的多。  表四、不同語速下預估靜音停頓的結果，XX*代表預測為靜音停頓或非靜音停頓（以百 分比表示），Total 為 Non-SP 或 SP 的總個數。  慢  Inside  Outside  Non-SP* SP*  Total  Non-SP* SP*  Total  Non-SP 90.05  9.95  28108  Non-SP 89.66  10.34  1885  SP  30.19 69.81 20486  中  Inside  SP Outside  33.57  66.43  1415  Non-SP* SP*  Total  Non-SP* SP*  Total  Non-SP 92.77  7.23  29119  Non-SP 91.55  8.45  1977  SP  37.81 62.19 19314  快  Inside  SP Outside  39.61  60.39  1323  Non-SP* SP*  Total  Non-SP* SP*  Total  Non-SP 96.34  3.66  35380  Non-SP 94.83  5.17  2496  SP  49.5  50.5  11613  SP  52.74 47.26  804  第二個客觀測試，我們分別測量合成和目標語句其基頻、停頓靜音的長度以及音節 的長度的誤差，使用均方根誤差（Root Mean Square Error, RMSE）用來評估誤差值，因 語料庫只有三種語速，所以在測量快速語料的 RMSE 時，預測靜音停頓決策樹的權重 和隱藏馬可夫式模型的權重，均設定快速權重值為 1，其他權重設為 0，中慢速語料也 使用同樣的方法測量，表五為實驗結果。 由整體來看 Inside test 的 RMSE 都低於 Outside test 這是因為過度訓練（overfitting） 的關係，經觀察發現靜音停頓音長的預測不論 Outside test 和 Inside test 在語速快的 RMSE 均為最低，由於快速語速在靜音停頓的音長並不長，就算靜音停頓沒有正確預估 出來，誤差也不會太大，而慢速的靜音停頓就不一樣，靜音停頓音長較長，沒有正確預 估到靜音停頓誤差就會較大，我們觀察音節音長的 RMSE 也看到同樣的結果，在語速 快的音節音長 RMSE 均為最低，因為快速語料音節音長都較短，計算誤差也不會太大。  232  表五、快中慢語料作測試之 RMSE 值  測試項目  語速 Fast Median Slow  Inside F0 (Hz)  36.28 34.38 35.21  Outside F0 (Hz)  42.66 42.78 45.23  Inside sp duration (ms)  44.97 64.19 84.17  Outside sp duration (ms)  56.55 60.02 85.55  Inside syllable duration (ms) 37.53 41.44 44.19  Outside syllable duration (ms) 39.23 42.66 47.08  （二）主觀測試  主觀測試目的為測試系統兩組權重值不同的組合，以主觀測試判別合成語音的自然度， 對於快中慢兩組權重值設為：1-0-0、0-1-0、0-0-1、0-0.5-0.5（x-x-x 中的 x 順序代表慢 速、中速、以及快速權重值），因為考慮的組合數量過多，而且慢速跟中速語料依據 SR、 AR 以及基頻的統計差異並不大，因此不考慮 0.5-0.5-0 這個權重值組合，所以本實驗只 有 16 種靜音停頓-CDHMM 權重的組合。  每一個合成文本均為 outside test 的語句，一個文本依據兩組權重值變化會產生 16 種不同語速變化的合成音檔，各分為四組作測試，以同樣隱藏馬可夫模型的權重值為同 一組，目的為固定一組權重值，觀察不同權重預測靜音停頓的匹配程度。主觀測試中語 音自然度的評分為五分制，分數為一至五，一為最不自然，五為最自然，總共對 6 人作 主觀測試，每個測試者由九句文本中選聽兩句文本的語句，其中一句文本與另一個測試 者重複，因為每文本有 16 種語速權重組合，所以每個人聽 32 句測試語句，整個測試語 句共有 192 句，測試結果如表六所示。  表六、主觀測試的平均值± 一個標準差，x-x-x 中的 x 順序代表慢、中、快權重值  預測靜音停頓權重值  1-0-0  隱藏馬可夫權重值  0-1-0  0-0.5-0.5  0-0-1  1-0-0  2.33± 0.61  3.08± 1.36  2.79± 0.98  2.21± 0.70  0-1-0  2.54± 0.88  3.38± 0.96  3.25± 0.391  2.21± 0.52  0-0.5-0.5  2.67± 0.60  3.08± 0.99  3.67± 0.79  
In this paper, we propose a novel scheme in performing feature statistics normalization 236 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 236-250, Puli, Nantou,Taiwan, September 2010.  techniques for robust speech recognition. In the proposed approach, the processed temporal domain feature sequence is first converted into the modulation spectral domain. The magnitude part of the modulation spectrum is decomposed into overlapped non-uniform sub-band segments, and then each sub-band segment is individually processed by the well-known normalization methods, like mean normalization (MN) and mean and variance normalization (MVN). Finally, we reconstruct the feature stream with all the modified sub-band magnitude spectral segments and the original phase spectrum using the inverse DFT. With this process, the components that correspond to more important modulation spectral bands in the feature sequence can be processed separately and more spectral samples within each band give rise to more accurate statistic estimates due to overlapping the adjacent segments. For the Aurora-2 clean-condition training task, the new proposed overlapping sub-band spectral MN and MVN provide further error rate reductions over the conventional non-overlapping ones. 關鍵詞：語音辨識、調變頻譜、正規化、強健性語音特徵參數 Keywords: speech recognition, modulation spectrum, statistics normalization, robust speech features 一、簡介 目前多數的語音辨識系統若在不受干擾的安靜環境下，且辨識的字彙量不是很大時，一 般而言皆能得到相當滿意的辨識效果，但應用於許多真實的生活環境中，辨識效能一定 會有所衰減，主要是實際生活環境與系統發展之環境彼此之間不匹配的情況發生，其中 影 響 語 音 辨 識 的 變 異 性 有 訓 練 環 境 與 測 試 環 境 之 間 的 環 境 不 匹 配 environmental mismatch)、語者變異性(speaker variation)及發音的變異性(pronunciation variation)等因 素，這些因素都會明顯影響語音辨識系統的效能。如何抑制環境不匹配的背景雜訊干擾 問題，使其所帶來之語音辨識下降的影響達到最小，進而提升語音系統強健性，是本論 文主要研究的方向。 為了降低以上各種變異性所發展的各種技術，一般而言統稱為強健性技術(robustness techniques)，本論文主要著重於發展降低環境之雜訊干擾的強健性演算法。 而在多種 的強健性演算法中，有一類方法是將訓練與測試環境下的語音特徵 (通常為"倒頻譜''特 徵) 其時間序列統計特性做正規化，以降低訓練與測試環境之間的不匹配，進而達到提 升辨識率的目的。 著名的語音統計正規化法包括了倒頻譜平均值正規化法(cepstral mean normalization, CMN)[1]、倒頻譜平均值與變異數正規化法(cepstral mean and variance normalization, CMVN)[2]與統計圖等化法(histogram equalization, HEQ)[3]等。以 上各種方法主要是執行在語音倒頻譜特徵的時間序列域上，但在其效能的分析上，我們 通常進一步探討雜訊及通道效應對於原始特徵之調變頻譜造成的失真，分析這些方法對 於調變頻譜失真的改善效果，因此近年來，開始有學者提出直接於特徵之調變頻譜域上 使 用 統 計 正 規 化 技 術 ， 如 調 變 頻 譜 統 計 圖 等 化 法 (spectrum histogram equalization, SHE)[4] ， 此 方 法 是 針 對 語 音 特 徵 之 調 變 頻 譜 的 強 度 成 分 之 機 率 分 佈 (probability distribution)作正規化處理；而根據許多的研究[5][6]證實，對語音辨識而言，不同頻率 的調變頻譜成份具有不相等的重要性。學者 N. Kanedera 詳細指出大部分的語音辨識資 訊分布在 1 Hz 和 16 Hz 的調變頻率之間[7]，且主要集中在 4 Hz 附近。藉由以上之各 觀 點 ， 因 此 過 去 本 實 驗 室 的 研 究 中 [8] ， 嘗 試 將 調 變 頻 譜 中 的 強 度 頻 譜 (magnitude 237  spectrum)切割成許多子頻段，且將低頻的部份切的比較細，再分別對各自子頻段的統計 值作正規化處理，這種方法可以強調出低頻成份中語音的資訊，進而提升辨識率。 但 是我們發現，當我們將頻段數目分的越多，低頻的部份會因為頻段過窄，導致包含的頻 譜點數變少，進而影響我們求取統計量的精確度。 也因為如此，在本論文中，我們提 出了基於強度頻譜之重疊式分頻段調變頻譜統計正規化法，其目的是藉由子頻段範圍的 延伸，相對的增加各子頻段的頻譜點數，使我們能更精確的計算出統計值。而也因為各 子頻段彼此重疊，能增加彼此的相關性，並且提升辨識的效能。 本論文其他章節概要如下：在第二章將介紹本論文所提出之重疊式分頻段的統計正規化 法其背景原理及其相關的步驟說明。第三章將呈現並討論一系列重疊式分頻段調變頻譜 統計正規化法的實驗結果，並且分析結合其他強健性特徵正規化法的效能。第四章則是 結論與未來展望。 二、重疊式分頻帶調變頻譜統計正規化法 如前章所述，若以分頻段的形式分別對頻譜正規化，會得到較佳的強健性效果。然而， 以不等切的方式將調變頻譜切割成大小不一的頻段（如圖一(a)所示），進而正規化，雖 然效果相較於未切割的全頻式處理效果較好，然而其潛在問題為： 1. 我們不能將頻段切得太細，否則將會使較窄細之頻段內所包含的頻譜點數過少甚至 等於零，進而使欲估測的統計值因點數過少而精確度大打折扣，甚至無法計算統計 值，而使正規化演算法無法執行。 2. 由於這些頻譜點數較少的頻段恰好都在較低頻的位置，如前所述，較低頻的成分對 於語音辨識相對重要，若在這些低頻頻段處理不當，會使原先分頻段正規化處理的 優勢降低。 基於以上的觀察，在本論文中，提出了一系列的改進方法來補償上述之低頻段頻譜點數 不足的缺點，這一系列的改進方法，基本精神就是將原先定義的子頻段作某程度上的延 伸，使其涵蓋的頻率範圍變大，進而得以包括較多的頻譜點數，目的在於求取較精確的 統計值，來使原先所構想的分頻段統計值正規化法能有更佳的表現。以上所述之延伸子 頻段長度的方式，是將原先方法中互不重疊(non-overlapping)的子頻段，修正為相鄰子 頻段可以互相重疊(overlapping)，且進而衍生出三種重疊的方式（目前初始的研究中， 我們暫定其被重疊的頻段寬度為原始寬度的一半），分別為 1. 向左重疊式(left-sided overlapping)子頻段切割法，即新的子頻段為原始子頻段向左 （較低頻）延伸，額外包含了其左方第一個子頻段的一半寬度（最低子頻段除外）， 此類型所對應的各種正規化法，將附帶以上標「lo」表示，其中的字母 l 與 o 分別 代表了"left-sided"與"overlapping"。示意圖如圖一(b)所示。 2. 向右重疊式(right-sided overlapping)子頻段切割法，即新的子頻段為原始子頻段向右 （較高頻）延伸，額外包含了其右方第一個子頻段的一半寬度（最高子頻段除外）， 此類型所對應的各種正規化法，將附帶以上標「ro」表示，其中的字母 r 與 o 分別 代表了"right-sided"與"overlapping"。示意圖如圖一(c)所示。 3. 雙邊重疊式(two-sided overlapping)子頻段切割法，即新的子頻段為原始子頻段向右 238  (較高頻)延伸，額外包含了其右方第一個子頻段的一半寬度（最低子頻段僅向右延 伸，而最高子頻段則僅向左延伸），此類型所對應的各種正規化法，將附帶以上標「to」 表示，其中的字母 t 與 o 分別代表了"two-sided"與"overlapping"。示意圖如圖一(d) 所示。  圖一：各種分頻段方法之示意圖 (a)非重疊式(b)向左重疊式(c)向右重疊式(d)雙邊重疊式  另外，以上所提到之原始方法裡，低頻帶之子頻段頻譜點數較少、導致求取此子頻段之 頻譜統計量精確度不足問題，主要是發生在處理單一語句特徵的情形下，當我們欲得到 單一（非重疊）子頻段之目標統計值(target statistics)時，由於是把所有訓練語句之此一 子頻段頻譜點集合起來再作運算，因此較無上述之點數不足的問題，因此，在本論文中， 我們新提出的重疊式方法又可分成兩種，分別為： 第 I 型：利用全部訓練語句求取單一子頻段的目標統計值時，各子頻段是可以相互重疊 的，但重疊方式必須與執行單一語句之子頻段正規化所使用的重疊方式相同。 第 II 型：利用全部訓練語句求取單一子頻段的目標統計值時，各子頻段並不相互重疊， 即求取目標統計值時，各子頻段的切割方式與原始方法相同。  根據以上的敘述，在固定子頻段數目的前提下，子頻段的目標統計值上的計算法有兩種 選擇，子頻段重疊的樣式上有三種選擇，排列組合之下，可推演出新的分頻段正規化方 法共有六類。為了英文縮寫的表示上可以不至混淆，我們定義以下的表示式：  SB正規化法名稱((分子頻頻個段數重)疊方式) - X  其中 SB 表示分頻段(sub-band)，正規化名稱可為 SMN（頻譜平均值正規化法）、SMVN （頻譜平均值與變異數正規化法）與 SHE（頻譜統計圖等化法），分頻個數介於 4 到 6 之間，子頻段重疊方式為 lo, ro 與 to 三種，而 X 可為 I（第 I 型：目標統計值以重疊式 子頻段計算）或 II（第二型：目標統計值以非重疊式子頻段計算）。舉例而言，  SBMVN  (to ) (6)  -  I  表示了分頻式平均值與變異數正規化法，其中分了  6  個子頻段，子頻段  是左右重疊，且目標統計值是以重疊式子頻段方式計算。  239  以下為我們所提出之重疊式分頻段調變頻譜統計正規化的步驟，同時，圖二為此方法的  流程圖：  1. 假設一段語音的倒頻譜特徵參數序列如下式(1)表示：  {x (m) [n ];1 £ n £ N }, 1 £ m £ M ,  式(1)  其中 M 為一語音特徵向量中特徵的總數，N 代表單一語句的音框總數。每個特徵序列 {x (m) [n ]} 經正規化處理後，以 {x(m) [n ]} 表示，我們希望新的特徵序列 {x(m) [n ]} 相對於原  始特徵序列而言，更具有強健性，才能讓辨識效果有明顯地提升。在之後的敘述，為了  精簡符號的標示，我們省略了上標"(m)"符號。將特徵序列{x [n];1 £ n £ N } 經 N 點離 散傅立葉轉換(discrete Fourier transform, DFT)後得到其調變頻譜{X [k ]} ，如下式(2)表 示。  å X  éêëk ùúû  =  N -1  x  éêën  ùúû  e-j  2  pnk N  ,  n=0  0  £  k  £  éêêê  N 2  ùúúú  ,  式(2)  假設 {x [n ]}的音框取樣頻率(frame rate)為 Fs Hz，則在其調變頻譜域上{X [k ]} 的頻率範  圍為  éêêë0,  Fs 2  ùúúû  ；因  X  [k  ]  通常為一複數，我們以極座標(polar  form)表示  X  [k  ]：  X [k ] = A[k ]e jqk ,  式(3)  其中 A[k ] 是 X [k ] 的強度成份， q [k ] 是 X [k ] 的相位成份，接下來我們只針對強度成份 {A[k ]} 去做處理，而保留相位成份{q[k ]}不變。  2. 將上一步驟調變頻譜的強度成分 ïíïïîìA[k ]; 0 £ k £ éêêN2 ùúúïýïïþü 以不等切(non-uniform)且重疊 (overlapping)的方式，切割成 L 個頻段，依照我們所提出的三種重疊方法，每個頻段的 範圍如下所示：  （1）向左重疊式的方法，每個頻段的範圍表示為式(4)：  ïïïïîìïïïíïïïïëéêêëéêêçèæçç02,22-LL12-1+æççè  Fs 2  ÷ö÷÷øùúúû  ,  2-2 2L+1  ÷øö÷÷èæçç  Fs 2  ø÷÷÷ö,  2-1 2L-1  èççæ  Fs 2  ø÷÷÷öûùúú  ,  if  = 1. if  = 2, 3,..., L.  式(4)  （2）向右重疊式的方法，每個頻段的範圍表示為式(5)：  240  ïïïïîìïïïïïïïïïïíïïïïïïêêëêêêêëéêëééêê02222,LLççæçè----22211 L1çççççççèæçèæ-FF122ss  + øö÷÷÷÷, ÷÷ø÷ö÷,  
The following represents a tentative list of topics that will be covered: * Introduction to parallel and distributed processing * Introduction to MapReduce * Tradeoffs and issues in algorithm design * Simple counting applications (e.g., relative frequency estimation) * Applications to inverted indexing and text retrieval * Applications to graph algorithms 
 developing successful applications. This part will focus on supervised learning and the duration will be approximately an hour. 
Part C: Transition-based Dependency Parsing Models - Learning Algorithms (Local Learning vs. online Learning) - Parsing Algorithms (Shift-reduce Parsing) - Features 
Tutorial Outline After shortly motivating and introducing the general framework, the main part of the tutorial is a methodological presentation of some of the key computational issues studied within CCMs that we will present by looking at case studies published in the NLP literature. In the last part of the tutorial, we will discuss engineering issues that arise in using CCMs and present some tool that facilitate developing CCM models. 
2. TUTORIAL OUTLINE 1) Introduction - motivation and brief history of distributional semantics - common DSM architectures - prototypical applications - concrete examples used in the tutorial 
Deﬁne and motivate the Recognizing Textual Entailment (RTE) task. Introduce the RTE evaluation framework. Deﬁne the relationship between RTE and other major NLP tasks. Identify (some of) the semantic challenges inherent in the RTE task, including the introduction of 'contradiction' as an entailment category. Describe the use of RTE components/techniques in Question Answering, Machine Translation, and Relation Extraction. 2. The State of the Art (35 minutes) 
 a signiﬁcant performance boost from an overall F-  measure of 24.7 to 27.6. Coordination structures are  Determining the correct structure of coordinating conjunctions and the syntactic constituents that they coordinate is a difﬁcult task. This subtask of syntactic parsing is explored here for biomedical scientiﬁc literature. In particular, the intuition that sentences containing coordinating conjunctions can often be rephrased as two or more smaller sentences derived from the coordination structure is exploited. Generating candidate sentences corresponding to different possible coordination structures and comparing them with a language model is employed to help determine which coordination structure is best. This strategy is used to augment a simple baseline system for coordination resolution which outperforms both the baseline system and a constituent parser on the same task.  the source of a disproportionate number of parsing errors for both constituent parsers (Clegg and Shepherd, 2007) and dependency parsers (Nivre and McDonald, 2008). CR is difﬁcult for a variety of reasons related to the linguistic complexity of the phenomenon. There are a number of measurable characteristics of coordination structures that support this claim including the following: constituent types of conjuncts, number of words per conjunct, number of conjuncts per conjunction, and the number of conjunctions that are nested inside the conjunct of another conjunction, among others. Each of these metrics reveal wide variability of coordination structures. For example, roughly half of all conjuncts consist of one or two words while the other half consist of three or more  words including 15% of all conjuncts that have ten  
 Readability or Fluency of a summary is categor-  Readability of a summary is usually graded manually on ﬁve aspects of readability: grammaticality, coherence and structure, focus, referential clarity and non-redundancy. In the context of automated metrics for evaluation of summary quality, content evaluations have been presented through the last decade and  ically measured based on a set of linguistic quality questions that manual assessors answer for each summary. The linguistic quality markers are: grammaticality, Non-Redundancy, Referential Clarity, Focus and Structure and Coherence. Hence readability assessment is a manual method where expert assessors give a rating for each summary on  continue to evolve, however a careful examination of readability aspects of summary quality has not been as exhaustive. In this paper we explore alternative evaluation metrics for ‘grammaticality’ and ‘coherence and structure’ that are able to strongly correlate with manual ratings. Our results establish that our methods are able to perform pair-wise rank-  the Likert Scale for each of the linguistic quality markers. Manual evaluation being time-consuming and expensive doesn’t help system developers — who appreciate fast, reliable and most importantly automated evaluation metric. So despite having a sound manual evaluation methodology for readability, there is an need for reliable automatic metrics.  ing of summaries based on grammaticality, as strongly as ROUGE is able to distinguish for content evaluations. We observed that none  All the early approaches like Flesch Reading Ease (Flesch, 1948) were developed for general texts and  of the ﬁve aspects of readability are independent of each other, and hence by addressing the individual criterion of evaluation we aim to achieve automated appreciation of readability of summaries.  none of these techniques have tried to characterize themselves as approximations to grammaticality or structure or coherence. In assessing readability of summaries, there hasn’t been much of dedicated analysis with text summaries, except in  
A Progressive summary helps a user to monitor changes in evolving news topics over a period of time. Detecting novel information is the essential part of progressive summarization that differentiates it from normal multi document summarization. In this work, we explore the possibility of detecting novelty at various stages of summarization. New scoring features, Re-ranking criterions and ﬁltering strategies are proposed to identify “relevant novel” information. We compare these techniques using an automated evaluation framework ROUGE, and determine the best. Overall, our summarizer is able to perform on par with existing prime methods in progressive summarization. 
This paper reports on one aspect of Locutus, a natural language interface to databases (NLIDB) which uses the output of a highprecision broad-coverage grammar to build semantic representations and ultimately SQL queries. Rather than selecting just a subset of the parses provided by the grammar to use in further processing, Locutus uses all of them. If the meaning of a parse does not conform to the semantic domain of the database, no query is built for it. Thus, intended parses are chosen extrinsically. The parser gives an average of 3.01 parses to the sentences in the GEOQUERY250 corpus. Locutus generates an average of 1.02 queries per sentence for this corpus, all of them correct.  processing. For some applications, however, it is possible, and indeed preferable, to pass all the parses on and let downstream processing decide which parses to use. This paper describes such an application. Locutus (Goss-Grubbs to appear), a natural language interface to relational databases (NLIDB), creates semantic representations for the parses assigned by a high-precision broad-coverage grammar, and from those creates SQL queries. It does not include a step where one or more “best” parses are selected for further processing. Queries are built for all parses for which it is possible to do so. For a standard corpus of NLIDB training sentences, it is able to generate the correct query whenever a suitable analysis is given by the parser. In the rare case where it generates two queries, both queries are equally correct.  
Vector-based distributional models of semantics have proven useful and adequate in a variety of natural language processing tasks. However, most of them lack at least one key requirement in order to serve as an adequate representation of natural language, namely sensitivity to structural information such as word order. We propose a novel approach that offers a potential of integrating order-dependent word contexts in a completely unsupervised manner by assigning to words characteristic distributional matrices. The proposed model is applied to the task of free associations. In the end, the ﬁrst results as well as directions for future work are discussed. 
When humans communicate via natural language, they frequently make use of metalanguage to clarify what they mean and promote a felicitous exchange of ideas. One key aspect of metalanguage is the mention of words and phrases, as distinguished from their use. This paper presents ongoing work on identifying and categorizing instances of languagemention, with the goal of building a system capable of automatic recognition of the phenomenon. A definition of language-mention and a corpus of instances gathered from Wikipedia are discussed, and the future direction of the project is described. 
 Resampling methods have been found effective in  In this paper we present a novel resampling model for extractive meeting summarization. With resampling based on the output of a baseline classiﬁer, our method outperforms previous research in the ﬁeld. Further, we compare an existing resampling technique with our model. We report on an extensive series of experiments on a large meeting corpus which leads to classiﬁcation improvement in weighted precision and f-score.  catering to the data imbalance problem mentioned above. (Corbett and Copestake, 2008) used a resampling module for chemical named entity recognition. The pre-classiﬁer, based on n-gram character features, assigned a probability of being a chemical word, to each token. Only tokens having probability greater than a predeﬁned threshold were preserved and the output of the ﬁrst stage classiﬁcation along with word sufﬁx were used as features in further classiﬁcation steps. (Hinrichs et al., 2005) used a  
 classiﬁed into a relation among fourteen relations.  Temporal relation classiﬁcation task has issues of fourteen target relations, skewed distribution of the target relations, and relatively small amount of data. To overcome the issues, methods such as merging target relations and increasing data size with closure algorithm have been used. However, the method using merged relations has a problem on how to recover original relations. In this paper, a new reduced-relation method is proposed. The method decomposes a target relation into four pairs of endpoints with three target relations. After classifying a relation of each endpoint pair, four classiﬁed relations are combined into a relation of original fourteen target relations. In the combining step, two heuristics are examined.  The second issue is that the number of TLINKs is relatively small in spite of the fourteen targets. The third issue is skewed distributions of the relations. Without the solutions of the issues, it is impossible to achieve good performance in temporal relation identiﬁcation through machine learning techniques. Several solutions have been suggested such as increased number of TLINKs with a transitivity closure algorithm (Mani et al., 2007; Chambers et al., 2007) and decreased target relations into six (Mani et al., 2006; Chambers et al., 2007; Tatu and Srikanth, 2008) or three (Verhagen et al., 2007). An issue of the reduced-relation method is how to recover original relations. A module for the recovery can cause performance degeneration and seems intuitively inappropriate.  
In this paper, we propose to identify opinion holders and targets with dependency parser in Chinese news texts, i.e. to identify opinion holders by means of reporting verbs and to identify opinion targets by considering both opinion holders and opinion-bearing words. The experiments with NTCIR-7 MOAT’s Chinese test data show that our approach provides better performance than the baselines and most systems reported at NTCIR-7.  opinion targets should not be done alone without considering opinion holders, because opinion holders are much easier to be identified in news texts and the identified holders are quite useful for the identification of the associated targets. Our approach shows encouraging performance on opinion holder/target identification, and the results are much better than the baseline results and most results reported in NTCIR-7 (Seki et al., 2008). The paper is organized as follows. Sec. 2 introduces related work. Sec. 3 gives the linguistic analysis of opinion holder/target. The proposed approach is described in Sec. 4, followed by the experiments in Sec. 5. Lastly we conclude in Sec. 6.  
In this paper, we describe a syntax based source side reordering method for phrasebased statistical machine translation (SMT) systems. The source side training corpus is ﬁrst parsed, then reordering rules are automatically learnt from source-side phrases and word alignments. Later the source side training and test corpus are reordered and given to the SMT system. Reordering is a common problem observed in language pairs of distant language origins. This paper describes an automated approach for learning reorder rules from a word-aligned parallel corpus using association rule mining. Reordered and generalized rules are the most signiﬁcant in our approach. Our experiments were conducted on an English-Hindi EILMT corpus. 
This paper describes an experiment designed to evaluate the development of a Statistical Transfer-based Brazilian Portuguese to English Machine Translation system. We compare the performance of the system with the inclusion of new syntactic written rules concerning verbal tense between the Brazilian Portuguese and English languages. Results indicate that the system performance improved compared with an initial version of the system. However significant adjustments remain to be done. 
We describe a novel semantic search engine for scientiﬁc literature. The Camtology system allows for sentence-level searches of PDF ﬁles and combines text and image searches, thus facilitating the retrieval of information present in tables and ﬁgures. It allows the user to generate complex queries for search terms that are related through particular grammatical/semantic relations in an intuitive manner. The system uses Grid processing to parallelise the analysis of large numbers of papers. 
This demo describes the summarization of textual material about locations in the context of a geo-spatial information display system. When the amount of associated textual data is large, it is organized and summarized before display. A hierarchical summarization framework, conditioned on the small space available for display, has been fully implemented. Snapshots of the system, with narrative descriptions, demonstrate our results. 
We present a new Java-based open source toolkit for phrase-based machine translation. The key innovation provided by the toolkit is to use APIs for integrating new features (/knowledge sources) into the decoding model and for extracting feature statistics from aligned bitexts. The package includes a number of useful features written to these APIs including features for hierarchical reordering, discriminatively trained linear distortion, and syntax based language models. Other useful utilities packaged with the toolkit include: a conditional phrase extraction system that builds a phrase table just for a speciﬁc dataset; and an implementation of MERT that allows for pluggable evaluation metrics for both training and evaluation with built in support for a variety of metrics (e.g., TERp, BLEU, METEOR). 
This paper demonstrates two annotation tools related to Propbank: Cornerstone and Jubilee. Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles. Propbank annotation also requires the choice of a sense id for each predicate, deﬁned in the corresponding frameset ﬁle. Jubilee expedites the annotation process by displaying several resources of syntactic and semantic information simultaneously; easy access to each of these resources allows the annotator to quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and eﬃcient annotation. Cornerstone is a user-friendly xml editor, customized to allow frame authors to create and edit frameset ﬁles. Both tools have been successfully adapted to many Propbank projects; they run platform independently, are light enough to run as X11 applications and support multiple languages such as Arabic, Chinese, English, Hindi and Korean. 
We present an innovative application of dialogue processing concepts to educational technology. In a previous corpus analysis of peer learning dialogues, we found that initiative and initiative shifts are indicative of learning, and of learning-conducive episodes. We have incorporated this ﬁnding in KSC-PaL, a peer learning agent. KSC-PaL promotes learning by encouraging shifts in task initiative. 
We present an English lexical database which is fuller, more accurate and more consistent than any other. We believe this to be so because the project has been well-planned, with a 12-month intensive planning phase prior to the lexicography beginning; well-resourced, employing a team of ﬁfteen highly experienced lexicographers for a thirty-month main phase; it has had access to the latest corpus and dictionary-editing technology; it has not been constrained to meet any goals other than an accurate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use.  register to collocation to sense distinction) that make lexicography complex. Unsupervised corpus methods are intellectually exciting but do not provide the lexical facts that many applications need. We present DANTE (Database of Analysed Texts of English), an English lexical database. For the commonest 50,000 words of English, it gives a detailed account of the word’s meaning(s), grammar, phraseology and collocation and any noteworthy facts about its pragmatics or distribution. In outline this is what dictionaries have been doing for many years. This database is of more interest to NLP than others (for English) because of its: • quality and consistency • level of detail • number of examples  
This demo abstract presents an interactive tool for supporting error analysis for text mining, which is situated within the Summarization Integrated Development Environment (SIDE). This freely downloadable tool was designed based on repeated experience teaching text mining over a number of years, and has been successfully tested in that context as a tool for students to use in conjunction with machine learning projects.  In the remainder of this demo abstract, we offer an overview of the development environment that provides the context for this work. We then describe on a conceptual level the error analysis process that the tool seeks to support. Next, we step through the process of conducting an error analysis with the interface. We conclude with some directions for our continued work, based on observation of students’ use of this interface. 2 Overview of SIDE  
In this demonstration we will present technologies that enable learners to engage in spoken conversations in foreign languages, integrating intelligent tutoring and serious game capabilities into a package that helps learners quickly acquire communication skills. Conversational AI technologies based on the SAIBA framework for dialog modeling are realized in this 3-D game environment. Participants will be introduced to tools for authoring dialogs in this framework, and will have an opportunity to experience learning with Alelo products, including the Operational Language and Culture Training System (OLCTS). 
Dialogue systems typically follow a rigid pace of interaction where the system waits until the user has ﬁnished speaking before producing a response. Interpreting user utterances before they are completed allows a system to display more sophisticated conversational behavior, such as rapid turn-taking and appropriate use of backchannels and interruptions. We demonstrate a natural language understanding approach for partial utterances, and its use in a virtual human dialogue system that can often complete a user’s utterances in real time. 
This paper introduces a Web-based demonstration of an interactive-predictive framework for syntactic tree annotation, where the user is tightly integrated into the interactive parsing system. In contrast with the traditional postediting approach, both the user and the system cooperate to generate error-free annotated trees. User feedback is provided by means of natural mouse gestures and keyboard strokes. 
SIMPLIFICA is an authoring tool for producing simplified texts in Portuguese. It provides functionalities for lexical and syntactic simplification and for readability assessment. This tool is the first of its kind for Portuguese; it brings innovative aspects for simplification tools in general, since the authoring process is guided by readability assessment based on the levels of literacy of the Brazilian population. 
This document describes the properties and some applications of the Microsoft Web Ngram corpus. The corpus is designed to have the following characteristics. First, in contrast to static data distribution of previous corpus releases, this N-gram corpus is made publicly available as an XML Web Service so that it can be updated as deemed necessary by the user community to include new words and phrases constantly being added to the Web. Secondly, the corpus makes available various sections of a Web document, specifically, the body, title, and anchor text, as separates models as text contents in these sections are found to possess significantly different statistical properties and therefore are treated as distinct languages from the language modeling point of view. The usages of the corpus are demonstrated here in two NLP tasks: phrase segmentation and word breaking. 
This paper is about interpreting human communication in meetings using audio, video and other signals. Automatic meeting recognition and understanding is extremely challenging, since communication in a meeting is spontaneous and conversational, and involves multiple speakers and multiple modalities. This leads to a number of signiﬁcant research problems in signal processing, in speech recognition, and in discourse interpretation, taking account of both individual and group behaviours. Addressing these problems requires an interdisciplinary effort. In this paper, I discuss the capture and annotation of multimodal meeting recordings—resulting in the AMI meeting corpus—and how we have built on this to develop techniques and applications for the recognition and interpretation of meetings. 
In this paper, we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars. The general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks, and is particularly suited to domain-speciﬁc lexical tuning and lexical acquisition using lowcoverage grammars. As an illustration of the functionality of our proposed technique, we develop a lexical acquisition model for English verb particle constructions which operates over unlexicalised features mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features. 
We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-ofthe-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German. 
Current statistical parsers tend to perform well only on their training domain and nearby genres. While strong performance on a few related domains is sufﬁcient for many situations, it is advantageous for parsers to be able to generalize to a wide variety of domains. When parsing document collections involving heterogeneous domains (e.g. the web), the optimal parsing model for each document is typically not obvious. We study this problem as a new task — multiple source parser adaptation. Our system trains on corpora from many different domains. It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy. Given a speciﬁc target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to speciﬁc domains. 
This paper investigates using prosodic information in the form of ToBI break indexes for parsing spontaneous speech. We revisit two previously studied approaches, one that hurt parsing performance and one that achieved minor improvements, and propose a new method that aims to better integrate prosodic breaks into parsing. Although these approaches can improve the performance of basic probabilistic context free grammar (PCFG) parsers, they all fail to produce ﬁne-grained PCFG models with latent annotations (PCFGLA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) that perform signiﬁcantly better than the baseline PCFG-LA model that does not use break indexes, partially due to mis-alignments between automatic prosodic breaks and true phrase boundaries. We propose two alternative ways to restrict the search space of the prosodically enriched parser models to the nbest parses from the baseline PCFG-LA parser to avoid egregious parses caused by incorrect breaks. Our experiments show that all of the prosodically enriched parser models can then achieve signiﬁcant improvement over the baseline PCFG-LA parser. 
For extractive meeting summarization, previous studies have shown performance degradation when using speech recognition transcripts because of the relatively high speech recognition errors on meeting recordings. In this paper we investigated using confusion networks to improve the summarization performance on the ASR condition under an unsupervised framework by considering more word candidates and their conﬁdence scores. Our experimental results showed improved summarization performance using our proposed approach, with more contribution from leveraging the conﬁdence scores. We also observed that using these rich speech recognition results can extract similar or even better summary segments than using human transcripts. 
Mobile devices are becoming the dominant mode of information access despite being cumbersome to input text using small keyboards and browsing web pages on small screens. We present Qme!, a speech-based question-answering system that allows for spoken queries and retrieves answers to the questions instead of web pages. We present bootstrap methods to distinguish dynamic questions from static questions and we show the beneﬁts of tight coupling of speech recognition and retrieval components of the system. 
In this paper we present an opinion summarization technique in spoken dialogue systems. Opinion mining has been well studied for years, but very few have considered its application in spoken dialogue systems. Review summarization, when applied to real dialogue systems, is much more complicated than pure text-based summarization. We conduct a systematic study on dialogue-system-oriented review analysis and propose a three-level framework for a recommendation dialogue system. In previous work we have explored a linguistic parsing approach to phrase extraction from reviews. In this paper we will describe an approach using statistical models such as decision trees and SVMs to select the most representative phrases from the extracted phrase set. We will also explain how to generate informative yet concise review summaries for dialogue purposes. Experimental results in the restaurant domain show that the proposed approach using decision tree algorithms achieves an outperformance of 13% compared to SVM models and an improvement of 36% over a heuristic rule baseline. Experiments also show that the decision-treebased phrase selection model can achieve rather reliable predictions on the phrase label, comparable to human judgment. The proposed statistical approach is based on domain-independent learning features and can be extended to other domains effectively. 
Extraction of entities from ad creatives is an important problem that can beneﬁt many computational advertising tasks. Supervised and semi-supervised solutions rely on labeled data which is expensive, time consuming, and difﬁcult to procure for ad creatives. A small set of manually derived constraints on feature expectations over unlabeled data can be used to partially and probabilistically label large amounts of data. Utilizing recent work in constraint-based semi-supervised learning, this paper injects light weight supervision speciﬁed as these “constraints” into a semiMarkov conditional random ﬁeld model of entity extraction in ad creatives. Relying solely on the constraints, the model is trained on a set of unlabeled ads using an online learning algorithm. We demonstrate signiﬁcant accuracy improvements on a manually labeled test set as compared to a baseline dictionary approach. We also achieve accuracy that approaches a fully supervised classiﬁer. 
 Taxonomies are an important resource for a variety of Natural Language Processing (NLP) applications. Despite this, the current stateof-the-art methods in taxonomy learning have disregarded word polysemy, in effect, developing taxonomies that conﬂate word senses. In this paper, we present an unsupervised method that builds a taxonomy of senses learned automatically from an unlabelled corpus. Our evaluation on two WordNet-derived taxonomies shows that the learned taxonomies capture a higher number of correct taxonomic relations compared to those produced by traditional distributional similarity approaches that merge senses by grouping the features of each word into a single vector. 
The question of how meaning might be acquired by young children and represented by adult speakers of a language is one of the most debated topics in cognitive science. Existing semantic representation models are primarily amodal based on information provided by the linguistic input despite ample evidence indicating that the cognitive system is also sensitive to perceptual information. In this work we exploit the vast resource of images and associated documents available on the web and develop a model of multimodal meaning representation which is based on the linguistic and visual context. Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account. 
This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on pointwise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best. 
Syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages. Instead, we propose an unsupervised ITG alignment model that directly aligns syntactic structures. Our model aligns spans in a source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 
Syntactic machine translation systems extract rules from bilingual, word-aligned, syntactically parsed text, but current systems for parsing and word alignment are at best cascaded and at worst totally independent of one another. This work presents a uniﬁed joint model for simultaneous parsing and word alignment. To ﬂexibly model syntactic divergence, we develop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English parsing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task’s independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 
We present a random-walk-based approach to learning paraphrases from bilingual parallel corpora. The corpora are represented as a graph in which a node corresponds to a phrase, and an edge exists between two nodes if their corresponding phrases are aligned in a phrase table. We sample random walks to compute the average number of steps it takes to reach a ranking of paraphrases with better ones being “closer” to a phrase of interest. This approach allows “feature” nodes that represent domain knowledge to be built into the graph, and incorporates truncation techniques to prevent the graph from growing too large for efﬁciency. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to ﬁnding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 
This paper proposes a novel approach to the problem of training classiﬁers to detect and correct grammar and usage errors in text by selectively introducing mistakes into the training data. When training a classiﬁer, we would like the distribution of examples seen in training to be as similar as possible to the one seen in testing. In error correction problems, such as correcting mistakes made by second language learners, a system is generally trained on correct data, since annotating data for training is expensive. Error generation methods avoid expensive data annotation and create training data that resemble non-native data with errors. We apply error generation methods and train classiﬁers for detecting and correcting article errors in essays written by non-native English speakers; we show that training on data that contain errors produces higher accuracy when compared to a system that is trained on clean native data. We propose several training paradigms with error generation and show that each such paradigm is superior to training a classiﬁer on native data. We also show that the most successful error generation methods are those that use knowledge about the article distribution and error patterns observed in non-native text. 
We present results from a range of experiments on article and preposition error correction for non-native speakers of English. We first compare a language model and errorspecific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction. We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the metaclassifier. The meta-classifier in turn is trained on error-annotated learner data, optimizing the error detection and correction performance on this domain. The meta-classification approach results in substantial gains over the classifieronly and language-model-only scenario. Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier. All evaluations are conducted on a large errorannotated corpus of learner English. 
We propose the ﬁrst unsupervised approach to the problem of modeling dialogue acts in an open domain. Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances. Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in a new medium. We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task. This work is inspired by a corpus of 1.3 million Twitter conversations, which will be made publicly available. This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. 
With the recent rise in popularity and size of social media, there is a growing need for systems that can extract useful information from this amount of data. We address the problem of detecting new events from a stream of Twitter posts. To make event detection feasible on web-scale corpora, we present an algorithm based on locality-sensitive hashing which is able overcome the limitations of traditional approaches, while maintaining competitive results. In particular, a comparison with a stateof-the-art system on the ﬁrst story detection task shows that we achieve over an order of magnitude speedup in processing time, while retaining comparable performance. Event detection experiments on a collection of 160 million Twitter posts show that celebrity deaths are the fastest spreading news on Twitter. 
In this paper we propose a novel general framework for unsupervised model adaptation. Our method is based on entropy which has been used previously as a regularizer in semi-supervised learning. This technique includes another term which measures the stability of posteriors w.r.t model parameters, in addition to conditional entropy. The idea is to use parameters which result in both low conditional entropy and also stable decision rules. As an application, we demonstrate how this framework can be used for adjusting language model interpolation weight for speech recognition task to adapt from Broadcast news data to MIT lecture data. We show how the new technique can obtain comparable performance to completely supervised estimation of interpolation parameters. 
We address the problem of formatting the output of an automatic speech recognition (ASR) system for readability, while preserving wordlevel timing information of the transcript. Our system enriches the ASR transcript with punctuation, capitalization and properly written dates, times and other numeric entities, and our approach can be applied to other formatting tasks. The method we describe combines hand-crafted grammars with a class-based language model trained on written text and relies on Weighted Finite State Transducers (WFSTs) for the preservation of start and end time of each word. 
Deploying an automatic speech recognition system with reasonable performance requires expensive and time-consuming in-domain transcription. Previous work demonstrated that non-professional annotation through Amazon’s Mechanical Turk can match professional quality. We use Mechanical Turk to transcribe conversational speech for as little as one thirtieth the cost of professional transcription. The higher disagreement of nonprofessional transcribers does not have a signiﬁcant eﬀect on system performance. While previous work demonstrated that redundant transcription can improve data quality, we found that resources are better spent collecting more data. Finally, we describe a quality control method without needing professional transcription. 
Out-of-vocabulary (OOV) words represent an important source of error in large vocabulary continuous speech recognition (LVCSR) systems. These words cause recognition failures, which propagate through pipeline systems impacting the performance of downstream applications. The detection of OOV regions in the output of a LVCSR system is typically addressed as a binary classiﬁcation task, where each region is independently classiﬁed using local information. In this paper, we show that jointly predicting OOV regions, and including contextual information from each region, leads to substantial improvement in OOV detection. Compared to the state-of-the-art, we reduce the missed OOV rate from 42.6% to 28.4% at 10% false alarm rate. 
A variety of information extraction techniques rely on the fact that instances of the same relation are “distributionally similar,” in that they tend to appear in similar textual contexts. We demonstrate that extraction accuracy depends heavily on the accuracy of the language model utilized to estimate distributional similarity. An unsupervised model selection technique based on this observation is shown to reduce extraction and type-checking error by 26% over previous results, in experiments with Hidden Markov Models. The results suggest that optimizing statistical language models over unlabeled data is a promising direction for improving weakly supervised and unsupervised information extraction. 
Language identiﬁcation is the task of identifying the language a given document is written in. This paper describes a detailed examination of what models perform best under different conditions, based on experiments across three separate datasets and a range of tokenisation strategies. We demonstrate that the task becomes increasingly difﬁcult as we increase the number of languages, reduce the amount of training data and reduce the length of documents. We also show that it is possible to perform language identiﬁcation without having to perform explicit character encoding detection. 
 constraints or Gibbs sampling. In this work we show  This paper describes an efﬁcient sampler for synchronous grammar induction under a nonparametric Bayesian prior. Inspired by ideas from slice sampling, our sampler is able to draw samples from the posterior distributions  that naive Gibbs sampling (speciﬁcally, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampling over sentence pairs allows much faster mixing,  of models for which the standard dynamic programing based sampler proves intractable on non-trivial corpora. We compare our sampler to a previously proposed Gibbs sampler and demonstrate strong improvements in terms of both training log-likelihood and performance on an end-to-end translation evaluation.  but done in the obvious way (following Johnson et al. (2007)) would incur a O(|f |3|e|3) time complexity. Here we draw inspiration from the work of Van Gael et al. (2008) on inference in inﬁnite hidden Markov models to develop a novel algorithm for efﬁcient sampling from a SCFG. We develop an auxiliary variable ‘slice’ sampler which can dramati-  
We conduct a pilot study for task-oriented evaluation of Multiword Expression (MWE) in Statistical Machine Translation (SMT). We propose two different integration strategies for MWE in SMT, which take advantage of different degrees of MWE semantic compositionality and yield complementary improvements in SMT quality on a large-scale translation task.1 
Semantic role labeling (SRL) not only needs lexical and syntactic information, but also needs word sense information. However, because of the lack of corpus annotated with both word senses and semantic roles, there is few research on using word sense for SRL. The release of OntoNotes provides an opportunity for us to study how to use word sense for SRL. In this paper, we present some novel word sense features for SRL and ﬁnd that they can improve the performance signiﬁcantly. 
This paper presents METEOR-NEXT, an extended version of the METEOR metric designed to have high correlation with postediting measures of machine translation quality. We describe changes made to the metric’s sentence aligner and scoring scheme as well as a method for tuning the metric’s parameters to optimize correlation with humantargeted Translation Edit Rate (HTER). We then show that METEOR-NEXT improves correlation with HTER over baseline metrics, including earlier versions of METEOR, and approaches the correlation level of a state-of-theart metric, TER-plus (TERp). 
I brieﬂy describe a system for automatically creating an implemented grammar of a natural language based on answers to a web-based questionnaire, then present a grammar of Sahaptin, a language of the Paciﬁc Northwest with complex argument-marking and agreement patterns, that was developed to test the system. The development of this grammar has proved useful in three ways: (1) verifying the correct functioning of the grammar customization system, (2) motivating the addition of a new pattern of agreement to the system, and (3) making detailed predictions that uncovered gaps in the linguistic descriptions of Sahaptin.  
We describe a synchronous parsing algorithm that is based on two successive monolingual parses of an input sentence pair. Although the worst-case complexity of this algorithm is and must be O(n6) for binary SCFGs, its average-case run-time is far better. We demonstrate that for a number of common synchronous parsing problems, the two-parse algorithm substantially outperforms alternative synchronous parsing strategies, making it efﬁcient enough to be utilized without resorting to a pruned search. 
A variety of query systems have been developed for interrogating parsed corpora, or treebanks. With the arrival of efﬁcient, widecoverage parsers, it is feasible to create very large databases of trees. However, existing approaches that use in-memory search, or relational or XML database technologies, do not scale up. We describe a method for storage, indexing, and query of treebanks that uses an information retrieval engine. Several experiments with a large treebank demonstrate excellent scaling characteristics for a wide range of query types. This work facilitates the curation of much larger treebanks, and enables them to be used effectively in a variety of scientiﬁc and engineering tasks. 
The use of well-nested linear context-free rewriting systems has been empirically motivated for modeling of the syntax of languages with discontinuous constituents or relatively free word order. We present a chart-based parsing algorithm that asymptotically improves the known running time upper bound for this class of rewriting systems. Our result is obtained through a linear space construction of a binary normal form for the grammar at hand. 
We describe a utility evaluation to determine whether cross-document information extraction (IE) techniques measurably improve user performance in news summary writing. Two groups of subjects were asked to perform the same time-restricted summary writing tasks, reading news under different conditions: with no IE results at all, with traditional singledocument IE results, and with cross-document IE results. Our results show that, in comparison to using source documents only, the quality of summary reports assembled using IE results, especially from cross-document IE, was significantly better and user satisfaction was higher. We also compare the impact of different user groups on the results. 
We consider the problem of predicting a movie’s opening weekend revenue. Previous work on this problem has used metadata about a movie—e.g., its genre, MPAA rating, and cast—with very limited work making use of text about the movie. In this paper, we use the text of ﬁlm critics’ reviews from several sources to predict opening weekend revenue. We describe a new dataset pairing movie reviews with metadata and revenue data, and show that review text can substitute for metadata, and even improve over it, for prediction. 
We present a Gaussian Mixture model for detecting different types of ﬁgurative language in context. We show that this model performs well when the parameters are estimated in an unsupervised fashion using EM. Performance can be improved further by estimating the parameters from a small annotated data set. 
We investigate methods of generating additional bilingual phrase pairs for a phrasebased decoder by translating short sequences of source text. Because our translation task is more constrained, we can use a model that employs more linguistically rich features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations.  this paper, we describe a particular method of generating additional bilingual phrase pairs for a new source text, using what we call phrase prototypes, which are are learned from bilingual training data. Our goal is to generate improved translations of relatively short phrase pairs to provide the SMT decoder with better phrasal choices. We validate the idea through experiments on Arabic-English translation. Our method produces a 1.3 BLEU score increase (3.3% relative) on a test set. 2 Approach  
This work represents an initial attempt to move beyond “single-shot” summarization to interactive summarization. We present an extension to the classic Maximal Marginal Relevance (MMR) algorithm that places a user “in the loop” to assist in candidate selection. Experiments in the complex interactive Question Answering (ciQA) task at TREC 2007 show that interactively-constructed responses are signiﬁcantly higher in quality than automatically-generated ones. This novel algorithm provides a starting point for future work on interactive summarization. 
In this paper we examine different linguistic features for sentimental polarity classiﬁcation, and perform a comparative study on this task between blog and review data. We found that results on blog are much worse than reviews and investigated two methods to improve the performance on blogs. First we explored information retrieval based topic analysis to extract relevant sentences to the given topics for polarity classiﬁcation. Second, we adopted an adaptive method where we train classiﬁers from review data and incorporate their hypothesis as features. Both methods yielded performance gain for polarity classiﬁcation on blog data. 
Two of the mechanisms for creating natural transitions between adjacent sentences in a text, resulting in local coherence, involve discourse relations and switches of focus of attention between discourse entities. These two aspects of local coherence have been traditionally discussed and studied separately. But some empirical studies have given strong evidence for the necessity of understanding how the two types of coherence-creating devices interact. Here we present a joint corpus study of discourse relations and entity coherence exhibited in news texts from the Wall Street Journal and test several hypotheses expressed in earlier work about their interaction. 
Sentence fusion enables summarization and question-answering systems to produce output by combining fully formed phrases from different sentences. Yet there is little data that can be used to develop and evaluate fusion techniques. In this paper, we present a methodology for collecting fusions of similar sentence pairs using Amazon’s Mechanical Turk, selecting the input pairs in a semiautomated fashion. We evaluate the results using a novel technique for automatically selecting a representative sentence from multiple responses. Our approach allows for rapid construction of a high accuracy fusion corpus. 
This paper investigates cross-lingual textual entailment as a semantic relation between two text portions in different languages, and proposes a prospective research direction. We argue that cross-lingual textual entailment (CLTE) can be a core technology for several cross-lingual NLP applications and tasks. Through preliminary experiments, we aim at proving the feasibility of the task, and providing a reliable baseline. We also introduce new applications for CLTE that will be explored in future work. 
Sentence retrieval is a very important part of question answering systems. Term clustering, in turn, is an effective approach for improving sentence retrieval performance: the more similar the terms in each cluster, the better the performance of the retrieval system. A key step in obtaining appropriate word clusters is accurate estimation of pairwise word similarities, based on their tendency to co-occur in similar contexts. In this paper, we compare four different methods for estimating word co-occurrence frequencies from two different corpora. The results show that different, commonly-used contexts for deﬁning word co-occurrence differ signiﬁcantly in retrieval performance. Using an appropriate co-occurrence criterion and corpus is shown to improve the mean average precision of sentence retrieval form 36.8% to 42.1%. 
This paper presents an empirical comparison of similarity measures for pairs of concepts based on Information Content. It shows that using modest amounts of untagged text to derive Information Content results in higher correlation with human similarity judgments than using the largest available corpus of manually annotated sense–tagged text.  judgements than do those based on the largest corpus of sense-tagged text currently available.1 The key to this success is not in the speciﬁc type of corpora used, but rather in increasing the number of concepts in WordNet that have counts associated with them. These results show that Information Content measures of semantic similarity can be signiﬁcantly improved without requiring the creation of sense– tagged corpora (which is very expensive).  
Generating expository dialogue from monologue is a task that poses an interesting and rewarding challenge for Natural Language Processing. This short paper has three aims: ﬁrstly, to motivate the importance of this task, both in terms of the beneﬁts of expository dialogue as a way to present information and in terms of potential applications; secondly, to introduce a parallel corpus of monologues and dialogues which enables a data-driven approach to this challenge; and, ﬁnally, to describe work-in-progress on semi-automatic construction of Monologueto-Dialogue (M2D) generation rules. 
There are many misconceptions about dependency representations and phrase structure representations for syntax. They are partly due to terminological confusion, partly due to a lack of meta-scientiﬁc clarity about the roles of representations and linguistic theories. This opinion piece argues for a simple but clear view of syntactic representation. 
 2 Background  The class of Linear Inversion Transduction Grammars (LITGs) is introduced, and used to induce a word alignment over a parallel corpus. We show that alignment via Stochastic Bracketing LITGs is considerably faster than Stochastic Bracketing ITGs, while still yielding alignments superior to the widelyused heuristic of intersecting bidirectional IBM alignments. Performance is measured as the translation quality of a phrase-based machine translation system built upon the word alignments, and an improvement of 2.85 BLEU points over baseline is noted for French– English. 
Named entity recognition systems sometimes have difﬁculty when applied to data from domains that do not closely match the training data. We ﬁrst use a simple rule-based technique for domain adaptation. Data for robust validation of the technique is then generated, and we use crowdsourcing techniques to show that this strategy produces reliable results even on data not seen by the rule designers. We show that it is possible to extract large improvements on the target data rapidly at low cost using these techniques. 
Hierarchical phrase-based translation (Hiero, (Chiang, 2005)) provides an attractive framework within which both short- and longdistance reorderings can be addressed consistently and ef ciently. However, Hiero is generally implemented with a constraint preventing the creation of rules with adjacent nonterminals, because such rules introduce computational and modeling challenges. We introduce methods to address these challenges, and demonstrate that rules with adjacent nonterminals can improve Hiero's generalization power and lead to signi cant performance gains in Chinese-English translation.  the challenges that arise in relaxing this constraint. In Section 4 we introduce new methods to address those challenges, and Section 5 validates the approach empirically. Improving Hiero via variations on rule pruning and ltering is well explored, e.g., (Chiang, 2005; Chiang et al., 2008; Zollmann and Venugopal, 2006), to name just a few. These proposals differ from each other mainly in the speci c linguistic knowledge being used, and on which side the constraints are applied. In contrast, we complement previous work by showing that adding rules to Hiero can provide bene ts if done judiciously. 2 Judicious Use of Adjacent Nonterminals  
 2 Application to a Range of Data Sets  Several methods for automatically generating labeled examples that can be used as training data for WSD systems have been proposed, including a semisupervised approach based on relevance feedback (Stevenson et al., 2008a). This approach was shown to generate examples that improved the performance of a WSD system for a set of ambiguous terms from the biomedical domain. However, we ﬁnd that this approach does not perform as well on other data sets. The levels of ambiguity in these data sets are analysed and we suggest this is the reason for this negative result. 
We explore the relation between word sense subjectivity and cross-lingual lexical substitution, following the intuition that good substitutions will transfer a word’s (contextual) sentiment from the source language into the target language. Experiments on English-Chinese lexical substitution show that taking a word’s subjectivity into account can indeed improve performance. We also show that just using word sense subjectivity can perform as well as integrating fully-ﬂedged ﬁne-grained word sense disambiguation for words which have both subjective and objective senses. 
Understanding query ambiguity in web search remains an important open problem. In this paper we reexamine query ambiguity by analyzing the result clickthrough data. Previously proposed clickthrough-based metrics of query ambiguity tend to conﬂate informational and ambiguous queries. To distinguish between these query classes, we introduce novel metrics based on the entropy of the click distributions of individual searchers. Our experiments over a clickthrough log of commercial search engine demonstrate the beneﬁts of our approach for distinguishing informational from truly ambiguous queries. 
We report on work in progress on extracting lexical simpliﬁcations (e.g., “collaborate” → “work together”), focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simpliﬁcation probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simpliﬁcation operations. We ﬁnd our methods to outperform a reasonable baseline and yield many high-quality lexical simpliﬁcations not included in an independently-created manually prepared list. 
In the field of machine translation, automatic metrics have proven quite valuable in system development for tracking progress and measuring the impact of incremental changes. However, human judgment still plays a large role in the context of evaluating MT systems. For example, the GALE project uses humantargeted translation edit rate (HTER), wherein the MT output is scored against a post-edited version of itself (as opposed to being scored against an existing human reference). This poses a problem for MT researchers, since HTER is not an easy metric to calculate, and would require hiring and training human annotators to perform the editing task. In this work, we explore soliciting those edits from untrained human annotators, via the online service Amazon Mechanical Turk. We show that the collected data allows us to predict HTER-ranking of documents at a significantly higher level than the ranking obtained using automatic metrics. 
This work incorporates Selectional Preferences (SP) into a Semantic Role (SR) Classiﬁcation system. We learn separate selectional preferences for noun phrases and prepositional phrases and we integrate them in a state-of-the-art SR classiﬁcation system both in the form of features and individual class predictors. We show that the inclusion of the reﬁned SPs yields statistically signiﬁcant improvements on both in domain and out of domain data (14.07% and 11.67% error reduction, respectively). The key factor for success is the combination of several SP methods with the original classiﬁcation model using metaclassiﬁcation. 
Noun phrases (NP) in a product review are always considered as the product attribute candidates in previous work. However, this method limits the recall of the product attribute extraction. We therefore propose a novel approach by generalizing syntactic structures of the product attributes with two strategies: intuitive heuristics and syntactic structure similarity. Experiments show that the proposed approach is effective. 
We evaluate the Berkeley parser on text from an online discussion forum. We evaluate the parser output with and without gold tokens and spellings (using Sparseval and Parseval), and we compile a list of problematic phenomena for this domain. The Parseval f-score for a small development set is 77.56. This increases to 80.27 when we apply a set of simple transformations to the input sentences and to the Wall Street Journal (WSJ) training sections. 
Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner. Our semantic representation ﬁrst hypothesizes an underlying set of latent entity types, which generate speciﬁc entities that in turn render individual mentions. By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task. 
Typical statistical machine translation systems are trained with static parallel corpora. Here we account for scenarios with a continuous incoming stream of parallel training data. Such scenarios include daily governmental proceedings, sustained output from translation agencies, or crowd-sourced translations. We show incorporating recent sentence pairs from the stream improves performance compared with a static baseline. Since frequent batch retraining is computationally demanding we introduce a fast incremental alternative using an online version of the EM algorithm. To bound our memory requirements we use a novel data-structure and associated training regime. When compared to frequent batch retraining, our online time and space-bounded model achieves the same performance with signiﬁcantly less computational overhead. 
The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages. We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity. We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model. Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented. 
This paper investigates the impact of misspelled words in statistical machine translation and proposes an extension of the translation engine for handling misspellings. The enhanced system decodes a word-based confusion network representing spelling variations of the input text. We present extensive experimental results on two translation tasks of increasing complexity which show how misspellings of different types do affect performance of a statistical machine translation decoder and to what extent our enhanced system is able to recover from such errors. 
Most state of the art approaches for machine transliteration are data driven and require signiﬁcant parallel names corpora between languages. As a result, developing transliteration functionality among n languages could be a resource intensive task requiring parallel names corpora in the order of nC2. In this paper, we explore ways of reducing this high resource requirement by leveraging the available parallel data between subsets of the n languages, transitively. We propose, and show empirically, that reasonable quality transliteration engines may be developed between two languages, X and Y , even when no direct parallel names data exists between them, but only transitively through language Z. Such systems alleviate the need for O(nC2) corpora, signiﬁcantly. In addition we show that the performance of such transitive transliteration systems is in par with direct transliteration systems, in practical applications, such as CLIR systems. 
This paper proposes a general learning framework for a class of problems that require learning over latent intermediate representations. Many natural language processing (NLP) decision problems are deﬁned over an expressive intermediate representation that is not explicit in the input, leaving the algorithm with both the task of recovering a good intermediate representation and learning to classify correctly. Most current systems separate the learning problem into two stages by solving the ﬁrst step of recovering the intermediate representation heuristically and using it to learn the ﬁnal classiﬁer. This paper develops a novel joint learning algorithm for both tasks, that uses the ﬁnal prediction to guide the selection of the best intermediate representation. We evaluate our algorithm on three different NLP tasks – transliteration, paraphrase identiﬁcation and textual entailment – and show that our joint method signiﬁcantly improves performance. 
A number of recent articles in computational linguistics venues called for a closer examination of the type of noise present in annotated datasets used for benchmarking (Reidsma and Carletta, 2008; Beigman Klebanov and Beigman, 2009). In particular, Beigman Klebanov and Beigman articulated a type of noise they call annotation noise and showed that in worst case such noise can severely degrade the generalization ability of a linear classiﬁer (Beigman and Beigman Klebanov, 2009). In this paper, we provide quantitative empirical evidence for the existence of this type of noise in a recently benchmarked dataset. The proposed methodology can be used to zero in on unreliable instances, facilitating generation of cleaner gold standards for benchmarking. 
We describe a Bayesian inference algorithm that can be used to train any cascade of weighted ﬁnite-state transducers on end-toend data. We also investigate the problem of automatically selecting from among multiple training runs. Our experiments on four diﬀerent tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 
Perceptron training is widely applied in the natural language processing community for learning complex structured models. Like all structured prediction learning frameworks, the structured perceptron can be costly to train as training complexity is proportional to inference, which is frequently non-linear in example sequence length. In this paper we investigate distributed training strategies for the structured perceptron as a means to reduce training times when computing clusters are available. We look at two strategies and provide convergence bounds for a particular mode of distributed structured perceptron training based on iterative parameter mixing (or averaging). We present experiments on two structured prediction problems – namedentity recognition and dependency parsing – to highlight the efﬁciency of this method. 
Many implementations of Latent Dirichlet Allocation (LDA), including those described in Blei et al. (2003), rely at some point on the removal of stopwords, words which are assumed to contribute little to the meaning of the text. This step is considered necessary because otherwise high-frequency words tend to end up scattered across many of the latent topics without much rhyme or reason. We show, however, that the ‘problem’ of high-frequency words can be dealt with more elegantly, and in a way that to our knowledge has not been considered in LDA, through the use of appropriate weighting schemes comparable to those sometimes used in Latent Semantic Indexing (LSI). Our proposed weighting methods not only make theoretical sense, but can also be shown to improve precision significantly on a non-trivial cross-language retrieval task. 
The goal of this work is to integrate query similarity metrics as features into a dense model that can be trained on large amounts of query log data, in order to rank query rewrites. We propose features that incorporate various notions of syntactic and semantic similarity in a generalized edit distance framework. We use the implicit feedback of user clicks on search results as weak labels in training linear ranking models on large data sets. We optimize different ranking objectives in a stochastic gradient descent framework. Our experiments show that a pairwise SVM ranker trained on multipartite rank levels outperforms other pairwise and listwise ranking methods under a variety of evaluation metrics. 
This paper address the problem of entity linking. Speciﬁcally, given an entity mentioned in unstructured texts, the task is to link this entity with an entry stored in the existing knowledge base. This is an important task for information extraction. It can serve as a convenient gateway to encyclopedic information, and can greatly improve the web users’ experience. Previous learning based solutions mainly focus on classiﬁcation framework. However, it’s more suitable to consider it as a ranking problem. In this paper, we propose a learning to rank algorithm for entity linking. It effectively utilizes the relationship information among the candidates when ranking. The experiment results on the TAC 20091 dataset demonstrate the effectiveness of our proposed framework. The proposed method achieves 18.5% improvement in terms of accuracy over the classiﬁcation models for those entities which have corresponding entries in the Knowledge Base. The overall performance of the system is also better than that of the state-of-the-art methods.  Base. Wikipedia is an online encyclopedia, and now it becomes one of the largest repositories of encyclopedic knowledge. In this paper, we use Wikipedia as our Knowledge Base. Entity linking can be used to automatically augment text with links, which serve as a convenient gateway to encyclopedic information, and can greatly improve user experience. For example, Figure 1 shows news from BBC.com. When a user is interested in ”Thierry Henry”, he can acquire more detailed information by linking ”Thierry Henry” to the corresponding entry in the Knowledge Base.  
Although Wikipedia has emerged as a powerful collaborative Encyclopedia on the Web, it is only partially multilingual as most of the content is in English and a small number of other languages. In real-life scenarios, nonEnglish users in general and ESL/EFL 1 users in particular, have a need to search for relevant English Wikipedia articles as no relevant articles are available in their language. The multilingual experience of such users can be signiﬁcantly improved if they could express their information need in their native language while searching for English Wikipedia articles. In this paper, we propose a novel crosslanguage name search algorithm and employ it for searching English Wikipedia articles in a diverse set of languages including Hebrew, Hindi, Russian, Kannada, Bangla and Tamil. Our empirical study shows that the multilingual experience of users is signiﬁcantly improved by our approach. 
Most work on language acquisition treats word segmentation—the identiﬁcation of linguistic segments from continuous speech— and word learning—the mapping of those segments to meanings—as separate problems. These two abilities develop in parallel, however, raising the question of whether they might interact. To explore the question, we present a new Bayesian segmentation model that incorporates aspects of word learning and compare it to a model that ignores word meanings. The model that learns word meanings proposes more adult-like segmentations for the meaning-bearing words. This result suggests that the non-linguistic context may supply important information for learning word segmentations as well as word meanings. 
For millions of people in less resourced regions of the world, text messages (SMS) provide the only regular contact with their doctor. Classifying messages by medical labels supports rapid responses to emergencies, the early identiﬁcation of epidemics and everyday administration, but challenges include textbrevity, rich morphology, phonological variation, and limited training data. We present a novel system that addresses these, working with a clinic in rural Malawi and texts in the Chichewa language. We show that modeling morphological and phonological variation leads to a substantial average gain of F=0.206 and an error reduction of up to 63.8% for speciﬁc labels, relative to a baseline system optimized over word-sequences. By comparison, there is no signiﬁcant gain when applying the same system to the English translations of the same texts/labels, emphasizing the need for subword modeling in many languages. Language independent morphological models perform as accurately as language speciﬁc models, indicating a broad deployment potential. 
We are interested in diacritizing Semitic languages, especially Syriac, using only diacritized texts. Previous methods have required the use of tools such as part-of-speech taggers, segmenters, morphological analyzers, and linguistic rules to produce state-of-the-art results. We present a low-resource, data-driven, and language-independent approach that uses a hybrid word- and consonant-level conditional Markov model. Our approach rivals the best previously published results in Arabic (15% WER with case endings), without the use of a morphological analyzer. In Syriac, we reduce the WER over a strong baseline by 30% to achieve a WER of 10.5%. We also report results for Hebrew and English. 
Word Segmentation is the foremost obligatory task in almost all the NLP applications where the initial phase requires tokenization of input into words. Urdu is amongst the Asian languages that face word segmentation challenge. However, unlike other Asian languages, word segmentation in Urdu not only has space omission errors but also space insertion errors. This paper discusses how orthographic and linguistic features in Urdu trigger these two problems. It also discusses the work that has been done to tokenize input text. We employ a hybrid solution that performs an n-gram ranking on top of rule based maximum matching heuristic. Our best technique gives an error detection of 85.8% and overall accuracy of 95.8%. Further issues and possible future directions are also discussed. 
We carried out a study on monolingual translators with no knowledge of the source language, but aided by post-editing and the display of translation options. On Arabic-English and Chinese-English, using standard test data and current statistical machine translation systems, 10 monolingual translators were able to translate 35% of Arabic and 28% of Chinese sentences correctly on average, with some of the participants coming close to professional bilingual performance on some of the documents. While machine translation systems have advanced greatly over the last decade, nobody seriously expects human-level performance any time soon, except for very constraint settings. But are todays systems good enough to enable monolingual speakers of the target language without knowledge of the source language to generate correct translations? And what type of assistance from machine translation is most helpful for such translators? We carried out a study that involved monolingual translators who had no knowledge of Chinese and Arabic to translate documents from the NIST 20081 test sets, being assisted by statistical machine translation systems trained on data created under the GALE2 research program. Our study shows that monolingual translators were able to translate 35% of Arabic and 28% of Chinese sentences, under a strict standard of correctness that scored professional bilingual translations as 61% and 66% correct for Arabic and Chinese, respectively. We found also large variability among the participants and between the documents in the 1http://www.itl.nist.gov/iad/mig/tests/mt/ 2http://www.darpa.mil/ipto/programs/gale/gale.asp  study, indicating the importance of general language skills and domain knowledge. The results suggest that a skilled monolingual translator can compete with a bilingual translator, when using todays machine translation systems. 
State-of-the-art Machine Translation (MT) systems are still far from being perfect. An alternative is the so-called Interactive Machine Translation (IMT) framework. In this framework, the knowledge of a human translator is combined with a MT system. The vast majority of the existing work on IMT makes use of the well-known batch learning paradigm. In the batch learning paradigm, the training of the IMT system and the interactive translation process are carried out in separate stages. This paradigm is not able to take advantage of the new knowledge produced by the user of the IMT system. In this paper, we present an application of the online learning paradigm to the IMT framework. In the online learning paradigm, the training and prediction stages are no longer separated. This feature is particularly useful in IMT since it allows the user feedback to be taken into account. The online learning techniques proposed here incrementally update the statistical models involved in the translation process. Empirical results show the great potential of online learning in the IMT framework. 
Translation systems are generally trained to optimize BLEU, but many alternative metrics are available. We explore how optimizing toward various automatic evaluation metrics (BLEU, METEOR, NIST, TER) affects the resulting model. We train a state-of-the-art MT system using MERT on many parameterizations of each metric and evaluate the resulting models on the other metrics and also using human judges. In accordance with popular wisdom, we ﬁnd that it’s important to train on the same metric used in testing. However, we also ﬁnd that training to a newer metric is only useful to the extent that the MT model’s structure and features allow it to take advantage of the metric. Contrasting with TER’s good correlation with human judgments, we show that people tend to prefer BLEU and NIST trained models to those trained on edit distance based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 
Adaptor grammars extend probabilistic context-free grammars to deﬁne prior distributions over trees with “rich get richer” dynamics. Inference for adaptor grammars seeks to ﬁnd parse trees for raw text. This paper describes a variational inference algorithm for adaptor grammars, providing an alternative to Markov chain Monte Carlo methods. To derive this method, we develop a stick-breaking representation of adaptor grammars, a representation that enables us to deﬁne adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a signiﬁcant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 
Most existing algorithms for learning latentvariable models—such as EM and existing Gibbs samplers—are token-based, meaning that they update the variables associated with one sentence at a time. The incremental nature of these methods makes them susceptible to local optima/slow mixing. In this paper, we introduce a type-based sampler, which updates a block of variables, identiﬁed by a type, which spans multiple sentences. We show improvements on part-of-speech induction, word segmentation, and learning tree-substitution grammars. 
We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models. 
This paper describes a method for checking the acceptability of paraphrases in context. We use the Google n-gram data and a CCG parser to certify the paraphrasing grammaticality and ﬂuency. We collect a corpus of human judgements to evaluate our system. The ultimate goal of our work is to integrate text paraphrasing into a Linguistic Steganography system, by using paraphrases to hide information in a cover text. We propose automatically generated paraphrases as a new and useful source of transformations for Linguistic Steganography, and show that our method for checking paraphrases is effective at maintaining a high level of imperceptibility, which is crucial for effective steganography. 
Producing a ﬂuent ordering for a set of prenominal modiﬁers in a noun phrase (NP) is a problematic task for natural language generation and machine translation systems. We present a novel approach to this issue, adapting multiple sequence alignment techniques used in computational biology to the alignment of modiﬁers. We describe two training techniques to create such alignments based on raw text, and demonstrate ordering accuracies superior to earlier reported approaches. 
We address the challenge of automatically generating questions from reading materials for educational practice and assessment. Our approach is to overgenerate questions, then rank them. We use manually written rules to perform a sequence of general purpose syntactic transformations (e.g., subject-auxiliary inversion) to turn declarative sentences into questions. These questions are then ranked by a logistic regression model trained on a small, tailored dataset consisting of labeled output from our system. Experimental results show that ranking nearly doubles the percentage of questions rated as acceptable by annotators, from 27% of all questions to 52% of the top ranked 20% of questions. 
Open-class semantic lexicon induction is of great interest for current knowledge harvesting algorithms. We propose a general framework that uses patterns in bootstrapping fashion to learn open-class semantic lexicons for different kinds of relations. These patterns require seeds. To estimate the goodness (the potential yield) of new seeds, we introduce a regression model that considers the connectivity behavior of the seed during bootstrapping. The generalized regression model is evaluated on six different kinds of relations with over 10000 different seeds for English and Spanish patterns. Our approach reaches robust performance of 90% correlation coefﬁcient with 15% error rate for any of the patterns when predicting the goodness of seeds. 
Like most natural language disambiguation tasks, word sense disambiguation (WSD) requires world knowledge for accurate predictions. Several proxies for this knowledge have been investigated, including labeled corpora, user-contributed knowledge, and machine readable dictionaries, but each of these proxies requires signiﬁcant manual effort to create, and they do not cover all of the ambiguous terms in a language. We investigate the task of automatically extracting world knowledge, in the form of glosses, from an unlabeled corpus. We demonstrate how to use these glosses to automatically label a training corpus to build a statistical WSD system that uses no manually-labeled data, with experimental results approaching that of a supervised SVMbased classiﬁer. 
There is signiﬁcant evidence in the literature that integrating knowledge about multiword expressions can improve shallow parsing accuracy. We present an experimental study to quantify this improvement, focusing on compound nominals, proper names and adjectivenoun constructions. The evaluation set of multiword expressions is derived from WordNet and the textual data are downloaded from the web. We use a classiﬁcation method to aid human annotation of output parses. This method allows us to conduct experiments on a large dataset of unannotated data. Experiments show that knowledge about multiword expressions leads to an increase of between 7.5% and 9.5% in accuracy of shallow parsing in sentences containing these multiword expressions. 
In a supertagging task, sequence labeling models are commonly used. But their limited ability to model long-distance information presents a bottleneck to make further improvements. In this paper, we modeled this long-distance information in dependency formalism and integrated it into the process of HPSG supertagging. The experiments showed that the dependency information is very informative for supertag disambiguation. We also evaluated the improved supertagger in the HPSG parser. 
Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking. In this paper we implemented such a study for English dependency parsing and ﬁnd several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classiﬁcation), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without signiﬁcant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models. This study proves that fast and accurate ensemble parsers can be built with minimal effort. 
 2 SITG Parsing  Stochastic Inversion Transduction Grammars constitute a powerful formalism in Machine Translation for which an efﬁcient Dynamic Programming parsing algorithm exists. In this work, we review this parsing algorithm and propose important modiﬁcations that enlarge the search space. These modiﬁcations allow the parsing algorithm to search for more and better solutions. 
The paper describes a data driven dependency parsing approach which uses clausal information of a sentence to improve the parser performance. The clausal information is added automatically during the parsing process. We demonstrate the experiments on Hindi, a language with relatively rich case marking system and free-word-order. All the experiments are done using a modified version of MSTParser. We did all the experiments on the ICON 2009 parsing contest data. We achieved an improvement of 0.87% and 0.77% in unlabeled attachment and labeled attachment accuracies respectively over the baseline parsing accuracies. 
Recent work has proposed the use of an extracted tree grammar as the basis for treebank analysis and search queries, in which queries are stated over the elementary trees, which are small chunks of syntactic structure. However, this work was lacking in two crucial ways. First, it did not allow for including lexical properties of tokens in the search. Second, it did not allow for using the derivation tree in the search, describing how the elementary trees are connected together. In this work we describe an implementation that overcomes these problems. 
The Brown and the Berkeley parsers are two state-of-the-art generative parsers. Since both parsers produce n-best lists, it is possible to apply reranking techniques to the output of both of these parsers, and to their union. We note that the standard reranker feature set distributed with the Brown parser does not do well with the Berkeley parser, and propose an extended set that does better. An ablation experiment shows that different parsers beneﬁt from different reranker features. 
In a corpus of expert tutoring dialogue, conversation that is considered to be “off topic” (non-pedagogical) according to a previous coding scheme is explored for its value in tutoring dynamics. Using the Linguistic Inquiry and Word Count (LIWC) tool, phases of tutoring categorized as “off topic” were compared with interactive problem solving phases to explore how the two differ on the emotional, psychological, and topical dimensions analyzed by LIWC. The results suggest that conversation classified as “off topic” serves as motivation and broad pedagogy in tutoring. These findings can be used to orient future research on “off topic” conversation, and help to make sense of both previous coding schemes and noisy data sets. 
In this paper we investigate how to identify initiation-response pairs in asynchronous, multi-threaded, multi-party conversations. We formulate the task of identifying initiation-response pairs as a pairwise ranking problem. A novel variant of Latent Semantic Analysis (LSA) is proposed to overcome a limitation of standard LSA models, namely that uncommon words, which are critical for signaling initiation-response links, tend to be deemphasized as it is the more frequent terms that end up closer to the latent factors selected through singular value decomposition. We present experimental results demonstrating significantly better performance of the novel variant of LSA over standard LSA. 
Conversational Agents have been shown to be effective tutors in a wide range of educational domains. However, these agents are often ignored and abused in collaborative learning scenarios involving multiple students. In our work presented here, we design and evaluate interaction strategies motivated from prior research in small group communication. We will discuss how such strategies can be implemented in agents. As a first step towards evaluating agents that can interact socially, we report results showing that human tutors employing these strategies are able to cover more concepts with the students besides being rated as better integrated, likeable and friendlier. 
We show how the Barzilay and Lapata entitybased coherence algorithm (2008) can be applied to a new, noisy data domain – student essays. We demonstrate that by combining Barzilay and Lapata’s entity-based features with novel features related to grammar errors and word usage, one can greatly improve the performance of automated coherence prediction for student essays for different populations. 
In this paper, we focus on a recent Web trend called microblogging, and in particular a site called Twitter. The content of such a site is an extraordinarily large number of small textual messages, posted by millions of users, at random or in response to perceived events or situations. We have developed an algorithm that takes a trending phrase or any phrase specified by a user, collects a large number of posts containing the phrase, and provides an automatically created summary of the posts related to the term. We present examples of summaries we produce along with initial evaluation. 
This paper introduces a system designed for automatically generating personalized annotation tags to label Twitter user’s interests and concerns. We applied TFIDF ranking and TextRank to extract keywords from Twitter messages to tag the user. The user tagging precision we obtained is comparable to the precision of keyword extraction from web pages for content-targeted advertising. 
The task of identifying the language of text or utterances has a number of applications in natural language processing. Language identiﬁcation has traditionally been approached with character-level language models. However, the language model approach crucially depends on the length of the text in question. In this paper, we consider the problem of language identiﬁcation of names. We show that an approach based on SVMs with n-gram counts as features performs much better than language models. We also experiment with applying the method to pre-process transliteration data for the training of separate models. 
Phonetic string transduction problems, such as letter-to-phoneme conversion and name transliteration, have recently received much attention in the NLP community. In the past few years, two methods have come to dominate as solutions to supervised string transduction: generative joint n-gram models, and discriminative sequence models. Both approaches beneﬁt from their ability to consider large, ﬂexible spans of source context when making transduction decisions. However, they encode this context in different ways, providing their respective models with different information. To combine the strengths of these two systems, we include joint n-gram features inside a state-of-the-art discriminative sequence model. We evaluate our approach on several letter-to-phoneme and transliteration data sets. Our results indicate an improvement in overall performance with respect to both the joint n-gram approach and traditional feature sets for discriminative models. 
In this work, we try a hybrid methodology for language modeling where both morphological decomposition and factored language modeling (FLM) are exploited to deal with the complex morphology of Arabic language. At the end, we are able to obtain from 3.5% to 7.0% relative reduction in word error rate (WER) with respect to a traditional full-words system, and from 1.0% to 2.0% relative WER reduction with respect to a non-factored decomposed system. 
In this paper, we compare two novel methods for part of speech tagging of Arabic without the use of gold standard word segmentation but with the full POS tagset of the Penn Arabic Treebank. The ﬁrst approach uses complex tags without any word segmentation, the second approach is segmention-based, using a machine learning segmenter. Surprisingly, word-based POS tagging yields the best results, with a word accuracy of 94.74%. 
We investigate in this paper the adequate unit of analysis for Arabic Mention Detection. We experiment different segmentation schemes with various feature-sets. Results show that when limited resources are available, models built on morphologically segmented data outperform other models by up to 4F points. On the other hand, when more resources extracted from morphologically segmented data become available, models built with Arabic TreeBank style segmentation yield to better results. We also show additional improvement by combining different segmentation schemes. 
We address a key problem in grapheme-tophoneme conversion: the ambiguity in mapping grapheme units to phonemes. Rather than using single letters and phonemes as units, we propose learning chunks, or subwords, to reduce ambiguity. This can be interpreted as learning a lexicon of subwords that has minimum description length. We implement an algorithm to build such a lexicon, as well as a simple decoder that uses these subwords. 
This paper addresses the problem of learning phrase patterns for unsupervised speaker role classiﬁcation. Phrase patterns are automatically extracted from large corpora, and redundant patterns are removed via a graph pruning algorithm. In experiments on English and Mandarin talk shows, the use of phrase patterns results in an increase of role classiﬁcation accuracy over n-gram lexical features, and more compact phrase pattern lists are obtained due to the redundancy removal. 
 2 Related Work  We present Quantized Contour Modeling (QCM), a Bayesian approach to the classiﬁcation of acoustic contours. We evaluate the performance of this technique in the classiﬁcation of prosodic events. We ﬁnd that, on BURNC, this technique can successfully classify pitch accents with 63.99% accuracy (.4481 CER), and phrase ending tones with 72.91% accuracy. 
We suggest improvements to a previously proposed framework for integrating Conditional Random Fields and Hidden Markov Models, dubbed a Crandem system (2009). The previous authors’ work suggested that local label posteriors derived from the CRF were too low-entropy for use in word-level automatic speech recognition. As an alternative to the log posterior representation used in their system, we explore frame-level representations derived from the CRF feature functions. We also describe a weight normalization transformation that leads to increased entropy of the CRF posteriors. We report signiﬁcant gains over the previous Crandem system on the Wall Street Journal word recognition task. 
Most learning algorithms for undirected graphical models require complete inference over at least one instance before parameter updates can be made. SampleRank is a rankbased learning framework that alleviates this problem by updating the parameters during inference. Most semi-supervised learning algorithms also perform full inference on at least one instance before each parameter update. We extend SampleRank to semi-supervised learning in order to circumvent this computational bottleneck. Different approaches to incorporate unlabeled data and prior knowledge into this framework are explored. When evaluated on a standard information extraction dataset, our method signiﬁcantly outperforms the supervised method, and matches results of a competing state-of-the-art semi-supervised learning approach. 
We describe a method of incorporating taskspeciﬁc cost functions into standard conditional log-likelihood (CLL) training of linear structured prediction models. Recently introduced in the speech recognition community, we describe the method generally for structured models, highlight connections to CLL and max-margin learning for structured prediction (Taskar et al., 2003), and show that the method optimizes a bound on risk. The approach is simple, efﬁcient, and easy to implement, requiring very little change to an existing CLL implementation. We present experimental results comparing with several commonly-used methods for training structured predictors for named-entity recognition. 
We present a method for disambiguating syntactic subjects from syntactic objects (a frequent ambiguity) in German sentences taken from an English-German bitext. We exploit the fact that subject and object are usually easily determined in English. We show that a simple method disambiguates some subjectobject ambiguities in German, while making few errors. We view this procedure as the ﬁrst step in automatically acquiring (mostly) correct labeled data. We also evaluate using it to improve a state of the art statistical parser.  an English-German bitext and exploit the fact that subject and object roles are rarely ambiguous in English. Using a new gold standard we created we show that our method disambiguates a signiﬁcant proportion of subject-object ambiguities in German with high precision. We view this procedure as the ﬁrst step in automatically acquiring (mostly) correct labeled data for training a statistical disambiguator that can be used on German text (even when no translation is available). In addition to measuring algorithm performance directly, we present experiments on improving the disambiguation of BitPar, a state of the art statistical parser.  
We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure ﬁrst in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-ﬁrst, O(nlogn) parser, which is signiﬁcantly more accurate than best-ﬁrst transition based parsers, and nears the performance of globally optimized parsing models. 
We present three approaches for unsupervised grammar induction that are sensitive to data complexity and apply them to Klein and Manning’s Dependency Model with Valence. The ﬁrst, Baby Steps, bootstraps itself via iterated learning of increasingly longer sentences and requires no initialization. This method substantially exceeds Klein and Manning’s published scores and achieves 39.4% accuracy on Section 23 (all sentences) of the Wall Street Journal corpus. The second, Less is More, uses a low-complexity subset of the available data: sentences up to length 15. Focusing on fewer but simpler examples trades off quantity against ambiguity; it attains 44.1% accuracy, using the standard linguisticallyinformed prior and batch training, beating state-of-the-art. Leapfrog, our third heuristic, combines Less is More with Baby Steps by mixing their models of shorter sentences, then rapidly ramping up exposure to the full training set, driving up accuracy to 45.0%. These trends generalize to the Brown corpus; awareness of data complexity may improve other parsing models and unsupervised algorithms. 
Recently, relaxation approaches have been successfully used for MAP inference on NLP problems. In this work we show how to extend the relaxation approach to marginal inference used in conditional likelihood training, posterior decoding, conﬁdence estimation, and other tasks. We evaluate our approach for the case of second-order dependency parsing and observe a tenfold increase in parsing speed, with no loss in accuracy, by performing inference over a small subset of the full factor graph. We also contribute a bound on the error of the marginal probabilities by a sub-graph with respect to the full graph. Finally, while only evaluated with BP in this paper, our approach is general enough to be applied with any marginal inference method in the inner loop. 
Factorization is the operation of transforming a production in a Linear Context-Free Rewriting System (LCFRS) into two simpler productions by factoring out a subset of the nonterminals on the production’s righthand side. Factorization lowers the rank of a production but may increase its fan-out. We show how to apply factorization in order to minimize the parsing complexity of the resulting grammar, and study the relationship between rank, fanout, and parsing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out. Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to ﬁnd the most efﬁcient parsing strategy for the syntactic phenomena found in non-projective trees. 
We examine the viability of building large polarity lexicons semi-automatically from the web. We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; BlairGoldensohn et al., 2008; Rao and Ravichandran, 2009). We then apply this technique to build an English lexicon that is signiﬁcantly larger than those previously studied. Crucially, this web-derived lexicon does not require WordNet, part-of-speech taggers, or other language-dependent resources typical of sentiment analysis systems. As a result, the lexicon is not limited to speciﬁc word classes – e.g., adjectives that occur in WordNet – and in fact contains slang, misspellings, multiword expressions, etc. We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet. 
In this paper, we present a dependency treebased method for sentiment classiﬁcation of Japanese and English subjective sentences using conditional random ﬁelds with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classiﬁcation, which is difﬁcult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classiﬁcation for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features. 
Opinion holder extraction is one of the important subtasks in sentiment analysis. The effective detection of an opinion holder depends on the consideration of various cues on various levels of representation, though they are hard to formulate explicitly as features. In this work, we propose to use convolution kernels for that task which identify meaningful fragments of sequences or trees by themselves. We not only investigate how different levels of information can be effectively combined in different kernels but also examine how the scope of these kernels should be chosen. In general relation extraction, the two candidate entities thought to be involved in a relation are commonly chosen to be the boundaries of sequences and trees. The deﬁnition of boundaries in opinion holder extraction, however, is less straightforward since there might be several expressions beside the candidate opinion holder to be eligible for being a boundary. 
With the increase in popularity of online review sites comes a corresponding need for tools capable of extracting the information most important to the user from the plain text data. Due to the diversity in products and services being reviewed, supervised methods are often not practical. We present an unsupervised system for extracting aspects and determining sentiment in review text. The method is simple and ﬂexible with regard to domain and language, and takes into account the inﬂuence of aspect on sentiment polarity, an issue largely ignored in previous literature. We demonstrate its effectiveness on both component tasks, where it achieves similar results to more complex semi-supervised methods that are restricted by their reliance on manual annotation and extensive knowledge sources. 
In evidence-based medicine, clinical questions involve four aspects: Patient/Problem (P), Intervention (I), Comparison (C) and Outcome (O), known as PICO elements. In this paper we present a method that extends the language modeling approach to incorporate both document structure and PICO query formulation. We present an analysis of the distribution of PICO elements in medical abstracts that motivates the use of a location-based weighting strategy. In experiments carried out on a collection of 1.5 million abstracts, the method was found to lead to an improvement of roughly 60% in MAP and 70% in P@10 as compared to state-of-the-art methods. 
Image annotation, the task of automatically generating description words for a picture, is a key component in various image search and retrieval applications. Creating image databases for model development is, however, costly and time consuming, since the keywords must be hand-coded and the process repeated for new collections. In this work we exploit the vast resource of images and documents available on the web for developing image annotation models without any human involvement. We describe a probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. We show that this model outperforms previously proposed approaches when applied to image annotation and the related task of text illustration despite the noisy nature of our dataset. 
In a Wizard-of-Oz experiment with multiple wizard subjects, each wizard viewed automated speech recognition (ASR) results for utterances whose interpretation is critical to task success: requests for books by title from a library database. To avoid non-understandings, the wizard directly queried the application database with the ASR hypothesis (voice search). To learn how to avoid misunderstandings, we investigated how wizards dealt with uncertainty in voice search results. Wizards were quite successful at selecting the correct title from query results that included a match. The most successful wizard could also tell when the query results did not contain the requested title. Our learned models of the best wizard’s behavior combine features available to wizards with some that are not, such as recognition confidence and acoustic model scores. 
This paper presents a direct word reordering model with novel syntax-based features for statistical machine translation. Reordering models address the problem of reordering source language into the word order of the target language. IBM Models 3 through 5 have reordering components that use surface word information but very little context information to determine the traversal order of the source sentence. Since the late 1990s, phrase-based machine translation solves much of the local reorderings by using phrasal translations. The problem of longdistance reordering has become a central research topic in modeling distortions. We present a syntax driven maximum entropy reordering model that directly predicts the source traversal order and is able to model arbitrarily long distance word movement. We show that this model significantly improves machine translation quality. 
We describe a class of translation model in which a set of input variants encoded as a context-free forest is translated using a ﬁnitestate translation model. The forest structure of the input is well-suited to representing word order alternatives, making it straightforward to model translation as a two step process: (1) tree-based source reordering and (2) phrase transduction. By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is signiﬁcantly smaller than a comparable hierarchical phrase-based translation model. 
The distortion cost function used in Mosesstyle machine translation systems has two ﬂaws. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, all distortion is penalized linearly, even when appropriate reorderings are performed. Because the cost function does not effectively constrain search, translation quality decreases at higher distortion limits, which are often needed when translating between languages of different typologies such as Arabic and English. To address these problems, we introduce a method for estimating future linear distortion cost, and a new discriminative distortion model that predicts word movement during translation. In combination, these extensions give a statistically signiﬁcant improvement over a baseline distortion parameterization. When we triple the distortion limit, our model achieves a +2.32 BLEU average gain over Moses. 
Synchronous tree substitution grammars are a translation model that is used in syntax-based machine translation. They are investigated in a formal setting and compared to a competitor that is at least as expressive. The competitor is the extended multi bottom-up tree transducer, which is the bottom-up analogue with one essential additional feature. This model has been investigated in theoretical computer science, but seems widely unknown in natural language processing. The two models are compared with respect to standard algorithms (binarization, regular restriction, composition, application). Particular attention is paid to the complexity of the algorithms. 
We present a new method that compresses sentences by removing words. In a ﬁrst stage, it generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classiﬁer. In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression model. Experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules. 
We address the challenge of automatically abstracting conversations such as face-to-face meetings and emails. We focus here on the stages of interpretation, where sentences are mapped to a conversation ontology, and transformation, where the summary content is selected. Our approach is fully developed and tested on meeting speech, and we subsequently explore its application to email conversations. 
This paper analyzes the topic identiﬁcation stage of single-document automatic text summarization across four different domains, consisting of newswire, literary, scientiﬁc and legal documents. We present a study that explores the summary space of each domain via an exhaustive search strategy, and ﬁnds the probability density function (pdf) of the ROUGE score distributions for each domain. We then use this pdf to calculate the percentile rank of extractive summarization systems. Our results introduce a new way to judge the success of automatic summarization systems and bring quantiﬁed explanations to questions such as why it was so hard for the systems to date to have a statistically signiﬁcant improvement over the lead baseline in the news domain. 
We treat the text summarization problem as maximizing a submodular function under a budget constraint. We show, both theoretically and empirically, a modiﬁed greedy algorithm can efﬁciently solve the budgeted submodular maximization problem near-optimally, and we derive new approximation bounds in doing so. Experiments on DUC’04 task show that our approach is superior to the bestperforming method from the DUC’04 evaluation on ROUGE-1 scores. 
We describe a cross-lingual method for the induction of selectional preferences for resourcepoor languages, where no accurate monolingual models are available. The method uses bilingual vector spaces to “translate” foreign language predicate-argument structures into a resource-rich language like English. The only prerequisite for constructing the bilingual vector space is a large unparsed corpus in the resource-poor language, although the model can proﬁt from (even noisy) syntactic knowledge. Our experiments show that the cross-lingual predictions correlate well with human ratings, clearly outperforming monolingual baseline models. 
Datasets annotated with semantic roles are an important prerequisite to developing highperformance role labeling systems. Unfortunately, the reliance on manual annotations, which are both difﬁcult and highly expensive to produce, presents a major obstacle to the widespread application of these systems across different languages and text genres. In this paper we describe a method for inducing the semantic roles of verbal arguments directly from unannotated text. We formulate the role induction problem as one of detecting alternations and ﬁnding a canonical syntactic form for them. Both steps are implemented in a novel probabilistic model, a latent-variable variant of the logistic classiﬁer. Our method increases the purity of the induced role clusters by a wide margin over a strong baseline. 
This paper contributes a formalization of frame-semantic parsing as a structure prediction problem and describes an implemented parser that transforms an English sentence into a frame-semantic representation. It ﬁnds words that evoke FrameNet frames, selects frames for them, and locates the arguments for each frame. The system uses two featurebased, discriminative probabilistic (log-linear) models, one with latent variables to permit disambiguation of new predicate words. The parser is demonstrated to signiﬁcantly outperform previously published results. 
This paper presents efﬁcient algorithms for expected similarity maximization, which coincides with minimum Bayes decoding for a similarity-based loss function. Our algorithms are designed for similarity functions that are sequence kernels in a general class of positive deﬁnite symmetric kernels. We discuss both a general algorithm and a more efﬁcient algorithm applicable in a common unambiguous scenario. We also describe the application of our algorithms to machine translation and report the results of experiments with several translation data sets which demonstrate a substantial speed-up. In particular, our results show a speed-up by two orders of magnitude with respect to the original method of Tromble et al. (2008) and by a factor of 3 or more even with respect to an approximate algorithm speciﬁcally designed for that task. These results open the path for the exploration of more appropriate or optimal kernels for the speciﬁc tasks considered. 
A principal weakness of conventional (i.e., non-hierarchical) phrase-based statistical machine translation is that it can only exploit continuous phrases. In this paper, we extend phrase-based decoding to allow both source and target phrasal discontinuities, which provide better generalization on unseen data and yield signiﬁcant improvements to a standard phrase-based system (Moses). More interestingly, our discontinuous phrasebased system also outperforms a state-of-the-art hierarchical system (Joshua) by a very signiﬁcant margin (+1.03 BLEU on average on ﬁve ChineseEnglish NIST test sets), even though both Joshua and our system support discontinuous phrases. Since the key difference between these two systems is that ours is not hierarchical—i.e., our system uses a string-based decoder instead of CKY, and it imposes no hard hierarchical reordering constraints during training and decoding—this paper sets out to challenge the commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses. 
Machine translation beneﬁts from two types of decoding techniques: consensus decoding over multiple hypotheses under a single model and system combination over hypotheses from different models. We present model combination, a method that integrates consensus decoding and system combination into a uniﬁed, forest-based technique. Our approach makes few assumptions about the underlying component models, enabling us to combine systems with heterogenous structure. Unlike most system combination techniques, we reuse the search space of component models, which entirely avoids the need to align translation hypotheses. Despite its relative simplicity, model combination improves translation quality over a pipelined approach of ﬁrst applying consensus decoding to individual systems, and then applying system combination to their output. We demonstrate BLEU improvements across data sets and language pairs in large-scale experiments. 
Automatically ﬁnding email messages that contain requests for action can provide valuable assistance to users who otherwise struggle to give appropriate attention to the actionable tasks in their inbox. As a speech act classiﬁcation task, however, automatically recognising requests in free text is particularly challenging. The problem is compounded by the fact that typical emails contain extraneous material that makes it difﬁcult to isolate the content that is directed to the recipient of the email message. In this paper, we report on an email classiﬁcation system which identiﬁes messages containing requests; we then show how, by segmenting the content of email messages into different functional zones and then considering only content in a small number of message zones when detecting requests, we can improve the accuracy of message-level automated request classiﬁcation to 83.76%, a relative increase of 15.9%. This represents an error reduction of 41% compared with the same request classiﬁer deployed without email zoning. 
 1.1 The structure of discourse  Hierarchical discourse segmentation is a useful technology, but it is difﬁcult to evaluate. I propose an error measure based on the word error rate of Beeferman et al. (1999). I then show that this new measure not only reliably distinguishes baseline segmentations from lexically-informed hierarchical segmentations and more informed segmentations from less informed segmentations, but it also offers an improvement over previous linear error measures. 
In this paper we report a behavioural experiment documenting that different lexicosyntactic formulations of the discourse relation of causation are deemed more or less acceptable by different categories of readers. We further report promising results for automatically selecting the formulation that is most appropriate for a given category of reader using supervised learning. This investigation is embedded within a longer term research agenda aimed at summarising scientiﬁc writing for lay readers using appropriate paraphrasing.  
We describe tree edit models for representing sequences of tree transformations involving complex reordering phenomena and demonstrate that they offer a simple, intuitive, and effective method for modeling pairs of semantically related sentences. To efﬁciently extract sequences of edits, we employ a tree kernel as a heuristic in a greedy search routine. We describe a logistic regression model that uses 33 syntactic features of edit sequences to classify the sentence pairs. The approach leads to competitive performance in recognizing textual entailment, paraphrase identiﬁcation, and answer selection for question answering. 
In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 
We present a novel approach to metaphor interpretation and a system that produces literal paraphrases for metaphorical expressions. Such a representation is directly transferable to other applications that can beneﬁt from a metaphor processing component. Our method is distinguished from the previous work in that it does not rely on any hand-crafted knowledge about metaphor, but in contrast employs automatically induced selectional preferences. Being the ﬁrst of its kind, our system is capable of paraphrasing metaphorical expressions with a high accuracy (0.81). 
This paper first describes an experiment to construct an English-Chinese parallel corpus, then applying the Uplug word alignment tool on the corpus and finally produce and evaluate an English-Chinese word list. The Stockholm English-Chinese Parallel Corpus (SEC) was created by downloading English-Chinese parallel corpora from a Chinese web site containing law texts that have been manually translated from Chinese to English. The parallel corpus contains 104 563 Chinese characters equivalent to 59 918 Chinese words, and the corresponding English corpus contains 75 766 English words. However Chinese writing does not utilize any delimiters to mark word boundaries so we had to carry out word segmentation as a preprocessing step on the Chinese corpus. Moreover since the parallel corpus is downloaded from Internet the corpus is noisy regarding to alignment between corresponding translated sentences. Therefore we used 60 hours of manually work to align the sentences in the English and Chinese parallel corpus before performing automatic word alignment using Uplug. The word alignment with Uplug was carried out from English to Chinese. Nine respondents evaluated the resulting English-Chinese word list with frequency equal to or above three and we obtained an accuracy of 73.1 percent.
FreeLing is an open-source multilingual language processing library providing a wide range of language analyzers for several languages. It offers text processing and language annotation facilities to natural language processing application developers, simplifying the task of building those applications. FreeLing is customizable and extensible. Developers can use the default linguistic resources (dictionaries, lexicons, grammars, etc.) directly, or extend them, adapt them to specific domains, or even develop new ones for specific languages. This paper overviews the recent history of this tool, summarizes the improvements and extensions incorporated in the latest version, and depicts the architecture of the library. Special focus is brought to the fact and consequences of the library being open-source: After five years and over 35,000 downloads, a growing user community has extended the initial threelanguages (English, Spanish and Catalan) to eight (adding Galician, Italian, Welsh, Portuguese, and Asturian), proving that the collaborative open model is a productive approach for the development of NLP tools and resources.
Transliteration is the rendering in one language of terms from another language (and, possibly, another writing system), approximating spelling and/or phonetic equivalents between the two languages. A transliteration dictionary is a crucial resource for a variety of natural language applications, most notably machine translation. We describe a general method for creating bilingual transliteration dictionaries from Wikipedia article titles. The method can be applied to any language pair with Wikipedia presence, independently of the writing systems involved, and requires only a single simple resource that can be provided by any literate bilingual speaker. It was successfully applied to extract a Hebrew-English transliteration dictionary which, when incorporated in a machine translation system, indeed improved its performance.
Blog posts containing many personal experiences or perspectives toward specific subjects are useful. Blogs allow readers to interact with bloggers by placing comments on specific blog posts. The comments carry viewpoints of readers toward the targets described in the post, or supportive/non-supportive attitude toward the post. Comment extraction is challenging due to that there does not exist a unique template among all blog service providers. This paper proposes methods to deal with this problem. Firstly, the repetitive patterns and their corresponding blocks are extracted from input posts by pattern identification algorithm. Secondly, three filtering strategies, i.e., tag pattern loop filtering, rule overlap filtering, and longest rule first, are used to remove non-comment blocks. Finally, a comment/non-comment classifier is learned to distinguish comment blocks from non-comment blocks with 14 block-level features and 5 rule-level features. In the experiments, we randomly select 600 blog posts from 12 blog service providers. F-measure, recall, and precision are 0.801, 0.855, and 0.780, respectively, by using all of the three filtering strategies together with some selected features. The application of comment extraction to blog mining is also illustrated. We show how to identify the relevant opinionated objects ― say, opinion holders, opinions, and targets, from posts.
This paper presents FOLKER, an annotation tool developed for the efficient transcription of natural, multi-party interaction in a conversation analysis framework. FOLKER is being developed at the Institute for German Language in and for the FOLK project, whose aim is the construction of a large corpus of spoken present-day German, to be used for research and teaching purposes. FOLKER builds on the experience gained with multi-purpose annotation tools like ELAN and EXMARaLDA, but attempts to improve transcription efficiency by restricting and optimizing both data model and tool functionality to a single, well-defined purpose. The tools most important features in this respect are the possibility to freely switch between several editable views according to the requirements of different steps in the annotation process, and an automatic syntax check of annotations during input for their conformance to the GAT transcription convention. This paper starts with a description of the GAT transcription conventions and the data model underlying the tool. It then gives an overview of the tool functionality and compares this functionality to that of other widely used tools.
This paper presents and analyzes an annotated corpus of definitions, created to train an algorithm for the automatic extraction of definitions and hypernyms from web documents. As an additional resource, we also include a corpus of non-definitions with syntactic patterns similar to those of definition sentences, e.g.: ''``An android is a robot'''' vs. ''``Snowcap is unmistakable''''. Domain and style independence is obtained thanks to the annotation of a large and domain-balanced corpus and to a novel pattern generalization algorithm based on word-class lattices (WCL). A lattice is a directed acyclic graph (DAG), a subclass of nondeterministic finite state automata (NFA). The lattice structure has the purpose of preserving the salient differences among distinct sequences, while eliminating redundant information. The WCL algorithm will be integrated into an improved version of the GlossExtractor Web application (Velardi et al., 2008). This paper is mostly concerned with a description of the corpus, the annotation strategy, and a linguistic analysis of the data. A summary of the WCL algorithm is also provided for the sake of completeness.
Without any doubt corpora are vital tools for linguistic studies and solution for applied tasks. Although corpora opportunities are very useful, there is a need of another kind of software for further improvement of linguistic research as it is impossible to process huge amount of linguistic data manually. The Sketch Engine representing itself a corpus tool which takes as input a corpus of any language and corresponding grammar patterns. The paper describes the writing of Sketch grammar for the Russian language as a part of the Sketch Engine system. The system gives information about a words collocability on concrete dependency models, and generates lists of the most frequent phrases for a given word based on appropriate models. The paper deals with two different approaches to writing rules for the grammar, based on morphological information, and also with applying word sketches to the Russian language. The data evidences that such results may find an extensive use in various fields of linguistics, such as dictionary compiling, language learning and teaching, translation (including machine translation), phraseology, information retrieval etc.
This paper proposes to introduce a novel reordering model in the open-source Moses toolkit. The main idea is to provide weighted reordering hypotheses to the SMT decoder. These hypotheses are built using a first-step Ngram-based SMT translation from a source language into a third representation that is called reordered source language. Each hypothesis has its own weight provided by the Ngram-based decoder. This proposed reordering technique offers a better and more efficient translation when compared to both the distance-based and the lexicalized reordering. In addition to this reordering approach, this paper describes a domain adaptation technique which is based on a linear combination of an specific in-domain and an extra out-domain translation models. Results for both approaches are reported in the Arabic-to-English 2008 IWSLT task. When implementing the weighted reordering hypotheses and the domain adaptation technique in the final translation system, translation results reach improvements up to 2.5 BLEU compared to a standard state-of-the-art Moses baseline system.
The Sign Linguistics Corpora Network is a three-year network initiative that aims to collect existing knowledge and practices on the creation and use of signed language resources. The concrete goals are to organise a series of four workshops in 2009 and 2010, create a stable Internet location for such knowledge, and generate new ideas for employing the most recent technologies for the study of signed languages. The network covers a wide range of subjects: data collection, metadata, annotation, and exploitation; these are the topics of the four workshops. The outcomes of the first two workshops are summarised in this paper; both workshops demonstrated that the need for dedicated knowledge on sign language corpora is especially salient in countries where researchers work alone or in small groups, which is still quite common in many places in Europe. While the original goal of the network was primarily to focus on corpus linguistics and language documentation, human language technology has gradually been incorporated as a user group of signed language resources.
We present a three-part bilingual specialized dictionary Mexican Sign Language-Spanish / Spanish-Mexican Sign Language. This dictionary will be the outcome of a three-years agreement between the Italian Consiglio Nazionale delle Ricerche and the Mexican Conacyt. Although many other sign language dictionaries have been provided to deaf communities, there are no Mexican Sign Language dictionaries in Mexico, yet. We want to stress on the specialized feature of the proposed dictionary: the bilingual dictionary will contain frequently used general Spanish forms along with scholastic course specific specialized words whose meanings warrant comprehension of school curricula. We emphasize that this aspect of the bilingual dictionary can have a deep social impact, since we will furnish to deaf people the possibility to get competence in official language, which is necessary to ensure access to school curriculum and to become full-fledged citizens. From a technical point of view, the dictionary consists of a relational database, where we have saved the sign parameters and a graphical user interface especially designed to allow deaf children to retrieve signs using the relevant parameters and,thus, the meaning of the sign in Spanish.
We present experiments in automatic genre classification on web corpora, comparing a wide variety of features on several different genreannotated datasets (HGC, I-EN, KI-04, KRYS-I, MGC and SANTINIS).We investigate the performance of several types of features (POS n-grams, character n-grams and word n-grams) and show that simple character n-grams perform best on current collections because of their ability to generalise both lexical and syntactic phenomena related to genres. However, we also show that these impressive results might not be transferrable to the wider web due to the lack of comparability between different annotation labels (many webpages cannot be described in terms of the genre labels in individual collections), lack of representativeness of existing collections (many genres are represented by webpages coming from a small number of sources) as well as problems in the reliability of genre annotation (many pages from the web are difficult to interpret in terms of the labels available). This suggests that more research is needed to understand genres on the Web.
In the first part of this paper, we present a framework for enriching arbitrary upper or domain-specific ontologies with a concept of time. To do so, we need the notion of a time slice. Contrary to other approaches, we directly interpret the original entities as time slices in order to (i) avoid a duplication of the original ontology and (ii) to prevent a knowledge engineer from ontology rewriting. The diachronic representation of time is complemented by a sophisticated time ontology that supports underspecification and an arbitrarily fine granularity of time. As a showcase, we describe how the time ontology has been interfaced with the PROTON upper ontology. The second part investigates a temporal extension of RDF that replaces the usual triple notation by a more general tuple representation. In this setting, Hayes/ter Horst-like entailment rules are replaced by their temporal counterparts. Our motivation to move towards this direction is twofold: firstly, extending binary relation instances with time leads to a massive proliferation of useless objects (independently of the encoding); secondly, reasoning and querying with such extended relations is extremely complex, expensive, and error-prone.
We describe ScriptTranscriber, an open source toolkit for extracting transliterations in comparable corpora from languages written in different scripts. The system includes various methods for extracting potential terms of interest from raw text, for providing guesses on the pronunciations of terms, and for comparing two strings as possible transliterations using both phonetic and temporal measures. The system works with any script in the Unicode Basic Multilingual Plane and is easily extended to include new modules. Given comparable corpora, such as newswire text, in a pair of languages that use different scripts, ScriptTranscriber provides an easy way to mine transliterations from the comparable texts. This is particularly useful for underresourced languages, where training data for transliteration may be lacking, and where it is thus hard to train good transliterators. ScriptTranscriber provides an open source package that allows for ready incorporation of more sophisticated modules ― e.g. a trained transliteration model for a particular language pair. ScriptTranscriber is available as part of the nltk contrib source tree at http://code.google.com/p/nltk/.
Systems that locate mentions of concepts from ontologies in free text are known as ontology concept recognition systems. This paper describes an approach to the evaluation of the workings of ontology concept recognition systems through use of a structured test suite and presents a publicly available test suite for this purpose. It is built using the principles of descriptive linguistic fieldwork and of software testing. More broadly, we also seek to investigate what general principles might inform the construction of such test suites. The test suite was found to be effective in identifying performance errors in an ontology concept recognition system. The system could not recognize 2.1{\%} of all canonical forms and no non-canonical forms at all. Regarding the question of general principles of test suite construction, we compared this test suite to a named entity recognition test suite constructor. We found that they had twenty features in total and that seven were shared between the two models, suggesting that there is a core of feature types that may be applicable to test suite construction for any similar type of application.
The simple access to texts on digital libraries and the World Wide Web has led to an increased number of plagiarism cases in recent years, which renders manual plagiarism detection infeasible at large. Various methods for automatic plagiarism detection have been developed whose objective is to assist human experts in the analysis of documents for plagiarism. The methods can be divided into two main approaches: intrinsic and external. Unlike other tasks in natural language processing and information retrieval, it is not possible to publish a collection of real plagiarism cases for evaluation purposes since they cannot be properly anonymized. Therefore, current evaluations found in the literature are incomparable and, very often not even reproducible. Our contribution in this respect is a newly developed large-scale corpus of artificial plagiarism useful for the evaluation of intrinsic as well as external plagiarism detection. Additionally, new detection performance measures tailored to the evaluation of plagiarism detection algorithms are proposed.
The amount of research data in the Humanities is increasing at fast speed. Metadata helps describing and making accessible this data to interested researchers within and across institutions. While metadata interoperability is an issue that is being recognised and addressed, the systematic and user-driven provision of annotations and the linking together of resources into new organisational layers have received much less attention. This paper gives an overview of our evolving technological eScience environment to support such functionality. It describes two tools, ADDIT and ViCoS, which enable researchers, rather than archive managers, to organise and reorganise research data to fit their particular needs. The two tools, which are embedded into our institute's existing software landscape, are an initial step towards an eScience environment that gives our scientists easy access to (multimodal) research data of their interest, and empowers them to structure, enrich, link together, and share such data as they wish.
Email can be considered as a virtual working environment in which users are constantly struggling to manage the vast amount of exchanged data. Although most of this data belongs to well-defined workflows, these are implicit and largely unsupported by existing email clients. Semanta provides this support by enabling Semantic Email ― email enhanced with machine-processable metadata about specific types of email Action Items (e.g. Task Assignment, Meeting Proposal). In the larger picture, these items form part of ad-hoc workflows (e.g. Task Delegation, Meeting Scheduling). Semanta is faced with a knowledge-acquisition bottleneck, as users cannot be expected to annotate each action item, and their automatic recognition proves difficult. This paper focuses on applying computationally treatable aspects of speech act theory for the classification of email action items. A rule-based classification model is employed, based on the presence or form of a number of linguistic features. The technologys evaluation suggests that whereas full automation is not feasible, the results are good enough to be presented as suggestions for the user to review. In addition the rule-based system will bootstrap a machine learning system that is currently in development, to generate the initial training sets which are then improved through the users reviewing.
Parallel corpora are indispensable resources for a variety of multilingual natural language processing tasks. This paper presents a technique for fully automatic construction of constantly growing parallel corpora. We propose a simple and effective dictionary-based algorithm to extract parallel document pairs from a large collection of articles retrieved from the Internet, potentially containing manually translated texts. This algorithm was implemented and tested on Hebrew-English parallel texts. With properly selected thresholds, precision of 100{\%} can be obtained.
In this work we propose a strategy to reduce the impact of the sparse data problem in the tasks of lexical information acquisition based on the observation of linguistic cues. We propose a way to handle the uncertainty created by missing values, that is, when a zero value could mean either that the cue has not been observed because the word in question does not belong to the class, i.e. negative evidence, or that the word in question has just not been observed in the context sought by chance, i.e. lack of evidence. This uncertainty creates problems to the learner, because zero values for incompatible labelled examples make the cue lose its predictive capacity and even though some samples display the sought context, it is not taken into account. In this paper we present the results of our experiments to try to reduce this uncertainty by, as other authors do (Joanis et al. 2007, for instance), substituting zero values for pre-processed estimates. Here we present a first round of experiments that have been the basis for the estimates of linguistic information motivated by lexical classes. We obtained experimental results that show a clear benefit of the proposed approach.
Electronic patient records (EPRs) are a valuable resource for research but for confidentiality reasons they cannot be used freely. In order to make EPRs available to a wider group of researchers, sensitive information such as personal names has to be removed. De-identification is a process that makes this possible. Both rule-based as well as statistical and machine learning based methods exist to perform de-identification, but the second method requires annotated training material which exists only very sparsely for patient names. It is therefore necessary to use rule-based methods for de-identification of EPRs. Not much is known, however, about the order in which the various rules should be applied and how the different rules influence precision and recall. This paper aims to answer this research question by implementing and evaluating four common rules for de-identification of personal names in EPRs written in Swedish: (1) dictionary name matching, (2) title matching, (3) common words filtering and (4) learning from previous modules. The results show that to obtain the highest recall and precision, the rules should be applied in the following order: title matching, common words filtering and dictionary name matching.
Machine translation systems can be classified into rule-based and corpus-based approaches, in terms of their core technology. Since both paradigms have largely been used during the last years, one of the aims in the research community is to know how these systems differ in terms of translation quality. To this end, this paper reports a study and comparison of a rule-based and a corpus-based (particularly, statistical) Catalan-Spanish machine translation systems, both of them freely available in the web. The translation quality analysis is performed under two different domains: journalistic and medical. The systems are evaluated by using standard automatic measures, as well as by native human evaluators. Automatic results show that the statistical system performs better than the rule-based system. Human judgements show that in the Spanish-to-Catalan direction the statistical system also performs better than the rule-based system, while in the Catalan-to-Spanish direction is the other way round. Although the statistical system obtains the best automatic scores, its errors tend to be more penalized by human judgements than the errors of the rule-based system. This can be explained because statistical errors are usually unexpected and they do not follow any pattern.
Language resources can be classified under several categories. To be able to query and operate on all (or most of) these categories using a single digital tool would be very helpful for a large number of researchers working on languages. We describe such a tool in this paper. It is different from other such tools in that it allows querying and transformation on different kinds of resources (such as corpora, lexicon and language models) with the same framework. Search options can be given based on the kind of resource being queried. It is possible to select a matched resource and open it for editing in the specialized interfaces with which that resource is associated. The tool also allows the extracted or modified data to be saved separately, apart from having the usual facilities like displaying the results in KeyWord-In-Context (KWIC) format. We also present the notation used for querying and transformation, which is comparable to but different from the Corpus Query Language (CQL).
Traditional Danish reading training for dyslexic readers typically involves the presence of a professional reading therapist for guidance, advice and evaluation. Allowing dyslexic readers to train their reading skills on their own could not only benefit the dyslexics themselves in terms of increased flexibility but could also allow professional therapists to increase the amount of dyslexic readers to whom they have a professional contact. It is envisioned that an automated reading training tool operating on the basis of ASR could provide dyslexic users with such independence. However, only limited experience in handling dyslexic input (in Danish) by a speech recognizer exists currently. This paper reports on the establishment of a speech corpus of Danish dyslexic speech along with an annotation hereof and the setup of a proof-of-concept training tool allowing dyslexic users to improve their reading skills on their own. Despite relatively limited ASR performance, a usability evaluation by dyslexic users shows an unconditional belief in the fairness of the system and indicates furthermore willingness for using such a training tool.
The Arabic language has a very rich morphology where a word is composed of zero or more prefixes, a stem and zero or more suffixes. This makes Arabic data sparse compared to other languages, such as English, and consequently word segmentation becomes very important for many Natural Language Processing tasks that deal with the Arabic language. We present in this paper two segmentation schemes that are morphological segmentation and Arabic TreeBank segmentation and we show their impact on an important natural language processing task that is mention detection. Experiments on Arabic TreeBank corpus show 98.1{\%} accuracy on morphological segmentation and 99.4{\%} on morphological segmentation. We also discuss the importance of segmenting the text; experiments show up to 6F points improvement of the mention detection system performance when morphological segmentation is used instead of not segmenting the text. Obtained results also show up to 3F points improvement is achieved when the appropriate segmentation style is used.
In this paper, we present ISO-TimeML, a revised and interoperable version of the temporal markup language, TimeML. We describe the changes and enrichments made, while framing the effort in a more general methodology of semantic annotation. In particular, we assume a principled distinction between the annotation of an expression and the representation which that annotation denotes. This involves not only the specification of an annotation language for a particular phenomenon, but also the development of a meta-model that allows one to interpret the syntactic expressions of the specification semantically.
This paper introduces the results of integration of lexical and terminological resources, most of them developed within the Human Language Technology (HLT) Group at the University of Belgrade, with the Geological information system of Serbia (GeolISS) developed at the Faculty of Mining and Geology and funded by the Ministry of the Environmental protection. The approach to GeolISS development, which is aimed at the integration of existing geologic archives, data from published maps on different scales, newly acquired field data, and intranet and internet publishing of geologic is given, followed by the description of the geologic multilingual vocabulary and other lexical and terminological resources used. Two basic results are outlined: multilingual map annotation and improvement of queries for the GeolISS geodatabase. Multilingual labelling and annotation of maps for their graphic display and printing have been tested with Serbian, which describes regional information in the local language, and English, used for sharing geographic information with the world, although the geological vocabulary offers the possibility for integration of other languages as well. The resources also enable semantic and morphological expansion of queries, the latter being very important in highly inflective languages, such as Serbian.
This paper describes a new language resource of events and semantic roles that characterize real-world situations. Narrative schemas contain sets of related events (edit and publish), a temporal ordering of the events (edit before publish), and the semantic roles of the participants (authors publish books). This type of world knowledge was central to early research in natural language understanding, scripts being one of the main formalisms, they represented common sequences of events that occur in the world. Unfortunately, most of this knowledge was hand-coded and time consuming to create. Current machine learning techniques, as well as a new approach to learning through coreference chains, has allowed us to automatically extract rich event structure from open domain text in the form of narrative schemas. The narrative schema resource described in this paper contains approximately 5000 unique events combined into schemas of varying sizes. We describe the resource, how it is learned, and a new evaluation of the coverage of these schemas over unseen documents.
This paper describes the development of a new Swedish scientific medical corpus. We provide a detailed description of the characteristics of this new collection as well results of an application of the corpus on term management tasks, including terminology validation and terminology extraction. Although the corpus is representative for the scientific medical domain it still covers in detail a lot of specialised sub-disciplines such as diabetes and osteoporosis which makes it suitable for facilitating the production of smaller but more focused sub-corpora. We address this issue by making explicit some features of the corpus in order to demonstrate the usability of the corpus particularly for the quality assessment of subsets of official terminologies such as the Systematized NOmenclature of MEDicine - Clinical Terms (SNOMED CT). Domain-dependent language resources, labelled or not, are a crucial key components for progressing R{\&}D in the human language technology field since such resources are an indispensable, integrated part for terminology management, evaluation, software prototyping and design validation and a prerequisite for the development and evaluation of a number of sublanguage dependent applications including information extraction, text mining and information retrieval.
In Natural Language Processing (NLP), the quality of a system depends to a great extent on the quality of the linguistic resources it uses. One area where precise information is particularly needed is valency. The unpredictable character of valency properties requires a reliable source of information for syntactic and semantic analysis. There are several (electronic) dictionaries that provide the necessary information. One such dictionary that contains especially detailed valency descriptions is the Valency Dictionary of English. We will discuss how the Valency Dictionary of English in machine-readable form can be used as a resource for NLP. We will use valency descriptions that are freely available online via the Erlangen Valency Pattern Bank which contains most of the information from the printed dictionary. We will show that the valency data can be used for accurately parsing natural language with a rule-based approach by integrating it into a Left-Associative Grammar. The Valency Dictionary of English can therefore be regarded as being well suited for NLP purposes.
This paper deals with the main problems that arise in the query translation process in dictionary-based Cross-lingual Information Retrieval (CLIR): translation selection, presence of Out-Of-Vocabulary (OOV) terms and translation of Multi-Word Expressions (MWE). We analyse to what extent each problem affects the retrieval performance for the Basque-English pair of languages, and the improvement obtained when using parallel corpora free methods to address them. To tackle the translation selection problem we provide novel extensions of an already existing monolingual target co-occurrence-based method, the Out-Of Vocabulary terms are dealt with by means of a cognate detection-based method and finally, for the Multi-Word Expression translation problem, a na{\"\i
In this paper, we introduce the FastKwic (Key Word In Context using FASTR), a new concordancer for French and English that does not require users to learn any particular request language. Built on FASTR, it shows them not only occurrences of the searched term but also of several morphological, morpho-syntactic and syntactic variants (for example, image enhancement, enhancement of image, enhancement of fingerprint image, image texture enhancement). Fastkwic is freely available. It consists of two UTF-8 compliant Perl modules that depend on several external tools and resources : FASTR, TreeTagger, Flemm (for French). Licenses of theses tools and resources permitting, the FastKwic package is nevertheless self-sufficient. FastKwic first modules is for terminological resource compilation. Its input is a list of terms - as required by FASTR. FastKwic second module is for processing concordances. It relies on FASTR again for indexing the input corpus with terms and their variants. Its output is a concordancer: for each term and its variants, the context of occurrence is provided.
In this paper we discuss some well-known morphological descriptions used in various projects and applications (most notably MULTEXT-East and Unitex) and illustrate the encountered problems on Serbian. We have spotted four groups of problems: the lack of a value for an existing category, the lack of a category, the interdependence of values and categories lacking some description, and the lack of a support for some types of categories. At the same time, various descriptions often describe exactly the same morphological property using different approaches. We propose a new morphological description for Serbian following the feature structure representation defined by the ISO standard. In this description we try do incorporate all characteristics of Serbian that need to be specified for various applications. We have developed several XSLT scripts that transform our description into descriptions needed for various applications. We have developed the first version of this new description, but we treat it as an ongoing project because for some properties we have not yet found the satisfactory solution.
The paper presents a new fuzzy agreement measure {\$}{\textbackslash}gamma{\_}f{\$} for determining the agreement in multi-label and subjective annotation task. In this annotation framework, one data item may belong to a category or a class with a belief value denoting the degree of confidence of an annotator in assigning the data item to that category. We have provided a notion of disagreement based on the belief values provided by the annotators with respect to a category. The fuzzy agreement measure {\$}{\textbackslash}gamma{\_}f{\$} has been proposed by defining different fuzzy agreement sets based on the distribution of difference of belief values provided by the annotators. The fuzzy agreement has been computed by studying the average agreement over all the data items and annotators. Finally, we elaborate on the computation {\$}{\textbackslash}gamma{\_}f{\$} measure with a case study on emotion text data where a data item (sentence) may belong to more than one emotion category with varying belief values.
This paper presents the participation of FIDJI system to the Web Question-Answering evaluation campaign organized by Quaero in 2009. FIDJI is an open-domain question-answering system which combines syntactic information with traditional QA techniques such as named entity recognition and term weighting in order to validate answers through multiple documents. It was originally designed to process ``clean'' document collections. Overall results are significantly lower than in traditional campaigns but results (for French evaluation) are quite good compared to other state-of-the-art systems. They show that a syntax-based strategy, applied on uncleaned Web data, can still obtain good results. Moreover, we obtain much higher scores on ``complex'' questions, i.e. `how' and `why' questions, which are more representative of real user needs. These results show that questioning the Web with advanced linguistic techniques can be done without heavy pre-processing and with results that come near to best systems that use strong resources and large structured indexes.
Annotation Science, a discipline dedicated to developing and maturing methodology for the annotation of language resources, is playing a prominent role in the fields of computational and corpus linguistics. While progress in the search for the right annotation model and format is undeniable, these results only sparsely become manifest in actual solutions (i.e. software tools) that could be used by researchers wishing to annotate their resources right away, even less so for resources of spoken language transcriptions. The paper presents a solution consisting of a data model and an annotation tool that tries to fill this gap between {\^a}annotation science and the practice of transcribing spoken language in the area of discourse analysis and pragmatics, where the lack of ready-to-use annotation solutions is especially remarkable. The chosen model combines feature structures in standoff-annotation and a data model based on annotation graphs, combining their advantages. It is ideally fitted for the transcription of spoken language by centering on the temporal relations of the speakers utterances and is implemented in reliable tools that support an iterative workflow. The standoff annotation allows for more complex annotations and relies on an established and well documented model.
In this paper I present the CLARIN-NL project, the Dutch national project that aims to play a central role in the European CLARIN infrastructure, not only for the preparatory phase, but also for the implementation and exploitation phases. I argue that the way the CLARIN-NL project has been set-up can serve as an excellent example for other national CLARIN projects, for the following reasons: (1) it is a mix between a programme and a project; (2) it offers opportunities to seriously test standards and protocols currently proposed by CLARIN, thus providing evidence-based requirements and desiderata for the CLARIN infrastructure and ensuring compatibility of CLARIN with national data and tools; (3) it brings the intended users (humanities researchers) and the technology providers (infrastructure specialists and language and speech technology researchers) together in concrete cooperation projects, with a central role for the users research questions,, thus ensuring that the infrastructure will provide functionality that is needed by its intended users.
This paper presents the language resource management system for the development and dissemination of Asian WordNet (AWN) and its web service application. We develop the platform to establish a network for the cross language WordNet development. Each node of the network is designed for maintaining the WordNet for a language. Via the table that maps between each language WordNet and the Princeton WordNet (PWN), the Asian WordNet is realized to visualize the cross language WordNet between the Asian languages. We propose a language resource management system, called WordNet Management System (WNMS), as a distributed management system that allows the server to perform the cross language WordNet retrieval, including the fundamental web service applications for editing, visualizing and language processing. The WNMS is implemented on a web service protocol therefore each node can be independently maintained, and the service of each language WordNet can be called directly through the web service API. In case of cross language implementation, the synset ID (or synset offset) defined by PWN is used to determined the linkage between the languages.
This paper gives guidelines of how to create and update Propbank frameset files using a dedicated editor, Cornerstone. Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate. Propbank annotation also requires the choice of a sense ID for each predicate. Thus, for each predicate in Propbank, there exists a corresponding frameset file showing the expected predicate argument structure of each sense related to the predicate. Since most Propbank annotations are based on the predicate argument structure defined in the frameset files, it is important to keep the files consistent, simple to read as well as easy to update. The frameset files are written in XML, which can be difficult to edit when using a simple text editor. Therefore, it is helpful to develop a user-friendly editor such as Cornerstone, specifically customized to create and edit frameset files. Cornerstone runs platform independently, is light enough to run as an X11 application and supports multiple languages such as Arabic, Chinese, English, Hindi and Korean.
This paper deals with the derivational morphology of automatic word form recognition. It presents a set of declarative rules which augment lexical entries with information governing the allomorphic changes of derivation in addition to the existing allomorphy rules for inflection. The resulting component generates a single lexicon for derivational and inflectional allomorphy from an elementary base-form lexicon. Thereby our focus lies both on avoiding redundant allomorph entries and on the suitability of the resulting lexical entries for morphological analysis. We prove the usability of our approach by using the generated allomorphs as the lexicon for automatic wordform recognition.
The Bank of Russian Constructions and Valencies (Russian FrameBank) is an annotation project that takes as input samples from the Russian National Corpus (http://www.ruscorpora.ru). Since Russian verbs and predicates from other POS classes have their particular and not always predictable case pattern, these words and their argument structures are to be described as lexical constructions. The slots of partially filled phrasal constructions (e.g. vzjal i uexal he suddenly (lit. took and) went away) are also under analysis. Thus, the notion of construction is understood in the sense of Fillmores Construction Grammar and is not limited to that of argument structure of verbs. FrameBank brings together the dictionary of constructions and the annotated collection of examples. Our goal is to mark the set of arguments and adjuncts of a certain construction. The main focus is on realization of the elements in the running text, to facilitate searches through pattern realizations by a certain combination of features. The relevant dataset involves lexical, POS and other morphosyntactic tags, semantic classes, as well as grammatical constructions that introduce or license the use of elements within a given construction.
Information extraction deals with extracting entities (such as people, organizations or locations) and named relations between entities (such as ''``People born-in Country'''') from text documents. An important challenge in information extraction is the labeling of training data which is usually done manually and is therefore very laborious and in certain cases impractical. This paper introduces a new model to extract semantic relations fully automatically from text using the Encarta encyclopedia and lexical-semantic relations discovered by MindNet. MindNet is a lexical knowledge base that can be constructed fully automatically from a given text corpus without any human intervention. Encarta articles are categorized and linked to related articles by experts. We demonstrate how the structured data available in Encarta and the lexical semantic relations between words in MindNet can be used to enrich MindNet with semantic relations between entities. With a slight trade off of accuracy a semantically enriched MindNet can be used to extract relations from a text corpus without any human intervention.
For many languages there are no large, general-language corpora available. Until the web, all but the institutions could do little but shake their heads in dismay as corpus-building was long, slow and expensive. But with the advent of the Web it can be highly automated and thereby fast and inexpensive. We have developed a corpus factory where we build large corpora. In this paper we describe the method we use, and how it has worked, and how various problems were solved, for eight languages: Dutch, Hindi, Indonesian, Norwegian, Swedish, Telugu, Thai and Vietnamese. We use the BootCaT method: we take a set of 'seed words' for the language from Wikipedia. Then, several hundred times over, we * randomly select three or four of the seed words * send as a query to Google or Yahoo or Bing, which returns a 'search hits' page * gather the pages that Google or Yahoo point to and save the text. This forms the corpus, which we then * 'clean' (to remove navigation bars, advertisements etc) * remove duplicates * tokenise and (if tools are available) lemmatise and part-of-speech tag * load into our corpus query tool, the Sketch Engine The corpora we have developed are available for use in the Sketch Engine corpus query tool.
This paper describes a new flexible representation for the annotation of complex structures of metadata over heterogeneous data collections containing text and other types of media such as images or audio files. We argue that existing frameworks are not suitable for this purpose, most importantly because they do not easily generalize to multi-document and multimodal corpora, and because they often require the use of particular software frameworks. In the paper, we define a data model to represent such structured data over multimodal collections. Furthermore, we define a surface realization of the data structure as a simple and readable XML format. We present two examples of annotation tasks to illustrate how the representation and format work for complex structures involving multimodal annotation and cross-document links. The representation described here has been used in a large-scale project focusing on the annotation of a wide range of information ― from low-level features to high-level semantics ― in a multimodal data collection containing both text and images.
The Quaero project organized a set of evaluations of Named Entity recognition systems in 2009. One of the sub-tasks consists in extracting citations from patents, i.e. references to other documents, either other patents or general literature from English-language patents. We present in this paper the participation of LIMSI in this evaluation, with a complete system description and the evaluation results. The corpus shown that patent and non-patent citations have a very different nature. We then separated references to other patents and to general literature papers and we created a hybrid system. For patent citations, the system used rule-based expert knowledge on the form of regular expressions. The system for detecting non-patent citations, on the other hand, is purely stochastic (machine learning with CRF++). Then we mixed both approaches to provide a single output. 4 teams participated to this task and our system obtained the best results of this evaluation campaign, even if the difference between the first two systems is poorly significant.
In this paper describes the effects of the evolution of an Italian dependency grammar on a task of multilingual FrameNet acquisition. The task is based on the creation of virtual English/Italian parallel annotation corpora, which are then aligned at dependency level by using two manually encoded grammar based dependency parsers. We show how the evolution of the LAS (Labeled Attachment Score) metric for the considered grammar has a direct impact on the quality of the induced FrameNet, thus proving that the evolution of the quality of syntactic resources is mirrored by an analogous evolution in semantic ones. In particular we show that an improvement of 30{\%} in LAS causes an improvement of precision for the induced resource ranging from 5{\%} to 10{\%}, depending on the type of evaluation.
In Chinese texts, words composed of single or multiple characters are not separated by spaces, unlike most western languages. Therefore Chinese word segmentation is considered an important first step in machine translation (MT) and its performance impacts MT results. Many factors affect Chinese word segmentations, including the segmentation standards and segmentation strategies. The performance of a corpus-based word segmentation model depends heavily on the quality and the segmentation standard of the training corpora. However, we observed that existing manually annotated Chinese corpora tend to have low segmentation granularity and provide poor morphological information due to the present segmentation standards. In this paper, we introduce a short-unit standard of Chinese word segmentation, which is particularly suitable for machine translation, and propose a semi-automatic method of transforming the existing corpora into the ones that can satisfy our standards. We evaluate the usefulness of our approach on the basis of translation tasks from the technology newswire domain and the scientific paper domain, and demonstrate that it significantly improves the performance of Chinese-Japanese machine translation (over 1.0 BLEU increase).
This paper focuses on the improvement of the conceptual structure of FrameNet (FN) for the sake of applying this resource to knowledge-intensive NLP tasks requiring reasoning, such as question answering, information extraction etc. In this paper we show that in addition to coverage incompleteness, the current version of FN suffers from conceptual inconsistency and lacks axiomatization which can prevent appropriate inferences. For the sake of discovering and classifying conceptual problems in FN we investigate the FrameNet-Annotated corpus for Textual Entailment. Then we propose a methodology for improving the conceptual organization of FN. The main issue we focus on in our study is enriching, axiomatizing and cleaning up frame relations. Our methodology includes a data-driven analysis of frames resulting in discovering new frame relations and an ontological analysis of frames and frame relations resulting in axiomatizing relations and formulating constraints on them. In this paper, frames and frame relations are analyzed in terms of the DOLCE formal ontology. Additionally, we have described a case study aiming at demonstrating how the proposed methodology works in practice as well as investigating the impact of the restructured and axiomatized frame relations on recognizing textual entailment.
In this paper, we describe our experience with collecting and creating an annotated corpus of multi-party online conversations in a chat-room environment. This effort is part of a larger project to develop computational models of social phenomena such as agenda control, influence, and leadership in on-line interactions. Such models will help capturing the dialogue dynamics that are essential for developing, among others, realistic human-machine dialogue systems, including autonomous virtual chat agents. In this paper we describe data collection method used and the characteristics of the initial dataset of English chat. We have devised a multi-tiered collection process in which the subjects start from simple, free-flowing conversations and progress towards more complex and structured interactions. In this paper, we report on the first two stages of this process, which were recently completed. The third, large-scale collection effort is currently being conducted. All English dialogue has been annotated at four levels: communication links, dialogue acts, local topics and meso-topics. Some details of these annotations will be discussed later in this paper, although a full description is impossible within the scope of this article.
Automatic evaluation metrics are fast and cost-effective measurements of the quality of a Machine Translation (MT) system. However, as humans are the end-user of MT output, human judgement is the benchmark to assess the usefulness of automatic evaluation metrics. While most studies report the correlation between human evaluation and automatic evaluation at corpus level, our study examines their correlation at sentence level. In addition to the statistical correlation scores, such as Spearman's rank-order correlation coefficient, a finer-grained and detailed examination of the sensitivity of automatic metrics compared to human evaluation is also reported in this study. The results show that the threshold for human evaluators to agree with the judgements of automatic metrics varies with the automatic metrics at sentence level. While the automatic scores for two translations are greatly different, human evaluators may consider the translations to be qualitatively similar and vice versa. The detailed analysis of the correlation between automatic and human evaluation allows us determine with increased confidence whether an increase in the automatic scores will be agreed by human evaluators or not.
Synonyms dictionaries are useful resources for natural language processing. Unfortunately their availability in digital format is limited, as publishing companies do not release their dictionaries in open digital formats. Dicion{\'a}rio-Aberto (Sim{\~o}es and Farinha, 2010) is an open and free digital synonyms dictionary for the Portuguese language. It is under public domain and in textual digital format, which makes it usable for any task. Synonyms dictionaries are commonly used for the extraction of relations between words, the construction of complex structures like ontologies or thesaurus (comparable to WordNet (Miller et al., 1990)), or just the extraction of lists of words of specific type. This article will present Dicion{\'a}rio-Aberto, discussing how it was created, its main characteristics, the type of information present on it and the formats in which it is available. Follows the description of an API designed specifically to help Dicion{\'a}rio-Aberto processing without the need to tackle with the dictionary format. Finally, we will analyze the results on some data extraction experiments, extracting lists of words from a specific class, and extracting relationships between words.
In this paper, we propose GermanPolarityClues, a new publicly available lexical resource for sentiment analysis for the German language. While sentiment analysis and polarity classification has been extensively studied at different document levels (e.g. sentences and phrases), only a few approaches explored the effect of a polarity-based feature selection and subjectivity resources for the German language. This paper evaluates four different English and three different German sentiment resources in a comparative manner by combining a polarity-based feature selection with SVM-based machine learning classifier. Using a semi-automatic translation approach, we were able to construct three different resources for a German sentiment analysis. The manually finalized GermanPolarityClues dictionary offers thereby a number of 10, 141 polarity features, associated to three numerical polarity scores, determining the positive, negative and neutral direction of specific term features. While the results show that the size of dictionaries clearly correlate to polarity-based feature coverage, this property does not correlate to classification accuracy. Using a polarity-based feature selection, considering a minimum amount of prior polarity features, in combination with SVM-based machine learning methods exhibits for both languages the best performance (F1: 0.83-0.88).
In this paper, we present an ontology-based methodology and architecture for the comparison, assessment, combination (and, to some extent, also contrastive evaluation) of the results of different linguistic tools. More specifically, we describe an experiment aiming at the improvement of the correctness of lemma tagging for Spanish. This improvement was achieved by means of the standardisation and combination of the results of three different linguistic annotation tools (Bitexts DataLexica, Connexors FDG Parser and LACELLs POS tagger), using (1) ontologies, (2) a set of lemma tagging correction rules, determined empirically during the experiment, and (3) W3C standard languages, such as XML, RDF(S) and OWL. As we show in the results of the experiment, the interoperation of these tools by means of ontologies and the correction rules applied in the experiment improved significantly the quality of the resulting lemma tagging (when compared to the separate lemma tagging performed by each of the tools that we made interoperate).
Wikipedia has been used as a knowledge source in many areas of natural language processing. As most studies only use a certain Wikipedia snapshot, the influence of Wikipedias massive growth on the results is largely unknown. For the first time, we perform an in-depth analysis of this influence using semantic relatedness as an example application that tests a wide range of Wikipedias properties. We find that the growth of Wikipedia has almost no effect on the correlation of semantic relatedness measures with human judgments, while the coverage steadily increases.
This paper describes a software toolkit for the interactive display and analysis of automatically extracted or manually derived annotation features of visual and audio data. It has been extensively tested with material collected as part of the FreeTalk Multimodal Conversation Corpus. Both the corpus and the software are available for download from sites in Europe and Japan. The corpus consists of several hours of video and audio recordings from a variety of capture devices, and includes subjective annotations of the content, along with derived data obtained from image processing. Because of the large size of the corpus, it is unrealistic to expect researchers to download all the material before deciding whether it will be useful to them in their research. We have therefore devised a means for interactive browsing of the content and for viewing at different levels of granularity. This has resulted in a simple set of tools that can be added to any website to allow similar browsing of audio- video recordings and their related data and annotations.
Named Entity Recognition (NER) plays a relevant role in several Natural Language Processing tasks. Question-Answering (QA) is an example of such, since answers are frequently named entities in agreement with the semantic category expected by a given question. In this context, the recognition of named entities is usually applied in free text data. NER in natural language questions can also aid QA and, thus, should not be disregarded. Nevertheless, it has not yet been given the necessary importance. In this paper, we approach the identification and classification of named entities in natural language questions. We hypothesize that NER results can benefit with the inclusion of previously labeled questions in the training corpus. We present a broad study addressing that hypothesis, focusing on the balance to be achieved between the amount of free text and questions in order to build a suitable training corpus. This work also contributes by providing a set of nearly 5,500 annotated questions with their named entities, freely available for research purposes.
This paper presents the multimodal corpora that are being collected and annotated in the Nordic NOMCO project. The corpora will be used to study communicative phenomena such as feedback, turn management and sequencing. They already include video material for Swedish, Danish, Finnish and Estonian, and several social activities are represented. The data will make it possible to verify empirically how gestures (head movements, facial displays, hand gestures and body postures) and speech interact in all the three mentioned aspects of communication. The data are being annotated following the MUMIN annotation scheme, which provides attributes concerning the shape and the communicative functions of head movements, face expressions, body posture and hand gestures. After having described the corpora, the paper discusses how they will be used to study the way feedback is expressed in speech and gestures, and reports results from two pilot studies where we investigated the function of head gestures ― both single and repeated ― in combination with feedback expressions. The annotated corpora will be valuable sources for research on intercultural communication as well as for interaction in the individual languages.
Compilation of a 100 million words balanced corpus called the Balanced Corpus of Contemporary Written Japanese (or BCCWJ) is underway at the National Institute for Japanese Language and Linguistics. The corpus covers a wide range of text genres including books, magazines, newspapers, governmental white papers, textbooks, minutes of the National Diet, internet text (bulletin board and blogs) and so forth, and when possible, samples are drawn from the rigidly defined statistical populations by means of random sampling. All texts are dually POS-analyzed based upon two different, but mutually related, definitions of word. Currently, more than 90 million words have been sampled and XML annotated with respect to text-structure and lexical and character information. A preliminary linear discriminant analysis of text genres using the data of POS frequencies and sentence length revealed it was possible to classify the text genres with a correct identification rate of 88{\%} as far as the samples of books, newspapers, whitepapers, and internet bulletin boards are concerned. When the samples of blogs were included in this data set, however, the identification rate went down to 68{\%}, suggesting the considerable variance of the blog texts in terms of the textual register and style.
The importance of sentence-aligned parallel corpora has been widely acknowledged. Reference corpora in which sub-sentential translational correspondences are indicated manually are more labour-intensive to create, and hence less wide-spread. Such manually created reference alignments -- also called Gold Standards -- have been used in research projects to develop or test automatic word alignment systems. In most translations, translational correspondences are rather complex; for example word-by-word correspondences can be found only for a limited number of words. A reference corpus in which those complex translational correspondences are aligned manually is therefore also a useful resource for the development of translation tools and for translation studies. In this paper, we describe how we created a Gold Standard for the Dutch-English language pair. We present the annotation scheme, annotation guidelines, annotation tool and inter-annotator results. To cover a wide range of syntactic and stylistic phenomena that emerge from different writing and translation styles, our Gold Standard data set contains texts from different text types. The Gold Standard will be publicly available as part of the Dutch Parallel Corpus.
A lot of research effort has been spent on the development of emotion theories and modeling, however, their suitability and applicability to expressions in human computer interaction has not exhaustively been evaluated. Furthermore, investigations concerning the ability of the annotators to map certain expressions onto the developed emotion models is lacking proof. The proposed annotation tool, which incorporates the standard Geneva Emotional Wheel developed by Klaus Scherer and a novel temporal characteristic description feature, is aiming towards enabling the annotator to label expressions recorded in human computer interaction scenarios on an utterance level. Further, it is respecting key features of realistic and natural emotional expressions, such as their sequentiality, temporal characteristics, their mixed occurrences, and their expressivity or clarity of perception. Additionally, first steps towards evaluating the proposed tool, by analyzing utterance annotations taken from two expressive speech corpora, are undertaken and some future goals including the open source accessibility of the tool are given.
At present there is no publicly available data set to evaluate the performance of different summarization systems on the task of generating location-related extended image captions. In this paper we describe a corpus of human generated model captions in English and German. We have collected 932 model summaries in English from existing image descriptions and machine translated these summaries into German. We also performed post-editing on the translated German summaries to ensure high quality. Both English and German summaries are evaluated using a readability assessment as in DUC and TAC to assess their quality. Our model summaries performed similar to the ones reported in Dang (2005) and thus are suitable for evaluating automatic summarization systems on the task of generating image descriptions for location related images. In addition, we also investigated whether post-editing of machine-translated model summaries is necessary for automated ROUGE evaluations. We found a high correlation in ROUGE scores between post-edited and non-post-edited model summaries which indicates that the expensive process of post-editing is not necessary.
This presentation and accompanying demonstration focuses on the development of a mobile platform for e-learning purposes with enhanced text-to-speech capabilities. It reports on an international consortium project entitled Mobile E-learning for Africa (MELFA), which includes a reading and literacy training component, particularly focusing on an African language, isiXhosa. The high penetration rate of mobile phones within the African continent has created new opportunities for delivering various kinds of information, including e-learning material to communities that have not had appropriate infrastructures. Aspects of the mobile platform development are described paying attention to basic functionalities of the user interface, as well as to the underlying web technologies involved. Some of the main features of the literacy training module are described, such as grapheme-sound correspondence, syllabification-sound relationships, varying tempo of presentation. A particular point is made for using HMM (HTS) synthesis in this case, as it seems to be very appropriate for less resourced languages.
In Knowledge Management, variations in information expressions have proven a real challenge. In particular, classical semantic relations (e.g. synonymy) do not connect words with different parts-of-speech. The method proposed tries to address this issue. It consists in building a derivational resource from a morphological derivation tool together with derivational guidelines from a dictionary in order to store only correct derivatives. This resource, combined with a syntactic parser, a semantic disambiguator and some derivational patterns, helps to reformulate an original sentence while keeping the initial meaning in a convincing manner This approach has been evaluated in three different ways: the precision of the derivatives produced from a lemma; its ability to provide well-formed reformulations from an original sentence, preserving the initial meaning; its impact on the results coping with a real issue, {\textbackslash}textit{ie} a question answering task . The evaluation of this approach through a question answering system shows the pros and cons of this system, while foreshadowing some interesting future developments.
Errors in machine translations of English-Iraqi Arabic dialogues were analyzed at two different points in the systems? development using HTER methods to identify errors and human annotations to refine TER annotations. The analyses were performed on approximately 100 translations into each language from 4 translation systems collected at two annual evaluations. Although the frequencies of errors in the more mature systems were lower, the proportions of error types exhibited little change. Results include high frequencies of pronoun errors in translations to English, high frequencies of subject person inflection in translations to Iraqi Arabic, similar frequencies of word order errors in both translation directions, and very low frequencies of polarity errors. The problems with many errors can be generalized as the need to insert lexemes not present in the source or vice versa, which includes errors in multi-word expressions. Discourse context will be required to resolve some problems with deictic elements like pronouns.
This paper describes a method based on morphological analysis of words for a Persian Part-Of-Speech (POS) tagging system. This is a main part of a process for expanding a large Persian corpus called Peyekare (or Textual Corpus of Persian Language). Peykare is arranged into two parts: annotated and unannotated parts. We use the annotated part in order to create an automatic morphological analyzer, a main segment of the system. Morphosyntactic features of Persian words cause two problems: the number of tags is increased in the corpus (586 tags) and the form of the words is changed. This high number of tags debilitates any taggers to work efficiently. From other side the change of word forms reduces the frequency of words with the same lemma; and the number of words belonging to a specific tag reduces as well. This problem also has a bad effect on statistical taggers. The morphological analyzer by removing the problems helps the tagger to cover a large number of tags in the corpus. Using a Markov tagger the method is evaluated on the corpus. The experiments show the efficiency of the method in Persian POS tagging.
The focus of information retrieval evaluations, such as NISTs TREC evaluations (e.g. Voorhees 2003), is on evaluation of the information content of system responses. On the other hand, retrieval tasks usually involve two different dimensions: reporting relevant information and providing sources of information, including corroborating evidence and alternative documents. Under the DARPA Global Autonomous Language Exploitation (GALE) program, Distillation provides succinct, direct responses to the formatted queries using the outputs of automated transcription and translation technologies. These responses are evaluated in two dimensions: information content, which measures the amount of relevant and non-redundant information, and document support, which measures the number of alternative sources provided in support of reported information. The final metric in the overall GALE distillation evaluation combines the results of scoring of both query responses and document citations. In this paper, we describe our evaluation framework with emphasis on the scoring of document citations and an analysis of how systems perform at providing sources of information.
This paper presents TRmorph, a two-level morphological analyzer for Turkish. TRmorph is a fairly complete and accurate morphological analyzer for Turkish. However, strength of TRmorph is neither in its performance, nor in its novelty. The main feature of this analyzer is its availability. It has completely been implemented using freely available tools and resources, and the two-level description is also distributed with a license that allows others to use and modify it freely for different applications. To our knowledge, TRmorph is the first freely available morphological analyzer for Turkish. This makes TRmorph particularly suitable for applications where the analyzer has to be changed in some way, or as a starting point for morphological analyzers for similar languages. TRmorph's specification of Turkish morphology is relatively complete, and it is distributed with a large lexicon. Along with the description of how the analyzer is implemented, this paper provides an evaluation of the analyzer on two large corpora.
This paper describes our efforts to build a multilingual heritage corpus of alpine texts. Currently we digitize the yearbooks of the Swiss Alpine Club which contain articles in French, German, Italian and Romansch. Articles comprise mountaineering reports from all corners of the earth, but also scientific topics such as topography, geology or glacierology as well as occasional poetry and lyrics. We have already scanned close to 70,000 pages which has resulted in a corpus of 25 million words, 10{\%} of which is a parallel French-German corpus. We have solved a number of challenges in automatic language identification and text structure recognition. Our next goal is to identify the great variety of toponyms (e.g. names of mountains and valleys, glaciers and rivers, trails and cabins) in this corpus, and we sketch how a large gazetteer of Swiss topographical names can be exploited for this purpose. Despite the size of the resource, exact matching leads to a low recall because of spelling variations, language mixtures and partial repetitions.
In this paper we describe the development of a schema for the annotation of attribution relations and present the first findings and some relevant issues concerning this phenomenon. Following the D-LTAG approach to discourse, we have developed a lexically anchored description of attribution, considering this relation, contrary to the approach in the PDTB, independently from other discourse relations. This approach has allowed us to deal with the phenomenon in a broader perspective than previous studies, reaching therefore a more accurate description of it and making it possible to raise some still unaddressed issues. Following this analysis, we propose an annotation schema and discuss the first results concerning its applicability. The schema has been applied to a pilot portion of the ISST corpus of Italian and represents the initial phase of a project aiming at the creation of an Italian Discourse Treebank. We believe this work will raise some awareness concerning the fundamental importance of attribution relations. The identification of the source has in fact strong implications for the attributed material. Moreover, it will make overt the complexity of a phenomenon for long underestimated.
Evaluation of complex, collaborative dialogue systems is a difficult task. Traditionally, developers have relied upon subjective feedback from the user, and parametrisation over observable metrics. However, both models place some reliance on the notion of a task; that is, the system is helping to user achieve some clearly defined goal, such as book a flight or complete a banking transaction. It is not clear that such metrics are as useful when dealing with a system that has a more complex task, or even no definable task at all, beyond maintain and performing a collaborative dialogue. Working within the EU funded COMPANIONS program, we investigate the use of appropriateness as a measure of conversation quality, the hypothesis being that good companions need to be good conversational partners . We report initial work in the direction of annotating dialogue for indicators of good conversation, including the annotation and comparison of the output of two generations of the same dialogue system.
We describe a syntactically annotated parallel corpus containing typologically partly different languages, namely English, Swedish and Turkish. The corpus consists of approximately 300 000 tokens in Swedish, 160 000 in Turkish and 150 000 in English, containing both fiction and technical documents. We build the corpus by using the Uplug toolkit for automatic structural markup, such as tokenization and sentence segmentation, as well as sentence and word alignment. In addition, we use basic language resource kits for the linguistic analysis of the languages involved. The annotation is carried on various layers from morphological and part of speech analysis to dependency structures. The tools used for linguistic annotation, e.g.,{\textbackslash} HunPos tagger and MaltParser, are freely available data-driven resources, trained on existing corpora and treebanks for each language. The parallel treebank is used in teaching and linguistic research to study the relationship between the structurally different languages. In order to study the treebank, several tools have been developed for the visualization of the annotation and alignment, allowing search for linguistic patterns.
Although the Semantic web is steadily gaining in popularity, it remains a mystery to a large percentage of Internet users. This can be attributed to the complexity of the technologies that form its core. Creating intuitive interfaces which completely abstract the technologies underneath, is one way to solve this problem. A contrasting approach is to ease the user into understanding the technologies. We propose a solution which anchors on using controlled languages as interfaces to semantic web applications. This paper describes one such approach for the domain of meeting minutes, status reports and other project specific documents. A controlled language is developed along with an ontology to handle semi-automatic knowledge extraction. The contributions of this paper include an ontology designed for the domain of meeting minutes and status reports, and a controlled language grammar tailored for the above domain to perform the semi-automatic knowledge acquisition and generate RDF triples. This paper also describes two grammar prototypes, which were developed and evaluated prior to the development of the final grammar, as well as the Link grammar, which was the grammar formalism of choice.
The linguistic resources presented in this paper are designed for the recognition and semantic tagging of calendar expressions in French. While existing resources generally put the emphasis on describing calendar bases pointed out by calendar expressions (which are considered as named entities), our approach tries to explicit how references to calendar are linguistically built up, taking into account not only the calendar bases but as well the prepositions and units that operate on them, as they provide valuable information on how texts refer to the calendar. The modelling of these expressions led us to consider calendar expressions as a conjunction of operators interacting with temporal references. Though the resources aim to be generic and easily reusable, we illustrate the interest of our approach by using the resources output to feed a text navigation tool that is currently being improved, in order to offer users a way of temporally progressing or navigating in texts.
This paper describes the Alborada-I3A corpus of disordered speech, acquired during the recent years for the research in different speech technologies for the handicapped like Automatic Speech Recognition or pronunciation assessment. It contains more than 2 hours of speech from 14 young impaired speakers and nearly 9 hours from 232 unimpaired age-matched peers whose collaboration was possible by the joint work with different educational and assistive institutions. Furthermore, some extra resources are provided with the corpus, including the results of a perceptual human-based labeling of the lexical mispronunciations made by the impaired speakers. The corpus has been used to achieve results in different tasks like analyses on the speech production in impaired children, acoustic and lexical adaptation for ASR and studies on the speech proficiency of the impaired speakers. Finally, the full corpus is freely available for the research community with the only restrictions of maintaining all its data and resources for research purposes only and keeping the privacy of the speakers and their speech data
In this paper, we present the main features of a text mining based search engine for the UK Educational Evidence Portal available at the UK National Centre for Text Mining (NaCTeM), together with a user-centred framework for the evaluation of the search engine. The framework is adapted from an existing proposal by the ISLE (EAGLES) Evaluation Working group. We introduce the metrics employed for the evaluation, and explain how these relate to the text mining based search engine. Following this, we describe how we applied the framework to the evaluation of a number of key text mining features of the search engine, namely the automatic clustering of search results, classification of search results according to a taxonomy, and identification of topics and other documents that are related to a chosen document. Finally, we present the results of the evaluation in terms of the strengths, weaknesses and improvements identified for each of these features.
One of the methods that has been proposed for dealing with real-word errors (errors that occur when a correctly spelled word is substituted for the one intended) is the ''``confusion-set'''' approach - a confusion set being a small group of words that are likely to be confused with one another. Using a list of confusion sets drawn up in advance, a spellchecker, on finding one of these words in a text, can assess whether one of the other members of its set would be a better fit and, if it appears to be so, propose that word as a correction. Much of the research using this approach has suffered from two weaknesses. The first is the small number of confusion sets used. The second is that systems have largely been tested on artificial errors. In this paper we address these two weaknesses. We describe the creation of a realistically sized list of confusion sets, then the assembling of a corpus of real-word errors, and then we assess the potential of that list in relation to that corpus.
We present Witchcraft, an open-source framework for the evaluation of prediction models for spoken dialogue systems based on interaction logs and audio recordings. The use of Witchcraft is two fold: first, it provides an adaptable user interface to easily manage and browse thousands of logged dialogues (e.g. calls). Second, with help of the underlying models and the connected machine learning framework RapidMiner the workbench is able to display at each dialogue turn the probability of the task being completed based on the dialogue history. It estimates the emotional state, gender and age of the user. While browsing through a logged conversation, the user can directly observe the prediction result of the models at each dialogue step. By that, Witchcraft allows for spotting problematic dialogue situations and demonstrates where the current system and the prediction models have design flaws. Witchcraft will be made publically available to the community and will be deployed as open-source project.
We describe the construction of the CODA corpus, a parallel corpus of monologues and expository dialogues. The dialogue part of the corpus consists of expository, i.e., information-delivering rather than dramatic, dialogues written by several acclaimed authors. The monologue part of the corpus is a paraphrase in monologue form of these dialogues by a human annotator. The annotator-written monologue preserves all information present in the original dialogue and does not introduce any new information that is not present in the original dialogue. The corpus was constructed as a resource for extracting rules for automated generation of dialogue from monologue. Using authored dialogues allows us to analyse the techniques used by accomplished writers for presenting information in the form of dialogue. The dialogues are annotated with dialogue acts and the monologues with rhetorical structure. We developed annotation and translation guidelines together with a custom-developed tool for carrying out translation, alignment and annotation of the dialogues. The final parallel CODA corpus consists of 1000 dialogue turns that are tagged with dialogue acts and aligned with monologue that expresses the same information and has been annotated with rhetorical structure relations.
Proper annotation process management is crucial to the construction of corpora, which are in turn indispensable to the data-driven techniques that have come to the forefront in NLP during the last two decades. It is still common to see ad-hoc tools created for a specific annotation project, but it is time this changed; creation of such tools is labor and time expensive, and is secondary to corpus creation. In addition, such tools likely lack proper annotation process management, increasingly more important as corpora sizes grow in size and complexity. This paper first raises a list of ten needs that any general purpose annotation system should address moving forward, such as user {\&} role management, delegation {\&} monitoring of work, diffing {\&} merging annotators work, versioning of corpora, multilingual support, import/export format flexibility, and so on. A framework to address these needs is then proposed, and how having proper annotation process management can be beneficial to the creation and maintenance of corpora explained. The paper then introduces SLATE (Segment and Link-based Annotation Tool Enhanced), the second iteration of a web-based annotation tool, which is being rewritten to implement the proposed framework.
A novel method to automatically associate ontological concepts to their realisations in texts is presented. The method has been developed in the context of the Papyrus project to annotate texts and audio transcripts with a set of relevant concepts from the Papyrus News Ontology. To avoid strong dependency on a specific ontology, the annotation process starts by performing a Wikipedia-based annotation of news items: the most relevant keywords are detected and the Wikipedia pages that best describe their actual meaning are identified. In a later step this annotation is translated into an Ontology-based one: keywords are connected to the most appropriate ontology classes on the basis of a relatedness measure that relies on Wikipedia knowledge. Wikipedia-annotation provides a domain independent abstraction layer that simplify the adaptation of the approach to other domains and ontologies. Evaluation has been performed on a set of manually annotated news, resulting in 58{\%} F1 score for relevant Wikipedia pages and 64{\%} for relevant ontology concepts identification.
With a view to rationalise the evaluation process within the Orange Labs spoken dialogue system projects, a field audit has been realised among the various related professionals. The article presents the study's main conclusions and draws work perspectives to enhance the evaluation process in such a complex organisation. We first present the typical spoken dialogue system project lifecycle and the involved communities of stakeholders. We then sketch a map of indicators used across the teams. It shows that each professional category designs its evaluation metrics according to a case-by-case strategy, each one targeting different goals and methodologies. And last, we identify weaknesses in the evaluation process is handled by the various teams. Among others, we mention: the dependency on the design and exploitation tools that may not be suitable for an adequate collection of relevant indicators, the need to refine some indicators' definition and analysis to obtain valuable information for system enhancement, the sharing issue that advocates for a common definition of indicators across the teams and, as a consequence, the need for shared applications that support and encourage such a rationalisation.
Recently, the credibility of information on the Web has become an important issue. In addition to telling about content of source documents, indicating how to interpret the content, especially showing interpretation of the relation between statements appeared to contradict each other, is important for helping a user judge the credibility of information. In this paper, we will describe the purpose and the way in the construction of a text summarization corpus. Our purpose in the construction of the corpus includes the following three points; to collect Web documents relevant to several query sentences, to prepare gold standard data to evaluate smaller sub-processes in the extraction process and the summary generation process, to investigate the summaries made by human summarizers. The constructed corpus contains six query sentences, 24 manually-constructed summaries, and 24 collections of source Web documents. We also investigated how the descriptions of interpretation, which help a user judge the credibility of other descriptions in the summary, appear in the corpus. As a result, we confirmed that showing interpretation on conflicts is important for helping a user judge the credibility of information.
In this paper we present LX-Parser, a probabilistic, robust constituency parser for Portuguese. This parser achieves ca. 88{\%} f-score in the labeled bracketing task, thus reaching a state-of-the-art performance score that is in line with those that are currently obtained by top-ranking parsers for English, the most studied natural language. To the best of our knowledge, LX-Parser is the first state-of-the-art, robust constituency parser for Portuguese that is made freely available. This parser is being distributed in a variety of ways, each suited for a different type of usage. More specifically, LX-Parser is being made available (i) as a downloadable, stand-alone parsing tool that can be run locally by its users; (ii) as a Web service that exposes an interface that can be invoked remotely and transparently by client applications; and finally (iii) as an on-line parsing service, aimed at human users, that can be accessed through any common Web browser.
This paper is concerned with resources for controlled languages for alert messages and protocols in the European perspective. These resources have been produced as the outcome of a project (Alert Messages and Protocols: MESSAGE) which has been funded with the support of the European Commission - Directorate-General Justice, Freedom and Security, and with the specific objective of 'promoting and supporting the development of security standards, and an exchange of know-how and experience on protection of people'. The MESSAGE project involved the development and transfer of a methodology for writing safe and safely translatable alert messages and protocols created by Centre Tesni{\`e}re in collaboration with the aircraft industry, the health profession, and emergency services by means of a consortium of four partners to their four European member states in their languages (ES, FR (Coordinator), GB, PL). The paper describes alert messages and protocols, controlled languages for safety and security, the target groups involved, controlled language evaluation, dissemination, the resources that are available, both Freely available and From Owner, together with illustrations of the resources, and the potential transferability to other sectors and users.
The paper presents the fourth, ``Mondilex'' edition of the MULTEXT-East language resources, a multilingual dataset for language engineering research and development, focused on the morphosyntactic level of linguistic description. This standardised and linked set of resources covers a large number of mainly Central and Eastern European languages and includes the EAGLES-based morphosyntactic specifications; morphosyntactic lexica; and annotated parallel, comparable, and speech corpora. The fourth release of these resources introduces XML-encoded morphosyntactic specifications and adds six new languages, bringing the total to 16: to Bulgarian, Croatian, Czech, Estonian, English, Hungarian, Romanian, Serbian, Slovene, and the Resian dialect of Slovene it adds Macedonian, Persian, Polish, Russian, Slovak, and Ukrainian. This dataset, unique in terms of languages covered and the wealth of encoding, is extensively documented, and freely available for research purposes at http://nl.ijs.si/ME/V4/.
The JOS language resources are meant to facilitate developments of HLT and corpus linguistics for the Slovene language and consist of the morphosyntactic specifications, defining the Slovene morphosyntactic features and tagset; two annotated corpora (jos100k and jos1M); and two web services (a concordancer and text annotation tool). The paper introduces these components, and concentrates on jos100k, a 100,000 word sampled balanced monolingual Slovene corpus, manually annotated for three levels of linguistic description. On the morphosyntactic level, each word is annotated with its morphosyntactic description and lemma; on the syntactic level the sentences are annotated with dependency links; on the semantic level, all the occurrences of 100 top nouns in the corpus are annotated with their wordnet synset from the Slovene semantic lexicon sloWNet. The JOS corpora and specifications have a standardised encoding (Text Encoding Initiative Guidelines TEI P5) and are available for research from http://nl.ijs.si/jos/ under the Creative Commons licence.
In this paper, we discuss our analysis and resulting new annotations of Penn Discourse Treebank (PDTB) data tagged as Concession. Concession arises whenever one of the two arguments creates an expectation, and the other ones denies it. In Natural Languages, typical discourse connectives conveying Concession are 'but', 'although', 'nevertheless', etc. Extending previous theoretical accounts, our corpus analysis reveals that concessive interpretations are due to different sources of expectation, each giving rise to critical inferences about the relationship of the involved eventualities. We identify four different sources of expectation: Causality, Implication, Correlation, and Implicature. The reliability of these categories is supported by a high inter-annotator agreement score, computed over a sample of one thousand tokens of explicit connectives annotated as Concession in PDTB. Following earlier work of (Hobbs, 1998) and (Davidson, 1967) notion of reification, we extend the logical account of Concession originally proposed in (Robaldo et al., 2008) to provide refined formal descriptions for the first three mentioned sources of expectations in Concessive relations.
The paper presents an innovative approach to extract Slovene definition candidates from domain-specific corpora using morphosyntactic patterns, automatic terminology recognition and semantic tagging with wordnet senses. First, a classification model was trained on examples from Slovene Wikipedia which was then used to find well-formed definitions among the extracted candidates. The results of the experiment are encouraging, with accuracy ranging from 67{\%} to 71{\%}. The paper also addresses some drawbacks of the approach and suggests ways to overcome them in future work.
We describe a new method for sentiment load annotation of the synsets of a wordnet, along the principles of Osgoods Semantic Differential theory and extending the Kamp and Marx calculus, by taking into account not only the WordNet structure but also the SUMO/MILO (Niles {\&} Pease, 2001) and DOMAINS (Bentivogli et al., 2004) knowledge sources. We discuss the method to annotate all the synsets in PWN2.0, irrespective of their part of speech. As the number of possible factors (semantic oppositions, along which the synsets are ranked) is very large, we developed also an application allowing the text analyst to select the most discriminating factors for the type of text to be analyzed. Once the factors have been selected, the underlying wordnet is marked-up on the fly and it can be used for the intended textual analysis. We anticipate that these annotations can be imported in other language wordnets, provided they are aligned to PWN2.0. The method for the synsets annotation generalizes the usual subjectivity mark-up (positive, negative and objective) according to a user-based multi-criteria differential semantics model.
The paper introduces the Multimodal Russian Corpus (MURCO), which has been created in the framework of the Russian National Corpus (RNC). The MURCO provides the users with the great amount of phonetic, orthoepic, intonational information related to Russian. Moreover, the deeply annotated part of the MURCO contains the data concerning Russian gesticulation, speech act system, types of vocal gestures and interjections in Russian, and so on. The Corpus is on free access. The paper describes the main types of annotation and the interface structure of the MURCO. The MURCO consists of two parts, the second part being the subset of the first: 1) the whole Corpus, which is annotated from the lexical (lemmatization), morphological, semantic, accentological, metatextual, socioligical point of view (these types of annotation are standard for the RNC), and also from the point of view of phonetics (the orthoepic annotation and the mark-up of accentological word structure), 2) the deeply annotated MURCO, which is annotated in addition from the point of view of gesticulation and speech act structure.
In this paper we present an experimental toolbox for automatic tree-to-tree alignment based on a binary classification model. The aligner implements a recurrent architecture for structural prediction using history features and a sequential classification procedure. The discriminative base classifier uses a log-linear model in the current setup which enables simple integration of various features extracted from the data. The Lingua-Align toolbox provides a flexible framework for feature extraction including contextual properties and implements several alignment inference procedures. Various settings and constraints can be controlled via a simple frontend or called from external scripts. Lingua-Align supports different treebank formats and includes additional tools for conversion and evaluation. In our experiments we can show that our tree aligner produces results with high quality and outperforms unsupervised techniques proposed otherwise. It also integrates well with another existing tool for manual tree alignment which makes it possible to quickly integrate additional training material and to run semi-automatic alignment strategies.
The annotation of causal relations in natural language texts can lead to a low inter-annotator agreement. A French corpus annotated with causal relations would be helpful for the evaluation of programs that extract causal knowledge, as well as for the study of the expression of causation. As previous theoretical work provides no necessary and sufficient condition that would allow an annotator to easily identify causation, we explore features that are associated with causation in human judgements. We present an experiment that allows us to elicit intuitive features of causation. We test the statistical association of features of causation from theoretical previous work with causation itself in human judgements in an annotation experiment. We then establish guidelines based on these features for annotating a French corpus. We argue that our approach leads to coherent annotation guidelines, since it allows us to obtain a κ = 0.84 agreement between the majority of the annotators answers and our own educated judgements. We present these annotation instructions in detail.
A research has been carried on and is still in progress aimed at the construction of three specialized lexicons organized as databases of relational type. The three databases contain terms belonging to the specialized knowledge fields of maritime terminology (technical-nautical and maritime transport domain), taxation law, and labour law with union labour rules, respectively. The EuroWordNet/ItalWordNet model was firstly used to structure the terminological database of maritime domain. The methodology experimented for its construction was applied to construct the next databases. It consists in i) the management of corpora of specialized languages and ii) the use of generic databases to identify and extract a set of candidate terms to be codified in the terminological databases. The three specialized resources are described highlighting the various kinds of lexical semantic relations linking each term to the others within the single terminological database and to the generic resources WordNet and ItalWordNet. The construction of these specialized lexicons was carried on in the framework of different projects; but they can be seen as a first nucleus of an organized network of generic and specialized lexicons with the purpose of making the meaning of each term clearer from a cognitive point of view.
Existing approaches to classifying documents by sentiment include machine learning with features created from n-grams and part of speech. This paper explores a different approach and examines performance of one selected machine learning algorithm, Support Vector Machines, with features computed using existing lexical resources. Special attention has been paid to fine tuning of the algorithm regarding number of features. The immediate purpose of this experiment is to evaluate lexical and sentiment resources in document-level sentiment classification task. Results described in the paper are also useful to indicate how lexicon design, different language dimensions and semantic categories contribute to document-level sentiment recognition. In a less direct way (through the examination of evaluated resources), the experiment analyzes adequacy of lexemes, word senses and synsets as different possible layers for ascribing sentiment, or as candidates for sentiment carriers. The proposed approach of machine learning word category frequencies instead of n-grams and part of speech features can potentially exhibit improvements in domain independency, but this hypothesis has to be verified in future works.
We present a new morphological processor for Biscayan, a dialect of Basque, developed on the description of the morphology of standard Basque. The database for the standard morphology has been extended for dialects and an open-source tool for morphological description named foma is used for building the processor. Biscayan is a dialect of the Basque language spoken mainly in Biscay, a province on the western of the Basque Country. The description of the lexicon and the morphotactics (or word grammar) for the standard Basque was carried out using a relational database and the database has been extended in order to include dialectal variants linked to the standard entries. XuxenB, a spelling checker/corrector for this dialect, is the first application of this work. Additionally to the basic analyzer used for spelling, a new transducer is included. It is an enhanced analyzer for linking standard form with the corresponding standard ones. It is used in correction for generation of proposals when in the input text appear standard forms which we want to replace with dialectal forms.
The aim of the paper is to present recent ― as of March 2010 ― developments in the construction of the National Corpus of Polish (NKJP). The NKJP project was launched at the very end of 2007 and it is aimed at compiling a large, linguistically annotated corpus of contemporary Polish by the end of 2010. Out of the total pool of 1 billion words of text data collected in the project, a 300 million word balanced corpus will be selected to match a set of predefined representativeness criteria. This present paper outlines a number of recent developments in the NKJP project, including: 1) the design of text encoding XML schemata for various levels of linguistic information, 2) a new tool for manual annotation at various levels, 3) numerous improvements in search tools. As the work on NKJP progresses, it becomes clear that this project serves as an important testbed for linguistic annotation and interoperability standards. We believe that our recent experiences will prove relevant to future large-scale language resource compilation efforts.
Corpora of sentences annotated with grammatical information have been deployed by extending the basic lexical and morphological data with increasingly complex information, such as phrase constituency, syntactic functions, semantic roles, etc. As these corpora grow in size and the linguistic information to be encoded reaches higher levels of sophistication, the utilization of annotation tools and, above all, supporting computational grammars appear no longer as a matter of convenience but of necessity. In this paper, we report on the design features, the development conditions and the methodological options of a deep linguistic databank, the CINTIL DeepGramBank. In this corpus, sentences are annotated with fully fledged linguistically informed grammatical representations that are produced by a deep linguistic processing grammar, thus consistently integrating morphological, syntactic and semantic information. We also report on how such corpus permits to straightforwardly obtain a whole range of past generation annotated corpora (POS, NER and morphology), current generation treebanks (constituency treebanks, dependency banks, propbanks) and next generation databanks (logical form banks) simply by means of a very residual selection/extraction effort to get the appropriate ''``views'''' exposing the relevant layers of information.
We present our ongoing work on language technology-based e-science in the humanities, social sciences and education, with a focus on text-based research in the historical sciences. An important aspect of language technology is the research infrastructure known by the acronym BLARK (Basic LAnguage Resource Kit). A BLARK as normally presented in the literature arguably reflects a modern standard language, which is topic- and genre-neutral, thus abstracting away from all kinds of language variation. We argue that this notion could fruitfully be extended along any of the three axes implicit in this characterization (the social, the topical and the temporal), in our case the temporal axis, towards a diachronic BLARK for Swedish, which can be used to develop e-science tools in support of historical studies.
We present a new reference Grammar of French (La Grande Grammaire du fran{\c{c}}ais), which is a collective project (gathering around fifty contributors), producing a book (about 2200 pages, to be published en 2011) and associated databases. Like the recent reference grammars of the other Romance Languages, it takes into account the important results of the linguistic research of the past thrity years, while aiming at a non specialist audience and avoiding formalization. We differ from existing French grammar by being focused on contemporary French from a purely descriptive point of view, and by taking spoken data into account. We include a description of all the syntactic phenomena, as well as lexical, semantic, pragmatic and prosodic insights, specially as they interact with syntax. The analysis concerns the data from contemporary written French, but also includes data from spoken corpora and regional or non standard French (when accessible). Throughout the grammar, a simple phrase structure grammar is used, in order to maintain a common representation. The analyses are modular with a strict division of labor between morphology, syntax and semantics. From the syntactic point of view, POS are also distinguished from grammatical relations (or functions). The databases include a terminological glossary, different lexical databases for certain POS, certain valence frames and certain semantic classes, and a bibliographical database.
We developed a search tool for ngrams extracted from a very large corpus (the current system uses the entire Wikipedia, which has 1.7 billion tokens). The tool supports queries with an arbitrary number of wildcards and/or specification by a combination of token, POS, chunk (such as NP, VP, PP) and Named Entity (NE). The previous system (Sekine 08) can only handle tokens and unrestricted wildcards in the query, such as * was established in *. However, being able to constrain the wildcards by POS, chunk or NE is quite useful to filter out noise. For example, the new system can search for NE=COMPANY was established in POS=CD. This finer specification reduces the number of outputs to less than half and avoids the ngrams which have a comma or a common noun at the first position or location information at the last position. It outputs the matched ngrams with their frequencies as well as all the contexts (i.e. sentences, KWIC lists and document ID information) where the matched ngrams occur in the corpus. It takes a fraction of a second for a search on a single CPU Linux-PC (1GB memory and 500GB disk) environment.
This paper addresses the recognition of elderly callers based on short and narrow-band utterances, which are typical for Interactive Voice Response (IVR) systems. Our study is based on 2308 short utterances from a deployed IVR application. We show that features such as speaking rate, jitter and shimmer that are considered as most meaningful ones for determining elderly users underperform when used in the IVR context while pitch and intensity features seem to gain importance. We further demonstrate the influence of the utterance length on the classifiers performance: for both humans and classifier, the distinction between aged and non-aged voices becomes increasingly difficult the shorter the utterances get. Our setup based on a Support Vector Machine (SVM) with linear kernel reaches a comparably poor performance of 58{\%} accuracy, which can be attributed to an average utterance length of only 1.6 seconds. The automatic distinction between aged and non-aged utterances drops to random when the utterance length falls below 1.2 seconds.
The task of coreference resolution requires people or systems to decide when two referring expressions refer to the 'same' entity or event. In real text, this is often a difficult decision because identity is never adequately defined, leading to contradictory treatment of cases in previous work. This paper introduces the concept of 'near-identity', a middle ground category between identity and non-identity, to handle such cases systematically. We present a typology of Near-Identity Relations (NIDENT) that includes fifteen types―grouped under four main families―that capture a wide range of ways in which (near-)coreference relations hold between discourse entities. We validate the theoretical model by annotating a small sample of real data and showing that inter-annotator agreement is high enough for stability (K=0.58, and up to K=0.65 and K=0.84 when leaving out one and two outliers, respectively). This work enables subsequent creation of the first internally consistent language resource of this type through larger annotation efforts.
This paper reports on the annotation of a corpus of 1 million words with four semantic annotation layers, including named entities, co- reference relations, semantic roles and spatial and temporal expressions. These semantic annotation layers can benefit from the manually verified part of speech tagging, lemmatization and syntactic analysis (dependency tree) information layers which resulted from an earlier project (Van Noord et al., 2006) and will thus result in a deeply syntactically and semantically annotated corpus. This annotation effort is carried out in the framework of a larger project which aims at the collection of a 500-million word corpus of contemporary Dutch, covering the variants used in the Netherlands and Flanders, the Dutch speaking part of Belgium. All the annotation schemes used were (co-)developed by the authors within the Flemish-Dutch STEVIN-programme as no previous schemes for Dutch were available. They were created taking into account standards (either de facto or official (like ISO)) used elsewhere.
We describe our computer-supported framework to overcome the rule of metadata schism. It combines the use of controlled vocabularies, managed by a data category registry, with a component-based approach, where the categories can be combined to yield complex metadata structures. A metadata scheme devised in this way will thus be grounded in its use of categories. Schema designers will profit from existing prefabricated larger building blocks, motivating re-use at a larger scale. The common base of any two metadata schemes within this framework will solve, at least to a good extent, the semantic interoperability problem, and consequently, further promote systematic use of metadata for existing resources and tools to be shared.
Many techniques are developed to derive automatically lexical resources for opinion mining. In this paper we present a gold standard for Dutch adjectives developed for the evaluation of these techniques. In the first part of the paper we introduce our annotation guidelines. They are based upon guidelines recently developed for English which annotate subjectivity and polarity at word sense level. In addition to subjectivity and polarity we propose a third annotation category: that of the attitude holder. The identity of the attitude holder is partly implied by the word itself and may provide useful information for opinion mining systems. In the second part of paper we present the criteria adopted for the selection of items which should be included in this gold standard. Our design is aimed at an equal representation of all dimensions of the lexicon , like frequency and polysemy, in order to create a gold standard which can be used not only for benchmarking purposes but also may help to improve in a systematic way, the methods which derive the word lists. Finally we present the results of the annotation task including annotator agreement rates and disagreement analysis.
This paper compares several indexing methods for person names extracted from text, developed for an information retrieval system with requirements for fast approximate matching of noisy and multicultural Romanized names. Such matching algorithms are computationally expensive and unacceptably slow when used without an indexing or blocking step. The goal is to create a small candidate pool containing all the true matches that can be exhaustively searched by a more effective but slower name comparison method. In addition to dramatically faster search, some of the methods evaluated here led to modest gains in effectiveness by eliminating false positives. Four indexing techniques using either phonetic keys or substrings of name segments, with and without name segment stopword lists, were combined with three name matching algorithms. On a test set of 700 queries run against 70K noisy and multicultural names, the best-performing technique took just 2.1{\%} as long as a naive exhaustive search and increased F1 by 3 points, showing that an appropriate indexing technique can increase both speed and effectiveness.
This paper proposes the method to detect peculiar examples of the target word from a corpus. In this paper we regard following examples as peculiar examples: (1) a meaning of the target word in the example is new, (2) a compound word consisting of the target word in the example is new or very technical. The peculiar example is regarded as an outlier in the given example set. Therefore we can apply many methods proposed in the data mining domain to our task. In this paper, we propose the method to combine the density based method, Local Outlier Factor (LOF), and One Class SVM, which are representative outlier detection methods in the data mining domain. In the experiment, we use the Whitepaper text in BCCWJ as the corpus, and 10 noun words as target words. Our method improved precision and recall of LOF and One Class SVM. And we show that our method can detect new meanings by using the noun `midori (green)'. The main reason of un-detections and wrong detection is that similarity measure of two examples is inadequacy. In future, we must improve it.
TimeBank (Pustejovsky et al, 2003a), a reference for TimeML (Pustejovsky et al, 2003b) compliant annotation, is widely used temporally annotated corpus in the community. It captures time expressions, events, and relations between events and event and temporal expression; but there is room for improvements in this hand-annotated widely used TimeBank corpus. This work is one such effort to extend the TimeBank corpus. Our first goal is to suggest missing TimeBank events and temporal expressions, i.e. events and temporal expressions that were missed by TimeBank annotators. Along with that this paper also suggests some additions to TimeML language by adding new event features (ontology type), some more SLINKs and also relations between events with their arguments, which we call RLINK (relation link). With our new suggestions we present the TRIOS-TimeBank corpus, an extended TimeBank corpus. We conclude by suggesting our future work to clean the TimeBank corpus even more and automatically generating larger temporally annotated corpus for the community.
We present the problem of categorizing web services according to a shallow ontology for presentation on a specialist portal, using their WSDL and associated textual documents found by a crawler. We treat this as a text classification problem and apply first information extraction (IE) techniques (voting using keywords weight according to their context), then machine learning (ML), and finally a combined approach in which ML has priority over weighted keywords, but the latter can still make up categorizations for services for which ML does not produce enough. We evaluate the techniques (using data manually annotated through the portal, which we also use as the training data for ML) according to standard IE measures for flat categorization as well as the Balanced Distance Metric (more suitable for ontological classification) and compare them with related work in web service categorization. The ML and combined categorization results are good and the system is designed to take users' contributions through the portal's Web 2.0 features as additional training data.
To solve the unknown morpheme problem in Japanese morphological analysis, we previously proposed a novel framework of online unknown morpheme acquisition and its implementation. This framework poses a previously unexplored problem, online unknown morpheme detection. Online unknown morpheme detection is a task of finding morphemes in each sentence that are not listed in a given lexicon. Unlike in English, it is a non-trivial task because Japanese does not delimit words by white space. We first present a baseline method that simply uses the output of the morphological analyzer. We then show that it fails to detect some unknown morphemes because they are over-segmented into shorter registered morphemes. To cope with this problem, we present a simple solution, the use of orthographic variation of Japanese. Under the assumption that orthographic variants behave similarly, each over-segmentation candidate is checked against its counterparts. Experiments show that the proposed method improves the recall of detection and contributes to improving unknown morpheme acquisition.
We present a corpus of transcribed spoken Hebrew that forms an integral part of a comprehensive data system that has been developed to suit the specific needs and interests of child language researchers: CHILDES (Child Language Data Exchange System). We introduce a dedicated transcription scheme for the spoken Hebrew data that is aware both of the phonology and of the standard orthography of the language. We also introduce a morphological analyzer that was specifically developed for this corpus.
This paper concerns non-verbal communication, and describes especially the use of eye-gaze to signal turn-taking and feedback in conversational settings. Eye-gaze supports smooth interaction by providing signals that the interlocutors interpret with respect to such conversational functions as taking turns and giving feedback. New possibilities to study the effect of eye-gaze on the interlocutors communicative behaviour have appeared with the eye-tracking technology which in the past years has matured to the level where its use to study naturally occurring dialogues have become easier and more reliable to conduct. It enables the tracking of eye-fixations and gaze-paths, and thus allows analysis of the persons turn-taking and feedback behaviour through the analysis of their focus of attention. In this paper, experiments on the interlocutors non-verbal communication in conversational settings using the eye-tracker are reported, and results of classifying turn-taking using eye-gaze and gesture information are presented. Also the hybrid method that combines signal level analysis with human interpretation is discussed.
Automatic language recognition on spontaneous speech has experienced a rapid development in the last few years. This development has been in part due to the competitive technological Language Recognition Evaluations (LRE) organized by the National Institute of Standards and Technology (NIST). Until now, the need to have clearly defined and consistent evaluations has kept some real-life application issues out of these evaluations. In particular, all past NIST LREs have used exclusively conversational telephone speech (CTS) for development and test. Fortunately this has changed in the current NIST LRE since it includes also broadcast speech. However, for testing only the telephone speech found in broadcast data will be used. In real-life applications, there could be several more types of speech and systems could be forced to use a mix of different types of data for training and development and recognition. In this article, we have defined a test-bed including several types of speech data and have analyzed how a typical language recognition system works using different types of speech, and also a combination of different types of speech, for training and testing.
This paper describes a system for linking the thesaurus of the Netherlands Institute for Sound and Vision to English WordNet and dbpedia. The thesaurus contains subject (concept) terms, and names of persons, locations, and miscalleneous names. We used EuroWordNet, a multilingual wordnet, and Dutch Wikipedia as intermediaries for the two alignments. EuroWordNet covers most of the subject terms in the thesaurus, but the organization of the cross-lingual links makes selection of the most appropriate English target term almost impossible. Precision and recall of the automatic alignment with WordNet for subject terms is 0.59. Using page titles, redirects, disambiguation pages, and anchor text harvested from Dutch Wikipedia gives reasonable performance on subject terms and geographical terms. Many person and miscalleneous names in the thesaurus could not be located in (Dutch or English) Wikipedia. Precision for miscellaneous names, subjects, persons and locations for the alignment with Wikipedia ranges from 0.63 to 0.94, while recall for subject terms is 0.62.
This paper deals with the problem of finding sign occurrences in a sign language (SL) video. It begins with an analysis of sign models and the way they can take into account the sign variability. Then, we review the most popular technics dedicated to automatic sign language processing and we focus on their adaptation to model sign variability. We present a new method to provide a parametric description of the sign as a set of continuous and discrete parameters. Signs are classified according to there categories (ballistic movements, circles ...), the symmetry between the hand movements, hand absolute and relative locations. Membership grades to sign categories and continuous parameter comparisons can be combined to estimate the similarity between two signs. We set out our system and we evaluate how much time can be saved when looking for a sign in a french sign language video. By now, our formalism only uses hand 2D locations, we finally discuss about the way of integrating other parameters as hand shape or facial expression in our framework.
Previous work has shown that large scale subcategorisation lexicons could be extracted from parsed corpora with reasonably high precision. In this paper, we apply a standard extraction procedure to a 100 millions words parsed corpus of french and obtain rather poor results. We investigate different factors likely to improve performance such as in particular, the specific extraction procedure and the parser used; the size of the input corpus; and the type of frames learned. We try out different ways of interleaving the output of several parsers with the lexicon extraction process and show that none of them improves the results. Conversely, we show that increasing the size of the input corpus and modifying the extraction procedure to better differentiate prepositional arguments from prepositional modifiers improves performance. In conclusion, we suggest that a more sophisticated approach to parser combination and better probabilistic models of the various types of prepositional objects in French are likely ways to get better results.
The creation of language resources for less-resourced languages like the historical ones benefits from the exploitation of language-independent tools and methods developed over the years by many projects for modern languages. Along these lines, a number of treebanks for historical languages started recently to arise, including treebanks for Latin. Among the Latin treebanks, the Index Thomisticus Treebank is a 68,000 token dependency treebank based on the Index Thomisticus by Roberto Busa SJ, which contains the opera omnia of Thomas Aquinas (118 texts) as well as 61 texts by other authors related to Thomas, for a total of approximately 11 million tokens. In this paper, we describe a number of modifications that we applied to the dependency parser DeSR, in order to improve the parsing accuracy rates on the Index Thomisticus Treebank. First, we adapted the parser to the specific processing of Medieval Latin, defining an ad-hoc configuration of its features. Then, in order to improve the accuracy rates provided by DeSR, we applied a revision parsing method and we combined the outputs produced by different algorithms. This allowed us to improve accuracy rates substantially, reaching results that are well beyond the state of the art of parsing for Latin.
In this paper we want to point out some issues arising when a natural language processing task involves several languages (like multi- lingual, multidocument summarization and the machine translation aspects involved) which are often neglected. These issues are of a more cultural nature, and may even come into play when several documents in a single language are involved. We pay special attention to those aspects dealing with the spatiotemporal characteristics of a text. Correct automatic selection of (parts of) texts such as handling the same eventuality, presupposes spatiotemporal disambiguation at a rather specific level. The same holds for the analysis of the query. For generation and translation purposes, spatiotemporal aspects may be relevant as well. At the moment English (both the British and American variants) and Dutch (the Flemish and Dutch variant) are covered, all taking into account the perspective of a contemporary, Flemish user. In our approach the cultural aspects associated with for example the language of publication and the language used by the user play a crucial role.
Natural language processing technology has developed remarkably, but it is still difficult for computers to understand contextual meanings as humans do. The purpose of our work has been to construct an associative concept dictionary for Japanese verbs and make computers understand contextual meanings with a high degree of accuracy. We constructed an automatic system that can be used to estimate elliptical words. We present the result of comparing words that were estimated both by our proposed system (VNACD) and three baseline systems (VACD, NACD, and CF). We then calculated the mean reciprocal rank (MRR), top N accuracy (top 1, top 5, and top 10), and the mean average precision (MAP). Finally, we showed the effectiveness of our method for which both an associative concept dictionary for verbs (Verb-ACD) and one for nouns (Noun-ACD) were used. From the results, we conclude that both the Verb-ACD and the Noun-ACD play a key role in estimating elliptical words.
Machine transliteration is used in a number of NLP applications ranging from machine translation and information retrieval to input mechanisms for non-roman scripts. Many popular Input Method Editors for Indian languages, like Baraha, Akshara, Quillpad etc, use back-transliteration as a mechanism to allow users to input text in a number of Indian language. The lack of a standard dataset to evaluate these systems makes it difficult to make any meaningful comparisons of their relative accuracies. In this paper, we describe the methodology for the creation of a dataset of {\textasciitilde}2500 transliterated sentence pairs each in Bangla, Hindi and Telugu. The data was collected across three different modes from a total of 60 users. We believe that this dataset will prove useful not only for the evaluation and training of back-transliteration systems but also help in the linguistic analysis of the process of transliterating Indian languages from native scripts to Roman.
This paper presents project Nomage, which aims at describing the aspectual properties of deverbal nouns in an empirical way. It is centered on the development of two resources: a semantically annotated corpus of deverbal nouns, and an electronic lexicon. They are both presented in this paper, and emphasize how the semantic annotations of the corpus allow the lexicographic description of deverbal nouns to be validated, in particular their polysemy. Nominalizations have occupied a central place in grammatical analysis, with a focus on morphological and syntactic aspects. More recently, researchers have begun to address a specific issue often neglected before, i.e. the semantics of nominalizations, and its implications for Natural Language Processing applications such as electronic ontologies or Information Retrieval. We focus on precisely this issue in the research project NOMAGE, funded by the French National Research Agency (ANR-07-JCJC-0085-01). In this paper, we present the Nomage corpus and the annotations we make on deverbal nouns (section 2). We then show how we build our lexicon with the semantically annotated corpus and illustrate the kind of generalizations we can make from such data (section 3).
In this paper, we make a qualitative and quantitative analysis of discourse relations within the LUNA conversational spoken dialog corpus. In particular, we first describe the Penn Discourse Treebank (PDTB) and then we detail the adaptation of its annotation scheme to the LUNA corpus of Italian task-oriented dialogs in the domain of software/hardware assistance. We discuss similarities and differences between our approach and the PDTB paradigm and point out the peculiarities of spontaneous dialogs w.r.t. written text, which motivated some changes in the annotation strategy. In particular, we introduced the annotation of relations between non-contiguous arguments and we modified the sense hierarchy in order to take into account the important role of pragmatics in dialogs. In the final part of the paper, we present a comparison between the sense and connective frequency in a representative subset of the LUNA corpus and in the PDTB. Such analysis confirmed the differences between the two corpora and corroborates our choice to introduce dialog-specific adaptations.
We describe a corpus of numerical expressions, developed as part of the NUMGEN project. The corpus contains newspaper articles and scientific papers in which exactly the same numerical facts are presented many times (both within and across texts). Some annotations of numerical facts are original: for example, numbers are automatically classified as round or non-round by an algorithm derived from Jansen and Pollmann (2001); also, numerical hedges such as about or a little under are marked up and classified semantically using arithmetical relations. Through explicit alignment of phrases describing the same fact, the corpus can support research on the influence of various contextual factors (e.g., document position, intended readership) on the way in which numerical facts are expressed. As an example we present results from an investigation showing that when a fact is mentioned more than once in a text, there is a clear tendency for precision to increase from first to subsequent mentions, and for mathematical level either to remain constant or to increase.
In the QA and information retrieval domains progress has been assessed via evaluation campaigns(Clef, Ntcir, Equer, Trec).In these evaluations, the systems handle independent questions and should provide one answer to each question, extracted from textual data, for both open domain and restricted domain. Qu{\ae}ro is a program promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Among the many research areas concerned by Qu{\ae}ro. The Quaero project organized a series of evaluations of Question Answering on Web Data systems in 2008 and 2009. For each language, English and French the full corpus has a size of around 20Gb for 2.5M documents. We describe the task and corpora, and especially the methodologies used in 2008 to construct the test of question and a new one in the 2009 campaign. Six types of questions were addressed, factual, Non-factual(How, Why, What), List, Boolean. A description of the participating systems and the obtained results is provided. We show the difficulty for a question-answering system to work with complex data and questions.
Complex Question Answering is a discipline that involves a deep understanding of question/answer relations, such as those characterizing definition and procedural questions and their answers. To contribute to the improvement of this technology, we deliver two question and answer corpora for complex questions, WEB-QA and TREC-QA, extracted by the same Question Answering system, YourQA, from the Web and from the AQUAINT-6 data collection respectively. We believe that such corpora can be useful resources to address a type of QA that is far from being efficiently solved. WEB-QA and TREC-QA are available in two formats: judgment files and training/testing files. Judgment files contain a ranked list of candidate answers to TREC-10 complex questions, extracted using YourQA as a baseline system and manually labelled according to a Likert scale from 1 (completely incorrect) to 5 (totally correct). Training and testing files contain learning instances compatible with SVM-light; these are useful for experimenting with shallow and complex structural features such as parse trees and semantic role labels. Our experiments with the above corpora have allowed to prove that structured information representation is useful to improve the accuracy of complex QA systems and to re-rank answers.
The Qu{\ae}ro program that promotes research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within its context a set of evaluations of Named Entity recognition systems was held in 2009. Four tasks were defined. The first two concerned traditional named entities in French broadcast news for one (a rerun of ESTER 2) and of OCR-ed old newspapers for the other. The third was a gene and protein name extraction in medical abstracts. The last one was the detection of references in patents. Four different partners participated, giving a total of 16 systems. We provide a synthetic descriptions of all of them classifying them by the main approaches chosen (resource-based, rules-based or statistical), without forgetting the fact that any modern system is at some point hybrid. The metric (the relatively standard Slot Error Rate) and the results are also presented and discussed. Finally, a process is ongoing with preliminary acceptance of the partners to ensure the availability for the community of all the corpora used with the exception of the non-Qu{\ae}ro produced ESTER 2 one.
We propose a Word Sense Disambiguation (WSD) method that accurately classifies ambiguous words to concepts in the Associative Concept Dictionary (ACD) even when the test corpus and the training corpus for WSD are acquired from different domains. Many WSD studies determine the context of the target ambiguous word by analyzing sentences containing the target word. However, they offer poor performance when they are applied to a corpus that differs from the training corpus. One solution is to use associated words that are domain-independently assigned to the concept in ACD; i.e. many users commonly imagine those words against a given concept. Furthermore, by using the associated words of a concept as search queries for a training corpus, our method extracts relevant words, those that are computationally judged to be related to that concept. By checking the frequency of associated words and relevant words that appear near to the target word in a sentence in the test corpus, our method classifies the target word to the concept in ACD. Our evaluation using two different types of corpus demonstrates its good accuracy.
This paper profiles the Europarl part of an English-Swedish parallel corpus and compares it with three other subcorpora of the same parallel corpus. We first describe our method for comparison which is based on manually reviewed word alignments. We investigate relative frequences of different types of correspondence, including null alignments, many-to-one correspondences and crossings. In addition, both halves of the parallel corpus have been annotated with morpho-syntactic information. The syntactic annotation uses labelled dependency relations. Thus, we can see how different types of correspondences are distributed on different parts-of-speech and compute correspondences at the structural level. In spite of the fact that two of the other subcorpora contains fiction, it is found that the Europarl part is the one having the highest proportion of many types of restructurings, including additions, deletions, long distance reorderings and dependency reversals. We explain this by the fact that the majority of Europarl segments are parallel translations rather than source texts and their translations.
In this paper, we present a system for transliterating the Arabic-based script of Urdu to a Roman transliteration scheme. The system is integrated into a larger system consisting of a morphology module, implemented via finite state technologies, and a computational LFG grammar of Urdu that was developed with the grammar development platform XLE (Crouch et al. 2008). Our long-term goal is to handle Hindi alongside Urdu; the two languages are very similar with respect to syntax and lexicon and hence, one grammar can be used to cover both languages. However, they are not similar concerning the script -- Hindi is written in Devanagari, while Urdu uses an Arabic-based script. By abstracting away to a common Roman transliteration scheme in the respective transliterators, our system can be enabled to handle both languages in parallel. In this paper, we discuss the pipeline architecture of the Urdu-Roman transliterator, mention several linguistic and orthographic issues and present the integration of the transliterator into the LFG parsing system.
Recent years witness a growing interest in the use of multimodal data for modelling of communicative behaviour in dialogue. Dybkjaer and Bernsen (2002), point out that coding schemes for multimodal data are used solely by their creators. Standardisation has been achieved to some extent for coding behavioural features for certain nonverbal expressions, e.g. for facial expression, however, for the semantic annotation of such expressions combined with other modalities such as speech there is still a long way to go. The majority of existing dialogue act annotation schemes that are designed to code semantic and pragmatic dialogue information are limited to analysis of spoken modality. This paper investigates the applicability of existing dialogue act annotation schemes to the semantic annotation of multimodal data, and the way a dialogue act annotation scheme can be extended to cover dialogue phenomena from multiple modalities. The general conclusion of our explorative study is that a multidimensional dialogue act taxonomy is usable for this purpose when some adjustments are made. We proposed a solution for adding these aspects to a dialogue act annotation scheme without changing its set of communicative functions, in the form of qualifiers that can be attached to communicative function tags.
As the interest of the NLP community grows to develop several treebanks also for languages other than English, we observe efforts towards evaluating the impact of different annotation strategies used to represent particular languages or with reference to particular tasks. This paper contributes to the debate on the influence of resources used for the training and development on the performance of parsing systems. It presents a comparative analysis of the results achieved by three different dependency parsers developed and tested with respect to two treebanks for the Italian language, namely TUT and ISST--TANL, which differ significantly at the level of both corpus composition and adopted dependency representations.
We describe a focused effort to investigate the performance of phrase-based, human evaluation of machine translation output achieving a high annotator agreement. We define phrase-based evaluation and describe the implementation of Appraise, a toolkit that supports the manual evaluation of machine translation results. Phrase ranking can be done using either a fine-grained six-way scoring scheme that allows to differentiate between ''``much better'''' and ''``slightly better'''', or a reduced subset of ranking choices. Afterwards we discuss kappa values for both scoring models from several experiments conducted with human annotators. Our results show that phrase-based evaluation can be used for fast evaluation obtaining significant agreement among annotators. The granularity of ranking choices should, however, not be too fine-grained as this seems to confuse annotators and thus reduces the overall agreement. The work reported in this paper confirms previous work in the field and illustrates that the usage of human evaluation in machine translation should be reconsidered. The Appraise toolkit is available as open-source and can be downloaded from the author's website.
We investigate the impact of input data scale in corpus-based learning using a study style of Zipfs law. In our research, Chinese word segmentation is chosen as the study case and a series of experiments are specially conducted for it, in which two types of segmentation techniques, statistical learning and rule-based methods, are examined. The empirical results show that a linear performance improvement in statistical learning requires an exponential increasing of training corpus size at least. As for the rule-based method, an approximate negative inverse relationship between the performance and the size of the input lexicon can be observed.
In this paper we investigate the problem of merging specialist taxonomies with the more intuitive folk taxonomies in lexical-semantic resources like wordnets; and we focus in particular on plants, animals and foods. We show that a traditional dictionary like Den Danske Ordbog (DDO) survives well with several inconsistencies between different taxonomies of the vocabulary and that a restructuring is therefore necessary in order to compile a consistent wordnet resource on its basis. To this end, we apply Cruses definitions for hyponymies, namely those of natural kinds (such as plants and animals) on the one hand and functional kinds (such as foods) on the other. We pursue this distinction in the development of the Danish wordnet, DanNet, which has recently been built on the basis of DDO and is made open source for all potential users at www.wordnet.dk. Not surprisingly, we conclude that cultural background influences the structure of folk taxonomies quite radically, and that wordnet builders must therefore consider these carefully in order to capture their central characteristics in a systematic way.
Folksonomies are unsystematic, unsophisticated collections of keywords associated by social bookmarking users to web content and, despite their inconsistency problems (typographical errors, spelling variations, use of space or punctuation as delimiters, same tag applied in different context, synonymy of concepts, etc.), their popularity is increasing among Web 2.0 application developers. In this paper, in addition to eliminating folksonomic irregularities existing at the lexical, syntactic or semantic understanding levels, we propose an algorithm that automatically builds a semantic representation of the folksonomy by exploiting the tags, their social bookmarking associations (co-occuring tags) and, more importantly, the content of labeled documents. We derive the semantics of each tag, discover semantic links between the folksonomic tags and expose the underlying semantic structure of the folksonomy, thus, enabling a number of information discovery and ontology-based reasoning applications.
After three years of work the Dutch Parallel Corpus (DPC) project has reached an end. The finalized corpus is a ten-million-word high-quality sentence-aligned bidirectional parallel corpus of Dutch, English and French, with Dutch as central language. In this paper we present the corpus and try to formulate some basic data collection principles, based on the work that was carried out for the project. Building a corpus is a difficult and time-consuming task, especially when every text sample included has to be cleared from copyrights. The DPC is balanced according to five text types (literature, journalistic texts, instructive texts, administrative texts and texts treating external communication) and four translation directions (Dutch-English, English-Dutch, Dutch-French and French-Dutch). All the text material was cleared from copyrights. The data collection process necessitated the involvement of different text providers, which resulted in drawing up four different licence agreements. Problems such as an unknown source language, copyright issues and changes to the corpus design are discussed in close detail and illustrated with examples so as to be of help to future corpus compilers.
In this paper, we report on a study that was performed within the Semantics of History project on how descriptions of historical events are realized in different types of text and what the implications are for modeling the event information. We believe that different historical perspectives of writers correspond in some degree with genre distinction and correlate with variation in language use. To capture differences between event representations in diverse text types and thus to identify relations between historical events, we defined an event model. We observed clear relations between particular parts of event descriptions - actors, time and location modifiers. Texts, written shortly after an event happened, use more specific and uniquely occurring event descriptions than texts describing the same events but written from a longer time perspective. We carried out some statistical corpus research to confirm this hypothesis. The ability to automatically determine relations between historical events and their sub-events over textual data, based on the relations between event participants, time markers and locations, will have important repercussions for the design of historical information retrieval systems.
Schulte im Walde et al. (2008) presented a novel approach to semantic verb classication. The predicate argument model (PAC) presented in their paper models selectional preferences by using soft clustering that incorporates the Expectation Maximization (EM) algorithm and the MDL principle. In this paper, I will show how the model handles the task of differentiating between plausible and implau- sible combinations of verbs, subcategorization frames and arguments by applying the pseudo-disambiguation evaluation method. The predicate argument clustering model will be evaluated in comparison with the latent semantic clustering model by Rooth et al. (1999). In particular, the influences of the model parameters, data frequency, and the individual components of the predicate argument model are examined. The results of these experiments show that (i) the selectional preference model overgeneralizes over arguments for the purpose of a pseudo-disambiguation task and that (ii) pseudo-disambiguation should not be used as a universal indicator for the quality of a model.
This paper presents a geometric approach to meaning representation within the framework of continuous mathematics. Meaning representation is a central issue in Natural Language Processing, in particular for tasks like word sense disambiguation or information extraction. We want here to discuss the relevance of using continuous models in semantics. We dont want to argue the continuous or discrete nature of lexical meaning. We use continuity as a tool to access and manipulate lexical meaning. Following Victorri (1994), we assume that continuity or discreteness are not properties of phenomena but characterizations of theories upon phenomena. We briefly describe our theoretical framework, the dynamical construction of meaning (Victorri and Fuchs, 1996), then present the way we automatically build continuous semantic spaces from a graph of synonymy and discuss their relevance and utility. We also think that discreteness and continuity can collaborate. We show here how we can complete our geometric representations with informations from discrete descriptions of meaning.
In recent years, blogs and social networks have particularly boosted interests for opinion mining research. In order to satisfy real-scale applicative needs, a main task is to create or to enhance lexical and semantic resources on evaluative language. Classical resources of the area are mostly built for english, they contain simple opinion word markers and are far to cover the lexical richness of this linguistic phenomenon. In particular, infrequent subjective words, idiomatic expressions, and cultural stereotypes are missing from resources. We propose a new method, applied on french, to enhance automatically an opinion word lexicon. This learning method relies on linguistic uses of internet users and on semantic tests to infer the degree of subjectivity of many new adjectives, nouns, verbs, noun phrases, verbal phrases which are usually forgotten by other resources. The final appraisal lexicon contains 3,456 entries. We evaluate the lexicon enhancement with and without textual context.
Evaluating systems and theories about persuasion represents a bottleneck for both theoretical and applied fields: experiments are usually expensive and time consuming. Still, measuring the persuasive impact of a message is of paramount importance. In this paper we present a new ``cheap and fast'' methodology for measuring the persuasiveness of communication. This methodology allows conducting experiments with thousands of subjects for a few dollars in a few hours, by tweaking and using existing commercial tools for advertising on the web, such as Google AdWords. The central idea is to use AdWords features for defining message persuasiveness metrics. Along with a description of our approach we provide some pilot experiments, conducted both with text and image based ads, that confirm the effectiveness of our ideas. We also discuss the possible application of research on persuasive systems to Google AdWords in order to add more flexibility in the wearing out of persuasive messages.
This paper introduces a new named entity corpus for Dutch. State-of-the-art named entity recognition systems require a substantial annotated corpus to be trained on. Such corpora exist for English, but not for Dutch. The STEVIN-funded SoNaR project aims to produce a diverse 500-million-word reference corpus of written Dutch, with four semantic annotation layers: named entities, coreference relations, semantic roles and spatiotemporal expressions. A 1-million-word subset will be manually corrected. Named entity annotation guidelines for Dutch were developed, adapted from the MUC and ACE guidelines. Adaptations include the annotation of products and events, the classification into subtypes, and the markup of metonymic usage. Inter-annotator agreement experiments were conducted to corroborate the reliability of the guidelines, which yielded satisfactory results (Kappa scores above 0.90). We are building a NER system, trained on the 1-million-word subcorpus, to automatically classify the remainder of the SoNaR corpus. To this end, experiments with various classification algorithms (MBL, SVM, CRF) and features have been carried out and evaluated.
Speech recognition technology suffers from a lack of robustness which limits its usability for fully automated speech-to-text transcription, and manual correction is generally required to obtain perfect transcripts. In this paper, we propose a general scheme for semi-automatic transcription, in which the system and the transcriptionist contribute jointly to the speech transcription. The proposed system relies on the editing of confusion networks and on reactive decoding, the latter one being supposed to take benefits from the manual correction and improve the error rates. In order to reduce the correction time, we evaluate various strategies aiming to guide the transcriptionist towards the critical areas of transcripts. These strategies are based on graph density-based criterion and two semantic consistency criterion; using a corpus-based method and a web-search engine. They allow to indicate to the user the areas which present severe lacks of understandability. We evaluate these driving strategies by simulating the correction process of French broadcast news transcriptions. Results show that interactive decoding improves the correction act efficiency with all driving strategies and semantic information must be integrated into the interactive decoding process.
This paper describes the process of building a newspaper corpus annotated with events described in specific documents. The main difference to the corpora built as part of the TDT initiative is that documents are not annotated by topics, but by specific events they describe. Additionally, documents are gathered from sixteen sources and all documents in the corpus are annotated with the corresponding event. The annotation process consists of a browsing and a searching step. Experiments are performed with a threshold that could be used in the browsing step yielding the result of having to browse through only 1{\%} of document pairs for a 2{\%} loss of relevant document pairs. A statistical analysis of the annotated corpus is undertaken showing that most events are described by few documents while just some events are reported by many documents. The inter-annotator agreement measures show high agreement concerning grouping documents into event clusters, but show a much lower agreement concerning the number of events the documents are organized into. An initial experiment is described giving a baseline for further research on this corpus.
Domain specific entity recognition often relies on domain-specific knowledge to improve system performance. However, such knowledge often suffers from limited domain portability and is expensive to build and maintain. Therefore, obtaining it in a generic and unsupervised manner would be a desirable feature for domain-specific entity recognition systems. In this paper, we introduce an approach that exploits domain-specificity of words as a form of domain-knowledge for entity-recognition tasks. Compared to prior work in the field, our approach is generic and completely unsupervised. We empirically show an improvement in entity extraction accuracy when features derived by our unsupervised method are used, with respect to baseline methods that do not employ domain knowledge. We also compared the results against those of existing systems that use manually crafted domain knowledge, and found them to be competitive.
In recent years, considerable attention has been dedicated to language modeling methods in information retrieval. Although these approaches generally allow exploitation of any type of language model, most of the published experiments were conducted with a classical n-gram model, usually limited only to unigrams. A few works exploiting syntax in information retrieval can be cited in this context, but significant contribution of syntax based language modeling for information retrieval is yet to be proved. In this paper, we propose, implement, and evaluate an enrichment of language model employing syntactic dependency information acquired automatically from both documents and queries. Our experiments are conducted on Czech which is a morphologically rich language and has a considerably free word order, therefore a syntactic language model is expected to contribute positively to the unigram and bigram language model based on surface word order. By testing our model on the Czech test collection from Cross Language Evaluation Forum 2007 Ad-Hoc track, we show positive contribution of using dependency syntax in this context.
A SuperSense Tagger is a tool for the automatic analysis of texts that associates to each noun, verb, adjective and adverb a semantic category within a general taxonomy. The developed tagger, based on a statistical model (Maximum Entropy), required the creation of an Italian annotated corpus, to be used as a training set, and the improvement of various existing tools. The obtained results significantly improved the current state-of-the art for this particular task.
This paper presents the work that has been carried out to annotate semantic roles in the Basque Dependency Treebank (BDT). We will describe the resources we have used and the way the annotation of 100 verbs has been done. We decide to follow the model proposed in the PropBank project that has been deployed in other languages, such as Chinese, Spanish, Catalan and Russian. The resources used are: an in-house database with syntactic/semantic subcategorization frames for Basque verbs, an English-Basque verb mapping based on Levins classification and the BDT itself. Detailed guidelines for human taggers have been established as a result of this annotation process. In addition, we have characterized the information associated to the semantic tag. Besides, and based on this study, we will define semi-automatic procedures that will facilitate the task of manual annotation for the rest of the verbs of the Treebank. We have also adapted AbarHitz, a tool used in the construction of the BDT, for the task of annotating semantic roles according to the proposed characterization.
This paper presents the outline and performance of an automatic syllable boundary detection system. The syllabification of phonemes is performed with a rule-based system, implemented in a Java program. Phonemes are categorized into 6 classes. A set of specific rules are developed and categorized as general rules which can be applied in all cases, and exception rules which are applied in some specific situations. These rules deal with a French spontaneous speech corpus. Moreover, the proposed phonemes, classes and rules are listed in an external configuration file of the tool (under GPL licence) that make the tool very easy to adapt to a specific corpus by adding or modifying rules, phoneme encoding or phoneme classes, by the use of a new configuration file. Finally, performances are evaluated and compared to 3 other French syllabification systems and show significant improvements. Automatic system output and expert's syllabification are in agreement for most of syllable boundaries in our corpus.
During the construction of a spoken dialogue system much effort is spent on improving the quality of speech recognition as possible. However, even if an application perfectly recognizes the input, its understanding may be far from what the user originally meant. The user should be informed about what the system actually understood so that an error will not have a negative impact in the later stages of the dialogue. One important aspect that this work tries to address is the effect of presenting the systems understanding during interaction with users. We argue that for specific kinds of applications its important to confirm the understanding of the system before obtaining the output. In this way the user can avoid misconceptions and problems occurring in the dialogue flow and he can enhance his confidence in the system. Nevertheless this has an impact on the interaction, as the mental workload increases, and the users behavior may adapt to the systems coverage. We focus on two applications that implement the notion of rephrasing users input in a different way. Our study took place among 14 subjects that used both systems on a Nokia N810 Internet Tablet.
This article presents a new freely available trilingual corpus (Catalan, Spanish, English) that contains large portions of the Wikipedia and has been automatically enriched with linguistic information. To our knowledge, this is the largest such corpus that is freely available to the community: In its present version, it contains over 750 million words. The corpora have been annotated with lemma and part of speech information using the open source library FreeLing. Also, they have been sense annotated with the state of the art Word Sense Disambiguation algorithm UKB. As UKB assigns WordNet senses, and WordNet has been aligned across languages via the InterLingual Index, this sort of annotation opens the way to massive explorations in lexical semantics that were not possible before. We present a first attempt at creating a trilingual lexical resource from the sense-tagged Wikipedia corpora, namely, WikiNet. Moreover, we present two by-products of the project that are of use for the NLP community: An open source Java-based parser for Wikipedia pages developed for the construction of the corpus, and the integration of the WSD algorithm UKB in FreeLing.
We investigate which distributional properties should be present in a tagset by examining different mappings of various current part-of-speech tagsets, looking at English, German, and Italian corpora. Given the importance of distributional information, we present a simple model for evaluating how a tagset mapping captures distribution, specifically by utilizing a notion of frames to capture the local context. In addition to an accuracy metric capturing the internal quality of a tagset, we introduce a way to evaluate the external quality of tagset mappings so that we can ensure that the mapping retains linguistically important information from the original tagset. Although most of the mappings we evaluate are motivated by linguistic concerns, we also explore an automatic, bottom-up way to define mappings, to illustrate that better distributional mappings are possible. Comparing our initial evaluations to POS tagging results, we find that more distributional tagsets can sometimes result in worse accuracy, underscring the need to carefully define the properties of a tagset.
Translation into the languages with relatively free word order has received a lot less attention than translation into fixed word order languages (English), or into analytical languages (Chinese). At the same time this translation task is found among the most difficult challenges for machine translation (MT), and intuitively it seems that there is some space in improvement intending to reflect the free word order structure of the target language. This paper presents a comparative study of two alternative approaches to statistical machine translation (SMT) and their application to a task of English-to-Latvian translation. Furthermore, a novel feature intending to reflect the relatively free word order scheme of the Latvian language is proposed and successfully applied on the n-best list rescoring step. Moving beyond classical automatic scores of translation quality that are classically presented in MT research papers, we contribute presenting a manual error analysis of MT systems output that helps to shed light on advantages and disadvantages of the SMT systems under consideration.
The paper presents an approach for constructing a weighted bilingual dictionary of inflectional forms using as input data a traditional bilingual dictionary, and not parallel corpora. An algorithm is developed that generates all possible morphological (inflectional) forms and weights them using information on distribution of corresponding grammar sets (grammar information) in large corpora for each language. The algorithm also takes into account the compatibility of grammar sets in a language pair; for example, verb in past tense in language L normally is expected to be translated by verb in past tense in Language L'. We consider that the developed method is universal, i.e. can be applied to any pair of languages. The obtained dictionary is freely available. It can be used in several NLP tasks, for example, statistical machine translation.
In this paper, we describe a new multi-purpose audio-visual database on the context of speech interfaces for controlling household electronic devices. The database comprises speech and video recordings of 19 speakers interacting with a HIFI audio box by means of a spoken dialogue system. Dialogue management is based on Bayesian Networks and the system is provided with contextual information handling strategies. Each speaker was requested to fulfil different sets of specific goals following predefined scenarios, according to both different complexity levels and degrees of freedom or initiative allowed to the user. Due to a careful design and its size, the recorded database allows comprehensive studies on speech recognition, speech understanding, dialogue modeling and management, microphone array based speech processing, and both speech and video-based acoustic source localisation. The database has been labelled for quality and efficiency studies on dialogue performance. The whole database has been validated through both objective and subjective tests.
In this paper, we present a case study for measuring inter-annotator agreement on a linguistic ontology for spatial language, namely the spatial extension of the Generalized Upper Model. This linguistic ontology specifies semantic categories, and it is used in dialogue systems for natural language of space in the context of human-computer interaction and spatial assistance systems. Its core representation for spatial language distinguishes how sentences can be structured and categorized into units that contribute certain meanings to the expression. This representation is here evaluated in terms of inter-annotator agreement: four uninformed annotators were instructed by a manual how to annotate sentences with the linguistic ontology. They have been assigned to annotate 200 sentences with varying length and complexity. Their resulting agreements are calculated together with our own 'expert annotation' of the same sentences. We show that linguistic ontologies can be evaluated with respect to inter-annotator agreement, and we present encouraging results of calculating agreements for the spatial extension of the Generalized Upper Model.
While the web provides a fantastic linguistic resource, collecting and processing data at web-scale is beyond the reach of most academic laboratories. Previous research has relied on search engines to collect online information, but this is hopelessly inefficient for building large-scale linguistic resources, such as lists of named-entity types or clusters of distributionally similar words. An alternative to processing web-scale text directly is to use the information provided in an N-gram corpus. An N-gram corpus is an efficient compression of large amounts of text. An N-gram corpus states how often each sequence of words (up to length N) occurs. We propose tools for working with enhanced web-scale N-gram corpora that include richer levels of source annotation, such as part-of-speech tags. We describe a new set of search tools that make use of these tags, and collectively lower the barrier for lexical learning and ambiguity resolution at web-scale. They will allow novel sources of information to be applied to long-standing natural language challenges.
Annotation of digital recordings in humanities research still is, to a large extend, a process that is performed manually. This paper describes the first pattern recognition based software components developed in the AVATecH project and their integration in the annotation tool ELAN. AVATecH (Advancing Video/Audio Technology in Humanities Research) is a project that involves two Max Planck Institutes (Max Planck Institute for Psycholinguistics, Nijmegen, Max Planck Institute for Social Anthropology, Halle) and two Fraunhofer Institutes (Fraunhofer-Institut f{\"u
This paper presents the acquisition and annotation of Slovenian Lombard Speech Database, the recording of which started in the year 2008. The database was recorded at the University of Maribor, Slovenia. The goal of this paper is to describe the hardware platform used for the acquisition of speech material, recording scenarios and tools used for the annotation of Slovenian Lombard Speech Database. The database consists of recordings of 10 Slovenian native speakers. Five males and five females were recorded. Each speaker pronounced a set of eight corpuses in two recording sessions with at least one week pause between recordings. The structure of the corpus is similar to SpeechDat II database. Approximately 30 minutes of speech material per speaker and per session was recorded. The manual annotation of speech material is performed with the LombardSpeechLabel tool developed at the University of Maribor. The speech and annotation material was saved on 10 DVDs (one speaker on one DVD).
This paper introduces a new lexicographic resource, the MuLeXFoR database, which aims to present word-formation processes in a multilingual environment. Morphological items represent a real challenge for lexicography, especially for the development of multilingual tools. Affixes can take part in several word-formation rules and, conversely, rules can be realised by means of a variety of affixes. Consequently, it is often difficult to provide enough information to help users understand the meaning(s) of an affix or familiarise with the most frequent strategies used to translate the meaning(s) conveyed by affixes. In fact, traditional dictionaries often fail to achieve this goal. The database introduced in this paper tries to take advantage of recent advances in electronic implementation and morphological theory. Word-formation is presented as a set of multilingual rules that users can access via different indexes (affixes, rules and constructed words). MuLeXFoR entries contain, among other things, detailed descriptions of morphological constraints and productivity notes, which are sorely lacking in currently available tools such as bilingual dictionaries.
Creating more fine-grained annotated data than previously relevent document sets is important for evaluating individual components in automatic question answering systems. In this paper, we describe using the Amazon's Mechanical Turk (AMT) to judge whether paragraphs in relevant documents answer corresponding list questions in TREC QA track 2004. Based on AMT results, we build a collection of 1300 gold-standard supporting paragraphs for list questions. Our online experiments suggested that recruiting more people per task assures better annotation quality. In order to learning true labels from AMT annotations, we investigated three approaches on two datasets with different levels of annotation errors. Experimental studies show that the Naive Bayesian model and EM-based GLAD model can generate results highly agreeing with gold-standard annotations, and dominate significantly over the majority voting method for true label learning. We also suggested setting higher HIT approval rate to assure better online annotation quality, which leads to better performance of learning methods.
In this paper, we base on the syntactic structural Chinese Treebank corpus, construct the Chinese Opinon Treebank for the research of opinion analysis. We introduce the tagging scheme and develop a tagging tool for constructing this corpus. Annotated samples are described. Information including opinions (yes or no), their polarities (positive, neutral or negative), types (expression, status, or action), is defined and annotated. In addition, five structure trios are introduced according to the linguistic relations between two Chinese words. Four of them that are possibly related to opinions are also annotated in the constructed corpus to provide the linguistic cues. The number of opinion sentences together with the number of their polarities, opinion types, and trio types are calculated. These statistics are compared and discussed. To know the quality of the annotations in this corpus, the kappa values of the annotations are calculated. The substantial agreement between annotations ensures the applicability and reliability of the constructed corpus.
In this paper we report a way of constructing a translation corpus that contains not only source and target texts, but draft and final versions of target texts, through the translation hosting site Minna no Hon'yaku (MNH). We made MNH publicly available on April 2009. Since then, more than 1,000 users have registered and over 3,500 documents have been translated, as of February 2010, from English to Japanese and from Japanese to English. MNH provides an integrated translation-aid environment, QRedit, which enables translators to look up high-quality dictionaries and Wikipedia as well as to search Google seamlessly. As MNH keeps translation logs, a corpus consisting of source texts, draft translations in several versions, and final translations is constructed naturally through MNH. As of 7 February, 764 documents with multiple translation versions are accumulated, of which 110 are edited by more than one translators. This corpus can be used for self-learning by inexperienced translators on MNH, and potentially for improving machine translation.
Large scale parallel data generation for new language pairs requires intensive human effort and availability of experts. It becomes immensely difficult and costly to provide Statistical Machine Translation (SMT) systems for most languages due to the paucity of expert translators to provide parallel data. Even if experts are present, it appears infeasible due to the impending costs. In this paper we propose Active Crowd Translation (ACT), a new paradigm where active learning and crowd-sourcing come together to enable automatic translation for low-resource language pairs. Active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowd-sourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. We experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. Similarly, our experiments with crowd-sourcing on Mechanical Turk have shown that it is possible to create parallel corpora using non-experts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality.
Clinical texts contain a large amount of information. Some of this information is embedded in contexts where e.g. a patient status is reasoned about, which may lead to a considerable amount of statements that indicate uncertainty and speculation. We believe that distinguishing such instances from factual statements will be very beneficial for automatic information extraction. We have annotated a subset of the Stockholm Electronic Patient Record Corpus for certain and uncertain expressions as well as speculative and negation keywords, with the purpose of creating a resource for the development of automatic detection of speculative language in Swedish clinical text. We have analyzed the results from the initial annotation trial by means of pairwise Inter-Annotator Agreement (IAA) measured with F-score. Our main findings are that IAA results for certain expressions and negations are very high, but for uncertain expressions and speculative keywords results are less encouraging. These instances need to be defined in more detail. With this annotation trial, we have created an important resource that can be used to further analyze the properties of speculative language in Swedish clinical text. Our intention is to release this subset to other research groups in the future after removing identifiable information.
Ontologies, and in particular upper ontologies, are foundational to the establishment of the Semantic Web. Upper ontologies are used as equivalence formalisms between domain specific ontologies. Multilingualism brings one of the key challenges to the development of these ontologies. Fundamental to the challenges of defining upper ontologies is the assumption that concepts are universally shared. The approach to developing linguistic ontologies aligned to upper ontologies, particularly in the non-Indo-European language families, has highlighted these challenges. Previously two approaches to developing new linguistic ontologies and the influence of these approaches on the upper ontologies have been well documented. These approaches are examined in a unique new context: the African, and in particular, the Bantu languages. In particular, we address the following two questions: Which approach is better for the alignment of the African languages to upper ontologies? Can the concepts that are linguistically shared amongst the African languages be aligned easily with upper ontology concepts claimed to be universally shared?
CASIA-CASSIL is a large-scale corpus base of Chinese human-human naturally-occurring telephone conversations in restricted domains. The first edition consists of 792 90-second conversations belonging to tourism domain, which are selected from 7,639 spontaneous telephone recordings in real scenarios. The corpus is now being annotated with wide range of linguistic and paralinguistic information in multi-levels. The annotations include Turns, Speaker Gender, Orthographic Transcription, Chinese Syllable, Chinese Phonetic Transcription, Prosodic Boundary, Stress of Sentence, Non-Speech Sounds, Voice Quality, Topic, Dialog-act and Adjacency Pairs, Ill-formedness, and Expressive Emotion as well, 13 levels in total. The abundant annotation will be effective especially for studying Chinese spoken language phenomena. This paper describes the whole process to build the conversation corpus, including collecting and selecting the original data, and the follow-up process such as transcribing, annotating, and so on. CASIA-CASSIL is being extended to a large scale corpus base of annotated Chinese dialogs for spoken Chinese study.
This paper investigates the effectiveness of online temporal language model adaptation when applied to a Thai broadcast news transcription task. Our adaptation scheme works as follow: first an initial language model is trained with broadcast news transcription available during the development period. Then the language model is adapted over time with more recent broadcast news transcription and online news articles available during deployment especially the data from the same time period as the broadcast news speech being recognized. We found that the data that are closer in time are more similar in terms of perplexity and are more suitable for language model adaptation. The LMs that are adapted over time with more recent news data are better, both in terms of perplexity and WER, than the static LM trained from only the initial set of broadcast news data. Adaptation data from broadcast news transcription improved perplexity by 38.3{\%} and WER by 7.1{\%} relatively. Though, online news articles achieved less improvement, it is still a useful resource as it can be obtained automatically. Better data pre-processing techniques and data selection techniques based on text similarity could be applied to the news articles to obtain further improvement from this promising result.
We present the D-TUNA corpus, which is the first semantically annotated corpus of referring expressions in Dutch. Its primary function is to evaluate and improve the performance of REG algorithms. Such algorithms are computational models that automatically generate referring expressions by computing how a specific target can be identified to an addressee by distinguishing it from a set of distractor objects. We performed a large-scale production experiment, in which participants were asked to describe furniture items and people, and provided all descriptions with semantic information regarding the target and the distractor objects. Besides being useful for evaluating REG algorithms, the corpus addresses several other research goals. Firstly, the corpus contains both written and spoken referring expressions uttered in the direction of an addressee, which enables systematic analyses of how modality (text or speech) influences the human production of referring expressions. Secondly, due to its comparability with the English TUNA corpus, our Dutch corpus can be used to explore the differences between Dutch and English speakers regarding the production of referring expressions.
This paper presents the ADN-Classifier, an Automatic classification system of Spanish Deverbal Nominalizations aimed at identifying its semantic denotation (i.e. event, result, underspecified, or lexicalized). The classifier can be used for NLP tasks such as coreference resolution or paraphrase detection. To our knowledge, the ADN-Classifier is the first effort in acquisition of denotations for nominalizations using Machine Learning.We compare the results of the classifier when using a decreasing number of Knowledge Sources, namely (1) the complete nominal lexicon (AnCora-Nom) that includes sense distictions, (2) the nominal lexicon (AnCora-Nom) removing the sense-specific information, (3) nominalizations context information obtained from a treebank corpus (AnCora-Es) and (4) the combination of the previous linguistic resources. In a realistic scenario, that is, without sense distinction, the best results achieved are those taking into account the information declared in the lexicon (89.40{\%} accuracy). This shows that the lexicon contains crucial information (such as argument structure) that corpus-derived features cannot substitute for.
Grammatical approaches to language technology are often considered less optimal than statistical approaches in multilingual settings, where large-scale portability becomes an important issue. The present paper argues that there is a notable gain in reusing grammatical resources when porting technology to new languages. The pivot language is North S{\'a}mi, and the paper discusses portability with respect to the closely related Lule and South S{\'a}mi, and to the unrelated Faroese and Greenlandic languages.
This paper describes the design and collection of NameDat, a database containing English proper names spoken by native Norwegians. The database was designed to cover the typical acoustic and phonetic variations that appear when Norwegians pronounce English names. The intended use of the database is acoustic and lexical modeling of these phonetic variations. The English names in the database have been enriched with several annotation tiers. The recorded names were selected according to three selection criteria: the familiarity of the name, the expected recognition performance and the coverage of non-native phonemes. The validity of the manual annotations was verified by means of an automatic recognition experiment of non-native names. The experiment showed that the use of the manual transcriptions from NameDat yields an increase in recognition performance over automatically generated transcriptions.
The national language of the Grand-Duchy of Luxembourg, Luxembourgish, has often been characterized as one of Europe's under-described and under-resourced languages. Because of a limited written production of Luxembourgish, poorly observed writing standardization (as compared to other languages such as English and French) and a large diversity of spoken varieties, the study of Luxembourgish poses many interesting challenges to automatic speech processing studies as well as to linguistic enquiries. In the present paper, we make use of large corpora to focus on typical writing and derived pronunciation variants in Luxembourgish, elicited by mobile -n deletion (hereafter shortened to MND). Using transcriptions from the House of Parliament debates and 10k words from news reports, we examine the reality of MND variants in written transcripts of speech. The goal of this study is manyfold: quantify the potential of variation due to MND in written Luxembourgish, check the mandatory status of the MND rule and discuss the arising problems for automatic spoken Luxembourgish processing.
The paper presents the procedure of syntactic annotation of the National Corpus of Polish. The paper concentrates on the delimitation of syntactic words (analytical forms, reflexive verbs, discontinuous conjunctions, etc.) and syntactic groups, as well as on problems encountered during the annotation process: syntactic group boundaries, multiword entities, abbreviations, discontinuous phrases and syntactic words. It includes the complete tagset for syntactic words and the list of syntactic groups recognized in NKJP. The tagset defines grammatical classes and categories according to morphosyntactic and syntactic criteria only. Syntactic annotation in the National Corpus of Polish is limited to making constituents of combinations of words. Annotation depends on shallow parsing and manual post-editing of the results by annotators. Manual annotation is performed by two independents annotators, with a referee in cases of disagreement. The manually constructed grammar, both for syntactic words and for syntactic groups, is encoded in the shallow parsing system Spejd.
In spoken dialogues, if a spoken dialogue system does not respond at all during users utterances, the user might feel uneasy because the user does not know whether or not the system has recognized the utterances. In particular, back-channel utterances, which the system outputs as voices such as yeah and uh huh in English have important roles for a driver in in-car speech dialogues because the driver does not look owards a listener while driving. This paper describes construction of a back-channel utterance corpus and its analysis to develop the system which can output back-channel utterances at the proper timing in the responsive in-car speech dialogue. First, we constructed the back-channel utterance corpus by integrating the back-channel utterances that four subjects provided for the drivers utterances in 60 dialogues in the CIAIR in-car speech dialogue corpus. Next, we analyzed the corpus and revealed the relation between back-channel utterance timings and information on bunsetsu, clause, pause and rate of speech. Based on the analysis, we examined the possibility of detecting back-channel utterance timings by machine learning technique. As the result of the experiment, we confirmed that our technique achieved as same detection capability as a human.
For some years now, the Nederlandse Taalunie (Dutch Language Union) has been active in promoting the development of human language technology (HLT) applications for users of Dutch with communication disabilities. The reason is that HLT products and services may enable these users to improve their verbal autonomy and communication skills. We sought to identify a minimum common set of HLT resources that is required to develop tools for a wide range of communication disabilities. In order to reach this goal, we investigated the specific HLT needs of communicatively disabled people and related these needs to the underlying HLT software components. By analysing the availability and quality of these essential HLT resources, we were able to identify which of the crucial elements need further research and development to become usable for developing applications for communicatively disabled users of Dutch. The results obtained in the current survey can be used to inform policy institutions on how they can stimulate the development of HLT resources for this target group. In the current study results were obtained for Dutch, but a similar approach can also be used for other languages.
This article describes an age-annotated database of German telephone speech. All in all 47 hours of prompted and free text was recorded, uttered by 954 paid participants in a style typical for automated voice services. The participants were selected based on an equal distribution of males and females within four age cluster groups; children, youth, adults and seniors. Within the children, gender is not distinguished, because it doesnt have a strong enough effect on the voice. The textual content was designed to be typical for automated voice services and consists mainly of short commands, single words and numbers. An additional database consists of 659 speakers (368 female and 291 male) that called an automated voice portal server and answered freely on one of the two questions What is your favourite dish? and What would you take to an island? (island set, 422 speakers). This data might be used for out-of domain testing. The data will be used to tune an age-detecting automated voice service and might be released to research institutes under controlled conditions as part of an open age and gender detection challenge.
A corpus called DutchParl is created which aims to contain all digitally available parliamentary documents written in the Dutch language. The first version of DutchParl contains documents from the parliaments of The Netherlands, Flanders and Belgium. The corpus is divided along three dimensions: per parliament, scanned or digital documents, written recordings of spoken text and others. The digital collection contains more than 800 million tokens, the scanned collection more than 1 billion. All documents are available as UTF-8 encoded XML files with extensive metadata in Dublin Core standard. The text itself is divided into pages which are divided into paragraphs. Every document, page and paragraph has a unique URN which resolves to a web page. Every page element in the XML files is connected to a facsimile image of that page in PDF or JPEG format. We created a viewer in which both versions can be inspected simultaneously. The corpus is available for download in several formats. The corpus can be used for corpus-linguistic and political science research, and is suitable for performing scalability tests for XML information systems.
This paper introduces GernEdiT (short for: GermaNet Editing Tool), a new graphical user interface for the lexicographers and developers of GermaNet, the German version of the Princeton WordNet. GermaNet is a lexical-semantic net that relates German nouns, verbs, and adjectives. Traditionally, lexicographic work for extending the coverage of GermaNet utilized the Princeton WordNet development environment of lexicographer files. Due to a complex data format and no opportunity of automatic consistency checks, this process was very error prone and time consuming. The GermaNet Editing Tool GernEdiT was developed to overcome these shortcomings. The main purposes of the GernEdiT tool are, besides supporting lexicographers to access, modify, and extend GermaNet data in an easy and adaptive way, as follows: Replace the standard editing tools by a more user-friendly tool, use a relational database as data storage, support export formats in the form of XML, and facilitate internal consistency and correctness of the linguistic resource. All these core functionalities of GernEdiT along with the main aspects of the underlying lexical resource GermaNet and its current database format are presented in this paper.
For researchers, it is especially important that primary research data are preserved and made available on a long-term basis and to a wide variety of researchers. In order to ensure long-term availability of the archived data, it is imperative that the data to be stored is conformant with standardized data formats and best practices followed by the relevant research communities. Storing, managing, and accessing such standard-conformant data requires a repository-based infrastructure. Two projects at the University of T{\"u
We describe a new method for extracting Negative Polarity Item candidates (NPI candidates) from dependency-parsed German text corpora. Semi-automatic extraction of NPIs is a challenging task since NPIs do not have uniform categorical or other syntactic properties that could be used for detecting them; they occur as single words or as multi-word expressions of almost any syntactic category. Their defining property is of a semantic nature, they may only occur in the scope of negation and related semantic operators. In contrast to an earlier approach to NPI extraction from corpora, we specifically target multi-word expressions. Besides applying statistical methods to measure the co-occurrence of our candidate expressions with negative contexts, we also apply linguistic criteria in an attempt to determine to which degree they are idiomatic. Our method is evaluated by comparing the set of NPIs we found with the most comprehensive electronic list of German NPIs, which currently contains 165 entries. Our method retrieved 142 NPIs, 114 of which are new.
The goal of this paper is to describe the annotation protocols and the Semantic Annotation Tool (SAT) used in the DutchSemCor project. The DutchSemCor project is aiming at aligning the Cornetto lexical database with the Dutch language corpus SoNaR. 250K corpus occurrences of the 3,000 most frequent and most ambiguous Dutch nouns, adjectives and verbs are being annotated manually using the SAT. This data is then used for bootstrapping 750K extra occurrences which in turn will be checked manually. Our main focus in this paper is the methodology applied in the project to attain the envisaged Inter-annotator Agreement (IA) of =80{\%}. We will also discuss one of the main objectives of DutchSemCor i.e. to provide semantically annotated language data with high scores for quantity, quality and diversity. Sample data with high scores for these three features can yield better results for co-training WSD systems. Finally, we will take a brief look at our annotation tool.
eScience - enhanced science - is a new paradigm of scientific work and research. In the humanities, eScience environments can be helpful in establishing new workflows and lifecycles of scientific data. WebLicht is such an eScience environment for linguistic analysis, making linguistic tools and resources available network-wide. Today, most digital language resources and tools (LRT) are available by download only. This is inconvenient for someone who wants to use and combine several tools because these tools are normally not compatible with each other. To overcome this restriction, WebLicht makes the functionality of linguistic tools and the resources themselves available via the internet as web services. In WebLicht, several kinds of linguistic tools are available which cover the basic functionality of automatic and incremental creation of annotated text corpora. To make use of the more than 70 tools and resources currently available, the end user needs nothing more than just a common web browser.
This article describes the preparation, recording and orthographic transcription of a new speech corpus, the Nijmegen Corpus of Casual Spanish (NCCSp). The corpus contains around 30 hours of recordings of 52 Madrid Spanish speakers engaged in conversations with friends. Casual speech was elicited during three different parts, which together provided around ninety minutes of speech from every group of speakers. While Parts 1 and 2 did not require participants to perform any specific task, in Part 3 participants negotiated a common answer to general questions about society. Information about how to obtain a copy of the corpus can be found online at http://mirjamernestus.ruhosting.nl/Ernestus/NCCSp
In this paper we describe GikiCLEF, the first evaluation contest that, to our knowledge, was specifically designed to expose and investigate cultural and linguistic issues involved in structured multimedia collections and searching, and which was organized under the scope of CLEF 2009. GikiCLEF evaluated systems that answered hard questions for both human and machine, in ten different Wikipedia collections, namely Bulgarian, Dutch, English, German, Italian, Norwegian (Bokm{\"a
Over the years, the field of Language Resources and Technology (LRT) has developed a tremendous amount of resources and tools. However, there is no ready-to-use map that researchers could use to gain a good overview and steadfast orientation when searching for, say corpora or software tools to support their studies. It is rather the case that information is scattered across project- or organisation-specific sites, which makes it hard if not impossible for less-experienced researchers to gather all relevant material. Clearly, the provision of metadata is central to resource and software exploration. However, in the LRT field, metadata comes in many forms, tastes and qualities, and therefore substantial harmonization and curation efforts are required to provide researchers with metadata-based guidance. To address this issue a broad alliance of LRT providers (CLARIN, the Linguist List, DOBES, DELAMAN, DFKI, ELRA) have initiated the Virtual Language Observatory portal to provide a low-barrier, easy-to-follow entry point to language resources and tools; it can be accessed via http://www.clarin.eu/vlo
The paper introduces CORPRES ― a fully annotated Russian speech corpus developed at the Department of Phonetics, St. Petersburg State University as a result of a three-year project. The corpus includes samples of different speaking styles produced by 4 male and 4 female speakers. Six levels of annotation cover all phonetic and prosodic information about the recorded speech data, including labels for pitch marks, phonetic events, narrow and wide phonetic transcription, orthographic and prosodic transcription. Precise phonetic transcription of the data provides an especially valuable resource for both research and development purposes. Overall corpus size is 528 458 running words and contains 60 hours of speech made up of 7.5 hours from each speaker. 40{\%} of the corpus was manually segmented and fully annotated on all six levels. 60{\%} of the corpus was partly annotated; there are labels for pitch period and phonetic event labels. Orthographic, prosodic and ideal phonetic transcription for this part was generated and stored as text files. The fully annotated part of the corpus covers all speaking styles included in the corpus and all speakers. The paper contains information about CORPRES design and annotation principles, overall data description and some speculation about possible use of the corpus.
In this paper the FAU IISAH corpus and its recording conditions are described: a new speech database consisting of human-machine and human-human interaction recordings. Beside close-talking microphones for the best possible audio quality of the recorded speech, far-distance microphones were used to acquire the interaction and communication. The recordings took place during a Wizard-of-Oz experiment in the intelligent, senior-adapted house (ISA-House). That is a living room with a speech controlled home assistance system for elderly people, based on a dialogue system, which is able to process spontaneous speech. During the studies in the ISA-House more than eight hours of interaction data were recorded including 3 hours and 27 minutes of spontaneous speech. The data were annotated in terms of human-human (off-talk) and human-machine (on-talk) interaction. The test persons used 2891 turns of off-talk and 2752 turns of on-talk including 1751 different words. Still in progress is the analysis under statistical and linguistical aspects.
The Quranic Arabic Corpus (http://corpus.quran.com) is an annotated linguistic resource with multiple layers of annotation including morphological segmentation, part-of-speech tagging, and syntactic analysis using dependency grammar. The motivation behind this work is to produce a resource that enables further analysis of the Quran, the 1,400 year old central religious text of Islam. This paper describes a new approach to morphological annotation of Quranic Arabic, a genre difficult to compare with other forms of Arabic. Processing Quranic Arabic is a unique challenge from a computational point of view, since the vocabulary and spelling differ from Modern Standard Arabic. The Quranic Arabic Corpus differs from other Arabic computational resources in adopting a tagset that closely follows traditional Arabic grammar. We made this decision in order to leverage a large body of existing historical grammatical analysis, and to encourage online collaborative annotation. In this paper, we discuss how the unique challenge of morphological annotation of Quranic Arabic is solved using a multi-stage approach. The different stages include automatic morphological tagging using diacritic edit-distance, two-pass manual verification, and online collaborative annotation. This process is evaluated to validate the appropriateness of the chosen methodology.
A new type of language resource called 'BAStat' has been released by the Bavarian Archive for Speech Signals at Ludwig Maximilians Universitaet, Munich. In contrast to primary resources like speech and text corpora BAStat comprises statistical estimates based on a number of primary spoken language resources: first and second order occurrence probability of phones, syllables and words, duration statistics, probabilities of pronunciation variants of words and probabilities of context information. Unlike other statistical speech resources BAStat is based solely on recordings of conversational German and therefore models spoken language not text. The resource consists of a bundle of 7-bit ASCII tables and matrices to maximize inter-operability between different operation systems and can be downloaded for free from the BAS web-site. This contribution gives a detailed description about the empirical basis, the contained data types, the format of the resulting statistical data, some interesting interpretations of grand figures and a brief comparison to the text-based statistical resource CELEX.
The Quranic Arabic Dependency Treebank (QADT) is part of the Quranic Arabic Corpus (http://corpus.quran.com), an online linguistic resource organized by the University of Leeds, and developed through online collaborative annotation. The website has become a popular study resource for Arabic and the Quran, and is now used by over 1,500 researchers and students daily. This paper presents the treebank, explains the choice of syntactic representation, and highlights key parts of the annotation guidelines. The text being analyzed is the Quran, the central religious book of Islam, written in classical Quranic Arabic (c. 600 CE). To date, all 77,430 words of the Quran have a manually verified morphological analysis, and syntactic analysis is in progress. 11,000 words of Quranic Arabic have been syntactically annotated as part of a gold standard treebank. Annotation guidelines are especially important to promote consistency for a corpus which is being developed through online collaboration, since often many people will participate from different backgrounds and with different levels of linguistic expertise. The treebank is available online for collaborative correction to improve accuracy, with suggestions reviewed by expert Arabic linguists, and compared against existing published books of Quranic Syntax.
There are many accurate methods for language identification of long text samples, but identification of very short strings still presents a challenge. This paper studies a language identification task, in which the test samples have only 5-21 characters. We compare two distinct methods that are well suited for this task: a naive Bayes classifier based on character n-gram models, and the ranking method by Cavnar and Trenkle (1994). For the n-gram models, we test several standard smoothing techniques, including the current state-of-the-art, the modified Kneser-Ney interpolation. Experiments are conducted with 281 languages using the Universal Declaration of Human Rights. Advanced language model smoothing techniques improve the identification accuracy and the respective classifiers outperform the ranking method. The higher accuracy is obtained at the cost of larger models and slower classification speed. However, there are several methods to reduce the size of an n-gram model, and our experiments with model pruning show that it provides an easy way to balance the size and the identification accuracy. We also compare the results to the language identifier in Google AJAX Language API, using a subset of 50 languages.
As users become more accustomed to using their mobile devices to organize and schedule their lives, there is more of a demand for applications that can make that process easier. Automatic speech recognition technology has already been developed to enable essentially unlimited vocabulary in a mobile setting. Understanding the words that are spoken is the next challenge. In this paper, we describe efforts to develop a dataset and classifier to recognize named entities in speech. Using sets of both real and simulated data, in conjunction with a very large set of real named entities, we created a challenging corpus of training and test data. We use these data to develop a classifier to identify names and locations on a word-by-word basis. In this paper, we describe the process of creating the data and determining a set of features to use for named entity recognition. We report on our classification performance on these data, as well as point to future work in improving all aspects of the system.
This paper deals with the task of large vocabulary proper name recognition. In order to accomodate a wide diversity of possible name pronunciations (due to non-native name origins or speaker tongues) a multilingual acoustic model is combined with a lexicon comprising 3 grapheme-to-phoneme (G2P) transcriptions from G2P transcribers for 3 different languages) and up to 4 so-called phoneme-to-phoneme (P2P) transcriptions. The latter are generated with (speaker tongue, name source) specific P2P converters that try to transform a set of baseline name transcriptions into a pool of transcription variants that lie closer to the `true name pronunciations. The experimental results show that the generated P2P variants can be employed to improve name recognition, and that the obtained accuracy is comparable to what is achieved with typical (TY) transcriptions (made by a human expert). Furthermore, it is demonstrated that the P2P conversion can best be instantiated from a baseline transcription in the name source language, and that knowledge of the speaker tongue is an important input as well for the P2P transcription process.
Morphological analyzers and part-of-speech taggers are key technologies for most text analysis applications. Our aim is to develop a part-of-speech tagger for annotating a wide range of Arabic text formats, domains and genres including both vowelized and non-vowelized text. Enriching the text with linguistic analysis will maximize the potential for corpus re-use in a wide range of applications. We foresee the advantage of enriching the text with part-of-speech tags of very fine-grained grammatical distinctions, which reflect expert interest in syntax and morphology, but not specific needs of end-users, because end-user applications are not known in advance. In this paper we review existing Arabic Part-of-Speech Taggers and tag-sets, and illustrate four different Arabic PoS tag-sets for a sample of Arabic text from the Quran. We describe the detailed fine-grained morphological feature tag set of Arabic, and the fine-grained Arabic morphological analyzer algorithm. We faced practical challenges in applying the morphological analyzer to the 100-million-word Web Arabic Corpus: we had to port the software to the National Grid Service, adapt the analyser to cope with spelling variations and errors, and utilise a Broad-Coverage Lexical Resource combining 23 traditional Arabic lexicons. Finally we outline the construction of a Gold Standard for comparative evaluation.
Natural language understanding systems require a knowledge base provided with conceptual representations reflecting the structure of human beings cognitive system. Although surface semantics can be sufficient in some other systems, the construction of a robust knowledge base guarantees its use in most natural language processing applications, consolidating thus the concept of resource reuse. In this scenario, FunGramKB is presented as a multipurpose knowledge base whose model has been particularly designed for natural language understanding tasks. The theoretical basement of this knowledge engineering project lies in the construction of two complementary types of interlingua: the conceptual logical structure, i.e. a lexically-driven interlingua which can predict linguistic phenomena according to the Role and Reference Grammar syntax-semantics interface, and the COREL scheme, i.e. a concept-oriented interlingua on which our rule-based reasoning engine is able to make inferences effectively. The objective of the paper is to describe the different conceptual, lexical and grammatical modules which make up the architecture of FunGramKB, together with an exploratory outline on how to exploit such a knowledge base within an NLP system.
In anticipation of upcoming mobile telephony services with higher speech quality, a wideband (50 Hz to 7 kHz) mobile telephony derivative of TIMIT has been recorded called WTIMIT. It opens up various scientific investigations; e.g., on speech quality and intelligibility, as well as on wideband upgrades of network-side interactive voice response (IVR) systems with retrained or bandwidth-extended acoustic models for automatic speech recognition (ASR). Wideband telephony could enable network-side speech recognition applications such as remote dictation or spelling without the need of distributed speech recognition techniques. The WTIMIT corpus was transmitted via two prepared Nokia 6220 mobile phones over T-Mobile's 3G wideband mobile network in The Hague, The Netherlands, employing the Adaptive Multirate Wideband (AMR-WB) speech codec. The paper presents observations of transmission effects and phoneme recognition experiments. It turns out that in the case of wideband telephony, server-side ASR should not be carried out by simply decimating received signals to 8 kHz and applying existent narrowband acoustic models. Nor do we recommend just simulating the AMR-WB codec for training of wideband acoustic models. Instead, real-world wideband telephony channel data (such as WTIMIT) provides the best training material for wideband IVR systems.
Since the first half of the 20th century, readability formulas have been widely employed to automatically predict the readability of an unseen text. In this article, the formulas and the text characteristics they are composed of are evaluated in the context of large Dutch and English corpora. We describe the behaviour of the formulas and the text characteristics by means of correlation matrices and a principal component analysis, and test the methodological validity of the formulas by means of collinearity tests. Both the correlation matrices and the principal component analysis show that the formulas described in this paper strongly correspond, regardless of the language for which they were designed. Furthermore, the collinearity test reveals shortcomings in the methodology that was used to create some of the existing readability formulas. All of this leads us to conclude that a new readability prediction method is needed. We finally make suggestions to come to a cleaner methodology and present web applications that will help us collect data to compile a new gold standard for readability prediction.
Broad-coverage language resources which provide prior linguistic knowledge must improve the accuracy and the performance of NLP applications. We are constructing a broad-coverage lexical resource to improve the accuracy of morphological analyzers and part-of-speech taggers of Arabic text. Over the past 1200 years, many different kinds of Arabic language lexicons were constructed; these lexicons are different in ordering, size and aim or goal of construction. We collected 23 machine-readable lexicons, which are freely available on the web. We combined lexical resources into one large broad-coverage lexical resource by extracting information from disparate formats and merging traditional Arabic lexicons. To evaluate the broad-coverage lexical resource we computed coverage over the Quran, the Corpus of Contemporary Arabic, and a sample from the Arabic Web Corpus, using two methods. Counting exact word matches between test corpora and lexicon scored about 65-68{\%}; Arabic has a rich morphology with many combinations of roots, affixes and clitics, so about a third of words in the corpora did not have an exact match in the lexicon. The second approach is to compute coverage in terms of use in a lemmatizer program, which strips clitics to look for a match for the underlying lexeme; this scored about 82-85{\%}.
This paper presents the large audiovisual laughter database recorded as part of the AVLaughterCycle project held during the eNTERFACE09 Workshop in Genova. 24 subjects participated. The freely available database includes audio signal and video recordings as well as facial motion tracking, thanks to markers placed on the subjects face. Annotations of the recordings, focusing on laughter description, are also provided and exhibited in this paper. In total, the corpus contains more than 1000 spontaneous laughs and 27 acted laughs. The laughter utterances are highly variable: the laughter duration ranges from 250ms to 82s and the sounds cover voiced vowels, breath-like expirations, hum-, hiccup- or grunt-like sounds, etc. However, as the subjects had no one to interact with, the database contains very few speech-laughs. Acted laughs tend to be longer than spontaneous ones and are more often composed of voiced vowels. The database can be useful for automatic laughter processing or cognitive science works. For the AVLaughterCycle project, it has served to animate a laughing virtual agent with an output laugh linked to the conversational partners input laugh.
We present our efforts to create a large-scale, semi-automatically annotated parallel corpus of cleft constructions. The corpus is intended to reduce or make more effective the manual task of finding examples of clefts in a corpus. The corpus is being developed in the context of the Collaborative Research Centre SFB 632, which is a large, interdisciplinary research initiative to study information structure, at the University of Potsdam and the Humboldt University in Berlin. The corpus is based on the Europarl corpus (version 3). We show how state-of-the-art NLP tools, like POS taggers and statistical dependency parsers, may facilitate powerful and precise searches. We argue that identifying clefts using automatically added syntactic structure annotation is ultimately to be preferred over using lower level, though more robust, extraction methods like regular expression matching. An evaluation of the extraction method for one of the languages also offers some support for this method. We end the paper by discussing the resulting corpus itself. We present some examples of interesting clefts and translational counterparts from the corpus and suggest ways of exploiting our newly created resource in the cross-linguistic study of clefts.
Determining semantic relatedness between words or concepts is a fundamental process to many Natural Language Processing applications. Approaches for this task typically make use of knowledge resources such as WordNet and Wikipedia. However, these approaches only make use of limited number of features extracted from these resources, without investigating the usefulness of combining various different features and their importance in the task of semantic relatedness. In this paper, we propose a random walk model based approach to measuring semantic relatedness between words or concepts, which seamlessly integrates various features extracted from Wikipedia to compute semantic relatedness. We empirically study the usefulness of these features in the task, and prove that by combining multiple features that are weighed according to their importance, our system obtains competitive results, and outperforms other systems on some datasets.
In this paper, we present a simple protocol to evaluate word aligners on bilingual lexicon induction tasks from parallel corpora. Rather than resorting to gold standards, it relies on a comparison of the outputs of word aligners against a reference bilingual lexicon. The quality of this reference bilingual lexicon does not need to be particularly high, because evaluation quality is ensured by systematically filtering this reference lexicon with the parallel corpus the word aligners are trained on. We perform a comparison of three freely available word aligners on numerous language pairs from the Bible parallel corpus (Resnik et al., 1999): MGIZA++ (Gao and Vogel, 2008), BerkeleyAligner (Liang et al., 2006), and Anymalign (Lardilleux and Lepage, 2009). We then select the most appropriate one to produce bilingual lexicons for all language pairs of this corpus. These involve Cebuano, Chinese, Danish, English, Finnish, French, Greek, Indonesian, Latin, Spanish, Swedish, and Vietnamese. The 66 resulting lexicons are made freely available.
Repetition is a common concept in human communication. This paper investigates possible benefits of repetition for automatic speech recognition under controlled conditions. Testing is performed on the newly created Autonomata TOO speech corpus, consisting of multilingual names for Points-Of-Interest as spoken by both native and non-native speakers. During corpus recording, ASR was being performed under baseline conditions using a Nuance Vocon 3200 system. On failed recognition, additional attempts for the same utterances were added to the corpus. Substantial improvements in recognition results are shown for all categories of speakers and utterances, even if speakers did not noticeably alter their previously misrecognized pronunciation. A categorization is proposed for various types of differences between utterance realisations. The number of attempts, the pronunciation of an utterance over multiple attempts compared to both previous attempts and reference pronunciation is analyzed for difference type and frequency. Variables such as the native language of the speaker and the languages in the lexicon are taken into account. Possible implications for ASR research are discussed.
This study examines the relationship between two kinds of semantic spaces ― i.e., spaces based on term frequency (tf) and word cooccurrence frequency (co) ― and four semantic relations ― i.e., synonymy, coordination, superordination, and collocation ― by comparing, for each semantic relation, the performance of two semantic spaces in predicting word association. The simulation experiment demonstrates that the tf-based spaces perform better in predicting word association based on the syntagmatic relation (i.e., superordination and collocation), while the co-based semantic spaces are suited for predicting word association based on the paradigmatic relation (i.e., synonymy and coordination). In addition, the co-based space with a larger context size yields better performance for the syntagmatic relation, while the co-based space with a smaller context size tends to show better performance for the paradigmatic relation. These results indicate that different semantic spaces can be used depending on what kind of semantic relatedness should be computed.
Current state-of-the-art systems for automatic phonetic transcription (APT) are mostly phone recognizers based on Hidden Markov models (HMMs). We present a different approach for APT especially designed for transcription with a large inventory of phonetic symbols. In contrast to most systems which are model-based, our approach is non-parametric using techniques derived from concatenative speech synthesis and template-based speech recognition. This example-based approach not only produces draft transcriptions that just need to be corrected instead of created from scratch but also provides a validation mechanism for ensuring consistency within the corpus. Implementations of this transcription framework are available as standalone Java software and extension to the ELAN linguistic annotation software. The transcription system was tested with audio files and reference transcriptions from the Austrian Pronunciation Database (ADABA) and compared to an HMM-based system trained on the same data set. The example-based and the HMM-based system achieve comparable phone recognition rates. A combination of rule-based and example-based APT in a constrained phone recognition scenario returned the best results.
This paper presents a corpus of human answers in natural language collected in order to build a base of examples useful when generating natural language answers. We present the corpus and the way we acquired it. Answers correspond to questions with fixed linguistic form, focus, and topic. Answers to a given question exist for two modalities of interaction: oral and written. The whole corpus of answers was annotated manually and automatically on different levels including words from the questions being reused in the answer, the precise element answering the question (or information-answer), and completions. A detailed description of the annotations is presented. Two examples of corpus analyses are described. The first analysis shows some differences between oral and written modality especially in terms of length of the answers. The second analysis concerns the reuse of the question focus in the answers.
Corpus-based dialogue systems rely on statistical models, whose parameters are inferred from annotated dialogues. The dialogues are usually annotated in terms of Dialogue Acts (DA), and the manual annotation is difficult (as annotation rule are hard to define), error-prone and time-consuming. Therefore, several semi-automatic annotation processes have been proposed to speed-up the process and consequently obtain a dialogue system in less total time. These processes are usually based on statistical models. The standard statistical annotation model is based on Hidden Markov Models (HMM). In this work, we explore the impact of different types of HMM, with different number of states, on annotation accuracy. We performed experiments using these models on two dialogue corpora (Dihana and SwitchBoard) of dissimilar features. The results show that some types of models improve standard HMM in a human-computer task-oriented dialogue corpus (Dihana corpus), but their impact is lower in a human-human non-task-oriented dialogue corpus (SwitchBoard corpus).
Biomedical corpora annotated with event-level information provide an important resource for the training of domain-specific information extraction (IE) systems. These corpora concentrate primarily on creating classified, structured representations of important facts and findings contained within the text. However, bio-event annotations often do not take into account additional information (meta-knowledge) that is expressed within the textual context of the bio-event, e.g., the pragmatic/rhetorical intent and the level of certainty ascribed to a particular bio-event by the authors. Such additional information is indispensible for correct interpretation of bio-events. Therefore, an IE system that simply presents a list of bare bio-events, without information concerning their interpretation, is of little practical use. We have addressed this sparseness of meta-knowledge available in existing bio-event corpora by developing a multi-dimensional annotation scheme tailored to bio-events. The scheme is intended to be general enough to allow integration with different types of bio-event annotation, whilst being detailed enough to capture important subtleties in the nature of the meta-knowledge expressed about different bio-events. To our knowledge, our scheme is unique within the field with regards to the diversity of meta-knowledge aspects annotated for each event.
We will look at how maps can be integrated in research resources, such as language databases and language corpora. By using maps, search results can be illustrated in a way that immediately gives the user information that words or numbers on their own would not give. We will illustrate with two different resources, into which we have now added a Google Maps application: The Nordic Dialect Corpus (Johannessen et al. 2009) and The Nordic Syntactic Judgments Database (Lindstad et al. 2009). We have integrated Google Maps into these applications. The database contains some hundred syntactic test sentences that have been evaluated by four speakers in more than hundred locations in Norway and Sweden. Searching for the evaluations of a particular sentence gives a list of several hundred judgments, which are difficult for a human researcher to assess. With the map option, isoglosses are immediately visible. We show in the paper that both with the maps depicting corpus hits and with the maps depicting database results, the map visualizations actually show clear geographical differences that would be very difficult to spot just by reading concordance lines or database tables.
Automatic content scoring for free-text responses has started to emerge as an application of Natural Language Processing in its own right, much like question answering or machine translation. The task, in general, is reduced to comparing a students answer to a model answer. Although a considerable amount of work has been done, common benchmarks and evaluation measures for this application do not currently exist. It is yet impossible to perform a comparative evaluation or progress tracking of this application across systems ― an application that we view as a textual entailment task. This paper concentrates on introducing an Educational Testing Service-built test suite that makes a step towards establishing such a benchmark. The suite can be used as regression and performance evaluations both intra-c-rater{\^A}® or inter automatic content scoring technologies. It is important to note that existing textual entailment test suites like PASCAL RTE or FraCas, though beneficial, are not suitable for our purposes since we deal with atypical naturally-occurring student responses that need to be categorized in order to serve as regression test cases.
A large effort has been devoted to the development of textual knowledge acquisition (KA) tools, but it is still difficult to assess the progress that has been made. The results produced by these tools are difficult to compare, due to the heterogeneity of the proposed methods and of their goals. Various experiments have been made to evaluate terminological and ontological tools. They show that in terminology as well as in ontology acquisition, it remains difficult to compare existing tools and to analyse their advantages and drawbacks. From our own experiments in evaluating terminology and ontology acquisition tools, it appeared that the difficulties and solutions are similar for both tasks. We propose a unified approach for the evaluation of textual KA tools that can be instantiated in different ways for various tasks. The main originality of this approach lies in the way it takes into account the subjectivity of evaluation and the relativity of gold standards. In this paper, we highlight the major difficulties of KA evaluation, we then present a unified proposal for the evaluation of terminologies and ontologies acquisition tools and the associated experiments. The proposed protocols take into consideration the specificity of this type of evaluation.
Language users are increasingly turning to electronic resources to address their lexical information needs, due to their convenience and their ability to simultaneously capture different facets of lexical knowledge in a single interface. In this paper, we discuss techniques to respond to a user's lexical queries by providing multilingual and multimodal information, and facilitating navigating along different types of links. To this end, structured information from sources like WordNet, Wikipedia, Wiktionary, as well as Web services is linked and integrated to provide a multi-faceted yet consistent response to user queries. The meanings of words in many different languages are characterized by mapping them to appropriate WordNet sense identifiers and adding multilingual gloss descriptions as well as example sentences. Relationships are derived from WordNet and Wiktionary to allow users to discover semantically related words, etymologically related words, alternative spellings, as well as misspellings. Last but not least, images, audio recordings, and geographical maps extracted from Wikipedia and Wiktionary allow for a multimodal experience.
n this paper, we outline the methodology we adopted to develop a FrameNet for Italian. The main element of novelty with respect to the original FrameNet is represented by the fact that the creation and annotation of Lexical Units is strictly grounded in distributional information (statistical distribution of verbal subcategorization frames, lexical and semantic preferences of each frame) automatically acquired from a large, dependency-parsed corpus. We claim that this approach allows us to overcome some of the shortcomings of the classical lexicographic method used to create FrameNet, by complementing the accuracy of manual annotation with the robustness of data on the global distributional patterns of a verb. In the paper, we describe our method for extracting distributional data from the corpus and the way we used it for the encoding and annotation of LUs. The long-term goal of our project is to create an electronic lexicon for Italian similar to the original English FrameNet. For the moment, we have developed a database of syntactic valences that will be made freely accessible via a web interface. This represents an autonomous resource besides the FrameNet lexicon, of which we have a beginning nucleus consisting of 791 annotated sentences.
Spontal-N is a corpus of spontaneous, interactional Norwegian. To our knowledge, it is the first corpus of Norwegian in which the majority of speakers have spent significant parts of their lives in Sweden, and in which the recorded speech displays varying degrees of interference from Swedish. The corpus consists of studio quality audio- and video-recordings of four 30-minute free conversations between acquaintances, and a manual orthographic transcription of the entire material. On basis of the orthographic transcriptions, we automatically annotated approximately 50 percent of the material on the phoneme level, by means of a forced alignment between the acoustic signal and pronunciations listed in a dictionary. Approximately seven percent of the automatic transcription was manually corrected. Taking the manual correction as a gold standard, we evaluated several sources of pronunciation variants for the automatic transcription. Spontal-N is intended as a general purpose speech resource that is also suitable for investigating phonetic detail.
The paper presents Bulgarian National Corpus project (BulNC) - a large-scale, representative, online available corpus of Bulgarian. The BulNC is also a monolingual general corpus, fully morpho-syntactically (and partially semantically) annotated, and manually provided with detailed meta-data descriptions. Presently the Bulgarian National corpus consists of about 320 000 000 graphical words and includes more than 10 000 samples. Briefly the corpus structure and the accepted criteria for representativeness and well-balancing are presented. The query language for advance search of collocations and concordances is demonstrated with some examples - it allows to retrieve word combinations, ordered queries, inflexionally and semantically related words, part-of-speech tags, utilising Boolean operations and grouping as well. The BulNC already plays a significant role in natural language processing of Bulgarian contributing to scientific advances in spelling and grammar checking, word sense disambiguation, speech recognition, text categorisation, topic extraction and machine translation. The BulNC can also be used in different investigations going beyond the linguistics: library studies, social sciences research, teaching methods studies, etc.
With the development of the Internet environments, more and more language services become accessible for common people. However, the gap between human translators and machine translators remains huge especially for the domain of localization processes that requires high translation quality. Although efforts of combining human and machine translators for supporting multilingual communication have been reported in previous research, how to apply such approaches for improving localization processes are rarely discussed. In this paper, we aim at improving localization processes by composing human and machine translation services based on the Language Grid, which is a language service platform that we have developed. Further, we conduct experiments to compare the translation quality and translation cost using several translation processes, including absolute machine translation processes, absolute human translation processes and translation processes by human and machine translation services. The experiment results show that composing monolingual roles and dictionary services improves the translation quality of machine translators, and that collaboration of human and machine translators is possible to reduce the cost comparing with the absolute bilingual human translation. We also discuss the generality of the experimental results and further challenging issues of the proposed localization processes.
In this paper, we present a morphosyntactic tagset for Afrikaans based on the guidelines developed by the Expert Advisory Group on Language Engineering Standards (EAGLES). We compare our slim yet expressive tagset, MAATS (Morphosyntactic AfrikAans TagSet), with an existing one which primarily focuses on a detailed morphosyntactic and semantic description of word forms. MAATS will primarily be used for the extraction of lexical data from large pos-tagged corpora. We not only focus on morphosyntactic properties but also on the processability with statistical tagging. We discuss the tagset design and motivate our classification of Afrikaans word forms, in particular we focus on the categorization of verbs and conjunctions. The complete tagset in presented and we briefly discuss each word class. In a case study with an Afrikaans newspaper corpus, we evaluate our tagset with four different statistical taggers. Despite a relatively small amount of training data, however with a large tagger lexicon, TnT-Tagger scores 97.05 {\%} accuracy. Additionally, we present some error sources and discuss future work.
Emotion processing has always been a great challenge. Given the fact that an emotion is triggered by cause events and that cause events are an integral part of emotion, this paper constructs a Chinese emotion cause corpus as a first step towards automatic inference of cause-emotion correlation. The corpus focuses on five primary emotions, namely happiness, sadness, fear, anger, and surprise. It is annotated with emotion cause events based on our proposed annotation scheme. Corpus data shows that most emotions are expressed with causes, and that causes mostly occur before the corresponding emotion verbs. We also examine the correlations between emotions and cause events in terms of linguistic cues: causative verbs, perception verbs, epistemic markers, conjunctions, prepositions, and others. Results show that each group of linguistic cues serves as an indicator marking the cause events in different structures of emotional constructions. We believe that the emotion cause corpus will be the useful resource for automatic emotion cause detection as well as emotion detection and classification.
This paper deals with the uses of the annotations of third person singular neuter pronouns in the DAD parallel and comparable corpora of Danish and Italian texts and spoken data. The annotations contain information about the functions of these pronouns and their uses as abstract anaphora. Abstract anaphora have constructions such as verbal phrases, clauses and discourse segments as antecedents and refer to abstract objects comprising events, situations and propositions. The analysis of the annotated data shows the language specific characteristics of abstract anaphora in the two languages compared with the uses of abstract anaphora in English. Finally, the paper presents machine learning experiments run on the annotated data in order to identify the functions of third person singular neuter personal pronouns and neuter demonstrative pronouns. The results of these experiments vary from corpus to corpus. However, they are all comparable with the results obtained in similar tasks in other languages. This is very promising because the experiments have been run on both written and spoken data using a classification of the pronominal functions which is much more fine-grained than the classifications used in other studies.
We present LIPS (Lexical Isolation Point Software), a tool for accurate lexical isolation point (IP) prediction in recordings of speech. The IP is the point in time in which a word is correctly recognised given the acoustic evidence available to the hearer. The ability to accurately determine lexical IPs is of importance to work in the field of cognitive processing, since it enables the evaluation of competing models of word recognition. IPs are also of importance in the field of neurolinguistics, where the analyses of high-temporal-resolution neuroimaging data require a precise time alignment of the observed brain activity with the linguistic input. LIPS provides an attractive alternative to costly multi-participant perception experiments by automatically computing IPs for arbitrary words. On a test set of words, the LIPS system predicts IPs with a mean difference from the actual IP of within 1ms. The difference from the predicted and actual IP approximate to a normal distribution with a standard deviation of around 80ms (depending on the model used).
Investigating differences in linguistic usage between individuals who have suffered brain injury (hereafter patients) and those who havent can yield a number of benefits. It provides a better understanding about the precise way in which impairments affect patients language, improves theories of how the brain processes language, and offers heuristics for diagnosing certain types of brain damage based on patients speech. One method for investigating usage differences involves the analysis of spontaneous speech. In the work described here we construct a text corpus consisting of transcripts of individuals speech produced during two tasks: the Boston-cookie-theft picture description task (Goodglass and Kaplan, 1983) and a spontaneous speech task, which elicits a semi-prompted monologue, and/or free speech. Interviews with patients from 19yrs to 89yrs were transcribed, as were interviews with a comparable number of healthy individuals (20yrs to 89yrs). Structural brain images are available for approximately 30{\%} of participants. This unique data source provides a rich resource for future research in many areas of language impairment and has been constructed to facilitate analysis with natural language processing and corpus linguistics techniques.
Enhanced Publications are a new way to publish scientific and other results in an electronic article. The advantage of EPs is that the relation between the article and the underlying data facilitate the peer review process and other quality assessment activities. Due to the link between de publication and the research data the publication can be much richer than a paper edition permits. We present an example of EPs in which links are made to interview fragments that include transcripts, audio segments, annotations and metadata. EPs call for a new paradigm of research methodology in which digital persistent access to research data are a central issue. In this contribution we highlight 1. The research data as it is archived and curated, 2. the concept ''``enhanced publication'''' and its scientific value, 3. the ''``fragment fitter tool'''', a language processing tool to facilitate the creation of EPs, 4. IPR issues related to the re-use of the interview data.
Questions are not asked in isolation. Their context, viz. the preceding interactions, might be of help to understand them and retrieve the correct answer. Previous research in Interactive Question Answering showed that context fusion has a big potential to improve the performance of answer retrieval. In this paper, we study how much context, and what elements of it, should be considered to answer Follow-Up Questions (FU Qs). Following previous research, we exploit Logistic Regression Models to learn aspects of dialogue structure relevant to answering FU Qs. We enrich existing models based on shallow features with deep features, relying on the theory of discourse structure of (Chai and Jin, 2004), and on Centering Theory, respectively. Using models trained on realistic IQA data, we show which of the various theoretically motivated features hold up against empirical evidence. We also show that, while these deep features do not outperform the shallow ones on their own, an IQA system's answer correctness increases if the shallow and deep features are combined.
Conventional methods for disambiguation problems have been using statistical methods with co-occurrence of words in their contexts. It seems that human-beings assign appropriate word senses to the given ambiguous word in the sentence depending on the words which followed the ambiguous word when they could not disambiguate by using the previous contextual information. In this research, Contextual Dynamic Network Model is developed using the Associative Concept Dictionary which includes semantic relations among concepts/words and the relations can be represented with quantitative distances among them. In this model, an interactive activation method is used to identify a words meaning on the Contextual Semantic Network where the activation values on the network are calculated using the distances. The proposed method constructs dynamically the Contextual Semantic Network according to the input words sequentially that appear in the sentence including an ambiguous word. Therefore, in this research, after the model calculates the activation values, if there is little difference between the activation values, it reconstructs the network depending on the next words in input sentence. The evaluation of proposed method showed that the accuracy rates are high when Contextual Semantic Network has high density whose node are extended using around the ambiguous word.
In this paper, we report on the design of a part-of-speech-tagset for Wolof and on the creation of a semi-automatically annotated gold standard. In order to achieve high-quality annotation relatively fast, we first generated an accurate lexicon that draws on existing word and name lists and takes into account inflectional and derivational morphology. The main motivation for the tagged corpus is to obtain data for training automatic taggers with machine learning approaches. Hence, we took machine learning considerations into account during tagset design and we present training experiments as part of this paper. The best automatic tagger achieves an accuracy of 95.2{\%} in cross-validation experiments. We also wanted to create a basis for experimenting with annotation projection techniques, which exploit parallel corpora. For this reason, it was useful to use a part of the Bible as the gold standard corpus, for which sentence-aligned parallel versions in many languages are easy to obtain. We also report on preliminary experiments exploiting a statistical word alignment of the parallel text.
In this paper we present a description of negation cues and their scope in biomedical texts, based on the cues that occur in the BioScope corpus. We provide information about the morphological type of the cue, the characteristics of the scope in relation to the morpho-syntactic features of the cue and of the clause, and the ambiguity level of the cue by describing in which cases certain negation cues do not express negation. Additionally, we provide positive and negative examples per cue from the BioScope corpus. We show that the scope depends mostly on the part-of-speech of the cue and on the syntactic features of the clause. Although several studies have focused on processing negation in biomedical texts, we are not aware of publicly available resources that describe the scope of negation cues in detail. This paper aims at providing information for producing guidelines to annotate corpora with a negation layer, and for building resources that find the scope of negation cues automatically.
The current study presents a conversion and unification of the Penn Discourse TreeBank 2.0 (PDTB) and the Penn TreeBank (PTB) under XML format. The main goal of the PDTB XML is to create a tool for efficient and broad querying of the syntax and discourse information simultaneously. The key stages of the project are developing proper cross-references between different data types and their representation in the modified TIGER-XML format, and then writing the required declarative languages (XML Schema). PTB XML is compatible with TIGER-XML format. The PDTB XML is developed as a unified format for the convenience of XQuery users; it integrates discourse relations and XML structures into one unified hierarchy and builds the cross references between the syntactic trees and the discourse relations. The syntactic and discourse elements are assigned with unique IDs in order to build cross-references between them. The converted corpus allows for a simultaneous search for syntactically specified discourse information based on the XQuery standard, which is illustrated with a simple example in the article.
The paper presents a corpus of Polish spoken dialogues annotated on several levels, from transcription of dialogues and their morphosyntactic analysis, to semantic annotation. The LUNA.PL corpus is the first semantically annotated corpus of Polish spontaneous speech. It contains 500 dialogues recorded at the Warsaw Transport Authority call centre. For each dialogue, the corpus contains recorded audio signal, its transcription and five XML files with annotations on subsequent levels. Speech transcription was done manually. Text annotation was constructed using a combination of rule based programmes and computer-aided manual work. For morphological annotation we used the already existing analyzer and manually disambiguated the results. Morphologically annotated texts of dialogues were automatically segmented into elementary syntactic chunks. Semantic annotation was done by a set of specially designed rules and then manually corrected. The paper describes details of the domain related semantic annotation which consists of two levels - concept level at which around 200 attributes and their values are annotated, and predicate level at which 47 frame types are recognized. We describe the domain model accepted, and the statistics over the entire annotated set of dialogues.
Sense-tagged corpora are used to evaluate word sense disambiguation (WSD) systems. Manual creation of such resources is often prohibitively expensive. That is why the concept of pseudowords - conflations of two or more unambiguous words - has been integrated into WSD evaluation experiments. This paper presents a new method of pseudoword generation which takes into account semantic-relatedness of the candidate words forming parts of the pseudowords to the particular senses of the word to be disambiguated. We compare the new approach to its alternatives and show that the results on pseudowords, that are more similar to real ambiguous words, better correspond to the actual results. Two techniques assessing the similarity are studied - the first one takes advantage of manually created dictionaries (wordnets), the second one builds on the automatically computed statistical data obtained from large corpora. Pros and cons of the two techniques are discussed and the results on a standard task are demonstrated.
We present a set of tools and resources for the analysis of interviews during psychotherapy sessions. One of the main components of the work is a dictionary-based text interpretation tool for the Spanish language. The tool is designed to identify a subset of Freudian drives in patient and therapist discourse.
Language resources are typically defined and created for application in speech technology contexts, but the documentation of languages which are unlikely ever to be provided with enabling technologies nevertheless plays an important role in defining the heritage of a speech community and in the provision of basic insights into the language oriented components of human cognition. This is particularly true of endangered languages. The present case study concerns the documentation both of the birth and of the endangerment within a rather short space of time of a spirit language, Medefaidrin, created and used as a vehicular language by a religious community in South-Eastern Nigeria. The documentation shows phonological, orthographic, morphological, syntactic and textual typological features of Medefaidrin which indicate that typological properties of English were a model for the creation of the language, rather than typological properties of the enclaving language, Ibibio. The documentation is designed as part of the West African Language Archive (WALA), following OLAC metadata standards.
This paper proposes a simple and fast person-name filter, which plays an important role in automatic compilation of a large bilingual person-name lexicon. This filter is based on pn{\_}score, which is the sum of two component scores, the score of the first name and that of the last name. Each score is calculated from two term sets: one is a dense set in which most of the members are person names; another is a baseline set that contains less person names. The pn{\_}score takes one of five values, {+2, +1, 0, -1, -2}, which correspond to strong positive, positive, undecidable, negative, and strong negative, respectively. This pn{\_}score can be easily extended to bilingual pn{\_}score that takes one of nine values, by summing scores of two languages. Experimental results show that our method works well for monolingual person names in English and Japanese; the F-score of each language is 0.929 and 0.939, respectively. The performance of the bilingual person-name filter is better; the F-score is 0.955.
This paper describes the first phase of building a lexicon of Egyptian Cairene Arabic (ECA) ― one of the most widely understood dialects in the Arab World ― and Modern Standard Arabic (MSA). Each ECA entry is mapped to its MSA synonym, Part-of-Speech (POS) tag and top-ranked contexts based on Web queries; and thus each entry is provided with basic syntactic and semantic information for a generic lexicon compatible with multiple NLP applications. Moreover, through their MSA synonyms, ECA entries acquire access to MSA available NLP tools and resources which are considerably available. Using an associationist approach based on the correlations between word co-occurrence patterns in both dialects, we change the direction of the acquisition process from parallel to circular to overcome a bottleneck of current research on Arabic dialects, namely the lack of parallel corpora, and to alleviate accuracy rates for using unrelated Web documents which are more frequently available. Manually evaluated for 1,000 word entries by two native speakers of the ECA-MSA varieties, the proposed approach achieves a promising F-measured performance rate of 70.9{\%}. In discussion to the proposed algorithm, different semantic issues are highlighted for upcoming phases of the induction of a more comprehensive ECA-MSA lexicon.
We provide a detailed look on the functioning of the OwlSpeak Spoken Dialogue Manager, which is part of the EU-funded project ATRACO. OwlSpeak interprets Spoken Dialogue Ontologies and on this basis generates VoiceXML dialogue snippets. The dialogue snippets can be interpreted by all speech servers that provide VoiceXML support and therefore make the dialogue management independent from the hosting systems providing speech recognition and synthesis. Ontologies are used within the framework of our prototype to represent specific spoken dialogue domains that can dynamically be broadened or tightened during an ongoing dialogue. We provide an exemplary dialogue encoded as OWL model and explain how this model is interpreted by the dialogue manager. The combination of a unified model for dialogue domains and the strict model-view-controller architecture that underlies the dialogue manager lead to an efficient system that allows for a new way of spoken dialogue system development and can be used for further research on adaptive spoken dialogue strategies.
The SignSpeak project will be the first step to approach sign language recognition and translation at a scientific level already reached in similar research fields such as automatic speech recognition or statistical machine translation of spoken languages. Deaf communities revolve around sign languages as they are their natural means of communication. Although deaf, hard of hearing and hearing signers can communicate without problems amongst themselves, there is a serious challenge for the deaf community in trying to integrate into educational, social and work environments. The overall goal of SignSpeak is to develop a new vision-based technology for recognizing and translating continuous sign language to text. New knowledge about the nature of sign language structure from the perspective of machine recognition of continuous sign language will allow a subsequent breakthrough in the development of a new vision-based technology for continuous sign language recognition and translation. Existing and new publicly available corpora will be used to evaluate the research progress throughout the whole project.
In this paper, we propose a method for automatic term recognition (ATR) which uses the statistical differences of relative frequencies of terms in target domain corpus and elsewhere. Generally, the target terms appear more frequently in target domain corpus than in other domain corpora. Utilizing such characteristics will lead to the improvement of extraction performance. Most of the ATR methods proposed so far only use the target domain corpus and do not take such characteristics into account. For the extraction experiment, we used the abstracts of a women's studies journal as a target domain corpus and those of academic journals of 39 domains as other domain corpora. The women's studies terms which were used for extraction evaluation were manually identified terms in the abstracts. The extraction performance was analyzed and we found that our method outperformed earlier methods. The previous methods were based on C-value, FLR and methods which were also used with other domain corpora.
We describe work in progress on the development of a full syntactic parser for Romanian. This work is part of a larger project of multilingual extension of the Fips parser (Wehrli, 2007), already available for French, English, German, Spanish, Italian, and Greek, to four new languages (Romanian, Romansh, Russian and Japanese). The Romanian version was built by starting with the Fips generic parsing architecture for the Romance languages and customising the grammatical component, in close relation to the development of the lexical component. We describe this process and report on preliminary results obtained for journalistic texts.
We present the Spontal database of spontaneous Swedish dialogues. 120 dialogues of at least 30 minutes each have been captured in high-quality audio, high-resolution video and with a motion capture system. The corpus is currently being processed and annotated, and will be made available for research at the end of the project.
This paper describes the development of a structured document collection containing user-generated text and numerical metadata for exploring the exploitation of metadata in information retrieval (IR). The collection consists of more than 61,000 documents extracted from YouTube video pages on basketball in general and NBA (National Basketball Association) in particular, together with a set of 40 topics and their relevance judgements. In addition, a collection of nearly 250,000 user profiles related to the NBA collection is available. Several baseline IR experiments report the effect of using video-associated metadata on retrieval effectiveness. The results surprisingly show that searching the videos titles only performs significantly better than searching additional metadata text fields of the videos such as the tags or the description.
We describe a set of tools, resources, and experiments for opinion classification in business-related datasources in two languages. In particular we concentrate on SentiWordNet text interpretation to produce word, sentence, and text-based sentiment features for opinion classification. We achieve good results in experiments using supervised learning machine over syntactic and sentiment-based features. We also show preliminary experiments where the use of summaries before opinion classification provides competitive advantage over the use of full documents.
Typical broadcast material contains not only studio-recorded texts read by trained speakers, but also spontaneous and dialect speech, debates with cross-talk, voice-overs, and on-site reports with difficult acoustic environments. Standard approaches to speech and speaker recognition usually deteriorate under such conditions. This paper reports on the design, construction, and experimental analysis of DiSCo, a German corpus for the evaluation of speech and speaker recognition on challenging material from the broadcast domain. One of the key requirements for the design of this corpus was a good coverage of different types of serious programmes beyond clean speech and planned speech broadcast news. Corpus annotation encompasses manual segmentation, an orthographic transcription, and labelling with speech mode, dialect, and noise type. We indicate typical use cases for the corpus by reporting results from ASR, speech search, and speaker recognition on the new corpus, thereby obtaining insights into the difficulty of audio recognition on the various classes.
In this paper, we discuss the theoretical, sociolinguistic, methodological and technical objectives and issues of the French Creagest Project (2007-2012) in setting up, documenting and annotating a large corpus of adult and child French Sign Language (LSF) and of natural gestural language. The main objective of this ANR-funded research project is to set up a collaborative web-based platform for the study of semiogenesis in LSF (French Sign Language), i.e. the study of emerging structures and signs, be they used by Deaf adult signers, Deaf children, or even by Deaf and hearing subjects in interaction. In section 2, we address theoretical and practical issues, emphasizing the outstanding features of the Creagest Project. In section 3, we deal with methodological issues for data collection. Finally, in section 4, we examine technical aspects of LSF video data editing and corpus annotation, in the perspective of setting up a corpus-based formalized description of LSF.
A key aspect in the development of statistical translators is the synergic combination of different sources of knowledge. This work describes the effect and implications that would have adding additional other-than-voice information in a voice translation system. In the model discussed the additional information serves as the bases for the log-linear combination of several statistical models. A prototype that implements a real-time speech translation system from Spanish to English that is adapted to specific teaching-related environments is presented. In the scenario of analysis a teacher as speaker giving an educational class could use a real time translation system with foreign students. The teacher could add slides or class notes as additional reference to the voice translation system. Should notes be already translated into the destination language the system could have even more accuracy. We present the theoretical framework of the problem, summarize the overall architecture of the system, show how the system is enhanced with capabilities related to capturing the additional information; and finally present the initial performance results.
Accentological corpus provides a researcher an opportunity to study word stress and stress variation, which are very important for the Russian language. Moreover, Accentological corpus allows studying the history of the Russian language stress development. The research presents the main characteristics of Accentological corpus available at ruscorpora.ru. Corpora size, type and sources of text material, the way it is represented in the corpora, types of linguistic annotation, corpora composition and ways of their effective use according to their purposes are described. There are two zones in the Accentological corpus. 1) The zone of prose includes oral texts and films transcripts, in which stressed syllables are marked according to the real pronunciation. 2) The zone of poetry contains texts with marked accented syllables, so it is possible to define the exact word stress using special rules. The Accentological corpus has four types of annotations (metatextual, morphological, semantic and sociological) and also has its own accentological mark-up. Due to accentological annotation each word is supplied with stress marks, so a user can make queries and retrieve the stressed or unstressed word forms in combination with grammatical and semantic features.
We introduce CCASH (Cost-Conscious Annotation Supervised by Humans), an extensible web application framework for cost-efficient annotation. CCASH provides a framework in which cost-efficient annotation methods such as Active Learning can be explored via user studies and afterwards applied to large annotation projects. CCASHs architecture is described as well as the technologies that it is built on. CCASH allows custom annotation tasks to be built from a growing set of useful annotation widgets. It also allows annotation methods (such as AL) to be implemented in any language. Being a web application framework, CCASH offers secure centralized data and annotation storage and facilitates collaboration among multiple annotations. By default it records timing information about each annotation and provides facilities for recording custom statistics. The CCASH framework has been used to evaluate a novel annotation strategy presented in a concurrently published paper, and will be used in the future to annotate a large Syriac corpus.
This paper describes our work on developing corpora of three varieties of Viennese for unit selection speech synthesis. The synthetic voices for Viennese varieties, implemented with the open domain unit selection speech synthesis engine Multisyn of Festival will also be released within Festival. The paper especially focuses on two questions: how we selected the appropriate speakers and how we obtained the text sources needed for the recording of these non-standard varieties. Regarding the first one, it turned out that working with a prototypical professional speaker was much more preferable than striving for authenticity. In addition, we give a brief outline about the differences between the Austrian standard and its dialectal varieties and how we solved certain technical problems that are related to these differences. In particular, the specific set of phones applicable to each variety had to be determined by applying various constraints. Since such a set does not serve any descriptive purposes but rather is influencing the quality of speech synthesis, a careful design of such a set was an important task.
In recent years, text classification in sentiment analysis has mostly focused on two types of classification, the distinction between objective and subjective text, i.e. subjectivity detection, and the distinction between positive and negative subjective text, i.e. polarity classification. So far, there has been little work examining the distinction between definite polar subjectivity and indefinite polar subjectivity. While the former are utterances which can be categorized as either positive or negative, the latter cannot be categorized as either of these two categories. This paper presents a small set of domain independent features to detect indefinite polar sentences. The features reflect the linguistic structure underlying these types of utterances. We give evidence for the effectiveness of these features by incorporating them into an unsupervised rule-based classifier for sentence-level analysis and compare its performance with supervised machine learning classifiers, i.e. Support Vector Machines (SVMs) and Nearest Neighbor Classifier (kNN). The data used for the experiments are web-reviews collected from three different domains.
We present a web service-based environment for the use of linguistic resources and tools to address issues of terminology and language varieties. We discuss the architecture, corpus representation formats, components and a chainer supporting the combination of tools into task-specific services. Integrated into this environment, single web services also become part of complex scenarios for web service use. Our web services take for example corpora of several million words as an input on which they perform preprocessing, such as tokenisation, tagging, lemmatisation and parsing, and corpus exploration, such as collocation extraction and corpus comparison. Here we present an example on extraction of single and multiword items typical of a specific domain or typical of a regional variety of German. We also give a critical review on needs and available functions from a user's point of view. The work presented here is part of ongoing experimentation in the D-SPIN project, the German national counterpart of CLARIN.
In order to overcome the fragmentation that affects the field of Language Resources and Technologies, an Open and Distributed Resource Infrastructure is the necessary step for building on each other achievements, integrating resources and technologies and avoiding dispersed or conflicting efforts. Since this endeavour represents a true cultural turnpoint in the LRs field, it needs a careful preparation, both in terms of acceptance by the community and thoughtful investigation of the various technical, organisational and practical aspects implied. To achieve this, we need to act as a community able to join forces on a set of shared priorities and we need to act at a worldwide level. FLaReNet ― Fostering Language Resources Network ― is a Thematic Network funded under the EU eContent program that aims at developing the needed common vision and fostering a European and International strategy for consolidating the sector, thus enhancing competitiveness at EU level and worldwide. In this paper we present the activities undertaken by FLaReNet in order to prepare and support the establishment of such an Infrastructure, which is becoming now a reality within the new MetaNet initiative.
In this paper we present the LREC Map of Language Resources and Tools, an innovative feature introduced with this LREC. The purpose of the Map is to shed light on the vast amount of resources and tools that represent the background of the research presented at LREC, in the attempt to fill in a gap in the community knowledge about the resources and tools that are used or created worldwide. It also aims at a change of culture in the field, actively engaging each researcher in the documentation task about resources. The Map has been developed on the basis of the information provided by LREC authors during the submission of papers to the LREC 2010 conference and the LREC workshops, and contains information about almost 2000 resources. The paper illustrates the motivation behind this initiative, its main characteristics, its relevance and future impact in the field, the metadata used to describe the resources, and finally presents some of the most relevant findings.
Question Answering (QA) technology aims at providing relevant answers to natural language questions. Most Question Answering research has focused on mining document collections containing written texts to answer written questions. In addition to written sources, a large (and growing) amount of potentially interesting information appears in spoken documents, such as broadcast news, speeches, seminars, meetings or telephone conversations. The QAST track (Question-Answering on Speech Transcripts) was introduced in CLEF to investigate the problem of question answering in such audio documents. This paper describes in detail the evaluation protocol and tools designed and developed for the CLEF-QAST evaluation campaigns that have taken place between 2007 and 2009. We first remind the data, question sets, and submission procedures that were produced or set up during these three campaigns. As for the evaluation procedure, the interface that was developed to ease the assessors work is described. In addition, this paper introduces a methodology for a semi-automatic evaluation of QAST systems based on time slot comparisons. Finally, the QAST Evaluation Package 2007-2009 resulting from these evaluation campaigns is also introduced.
We present the Database of Catalan Adjectives (DCA), a database with 2,296 adjective lemmata enriched with morphological, syntactic and semantic information. This set of adjectives has been collected from a fragment of the Corpus Textual Informatitzat de la Llengua Catalana of the Institut dEstudis Catalans and constitutes a representative sample of the adjective class in Catalan as a whole. The database includes both manually coded and automatically extracted information regarding the most prominent properties used in the literature regarding the semantics of adjectives, such as morphological origin, suffix (if any), predicativity, gradability, adjective position with respect to the head noun, adjective modifiers, or semantic class. The DCA can be useful for NLP applications using adjectives (from POS-taggers to Opinion Mining applications) and for linguistic analysis regarding the morphological, syntactic, and semantic properties of adjectives. We now make it available to the research community under a Creative Commons Attribution Share Alike 3.0 Spain license.
This paper presents a word sense disambiguation (WSD) approach based on syntactic and logical representations. The objective here is to run a number of experiments to compare standard contexts (word windows, sentence windows) with contexts provided by a dependency parser (syntactic context) and a logical analyzer (logico-semantic context). The approach presented here relies on a dependency grammar for the syntactic representations. We also use a pattern knowledge base over the syntactic dependencies to extract flat predicative logical representations. These representations (syntactic and logical) are then used to build context vectors that are exploited in the WSD process. Various state-of-the-art algorithms including Simplified Lesk, Banerjee and Pedersen and frequency of co-occurrences are tested with these syntactic and logical contexts. Preliminary results show that defining context vectors based on these features may improve WSD by comparison with classical word and sentence context windows. However, future experiments are needed to provide more evidence over these issues.
Automatic acquisition of novel compounds is notoriously difficult because most novel compounds have relatively low frequency in a corpus. The current study proposes a new method to deal with the novel compound acquisition challenge. We model this task as a two-class classification problem in which a candidate compound is either classified as a compound or a non-compound. A machine learning method using SVM, incorporating two types of linguistically motivated features: semantic features and character features, is applied to identify rare but valid noun compounds. We explore two kinds of training data: one is virtual training data which is obtained by three statistical scores, i.e. co-occurrence frequency, mutual information and dependent ratio, from the frequent compounds; the other is real training data which is randomly selected from the infrequent compounds. We conduct comparative experiments, and the experimental results show that even with limited direct evidence in the corpus for the novel compounds, we can make full use of the typical frequent compounds to help in the discovery of the novel compounds.
For mining intellectual property texts (patents), a broad-coverage lexicon that covers general English words together with terminology from the patent domain is indispensable. The patent domain is very diffuse as it comprises a variety of technical domains (e.g. Human Necessities, Chemistry {\&} Metallurgy and Physics in the International Patent Classification). As a result, collecting a lexicon that covers the language used in patent texts is not a straightforward task. In this paper we describe the approach that we have developed for the semi-automatic construction of a broad-coverage lexicon for classification and information retrieval in the patent domain and which combines information from multiple sources. Our contribution is twofold. First, we provide insight into the difficulties of developing lexical resources for information retrieval and text mining in the patent domain, a research and development field that is expanding quickly. Second, we create a broad coverage lexicon annotated with rich lexical information and containing both general English word forms and domain terminology for various technical domains.
We focus on textual entailments mediated by syntax and propose a new methodology to evaluate textual entailment recognition systems on such data. The main idea is to generate a syntactically annotated corpus of pairs of (non-)entailments and to use error mining methodology from the parsing field to identify the most likely sources of errors. To generate the evaluation corpus we use a template based generation approach where sentences, semantic representations and syntactic annotations are all created at the same time. Furthermore, we adapt the error mining methodology initially proposed for parsing to the field of textual entailment. To illustrate the approach, we apply the proposed methodology to the Afazio RTE system (an hybrid system focusing on syntactic entailment) and show how it permits identifying the most likely sources of errors made by this system on a testsuite of 10 000 (non-)entailment pairs which is balanced in term of (non-)entailment and in term of syntactic annotations.
This paper presents a system for querying treebanks in a uniform way. The system is able to work with both dependency and constituency based treebanks in any language. We demonstrate its abilities on 11 different treebanks. The query language used by the system provides many features not available in other existing systems while still keeping the performance efficient. The paper also describes the conversion of ten treebanks into a common XML-based format used by the system, touching the question of standards and formats. The paper then shows several examples of linguistically interesting questions that the system is able to answer, for example browsing verbal clauses without subjects or extraposed relative clauses, generating the underlying grammar in a constituency treebank, searching for non-projective edges in a dependency treebank, or word-order typology of a language based on the treebank. The performance of several implementations of the system is also discussed by measuring the time requirements of some of the queries.
In this paper we will present work carried out to scale up the system for text understanding called GETARUNS, and port it to be used in dialogue understanding. The current goal is that of extracting automatically argumentative information in order to build argumentative structure. The long term goal is using argumentative structure to produce automatic summarization of spoken dialogues. Very much like other deep linguistic processing systems, our system is a generic text/dialogue understanding system that can be used in connection with an ontology ― WordNet - and other similar repositories of commonsense knowledge. We will present the adjustments we made in order to cope with transcribed spoken dialogues like those produced in the ICSI Berkeley project. In a final section we present preliminary evaluation of the system on two tasks: the task of automatic argumentative labeling and another frequently addressed task: referential vs. non-referential pronominal detection. Results obtained fair much higher than those reported in similar experiments with machine learning approaches.
Arabic is a morphologically rich language, which presents a challenge for part of speech tagging. In this paper, we compare two novel methods for POS tagging of Arabic without the use of gold standard word segmentation but with the full POS tagset of the Penn Arabic Treebank. The first approach uses complex tags that describe full words and does not require any word segmentation. The second approach is segmentation-based, using a machine learning segmenter. In this approach, the words are first segmented, then the segments are annotated with POS tags. Because of the word-based approach, we evaluate full word accuracy rather than segment accuracy. Word-based POS tagging yields better results than segment-based tagging (93.93{\%} vs. 93.41{\%}). Word based tagging also gives the best results on known words, the segmentation-based approach gives better results on unknown words. Combining both methods results in a word accuracy of 94.37{\%}, which is very close to the result obtained by using gold standard segmentation (94.91{\%}).
Microblogging today has become a very popular communication tool among Internet users. Millions of users share opinions on different aspects of life everyday. Therefore microblogging web-sites are rich sources of data for opinion mining and sentiment analysis. Because microblogging has appeared relatively recently, there are a few research works that were devoted to this topic. In our paper, we focus on using Twitter, the most popular microblogging platform, for the task of sentiment analysis. We show how to automatically collect a corpus for sentiment analysis and opinion mining purposes. We perform linguistic analysis of the collected corpus and explain discovered phenomena. Using the corpus, we build a sentiment classifier, that is able to determine positive, negative and neutral sentiments for a document. Experimental evaluations show that our proposed techniques are efficient and performs better than previously proposed methods. In our research, we worked with English, however, the proposed technique can be used with any other language.
The goal of this paper is to investigate French word segmentation strategies using phonemic and lexical transcriptions as well as prosodic and part-of-speech annotations. Average fundamental frequency (f0) profiles and phoneme duration profiles are measured using 13 hours of broadcast news speech to study prosodic regularities of French words. Some influential factors are taken into consideration for f0 and duration measurements: word syllable length, word-final schwa, part-of-speech. Results from average f0 profiles confirm word final syllable accentuation and from average duration profiles, we can observe long word final syllable length. Both are common tendencies in French. From noun phrase studies, results of average f0 profiles illustrate higher noun first syllable after determiner. Inter-vocalic duration profile results show long inter-vocalic duration between determiner vowel and preceding word vowel. These results reveal measurable cues contributing to word boundary location. Further studies will include more detailed within syllable f0 patterns, other speaking styles and languages.
Quotation extraction is an important information extraction task, especially when dealing with news wires. Quotations can be found in various configurations. In this paper, we focus on direct quotations introduced by a parenthetical clause, headed by a ''``quotation verb''''. Our study is based on a large French news wire corpus from the Agence France-Presse. We introduce and motivate an analysis at the discursive level of such quotations, which differs from the syntactic analyses generally proposed. We show how we enriched the Lefff syntactic lexicon so that it provides an account for quotation verbs heading a quotation parenthetical, especially those extracted from a news wire corpus. We also sketch how these lexical entries can be extended to the discursive level in order to model quotations introduced in a parenthetical clause in a complete way.
In this paper, we present several ways to measure and evaluate the annotation and annotators, proposed and used during the building of the Czech part of the Prague Czech-English Dependency Treebank. At first, the basic principles of the treebank annotation project are introduced (division to three layers: morphological, analytical and tectogrammatical). The main part of the paper describes in detail one of the important phases of the annotation process: three ways of evaluation of the annotators - inter-annotator agreement, error rate and performance. The measuring of the inter-annotator agreement is complicated by the fact that the data contain added and deleted nodes, making the alignment between annotations non-trivial. The error rate is measured by a set of automatic checking procedures that guard the validity of some invariants in the data. The performance of the annotators is measured by a booking web application. All three measures are later compared and related to each other.
The aim of this study was to assess the retrieval effectiveness of nursing students in the Dutch-speaking part of Belgium. We tested two groups: students from the master of Nursing and Midwifery training, and students from the bachelor of Nursing program. The test consisted of five parts: first, the students completed an enquiry about their computer skills, experiences with PubMed and how they assessed their own language skills. Secondly, an introduction into the use of MeSH in PubMed was given, followed by a PubMed search. After the literature search, a second enquiry was completed in which the students were asked to give their opinion about the test. To conclude, an official language test was completed. The results of the PubMed search, i.e. a list of articles the students deemed relevant for a particular question, were compared to a gold standard. Precision, recall and F-score were calculated in order to evaluate the efficiency of the PubMed search. We used information from the search process, such as search term formulation and MeSH term selection to evaluate the search process and examined their relationship with the results of the language test and the level of education.
In this paper, we propose a scheme for annotating utterance-level units in Japanese dialogs, which emerged from an analysis of the interrelationship among four schemes, i) inter-pausal units, ii) intonation units, iii) clause units, and iv) pragmatic units. The associations among the labels of these four units were illustrated by multiple correspondence analysis and hierarchical cluster analysis. Based on these results, we prescribe utterance-unit identification rules, which identify two sorts of utterance-units with different granularities: short and long utterance-units. Short utterance-units are identified by acoustic and prosodic disjuncture, and they are considered to constitute units of speaker's planning and hearer's understanding. Long utterance-units, on the other hand, are recognized by syntactic and pragmatic disjuncture, and they are regarded as units of interaction. We explore some characteristics of these utterance-units, focusing particularly on unit duration and syntactic property, other participants' responses, and mismatch between the two-levels. We also discuss how our two-level utterance-units are useful in analyzing cognitive and communicative aspects of spoken dialogs.
We first describe the automatic conversion of the French Treebank (Abeill{\'e} and Barrier, 2004), a constituency treebank, into typed projective dependency trees. In order to evaluate the overall quality of the resulting dependency treebank, and to quantify the cases where the projectivity constraint leads to wrong dependencies, we compare a subset of the converted treebank to manually validated dependency trees. We then compare the performance of two treebank-trained parsers that output typed dependency parses. The first parser is the MST parser (Mcdonald et al., 2006), which we directly train on dependency trees. The second parser is a combination of the Berkeley parser (Petrov et al., 2006) and a functional role labeler: trained on the original constituency treebank, the Berkeley parser first outputs constituency trees, which are then labeled with functional roles, and then converted into dependency trees. We found that used in combination with a high-accuracy French POS tagger, the MST parser performs a little better for unlabeled dependencies (UAS=90.3{\%} versus 89.6{\%}), and better for labeled dependencies (LAS=87.6{\%} versus 85.6{\%}).
Ontology-based semantic annotation aims at putting fragments of a text in correspondence with proper elements of an ontology such that the formal semantics encoded by the ontology can be exploited to represent text interpretation. In this paper, we formalize a resource for this goal. The main difficulty in achieving good semantic annotations consists in identifying fragments to be annotated and labels to be associated with them. To this end, our approach takes advantage of standard web ontology languages as well as rich linguistic annotation platforms. This in turn is concerned with how to formalize the combination of the ontological and linguistical information, which is a topical issue that has got an increasing discussion recently. Different from existing formalizations, our purpose is to extend ontologies by semantic annotation rules whose complexity increases along two dimensions: the linguistic complexity and the rule syntactic complexity. This solution allows reusing best NLP tools for the production of various levels of linguistic annotations. It also has the merit to distinguish clearly the process of linguistic analysis and the ontological interpretation.
A speech database, named KALAKA, was created to support the Albayzin 2008 Evaluation of Language Recognition Systems, organized by the Spanish Network on Speech Technologies from May to November 2008. This evaluation, designed according to the criteria and methodology applied in the NIST Language Recognition Evaluations, involved four target languages: Basque, Catalan, Galician and Spanish (official languages in Spain), and included speech signals in other (unknown) languages to allow open-set verification trials. In this paper, the process of designing, collecting data and building the train, development and evaluation datasets of KALAKA is described. Results attained in the Albayzin 2008 LRE are presented as a means of evaluating the database. The performance of a state-of-the-art language recognition system on a closed-set evaluation task is also presented for reference. Future work includes extending KALAKA by adding Portuguese and English as target languages and renewing the set of unknown languages needed to carry out open-set evaluations.
Meanings of morphological categories are an indispensable component of representation of sentence semantics. In the Prague Dependency Treebank 2.0, sentence semantics is represented as a dependency tree consisting of labeled nodes and edges. Meanings of morphological categories are captured as attributes of tree nodes; these attributes are called grammatemes. The present paper focuses on morphological meanings of verbs, i.e. on meanings of the morphological category of tense, mood, aspect etc. After several introductory remarks, seven verbal grammatemes used in the PDT 2.0 annotation scenario are briefly introduced. After that, each of the grammatemes is examined. Three verbal grammatemes of the original set were included in the new set without changes, one of the grammatemes was extended, and three of them were substituted for three new ones. The revised grammateme set is to be included in the forthcoming version of PDT (tentatively called PDT 3.0). Rules for automatic and manual assignment of the revised grammatemes are further discussed in the paper.
This paper presented an overview of Chinese bi-character words morphological types, and proposed a set of features for machine learning approaches to predict these types based on composite characters information. First, eight morphological types were defined, and 6,500 Chinese bi-character words were annotated with these types. After pre-processing, 6,178 words were selected to construct a corpus named Reduced Set. We analyzed Reduced Set and conducted the inter-annotator agreement test. The average kappa value of 0.67 indicates a substantial agreement. Second, Bi-character words morphological types are considered strongly related with the composite characters parts of speech in this paper, so we proposed a set of features which can simply be extracted from dictionaries to indicate the characters tendency of parts of speech. Finally, we used these features and adopted three machine learning algorithms, SVM, CRF, and Na{\"\i
Dialogue Acts have been well studied in linguistics and attracted computational linguistics research for a long time: they constitute the basis of everyday conversations and can be identified with the communicative goal of a given utterance (e.g. asking for information, stating facts, expressing opinions, agreeing or disagreeing). Even if not constituting any deep understanding of the dialogue, automatic dialogue act labeling is a task that can be relevant for a wide range of applications in both human-computer and human-human interaction. We present a qualitative analysis of the lexicon of Dialogue Acts: we explore the relationship between the communicative goal of an utterance and its affective content as well as the salience of specific word classes for each speech act. The experiments described in this paper fit in the scope of a research study whose long-term goal is to build an unsupervised classifier that simply exploits the lexical semantics of utterances for automatically annotate dialogues with the proper speech acts.
This paper presents a new algorithm for automatic summarization of specialized texts combining terminological and semantic resources: a term extractor and an ontology. The term extractor provides the list of the terms that are present in the text together their corresponding termhood. The ontology is used to calculate the semantic similarity among the terms found in the main body and those present in the document title. The general idea is to obtain a relevance score for each sentence taking into account both the termhood of the terms found in such sentence and the similarity among such terms and those terms present in the title of the document. The phrases with the highest score are chosen to take part of the final summary. We evaluate the algorithm with Rouge, comparing the resulting summaries with the summaries of other summarizers. The sentence selection algorithm was also tested as part of a standalone summarizer. In both cases it obtains quite good results although the perception is that there is a space for improvement.
This paper aims at measuring the reliability of judges in MT evaluation. The scope is two evaluation campaigns from the CESTA project, during which human evaluations were carried out on fluency and adequacy criteria for English-to-French documents. Our objectives were threefold: observe both inter- and intra-judge agreements, and then study the influence of the evaluation design especially implemented for the need of the campaigns. Indeed, a web interface was especially developed to help with the human judgments and store the results, but some design changes were made between the first and the second campaign. Considering the low agreements observed, the judges' behaviour has been analysed in that specific context. We also asked several judges to repeat their own evaluations a few times after the first judgments done during the official evaluation campaigns. Even if judges did not seem to agree fully at first sight, a less strict comparison led to a strong agreement. Furthermore, the evolution of the design during the project seemed to have been a source for the difficulties that judges encountered to keep the same interpretation of quality.
In this paper we describe a corpus set together from two sub-corpora. The CINEMO corpus contains acted emotional expression obtained by playing dubbing exercises. This new protocol is a way to collect mood-induced data in large amount which show several complex and shaded emotions. JEMO is a corpus collected with an emotion-detection game and contains more prototypical emotions than CINEMO. We show how the two sub-corpora balance and enrich each other and result in a better performance. We built male and female emotion models and use Sequential Fast Forward Feature Selection to improve detection performances. After feature-selection we obtain good results even with our strict speaker independent testing method. The global corpus contains 88 speakers (38 females, 50 males). This study has been done within the scope of the ANR (National Research Agency) Affective Avatar project which deals with building a system of emotions detection for monitoring an Artificial Agent by voice.
This paper presents the general architecture of the TMEKO protocol (Tutoring Methodology for Enriching the Kyoto Ontology) that guides non-expert users through the process of creating mappings from domain wordnet synsets to a shared ontology by answering natural language questions. TMEKO will be part of a Wiki-like community platform currently developed in the Kyoto project (http://www.kyoto-project.eu). The platform provides the architecture for ontology based fact mining to enable knowledge sharing across languages and cultures. A central part of the platform is the Wikyoto editing environment in which users can create their own domain wordnet for seven different languages and define relations to the central and shared ontology based on DOLCE. A substantial part of the mappings will involve important processes and qualities associated with the concept. Therefore, the TMEKO protocol provides specific interviews for creating complex mappings that go beyond subclass and equivalence relations. The Kyoto platform and the TMEKO protocol are developed and applied to the environment domain for seven different languages (English, Dutch, Italian, Spanish, Basque, Japanese and Chinese), but can easily be extended and adapted to other languages and domains.
Despite the large variety of corpora in the biomedical domain their annotations differ in many respects, e.g., the coverage of different, highly specialized knowledge domains, varying degrees of granularity of targeted relations, the specificity of linguistic anchoring of relations and named entities in documents, etc. We here present GeneReg (Gene Regulation Corpus), the result of an annotation campaign led by the Jena University Language {\&} Information Engineering (JULIE) Lab. The GeneReg corpus consists of 314 abstracts dealing with the regulation of gene expression in the model organism E. coli. Our emphasis in this paper is on the compatibility of the GeneReg corpus with the alternative Genia event corpus and with several in-domain and out-of-domain lexical resources, e.g., the Specialist Lexicon, FrameNet, and WordNet. The links we established from the GeneReg corpus to these external resources will help improve the performance of the automatic relation extraction engine JREx trained and evaluated on GeneReg.
In order to utilize the corpus-based techniques that have proven effective in natural language processing in recent years, costly and time-consuming manual creation of linguistic resources is often necessary. Traditionally these resources are created on the document or sentence-level. In this paper, we examine the benefit of annotating only particular words with high information content, as opposed to the entire sentence or document. Using the task of Japanese pronunciation estimation as an example, we devise a machine learning method that can be trained on data annotated word-by-word. This is done by dividing the estimation process into two steps (word segmentation and word-based pronunciation estimation), and introducing a point-wise estimator that is able to make each decision independent of the other decisions made for a particular sentence. In an evaluation, the proposed strategy is shown to provide greater increases in accuracy using a smaller number of annotated words than traditional sentence-based annotation techniques.
This paper describes general requirements for evaluating and documenting NLP tools with a focus on morphological analysers and the design of a Gold Standard. It is argued that any evaluation must be measurable and documentation thereof must be made accessible for any user of the tool. The documentation must be of a kind that it enables the user to compare different tools offering the same service, hence the descriptions must contain measurable values. A Gold Standard presents a vital part of any measurable evaluation process, therefore, the corpus-based design of a Gold Standard, its creation and problems that occur are reported upon here. Our project concentrates on SMOR, a morphological analyser for German that is to be offered as a web-service. We not only utilize this analyser for designing the Gold Standard, but also evaluate the tool itself at the same time. Note that the project is ongoing, therefore, we cannot present final results.
The identification of rare and novel senses is a challenge in lexicography. In this paper, we present a new method for finding such senses using a word aligned multilingual parallel corpus. We use the Europarl corpus and therein concentrate on French verbs. We represent each occurrence of a French verb as a high dimensional term vector. The dimensions of such a vector are the possible translations of the verb according to the underlying word alignment. The dimensions are weighted by a weighting scheme to adjust to the significance of any particular translation. After collecting these vectors we apply forms of the K-means algorithm on the resulting vector space to produce clusters of distinct senses, so that standard uses produce large homogeneous clusters while rare and novel uses appear in small or heterogeneous clusters. We show in a qualitative and quantitative evaluation that the method can successfully find rare and novel senses.
In this paper, we present Second HAREM, the second edition of an evaluation campaign for Portuguese, addressing named entity recognition (NER). This second edition also included two new tracks: the recognition and normalization of temporal entities (proposed by a group of participants, and hence not covered on this paper) and ReRelEM, the detection of semantic relations between named entities. We summarize the setup of Second HAREM by showing the preserved distinctive features and discussing the changes compared to the first edition. Furthermore, we present the main results achieved and describe the available resources and tools developed under this evaluation, namely,(i) the golden collections, i.e. a set of documents whose named entities and semantic relations between those entities were manually annotated, (ii) the Second HAREM collection (which contains the unannotated version of the golden collection), as well as the participating systems results on it, (iii) the scoring tools, and (iv) SAHARA, a Web application that allows interactive evaluation. We end the paper by offering some remarks about what was learned.
This paper describes DeReKo (Deutsches Referenzkorpus), the Archive of General Reference Corpora of Contemporary Written German at the Institut f{\"u
This article presents the use of NLP techniques (text mining, text analysis) to develop specific tools that allow to create linguistic resources related to the cultural heritage domain. The aim of our approach is to create tools for the building of an online knowledge network, automatically extracted from text materials concerning this domain. A particular methodology was experimented by dividing the automatic acquisition of texts, and consequently, the creation of reference corpus in two phases. In the first phase, on-line documents have been extracted from lists of links provided by human experts. All documents extracted from the web by means of automatic spider have been stored in a repository of text materials. On the basis of these documents, automatic parsers create the reference corpus for the cultural heritage domain. Relevant information and semantic concepts are then extracted from this corpus. In a second phase, all these semantically relevant elements (such as proper names, names of institutions, names of places, and other relevant terms) have been used as basis for a new search strategy of text materials from heterogeneous sources. In this case also specialized crawlers (TP-crawler) have been used to work on a bulk of text materials available on line.
This paper reports our experience when integrating differ resources and services into a grid environment. The use case we address implies the deployment of several NLP applications as web services. The ultimate objective of this task was to create a scenario where researchers have access to a variety of services they can operate. These services should be easy to invoke and able to interoperate between one another. We essentially describe the interoperability problems we faced, which involve metadata interoperability, data interoperability and service interoperability. We devote special attention to service interoperability and explore the possibility to define common interfaces and semantic description of services. While the web services paradigm suits the integration of different services very well, this requires mutual understanding and the accommodation to common interfaces that not only provide technical solution but also ease the user{\^a}s work. Defining common interfaces benefits interoperability but requires the agreement about operations and the set of inputs/outputs. Semantic annotation allows defining some sort of taxonomy that organizes and collects the set of admissible operations and types input/output parameters.
This paper describes the development of a specialized lexical resource for a specialized domain, namely medicine. First, in order to assess the linguistic phenomena that need to be adressed, we based our observation on a large collection of more than 300'000 terms, organised around conceptual identifiers. Based on these observations, we highlight the specificities that such a lexicon should take into account, namely in terms of inflectional and derivational knowledge. In a first experiment, we show that general resources lack a large part of the words needed to process specialized language. Secondly, we describe an experiment to feed semi-automatically a medical lexicon and populate it with inflectional information. This experiment is based on a semi-automatic methods that tries to acquire inflectional knowledge from frequent endings of words recorded in existing lexicon. Thanks to this, we increased the coverage of the target vocabulary from 14.1{\%} to 25.7{\%}.
This paper describes how heterogeneous data sources captured in the SignCom project may be used for the analysis and synthesis of French Sign Language (LSF) utterances. The captured data combine video data and multimodal motion capture (mocap) data, including body and hand movements as well as facial expressions. These data are pre-processed, synchronized, and enriched by text annotations of signed language elicitation sessions. The addition of mocap data to traditional data structures provides additional phonetic data to linguists who desire to better understand the various parts of signs (handshape, movement, orientation, etc.) to very exacting levels, as well as their interactions and relative timings. We show how the phonologies of hand configurations and articulator movements may be studied using signal processing and statistical analysis tools to highlight regularities or temporal schemata between the different modalities. Finally, mocap data allows us to replay signs using a computer animation engine, specifically editing and rearranging movements and configurations in order to create novel utterances.
After presenting opinion and sentiment analysis state of the art and the DOXA project, we review the few evaluation campaigns that have dealt in the past with opinion mining. Then we present the two level opinion and sentiment model that we will use for evaluation in the DOXA project and the annotation interface we use for hand annotating a reference corpus. We then present the corpus which will be used on DOXA and report on the hand-annotation task on a corpus of comments on video games and the solution adopted to obtain a sufficient level of inter-annotator agreement.
This paper focuses on the central role played by lexical information in the task of Recognizing Textual Entailment. In particular, the usefulness of lexical knowledge extracted from several widely used static resources, represented in the form of entailment rules, is compared with a method to extract lexical information from Wikipedia as a dynamic knowledge resource. The proposed acquisition method aims at maximizing two key features of the resulting entailment rules: coverage (i.e. the proportion of rules successfully applied over a dataset of TE pairs), and context sensitivity (i.e. the proportion of rules applied in appropriate contexts). Evaluation results show that Wikipedia can be effectively used as a source of lexical entailment rules, featuring both higher coverage and context sensitivity with respect to other resources.
One problem in statistical machine translation (SMT) is that the output often is ungrammatical. To address this issue, we have investigated the use of a grammar checker for two purposes in connection with SMT: as an evaluation tool and as a postprocessing tool. To assess the feasibility of the grammar checker on SMT output, we performed an error analysis, which showed that the precision of error identification in general was higher on SMT output than in previous studies on human texts. Using the grammar checker as an evaluation tool gives a complementary picture to standard metrics such as Bleu, which do not account well for grammaticality. We use the grammar checker as a postprocessing tool by automatically applying the error correction suggestions it gives. There are only small overall improvements of the postprocessing on automatic metrics, but the sentences that are affected by the changes are improved, as shown both by automatic metrics and by a human error analysis. These results indicate that grammar checker techniques are a useful complement to SMT.
We report about tools for the extraction of German multiword expressions (MWEs) from text corpora; we extract word pairs, but also longer MWEs of different patterns, e.g. verb-noun structures with an additional prepositional phrase or adjective. Next to standard association-based extraction, we focus on morpho-syntactic, syntactic and lexical-choice features of the MWE candidates. A broad range of such properties (e.g. number and definiteness of nouns, adjacency of the MWEs components and their position in the sentence, preferred lexical modifiers, etc.) along with relevant example sentences, are extracted from dependency-parsed text and stored in a data base. A sample precision evaluation and an analysis of extraction errors are provided along with the discussion of our extraction architecture. We furthermore measure the contribution of the features to the precision of the extraction: by using both morpho-syntactic and syntactic features, we achieve a higher precision in the identification of idiomatic MWEs, than by using only properties of one type.
The Live Memories corpus is an Italian corpus annotated for anaphoric relations. This annotation effort aims to contribute to two significant issues for the CL research: the lack of annotated anaphoric resources for Italian and the increasing interest for the social Web. The Live Memories Corpus contains texts from the Italian Wikipedia about the region Trentino/S{\"u
WikiWoods is an ongoing initiative to provide rich syntacto-semantic annotations for English Wikipedia. We sketch an automated processing pipeline to extract relevant textual content from Wikipedia sources, segment documents into sentence-like units, parse and disambiguate using a broad-coverage precision grammar, and support the export of syntactic and semantic information in various formats. The full parsed corpus is accompanied by a subset of Wikipedia articles for which gold-standard annotations in the same format were produced manually. This subset was selected to represent a coherent domain, Wikipedia entries on the broad topic of Natural Language Processing.
Historical cabinet protocols are a useful resource which enable historians to identify the opinions expressed by politicians on different subjects and at different points of time. While cabinet protocols are often available in digitized form, so far the only method to access their information content is by keyword-based search, which often returns sub-optimal results. We present a method for enriching German cabinet protocols with information about the originators of statements. This requires automatic speaker attribution. Unlike many other approaches, our method can also deal with cases in which the speaker is not explicitly identified in the sentence itself. Such cases are very common in our domain. To avoid costly manual annotation of training data, we design a rule-based system which exploits morpho-syntactic cues. We show that such a system obtains good results, especially with respect to recall which is particularly important for information access.
Within the EU-funded COMPANIONS project, we are working to evaluate new collaborative conversational models of dialogue. Such an evaluation requires us to benchmark approaches to companionable dialogue. In order to determine the impact of system strategies on our evaluation paradigm, we need to generate a range of companionable conversations, using dialogue strategies such as `empathy' and `positivity'. By companionable dialogue, we mean interactions that take user input of some scenario, and respond in a manner appropriate to the emotional content of the user utterance. In this paper, we describe our working Wizard of Oz (WoZ) system for systematically creating dialogues that fulfil these potential strategies, and enables us to deploy a range of potential techniques for selecting which parts of user input to address is which order, to inform the wizard response to the user based on a manual, on-the-fly assessment of the polarity of the user input.
We describe a new task-based corpus in the Spanish language. The corpus consists of videos, transcripts, and annotations of the inter- action between a naive speaker and a confederate listener. The speaker instructs the listener to MOVE, ROTATE, or PAINT objects on a computer screen. This resource can be used to study how participants produce instructions in a collaborative goal-oriented scenario, in Spanish. The data set is ideally suited for investigating incremental processes of the production and interpretation of language. We demonstrate here how to use this corpus to explore language-specific differences in utterance planning, for English and Spanish speakers.
In emergency situations it is crucial that instructions are straightforward to understand. For this reason a controlled language for crisis management (CLCM), based on psycholinguistic studies of human comprehension under stress, was developed. In order to test the impact of CLCM machine translatability of this particular kind of sub-language text, a previous experiment involving machine translation and human post-editing has been conducted. Employing two automatic evaluation metrics, a previous evaluation of the experiment has proved that instructions written according to this CL can improve machine translation (MT) performance. This paper presents a new cognitive evaluation approach for MT post-editing, which is tested on the previous controlled and uncontrolled textual data. The presented evaluation approach allows a deeper look into the post-editing process and specifically how much effort post-editors put into correcting the different kinds of MT errors. The method is based on existing MT error classification, which is enriched with a new error ranking motivated by the cognitive effort involved in the detection and correction of these MT errors. The preliminary results of applying this approach to a subset of the original data confirmed once again the positive impact of CLCM on emergency instructions' machine translatability and thus the validity of the approach.
This paper describes past, ongoing and planned work on the collection and transcription of spoken language samples for all the South African official languages and as part of this the training of researchers in corpus linguistic research skills. More specifically the work has involved (and still involves) establishing an international corpus linguistic network linked to a network hub at a UNISA website and the development of research tools, a corpus research guide and workbook for multimodal communication and spoken language corpus research. As an example of the work we are doing and hope to do more of in the future, we present a small pilot study of the influence of English and Afrikaans on the 100 most frequent words in spoken Xhosa as this is evidenced in the corpus of spoken interaction we have gathered so far. Other planned work, besides work on spoken language phenomena, involves comparison of spoken and written language and work on communicative body movements (gestures) and their relation to speech.
This paper gives an overview of an interdisciplinary research project that is concerned with the application of computational linguistics methods to the analysis of the structure and variance of rituals, as investigated in ritual science. We present motivation and prospects of a computational approach to ritual research, and explain the choice of specific analysis techniques. We discuss design decisions for data collection and processing and present the general NLP architecture. For the analysis of ritual descriptions, we apply the frame semantics paradigm with newly invented frames where appropriate. Using scientific ritual research literature, we experimented with several techniques of automatic extraction of domain terms for the domain of rituals. As ritual research is a highly interdisciplinary endeavour, a vocabulary common to all sub-areas of ritual research can is hard to specify and highly controversial. The domain terms extracted from ritual research literature are used as a basis for a common vocabulary and thus help the creation of ritual specific frames. We applied the tf*idf, 2 and PageRank algorithm to our ritual research literature corpus and two non-domain corpora: The British National Corpus and the British Academic Written English corpus. All corpora have been part of speech tagged and lemmatized. The domain terms have been evaluated by two ritual experts independently. Interestingly, the results of the algorithms were different for different parts of speech. This finding is in line with the fact that the inter-annotator agreement also differs between parts of speech.
We describe a new Arabic spelling correction system which is intended for use with electronic dictionary search by learners of Arabic. Unlike other spelling correction systems, this system does not depend on a corpus of attested student errors but on student- and teacher-generated ratings of confusable pairs of phonemes or letters. Separate error modules for keyboard mistypings, phonetic confusions, and dialectal confusions are combined to create a weighted finite-state transducer that calculates the likelihood that an input string could correspond to each citation form in a dictionary of Iraqi Arabic. Results are ranked by the estimated likelihood that a citation form could be misheard, mistyped, or mistranscribed for the input given by the user. To evaluate the system, we developed a noisy-channel model trained on students speech errors and use it to perturb citation forms from a dictionary. We compare our system to a baseline based on Levenshtein distance and find that, when evaluated on single-error queries, our system performs 28{\%} better than the baseline (overall MRR) and is twice as good at returning the correct dictionary form as the top-ranked result. We believe this to be the first spelling correction system designed for a spoken, colloquial dialect of Arabic.
Evaluating complex Natural Language Processing (NLP) systems can prove extremely difficult. In many cases, the best one can do is to evaluate these systems indirectly, by looking at the impact they have on the performance of the downstream use case. For complex end-to-end systems, these metrics are not always enlightening, especially from the perspective of NLP failure analysis, as component interaction can obscure issues specific to the NLP technology. We present an evaluation program for complex NLP systems designed to produce meaningful aggregate accuracy metrics with sufficient granularity to support active development by NLP specialists. Our goals were threefold: to produce reliable metrics, to produce useful metrics and to produce actionable data. Our use case is a graph-based Wikipedia search index. Since the evaluation of a complex graph structure is beyond the conceptual grasp of a single human judge, the problem needs to be broken down. Slices of complex data reflective of coherent Decision Points provide a good framework for evaluation using human judges (Medero et al., 2006). For NL semantics, there really is no substitute. Leveraging Decision Points allows complex semantic artifacts to be tracked with judge-driven evaluations that are accurate, timely and actionable.
MAGEAD is a morphological analyzer and generator for Modern Standard Arabic (MSA) and its dialects. We introduced MAGEAD in previous work with an implementation of MSA and Levantine Arabic verbs. In this paper, we port that system to MSA nominals (nouns and adjectives), which are far more complex to model than verbs. Our system is a functional morphological analyzer and generator, i.e., it analyzes to and generates from a representation consisting of a lexeme and linguistic feature-value pairs, where the features are syntactically (and perhaps semantically) meaningful, rather than just morphologically. A detailed evaluation of the current implementation comparing it to a commonly used morphological analyzer shows that it has good morphological coverage with precision and recall scores in the 90s. An error analysis reveals that the majority of recall and precision errors are problems in the gold standard or a result of the discrepancy between different models of form-based/functional morphology.
Statistical machine translation (SMT) requires a large parallel corpus, which is available only for restricted language pairs and domains. To expand the language pairs and domains to which SMT is applicable, we created a method for estimating translation pseudo-probabilities from bilingual comparable corpora. The essence of our method is to calculate pairwise correlations between the words associated with a source-language word, presently restricted to a noun, and its translations; word translation pseudo-probabilities are calculated based on the assumption that the more associated words a translation is correlated with, the higher its translation probability. We also describe a method we created for calculating noun-sequence translation pseudo-probabilities based on occurrence frequencies of noun sequences and constituent-word translation pseudo-probabilities. Then, we present a framework for merging the translation pseudo-probabilities estimated from in-domain comparable corpora with a translation model learned from an out-of-domain parallel corpus. Experiments using Japanese and English comparable corpora of scientific paper abstracts and a Japanese-English parallel corpus of patent abstracts showed promising results; the BLEU score was improved to some degree by incorporating the pseudo-probabilities estimated from the in-domain comparable corpora. Future work includes an optimization of the parameters and an extension to estimate translation pseudo-probabilities for verbs.
This paper describes an interface that was developed for processing large amounts of human judgments of linguistically annotated data. Freds Reusable Evaluation Device (Fred) provides administrators with a tool to submit linguistic evaluation tasks to judges. Each evaluation task is then presented to exactly two judges, who can submit their judgments at their own leisure. Fred then provides several metrics to administrators. The most important metric is precision, which is provided for each evaluation task and each annotator. Administrators can look at precision for a given data set over time, as well as by evaluation type, data set, or annotator. Inter-annotator agreement is also reported, and that can be tracked over time as well. The interface was developed to provide a tool for evaluating semantically marked up text. The types of evaluations Fred has been used for so far include things like correctness of subject-relation identification, and correctness of temporal relations. However, Freds full versatility has not yet been fully exploited.
Systems for syntactically parsing sentences have long been recognized as a priority in Natural Language Processing. Statistics-based systems require large amounts of high quality syntactically parsed data. Using the XLE toolkit developed at PARC and the LFG Parsebanker interface developed at Bergen, the Parsebank Project at Powerset has generated a rapidly increasing volume of syntactically parsed data. By using these tools, we are able to leverage the LFG framework to provide richer analyses via both constituent (c-) and functional (f-) structures. Additionally, the Parsebanking Project uses source data from Wikipedia rather than source data limited to a specific genre, such as the Wall Street Journal. This paper outlines the process we used in creating a large-scale LFG-Based Parsebank to address many of the shortcomings of previously-created parse banks such as the Penn Treebank. While the Parsebank corpus is still in progress, preliminary results using the data in a variety of contexts already show promise.
This paper describes our resource-building results for an eight-week JHU Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation. Specifically, we describe the construction of a modality annotation scheme, a modality lexicon, and two automated modality taggers that were built using the lexicon and annotation scheme. Our annotation scheme is based on identifying three components of modality: a trigger, a target and a holder. We describe how our modality lexicon was produced semi-automatically, expanding from an initial hand-selected list of modality trigger words and phrases. The resulting expanded modality lexicon is being made publicly available. We demonstrate that one tagger―a structure-based tagger―results in precision around 86{\%} (depending on genre) for tagging of a standard LDC data set. In a machine translation application, using the structure-based tagger to annotate English modalities on an English-Urdu training corpus improved the translation quality score for Urdu by 0.3 Bleu points in the face of sparse training data.
ConceptMapper is an open source tool we created for classifying mentions in an unstructured text document based on concept terminologies (dictionaries) and yielding named entities as output. It is implemented as a UIMA (Unstructured Information Management Architecture) annotator and is highly configurable: concepts can come from standardised or proprietary terminologies; arbitrary attributes can be associated with dictionary entries, and those attributes can then be associated with the named entities in the output; numerous search strategies and search options can be specified; any tokenizer packaged as a UIMA annotator can be used to tokenize the dictionary, so the same tokenization can be guaranteed for the input and dictionary, minimising tokenization mismatch errors; and the types and features of UIMA annotations used as input and generated as output can also be controlled. We describe ConceptMapper and its configuration parameters and their trade-offs, then describe the results of an experiment wherein some of these parameters are varied and precision and recall are subsequently measured in the task of in identifying concepts in a collection English-language clinical reports (colon cancer pathology). ConceptMapper is available from the Apache UIMA Sandbox, covered by the Apache Open Source license.
This paper shows that a LAF/GrAF-based annotation schema can be used for the adequate representation of syntactic dependency structures possibly in many languages. We first argue that there are at least two types of textual units that can be annotated with dependency information: words/tokens and chunks/phrases. We especially focus on importance of the latter dependency unit: it is particularly useful for representing Japanese dependency structures, known as Kakari-Uke structure. Based on this consideration, we then discuss a sub-typing of GrAF to represent the corresponding dependency structures. We derive three node types, two edge types, and the associated constraints for properly representing both the token-based and the chunk-based dependency structures. We finally propose a wrapper program that, as a proof of concept, converts output data from different dependency parsers in proprietary XML formats to the GrAF-compliant XML representation. It partially proves the value of an international standard like LAF/GrAF in the Web service context: an existing dependency parser can be, in a sense, standardized, once wrapped by a data format conversion process.
Expert human input can contribute in various ways to facilitate automatic annotation of natural language text. For example, a part-of-speech tagger can be trained on labeled input provided offline by experts. In addition, expert input can be solicited by way of active learning to make the most of annotator expertise. However, hiring individuals to perform manual annotation is costly both in terms of money and time. This paper reports on a user study that was performed to determine the degree of effect that a part-of-speech dictionary has on a group of subjects performing the annotation task. The user study was conducted using a modular, web-based interface created specifically for text annotation tasks. The user study found that for both native and non-native English speakers a dictionary with greater than 60{\%} coverage was effective at reducing annotation time and increasing annotator accuracy. On the basis of this study, we predict that using a part-of-speech tag dictionary with coverage greater than 60{\%} can reduce the cost of annotation in terms of both time and money.
The intuition and basic hypothesis that this paper explores is that names are more characteristic of their language than common words are, and that a single name can have enough clues to confidently identify its language where random text of the same length wouldn't. To test this hypothesis, n-gramm modelling is used to learn language models which identify the language of isolated names and equally short document fragments. As the empirical results corroborate the prior intuition, an explanation is sought for the higher accuracy at which the language of names can be identified. The results of the application of these models, as well as the models themselves, are quantitatively and qualitatively analysed and a hypothesis is formed about the explanation of this difference. The conclusions derived are both technologically useful in information extraction or text-to-speech tasks, and theoretically interesting as a tool for improving our understanding of the morphology and phonology of the languages involved in the experiments.
In this paper we describe a proof-of-concept for the bootstrapping of a Persian WordNet. This effort was motivated by previous work done at Stanford University on bootstrapping an Arabic WordNet using a parallel corpus and an English WordNet. The principle of that work is based on the premise that paradigmatic relations are by nature deeply semantic, and as such, are likely to remain intact between languages. We performed our task on a Persian-English bilingual corpus of George Orwells Nineteen Eighty-Four. The corpus was neither aligned nor sense tagged, so it was necessary that these were undertaken first. A combination of manual and semiautomated methods were used to tag and sentence align the corpus. Actual mapping of English word senses onto Persian was done using automated techniques. Although Persian is written in Arabic script, it is an Indo-European language, while Arabic is a Central Semitic language. Despite their linguistic differences, we endeavor to test the applicability of the Stanford strategy to our task.
Human language technologies (HLT) can play a vital role in bridging the digital divide and thus the HLT field has been recognised as a priority area by the South African government. We present our work on conducting a technology audit on the South African HLT landscape across the countrys eleven official languages. The process and the instruments employed in conducting the audit are described and an overview of the various complementary approaches used in the results analysis is provided. We find that a number of HLT language resources (LRs) are available in SA but they are of a very basic and exploratory nature. Lessons learnt in conducting a technology audit in a young and multilingual context are also discussed.
There is by now widespread agreement that the most realistic way to construct the large-scale commonsense knowledge repositories required by natural language and artificial intelligence applications is by letting machines learn such knowledge from large quantities of data, like humans do. A lot of attention has consequently been paid to the development of increasingly sophisticated machine learning algorithms for knowledge extraction. However, the nature of the input that humans are exposed to while learning commonsense knowledge has received much less attention. The BabyExp project is collecting very dense audio and video recordings of the first 3 years of life of a baby. The corpus constructed in this way will be transcribed with automated techniques and made available to the research community. Moreover, techniques to extract commonsense conceptual knowledge incrementally from these multimodal data are also being explored within the project. The current paper describes BabyExp in general, and presents pilot studies on the feasibility of the automated audio and video transcriptions.
This paper describes the first TTS evaluation campaign designed for Spanish. Seven research institutions took part in the evaluation campaign and developed a voice from a common speech database provided by the organisation. Each participating team had a period of seven weeks to generate a voice. Next, a set of sentences were released and each team had to synthesise them within a week period. Finally, some of the synthesised test audio files were subjectively evaluated via an online test according to the following criteria: similarity to the original voice, naturalness and intelligibility. Box-plots, Wilcoxon tests and WER have been generated in order to analyse the results. Two main conclusions can be drawn: On the one hand, there is considerable margin for improvement to reach the quality level of the natural voice. On the other hand, two systems get significantly better results than the rest: one is based on statistical parametric synthesis and the other one is a concatenative system that makes use of a sinusoidal model to modify both prosody and smooth spectral joints. Therefore, it seems that some kind of spectral control is needed when building voices with a medium size database for unrestricted domains.
In this paper, we present a system to aid human annotation of semantic information in the scope of the project AC/DC, called corte-e-costura. This system leverages on the human annotation effort, by providing the annotator with a simple system that applies rules incrementally. Our goal was twofold: first, to develop an easy-to-use system that required a minimum of learning from the part of the linguist; second, one that provided a straightforward way of checking the results obtained, in order to immediately evaluate the results of the rules devised. After explaining the motivation for its development from scratch, we present the current status of the AC/DC project and provide a quantitative description of its material in what concerns semantic annotation. We then present the corte-e-costura system in detail, providing the result of our first experiments with the semantic fields of colour and clothing. We end the paper with some discussion of future work as well as of the experience gained.
Spoken Document Retrieval (SDR) is a promising technology for enhancing the utility of spoken materials. After the spoken documents have been transcribed by using a Large Vocabulary Continuous Speech Recognition (LVCSR) decoder, a text-based ad hoc retrieval method can be applied directly to the transcribed documents. However, recognition errors will significantly degrade the retrieval performance. To address this problem, we have previously proposed a method that aimed to fill the gap between automatically transcribed text and correctly transcribed text by using a statistical translation technique. In this paper, we extend the method by (1) using neighboring context to index the target passage, and (2) applying a language modeling approach for document retrieval. Our experimental evaluation shows that context information can improve retrieval performance, and that the language modeling approach is effective in incorporating context information into the proposed SDR method, which uses a translation model.
The paper offers an overview of the key issues raised during the seven years activity of the Multilingual Question Answering Track at the Cross Language Evaluation Forum (CLEF). The general aim of the Multilingual Question Answering Track has been to test both monolingual and cross-language Question Answering (QA) systems that process queries and documents in several European languages, also drawing attention to a number of challenging issues for research in multilingual QA. The paper gives a brief description of how the task has evolved over the years and of the way in which the data sets have been created, presenting also a brief summary of the different types of questions developed. The document collections adopted in the competitions are sketched as well, and some data about the participation are provided. Moreover, the main evaluation measures used to evaluate system performances are explained and an overall analysis of the results achieved is presented.
Herein, we present the process of developing the first Hungarian Dependency TreeBank. First, short references are made to dependency grammars we considered important in the development of our Treebank. Second, mention is made of existing dependency corpora for other languages. Third, we present the steps of converting the Szeged Treebank into dependency-tree format: from the originally phrase-structured treebank, we produced dependency trees by automatic conversion, checked and corrected them thereby creating the first manually annotated dependency corpus for Hungarian. We also go into detail about the two major sets of problems, i.e. coordination and predicative nouns and adjectives. Fourth, we give statistics on the treebank: by now, we have completed the annotation of business news, newspaper articles, legal texts and texts in informatics, at the same time, we are planning to convert the entire corpus into dependency tree format. Finally, we give some hints on the applicability of the system: the present database may be utilized ― among others ― in information extraction and machine translation as well.
In ontology learning from texts, we have ontology-rich domains where we have large structured domain knowledge repositories or we have large general corpora with large general structured knowledge repositories such as WordNet (Miller, 1995). Ontology learning methods are more useful in ontology-poor domains. Yet, in these conditions, these methods have not a particularly high performance as training material is not sufficient. In this paper we present an LSP ontology learning method that can exploit models learned from a generic domain to extract new information in a specific domain. In our model, we firstly learn a model from training data and then we use the learned model to discover knowledge in a specific domain. We tested our model adaptation strategy using a background domain that is applied to learn the isa networks in the Earth Observation Domain as a specific domain. We will demonstrate that our method captures domain knowledge better than other generic models: our model better captures what is expected by domain experts than a baseline method based only on WordNet. This latter is better correlated with non-domain annotators asked to produce the ontology for the specific domain.
This paper introduces the general features of Senso Comune, an open knowledge base for the Italian language, focusing on the interplay of lexical and ontological knowledge, and outlining our approach to conceptual knowledge elicitation. Senso Comune consists of a machine-readable lexicon constrained by an ontological infrastructure. The idea at the basis of Senso Comune is that natural languages exist in use, and they belong to their users. In the line of Saussure's linguistics, natural languages are seen as a social product and their main strength relies on the users consensus. At the same time, language has specific goals: i.e. referring to entities that belong to the users world (be it physical or not) and that are made up in social environments where expressions are produced and understood. This usage leverages the creativity of those who produce words and try to understand them. This is the reason why ontology, i.e. a shared conceptualization of the world, can be regarded to as the soil on which the speakers' consensus may be rooted. Some final remarks concerning future work and applications are also given.
The idea that dictionaries are a good source for (computational) information has been around for a long while, and the extraction of taxonomic information from them is something that has been attempted several times. However, such information extraction was typically based on the systematic analysis of the text of a single dictionary. In this paper, we demonstrate how it is possible to extract taxonomic information without any analysis of the specific text, by comparing the same lexical entry in a number of different dictionaries. Counting word frequencies in the dictionary entry for the same word in different dictionaries leads to a surprisingly good recovery of taxonomic information, without the need for any syntactic analysis of the entries in question nor any kind of language-specific treatment. As a case in point, we will show in this paper an experiment extracting hyperonymy relations from several Spanish dictionaries, measuring the effect that the different number of dictionaries have on the results.
In the POS tagging task, there are two kinds of statistical models: one is generative model, such as the HMM, the others are discriminative models, such as the Maximum Entropy Model (MEM). POS multi-tagging decoding method includes the N-best paths method and forward-backward method. In this paper, we use the forward-backward decoding method based on a combined model of HMM and MEM. If P(t) is the forward-backward probability of each possible tag t, we first calculate P(t) according HMM and MEM separately. For all tags options in a certain position in a sentence, we normalize P(t) in HMM and MEM separately. Probability of the combined model is the sum of normalized forward-backward probabilities P norm(t) in HMM and MEM. For each word w, we select the best tag in which the probability of combined model is the highest. In the experiments, we use combined model and get higher accuracy than any single model on POS tagging tasks of three languages, which are Chinese, English and Dutch. The result indicates that our combined model is effective.
In this paper we present AhoTransf, a tool that enables analysis, visualization, modification and synthesis of speech. AhoTransf integrates a speech signal analysis model with a graphical user interface to allow visualization and modification of the parameters of the model. The synthesis capability allows hearing the modified signal thus providing a quick way to understand the perceptual effect of the changes in the parameters of the model. The speech analysis/synthesis algorithm is based in the Multiband Excitation technique, but uses a novel phase information representation the Relative Phase Shift (RPSs). With this representation, not only the amplitudes but also the phases of the harmonic components of the speech signal reveal their structured patterns in the visualization tool. AhoTransf is modularly conceived so that it can be used with different harmonic speech models.
In previous work, we presented a preliminary study to identify paraphrases between technical and lay discourse types from medical corpora dedicated to the French language. In this paper, we test the hypothesis that the same kinds of paraphrases as for French can be detected between English technical and lay discourse types and report the adaptation of our method from French to English. Starting from the constitution of monolingual comparable corpora, we extract two kinds of paraphrases: paraphrases between nominalizations and verbal constructions and paraphrases between neo-classical compounds and modern-language phrases. We do this relying on morphological resources and a set of extraction rules we adapt from the original approach for French. Results show that paraphrases could be identified with a rather good precision, and that these types of paraphrase are relevant in the context of the opposition between technical and lay discourse types. These observations are consistent with the results obtained for French, which demonstrates the portability of the approach as well as the similarity of the two languages as regards the use of those kinds of expressions in technical and lay discourse types.
In the present paper we report on a recent effort that resulted in the establishment of a unique multimodal database, referred to as the PROMETHEUS database. This database was created in support of research and development activities, performed within the European Commission FP7 PROMETHEUS project, aiming at the creation of a framework for monitoring and interpretation of human behaviours in unrestricted indoors and outdoors environments. In the present paper we discuss the design and the implementation of the audio part of the database and offer statistical information about the audio content. Specifically, it contains single-person and multi-person scenarios, but also covers scenarios with interactions between groups of people. The database design was conceived with extended support of research and development activities devoted to detection of typical and atypical events, emergency and crisis situations, which assist for achieving situational awareness and more reliable interpretation of the context in which humans behave. The PROMETHEUS database allows for embracing a wide range of real-world applications, including smart-home and human-robot interaction interfaces, indoors/outdoors public areas surveillance, airport terminals or city park supervision, etc. A major portion of the PROMETHEUS database will be made publically available by the end of year 2010.
Creating language resources is expensive and time-consuming, and this forms a bottleneck in the development of language technology, for less-studied non-European languages in particular. The recent internet phenomenon of crowd-sourcing offers a cost-effective and potentially fast way of overcoming such language resource acquisition bottlenecks. We present a methodology that takes as its input scanned documents of typed or hand-written text, and produces transcriptions of the text as its output. Instead of using Optical Character Recognition (OCR) technology, the methodology is game-based and produces such transcriptions as a by-product. The approach is intended particularly for languages for which language technology and resources are scarce and reliable OCR technology may not exist. It can be used in place of OCR for transcribing individual documents, or to create corpora of paired images and transcriptions required to train OCR tools. We present Minefield, a prototype implementation of the approach which is currently collecting Arabic transcriptions.
Annotation of digitized pages from historical document collections is very important to research on automatic extraction of text blocks, lines, and handwriting recognition. We have recently introduced a new handwritten text database, GERMANA, which is based on a Spanish manuscript from 1891. To our knowledge, GERMANA is the first publicly available database mostly written in Spanish and comparable in size to standard databases. In this paper, we present another handwritten text database, RODRIGO, completely written in Spanish and comparable in size to GERMANA. However, RODRIGO comes from a much older manuscript, from 1545, where the typical difficult characteristics of historical documents are more evident. In particular, the writing style, which has clear Gothic influences, is significantly more complex than that of GERMANA. We also provide baseline results of handwriting recognition for reference in future studies, using standard techniques and tools for preprocessing, feature extraction, HMM-based image modelling, and language modelling.
This paper proposes a methodology for the creation of specialized data sets for Textual Entailment, made of monothematic Text-Hypothesis pairs (i.e. pairs in which only one linguistic phenomenon relevant to the entailment relation is highlighted and isolated). The expected benefits derive from the intuition that investigating the linguistic phenomena separately, i.e. decomposing the complexity of the TE problem, would yield an improvement in the development of specific strategies to cope with them. The annotation procedure assumes that humans have knowledge about the linguistic phenomena relevant to inference, and a classification of such phenomena both into fine grained and macro categories is suggested. We experimented with the proposed methodology over a sample of pairs taken from the RTE-5 data set, and investigated critical issues arising when entailment, contradiction or unknown pairs are considered. The result is a new resource, which can be profitably used both to advance the comprehension of the linguistic phenomena relevant to entailment judgments and to make a first step towards the creation of large-scale specialized data sets.
We present the first effort towards producing an Arabic Discourse Treebank,a news corpus where all discourse connectives are identified and annotated with the discourse relations they convey as well as with the two arguments they relate.We discuss our collection of Arabic discourse connectives as well as principles for identifying and annotating them in context, taking into account properties specific to Arabic. In particular, we deal with the fact that Arabic has a rich morphology: we therefore include clitics as connectives as well as a wide range of nominalizations as potential arguments. We present a dedicated discourse annotation tool for Arabic and a large-scale annotation study. We show that both the human identification of discourse connectives and the determination of the discourse relations they convey is reliable. Our current annotated corpus encompasses a final 5651 annotated discourse connectives in 537 news texts. In future, we will release the annotated corpus to other researchers and use it for training and testing automated methods for discourse connective and relation recognition.
This paper presents a preliminary analysis of the role of some discourse markers and the vocalic hesitation ''``euh'''' in a corpus of spoken human utterances collected with the Ritel system, an open domain and spoken dialog system. The frequency and contextual combinatory of classical discourse markers and of the vocalic hesitation have been studied. This analysis pointed out some specificity in terms of combinatory of the analyzed items. The classical discourse markers seem to help initiating larger discursive blocks both at initial and medial positions of the on-going turns. The vocalic hesitation stand also for marking the user's embarrassments and wish to close the dialog.
The CINEMO corpus of French emotional speech provides a richly annotated resource to help overcome the apparent lack of learning and testing speech material for complex, i.e. blended or mixed emotions. The protocol for its collection was dubbing selected emotional scenes from French movies. 51 speakers are contained and the total speech time amounts to 2 hours and 13 minutes and 4k speech chunks after segmentation. Extensive labelling was carried out in 16 categories for major and minor emotions and in 6 continuous dimensions. In this contribution we give insight into the corpus statistics focusing in particular on the topic of complex emotions, and provide benchmark recognition results obtained in exemplary large feature space evaluations. In the result the labelling oft he collected speech clearly demonstrates that a complex handling of emotion seems needed. Further, the automatic recognition experiments provide evidence that the automatic recognition of blended emotions appears to be feasible.
In this work we present further development of the SpLaSH (Spoken Language Search Hawk) project. SpLaSH implements a data model for annotated speech corpora integrated with textual markup (i.e. POS tagging, syntax, pragmatics) including a toolkit used to perform complex queries across speech and text labels. The integration of time aligned annotations (TMA), represented making use of Annotation Graphs, with text aligned ones (TXA), stored in generic XML files, are provided by a data structure, the Connector Frame, acting as table-look-up linking temporal data to words in the text. SpLaSH imposes a very limited number of constraints to the data model design, allowing the integration of annotations developed separately within the same dataset and without any relative dependency. It also provides a GUI allowing three types of queries: simple query on TXA or TMA structures, sequence query on TMA structure and cross query on both TXA and TMA integrated structures. In this work new SpLaSH features will be presented: SpLaSH Query Language (SpLaSHQL) and Query Sequence.
Semantic Role Labeling cannot be performed without an associated linguistic resource. A key resource for such a task is the FrameNet resource based on Fillmores theory of frame semantics. Like many linguistic resources, FrameNet has been built by English native speakers for the English language. To overcome the lack of such resources in other languages, we propose a new approach to FrameNet translation by using bilingual dictionaries and filtering the wrong translations. We define six scores to filter, based on translation redundancy and FrameNet structure. We also present our work on the enrichment of the obtained resource with nouns. This enrichment uses semantic spaces built on syntactical dependencies and a multi-represented k-NN classifier. We evaluate both the tasks on the French language over a subset of ten frames and show improved results compared to the existing French FrameNet. Our final resource contains 15,132 associations lexical units-frames for an estimated precision of 86{\%}.
We present an annotation tool for the extended textual coreference and the bridging anaphora in the Prague Dependency Treebank{\^A}~2.0 (PDT 2.0). After we very briefly describe the annotation scheme, we focus on details of the annotation process from the technical point of view. We present the way of helping the annotators by several useful features implemented in the annotation tool, such as a possibility to combine surface and deep syntactic representation of sentences during the annotation, an automatic maintaining of the coreferential chain, underlining candidates for antecedents, etc. For studying differences among parallel annotations, the tool offers a simultaneous depicting of several annotations of the same data. The annotation tool can be used for other corpora too, as long as they have been transformed to the PML format. We present modifications of the tool for working with the coreference relations on other layers of language description, namely on the analytical layer and the morphological layer of PDT.
Discourse phenomena play a major role in text processing tasks. However, so far relatively little study has been devoted to the relevance of discourse phenomena for inference. Therefore, an experimental study was carried out to assess the relevance of anaphora and coreference for Textual Entailment (TE), a prominent inference framework. First, the annotation of anaphoric and coreferential links in the RTE-5 Search data set was performed according to a specifically designed annotation scheme. As a result, a new data set was created where all anaphora and coreference instances in the entailing sentences which are relevant to the entailment judgment are solved and annotated.. A by-product of the annotation is a new augmented data set, where all the referring expressions which need to be resolved in the entailing sentences are replaced by explicit expressions. Starting from the final output of the annotation, the actual impact of discourse phenomena on inference engines was investigated, identifying the kind of operations that the systems need to apply to address discourse phenomena and trying to find direct mappings between these operation and annotation types.
SentimentWortschatz, or SentiWS for short, is a publicly available German-language resource for sentiment analysis, opinion mining etc. It lists positive and negative sentiment bearing words weighted within the interval of [-1; 1] plus their part of speech tag, and if applicable, their inflections. The current version of SentiWS (v1.8b) contains 1,650 negative and 1,818 positive words, which sum up to 16,406 positive and 16,328 negative word forms, respectively. It not only contains adjectives and adverbs explicitly expressing a sentiment, but also nouns and verbs implicitly containing one. The present work describes the resources structure, the three sources utilised to assemble it and the semi-supervised method incorporated to weight the strength of its entries. Furthermore the resources contents are extensively evaluated using a German-language evaluation set we constructed. The evaluation set is verified being reliable and its shown that SentiWS provides a beneficial lexical resource for German-language sentiment analysis related tasks to build on.
Subjectivity analysis and authorship attribution are very popular areas of research. However, work in these two areas has been done separately. We believe that by combining information about subjectivity in texts and authorship, the performance of both tasks can be improved. In the paper a personalized approach to opinion mining is presented, in which the notions of personal sense and idiolect are introduced; the approach is applied to the polarity classification task. It is assumed that different authors express their private states in text individually, and opinion mining results could be improved by analyzing texts by different authors separately. The hypothesis is tested on a corpus of movie reviews by ten authors. The results of applying the personalized approach to opinion mining are presented, confirming that the approach increases the performance of the opinion mining task. Automatic authorship attribution is further applied to model the personalized approach, classifying documents by their assumed authorship. Although the automatic authorship classification imposes a number of limitations on the dataset for further experiments, after overcoming these issues the authorship attribution technique modeling the personalized approach confirms the increase over the baseline with no authorship information used.
We introduce a method for automatically labelling edges of word co-occurrence graphs with semantic relations. Therefore we only make use of training data already contained within the graph. Starting point of this work is a graph based on word co-occurrence of the German language, which is created by applying iterated co-occurrence analysis. The edges of the graph have been partially annotated by hand with semantic relationships. In our approach we make use of the commonly appearing network motif of three words forming a triangular pattern. We assume that the fully annotated occurrences of these structures contain information useful for our purpose. Based on these patterns rules for reasoning are learned. The obtained rules are then combined using Dempster-Shafer theory to infer new semantic relations between words. Iteration of the annotation process is possible to increase the number of obtained relations. By applying the described process the graph can be enriched with semantic information at a high precision.
This paper investigates the mapping between two semantic formalisms, namely the tectogrammatical layer of the Prague Dependency Treebank 2.0 (PDT) and (Robust) Minimal Recursion Semantics ((R)MRS). It is a first attempt to relate the dependency-based annotation scheme of PDT to a compositional semantics approach like (R)MRS. A mapping algorithm that converts PDT trees to (R)MRS structures is developed, associating (R)MRSs to each node on the dependency tree. Furthermore, composition rules are formulated and the relation between dependency in PDT and semantic heads in (R)MRS is analyzed. It turns out that structure and dependencies, morphological categories and some coreferences can be preserved in the target structures. Moreover, valency and free modifications are distinguished using the valency dictionary of PDT as an additional resource. The validation results show that systematically correct underspecified target representations can be obtained by a rule-based mapping approach, which is an indicator that (R)MRS is indeed robust in relation to the formal representation of Czech data. This finding is novel, for Czech, with its free word order and rich morphology, is typologically different than languages analyzed with (R)MRS to date.
The task of parse disambiguation has gained in importance over the last decade as the complexity of grammars used in deep linguistic processing has been increasing. In this paper we propose to employ the fine-grained HPSG formalism in order to investigate the contribution of deeper linguistic knowledge to the task of ranking the different trees the parser outputs. In particular, we focus on the incorporation of semantic features in the disambiguation component and the stability of our model cross domains. Our work is carried out within DELPH-IN (http://www.delph-in.net), using the LinGo Redwoods and the WeScience corpora, parsed with the English Resource Grammar and the PET parser.
Electronic dictionaries offer many possibilities unavailable in paper dictionaries to view, display or access information. However, even these resources fall short when it comes to access words sharing semantic features and certain aspects of form: few applications offer the possibility to access a word via a morphologically or semantically related word. In this paper, we present such an application, Polymots, a lexical database for contemporary French containing 20.000 words grouped in 2.000 families. The purpose of this resource is to group words into families on the basis of shared morpho-phonological and semantic information. Words with a common stem form a family; words in a family also share a set of common conceptual fragments (in some families there is a continuity of meaning, in others meaning is distributed). With this approach, we capitalize on the bidirectional link between semantics and morpho-phonology : the user can thus access words not only on the basis of ideas, but also on the basis of formal characteristics of the word, i.e. its morphological features. The resulting lexical database should help people learn French vocabulary and assist them to find words they are looking for, going thus beyond other existing lexical resources.
The aim of this paper is twofold. We focus, on the one hand, on the task of dynamically annotating English compound nouns, and on the other hand we propose disambiguation methods and techniques which facilitate the annotation task. Both the aforementioned are part of a larger on-going effort which aims to create HPSG annotation for the texts from theWall Street Journal (henceforward WSJ) sections of the Penn Treebank (henceforward PTB) with the help of a hand-written large-scale and wide-coverage grammar of English, the English Resource Grammar (henceforward ERG; Flickinger (2002)). As we show in this paper, such annotations are very rich linguistically, since apart from syntax they also incorporate semantics, which does not only ensure that the treebank is guaranteed to be a truly sharable, re-usable and multi-functional linguistic resource, but also calls for the necessity of a better disambiguation of the internal (syntactic) structure of larger units of words, such as compound nouns, since this has an impact on the representation of their meaning, which is of utmost interest if the linguistic annotation of a given corpus is to be further understood as the practice of adding interpretative linguistic information of the highest quality in order to give added value to the corpus.
The Internet is an ever growing source of information stored in documents of different languages. Hence, cross-lingual resources are needed for more and more NLP applications. This paper presents (i) a graph-based method for creating one such resource and (ii) a resource created using the method, a cross-lingual relatedness thesaurus. Given a word in one language, the thesaurus suggests words in a second language that are semantically related. The method requires two monolingual corpora and a basic dictionary. Our general approach is to build two monolingual word graphs, with nodes representing words and edges representing linguistic relations between words. A bilingual dictionary containing basic vocabulary provides seed translations relating nodes from both graphs. We then use an inter-graph node-similarity algorithm to discover related words. Evaluation with three human judges revealed that 49{\%} of the English and 57{\%} of the German words discovered by our method are semantically related to the target words. We publish two resources in conjunction with this paper. First, noun coordinations extracted from the German and English Wikipedias. Second, the cross-lingual relatedness thesaurus which can be used in experiments involving interactive cross-lingual query expansion.
We present a flexible toolkit-based approach to automatic coreference resolution on German text. We start with our previous work aimed at reimplementing the system from Soon et al. (2001) for English, and extend it to duplicate a version of the state-of-the-art proposal from Klenner and Ailloud (2009). Evaluation performed on a benchmarking dataset, namely the TueBa-D/Z corpus (Hinrichs et al., 2005b), shows that machine learning based coreference resolution can be robustly performed in a language other than English.
In the framework of the preparation of linguistic web services for corpus processing, the need for a representation format was felt, which supports interoperability between different web services in a corpus processing pipeline, but also provides a well-defined interface to both, legacy tools and their data formats and upcoming international standards. We present the D-SPIN text corpus format, TCF, which was designed for this purpose. It is a stand-off XML format, inspired by the philosophy of the emerging standards LAF (Linguistic Annotation Framework) and its ``instances'' MAF for morpho-syntactic annotation and SynAF for syntactic annotation. Tools for the exchange with existing (best practice) formats are available, and a converter from MAF to TCF is being tested in spring 2010. We describe the usage scenario where TCF is embedded and the properties and architecture of TCF. We also give examples of TCF encoded data and describe the aspects of syntactic and semantic interoperability already addressed.
We describe a dataset containing 16,000 translations produced by four machine translation systems and manually annotated for quality by professional translators. This dataset can be used in a range of tasks assessing machine translation evaluation metrics, from basic correlation analysis to training and test of machine learning-based metrics. By providing a standard dataset for such tasks, we hope to encourage the development of better MT evaluation metrics.
This paper describes and evaluates a prototype quality assurance system for LSP corpora. The system will be employed in compiling a corpus of 11 M tokens for various linguistic and terminological purposes. The system utilizes a number of linguistic features as quality indicators. These represent two dimensions of quality, namely readability/formality (e.g. word length and passive constructions) and density of specialized knowledge (e.g. out-of-vocabulary items). Threshold values for each indicator are induced from a reference corpus of general (fiction, magazines and newspapers) and specialized language (the domains of Health/Medicine and Environment/Climate). In order to test the efficiency of the indicators, a number of terminologically relevant, irrelevant and possibly relevant texts are manually selected from target web sites as candidate texts. By applying the indicators to these candidate texts, the system is able to filter out non-LSP and poor LSP texts with a precision of 100{\%} and a recall of 55{\%}. Thus, the experiment described in this paper constitutes fundamental work towards a formulation of best practice for implementing quality assurance when selecting appropriate texts for an LSP corpus. The domain independence of the quality indicators still remains to be thoroughly tested on more than just two domains.
We describe an experimentalWizard-of-Oz-setup for the integration of emotional strategies into spoken dialogue management. With this setup we seek to evaluate different approaches to emotional dialogue strategies in human computer interaction with a spoken dialogue system. The study aims to analyse what kinds of emotional strategies work best in spoken dialogue management especially facing the problem that users may not be honest about their emotions. Therefore as well direct (user is asked about his state) as indirect (measurements of psychophysiological features) evidence is considered for the evaluation of our strategies.
Delivering linguistic resources and easy-to-use methods to a broad public in the humanities is a challenging task. On the one hand users rightly demand easy to use interfaces but on the other hand want to have access to the full flexibility and power of the functions being offered. Even though a growing number of excellent systems exist which offer convenient means to use linguistic resources and methods, they usually focus on a specific domain, as for example corpus exploration or text categorization. Architectures which address a broad scope of applications are still rare. This article introduces the eHumanities Desktop, an online system for corpus management, processing and analysis which aims at bridging the gap between powerful command line tools and intuitive user interfaces.
We present a heuristic method for word alignment, which is the task of identifying corresponding words in parallel text. The heuristic method is based on parallel phrases extracted from manually word aligned sentence pairs. Word alignment is performed by matching parallel phrases to new sentence pairs, and adding word links from the parallel phrase to words in the matching sentence segment. Experiments on an English--Swedish parallel corpus showed that the heuristic phrase-based method produced word alignments with high precision but low recall. In order to improve alignment recall, phrases were generalized by replacing words with part-of-speech categories. The generalization improved recall but at the expense of precision. Two filtering strategies were investigated to prune the large set of generalized phrases. Finally, the phrase-based method was compared to statistical word alignment with Giza++ and we found that although statistical alignments based on large datasets will outperform phrase-based word alignment, a combination of phrase-based and statistical word alignment outperformed pure statistical alignment in terms of Alignment Error Rate (AER).
This paper presents the Demo / Kemo corpus of Dutch and Korean emotional speech. The corpus has been specifically developed for the purpose of cross-linguistic comparison, and is more balanced than any similar corpus available so far: a) it contains expressions by both Dutch and Korean actors as well as judgments by both Dutch and Korean listeners; b) the same elicitation technique and recording procedure was used for recordings of both languages; c) the same nonsense sentence, which was constructed to be permissible in both languages, was used for recordings of both languages; and d) the emotions present in the corpus are balanced in terms of valence, arousal, and dominance. The corpus contains a comparatively large number of emotions (eight) uttered by a large number of speakers (eight Dutch and eight Korean). The counterbalanced nature of the corpus will enable a stricter investigation of language-specific versus universal aspects of emotional expression than was possible so far. Furthermore, given the carefully controlled phonetic content of the expressions, it allows for analysis of the role of specific phonetic features in emotional expression in Dutch and Korean.
The Semantic Web is facing the important challenge to maintain its promise of a real world-wide graph of interconnected resources. Unfortunately, while URIs almost guarantee a direct reference to entities, the relation between the two is not bijective. Many different URI references to same concepts and entities can arise when -- in such a heterogeneous setting as the WWW -- people independently build new ontologies, or populate shared ones with new arbitrarily identified individuals. The proliferation of URIs is an unwanted, though natural effect strictly bound to the same principles which characterize the Semantic Web; reducing this phenomenon will improve the recall of Semantic Search engines, which could rely on explicit links between heterogeneous information sources. To address this problem, in this paper we present an integrated environment combining the semantic annotation and ontology building features available in the Semantic Turkey web browser extension, with globally unique identifiers for entities provided by the okkam Entity Name System, thus realizing a valuable resource for preventing diffusion of multiple URIs on the (Semantic) Web.
This paper describes Czech spontaneous speech database of lectures on digital signal processing topic collected at Czech Technical University in Prague, commonly with the procedure of its recording and annotation. The database contains 21.7 hours of speech material from 22 speakers recorded in 4 channels with 3 principally different microphones. The annotation of the database is composed from basic time segmentation, orthographic transcription including marks for speaker and environmental non-speech events, pronunciation lexicon in SAMPA alphabet, session and speaker information describing recording conditions, and the documentation. The orthographic transcription with time segmentation is saved in XML format supported by frequently used annotation tool Transcriber. In this article, special attention is also paid to the description of time synchronization of signals recorded by two independent devices: computer based recording platform using two external sound cards and commercial audio recorder Edirol R09. This synchronization is based on cross-correlation analysis with simple automated selection of suitable short signal subparts. The collection and annotation of this database is now complete and its availability via ELRA is currently under preparation.
The performance of question answering system is evaluated through successive evaluations campaigns. A set of questions are given to the participating systems which are to find the correct answer in a collection of documents. The creation process of the questions may change from one evaluation to the next. This may entail an uncontroled question difficulty shift. For the QAst 2009 evaluation campaign, a new procedure was adopted to build the questions. Comparing results of QAst 2008 and QAst 2009 evaluations, a strong performance loss could be measured in 2009 for French and English, while the Spanish systems globally made progress. The measured loss might be related to this new way of elaborating questions. The general purpose of this paper is to propose a measure to calibrate the difficulty of a question set. In particular, a reasonable measure should output higher values for 2009 than for 2008. The proposed measure relies on a distance measure between the critical elements of a question and those of the associated correct answer. An increase of the proposed distance measure for French and English 2009 evaluations as compared to 2008 could be established. This increase correlates with the previously observed degraded performances. We conclude on the potential of this evaluation criterion: the importance of such a measure for the elaboration of new question corpora for questions answering systems and a tool to control the level of difficulty for successive evaluation campaigns.
In this paper, we present work on enhancing the basic data resource of a context-aware system. Electronic text offers a wealth of information about geospatial data and can be used to improve the completeness and accuracy of geospatial resources (e.g., gazetteers). First, we introduce a supervised approach to extracting geographical relations on a fine-grained level. Second, we present a novel way of using Wikipedia as a corpus based on self-annotation. A self-annotation is an automatically created high-quality annotation that can be used for training and evaluation. Wikipedia contains two types of different context: (i) unstructured text and (ii) structured data: templates (e.g., infoboxes about cities), lists and tables. We use the structured data to annotate the unstructured text. Finally, the extracted fine-grained relations are used to complete gazetteer data. The precision and recall scores of more than 97 percent confirm that a statistical IE pipeline can be used to improve the data quality of community-based resources.
Question answering systems are complex systems using natural language processing. Some evaluation campaigns are organized to evaluate such systems in order to propose a classification of systems based on final results (number of correct answers). Nevertheless, teams need to evaluate more precisely the results obtained by their systems if they want to do a diagnostic evaluation. There are no tools or methods to do these evaluations systematically. We present REVISE, a tool for glass box evaluation based on diagnostic of question answering system results.
In this study, we extracted brain activities related to semantic relations and distances to improve the precision of distance calculation among concepts in the Associated Concept Dictionary (ACD). For the experiments, we used a multi-channel Near-infrared Spectroscopy (NIRS) device to measure the response properties of the changes in hemoglobin concentration during word-concept association tasks. The experiments stimuli were selected from pairs of stimulus words and associated words in the ACD and presented in the form of a visual stimulation to the subjects. In our experiments, we obtained subject response data and brain activation data in Broca's area ―a human brain region that is active in linguistic/word-concept decision tasks― and these data imply relations with the length of associative distance. This study showed that it was possible to connect brain activities to the semantic relation among concepts, and that it would improve the method for concept distance calculation in order to build a more human-like ontology model.
Most question-answering systems contain a classifier module which determines a question category, based on which each question is assigned an answer type. However, setting up syntactic patterns for this classification is a big challenge. In addition, in the case of ontology-based systems, the answer type should be aligned to the queried knowledge structure. In this paper, we present an approach for determining the answer type semi-automatically. We first identify the question focus using syntactic parsing, and then try to identify the answer type by combining the head of the focus with the ontology-based lookup. When this combination is not enough to make conclusions automatically, the user is engaged into a dialog in order to resolve the answer type. User selections are saved and used for training the system in order to improve its performance over time. Further on, the answer type is used to show the feedback and the concise answer to the user. Our approach is evaluated using 250 questions from the Mooney Geoquery dataset.
Question answering (QA) systems aim at retrieving precise information from a large collection of documents. To be considered as reliable by users, a QA system must provide elements to evaluate the answer. This notion of answer justification can also be useful when developping a QA system in order to give criteria for selecting correct answers. An answer justification can be found in a sentence, a passage made of several consecutive sentences or several passages of a document or several documents. Thus, we are interesting in pinpointing the set of information that allows to verify the correctness of the answer in a candidate passage and the question elements that are missing in this passage. Moreover, the relevant information is often given in texts in a different form from the question form: anaphora, paraphrases, synonyms. In order to have a better idea of the importance of all the phenomena we underlined, and to provide enough examples at the QA developer's disposal to study them, we decided to build an annotated corpus.
We present an experimental framework for Entity Mention Detection in which two different classifiers are combined to exploit Data Redundancy attained through the annotation of a large text corpus, as well as a number of Patterns extracted automatically from the same corpus. In order to recognize proper name, nominal, and pronominal mentions we not only exploit the information given by mentions recognized within the corpus being annotated, but also given by mentions occurring in an external and unannotated corpus. The system was first evaluated in the Evalita 2009 evaluation campaign obtaining good results. The current version is being used in a number of applications: on the one hand, it is used in the LiveMemories project, which aims at scaling up content extraction techniques towards very large scale extraction from multimedia sources. On the other hand, it is used to annotate corpora, such as Italian Wikipedia, thus providing easy access to syntactic and semantic annotation for both the Natural Language Processing and Information Retrieval communities. Moreover a web service version of the system is available and the system is going to be integrated into the TextPro suite of NLP tools.
Aligning the NPs of parallel corpora is logically halfway between the sentence- and word-alignment tasks that occupy much of the MT literature, but has received far less attention. NP alignment is a challenging problem, capable of rapidly exposing flaws both in the word-alignment and in the NP chunking algorithms one may bring to bear. It is also a very rewarding problem in that NPs are semantically natural translation units, which means that (i) word alignments will cross NP boundaries only exceptionally, and (ii) within sentences already aligned, the proportion of 1-1 alignments will be higher for NPs than words. We created a simple gold standard for English-Hungarian, Orwells 1984, (since this already exists in manually verified POS-tagged format in many languages thanks to the Multex and MultexEast project) by manually verifying the automaticaly generated NP chunking (we used the yamcha, mallet and hunchunk taggers) and manually aligning the maximal NPs and PPs. The maximum NP chunking problem is much harder than base NP chunking, with F-measure in the .7 range (as opposed to over .94 for base NPs). Since the results are highly impacted by the quality of the NP chunking, we tested our alignment algorithms both with real world (machine obtained) chunkings, where results are in the .35 range for the baseline algorithm which propagates GIZA++ word alignments to the NP level, and on idealized (manually obtained) chunkings, where the baseline reaches .4 and our current system reaches .64.
We present the GIVE-2 Corpus, a new corpus of human instruction giving. The corpus was collected by asking one person in each pair of subjects to guide the other person towards completing a task in a virtual 3D environment with typed instructions. This is the same setting as that of the recent GIVE Challenge, and thus the corpus can serve as a source of data and as a point of comparison for NLG systems that participate in the GIVE Challenge. The instruction-giving data we collect is multilingual (45 German and 63 English dialogues), and can easily be extended to further languages by using our software, which we have made available. We analyze the corpus to study the effects of learning by repeated participation in the task and the effects of the participants' spatial navigation abilities. Finally, we present a novel annotation scheme for situated referring expressions and compare the referring expressions in the German and English data.
Audiovisual speech recognition (AVSR) systems have been proven superior over audio-only speech recognizers in noisy environments by incorporating features of the visual modality. In order to develop reliable AVSR systems, appropriate simultaneously recorded speech and video data is needed. In this paper, we will introduce a corpus (WAPUSK20) that consists of audiovisual data of 20 speakers uttering 100 sentences each with four channels of audio and a stereoscopic video. The latter is intended to support more accurate lip tracking and the development of stereo data based normalization techniques for greater robustness of the recognition results. The sentence design has been adopted from the GRID corpus that has been widely used for AVSR experiments. Recordings have been made under acoustically realistic conditions in a usual office room. Affordable hardware equipment has been used, such as a pre-calibrated stereo camera and standard PC components. The software written to create this corpus was designed in MATLAB with help of hardware specific software provided by the hardware manufacturers and freely available open source software.
Graph-based similarity over WordNet has been previously shown to perform very well on word similarity. This paper presents a study of the performance of such a graph-based algorithm when using different relations and versions of Wordnet. The graph algorithm is based on Personalized PageRank, a random-walk based algorithm which computes the probability of a random-walk initiated in the target word to reach any synset following the relations in WordNet (Haveliwala, 2002). Similarity is computed as the cosine of the probability distributions for each word over WordNet. The best combination of relations includes all relations in WordNet 3.0, included disambiguated glosses, and automatically disambiguated topic signatures called KnowNets. All relations are part of the official release of WordNet, except KnowNets, which have been derived automatically. The results over the WordSim 353 dataset show that using the adequate relations the performance improves over previously published WordNet-based results on the WordSim353 dataset (Finkelstein et al., 2002). The similarity software and some graphs used in this paper are publicly available at http://ixa2.si.ehu.es/ukb.
In this article we describe two different strategies for the automatic tagging of a Spanish diachronic corpus involving the adaptation of existing NLP tools developed for modern Spanish. In the initial approach we follow a state-of-the-art strategy, which consists on standardizing the spelling and the lexicon. This approach boosts POS-tagging accuracy to 90, which represents a raw improvement of over 20{\%} with respect to the results obtained without any pre-processing. In order to enable non-expert users in NLP to use this new resource, the corpus has been integrated into IAC (Corpora Interface Access). We discuss the shortcomings of the initial approach and propose a new one, which does not consist in adapting the source texts to the tagger, but rather in modifying the tagger for the direct treatment of the old variants.This second strategy addresses some important shortcomings in the previous approach and is likely to be useful not only in the creation of diachronic linguistic resources but also for the treatment of dialectal or non-standard variants of synchronic languages as well.
This paper describes an approach for inferring syntactic frames of verbs in Urdu from an untagged corpus. Urdu, like many other South Asian languages, is a free word order and case-rich language. Separable lexical units mark different constituents for case in phrases and clauses and are called case clitics. There is not always a one to one correspondence between case clitic form and case, and case and grammatical function in Urdu. Case clitics, therefore, can not serve as direct clues for extracting the syntactic frames of verbs. So a two-step approach has been implemented. In a first step, all case clitic combinations for a verb are extracted and the unreliable ones are filtered out by applying the inferential statistics. In a second step, the information of occurrences of case clitic forms in different combinations as a whole and on individual level is processed to infer all possible syntactic frames of the verb.
The increasing amount of available textual information makes necessary the use of Natural Language Processing (NLP) tools. These tools have to be used on large collections of documents in different languages. But NLP is a complex task that relies on many processes and resources. As a consequence, NLP tools must be both configurable and efficient: specific software architectures must be designed for this purpose. We present in this paper the LIMA multilingual analysis platform, developed at CEA LIST. This configurable platform has been designed to develop NLP based industrial applications while keeping enough flexibility to integrate various processes and resources. This design makes LIMA a linguistic analyzer that can handle languages as different as French, English, German, Arabic or Chinese. Beyond its architecture principles and its capabilities as a linguistic analyzer, LIMA also offers a set of tools dedicated to the test and the evaluation of linguistic modules and to the production and the management of new linguistic resources.
Text clustering is potentially very useful for exploration of text sets that are too large to study manually. The success of such a tool depends on whether the results can be explained to the user. An automatically extracted cluster description usually consists of a few words that are deemed representative for the cluster. It is preferably short in order to be easily grasped. However, text cluster content is often diverse. We introduce a trimming method that removes texts that do not contain any, or a few of the words in the cluster description. The result is clusters that match their descriptions better. In experiments on two quite different text sets we obtain significant improvements in both internal and external clustering quality for the trimmed clustering compared to the original. The trimming thus has two positive effects: it forces the clusters to agree with their descriptions (resulting in better descriptions) and improves the quality of the trimmed clusters.
Currently, a great effort is being carried out in the digitalisation of large historical document collections for preservation purposes. The documents in these collections are usually written in ancient languages, such as Latin or Greek, which limits the access of the general public to their content due to the language barrier. Therefore, digital libraries aim not only at storing raw images of digitalised documents, but also to annotate them with their corresponding text transcriptions and translations into modern languages. Unfortunately, ancient languages have at their disposal scarce electronic resources to be exploited by natural language processing techniques. This paper describes the compilation process of a novel Latin-Catalan parallel corpus as a new task for statistical machine translation (SMT). Preliminary experimental results are also reported using a state-of-the-art phrase-based SMT system. The results presented in this work reveal the complexity of the task and its challenging, but interesting nature for future development.
Manual text annotation is a resource-consuming endeavor necessary for NLP systems when they target new tasks or domains for which there are no existing annotated corpora. Distributing the annotation work across multiple contributors is a natural solution to reduce and manage the effort required. Although there are a few publicly available tools which support distributed collaborative text annotation, most of them have complex user interfaces and require a significant amount of involvement from the annotators/contributors as well as the project developers and administrators. We present a light-weight web application for highly distributed annotation projects - Djangology. The application takes advantage of the recent advances in web framework architecture that allow rapid development and deployment of web applications thus minimizing development time for customization. The application's web-based interface gives project administrators the ability to easily upload data, define project schemas, assign annotators, monitor progress, and review inter-annotator agreement statistics. The intuitive web-based user interface encourages annotator participation as contributors are not burdened by tool manuals, local installation, or configuration. The system has achieved a user response rate of 70{\%} in two annotation projects involving more than 250 medical experts from various geographic locations.
We present CAVaT, a tool that performs Corpus Analysis and Validation for TimeML. CAVaT is an open source, modular checking utility for statistical analysis of features specific to temporally-annotated natural language corpora. It provides reporting, highlights salient links between a variety of general and time-specific linguistic features, and also validates a temporal annotation to ensure that it is logically consistent and sufficiently annotated. Uniquely, CAVaT provides analysis specific to TimeML-annotated temporal information. TimeML is a standard for annotating temporal information in natural language text. In this paper, we present the reporting part of CAVaT, and then its error-checking ability, including the workings of several novel TimeML document verification methods. This is followed by the execution of some example tasks using the tool to show relations between times, events, signals and links. We also demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been detected with CAVaT.
In The Low Countries, a major reference corpus for written Dutch is being built. We discuss the interplay between data acquisition and data processing during the creation of the SoNaR Corpus. Based on developments in traditional corpus compiling and new web harvesting approaches, SoNaR is designed to contain 500 million words, balanced over 36 text types including both traditional and new media texts. Beside its balanced design, every text sample included in SoNaR will have its IPR issues settled to the largest extent possible. This data collection task presents many challenges because every decision taken on the level of text acquisition has ramifications for the level of processing and the general usability of the corpus. As far as the traditional text types are concerned, each text brings its own processing requirements and issues. For new media texts - SMS, chat - the problem is even more complex, issues such as anonimity, recognizability and citation right, all present problems that have to be tackled. The solutions actually lead to the creation of two corpora: a gigaword SoNaR, IPR-cleared for research purposes, and the smaller - of commissioned size - more privacy compliant SoNaR, IPR-cleared for commercial purposes as well.
The fast evolution of language technology has produced pressing needs in standardization. The multiplicity of language resources representation levels and the specialization of these representations make difficult the interaction between linguistic resources and components manipulating these resources. In this paper, we describe the MultiLingual Information Framework (MLIF ― ISO CD 24616). MLIF is a metamodel which allows the representation and the exchange of multilingual textual information. This generic metamodel is designed to provide a common platform for all the tools developed around the existing multilingual data exchange formats. This platform provides, on the one hand, a set of generic data categories for various application domains, and on the other hand, strategies for the interoperability with existing standards. The objective is to reach a better convergence between heterogeneous standardisation activities that are taking place in the domain of data modeling (XML; W3C), text management (TEI; TEIC), multilingual information (TMX-LISA; XLIFF-OASIS) and multimedia (SMILText; W3C). This is a work in progress within ISO-TC37 in order to define a new ISO standard.
We present a method and a software tool, the FrameNet Transformer, for deriving customized versions of the FrameNet database based on frame and frame element relations. The FrameNet Transformer allows users to iteratively coarsen the FrameNet sense inventory in two ways. First, the tool can merge entire frames that are related by user-specified relations. Second, it can merge word senses that belong to frames related by specified relations. Both methods can be interleaved. The Transformer automatically outputs format-compliant FrameNet versions, including modified corpus annotation files that can be used for automatic processing. The customized FrameNet versions can be used to determine which granularity is suitable for particular applications. In our evaluation of the tool, we show that our method increases accuracy of statistical semantic parsers by reducing the number of word-senses (frames) per lemma, and increasing the number of annotated sentences per lexical unit and frame. We further show in an experiment on the FATE corpus that by coarsening FrameNet we do not incur a significant loss of information that is relevant to the Recognizing Textual Entailment task.
In this paper, we present a novel approach to multi-word terminology extraction combining a well-known automatic term recognition approach, the C--NC value method, with a contrastive ranking technique, aimed at refining obtained results either by filtering noise due to common words or by discerning between semantically different types of terms within heterogeneous terminologies. Differently from other contrastive methods proposed in the literature that focus on single terms to overcome the multi-word terms' sparsity problem, the proposed contrastive function is able to handle variation in low frequency events by directly operating on pre-selected multi-word terms. This methodology has been tested in two case studies carried out in the History of Art and Legal domains. Evaluation of achieved results showed that the proposed two--stage approach improves significantly multi--word term extraction results. In particular, for what concerns the legal domain it provides an answer to a well-known problem in the semi--automatic construction of legal ontologies, namely that of singling out law terms from terms of the specific domain being regulated.
This paper describes the process and the resources used to automatically annotate a French corpus of spontaneous speech transcriptions in super-chunks. Super-chunks are enhanced chunks that can contain lexical multiword units. This partial parsing is based on a preprocessing stage of the spoken data that consists in reformatting and tagging utterances that break the syntactic structure of the text, such as disfluencies. Spoken specificities were formalized thanks to a systematic linguistic study of a 40-hour-long speech transcription corpus. The chunker uses large-coverage and fine-grained language resources for general written language that have been augmented with resources specific to spoken French. It consists in iteratively applying finite-state lexical and syntactic resources and outputing a finite automaton representing all possible chunk analyses. The best path is then selected thanks to a hybrid disambiguation stage. We show that our system reaches scores that are comparable with state-of-the-art results in the field.
Sign Languages (SLs) are the visuo-gestural languages practised by the deaf communities. Research on SLs requires to build, to analyse and to use corpora. The aim of this paper is to present various kinds of new uses of SL corpora. The way data are used take advantage of the new capabilities of annotation software for visualisation, numerical annotation, and processing. The nature of the data can be video-based or motion capture-based. The aims of the studies include language analysis, animation processing, and evaluation. We describe here some LIMSIs studies, and some studies from other laboratories as examples.
This document reports the process of extending MorphoPro for Venetan, a lesser-used language spoken in the Nort-Eastern part of Italy. MorphoPro is the morphological component of TextPro, a suite of tools oriented towards a number of NLP tasks. In order to extend this component to Venetan, we developed a declarative representation of the morphological knowledge necessary to analyze and synthesize Venetan words. This task was challenging for several reasons, which are common to a number of lesser-used languages: although Venetan is widely used as an oral language in everyday life, its written usage is very limited; efforts for defining a standard orthography and grammar are very recent and not well established; despite recent attempts to propose a unified orthography, no Venetan standard is widely used. Besides, there are different geographical varieties and it is strongly influenced by Italian.
The Arabic Treebank (ATB) Project at the Linguistic Data Consortium (LDC) has embarked on a large corpus of Broadcast News (BN) transcriptions, and this has led to a number of new challenges for the data processing and annotation procedures that were originally developed for Arabic newswire text (ATB1, ATB2 and ATB3). The corpus requirements currently posed by the DARPA GALE Program, including English translation of Arabic BN transcripts, word-level alignment of Arabic and English data, and creation of a corresponding English Treebank, place significant new constraints on ATB corpus creation, and require careful coordination among a wide assortment of concurrent activities and participants. Nonetheless, in spite of the new challenges posed by BN data, the ATBs newly improved pipeline and revised annotation guidelines for newswire have proven to be robust enough that very few changes were necessary to account for the new genre of data. This paper presents the points where some adaptation has been necessary, and the overall pipeline as used in the production of BN ATB data.
This paper describes an approach based on word alignment on parallel corpora, which aims at facilitating the lexicographic work of dictionary building. Although this method has been widely used in the MT community for at least 16 years, as far as we know, it has not been applied to facilitate the creation of bilingual dictionaries for human use. The proposed corpus-driven technique, in particular the exploitation of parallel corpora, proved to be helpful in the creation of such dictionaries for several reasons. Most importantly, a parallel corpus of appropriate size guarantees that the most relevant translations are included in the dictionary. Moreover, based on the translational probabilities it is possible to rank translation candidates, which ensures that the most frequently used translation variants go first within an entry. A further advantage is that all the relevant example sentences from the parallel corpora are easily accessible, thus facilitating the selection of the most appropriate translations from possible translation candidates. Due to these properties the method is particularly apt to enable the production of active or encoding dictionaries.
This paper describes an ISO project which aims at developing a standard for annotating spoken and multimodal dialogue with semantic information concerning the communicative functions of utterances, the kind of semantic content they address, and their relations with what was said and done earlier in the dialogue. The project, ISO 24617-2 ''``Semantic annotation framework, Part 2: Dialogue acts'''', is currently at DIS stage. The proposed annotation schema distinguishes 9 orthogonal dimensions, allowing each functional segment in dialogue to have a function in each of these dimensions, thus accounting for the multifunctionality that utterances in dialogue often have. A number of core communicative functions is defined in the form of ISO data categories, available at http://semantic-annotation.uvt.nl/dialogue-acts/iso-datcats.pdf; they are divided into ''``dimension-specific'''' functions, which can be used only in a particular dimension, such as Turn Accept in the Turn Management dimension, and ''``general-purpose'''' functions, which can be used in any dimension, such as Inform and Request. An XML-based annotation language, ''``DiAML'''' is defined, with an abstract syntax, a semantics, and a concrete syntax.
We are in the process of creating a multi-representational and multi-layered treebank for Hindi/Urdu (Palmer et al., 2009), which has three main layers: dependency structure, predicate-argument structure (PropBank), and phrase structure. This paper discusses an important issue in treebank design which is often neglected: the use of empty categories (ECs). All three levels of representation make use of ECs. We make a high-level distinction between two types of ECs, trace and silent, on the basis of whether they are postulated to mark displacement or not. Each type is further refined into several subtypes based on the underlying linguistic phenomena which the ECs are introduced to handle. This paper discusses the stages at which we add ECs to the Hindi/Urdu treebank and why. We investigate methodically the different types of ECs and their role in our syntactic and semantic representations. We also examine our decisions whether or not to coindex each type of ECs with other elements in the representation.
This paper presents the development of an open-source Spanish Dependency Grammar implemented in FreeLing environment. This grammar was designed as a resource for NLP applications that require a step further in natural language automatic analysis, as is the case of Spanish-to-Basque translation. The development of wide-coverage rule-based grammars using linguistic knowledge contributes to extend the existing Spanish deep parsers collection, which sometimes is limited. Spanish FreeLing Dependency Grammar, named EsTxala, provides deep and robust parse trees, solving attachments for any structure and assigning syntactic functions to dependencies. These steps are dealt with hand-written rules based on linguistic knowledge. As a result, FreeLing Dependency Parser gives a unique analysis as a dependency tree for each sentence analyzed. Since it is a resource open to the scientific community, exhaustive grammar evaluation is being done to determine its accuracy as well as strategies for its manteinance and improvement. In this paper, we show the results of an experimental evaluation carried out over EsTxala in order to test our evaluation methodology.
This work reports the evaluation and selection of annotation tools to assign wh-question labels to verbal arguments in a sentence. Wh-question assignment discussed herein is a kind of semantic annotation which involves two tasks: making delimitation of verbs and arguments, and linking verbs to its arguments by question labels. As it is a new type of semantic annotation, there is no report about requirements an annotation tool should have to face it. For this reason, we decided to select the most appropriated tool in two phases. In the first phase, we executed the task with an annotation tool we have used before in another task. Such phase helped us to test the task and enabled us to know which features were or not desirable in an annotation tool for our purpose. In the second phase, guided by such requirements, we evaluated several tools and selected a tool for the real task. After corpus annotation conclusion, we report some of the annotation results and some comments on the improvements there should be made in an annotation tool to better support such kind of annotation task.
The term event extraction covers a wide range of information extraction tasks, and methods developed and evaluated for one task may prove quite unsuitable for another. Understanding these task differences is essential to making broad progress in event extraction. We look back at the MUC and ACE tasks in terms of one characteristic, the breadth of the scenario ― how wide a range of information is subsumed in a single extraction task. We examine how this affects strategies for collecting information and methods for semi-supervised training of new extractors. We also consider the heterogeneity of corpora ― how varied the topics of documents in a corpus are. Extraction systems may be intended in principle for general news but are typically evaluated on topic-focused corpora, and this evaluation context may affect system design. As one case study, we examine the task of identifying physical attack events in news corpora, observing the effect on system performance of shifting from an attack-event-rich corpus to a more varied corpus and considering how the impact of this shift may be mitigated.
Complications arise for standoff annotation when the annotation is not on the source text itself, but on a more abstract representation. This is particularly the case in a language such as Arabic with morphological and orthographic challenges, and we discuss various aspects of these issues in the context of the Arabic Treebank. The Standard Arabic Morphological Analyzer (SAMA) is closely integrated into the annotation workflow, as the basis for the abstraction between the explicit source text and the more abstract token representation. However, this integration with SAMA gives rise to various problems for the annotation workflow and for maintaining the link between the Treebank and SAMA. In this paper we discuss how we have overcome these problems with consistent and more precise categorization of all of the tokens for their relationship with SAMA. We also discuss how we have improved the creation of several distinct alternative forms of the tokens used in the syntactic trees. As a result, the Treebank provides a resource relating the different forms of the same underlying token with varying degrees of vocalization, in terms of how they relate (1) to each other, (2) to the syntactic structure, and (3) to the morphological analyzer.
The theoretical characterisation of multiword expressions (MWEs) is tightly connected to their actual occurrences in data and to their representation in lexical resources. We present three lexical resources for Italian MWEs, namely an electronic lexicon, a series of example corpora and a database of MWEs represented around morphosyntactic patterns. These resources are matched against, and created from, a very large web-derived corpus for Italian that spans across registers and domains. We can thus test expressions coded by lexicographers in a dictionary, thereby discarding unattested expressions, revisiting lexicographers's choices on the basis of frequency information, and at the same time creating an example sub-corpus for each entry. We organise MWEs on the basis of the morphosyntactic information obtained from the data in an electronic, flexible knowledge-base containing structured annotation exploitable for multiple purposes. We also suggest further work directions towards characterising MWEs by analysing the data organised in our database through lexico-semantic information available in WordNet or MultiWordNet-like resources, also in the perspective of expanding their set through the extraction of other similar compact expressions.
We explore the potential and limitations of a concept of building a bilingual valency lexicon based on the alignment of nodes in a parallel treebank. Our aim is to build an electronic Czech-{\textgreater}English Valency Lexicon by collecting equivalences from bilingual treebank data and storing them in two already existing electronic valency lexicons, PDT-VALLEX and Engvallex. For this task a special annotation interface has been built upon the TrEd editor, allowing quick and easy collecting of frame equivalences in either of the source lexicons. The issues encountered so far include limitations of technical character, theory-dependent limitations and limitations concerning the achievable degree of quality of human annotation. The issues of special interest for both linguists and MT specialists involved in the project include linguistically motivated non-balance between the frame equivalents, either in number or in type of valency participants. The first phases of annotation so far attest the assumption that there is a unique correspondence between the functors of the translation-equivalent frames. Also, hardly any linguistically significant non-balance between the frames has been found, which is partly promising considering the linguistic theory used and partly caused by little stylistic variety of the annotated corpus texts.
This paper presents the process of development and the characteristics of an evaluation collection for a personalisation system for digital newspapers. This system selects, adapts and presents contents according to a user model that define information needs. The collection presented here contains data that are cross-related over four different axes: a set of news items from an electronic newspaper, collected into subsets corresponding to a particular sequence of days, packaged together and cross-indexed with a set of user profiles that represent the particular evolution of interests of a set of real users over the given days, expressed in each case according to four different representation frameworks: newspaper sections, Yahoo categories, keywords, and relevance feedback over the set of news items for the previous day. This information provides a minimum starting material over which one can evaluate for a given system how it addresses the first two observations - adapting to different users and adapting to particular users over time - providing that the particular system implements the representation of information needs according to the four frameworks employed in the collection. This collection has been successfully used to perform some different experiments to determine the effectiveness of the personalization system presented.
Many contemporary language technology systems are characterized by long pipelines of tools with complex dependencies. Too often, these workflows are implemented by ad hoc scripts; or, worse, tools are run manually, making experiments difficult to reproduce. These practices are difficult to maintain in the face of rapidly evolving workflows while they also fail to expose and record important details about intermediate data. Further complicating these systems are hyperparameters, which often cannot be directly optimized by conventional methods, requiring users to determine which combination of values is best via trial and error. We describe LoonyBin, an open-source tool that addresses these issues by providing: 1) a visual interface for the user to create and modify workflows; 2) a well-defined mechanism for tracking metadata and provenance; 3) a script generator that compiles visual workflows into shell scripts; and 4) a new workflow representation we call a HyperWorkflow, which intuitively and succinctly encodes small experimental variations within a larger workflow.
This paper describes the development and evaluation of enhancements to the specialized information retrieval capabilities of a multimodal reporting system. The system enables collection and dissemination of information through a distributed data architecture by allowing users to input free text documents, which are indexed for subsequent search and retrieval by other users. This unstructured data entry method is essential for users of this system, but it requires an intelligent support system for processing queries against the data. The system, known as TIGR (Tactical Ground Reporting), allows keyword searching and geospatial filtering of results, but lacked the ability to efficiently index and search person names and perform approximate name matching. To improve TIGRs ability to provide accurate, comprehensive results for queries on person names we iteratively updated existing entity extraction and name matching technologies to better align with the TIGR use case. We evaluated each version of the entity extraction and name matching components to find the optimal configuration for the TIGR context, and combined those pieces into a named entity extraction, indexing, and search module that integrates with the current TIGR system. By comparing system-level evaluations of the original and updated TIGR search processes, we show that our enhancements to personal name search significantly improved the performance of the overall information retrieval capabilities of the TIGR system.
Intonation is an important aspect of vocal production, used for a variety of communicative needs. Its modeling is therefore crucial in many speech understanding systems, particularly those requiring inference of speaker intent in real-time. However, the estimation of pitch, traditionally the first step in intonation modeling, is computationally inconvenient in such scenarios. This is because it is often, and most optimally, achieved only after speech segmentation and recognition. A consequence is that earlier speech processing components, in todays state-of-the-art systems, lack intonation awareness by fiat; it is not known to what extent this circumscribes their performance. In the current work, we present a freely available implementation of an alternative to pitch estimation, namely the computation of the fundamental frequency variation (FFV) spectrum, which can be easily employed at any level within a speech processing system. It is our hope that the implementation we describe aid in the understanding of this novel acoustic feature space, and that it facilitate its inclusion, as desired, in the front-end routines of speech recognition, dialog act recognition, and speaker recognition systems.
A central purpose of referring expressions is to distinguish intended referents from other entities that are in the context; but how is this context determined? This paper draws a distinction between discourse context ―other entities that have been mentioned in the dialogue― and visual context ―visually available objects near the intended referent. It explores how these two different aspects of context have an impact on subsequent reference in a dialogic situation where the speakers share both discourse and visual context. In addition we take into account the impact of the reference history ―forms of reference used previously in the discourse― on forming what have been called conceptual pacts. By comparing the output of different parameter settings in our model to a data set of human-produced referring expressions, we determine that an approach to subsequent reference based on conceptual pacts provides a better explanation of our data than previously proposed algorithmic approaches which compute a new distinguishing description for the intended referent every time it is mentioned.
In this paper, we propose an estimation method of user satisfaction for a spoken dialog system using an N-gram-based dialog history model. We have collected a large amount of spoken dialog data accompanied by usability evaluation scores by users in real environments. The database is made by a field-test in which naive users used a client-server music retrieval system with a spoken dialog interface on their own PCs. An N-gram model is trained from the sequences that consist of users' dialog acts and/or the system's dialog acts for each one of six user satisfaction levels: from 1 to 5 and φ (task not completed). Then, the satisfaction level is estimated based on the N-gram likelihood. Experiments were conducted on the large real data and the results show that our proposed method achieved good classification performance; the classification accuracy was 94.7{\%} in the experiment on a classification into dialogs with task completion and those without task completion. Even if the classifier detected all of the task incomplete dialog correctly, our proposed method achieved the false detection rate of only 6{\%}.
The modeling of human behavior becomes more and more important due to the increasing popularity of context-aware computing and people-centric mobile applications. Inspired by the principle of action-as-language, we propose that human ambulatory behavior shares similar properties as natural languages. In addition, by exploiting this similarity, we will be able to index, recognize, cluster, retrieve, and infer high-level semantic meanings of human behaviors via the use of natural language processing techniques. In this paper, we developed a Life Logger system to help build the behavior language corpus which supports our ''``Behavior as Language'''' research. The constructed behavior corpus shows Zipf's distribution over the frequency of vocabularies which is aligned with our ''``Behavior as Language'''' assumption. Our preliminary results of using smoothed n-gram language model for activity recognition achieved an average accuracy rate of 94{\%} in distinguishing among human ambulatory behaviors including walking, running, and cycling. This behavior-as-language corpus will enable researchers to study higher level human behavior based on the syntactic and semantic analysis of the corpus data.
With the development of speech and language processing, speech translation systems have been developed. These studies target spoken dialogues, and employ consecutive interpretation, which uses a sentence as the translation unit. On the other hand, there exist a few researches about simultaneous interpreting, and recently, the language resources for promoting simultaneous interpreting research, such as the publication of an analytical large-scale corpus, has been prepared. For the future, it is necessary to make the corpora more practical toward realization of a simultaneous interpreting system. In this paper, we describe the construction of a bilingual corpus which can be used for simultaneous lecture interpreting research. Simultaneous lecture interpreting systems are required to recognize translation units in the middle of a sentence, and generate its translation at the proper timing. We constructed the bilingual lecture corpus by the following steps. First, we segmented sentences in the lecture data into semantically meaningful units for the simultaneous interpreting. And then, we assigned the translations to these units from the viewpoint of the simultaneous interpreting. In addition, we investigated the possibility of automatically detecting the simultaneous interpreting timing from our corpus.
Automatically detecting discourse segments is an important preliminary step towards full discourse parsing. Previous research on discourse segmentation have relied on the assumption that elementary discourse units (EDUs) in a document always form a linear sequence (i.e., they can never be nested). Unfortunately, this assumption turns out to be too strong, for some theories of discourse, like the ''``Segmented Discourse Representation Theory'''' or SDRT, allow for nested discourse units. In this paper, we present a simple approach to discourse segmentation that is able to produce nested EDUs. Our approach builds on standard multi-class classification techniques making use of a regularized maximum entropy model, combined with a simple repairing heuristic that enforces global coherence. Our system was developed and evaluated on the first round of annotations provided by the French Annodis project (an ongoing effort to create a discourse bank for French). Cross-validated on only 47 documents (1,445 EDUs), our system achieves encouraging performance results with an F-score of 73{\%} for finding EDUs.
With the growing interest in opinion mining from web data, more works are focused on mining in English and Chinese reviews. Probing into the problem of product opinion mining, this paper describes the details of our language resources, and imports them into the task of extracting product feature and sentiment task. Different from the traditional unsupervised methods, a supervised method is utilized to identify product features, combining the domain knowledge and lexical information. Nearest vicinity match and syntactic tree based methods are proposed to identify the opinions regarding the product features. Multi-level analysis module is proposed to determine the sentiment orientation of the opinions. With the experiments on the electronic reviews of COAE 2008, the validities of the product features identified by CRFs and the two opinion words identified methods are testified and compared. The results show the resource is well utilized in this task and our proposed method is valid.
Graph and tree transducers have been applied in many NLP areas―among them, machine translation, summarization, parsing, and text generation. In particular, the successful use of tree rewriting transducers for the introduction of syntactic structures in statistical machine translation contributed to their popularity. However, the potential of such transducers is limited because they do not handle graphs and because they consume the source structure in that they rewrite it instead of leaving it intact for intermediate consultations. In this paper, we describe an open source tree and graph transducer interpreter, which combines the advantages of graph transducers and two-tape Finite State Transducers and surpasses the limitations of state-of-the-art tree rewriting transducers. Along with the transducer, we present a graph grammar development environment that supports the compilation and maintenance of graph transducer grammatical and lexical resources. Such an environment is indispensable for any effort to create consistent large coverage NLP-resources by human experts.
The need for ontologies has increased in computer science or information science recently. Especially, NLP systems such as information retrieval, machine translation, etc. require ontologies whose concepts are connected to natural language words. There are a few Korean wordnets such as U-WIN, KorLex, CoreNet, etc. Most of them, however, stand alone without any link to an ontology. Hence, we need a Korean wordnet which is linked to a language-neutral ontology such as SUMO, OpenCyc, DOLCE, etc. In this paper, we will present a method of linking Korean word senses with the concepts of an ontology, which is part of an ongoing project. We use a Korean-English bilingual dictionary, Princeton WordNet (Fellbaum 1998), and the ontology SmartSUMO (Oberle et al. 2007). The current version of WordNet is mapped into SUMO, which constitutes a major part of SmartSUMO. We focus on mapping Korean word senses with corresponding English word senses by way of Princeton WordNet which is mapped into SUMO. This paper will show that we need to apply different algorithms of linking, depending on the information types that a bilingual dictionary contains.
In this paper we describe two geometrical models of meaning representation, the Semantic Atlas (SA) and the Automatic Contexonym Organizing Model (ACOM). The SA provides maps of meaning generated through correspondence factor analysis. The models can handle different types of word relations: synonymy in the SA and co-occurrence in ACOM. Their originality relies on an artifact called 'cliques' - a fine grained infra linguistic sub-unit of meaning. The SA is composed of several dictionaries and thesauri enhanced with a process of symmetrisation. It is currently available for French and English in monolingual versions as well as in a bilingual translation version. Other languages are under development and testing. ACOM deals with unannotated corpora. The models are used by research teams worldwide that investigate synonymy, translation processes, genre comparison, psycholinguistics and polysemy modeling. Both models can be consulted online via a flexible interface allowing for interactive navigation on http://dico.isc.cnrs.fr. This site is the most consulted address of the French National Center for Scientific Researchs domain (CNRS), one of the major research bodies in France. The international interest it has triggered led us to initiate the process of going open source. In the meantime, all our databases are freely available on request.
SINotas is a data-to-text NLG application intended to produce short textual reports on students academic performance from a database conveying their grades, weekly attendance rates and related academic information. Although developed primarily as a testbed for Portuguese Natural Language Generation, SINotas generates reports of interest to both students keen to learn how their professors would describe their efforts, and to the professors themselves, who may benefit from an at-a-glance view of the students performance. In a traditional machine learning approach, SINotas uses a data-text aligned corpus as training data for decision-tree induction. The current system comprises a series of classifiers that implement major Document Planning subtasks (namely, data interpretation, content selection, within- and between-sentence structuring), and a small surface realisation grammar of Brazilian Portuguese. In this paper we focus on the evaluation work of the system, applying a number of intrinsic and user-based evaluation metrics to a collection of text reports generated from real application data.
The work analyses the head nod, a down-up movement of the head, as a polysemic social signal, that is, a signal with a number of different meanings which all share some common semantic element. Based on the analysis of 100 nods drawn from the SSPNet corpus of TV political debates, a typology of nods is presented that distinguishes Speakers, Interlocutors and Third Listeners nods, with their subtypes (confirmation, agreement, approval, submission and permission, greeting and thanks, backchannel giving and backchannel request, emphasis, ironic agreement, literal and rhetoric question, and others). For each nod the analysis specifies: 1. characteristic features of how it is produced, among which main direction, amplitude, velocity and number of repetitions; 2. cues in other modalities, like direction and duration of gaze; 3. conversational context in which the nod typically occurs. For the Interlocutors or Third Listeners nod, the preceding speech act is relevant: yes/no answer or information for a nod of confirmation, expression of opinion for one of agreement, prosocial action for greetings and thanks; for the Speakers nods, instead, their meanings are mainly distinguished by accompanying signals.
A multi-lingual speech corpus used for modeling language acquisition called CAREGIVER has been designed and recorded within the framework of the EU funded Acquisition of Communication and Recognition Skills (ACORNS) project. The paper describes the motivation behind the corpus and its design by relying on current knowledge regarding infant language acquisition. Instead of recording infants and children, the voices of their primary and secondary caregivers were captured in both infant-directed and adult-directed speech modes over four languages in a read speech manner. The challenges and methods applied to obtain similar prompts in terms of complexity and semantics across different languages, as well as the normalized recording procedures employed at different locations, is covered. The corpus contains nearly 66000 utterance based audio files spoken over a two-year period by 17 male and 17 female native speakers of Dutch, English, Finnish, and Swedish. An orthographical transcription is available for every utterance. Also, time-aligned word and phone annotations for many of the sub-corpora also exist. The CAREGIVER corpus will be published via ELRA.
An increasing demand for new language resources of recent EU members and accessing countries has in turn initiated the development of different language tools and resources, such as alignment tools and corresponding translation memories for new languages pairs. The primary goal of this paper is to provide a description of a free sentence alignment tool CorAl (Corpus Aligner), developed at the Faculty of Electrical Engineering and Computing, University of Zagreb. The tool performs paragraph alignment at the first step of the alignment process, which is followed by sentence alignment. Description of the tool is followed by its evaluation. The paper describes an experiment with applying the CorAl aligner to a English-Croatian parallel corpus of legislative domain using metrics of precision, recall and F1-measure. Results are discussed and the concluding sections discuss future directions of CorAl development.
An important feature of spoken language corpora is existence of different spelling variants of words in transcription. So there is an important problem for linguist who works with large spoken corpora: how to find all variants of the word without annotating them manually? Our work describes a search engine that enables finding different spelling variants (true positives) from corpus of spoken language, and reduces efficiently the amount of false positives returned during the search. Our search engine uses a generalized variant of the edit distance algorithm that allows defining text-specific string to string transformations in addition to the default edit operations defined in edit distance. We have extended our algorithm with capability to block transformations in specific substrings of search words. User can mark certain regions (blocked regions) of the search word where edit operations are not allowed. Our material comes from the Corpus of Spoken Estonian of the University of Tartu which consists of about 2000 dialogues and texts, about 1.4 million running text units in total.
This paper describes the Spanish Resource Grammar, an open-source multi-purpose broad-coverage precise grammar for Spanish. The grammar is implemented on the Linguistic Knowledge Builder (LKB) system, it is grounded in the theoretical framework of Head-driven Phrase Structure Grammar (HPSG), and it uses Minimal Recursion Semantics (MRS) for the semantic representation. We have developed a hybrid architecture which integrates shallow processing functionalities -- morphological analysis, and Named Entity recognition and classification -- into the parsing process. The SRG has a full coverage lexicon of closed word classes and it contains 50,852 lexical entries for open word classes. The grammar also has 64 lexical rules to perform valence changing operations on lexical items, and 191 phrase structure rules that combine words and phrases into larger constituents and compositionally build up their semantic representation. The annotation of each parsed sentence in an LKB grammar simultaneously represents a traditional phrase structure tree, and a MRS semantic representation. We provide evaluation results on sentences from newspaper texts and discuss future work.
The current PASSAGE syntactic representation is the result of 9 years of constant evolution with the aim of providing a common ground for evaluating parsers of French whatever their type and supporting theory. In this paper we present the latest developments concerning the formalism and show first through a review of basic linguistic phenomena that it is a plausible minimal common ground for representing French syntax in the context of generic black box quantitative objective evaluation. For the phenomena reviewed, which include: the notion of syntactic head, apposition, control and coordination, we explain how PASSAGE representation relates to other syntactic representation schemes for French and English, slightly extending the annotation to address English when needed. Second, we describe the XML format chosen for PASSAGE and show that it is compliant with the latest propositions in terms of linguistic annotation standard. We conclude discussing the influence that corpus-based evaluation has on the characteristics of syntactic representation when willing to assess the performance of any kind of parser.
In this paper we use statistical machine translation and morphology information from two different morphological analyzers to try to improve translation quality by linguistically motivated segmentation. The morphological analyzers we use are the unsupervised Morfessor morpheme segmentation and analyzer toolkit and the rule-based morphological analyzer T3. Our translations are done using the Moses statistical machine translation toolkit with training on the JRC-Acquis corpora and translating on Estonian to English and English to Estonian language directions. In our work we model such linguistic phenomena as word lemmas and endings and splitting compound words into simpler parts. Also lemma information was used to introduce new factors to the corpora and to use this information for better word alignment or for alternative path back-off translation. From the results we find that even though these methods have shown previously and keep showing promise of improved translation, their success still largely depends on the corpora and language pairs used.
In political speeches, the audience tends to react or resonate to signals of persuasive communication, including an expected theme, a name or an expression. Automatically predicting the impact of such discourses is a challenging task. In fact nowadays, with the huge amount of textual material that flows on the Web (news, discourses, blogs, etc.), it can be useful to have a measure for testing the persuasiveness of what we retrieve or possibly of what we want to publish on Web. In this paper we exploit a corpus of political discourses collected from various Web sources, tagged with audience reactions, such as applause, as indicators of persuasive expressions. In particular, we use this data set in a machine learning framework to explore the possibility of classifying the transcript of political discourses, according to their persuasive power, predicting the sentences that possibly trigger applause. We also explore differences between Democratic and Republican speeches, experiment the resulting classifiers in grading some of the discourses in the Obama-McCain presidential campaign available on the Web.
Unit selection text-to-speech systems currently produce very natural synthesized phrases by concatenating speech segments from a large database. Recently, increasing demand for designing high quality voices with less data has created need for further optimization of the textual corpus recorded by the speaker. This corpus is traditionally the result of a condensation process: sentences are selected from a reference corpus, using an optimization algorithm (generally greedy) guided by the coverage rate of classic units (diphones, triphones, words{\^a}¦). Such an approach is, however, strongly constrained by the finite content of the reference corpus, providing limited language possibilities. To gain flexibility in the optimization process, in this paper, we introduce a new corpus building procedure based on sentence construction rather than sentence selection. Sentences are generated using Finite State Transducers, assisted by a human operator and guided by a new frequency-weighted coverage criterion based on Vocalic Sandwiches. This semi-automatic process requires time-consuming human intervention but seems to give access to much denser corpora, with a density increase of 30 to 40{\%} for a given coverage rate.
Plain text corpora contain much information which can only be accessed through human annotation and semantic analysis, which is typically very time consuming to perform. Analysis of such texts at a syntactic or grammatical structure level can however extract some of this information in an automated manner, even if identifying effective rules can be extremely difficult. One such type of implicit information present in texts is that of definitional phrases and sentences. In this paper, we investigate the use of evolutionary algorithms to learn classifiers to discriminate between definitional and non-definitional sentences in non-technical texts, and show how effective grammar-based definition discriminators can be automatically learnt with minor human intervention.
We describe a multilingual Open Source CALL game, CALL-SLT, which reuses speech translation technology developed using the Regulus platform to create an automatic conversation partner that allows intermediate-level language students to improve their fluency. We contrast CALL-SLT with Wang's and Seneff's ``translation game'' system, in particular focussing on three issues. First, we argue that the grammar-based recognition architecture offered by Regulus is more suitable for this type of application; second, that it is preferable to prompt the student in a language-neutral form, rather than in the L1; and third, that we can profitably record successful interactions by native speakers and store them to be reused as online help for students. The current system, which will be demoed at the conference, supports four L2s (English, French, Japanese and Swedish) and two L1s (English and French). We conclude by describing an evaluation exercise, where a version of CALL-SLT configured for English L2 and French L1 was used by several hundred high school students. About half of the subjects reported positive impressions of the system.
After several years of development, the vision of the Semantic Web is gradually becoming reality. Large data repositories have been created and offer semantic information in a machine-processable form for various domains. Semantic Web data can be published on the Web, gathered automatically, and reasoned about. All these developments open interesting perspectives for building a new class of domain-specific, broad-coverage information systems that overcome a long-standing bottleneck of AI systems, the notoriously incomplete knowledge base. We present a system that shows how the wealth of information in the Semantic Web can be interfaced with humans once again, using natural language for querying and answering rather than technical formalisms. Whereas current Question Answering systems typically select snippets from Web documents retrieved by a search engine, we utilize Semantic Web data, which allows us to provide natural-language answers that are tailored to the current dialog context. Furthermore, we show how to use natural language processing technologies to acquire new data and enrich existing data in a Semantic Web framework. Our system has acquired a rich biographic data resource by combining existing Semantic Web resources, which are discovered from semi-structured textual data in Web pages, with information extracted from free natural language texts.
Besides making our thoughts more vivid and filling our communication with richer imagery, metaphor also plays an important structural role in our cognition. Although there is a consensus in the linguistics and NLP research communities that the phenomenon of metaphor is not restricted to similarity-based extensions of meanings of isolated words, but rather involves reconceptualization of a whole area of experience (target domain) in terms of another (source domain), there still has been no proposal for a comprehensive procedure for annotation of cross-domain mappings. However, a corpus annotated for conceptual mappings could provide a new starting point for both linguistic and cognitive experiments. The annotation scheme we present in this paper is a step towards filling this gap. We test our procedure in an experimental setting involving multiple annotators and estimate their agreement on the task. The associated corpus annotated for source ― target domain mappings will be publicly available.
In this paper we describe FragmentSeeker, a tool which is capable to identify all those tree constructions which are recurring multiple times in a large Phrase Structure treebank. The tool is based on an efficient kernel-based dynamic algorithm, which compares every pair of trees of a given treebank and computes the list of fragments which they both share. We describe two different notions of fragments we will use, i.e. standard and partial fragments, and provide the implementation details on how to extract them from a syntactically annotated corpus. We have tested our system on the Penn Wall Street Journal treebank for which we present quantitative and qualitative analysis on the obtained recurring structures, as well as provide empirical time performance. Finally we propose possible ways our tool could contribute to different research fields related to corpus analysis and processing, such as parsing, corpus statistics, annotation guidance, and automatic detection of argument structure.
The present paper outlines the Vergina speech database, which was developed in support of research and development of corpus-based unit selection and statistical parametric speech synthesis systems for Modern Greek language. In the following, we describe the design, development and implementation of the recording campaign, as well as the annotation of the database. Specifically, a text corpus of approximately 5 million words, collected from newspaper articles, periodicals, and paragraphs of literature, was processed in order to select the utterances-sentences needed for producing the speech database and to achieve a reasonable phonetic coverage. The broad coverage and contents of the selected utterances-sentences of the database ― text corpus collected from different domains and writing styles ― makes this database appropriate for various application domains. The database, recorded in audio studio, consists of approximately 3,000 phonetically balanced Modern Greek utterances corresponding to approximately four hours of speech. Annotation of the Vergina speech database was performed using task-specific tools, which are based on a hidden Markov model (HMM) segmentation method, and then manual inspection and corrections were performed.
This paper describes a multi-lingual large-scale concept network obtained automatically by mining for concepts and relations and exploiting a variety of sources of knowledge from Wikipedia. Concepts and their lexicalizations are extracted from Wikipedia pages, in particular from article titles, hyperlinks, disambiguation pages and cross-language links. Relations are extracted from the category and page network, from the category names, from infoboxes and the body of the articles. The resulting network has two main components: (i) a central, language independent index of concepts, which serves to keep track of the concepts' lexicalizations both within a language and across languages, and to separate linguistic expressions of concepts from the relations in which they are involved (concepts themselves are represented as numeric IDs); (ii) a large network built on the basis of the relations extracted, represented as relations between concepts (more specifically, the numeric IDs). The various stages of obtaining the network were separately evaluated, and the results show a qualitative resource.
A considerable amount of work has been put into development of stemmers and morphological analysers. The majority of these approaches use hand-crafted suffix-replacement rules but a few try to discover such rules from corpora. While most of the approaches remove or replace suffixes, there are examples of derivational stemmers which are based on prefixes as well. In this paper we present a rule-based morphological analyser. We propose an approach that takes both prefixes as well as suffixes into account. Given a corpus and a dictionary, our method can be used to obtain a set of suffix-replacement rules for deriving an inflected words root form. We developed an approach for the Hindi language but show that the approach is portable, at least to related languages, by adapting it to the Gujarati language. Given that the entire process of developing such a ruleset is simple and fast, our approach can be used for rapid development of morphological analysers and yet it can obtain competitive results with analysers built relying on human authored rules.
This paper is a report on an on-going project of creating a new corpus focusing on Japanese particles. The corpus will provide deeper syntactic/semantic information than the existing resources. The initial target particle is ``to'' which occurs 22,006 times in 38,400 sentences of the existing corpus: the Kyoto Text Corpus. In this annotation task, an ``example-based'' methodology is adopted for the corpus annotation, which is different from the traditional annotation style. This approach provides the annotators with an example sentence rather than a linguistic category label. By avoiding linguistic technical terms, it is expected that any native speakers, with no special knowledge on linguistic analysis, can be an annotator without long training, and hence it can reduce the annotation cost. So far, 10,475 occurrences have been already annotated, with an inter-annotator agreement of 0.66 calculated by Cohen's kappa. The initial disagreement analyses and future directions are discussed in the paper.
Idioms and other figuratively used expressions pose considerable problems to natural language processing applications because they are very frequent and often behave idiosyncratically. Consequently, there has been much research on the automatic detection and extraction of idiomatic expressions. Most studies focus on type-based idiom detection, i.e., distinguishing whether a given expression can (potentially) be used idiomatically. However, many expressions such as ''``break the ice'''' can have both literal and non-literal readings and need to be disambiguated in a given context (token-based detection). So far relatively few approaches have attempted context-based idiom detection. One reason for this may be that few annotated resources are available that disambiguate expressions in context. With the IDIX corpus, we aim to address this. IDIX is available as an add-on to the BNC and disambiguates different usages of a subset of idioms. We believe that this resource will be useful both for linguistic and computational linguistic studies.
The present paper reports on a recent effort that resulted in the establishment of a unique multimodal affect database, referred to as the PlayMancer database. This database was created in support of the research and development activities, taking place within the PlayMancer project, which aim at the development of a serious game environment in support of treatment of patients with behavioural and addictive disorders, such as eating disorders and gambling addictions. Specifically, for the purpose of data collection, we designed and implemented a pilot trial with healthy test subjects. Speech, video and bio-signals (pulse-rate, SpO2) were captured synchronously, during the interaction of healthy people with a number of video games. The collected data were annotated by the test subjects (self-annotation), targeting proper interpretation of the underlying affective states. The broad-shouldered design of the PlayMancer database allows its use for the needs of research on multimodal affect-emotion recognition and multimodal human-computer interaction in serious games environment.
The multilingual diversity of India is one of the most unique in world. Currently there are 22 constitutionally recognized languages with 12 scripts. Apart from these, there are at least 35 different languages and 2000 dialects in 4 major language families. It is thus evident that, development and proliferation of software solutions in the Indic multilingual environment requires continuous and sustained effort to edge out challenges in all core areas namely storage and encoding, input mechanism, browser support and data exchange. Linguistic Resources and Tools are the key building blocks to develop multilingual solutions. In this paper, we shall present an overview of the major national initiative in India for the development and standardization of Linguistic Resources and Tools for developing and deployment of multilingual ICT solutions in India.
The phenomenon of coreference, covering entities, their mentions and their properties, is intricately linked to the phenomenon of coherence, covering the structure of rhetorical relations in a discourse. A text corpus that has both phenomena annotated can be used to test hypotheses about their interrelation or to detect other phenomena. We present the process by which C-3, a new corpus, was obtained by annotating the Discourse GraphBank coherence corpus with entity and mention information. The annotation followed a set of ACE guidelines adapted to favor coreference and to include entities of unknown types in the annotation. Together with the corpus we offer a new annotation tool specifically designed to annotate entity and mention information within a simple and functional graphical interface that combines the best of all worlds from available annotation tools. The potential usefulness of C-3 is discussed, as well as an application in which the corpus proved to be a valuable resource.
We describe a process for converting the Penn Arabic Treebank into the CCG formalism. Previous efforts have yielded CCGbanks in English, German, and Turkish, thus opening these languages to the sophisticated computational tools developed for CCG and enabling further cross-linguistic development. Conversion from a context free grammar treebank to a CCGbank is a four stage process: head finding, argument classification, binarization, and category conversion. In the process of implementing a basic CCGbank conversion algorithm, we reveal properties of Arabic grammar that interfere with conversion, such as subject topicalization, genitive constructions, relative clauses, and optional pronominal subjects. All of these problematic phenomena can be resolved in a variety of ways - we discuss advantages and disadvantages of each in their respective sections. We detail these and describe our categorial analysis of each of these Arabic grammatical phenomena in depth, as well as technical details on their integration into the conversion algorithm.
This paper presents the rationale, objectives and advances of an on-going project (the DesPho-APaDy project funded by the French National Agency of Research) which aims to provide a systematic and quantified description of French dysarthric speech, over a large population of patients and three dysarthria types (related to the parkinson's disease, the Amyotrophic Lateral Sclerosis disease, and a pure cerebellar alteration). The two French corpora of dysarthric patients, from which the speech data have been selected for analysis purposes, are firstly described. Secondly, this paper discusses and outlines the requirement of a structured and organized computerized platform in order to store, organize and make accessible (for selected and protected usage) dysarthric speech corpora and associated patients clinical information (mostly disseminated in different locations: labs, hospitals, {\^a}¦). The design of both a computer database and a multi-field query interface is proposed for the clinical context. Finally, advances of the project related to the selection of the population used for the dysarthria analysis, the preprocessing of the speech files, their orthographic transcription and their automatic alignment are also presented.
Analysts in various domains, especially intelligence and financial, have to constantly extract useful knowledge from large amounts of unstructured or semi-structured data. Keyword-based search, faceted search, question-answering, etc. are some of the automated methodologies that have been used to help analysts in their tasks. General-purpose and domain-specific ontologies have been proposed to help these automated methods in organizing data and providing access to useful information. However, problems in ontology creation and maintenance have resulted in expensive procedures for expanding/maintaining the ontology library available to support the growing and evolving needs of analysts. In this paper, we present a generalized and improved procedure to automatically extract deep semantic information from text resources and rapidly create semantically-rich domain ontologies while keeping the manual intervention to a minimum. We also present evaluation results for the intelligence and financial ontology libraries, semi-automatically created by our proposed methodologies using freely-available textual resources from the Web.
In this paper, we present a brief snapshot of the state of affairs in computational processing of Catalan and the initiatives that are starting to take place in an effort to bring the field a step forward, by making a better and more efficient use of the already existing resources and tools, by bridging the gap between research and market, and by establishing periodical meeting points for the community. In particular, we present the results of the First Workshop on the Computational Processing of Catalan, which succeeded in putting together a fair representation of the research in the area, and received attention from both the industry and the administration. Aside from facilitating communication among researchers and between developers and users, the Workshop provided the organizers with valuable information about existing resources, tools, developers and providers. This information has allowed us to go a step further by setting up a harvesting procedure which will hopefully build the seed of a portal-catalogue-observatory of language resources and technologies in Catalan.
We have recently converted a dependency treebank, consisting of ancient Greek and Latin texts, from one annotation scheme to another that was independently designed. This paper makes two observations about this conversion process. First, we show that, despite significant surface differences between the two treebanks, a number of straightforward transformation rules yield a substantial level of compatibility between them, giving evidence for their sound design and high quality of annotation. Second, we analyze some linguistic annotations that require further disambiguation, proposing some simple yet effective machine learning methods.
This paper presents a comparison of three computational approaches to selectional preferences: (i) an intuitive distributional approach that uses second-order co-occurrence of predicates and complement properties; (ii) an EM-based clustering approach that models the strengths of predicate--noun relationships by latent semantic clusters (Rooth et al., 1999); and (iii) an extension of the latent semantic clusters by incorporating the MDL principle into the EM training, thus explicitly modelling the predicate--noun selectional preferences by WordNet classes (Schulte im Walde et al., 2008). Concerning the distributional approach, we were interested not only in how well the model describes selectional preferences, but moreover which second-order properties are most salient. For example, a typical direct object of the verb 'drink' is usually fluid, might be hot or cold, can be bought, might be bottled, etc. The general question we ask is: what characterises the predicate's restrictions to the semantic realisation of its complements? Our second interest lies in the actual comparison of the models: How does a very simple distributional model compare to much more complex approaches, and which representation of selectional preferences is more appropriate, using (i) second-order properties, (ii) an implicit generalisation of nouns (by clusters), or (iii) an explicit generalisation of nouns by WordNet classes within clusters? We describe various experiments on German data and two evaluations, and demonstrate that the simple distributional model outperforms the more complex cluster-based models in most cases, but does itself not always beat the powerful frequency baseline.
Previous content extraction evaluations have neglected to address problems which complicate the incorporation of extracted information into an existing knowledge base. Previous question answering evaluations have likewise avoided tasks such as explicit disambiguation of target entities and handling a fixed set of questions about entities without previous determination of possible answers. In 2009 NIST conducted a Knowledge Base Population track at its Text Analysis Conference to unite the content extraction and question answering communities and jointly explore some of these issues. This exciting new evaluation attracted 13 teams from 6 countries that submitted results in two tasks, Entity Linking and Slot Filling. This paper explains the motivation and design of the tasks, describes the language resources that were developed for this evaluation, offers comparisons to previous community evaluations, and briefly summarizes the performance obtained by systems. We also identify relevant issues pertaining to target selection, challenging queries, and performance measures.
This paper presents an algorithm for aligning FrameNet lexical units to WordNet synsets. Both, FrameNet and WordNet, are well-known as well as widely-used resources by the entire research community. They help systems in the comprehension of the semantics of texts, and therefore, finding strategies to link FrameNet and WordNet involves challenges related to a better understanding of the human language. Such deep analysis is exploited by researchers to improve the performance of their applications. The alignment is achieved by exploiting the particular characteristics of each lexical-semantic resource, with special emphasis on the explicit, formal semantic relations in each. Semantic neighborhoods are computed for each alignment of lemmas, and the algorithm calculates correlation scores by comparing such neighborhoods. The results suggest that the proposed algorithm is appropriate for aligning the FrameNet and WordNet hierarchies. Furthermore, the algorithm can aid research on increasing the coverage of FrameNet, building FrameNets in other languages, and creating a system for querying a joint FrameNet-WordNet hierarchy.
Despite Arabic is the language of hundred millions of people over the world, little has been done in terms of computerized linguistic resources, tools or applications. In this paper we describe a project which aim is to contribute filling this gap. The project consists in building an ontology centered infrastructure for Arabic Language resources and applications. The core of this infrastructure is a linguistic ontology that is founded on Arabic Traditional Grammar. The methodology we have chosen consists in reusing an existing ontology, namely the Gold linguistic ontology. GOLD is the first ontology being designed for linguistic description on the semantic web. We first construct our ontology manually by relating our concepts from Arabic Linguistics to the upper concepts of GOLD, furthermore an information extraction algorithm is implemented to automatically enrich the ontology. We discuss the development of the ontology and present our vision for the whole project which aims at using this ontology for creating tools and resources for both linguists and NLP Researchers. Indeed, the ontology is seen , not only as a domain ontology but also as a resource for different linguistic and NLP applications.
We present a software module, the LAT Bridge, which enables bidirectional communication between the annotation and exploration tools developed at the Max Planck Institute for Psycholinguistics as part of our Language Archiving Technology (LAT) tool suite. These existing annotation and exploration tools enable the annotation, enrichment, exploration and archive management of linguistic resources. The user community has expressed the desire to use different combinations of LAT tools in conjunction with each other. The LAT Bridge is designed to cater for a number of basic data interaction scenarios between the LAT annotation and exploration tools. These interaction scenarios (e.g. bootstrapping a wordlist, searching for annotation examples or lexical entries) have been identified in collaboration with researchers at our institute. We had to take into account that the LAT tools for annotation and exploration represent a heterogeneous application scenario with desktop-installed and web-based tools. Additionally, the LAT Bridge has to work in situations where the Internet is not available or only in an unreliable manner (i.e. with a slow connection or with frequent interruptions). As a result, the LAT Bridges architecture supports both online and offline communication between the LAT annotation and exploration tools.
CzEng 0.9 is the third release of a large parallel corpus of Czech and English. For the current release, CzEng was extended by significant amount of texts from various types of sources, including parallel web pages, electronically available books and subtitles. This paper describes and evaluates filtering techniques employed in the process in order to avoid misaligned or otherwise damaged parallel sentences in the collection. We estimate the precision and recall of two sets of filters. The first set was used to process the data before their inclusion into CzEng. The filters from the second set were newly created to improve the filtering process for future releases of CzEng. Given the overall amount and variance of sources of the data, our experiments illustrate the utility of parallel data sources with respect to extractable parallel segments. As a similar behaviour can be expected for other language pairs, our results can be interpreted as guidelines indicating which sources should other researchers exploit first.
We present two complementary annotation schemes for sentence based annotation of full scientific papers, CoreSC and AZ-II, applied to primary research articles in chemistry. AZ-II is the extension of AZ for chemistry papers. AZ has been shown to have been reliably annotated by independent human coders and useful for various information access tasks. Like AZ, AZ-II follows the rhetorical structure of a scientific paper and the knowledge claims made by the authors. The CoreSC scheme takes a different view of scientific papers, treating them as the humanly readable representations of scientific investigations. It seeks to retrieve the structure of the investigation from the paper as generic high-level Core Scientific Concepts (CoreSC). CoreSCs have been annotated by 16 chemistry experts over a total of 265 full papers in physical chemistry and biochemistry. We describe the differences and similarities between the two schemes in detail and present the two corpora produced using each scheme. There are 36 shared papers in the corpora, which allows us to quantitatively compare aspects of the annotation schemes. We show the correlation between the two schemes, their strengths and weeknesses and discuss the benefits of combining a rhetorical based analysis of the papers with a content-based one.
Aggregation of long lists of concepts is important to avoid overwhelming a small display. Focusing on the domain of mobile local search, this paper presents the development of an application to perform filtering and aggregation of results obtained through the Yahoo! Local web service. First, we performed an analysis of the data available through Yahoo! Local by crawling its database with over 170 thousand local listings located in Chicago. Then, we compiled resources and developed algorithms to filter and aggregate local search results. The methods developed exploit Yahoo!s listings categorization to reduce the result space and pinpoint the category containing the most relevant results. Finally, we evaluated a prototype through a user study, which pitted our system against Yahoo! Local and against a plain list of search results. The results obtained from the study show that our aggregation methods are quite effective, cutting down the number of entries returned to the user by 43{\%} on average, but leaving search efficiency and user satisfaction unaffected.
This paper presents the EPAC corpus which is composed by a set of 100 hours of conversational speech manually transcribed and by the outputs of automatic tools (automatic segmentation, transcription, POS tagging, etc.) applied on the entire French ESTER 1 audio corpus: this concerns about 1700 hours of audio recordings from radiophonic shows. This corpus was built during the EPAC project funded by the French Research Agency (ANR) from 2007 to 2010. This corpus increases significantly the amount of French manually transcribed audio recordings easily available and it is now included as a part of the ESTER 1 corpus in the ELRA catalog without additional cost. By providing a large set of automatic outputs of speech processing tools, the EPAC corpus should be useful to researchers who want to work on such data without having to develop and deal with such tools. These automatic annotations are various: segmentation and speaker diarization, one-best hypotheses from the LIUM automatic speech recognition system with confidence measures, but also word-lattices and confusion networks, named entities, part-of-speech tags, chunks, etc. The 100 hours of speech manually transcribed were split into three data sets in order to get an official training corpus, an official development corpus and an official test corpus. These data sets were used to develop and to evaluate some automatic tools which have been used to process the 1700 hours of audio recording. For example, on the EPAC test data set our ASR system yields a word error rate equals to 17.25{\%}.
We describe the re-annotation of selected types of named entities (persons, organizations, locations) from the Muc7 corpus. The focus of this annotation initiative is on recording the time needed for the linguistic process of named entity annotation. Annotation times are measured on two basic annotation units -- sentences vs. complex noun phrases. We gathered evidence that decision times are non-uniformly distributed over the annotation units, while they do not substantially deviate among annotators. This data seems to support the hypothesis that annotation times very much depend on the inherent ''``hardness'''' of each single annotation decision. We further show how such time-stamped information can be used for empirically grounded studies of selective sampling techniques, such as Active Learning. We directly compare Active Learning costs on the basis of token-based vs. time-based measurements. The data reveals that Active Learning keeps its competitive advantage over random sampling in both scenarios though the difference is less marked for the time metric than for the token metric.
The Enron Email Corpus provides ``Real World'' text in the business email domain, which is a target domain for many speech and language applications. We present a section of this corpus annotated with number senses - labelling each number as a date, time, year, telephone number etc. We show that sense categories and their frequencies are very different in this domain than in newswire text. The annotated corpus can provide valuable material for the development of number sense disambiguation techniques. We have released the annotations into the public domain, to allow other researchers to perform comparisons.
Propp's influential structural analysis of fairy tales created a powerful schema for representing storylines in terms of character functions, which is directly exploitable for computational semantic analysis, and procedural generation of stories of this genre. We tackle two resources that draw on the Proppian model - one formalizes it as a semantic markup scheme and the other as an ontology -, both lacking linguistic phenomena explicitly represented in them. The need for integrating linguistic information into structured semantic resources is motivated by the emergence of suitable standards that facilitate this, as well as the benefits such joint representation would create for transdisciplinary research across Digital Humanities, Computational Linguistics, and Artificial Intelligence.
This paper describes a Web service for accessing WordNet-type semantic lexicons. The central idea behind the service design is: given a query, the primary functionality of lexicon access is to present a partial lexicon by extracting the relevant part of the target lexicon. Based on this idea, we implemented the system as a RESTful Web service whose input query is specified by the access URI and whose output is presented in a standardized XML data format. LMF, an ISO standard for modeling lexicons, plays the most prominent role: the access URI pattern basically reflects the lexicon structure as defined by LMF; the access results are rendered based on Wordnet-LMF, which is a version of LMF XML-serialization. The Web service currently provides accesses to Princeton WordNet, Japanese WordNet, as well as the EDR Electronic Dictionary as a trial. To accommodate the EDR dictionary within the same framework, we modeled it also as a WordNet-type semantic lexicon. This paper thus argues possible alternatives to model innately bilingual/multilingual lexicons like EDR with LMF, and proposes possible revisions to Wordnet-LMF.
This paper describes how we built a dependency Treebank for questions. The questions for the Treebank were drawn from questions from the TREC 10 QA task and from Yahoo! Answers. Among the uses for the corpus is to train a dependency parser achieving good accuracy on parsing questions without hurting its overall accuracy. We also explore active learning techniques to determine the suitable size for a corpus of questions in order to achieve adequate accuracy while minimizing the annotation efforts.
The meanings of words are not fixed but in fact undergo change, with new word senses arising and established senses taking on new aspects of meaning or falling out of usage. Two types of semantic change are amelioration and pejoration; in these processes a word sense changes to become more positive or negative, respectively. In this first computational study of amelioration and pejoration we adapt a web-based method for determining semantic orientation to the task of identifying ameliorations and pejorations in corpora from differing time periods. We evaluate our proposed method on a small dataset of known historical ameliorations and pejorations, and find it to perform better than a random baseline. Since this test dataset is small, we conduct a further evaluation on artificial examples of amelioration and pejoration, and again find evidence that our proposed method is able to identify changes in semantic orientation. Finally, we conduct a preliminary evaluation in which we apply our methods to the task of finding words which have recently undergone amelioration or pejoration.
NPCEditor is a system for building and deploying virtual characters capable of engaging a user in spoken dialog on a limited domain. The dialogue may take any form as long as the character responses can be specified a priori. For example, NPCEditor has been used for constructing question answering characters where a user asks questions and the character responds, but other scenarios are possible. At the core of the system is a state of the art statistical language classification technology for mapping from user's text input to system responses. NPCEditor combines the classifier with a database that stores the character information and relevant language data, a server that allows the character designer to deploy the completed characters, and a user-friendly editor that helps the designer to accomplish both character design and deployment tasks. In the paper we define the overall system architecture, describe individual NPCEditor components, and guide the reader through the steps of building a virtual character.
Textual information is an important communication medium contained rich expression of emotion, and emotion recognition on text has wide applications. Word emotion analysis is fundamental in the problem of textual emotion recognition. Through an analysis of the characteristics of word emotion expression, we use word emotion vector to describe the combined basic emotions in a word, which can be used to distinguish direct and indirect emotion words, express emotion ambiguity in words, and express multiple emotions in words. Based on Ren-CECps (a Chinese emotion corpus), we do an experiment to explore the role of emotion word for sentence emotion recognition and we find that the emotions of a simple sentence (sentence without negative words, conjunctions, or question mark) can be approximated by an addition of the word emotions. Then MaxEnt modeling is used to find which context features are effective for recognizing word emotion in sentences. The features of word, N-words, POS, Pre-N-words emotion, Pre-is-degree-word, Pre-is-negativeword, Pre-is-conjunction and their combination have been experimented. After that, we use the two metrics: Kappa coefficient of agreement and Voting agreement to measure the word annotation agreement of Ren-CECps. The experiments on above context features showed promising results compared with word emotion agreement on people's judgments.
The goal of our research is the development of algorithms for automatic estimation of a person's verbal intelligence based on the analysis of transcribed spoken utterances. In this paper we present the corpus of German native speakers' monologues and dialogues about the same topics collected at the University of Ulm, Germany. The monologues were descriptions of two short films; the dialogues were discussions about problems of German education. The data corpus contains the verbal intelligence quotients of each speaker, which were measured with the Hamburg Wechsler Intelligence Test for Adults. In this paper we describe our corpus, why we decided to create it, and how it was collected. We also describe some approaches which can be applied to the transcribed spoken utterances for extraction of different features which could have a correlation with a person's verbal intelligence. The data corpus consists of 71 monologues and 30 dialogues (about 10 hours of audio data).
In this paper, we present the design, collection, transcription and analysis of a Mandarin Chinese Broadcast Collection of over 3000 hours. The data was collected by Hong Kong University of Science and Technology (HKUST) in China on a cable TV and satellite transmission platform established in support of the DARPA Global Autonomous Language Exploitation (GALE) program. The collection includes broadcast news (BN) and broadcast conversation (BC) including talk shows, roundtable discussions, call-in shows, editorials and other conversational programs that focus on news and current events. HKUST also collects detailed information about all recorded programs. A subset of BC and BN recordings are manually transcribed with standard Chinese characters in UTF-8 encoding, using specific mark-ups for a small set of spontaneous and conversational speech phenomena. The collection is among the largest and first of its kind for Mandarin Chinese Broadcast speech, providing abundant and diverse samples for Mandarin speech recognition and other application-dependent tasks, such as spontaneous speech processing and recognition, topic detection, information retrieval, and speaker recognition. HKUST{\^a}s acoustic analysis of 500 hours of the speech and transcripts demonstrates the positive impact this data could have on system performance.
We present in this paper an on-going research: the construction and annotation of a Romanian Generative Lexicon (RoGL). Our system follows the specifications of CLIPS project for Italian language. It contains a corpus, a type ontology, a graphical interface and a database from which we generate data in XML format.
In this paper we bring to light a novel intersection between corpus linguistics and behavioral data that can be employed as an evaluation metric for resources for low-density languages, drawing on well-established psycholinguistic factors. Using the low-density language Maltese as a test case, we highlight the challenges that face researchers developing resources for languages with sparsely available data and identify a key empirical link between corpus and psycholinguistic research as a tool to evaluate corpus resources. Specifically, we compare two robust variables identified in the psycholinguistic literature: word frequency (as measured in a corpus) and word familiarity (as measured in a rating task). We then apply statistical methods to evaluate the extent to which familiarity ratings predict corpus frequency for verbs in the Maltese corpus from three angles: 1) token frequency, 2) frequency distributions and 3) morpho-syntactic type (binyan). This research provides a multidisciplinary approach to corpus development and evaluation, in particular for less-resourced languages that lack a wide access to diverse language data.
The development of technologies to address machine translation and distillation of multilingual broadcast data depends heavily on the collection of large volumes of material from modern data providers. To address the needs of GALE researchers, the Linguistic Data Consortium (LDC) developed a system for collecting broadcast news and conversation from a variety of Arabic, Chinese and English broadcasters. The system is highly automated, easily extensible and robust and is capable of collecting, processing and evaluating hundreds of hours of content from several dozen sources per day. In addition to this extensive system, LDC manages three remote collection sites to maximize the variety of available broadcast data and has designed a portable broadcast collection platform to facilitate remote collection. This paper will present a detailed a description of the design and implementation of LDCs collection system, the technical challenges and solutions to large scale broadcast data collection efforts and an overview of the systems operation. This paper will also discuss the challenges of managing remote collections, in particular, the strategies used to normalize data formats, naming conventions and delivery methods to achieve optimal integration of remotely-collected data into LDCs collection database and downstream tasking workflow.
Our paper presents the details of a pilot study in which we tagged portions of the American National Corpus (ANC) for idioms composed of verb-noun constructions, prepositional phrases, and subordinate clauses. The three data sets we analyzed included 1,500-sentence samples from the spoken, the nonfiction, and the fiction portions of the ANC. Our paper provides the details of the tagset we developed, the motivation behind our choices, and the inter-annotator agreement measures we deemed appropriate for this task. In tagging the ANC for idiomatic expressions, our annotators achieved a high level of agreement ({\textgreater} .80) on the tags but a low level of agreement ({\textless} .00) on what constituted an idiom. These findings support the claim that identifying idiomatic and metaphorical expressions is a highly difficult and subjective task. In total, 135 idiom types and 154 idiom tokens were identified. Based on the total tokens found for each idiom class, we suggest that future research on idiom detection and idiom annotation include prepositional phrases as this class of idioms occurred frequently in the nonfiction and spoken samples of our corpus
We present a fully functional Arabic information extraction (IE) system that is used to analyze large volumes of news texts every day to extract the named entity (NE) types person, organization, location, date and number, as well as quotations (direct reported speech) by and about people. The Named Entity Recognition (NER) system was not developed for Arabic, but - instead - a highly multilingual, almost language-independent NER system was adapted to also cover Arabic. The Semitic language Arabic substantially differs from the Indo-European and Finno-Ugric languages currently covered. This paper thus describes what Arabic language-specific resources had to be developed and what changes needed to be made to the otherwise language-independent rule set in order to be applicable to the Arabic language. The achieved evaluation results are generally satisfactory, but could be improved for certain entity types. The results of the IE tools can be seen on the Arabic pages of the freely accessible Europe Media Monitor (EMM) application NewsExplorer, which can be found at http://press.jrc.it/overview.html.
Incorporating linguistic knowledge into word alignment is becoming increasingly important for current approaches in statistical machine translation research. To improve automatic word alignment and ultimately machine translation quality, an annotation framework is jointly proposed by LDC (Linguistic Data Consortium) and IBM. The framework enriches word alignment corpora to capture contextual, syntactic and language-specific features by introducing linguistic tags to the alignment annotation. Two annotation schemes constitute the framework: alignment and tagging. The alignment scheme aims to identify minimum translation units and translation relations by using minimum-match and attachment annotation approaches. A set of word tags and alignment link tags are designed in the tagging scheme to describe these translation units and relations. The framework produces a solid ground-level alignment base upon which larger translation unit alignment can be automatically induced. To test the soundness of this work, evaluation is performed on a pilot annotation, resulting in inter- and intra- annotator agreement of above 90{\%}. To date LDC has produced manual word alignment and tagging on 32,823 Chinese-English sentences following this framework.
This paper introduces a novel corpus of natural language dialogues obtained from humans performing a cooperative, remote, search task (CReST) as it occurs naturally in a variety of scenarios (e.g., search and rescue missions in disaster areas). This corpus is unique in that it involves remote collaborations between two interlocutors who each have to perform tasks that require the other's assistance. In addition, one interlocutor's tasks require physical movement through an indoor environment as well as interactions with physical objects within the environment. The multi-modal corpus contains the speech signals as well as transcriptions of the dialogues, which are additionally annotated for dialog structure, disfluencies, and for constituent and dependency syntax. On the dialogue level, the corpus was annotated for separate dialogue moves, based on the classification developed by Carletta et al. (1997) for coding task-oriented dialogues. Disfluencies were annotated using the scheme developed by Lickley (1998). The syntactic annotation comprises POS annotation, Penn Treebank style constituent annotations as well as dependency annotations based on the dependencies of pennconverter.
In this paper we discuss the methodology behind the construction of elicited imitation (EI) test items. First we examine varying uses for EI tests in research and in testing overall oral proficiency. We also mention criticisms of previous test items. Then we identify the factors that contribute to the difficulty of an EI item as shown in previous studies. Based on this discussion, we describe a way of automating the creation of test items in order to better evaluate language learners' oral proficiency while improving item naturalness. We present a new item construction tool and the process that it implements in order to create test items from a corpus, identifying relevant features needed to compile a database of EI test items. We examine results from administration of a new EI test engineered in this manner, illustrating the effect that standard language resources can have on creating an effective EI test item repository. We also sketch ongoing work on test item generation for other languages and an adaptive test that will use this collection of test items.
This paper describes the development of a hybrid tool for a semi-automated process for validation of treebank annotation at various levels. The tool is developed for error detection at the part-of-speech, chunk and dependency levels of a Hindi treebank, currently under development. The tool aims to identify as many errors as possible at these levels to achieve consistency in the task of annotation. Consistency in treebank annotation is a must for making data as error-free as possible and for providing quality assurance. The tool is aimed at ensuring consistency and to make manual validation cost effective. We discuss a rule based and a hybrid approach (statistical methods combined with rule-based methods) by which a high-recall system can be developed and used to identify errors in the treebank. We report some results of using the tool on a sample of data extracted from the Hindi treebank. We also argue how the tool can prove useful in improving the annotation guidelines which would in turn, better the quality of annotation in subsequent iterations.
As conversational agents are now being developed to encounter more complex dialogue situations it is increasingly difficult to find satisfactory methods for evaluating these agents. Task-based measures are insufficient where there is no clearly defined task. While user-based evaluation methods may give a general sense of the quality of an agent's performance, they shed little light on the relative quality or success of specific features of dialogue that are necessary for system improvement. This paper examines current dialogue agent evaluation practices and motivates the need for a more detailed approach for defining and measuring the quality of dialogues between agent and user. We present a framework for evaluating the dialogue competence of artificial agents involved in complex and underspecified tasks when conversing with people. A multi-part coding scheme is proposed that provides a qualitative analysis of human utterances, and rates the appropriateness of the agent's responses to these utterances. The scheme is outlined, and then used to evaluate Staff Duty Officer Moleno, a virtual guide in Second Life.
We perform a large-scale evaluation of multiple off-the-shelf speech recognizers across diverse domains for virtual human dialogue systems. Our evaluation is aimed at speech recognition consumers and potential consumers with limited experience with readily available recognizers. We focus on practical factors to determine what levels of performance can be expected from different available recognizers in various projects featuring different types of conversational utterances. Our results show that there is no single recognizer that outperforms all other recognizers in all domains. The performance of each recognizer may vary significantly depending on the domain, the size and perplexity of the corpus, the out-of-vocabulary rate, and whether acoustic and language model adaptation has been used or not. We expect that our evaluation will prove useful to other speech recognition consumers, especially in the dialogue community, and will shed some light on the key problem in spoken dialogue systems of selecting the most suitable available speech recognition system for a particular application, and what impact training will have.
This paper introduces a new corpus of consulting dialogues designed for training a dialogue manager that can handle consulting dialogues through spontaneous interactions from the tagged dialogue corpus. We have collected more than 150 hours of consulting dialogues in the tourist guidance domain. We are developing the corpus that consists of speech, transcripts, speech act (SA) tags, morphological analysis results, dependency analysis results, and semantic content tags. This paper outlines our taxonomy of dialogue act (DA) annotation that can describe two aspects of an utterance: the communicative function (SA), and the semantic content of the utterance. We provide an overview of the Kyoto tour dialogue corpus and a preliminary analysis using the DA tags. We also show a result of a preliminary experiment for SA tagging via Support Vector Machines (SVMs). We introduce the current states of the corpus development In addition, we mention the usage of our corpus for the spoken dialogue system that is being developed.
This paper describes an annotation scheme for argumentation in opinionated texts such as newspaper editorials, developed from a corpus of approximately 500 English texts from Nepali and international newspaper sources. We present the results of analysis and evaluation of the corpus annotation ― currently, the inter-annotator agreement kappa value being 0.80 which indicates substantial agreement between the annotators. We also discuss some of linguistic resources (key factors for distinguishing facts from opinions, opinion lexicon, intensifier lexicon, pre-modifier lexicon, modal verb lexicon, reporting verb lexicon, general opinion patterns from the corpus etc.) developed as a result of our corpus analysis, which can be used to identify an opinion or a controversial issue, arguments supporting an opinion, orientation of the supporting arguments and their strength (intrinsic, relative and in terms of persuasion). These resources form the backbone of our work especially for performing the opinion analysis in the lower levels, i.e., in the lexical and sentence levels. Finally, we shed light on the perspectives of the given work clearly outlining the challenges.
This paper presents the preliminary works to put online a French oral corpus and its transcription. This corpus is the Socio-Linguistic Survey in Orleans, realized in 1968. First, we numerized the corpus, then we handwritten transcribed it with the Transcriber software adding different tags about speakers, time, noise, etc. Each document (audio file and XML file of the transcription) was described by a set of metadata stored in an XML format to allow an easy consultation. Second, we added different levels of annotations, recognition of named entities and annotation of personal information about speakers. This two annotation tasks used the CasSys system of transducer cascades. We used and modified a first cascade to recognize named entities. Then we built a second cascade to annote the designating entities, i.e. information about the speaker. These second cascade parsed the named entity annotated corpus. The objective is to locate information about the speaker and, also, what kind of information can designate him/her. These two cascades was evaluated with precision and recall measures.
Currently, research infrastructures are being designed and established in many disciplines since they all suffer from an enormous fragmentation of their resources and tools. In the domain of language resources and tools the CLARIN initiative has been funded since 2008 to overcome many of the integration and interoperability hurdles. CLARIN can build on knowledge and work from many projects that were carried out during the last years and wants to build stable and robust services that can be used by researchers. Here service centres will play an important role that have the potential of being persistent and that adhere to criteria as they have been established by CLARIN. In the last year of the so-called preparatory phase these centres are currently developing four use cases that can demonstrate how the various pillars CLARIN has been working on can be integrated. All four use cases fulfil the criteria of being cross-national.
In this paper, I introduce the DICI, an electronic dictionary of Italian collocations designed to support the acquisition of the collocational competence in learners of Italian as a second or foreign language. I briefly describe the composition of the reference Italian corpus from which the collocations are extracted, and the methodology of extraction and filtering of candidate collocations. It is an experimental methodology, based on POS filtering, frequency and statistical measures, and tested on a 12-million-word sample from the reference corpus. Furthermore, I explain the main criteria for the composition of the dictionary, in addition to its integration with a Virtual Learning Environment (VLE), aimed at supporting learning activities on collocations. I briefly describe some of the main features of this integration with the VLE, such as the automatic recognition of collocations in written Italian texts, the possibility for students to obtain further linguistic information on selected collocations, and the automatic generation of tests for collocational competence assessment of language learners. While the main goal of the DICI is pedagogical, it is also intended to contribute to research in the field of collocations.
Many natural language processing tasks, including information extraction, question answering and recognizing textual entailment, require analysis of the polarity, focus of polarity, tense, aspect, mood and source of the event mentions in a text in addition to its predicate-argument structure analysis. We refer to modality, polarity and other associated information as extended modality. In this paper, we propose a new annotation scheme for representing the extended modality of event mentions in a sentence. Our extended modality consists of the following seven components: Source, Time, Conditional, Primary modality type, Actuality, Evaluation and Focus. We reviewed the literature about extended modality in Linguistics and Natural Language Processing (NLP) and defined appropriate labels of each component. In the proposed annotation scheme, information of extended modality of an event mention is summarized at the core predicate of the event mention for immediate use in NLP applications. We also report on the current progress of our manual annotation of a Japanese corpus of about 50,000 event mentions, showing a reasonably high ratio of inter-annotator agreement.
This paper deals with translation of English documents to Oromo using statistical methods. Whereas English is the lingua franca of online information, Oromo, despite its relative wide distribution within Ethiopia and neighbouring countries like Kenya and Somalia, is one of the most resource scarce languages. The paper has two main goals: one is to test how far we can go with the available limited parallel corpus for the English ― Oromo language pair and the applicability of existing Statistical Machine Translation (SMT) systems on this language pair. The second goal is to analyze the output of the system with the objective of identifying the challenges that need to be tackled. Since the language is resource scarce as mentioned above, we cannot get as many parallel documents as we want for the experiment. However, using a limited corpus of 20,000 bilingual sentences and 163,000 monolingual sentences, translation accuracy in terms of BLEU Score of 17.74{\%} was achieved.
Reflecting the rapid growth of science, technology, and culture, it has become common practice to consult tools on the World Wide Web for various terms. Existing search engines provide an enormous volume of information, but retrieved information is not organized. Hand-compiled encyclopedias provide organized information, but the quantity of information is limited. To integrate the advantages of both tools, we have been proposing methods for encyclopedic search targeting information on the Web and patent information. In this paper, we propose a method to categorize multiple expository texts for a single term based on viewpoints. Because viewpoints required for explanation are different depending on the type of a term, such as animals and diseases, it is difficult to manually produce a large scale system. We use Wikipedia to extract a prototype of a viewpoint structure for each term type. We also use articles in Wikipedia for a machine learning method, which categorizes a given text into an appropriate viewpoint. We evaluate the effectiveness of our method experimentally.
We present a semi-supervised machine-learning approach for the classification of adjectives into property- vs. relation-denoting adjectives, a distinction that is highly relevant for ontology learning. The feasibility of this classification task is evaluated in a human annotation experiment. We observe that token-level annotation of these classes is expensive and difficult. Yet, a careful corpus analysis reveals that adjective classes tend to be stable, with few occurrences of class shifts observed at the token level. As a consequence, we opt for a type-based semi-supervised classification approach. The class labels obtained from manual annotation are projected to large amounts of unannotated token samples. Training on heuristically labeled data yields high classification performance on our own data and on a data set compiled from WordNet. Our results suggest that it is feasible to automatically distinguish adjectives denoting properties and relations, using small amounts of annotated data.
This paper describes the acquisition, preparation and properties of a corpus extracted from the official documents of the United Nations (UN). This corpus is available in all 6 official languages of the UN, consisting of around 300 million words per language. We describe the methods we used for crawling, document formatting, and sentence alignment. This corpus also includes a common test set for machine translation. We present the results of a French-Chinese machine translation experiment performed on this corpus.
In this article, we present an experiment of linguistic parameter tuning in the representation of the semantic space of polysemous words. We evaluate quantitatively the influence of some basic linguistic knowledge (lemmas, multi-word expressions, grammatical tags and syntactic relations) on the performances of a similarity-based Word-Sense disambiguation method. The question we try to answer, by this experiment, is which kinds of linguistic knowledge are most useful for the semantic disambiguation of polysemous words, in a multilingual framework. The experiment is about 20 French polysemous words (16 nouns and 4 verbs) and we make use of the French-English part of the sentence-aligned EuroParl Corpus for training and testing. Our results show a strong correlation between the system accuracy and the degree of precision of the linguistic features used, particularly the syntactic dependency relations. Furthermore, the lemma-based approach absolutely outperforms the word form-based approach. The best accuracy achieved by our system amounts to 90{\%}.
This paper describes a hybrid system (FrAG) for tagging / parsing French text, and presents results from ongoing development work, corpus annotation and evaluation. The core of the system is a sentence scope Constraint Grammar (CG), with linguist-written rules. However, unlike traditional CG, the system uses hybrid techniques on both its morphological input side and its syntactic output side. Thus, FrAG draws on a pre-existing probabilistic Decision Tree Tagger (DTT) before and in parallel with its own lexical stage, and feeds its output into a Phrase Structure Grammar (PSG) that uses CG syntactic function tags rather than ordinary terminals in its rewriting rules. As an alternative architecture, dependency tree structures are also supported. In the newest version, dependencies are assigned within the CG-framework itself, and can interact with other rules. To provide semantic context, a semantic prototype ontology for nouns is used, covering a large part of the lexicon. In a recent test run on Parliamentary debate transcripts, FrAG achieved F-scores of 98.7 {\%} for part of speech (PoS) and between 93.1 {\%} and 96.2 {\%} for syntactic function tags. Dependency links were correct in 95.9 {\%}.
Opinion Mining is a discipline that has attracted some attention lately. Most of the research in this field has been done for English or Asian languages, due to the lack of resources in other languages. In this paper we describe an approach of building a manually annotated multilingual corpus for the domain of product reviews, which can be used as a basis for fine-grained opinion analysis also considering direct and indirect opinion targets. For each sentence in a review, the mentioned product features with their respective opinion polarity and strength on a scale from 0 to 3 are labelled manually by two annotators. The languages represented in the corpus are English, German and Spanish and the corpus consists of about 500 product reviews per language. After a short introduction and a description of related work, we illustrate the annotation process, including a description of the annotation methodology and the developed tool for the annotation process. Then first results on the inter-annotator agreement for opinions and product features are presented. We conclude the paper with an outlook on future work.
A limited prototype of the CLARIN Language Technology Infrastructure (LTI) node is presented. The node prototype provides several types of web services for Polish. The functionality encompasses morpho-syntactic processing, shallow semantic processing of corpus on the basis of the SuperMatrix system and plWordNet browsing. We take the prototype as the starting point for the discussion on requirements that must be fulfilled by the LTI. Some possible solutions are proposed for less frequently discussed problems, e.g. streaming processing of language data on the remote processing node. We experimentally investigate how to tackle with several requirements from many discussed. Such aspects as processing large volumes of data, asynchronous mode of processing and scalability of the architecture to large number of users got especial attention in the constructed prototype of the Web Service for morpho-syntactic processing of Polish called TaKIPI-WS (http://plwordnet.pwr.wroc.pl/clarin/ws/takipi/). TaKIPI-WS is a distributed system with a three-layer architecture, an asynchronous model of request handling and multi-agent-based processing. TaKIPI-WS consists of three layers: WS Interface, Database and Daemons. The role of the Database is to store and exchange data between the Interface and the Daemons. The Daemons (i.e. taggers) are responsible for executing the requests queued in the database. Results of the performance tests are presented in the paper, too.
Ontology matching consists of generating a set of correspondences between the entities of two ontologies. This process is seen as a solution to data heterogeneity in ontology-based applications, enabling the interoperability between them. However, existing matching systems are designed by assuming that the entities of both source and target ontologies are written in the same languages ( English, for instance). Multi-lingual ontology matching is an open research issue. This paper describes an API for multi-lingual matching that implements two strategies, direct translation-based and indirect. The first strategy considers direct matching between two ontologies (i.e., without intermediary ontologies), with the help of external resources, i.e., translations. The indirect alignment strategy, proposed by (Jung et al., 2009), is based on composition of alignments. We evaluate these strategies using simple string similarity based matchers and three ontologies written in English, French, and Portuguese, an extension of the OAEI benchmark test 206.
The process engine for pattern recognition and information fusion tasks, the {\textbackslash}emph{pepr framework}, aims to empower the researcher to develop novel solutions in the field of pattern recognition and information fusion tasks in a timely manner, by supporting reuse and combination of well tested and established components in an environment, that eases the wiring of distinct algorithms and description of the control flow through graphical tooling. The framework, not only consisting of the runtime environment, comes with several highly useful components that can be leveraged as a starting point in creating new solutions, as well as a graphical process builder that allows for easy development of pattern recognition processes in a graphical, modeled manner. Additionally, numerous work has been invested in order to keep the entry barrier with regards to extending the framework as low as possible, enabling developers to add additional functionality to the framework in as less time as possible.
In this paper, we present an approach to measure the transliteration similarity of English-Hindi word pairs. Our approach has two components. First we propose a bi-directional mapping between one or more characters in the Devanagari script and one or more characters in the Roman script (pronounced as in English). This allows a given Hindi word written in Devanagari to be transliterated into the Roman script and vice-versa. Second, we present an algorithm for computing a similarity measure that is a variant of Dices coefficient measure and the LCSR measure and which also takes into account the constraints needed to match English-Hindi transliterated words. Finally, by evaluating various similarity metrics individually and together under a multiple measure agreement scenario, we show that it is possible to achieve a 0.92 f-measure in identifying English-Hindi word pairs that are transliterations. In order to assess the portability of our approach to other similar languages we adapt our system to the Gujarati language.
This paper presents Q-WordNet, a lexical resource consisting of WordNet senses automatically annotated by positive and negative polarity. Polarity classification amounts to decide whether a text (sense, sentence, etc.) may be associated to positive or negative connotations. Polarity classification is becoming important within the fields of Opinion Mining and Sentiment Analysis for determining opinions about commercial products, on companies reputation management, brand monitoring, or to track attitudes by mining online forums, blogs, etc. Inspired by work on classification of word senses by polarity (e.g., SentiWordNet), and taking WordNet as a starting point, we build Q-WordNet. Instead of applying external tools such as supervised classifiers to annotated WordNet synsets by polarity, we try to effectively maximize the linguistic information contained in WordNet, thereby taking advantage of the human effort put by lexicographers and annotators. The resulting resource is a subset of WordNet senses classified as positive or negative. In this approach, neutral polarity is seen as the absence of positive or negative polarity. The evaluation of Q-WordNet shows an improvement with respect to previous approaches. We believe that Q-WordNet can be used as a starting point for data-driven approaches in sentiment analysis.
In the ``Sandglass'' MT architecture, we identify the class of monosemous Japanese functional expressions and utilize it in the task of translating Japanese functional expressions into English. We employ the semantic equivalence classes of a recently compiled large scale hierarchical lexicon of Japanese functional expressions. We then study whether functional expressions within a class can be translated into a single canonical English expression. Based on the results of identifying monosemous semantic equivalence classes, this paper studies how to extract rules for translating functional expressions in Japanese patent documents into English. In this study, we use about 1.8M Japanese-English parallel sentences automatically extracted from Japanese-English patent families, which are distributed through the Patent Translation Task at the NTCIR-7 Workshop. Then, as a toolkit of a phrase-based SMT (Statistical Machine Translation) model, Moses is applied and Japanese-English translation pairs are obtained in the form of a phrase translation table. Finally, we extract translation pairs of Japanese functional expressions from the phrase translation table. Through this study, we found that most of the semantic equivalence classes judged as monosemous based on manual translation into English have only one translation rules even in the patent domain.
The relevance of syntactic dependency annotated corpora is nowadays unquestioned. However, a broad debate on the optimal set of dependency relation tags did not take place yet. As a result, largely varying tag sets of a largely varying size are used in different annotation initiatives. We propose a hierarchical dependency structure annotation schema that is more detailed and more flexible than the known annotation schemata. The schema allows us to choose the level of the desired detail of annotation, which facilitates the use of the schema for corpus annotation for different languages and for different NLP applications. Thanks to the inclusion of semantico-syntactic tags into the schema, we can annotate a corpus not only with syntactic dependency structures, but also with valency patterns as they are usually found in separate treebanks such as PropBank and NomBank. Semantico-syntactic tags and the level of detail of the schema furthermore facilitate the derivation of deep-syntactic and semantic annotations, leading to truly multilevel annotated dependency corpora. Such multilevel annotations can be readily used for the task of ML-based acquisition of grammar resources that map between the different levels of linguistic representation ― something which forms part of, for instance, any natural language text generator.
FrameSQL is a web-based application which the author (Sato, 2003; Sato 2008) created originally for searching the Berkeley FrameNet lexical database. FrameSQL now can handle the Japanese lexical database built by the Japanese FrameNet project (JFN) of Keio University in Japan. FrameSQL can search and view the JFN data released in March of 2009 on a standard web browser. Users do not need to install any additional software tools to use FrameSQL, nor do they even need to download the JFN data to their local computer, because FrameSQL accesses the database of the server computer, and executes searches. FrameSQL not only shows a clear view of the headwords grammar and combinatorial properties of the database, but also relates a Japanese word with its counterparts in English. FrameSQL puts together the Japanese and English lexical databases, and the user can access them seamlessly, as if they were a unified database. Mutual hyperlinks among these databases and the bilingual search mode make it easy to compare semantic structures of corresponding lexical units between these languages, and it could be useful for building multilingual lexical resources.
In Japanese, there are a large number of notational variants of words. This is because Japanese words are written in three kinds of characters: kanji (Chinese) characters, hiragara letters, and katakana letters. Japanese students study basic rules of Japanese writing in school for many years. However, it is difficult to learn which variant is suitable for a certain context in official, business, and technical documents because the rules have many exceptions. Previous Japanese writing support systems were not concerned with them sufficiently. This is because their main purposes were misspelling detection. Students often use variants which are not misspelling but unsuitable for the contexts in official, business, and technical documents. To solve this problem, we developed a context sensitive variant dictionary. A writing support system based on the context sensitive variant dictionary detects unsuitable variants for the contexts in students' reports and shows suitable ones to the students. In this study, we first show how to develop a context sensitive variant dictionary by which our system determines which variant is suitable for a context in official, business, and technical documents. Finally, we conducted a control experiment and show the effectiveness of our dictionary.
We introduce PerLex, a large-coverage and freely-available morphological lexicon for the Persian language. We describe the main features of the Persian morphology, and the way we have represented it within the Alexina formalism, on which PerLex is based. We focus on the methodology we used for constructing lexical entries from various sources, as well as the problems related to typographic normalisation. The resulting lexicon shows a satisfying coverage on a reference corpus and should therefore be a good starting point for developing a syntactic lexicon for the Persian language.
In this paper, we introduce the Lefff, a freely available, accurate and large-coverage morphological and syntactic lexicon for French, used in many NLP tools such as large-coverage parsers. We first describe Alexina, the lexical framework in which the Lefff is developed as well as the linguistic notions and formalisms it is based on. Next, we describe the various sources of lexical data we used for building the Lefff, in particular semi-automatic lexical development techniques and conversion and merging of existing resources. Finally, we illustrate the coverage and precision of the resource by comparing it with other resources and by assessing its impact in various NLP tools.
With the proliferation of applications sharing information represented in multiple ontologies, the development of automatic methods for robust and accurate ontology matching will be crucial to their success. Connecting and merging already existing semantic networks is perhaps one of the most challenging task related to knowledge engineering. This paper presents a new approach for aligning automatically a very large domain ontology of Species to WordNet in the framework of the KYOTO project. The approach relies on the use of knowledge-based Word Sense Disambiguation algorithm which accurately assigns WordNet synsets to the concepts represented in Species 2000.
In this paper, we investigate the acoustic properties of phonemes in three speaking styles: read speech, prepared speech and spontaneous speech. Our aim is to better understand why speech recognition systems still fails to achieve good performances on spontaneous speech. This work follows the work of Nakamura et al. on Japanese speaking styles, with the difference that we here focus on French. Using Nakamura's method, we use classical speech recognition features, MFCC, and try to represent the effects of the speaking styles on the spectral space. Two measurements are defined in order to represent the spectral space reduction and the spectral variance extension. Experiments are then carried on to investigate if indeed we find some differences between the three speaking styles using these measurements. We finally compare our results to those obtained by Nakamura on Japanese to see if the same phenomenon appears. We happen to find some cues, and it also seems that phone duration also plays an important role regarding spectral reduction, especially for spontaneous speech.
In this paper, we report on our attempt at assigning semantic information from the English FrameNet to lexical units in the Bulgarian valency lexicon. The paper briefly presents the model underlying the Bulgarian FrameNet (BulFrameNet): each lexical entry consists of a lexical unit; a semantic frame from the English FrameNet, expressing abstract semantic structure; a grammatical class, defining the inflexional paradigm; a valency frame describing (some of) the syntactic and lexical-semantic combinatory properties (an optional component); and (semantically and syntactically) annotated examples. The target is a corpus-based lexicon giving an exhaustive account of the semantic and syntactic combinatory properties of an extensive number of Bulgarian lexical units. The Bulgarian FrameNet database so far contains unique descriptions of over 3 000 Bulgarian lexical units, approx. one tenth of them aligned with appropriate semantic frames, supports XML import and export and will be accessible, i.e., displayed and queried via the web.
In the last decade, the Dutch Language Union has taken a serious interest in digital language resources and human language technologies (HLT), because they are crucial for a language to be able to survive in the information society. In this paper we report on the current state of the joint Flemish-Dutch efforts in the field of HLT for Dutch (HLTD) and how follow-up activities are being prepared. We explain the overall mechanism of evaluating an R{\&}D programme and the role of evaluation in the policy cycle to establish new R{\&}D funding activities. This is applied to the joint Flemish-Dutch STEVIN programme. Outcomes of the STEVIN scientific midterm review are shortly discussed as the overall final evaluation is currently still on-going. As part of preparing for future policy plans, an HLTD forecast is presented. Also new opportunities are outlined, in particular in the context of the European CLARIN infrastructure project that can lead to new avenues for joint Flemish-Dutch cooperation on HLTD.
In this paper we report the first results of an annotation exercise of argument coercion phenomena performed on Italian texts. Our corpus consists of ca 4000 sentences from the PAROLE sottoinsieme corpus (Bindi et al. 2000) annotated with Selection and Coercion relations among verb-noun pairs formatted in XML according to the Generative Lexicon Mark-up Language (GLML) format (Pustejovsky et al., 2008). For the purposes of coercion annotation, we selected 26 Italian verbs that impose semantic typing on their arguments in either Subject, Direct Object or Complement position. Every sentence of the corpus is annotated with the source type for the noun arguments by two annotators plus a judge. An overall agreement of 0.87 kappa indicates that the annotation methodology is reliable. A qualitative analysis of the results allows us to outline some suggestions for improvement of the task: 1) a different account of complex types for nouns has to be devised and 2) a more comprehensive account of coercion mechanisms requires annotation of the deeper meaning dimensions that are targeted in coercion operations, such as those captured by Qualia relations.
LT World (www.lt-world.org) is an ontology-driven web portal aimed at serving the global language technology community. Ontology-driven means, that the system is driven by an ontological schema to manage the research information and knowledge life-cycles: identify relevant concepts of information, structure and formalize them, assign relationships, functions and views, add states and rules, modify them. For modelling such a complex structure, we employ (i) concepts from the research domain, such as person, organisation, project, tool, data, patent, news, event (ii) concepts from the LT domain, such as technology and resource (iii) concepts from closely related domains, such as language, linguistics, and mathematics. Whereas the research entities represent the general context, that is, a research environment as such, the LT entities define the information and knowledge space of the field, enhanced by entities from closely related areas. By managing information holistically ― that is, within a research context ― its inherent semantics becomes much more transparent. This paper introduces LT World as a reference information portal through ontological eyes: its content, its system, its method for maintaining knowledge-rich items, its ontology as an asset.
In Natural Language Generation (NLG), template-based surface realisation is an effective solution to the problem of producing surface strings from a given semantic representation, but many applications may not be able to provide the input knowledge in the required level of detail, which in turn may limit the use of the available NLG resources. However, if we know in advance what the most likely output sentences are (e.g., because a corpus on the relevant application domain happens to be available), then corpus knowledge may be used to quickly deploy a surface realisation engine for small-scale applications, for which it may be sufficient to select a sentence (in natural language) that resembles the desired output, and then modify some or all of its constituents accordingly. In other words, the application may simply 'point to' an existing sentence in the corpus and specify only the changes that need to take place to obtain the desired surface string. In this paper we describe one such approach to surface realisation, in which we extract syntactically-structured templates from a target corpus, and use these templates to produce existing and modified versions of the target sentences by a combination of canned text and basic dependency-tree operations.
Web services are increasingly being used in the natural language processing community as a way to increase the interoperability amongst language resources. This paper extends our previous work on integrating two different platforms, i.e. Heart of Gold and Language Grid. The Language Grid is an infrastructure built on top of the Internet to provide distributed language services. Heart of Gold is known as middleware architecture for integrating deep and shallow natural language processing components. The new feature of the integrated architecture is the combination of composite language services in the Language Grid and the multiple linguistic processing components in Heart of Gold to provide a better quality of language resources available on the Web. Thus, language resources with different characteristics can be combined based on the concept of service oriented computing with different treatment for each combination. Having Heart of Gold fully integrated in the Language Grid environment would contribute to the heterogeneity of language services.
In this paper, we propose classifier ensemble selection for Named Entity Recognition (NER) as a single objective optimization problem. Thereafter, we develop a method based on genetic algorithm (GA) to solve this problem. Our underlying assumption is that rather than searching for the best feature set for a particular classifier, ensembling of several classifiers which are trained using different feature representations could be a more fruitful approach. Maximum Entropy (ME) framework is used to generate a number of classifiers by considering the various combinations of the available features. In the proposed approach, classifiers are encoded in the chromosomes. A single measure of classification quality, namely F-measure is used as the objective function. Evaluation results on a resource constrained language like Bengali yield the recall, precision and F-measure values of 71.14{\%}, 84.07{\%} and 77.11{\%}, respectively. Experiments also show that the classifier ensemble identified by the proposed GA based approach attains higher performance than all the individual classifiers and two different conventional baseline ensembles.
In this work, automatic recognition of Arabic dialects is proposed. An acoustic survey of the proportion of vocalic intervals and the standard deviation of consonantal intervals in nine dialects (Tunisia, Morocco, Algeria, Egypt, Syria, Lebanon, Yemen, Golfs Countries and Iraq) is performed using the platform Alize and Gaussian Mixture Models (GMM). The results show the complexity of the automatic identification of Arabic dialects since. No clear border can be found between the dialects, but a gradual transition between them. They can even vary slightly from one city to another. The existence of this gradual change is easy to understand: it corresponds to a human and social reality, to the contact, friendships forged and affinity in the environment more or less immediate of the individual. This document also raises questions about the classes or macro classes of Arabic dialects noticed from the confusion matrix and the design of the hierarchical tree obtained.
This paper describes an open source voice creation toolkit that supports the creation of unit selection and HMM-based voices, for the MARY (Modular Architecture for Research on speech Synthesis) TTS platform. We aim to provide the tools and generic reusable run-time system modules so that people interested in supporting a new language and creating new voices for MARY TTS can do so. The toolkit has been successfully applied to the creation of British English, Turkish, Telugu and Mandarin Chinese language components and voices. These languages are now supported by MARY TTS as well as German and US English. The toolkit can be easily employed to create voices in the languages already supported by MARY TTS. The voice creation toolkit is mainly intended to be used by research groups on speech technology throughout the world, notably those who do not have their own pre-existing technology yet. We try to provide them with a reusable technology that lowers the entrance barrier for them, making it easier to get started. The toolkit is developed in Java and includes intuitive Graphical User Interface (GUI) for most of the common tasks in the creation of a synthetic voice.
The paper explores the co-reference chains as a way for improving the density of concept annotation over domain texts. The idea extends authors previous work on relating the ontology to the text terms in two domains ― IT and textile. Here IT domain is used. The challenge is to enhance relations among concepts instead of text entities, the latter pursued in most works. Our ultimate goal is to exploit these additional chains for concept disambiguation as well as sparseness resolution at concept level. First, a gold standard was prepared with manually connected links among concepts, anaphoric pronouns and contextual equivalents. This step was necessary not only for test purposes, but also for better orientation in the co-referent types and distribution. Then, two automatic systems were tested on the gold standard. Note that these systems were not designed specially for concept chaining. The conclusion is that the state-of-the-art co-reference resolution systems might address the concept sparseness problem, but not so much the concept disambiguation task. For the latter, word-sense disambiguation systems have to be integrated.
We present a study that compares data-driven dependency parsers obtained by means of annotation projection between language pairs of varying structural similarity. We show how the partial dependency trees projected from English to Dutch, Italian and German can be exploited to train parsers for the target languages. We evaluate the parsers against manual gold standard annotations and find that the projected parsers substantially outperform our heuristic baselines by 9―25{\%} UAS, which corresponds to a 21―43{\%} reduction in error rate. A comparative error analysis focuses on how the projected target language parsers handle subjects, which is especially interesting for Italian as an instance of a pro-drop language. For Dutch, we further present experiments with German as an alternative source language. In both source languages, we contrast standard baseline parsers with parsers that are enhanced with the predictions from large-scale LFG grammars through a technique of parser stacking, and show that improvements of the source language parser can directly lead to similar improvements of the projected target language parser.
During the last decades, interdisciplinarity has become a central keyword in research. As a consequence, many concepts, theories and scientific methods get in contact with each other, resulting in many different strategies and variants of acquiring, structuring, and sharing data sets. To handle these kind of data sets, his paper introduces the Ariadne Corpus Management System that allows researchers to manage and create multimodal corpora from multiple heteogeneous data sources. After an introductory demarcation from other annotation and corpus management tools, the underlying data model is presented which enables users to represent and process heterogeneous data sets within a single, consistent framework. Secondly, a set of automatized procedures is described that offers assistance to researchers in various data-related use cases. Thirdly, an approach to easy yet powerful data retrieval is introduced in form of a specialised querying language for multimodal data. Finally, the web-based graphical user interface and its advantages are illustrated.
Automatically translating natural language into machine-readable instructions is one of major interesting and challenging tasks in Natural Language (NL) Processing. This problem can be addressed by using machine learning algorithms to generate a function that find mappings between natural language and programming language semantics. For this purpose suitable annotated and structured data are required. In this paper, we describe our method to construct and semi-automatically annotate these kinds of data, consisting of pairs of NL questions and SQL queries. Additionally, we describe two different datasets obtained by applying our annotation method to two well-known corpora, GeoQueries and RestQueries. Since we believe that syntactic levels are important, we also generate and make available relational pairs represented by means of their syntactic trees whose lexical content has been generalized. We validate the quality of our corpora by experimenting with them and our machine learning models to derive automatic NL/SQL translators. Our promising results suggest that our corpora can be effectively used to carry out research in the field of natural language interface to database.
One of the essential factors in community sites is anonymous submission. This is because anonymity gives users chances to submit messages (questions, problems, answers, opinions, etc.) without regard to shame and reputation. However, some users abuse the anonymity and disrupt communications in a community site. These users and their submissions discourage other users, keep them from retrieving good communication records, and decrease the credibility of the communication site. To solve this problem, we conducted an experimental study to detect submitters suspected of pretending to be someone else to manipulate communications in a community site by using machine learning techniques. In this study, we used messages in the data of Yahoo! chiebukuro for data training and examination.
Most of the research on the extraction of idiomatic multiword expressions (MWEs) focused on the acquisition of MWE types. In the present work we investigate whether a text instance of a potentially idiomatic MWE is actually used idiomatically in a given context or not. Inspired by the dataset provided by (Cook et al., 2008), we manually analysed 9,700 instances of potentially idiomatic prepositionnoun- verb triples (a frequent pattern among German MWEs) to identify, on token level, idiomatic vs. literal uses. In our dataset, all sentences are provided along with their morpho-syntactic properties. We describe our data extraction and annotation steps, and we discuss quantitative results from both EUROPARL and a German newspaper corpus. We discuss the relationship between idiomaticity and morpho-syntactic fixedness, and we address issues of ambiguity between literal and idiomatic use of MWEs. Our data show that EUROPARL is particularly well suited for MWE extraction, as most MWEs in this corpus are indeed used only idiomatically.
We investigate a number of approaches to generating Stanford Dependencies, a widely used semantically-oriented dependency representation. We examine algorithms specifically designed for dependency parsing (Nivre, Nivre Eager, Covington, Eisner, and RelEx) as well as dependencies extracted from constituent parse trees created by phrase structure parsers (Charniak, Charniak-Johnson, Bikel, Berkeley and Stanford). We found that constituent parsers systematically outperform algorithms designed specifically for dependency parsing. The most accurate method for generating dependencies is the Charniak-Johnson reranking parser, with 89{\%} (labeled) attachment F1 score. The fastest methods are Nivre, Nivre Eager, and Covington, used with a linear classifier to make local parsing decisions, which can parse the entire Penn Treebank development set (section 22) in less than 10 seconds on an Intel Xeon E5520. However, this speed comes with a substantial drop in F1 score (about 76{\%} for labeled attachment) compared to competing methods. By tuning how much of the search space is explored by the Charniak-Johnson parser, we are able to arrive at a balanced configuration that is both fast and nearly as good as the most accurate approaches.
Research on automatic humor recognition has developed several features which discriminate funny text from ordinary text. The features have been demonstrated to work well when classifying the funniness of single sentences up to entire blogs. In this paper we focus on evaluating a set of the best humor features reported in the literature over a corpus retrieved from the Slashdot Web site. The corpus is categorized in a community-driven process according to the following tags: funny, informative, insightful, offtopic, flamebait, interesting and troll. These kinds of comments can be found on almost every large Web site; therefore, they impose a new challenge to humor retrieval since they come along with unique characteristics compared to other text types. If funny comments were retrieved accurately, they would be of a great entertainment value for the visitors of a given Web page. Our objective, thus, is to distinguish between an implicit funny comment from a not funny one. Our experiments are preliminary but nonetheless large-scale: 600,000 Web comments. We evaluate the classification accuracy of naive Bayes classifiers, decision trees, and support vector machines. The results suggested interesting findings.
We present a method for acquiring reliable predicate-argument structures from raw corpora for automatic compilation of case frames. Such lexicon compilation requires highly reliable predicate-argument structures to practically contribute to Natural Language Processing (NLP) applications, such as paraphrasing, text entailment, and machine translation. However, to precisely identify predicate-argument structures, case frames are required. This issue is similar to the question ''``what came first: the chicken or the egg?'''' In this paper, we propose the first step in the extraction of reliable predicate-argument structures without using case frames. We first apply chunking to raw corpora and then extract reliable chunks to ensure that high-quality predicate-argument structures are obtained from the chunks. We conducted experiments to confirm the effectiveness of our approach. We successfully extracted reliable chunks of an accuracy of 98{\%} and high-quality predicate-argument structures of an accuracy of 97{\%}. Our experiments confirmed that we succeeded in acquiring highly reliable predicate-argument structures that can be used to compile case frames.
In this work we propose a hybrid unsupervised approach for semantic relation extraction from Italian and English texts. The system takes as input pairs of ''``distributionally similar'''' terms, possibly involved in a semantic relation. To validate and label the anonymous relations holding between the terms in input, the candidate pairs of terms are looked for on the Web in the context of reliable lexico-syntactic patterns. This paper focuses on the definition of the patterns, on the measures used to assess the reliability of the suggested specific semantic relation and on the evaluation of the implemented system. So far, the system is able to extract the following types of semantic relations: hyponymy, meronymy, and co-hyponymy. The approach can however be easily extended to manage other relations by defining the appropriate battery of reliable lexico-syntactic patterns. Accuracy of the system was measured with scores of 83.3{\%} for hyponymy, 75{\%} for meronymy and 72.2{\%} for co-hyponymy extraction.
This paper discusses our ongoing work on constructing an annotated corpus of childrens stories for further studies on the linguistic, computational, and cognitive aspects of story structure and understanding. Given its semantic nature and the need for extensive common sense and world knowledge, story understanding has been a notoriously difficult topic in natural language processing. In particular, the notion of story structure for maintaining coherence has received much attention, while its strong version in the form of story grammar has triggered much debate. The relation between discourse coherence and the interestingness, or the point, of a story has not been satisfactorily settled. Introspective analysis on story comprehension has led to some important observations, based on which we propose a preliminary annotation scheme covering the structural, functional, and emotional aspects connecting discourse segments in stories. The annotation process will shed light on how story structure interacts with story point via various linguistic devices, and the annotated corpus is expected to be a useful resource for computational discourse processing, especially for studying various issues regarding the interface between coherence and interestingness of stories.
The explosion of biomedical literature and with it the -uncontrolled- creation of abbreviations presents some special challenges for both human readers and computer applications. We developed an annotated corpus of Dutch medical text, and experimented with two approaches to abbreviation detection and resolution. Our corpus is composed of abstracts from two medical journals from the Low Countries in which approximately 65 percent (NTvG) and 48 percent (TvG) of the abbreviations have a corresponding full form in the abstract. Our first approach, a pattern-based system, consists of two steps: abbreviation detection and definition matching. This system has an average F-score of 0.82 for the detection of both defined and undefined abbreviations and an average F-score of 0.77 was obtained for the definitions. For our second approach, an SVM-based classifier was used on the preprocessed data sets, leading to an average F-score of 0.93 for the abbreviations; for the definitions an average F-score of 0.82 was obtained.
This paper reports on the syntactic annotation of a previously compiled and tagged corpus of European Portuguese (EP) dialects ― The Syntax-oriented Corpus of Portuguese Dialects (CORDIAL-SIN). The parsed version of CORDIAL-SIN is intended to be a more efficient resource for the purpose of studying dialect syntax by allowing automated searches for various syntactic constructions of interest. To achieve this goal we adopted a rich annotation system (the UPenn corpora annotation system) which codifies syntactic information of high relevance. The annotation produces tree representations, in form of labelled parenthesis, that are integrally searchable with CorpusSearch, a search engine for parsed corpora (Randall, 2005-2007). The present paper focuses on CORDIAL-SIN annotation issues, namely it presents the general principles and guidelines of the adopted annotation system and describes the methodology for constructing the parsed version of the corpus and for searching it (tools and procedures). Last section addresses the question of how an annotation system originally designed for Middle English can be adapted to meet the particular needs of a Portuguese corpus of dialectal speech.
The paper describes an approach to expedite the process of manual annotation of a Hindi dependency treebank which is currently under development. We propose a way by which consistency among a set of manual annotators could be improved. Furthermore, we show that our setup can also prove useful for evaluating when an inexperienced annotator is ready to start participating in the production of the treebank. We test our approach on sample sets of data obtained from an ongoing work on creation of this treebank. The results asserting our proposal are reported in this paper. We report results from a semi-automated approach of dependency annotation experiment. We find out the rate of agreement between annotators using Cohens Kappa. We also compare results with respect to the total time taken to annotate sample data-sets using a completely manual approach as opposed to a semi-automated approach. It is observed from the results that this semi-automated approach when carried out with experienced and trained human annotators improves the overall quality of treebank annotation and also speeds up the process.
The Brandeis Annotation Tool (BAT) is a web-based text annotation tool that is centered around the notions of layered annotation and task decomposition. It allows annotations to refer to other annotations and to take a complicated task and split it into easier subtasks. The central organizing concept of BAT is the annotation layer. A corpus administrator can create annotation layers that involve annotation of extents, attributes or relations. The layer definition includes the labels used, the attributes that are available and restrictions on the values for those attributes. For each annotation layer, files can be assigned to one or more annotators and one judge. When annotators log in, the assigned layers and files therein are presented. When selecting a file to annotate, the interface uses the layer definition to display the annotation interface. The web-interface connects administrators and annotators to a central repository for all data and simplifies many of the housekeeping tasks while keeping requirements at a minimum (that is, users only need an internet connection and a well-behaved browser). BAT has been used mainly for temporal annotation, but can be considered a more general tool for several kinds of textual annotation.
The LTSE-VAD is one of the best known algorithms for voice activity detection. In this paper we present a modified version of this algorithm, that makes the VAD decision not taking into account account the estimated background noise level, but the signal to noise ratio (SNR). This makes the algorithm robust not only to noise level changes, but also to signal level changes. We compare the modified algorithm with the original one, and with three other standard VAD systems. The results show that the modified version gets the lowest silence misclassification rate, while maintaining a reasonably low speech misclassification rate. As a result, this algorithm is more suitable for identification tasks, such as speaker or emotion recognition, where silence misclassification can be very harmful. A series of automatic emotion identification experiments are also carried out, proving that the modified version of the algorithm helps increasing the correct emotion classification rate.
We describe a web application called ANC2Go that enables the user to select data from the Open American National Corpus (OANC) and the Manually Annotated Sub-corpus (MASC) together with some or all of the annotations available. The user also may select from among a variety of options for output format, or may receive the selected portions of the corpus and annotations in their original GrAF XML standoff format.. The request is processed by merging the annotations selected and rendering them in the desired output format, then bundling the results and making it available for download. Thus, users can create a customized corpus with data and annotations of their choosing, delivered in the format that is most convenient for their use. ANC2Go will be released as a web service in the near future. Both the OANC and MASC are freely available for any use from the American National Corpus website and may be accessed through the ANC2Go application, or they may downloaded in their entirety.
Recently, language resources (LRs) are becoming indispensable for linguistic researches. However, existing LRs are often not fully utilized because their variety of usage is not well known, indicating that their intrinsic value is not recognized very well either. Regarding this issue, lists of usage information might improve LR searches and lead to their efficient use. In this research, therefore, we collect a list of usage information for each LR from academic articles to promote the efficient utilization of LRs. This paper proposes to construct a text corpus annotated with usage information (UI corpus). In particular, we automatically extract sentences containing LR names from academic articles. Then, the extracted sentences are annotated with usage information by two annotators in a cascaded manner. We show that the UI corpus contributes to efficient LR searches by combining the UI corpus with a metadata database of LRs and comparing the number of LRs retrieved with and without the UI corpus.
Today's natural language processing systems are growing more complex with the need to incorporate a wider range of language resources and more sophisticated statistical methods. In many cases, it is necessary to learn a component with input that includes the predictions of other learned components or to assign simultaneously the values that would be assigned by multiple components with an expressive, data dependent structure among them. As a result, the design of systems with multiple learning components is inevitably quite technically complex, and implementations of conceptually simple NLP systems can be time consuming and prone to error. Our new modeling language, Learning Based Java (LBJ), facilitates the rapid development of systems that learn and perform inference. LBJ has already been used to build state of the art NLP systems. In this paper, we first demonstrate that there exists a theoretical model that describes most NLP approaches adeptly. Second, we show how our improvements to the LBJ language enable the programmer to describe the theoretical model succinctly. Finally, we introduce the concept of data driven compilation, a translation process in which the efficiency of the generated code benefits from the data given as input to the learning algorithms.
In this paper we present a new approach for obtaining the terminology of a given domain using the category and page structures of the Wikipedia in a language independent way. Our approach consists basically, for each domain, on navigating the Category graph of the Wikipedia starting from the root nodes associated to the domain. A heavy filtering mechanism is carried out for preventing as much as possible the inclusion of spurious categories. For each selected category all the pages belonging to it are then recovered and filtered. This procedure is iterate several times until achieving convergence. Both category names and page names are considered candidates to belong to the terminology of the domain. This approach has been applied to three broad coverage domains: astronomy, chemistry and medicine, and two languages, English and Spanish, showing a promising performance.
We have previously reported on ProPOSEL, a purpose-built Prosody and PoS English Lexicon compatible with the Python Natural Language ToolKit. ProPOSEC is a new corpus research resource built using this lexicon, intended for distribution with the Aix-MARSEC dataset. ProPOSEC comprises multi-level parallel annotations, juxtaposing prosodic and syntactic information from different versions of the Spoken English Corpus, with canonical dictionary forms, in a query format optimized for Perl, Python, and text processing programs. The order and content of fields in the text file is as follows: (1) Aix-MARSEC file number; (2) word; (3) LOB PoS-tag; (4) C5 PoS-tag; (5) Aix SAM-PA phonetic transcription; (6) SAM-PA phonetic transcription from ProPOSEL; (7) syllable count; (8) lexical stress pattern; (9) default content or function word tag; (10) DISC stressed and syllabified phonetic transcription; (11) alternative DISC representation, incorporating lexical stress pattern; (12) nested arrays of phonemes and tonic stress marks from Aix. As an experimental dataset, ProPOSEC can be used to study correlations between these annotation tiers, where significant findings are then expressed as additional features for phrasing models integral to Text-to-Speech and Speech Recognition. As a training set, ProPOSEC can be used for machine learning tasks in Information Retrieval and Speech Understanding systems.
Collocations play a significant role in second language acquisition. In order to be able to offer efficient support to learners, an NLP-based CALL environment for learning collocations should be based on a representative collocation error annotated learner corpus. However, so far, no theoretically-motivated collocation error tag set is available. Existing learner corpora tag collocation errors simply as lexical errors ― which is clearly insufficient given the wide range of different collocation errors that the learners make. In this paper, we present a fine-grained three-dimensional typology of collocation errors that has been derived in an empirical study from the learner corpus CEDEL2 compiled by a team at the Autonomous University of Madrid. The first dimension captures whether the error concerns the collocation as a whole or one of its elements; the second dimension captures the language-oriented error analysis, while the third exemplifies the interpretative error analysis. To facilitate a smooth annotation along this typology, we adapted Knowtator, a flexible off-the-shelf annotation tool implemented as a Prot{\'e}g{\'e} plugin.
We present the methodology that underlies mew metrics for semantic machine translation evaluation we are developing. Unlike widely-used lexical and n-gram based MT evaluation metrics, the aim of semantic MT evaluation is to measure the utility of translations. We discuss the design of empirical studies to evaluate the utility of machine translation output by assessing the accuracy for key semantic roles. These roles are from the English 5W templates (who, what, when, where, why) used in recent GALE distillation evaluations. Recent work by Wu and Fung (2009) introduced semantic role labeling into statistical machine translation to enhance the quality of MT output. However, this approach has so far only been evaluated using lexical and n-gram based SMT evaluation metrics like BLEU which are not aimed at evaluating the utility of MT output. Direct data analysis are still needed to understand how semantic models can be leveraged to evaluate the utility of MT output. In this paper, we discuss a new methodology for evaluating the utility of the machine translation output, by assessing the accuracy with which human readers are able to complete the English 5W templates.
Recent developments on hybrid systems that combine rule-based machine translation (RBMT) systems with statistical machine translation (SMT) generally neglect the fact that RBMT systems tend to produce more syntactically well-formed translations than data-driven systems. This paper proposes a method that alleviates this issue by preserving more useful structures produced by RBMT systems and utilizing them in a SMT system that operates on hierarchical structures instead of flat phrases alone. For our experiments, we use Joshua as the decoder. It is the first attempt towards a tighter integration of MT systems from different paradigms that both support hierarchical analysis. Preliminary results show consistent improvements over the previous approach.
This paper summarizes our work on creating a full-scale coreference resolution (CR) system for Italian, using BART ― an open-source modular CR toolkit initially developed for English corpora. We discuss our experiments on language-specific issues of the task. As our evaluation experiments show, a language-agnostic system (designed primarily for English) can achieve a performance level in high forties (MUC F-score) when re-trained and tested on a new language, at least on gold mention boundaries. Compared to this level, we can improve our F-score by around 10{\%} introducing a small number of language-specific changes. This shows that, with a modular coreference resolution platform, such as BART, one can straightforwardly develop a family of robust and reliable systems for various languages. We hope that our experiments will encourage researchers working on coreference in other languages to create their own full-scale coreference resolution systems ― as we have mentioned above, at the moment such modules exist only for very few languages other than English.
Statistical machine translation to morphologically richer languages is a challenging task and more so if the source and target languages differ in word order. Current state-of-the-art MT systems thus deliver mediocre results. Adding more parallel data often helps improve the results; if it doesn't, it may be caused by various problems such as different domains, bad alignment or noise in the new data. In this paper we evaluate the English-to-Hindi MT task from this data perspective. We discuss several available parallel data sources and provide cross-evaluation results on their combinations using two freely available statistical MT systems. We demonstrate various problems encountered in the data and describe automatic methods of data cleaning and normalization. We also show that the contents of two independently distributed data sets can unexpectedly overlap, which negatively affects translation quality. Together with the error analysis, we also present a new tool for viewing aligned corpora, which makes it easier to detect difficult parts in the data even for a developer not speaking the target language.
We describe the compilation of a large corpus of French-Dutch sentence pairs from official Belgian documents which are available in the online version of the publication Belgisch Staatsblad/Moniteur belge, and which have been published between 1997 and 2006. After downloading files in batch, we filtered out documents which have no translation in the other language, documents which contain several languages (by checking on discriminating words), and pairs of documents with a substantial difference in length. We segmented the documents into sentences and aligned the latter, which resulted in 5 million sentence pairs (only one-to-one links were included in the parallel corpus); there are 2.4 million unique pairs. Sample-based evaluation of the sentence alignment results indicates a near 100{\%} accuracy, which can be explained by the text genre, the procedure filtering out weakly parallel articles and the restriction to one-to-one links. The corpus is larger than a number of well-known French-Dutch resources. It is made available to the community. Further investigation is needed in order to determine the original language in which documents were written.
In this paper, we present the first results of the parallel Czech discourse annotation in the Prague Dependency Treebank 2.0. Having established an annotation scenario for capturing semantic relations crossing the sentence boundary in a discourse, and having annotated the first sections of the treebank according to these guidelines, we report now on the results of the first evaluation of these manual annotations. We give an overview of the process of the annotation itself, which we believe is to a large degree language-independent and therefore accessible to any discourse researcher. Next, we describe the inter-annotator agreement measurement, and, most importantly, we classify and analyze the most common types of annotators disagreement and propose solutions for the next phase of the annotation. The annotation is carried out on dependency trees (on the tectogrammatical layer), this approach is quite novel and it brings us some advantages when interpreting the syntactic structure of the discourse units.
This paper presents a novel system HENNA (Hybrid Person Name Analyzer) for identifying language origin and analyzing linguistic structures of person names. We conduct ME-based classification methods for the language origin identification and achieve very promising performance. We will show that word-internal character sequences provide surprisingly strong evidence for predicting the language origin of person names. Our approach is context-, language- and domain-independent and can thus be easily adapted to person names in or from other languages. Furthermore, we provide a novel strategy to handle origin ambiguities or multiple origins in a name. HENNA also provides a person name parser for the analysis of linguistic and knowledge structures of person names. All the knowledge about a person name in HENNA is modelled in a person-name ontology, including relationships between language origins, linguistic features and grammars of person names of a specific language and interpretation of name elements. The approaches presented here are useful extensions of the named entity recognition task.
We provide a robust and detailed annotation scheme for information status, which is easy to use, follows a semantic rather than cognitive motivation, and achieves reasonable inter-annotator scores. Our annotation scheme is based on two main assumptions: firstly, that information status strongly depends on (in)definiteness, and secondly, that it ought to be understood as a property of referents rather than words. Therefore, our scheme banks on overt (in)definiteness marking and provides different categories for each class. Definites are grouped according to the information source by which the referent is identified. A special aspect of the scheme is that non-anaphoric expressions (e.g.{\textbackslash} names) are classified as to whether their referents are likely to be known or unknown to an expected audience. The annotation scheme provides a solution for annotating complex nominal expressions which may recursively contain embedded expressions. In annotating a corpus of German radio news bulletins, a kappa score of .66 for the full scheme was achieved, a core scheme of six top-level categories yields kappa = .78.
In this article I present a lexicon for Arabic verbs which exploits Levins verb-classes (Levin, 1993) and the basic development procedure used by (Schuler, 2005). The verb lexicon in its current state has 173 classes which contain 4392 verbs and 498 frames providing information about verb root, the deverbal form of the verb, the participle, thematic roles, subcategorisation frames and syntactic and semantic descriptions of each verb. The taxonomy is available in XML format. It can be ported to MYSQL, YAML or JSON and accessed either in Arabic characters or in the Buckwalter transliteration.
This paper presents the Kachna corpus of spontaneous speech, in which ten Czech and ten Norwegian speakers were recorded both in their native language and in English. The dialogues are elicited using a picture replication task that requires active cooperation and interaction of speakers by asking them to produce a drawing as close to the original as possible. The corpus is appropriate for the study of interactional features and speech reduction phenomena across native and second languages. The combination of productions in non-native English and in speakers native language is advantageous for investigation of L2 issues while providing a L1 behaviour reference from all the speakers. The corpus consists of 20 dialogues comprising 12 hours 53 minutes of recording, and was collected in 2008. Preparation of the transcriptions, including a manual orthographic transcription and an automatically generated phonetic transcription, is currently in progress. The phonetic transcriptions are automatically generated by aligning acoustic models with the speech signal on the basis of the orthographic transcriptions and a dictionary of pronunciation variants compiled for the relevant language. Upon completion the corpus will be made available via the European Language Resources Association (ELRA).
In this work we present SENTIWORDNET 3.0, a lexical resource explicitly devised for supporting sentiment classification and opinion mining applications. SENTIWORDNET 3.0 is an improved version of SENTIWORDNET 1.0, a lexical resource publicly available for research purposes, now currently licensed to more than 300 research groups and used in a variety of research projects worldwide. Both SENTIWORDNET 1.0 and 3.0 are the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. SENTIWORDNET 1.0 and 3.0 differ (a) in the versions of WORDNET which they annotate (WORDNET 2.0 and 3.0, respectively), (b) in the algorithm used for automatically annotating WORDNET, which now includes (additionally to the previous semi-supervised learning step) a random-walk step for refining the scores. We here discuss SENTIWORDNET 3.0, especially focussing on the improvements concerning aspect (b) that it embodies with respect to version 1.0. We also report the results of evaluating SENTIWORDNET 3.0 against a fragment of WORDNET 3.0 manually annotated for positivity, negativity, and neutrality; these results indicate accuracy improvements of about 20{\%} with respect to SENTIWORDNET 1.0.
In this paper, we present a multimodal parallel text-image corpus, and propose an image annotation method that exploits the textual information associated with images. Our corpus contains news articles composed of a text, images and image captions, and is significantly larger than the other news corpora proposed in image annotation papers (27,041 articles and 42,568 captionned images). In our experiments, we use the text of the articles as a textual information source to annotate images, and image captions as a groundtruth to evaluate our annotation algorithm. Our annotation method identifies relevant named entities in the texts, and associates them with high-level visual concepts detected in the images (in this paper, faces and logos). The named entities most suited to image annotation are selected using an unsupervised score based on their statistics, inspired from the weights used in information retrieval. Our experiments show that, although it is very simple, our annotation method achieves an acceptable accuracy on our real-world news corpus.
Lexical resources are basic components of many text processing system devoted to information extraction, question answering or dialogue. In paste years many resources have been developed such as FrameNet and WordNet. FrameNet describes prototypical situations (i.e. Frames) while WordNet defines lexical meaning (senses) for the majority of English nouns, verbs, adjectives and adverbs. A major difference between FrameNet and WordNet refers to their coverage. Due of this lack of coverage, in recent years some approaches have been studied to make a bridge between this two resources, so a resource is used to extend the coverage of the other one. The nature of these approaches leave from supervised to supervised methods. The major problem is that there is not a standard in evaluation of the mapping. Each different work have tested own approach with a custom gold standard. This work give an extensive evaluation of the model proposed in (De Cao et al., 2008) using gold standard proposed in other works. Moreover this work give an empirical comparison between other available resources. As outcome of this work we also release the full mapping resource made according to the model proposed in (De Cao et al., 2008).
We evaluate statistical parsing of French using two probabilistic models derived from the Tree Adjoining Grammar framework: a Stochastic Tree Insertion Grammars model (STIG) and a specific instance of this formalism, called Spinal Tree Insertion Grammar model which exhibits interesting properties with regard to data sparseness issues common to small treebanks such as the Paris 7 French Treebank. Using David Chiangs STIG parser (Chiang, 2003), we present results of various experiments we conducted to explore those models for French parsing. The grammar induction makes use of a head percolation table tailored for the French Treebank and which is provided in this paper. Using two evaluation metrics, we found that the parsing performance of a STIG model is tied to the size of the underlying Tree Insertion Grammar, with a more compact grammar, a spinal STIG, outperforming a genuine STIG. We finally note that a ''``spinal'''' framework seems to emerge in the literature. Indeed, the use of vertical grammars such as Spinal STIG instead of horizontal grammars such as PCFGs, afflicted with well known data sparseness issues, seems to be a promising path toward better parsing performance.
In recent years we have resgitered a renewed interest in event detection and temporal processing of text/discourse. TimeML (Pustejovsky et al., 2003a) has shed new lights on the notion of event and developed a new methodology for its annotation. On a parallel, works on anaphora resolution have developed a reliable methodology for the annotation and pointed out the core role of this phenomenon for the improvement of NLP systems. This paper tries to put together these two lines of research by describing a case study for the creation of an annotation scheme on event anaphora. We claim that this work could have consequences for the annotation of eventualities as proposed in TimeML and on the use of the tag and on the study of anaphora and its annotation. The annotation scheme and its guidelines have been developed on the basis of a coarse grained bottom up approach. In order to do this, we have performed a small sampling annotation which has highlighted shortcomings and open issues which need to be resolved.
The paper describes some of the work carried out within the European funded project MEDAR. The project has three streams of activity: the technical stream, the cooperation stream and the dissemination stream. MEDAR has first updated the existing surveys and BLARK for Arabic, and then the technical stream focused on machine translation. The consortium identified a number of freely available MT systems and then customized two versions of the famous MOSES package. The Consortium addressed the needs to package MOSES for English to Arabic (while the main MT stream is on Arabic to English). For performance assessment purposes, the partners produced test data that allowed carrying out an evaluation campaign with 5 different systems (including from outside the consortium) and two online ones. Both the MT baselines and the collected data will be made available via ELRA catalogue. The cooperation stream focuses mostly on the cooperation roadmap for Human Language Technologies for Arabic. Cooperation Roadmap for the region directed towards the Arabic HLT in general. It is the purpose of the roadmap to outline areas and priorities for collaboration, in terms of collaboration between EU countries and Arabic speaking countries, as well as cooperation in general: between countries, between universities, and last but not least between universities and industry.
Natural language use, acquisition, and understanding takes place usually in multisensory and multimedia communication environments. Therefore, for one to model language in its interaction and integration with sensorimotor experiences, one needs a representative corpus of such interplay. In this paper, we will present the first corpus of language use and sensorimotor experience recordings in everyday human:human interaction, in which spontaneous language communication has been recorded along with corresponding multiview video recordings, recordings of 3D full body kinematics, and 3D tracking of objects in focus. It is a twelve-hour corpus which comprises of six everyday human:human interaction scenes, each one performed 3 times by 4 different English-speaking couples (interaction between a male and a female actor), each couple acting each scene in two settings: a fully naturalistic setting in which 5-camera multi-view video recordings take place, and a high-tech setting, with full body motion capture for both individuals, a 2-camera multiview video recording, and 3D tracking of focus objects. The corpus has been developed within an EU-funded cognitive systems research project, POETICON (http://www.poeticon.eu), and represents a new type of language resources for cognitive systems. Namely, a corpus that reveals the dynamic role of language in its interplay with sensorimotor experiences and which allows one to computationally model this interplay.
Large annotation projects, typically those addressing the question of multimodal annotation in which many different kinds of information have to be encoded, have to elaborate precise and high level annotation schemes. Doing this requires first to define the structure of the information: the different objects and their organization. This stage has to be as much independent as possible from the coding language constraints. This is the reason why we propose a preliminary formal annotation model, represented with typed feature structures. This representation requires a precise definition of the different objects, their properties (or features) and their relations, represented in terms of type hierarchies. This approach has been used to specify the annotation scheme of a large multimodal annotation project (OTIM) and experimented in the annotation of a multimodal corpus (CID, Corpus of Interactional Data). This project aims at collecting, annotating and exploiting a dialogue video corpus in a multimodal perspective (including speech and gesture modalities). The corpus itself, is made of 8 hours of dialogues, fully transcribed and richly annotated (phonetics, syntax, pragmatics, gestures, etc.).
Semantic lexicons and lexical ontologies are some major resources in natural language processing. Developing such resources are time consuming tasks for which some automatic methods are proposed. This paper describes some methods used in semi-automatic development of FarsNet; a lexical ontology for the Persian language. FarsNet includes the Persian WordNet with more than 10000 synsets of nouns, verbs and adjectives. In this paper we discuss extraction of lexico-conceptual relations such as synonymy, antonymy, hyperonymy, hyponymy, meronymy, holonymy and other lexical or conceptual relations between words and concepts (synsets) from Persian resources. Relations are extracted from different resources like web, corpora, Wikipedia, Wiktionary, dictionaries and WordNet. In the system presented in this paper a variety of approaches are applied in the task of relation extraction to extract ladled or unlabeled relations. They exploit the texts, structures, hyperlinks and statistics of web documents as well as the relations of English WordNet and entries of mono and bi-lingual dictionaries.
In this paper we present a fairy tale corpus that was semantically organized and tagged. The proposed method uses latent semantic mapping to represent the stories and a top-n item-to-item recommendation algorithm to define clusters of similar stories. Each story can be placed in more than one cluster and stories in the same cluster are related to the same concepts. The results were manually evaluated regarding the groupings as perceived by human judges. The evaluation resulted in a precision of 0.81, a recall of 0.69, and an f-measure of 0.75 when using tf*idf for word frequency. Our method is topic- and language-independent, and, contrary to traditional clustering methods, automatically defines the number of clusters based on the set of documents. This method can be used as a setup for traditional clustering or classification. The resulting corpus will be used for recommendation purposes, although it can also be used for emotion extraction, semantic role extraction, meaning extraction, text classification, among others.
We present the ABLE document collection, which consists of a set of annotated volumes of the Bulletin of the British Museum (Natural History). These were developed during our ongoing work on automating the markup of scanned copies of the biodiversity literature. Such automation is required if historic literature is to be used to inform contemporary issues in biodiversity research. We consider an enhanced TEI XML markup language, which is used as an intermediate stage in translating from the initial XML obtained from Optical Character Recognition to taXMLit, the target annotation schema. The intermediate representation allows additional information from external sources such as a taxonomic thesaurus to be incorporated before the final translation into taXMLit. We give an overview of the project workflow in automating the markup process, and consider what extensions to existing markup schema will be required to best support working taxonomists. Finally, we discuss some of the particular issues which were encountered in converting between different XML formats.
The Greybeard Project was designed so as to enable research in speaker recognition using data that have been collected over a long period of time. Since 1994, LDC has been collecting speech samples for use in research and evaluations. By mining our earlier collections we assembled a list of subjects who had participated in multiple studies. These participants were then contacted and asked to take part in the Greybeard Project. The only constraint was that the participants must have made numerous calls in prior studies and the calls had to be a minimum of two years old. The archived data was sorted by participant and subsequent calls were added to their files. This is the first longitudinal study of its kind. The resulting corpus contains multiple calls for each participant that span anywhere from two to 12 years in time. It is our hope that these data will enable speaker recognition researchers to explore the effects of aging on voice.
This paper describes the result of a joint R{\&}D project between Microsoft Portugal and the Signal Theory Group of the University of Vigo (Spain), where a set of language resources was developed with application to Text―to―Speech synthesis. First, a large Corpus of 10000 Galician sentences was designed and recorded by a professional female speaker. Second, a lexicon with phonetic and grammatical information of over 90000 entries was collected and reviewed manually by a linguist expert. And finally, these resources were used for a MOS (Mean Opinion Score) perceptual test to compare two state―of―the―art speech synthesizers of both groups, the one from Microsoft based on HMM, and the one from the University of Vigo based on unit selection.
We examine pooling data as a method for improving Statistical Machine Translation (SMT) quality for narrowly defined domains, such as data for a particular company or public entity. By pooling all available data, building large SMT engines, and using domain-specific target language models, we see boosts in quality, and can achieve the generalizability and resiliency of a larger SMT but with the precision of a domain-specific engine.
Linguistic Data Consortiums Human Subjects Data Collection lab conducts multi-modal speech collections to develop corpora for use in speech, speaker and language research and evaluations. The Mixer collections have evolved over the years to best accommodate the ever changing needs of the research community and to hopefully keep one step ahead by providing increasingly challenging data. Over the years Mixer collections have grown to include socio-linguistic interviews, a wide variety of telephone conditions and multiple languages, recording conditions, channels and speech acts.. Mixer 6 was the most recent collection. This paper describes the Mixer 6 Phase 1 project. Mixer 6 Phase 1 was a study supporting linguistic research, technology development and education. The object of this study was to record speech in a variety of situations that vary formality and model multiple naturally occurring interactions as well as a variety of channel conditions
This paper contributes to the question of which degree of complexity is called for in representations of discourse structure. We review recent claims that tree structures do not suffice as a model for discourse structure, with a focus on the work done on the Discourse Graphbank (DGB) of Wolf and Gibson (2005, 2006). We will show that much of the additional complexity in the DGB is not inherent in the data, but due to specific design choices that underlie W{\&}Gs annotation. Three kinds of configuration are identified whose DGB analysis violates tree-structure constraints, but for which an analysis in terms of tree structures is possible, viz., crossed dependencies that are eventually based on lexical or referential overlap, multiple-parent structures that could be handled in terms of Marcus (1996) Nuclearity Principle, and potential list structures, in which whole lists of segments are related to a preceding segment in the same way. We also discuss the recent results which Lee et al. (2008) adduce as evidence for a complexity of discourse structure that cannot be handled in terms of tree structures.
We have adapted and extended the automatic Multilingual, Interoperable Named Entity Lexicon approach to Arabic, using Arabic WordNet (AWN) and Arabic Wikipedia (AWK). First, we extract AWNs instantiable nouns and identify the corresponding categories and hyponym subcategories in AWK. Then, we exploit Wikipedia inter-lingual links to locate correspondences between articles in ten different languages in order to identify Named Entities (NEs). We apply keyword search on AWK abstracts to provide for Arabic articles that do not have a correspondence in any of the other languages. In addition, we perform a post-processing step to fetch further NEs from AWK not reachable through AWN. Finally, we investigate diacritization using matching with geonames databases, MADA-TOKAN tools and different heuristics for restoring vowel marks of Arabic NEs. Using this methodology, we have extracted approximately 45,000 Arabic NEs and built, to the best of our knowledge, the largest, most mature and well-structured Arabic NE lexical resource to date. We have stored and organised this lexicon following the LMF ISO standard. We conduct a quantitative and qualitative evaluation against a manually annotated gold standard and achieve precision scores from 95.83{\%} (with 66.13{\%} recall) to 99.31{\%} (with 61.45{\%} recall) according to different values of a threshold.
Statistical Machine Translation (MT) systems have achieved impressive results in recent years, due in large part to the increasing availability of parallel text for system training and development. This paper describes recent efforts at Linguistic Data Consortium to create linguistic resources for MT, including corpora, specifications and resource infrastructure. We review LDC's three-pronged ap-proach to parallel text corpus development (acquisition of existing parallel text from known repositories, harvesting and aligning of potential parallel documents from the web, and manual creation of parallel text by professional translators), and describe recent adap-tations that have enabled significant expansions in the scope, variety, quality, efficiency and cost-effectiveness of translation resource creation at LDC.
This paper presents a novel automatic approach to partially integrate FrameNet and WordNet. In that way we expect to extend FrameNet coverage, to enrich WordNet with frame semantic information and possibly to extend FrameNet to languages other than English. The method uses a knowledge-based Word Sense Disambiguation algorithm for matching the FrameNet lexical units to WordNet synsets. Specifically, we exploit a graph-based Word Sense Disambiguation algorithm that uses a large-scale knowledge-base derived from existing semantic resources. We have developed and tested additional versions of this algorithm showing substantial improvements over state-of-the-art results. Finally, we show some examples and figures of the resulting semantic resource.
We examine the performance of three dependency parsing systems, in particular, their performance variation across Wikipedia domains. We assess the performance variation of (i) Alpino, a deep grammar-based system coupled with a statistical disambiguation versus (ii) MST and Malt, two purely data-driven statistical dependency parsing systems. The question is how the performance of each parser correlates with simple statistical measures of the text (e.g. sentence length, unknown word rate, etc.). This would give us an idea of how sensitive the different systems are to domain shifts, i.e. which system is more in need for domain adaptation techniques. To this end, we extend the statistical measures used by Zhang and Wang (2009) for English and evaluate the systems on several Wikipedia domains by focusing on a freer word-order language, Dutch. The results confirm the general findings of Zhang and Wang (2009), i.e. different parsing systems have different sensitivity against various statistical measure of the text, where the highest correlation to parsing accuracy was found for the measure we added, sentence perplexity.
In this paper we present a project of annotating event chains for an important scientific domain ― carbon sequestration. This domain aims to reduce carbon emissions and has been identified by the U.S. National Academy of Engineering (NAE) as a grand challenge problem for the 21st century. Given a collection of scientific literature, we identify a set of centroid experiments; and then link and order the observations and events centered around these experiments on temporal or causal chains. We describe the fundamental challenges on annotations and our general solutions to address them. We expect that our annotation efforts will produce significant advances in inter-operability through new information extraction techniques and permit scientists to build knowledge that will provide better understanding of important scientific challenges in this domain, share and re-use of diverse data sets and experimental results in a more efficient manner. In addition, the annotations of metadata and ontology for these literature will provide important support for data lifecycle activities.
This paper presents the Multiword Expression Toolkit (mwetoolkit), an environment for type and language-independent MWE identification from corpora. The mwetoolkit provides a targeted list of MWE candidates, extracted and filtered according to a number of user-defined criteria and a set of standard statistical association measures. For generating corpus counts, the toolkit provides both a corpus indexation facility and a tool for integration with web search engines, while for evaluation, it provides validation and annotation facilities. The mwetoolkit also allows easy integration with a machine learning tool for the creation and application of supervised MWE extraction models if annotated data is available. In our experiment, the mwetoolkit was tested and evaluated in the context of MWE extraction in the biomedical domain. Our preliminary results show that the toolkit performs better than other approaches, especially concerning recall. Moreover, this first version can also be extended in several ways in order to improve the quality of the results.
In the paper we investigate the impact of data size on a Word Sense Disambiguation task (WSD). We question the assumption that the knowledge acquisition bottleneck, which is known as one of the major challenges for WSD, can be solved by simply obtaining more and more training data. Our case study on 1,000 manually annotated instances of the German verb ''``drohen'''' (threaten) shows that the best performance is not obtained when training on the full data set, but by carefully selecting new training instances with regard to their informativeness for the learning process (Active Learning). We present a thorough evaluation of the impact of different sampling methods on the data sets and propose an improved method for uncertainty sampling which dynamically adapts the selection of new instances to the learning progress of the classifier, resulting in more robust results during the initial stages of learning. A qualitative error analysis identifies problems for automatic WSD and discusses the reasons for the great gap in performance between human annotators and our automatic WSD system.
Fusional languages have rich inflection. As a consequence, tagsets capturing their morphological features are necessarily large. A natural way to make a tagset manageable is to use a structured system. In this paper, we present a positional tagset for describing morphological properties of Russian. The tagset was inspired by the Czech positional system (Hajic, 2004). We have used preliminary versions of this tagset in our previous work (e.g., Hana et al. (2004, 2006); Feldman (2006); Feldman and Hana (2010)). Here, we both systematize and extend these preliminary versions (by adding information about animacy, aspect and reflexivity); give a more detailed description of the tagset and provide comparison with the Czech system. Each tag of the tagset consists of 16 positions, each encoding one morphological feature (part-of-speech, detailed part-of-speech, gender, animacy, number, case, possessor's gender and number, person, reflexivity, tense, aspect, degree of comparison, negation, voice, variant). The tagset contains approximately 2,000 tags.
This paper presents BlogBuster, a tool for extracting a corpus from the blogosphere. The topic of cleaning arbitrary web pages with the goal of extracting a corpus from web data, suitable for linguistic and language technology research and development, has attracted significant research interest recently. Several general purpose approaches for removing boilerplate have been presented in the literature; however the blogosphere poses additional requirements, such as a finer control over the extracted textual segments in order to accurately identify important elements, i.e. individual blog posts, titles, posting dates or comments. BlogBuster tries to provide such additional details along with boilerplate removal, following a rule-based approach. A small set of rules were manually constructed by observing a limited set of blogs from the Blogger and Wordpress hosting platforms. These rules operate on the DOM tree of an HTML page, as constructed by a popular browser, Mozilla Firefox. Evaluation results suggest that BlogBuster is very accurate when extracting corpora from blogs hosted in the Blogger and Wordpress, while exhibiting a reasonable precision when applied to blogs not hosted in these two popular blogging platforms.
Many NLP applications need fundamental tools to convert the input text into appropriate form or format and extract the primary linguistic knowledge of words and sentences. These tools perform segmentation of text into sentences, words and phrases, checking and correcting the spellings, doing lexical and morphological analysis, POS tagging and so on. Persian is among languages with complex preprocessing tasks. Having different writing prescriptions, spacings between or within words, character codings and spellings are some of the difficulties and challenges in converting various texts into a standard one. The lack of fundamental text processing tools such as morphological analyser (especially for derivational morphology) and POS tagger is another problem in Persian text processing. This paper introduces a set of fundamental tools for Persian text processing in STeP-1 package. STeP-1 (Standard Text Preparation for Persian language) performs a combination of tokenization, spell checking, morphological analysis and POS tagging. It also turns all Persian texts with different prescribed forms of writing to a series of tokens in the standard style introduced by Academy of Persian Language and Literature (APLL). Experimental results show high performance.
Large corpora are essential to modern methods of computational linguistics and natural language processing. In this paper, we describe an ongoing project whose aim is to build a largest corpus of Czech texts. We are building the corpus from Czech Internet web pages, using (and, if needed, developing) advanced downloading, cleaning and automatic linguistic processing tools. Our concern is to keep the whole process language independent and thus applicable also for building web corpora of other languages. In the paper, we briefly describe the crawling, cleaning, and part-of-speech tagging procedures. Using a prototype corpus, we provide a comparison with a current corpora (in particular, SYN2005, part of the Czech National Corpora). We analyse part-of-speech tag distribution, OOV word ratio, average sentence length and Spearman rank correlation coefficient of the distance of ranks of 500 most frequent words. Our results show that our prototype corpus is now quite homogenous. The challenging task is to find a way to decrease the homogeneity of the text while keeping the high quality of the data.
During the last years the campaign of mass digitization made available catalogues and valuable rare manuscripts and old printed books vie the Internet. The Manuscriptorium digital library ingested hundreds of olumes and it is expected that the volume will grow up in the next years. Other European initiatives like Europeana and Monasterium have also as central activities the online presentation of cultural heritage. With the growing of the available on-line volumes, a special attention was paid to the management and retrieval of documents within digital libraries. Enabling semantic technologies and intelligent linking and search are a big step forward, but they still do not succeed in making the content of old rare books intelligible to the broad public or specialists in other domains or languages. In this paper we will argue that multilingual language technologies have the potential to fill this gap. We overview the existent language resources for historical documents, and present an architecture which aims at presenting such texts to the normal user, without altering the character of the texts.
This paper describes the Error-Annotated German Learner Corpus (EAGLE), a corpus of beginning learner German with grammatical error annotation. The corpus contains online workbook and and hand-written essay data from learners in introductory German courses at The Ohio State University. We introduce an error typology developed for beginning learners of German that focuses on linguistic properties of lexical items present in the learner data and present the detailed error typologies for selection, agreement, and word order errors. The corpus uses an error annotation format that extends the multi-layer standoff format proposed by Luedeling et al. (2005) to include incremental target hypotheses for each error. In this format, each annotated error includes information about the location of tokens affected by the error, the error type, and the proposed target correction. The multi-layer standoff format allows us to annotate ambiguous errors with more than one possible target correction and to annotate the multiple, overlapping errors common in beginning learner productions.
The definition of lexical semantic similarity measures has been the subject of lots of works for many years. In this article, we focus more specifically on distributional semantic similarity measures. Although several evaluations of this kind of measures were already achieved for determining if they actually catch semantic relatedness, it is still difficult to determine if a measure that performs well in an evaluation framework can be applied more widely with the same success. In the work we present here, we first select a semantic similarity measure by testing a large set of such measures against the WordNet-based Synonymy Test, an extended TOEFL test proposed in (Freitag et al., 2005), and we show that its accuracy is comparable to the accuracy of the best state of the art measures while it has less demanding requirements. Then, we apply this measure for extracting automatically synonyms from a corpus and we evaluate the relevance of this process against two reference resources, WordNet and the Moby thesaurus. Finally, we compare our results in details to those of (Curran and Moens, 2002).
In this paper, we present the multilingual Sense Folder Corpus. After the analysis of different corpora, we describe the requirements that have to be satisfied for evaluating semantic multilingual retrieval approaches. Justified by the unfulfilled requirements explained, we start creating a small bilingual hand-tagged corpus of 502 documents retrieved from Web searches. The documents contained in this collection have been created using Google queries. A single ambiguous word has been searched and related documents (approx. the first 60 documents for every keyword) have been retrieved. The document collection has been extended at the query word level, using single ambiguous words for English (argument, bank, chair, network and rule) and for Italian (argomento, lingua, regola, rete and stampa). The search and annotation process has been done both in a monolingual way for the English and the Italian language. 252 English and 250 Italian documents have been retrieved from Google and saved in their original rank. The performance of semantic multilingual retrieval systems has been evaluated using such a corpus with three baselines (Random, First Sense and Most Frequent Sense) that are formally presented and discussed. The fine-grained evaluation of the Sense Folder approach is discussed in details.
This paper examines how Natural Language Process (NLP) resources and online dialogue corpora can be used to extend coverage of Information Extraction (IE) templates in a Spoken Dialogue system. IE templates are used as part of a Natural Language Understanding module for identifying meaning in a user utterance. The use of NLP tools in Dialogue systems is a difficult task given 1) spoken dialogue is often not well-formed and 2) there is a serious lack of dialogue data. In spite of that, we have devised a method for extending IE patterns using standard NLP tools and available dialogue corpora found on the web. In this paper, we explain our method which includes using a set of NLP modules developed using GATE (a General Architecture for Text Engineering), as well as a general purpose editing tool that we built to facilitate the IE rule creation process. Lastly, we present directions for future work in this area.
We investigate Arabic Context Free Grammar parsing with dependency annotation comparing lexicalised and unlexicalised parsers. We study how morphosyntactic as well as function tag information percolation in the form of grammar transforms (Johnson, 1998, Kulick et al., 2006) affects the performance of a parser and helps dependency assignment. We focus on the three most frequent functional tags in the Arabic Penn Treebank: subjects, direct objects and predicates . We merge these functional tags with their phrasal categories and (where appropriate) percolate case information to the non-terminal (POS) category to train the parsers. We then automatically enrich the output of these parsers with full dependency information in order to annotate trees with Lexical Functional Grammar (LFG) f-structure equations with produce f-structures, i.e. attribute-value matrices approximating to basic predicate-argument-adjunct structure representations. We present a series of experiments evaluating how well lexicalized, history-based, generative (Bikel) as well as latent variable PCFG (Berkeley) parsers cope with the enriched Arabic data. We measure quality and coverage of both the output trees and the generated LFG f-structures. We show that joint functional and morphological information percolation improves both the recovery of trees as well as dependency results in the form of LFG f-structures.
In this paper, we present our work on constructing a textual semantic relation corpus by making use of an existing treebank annotated with discourse relations. We extract adjacent text span pairs and group them into six categories according to the different discourse relations between them. After that, we present the details of our annotation scheme, which includes six textual semantic relations, 'backward entailment', 'forward entailment', 'equality', 'contradiction', 'overlapping', and 'independent'. We also discuss some ambiguous examples to show the difficulty of such annotation task, which cannot be easily done by an automatic mapping between discourse relations and semantic relations. We have two annotators and each of them performs the task twice. The basic statistics on the constructed corpus looks promising: we achieve 81.17{\%} of agreement on the six semantic relation annotation with a .718 kappa score, and it increases to 91.21{\%} if we collapse the last two labels with a .775 kappa score.
This paper presents research on building a model of grammatical error correction, for preposition errors in particular, in English text produced by language learners. Unlike most previous work which trains a statistical classifier exclusively on well-formed text written by native speakers, we train a classifier on a large-scale, error-tagged corpus of English essays written by ESL learners, relying on contextual and grammatical features surrounding preposition usage. First, we show that such a model can achieve high performance values: 93.3{\%} precision and 14.8{\%} recall for error detection and 81.7{\%} precision and 13.2{\%} recall for error detection and correction when tested on preposition replacement errors. Second, we show that this model outperforms models trained on well-edited text produced by native speakers of English. We discuss the implications of our approach in the area of language error modeling and the issues stemming from working with a noisy data set whose error annotations are not exhaustive.
The collection and transcription of speech data is typically an expensive and time-consuming task. Voice over IP and cloud computing are poised to greatly reduce this impediment to research on spoken language interfaces in many domains. This paper documents our efforts to deploy speech-enabled web interfaces to large audiences over the Internet via Amazon Mechanical Turk, an online marketplace for work. Using the open source WAMI Toolkit, we collected corpora in two different domains which collectively constitute over 113 hours of speech. The first corpus contains 100,000 utterances of read speech, and was collected by asking workers to record street addresses in the United States. For the second task, we collected conversations with FlightBrowser, a multimodal spoken dialogue system. The FlightBrowser corpus obtained contains 10,651 utterances composing 1,113 individual dialogue sessions from 101 distinct users. The aggregate time spent collecting the data for both corpora was just under two weeks. At times, our servers were logging audio from workers at rates faster than real-time. We describe the process of collection and transcription of these corpora while providing an analysis of the advantages and limitations to this data collection method.
This paper advocates a complementary measure of translation performance that focuses on the constrastive ability of two or more systems or system versions to adequately translate source words. This is motivated by three main reasons : 1) existing automatic metrics sometimes do not show significant differences that can be revealed by fine-grained focussed human evaluation, 2) these metrics are based on direct comparisons between system hypotheses with the corresponding reference translations, thus ignoring the input words that were actually translated, and 3) as these metrics do not take input hypotheses from several systems at once, fine-grained contrastive evaluation can only be done indirectly. This proposal is illustrated on a multi-source Machine Translation scenario where multiple translations of a source text are available. Significant gains (up to +1.3 BLEU point) are achieved on these experiments, and contrastive lexical evaluation is shown to provide new information that can help to better analyse a system's performance.
We present a partial dependency parser for Irish. Constraint Grammar (CG) based rules are used to annotate dependency relations and grammatical functions. Chunking is performed using a regular-expression grammar which operates on the dependency tagged sentences. As this is the first implementation of a parser for unrestricted Irish text (to our knowledge), there were no guidelines or precedents available. Therefore deciding what constitutes a syntactic unit, and how it should be annotated, accounts for a major part of the early development effort. Currently, all tokens in a sentence are tagged for grammatical function and local dependency. Long-distance dependencies, prepositional attachments or coordination are not handled, resulting in a partial dependency analysis. Evaluations show that the partial dependency analysis achieves an f-score of 93.60{\%} on development data and 94.28{\%} on unseen test data, while the chunker achieves an f-score of 97.20{\%} on development data and 93.50{\%} on unseen test data.
One of the objectives of the Language Technologies for Life-Long Learning (LTfLL) project, is to develop a knowledge sharing system that connects learners to resources and learners to other learners. To this end, we complement the formal knowledge represented by existing domain ontologies with the informal knowledge emerging from social tagging. More specifically, we crawl data from social media applications such as Delicious, Slideshare and YouTube. Similarity measures are employed to select possible lexicalizations of concepts that are related to the ones present in the given ontology and which are assumed to be socially relevant with respect to the input lexicalisation. In order to identify the appropriate relationships which exist between the extracted related terms and the existing domain ontology, we employ several heuristics that rely on the use of a large background knowledge base, such as DBpedia. An evaluation of the resulting ontology has been carried out. The methodology proposed allows for an appropriate enrichment process and produces a complementary vocabulary to that of a domain expert.
Naturally-occurring instances of linguistic phenomena are important both for training and for evaluating automatic text processing. When available in large quantities, they also prove interesting material for linguistic studies. In this article, we present WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), a new freely-available resource built by automatically mining Wikipedias revision history. The WiCoPaCo corpus focuses on local modifications made by human revisors and include various types of corrections (such as spelling error or typographical corrections) and rewritings, which can be categorized broadly into meaning-preserving and meaning-altering revisions. We present an initial hand-built typology of these revisions, but the resource allows for any possible annotation scheme. We discuss the main motivations for building such a resource and describe the main technical details guiding its construction. We also present applications and data analysis on French and report initial results on spelling error correction and morphosyntactic rewriting. The WiCoPaCo corpus can be freely downloaded from http://wicopaco.limsi.fr.
This paper investigates whether high-quality annotations for tasks involving semantic disambiguation can be obtained without a major investment in time or expense. We examine the use of untrained human volunteers from Amazons Mechanical Turk in disambiguating prepositional phrase (PP) attachment over sentences drawn from the Wall Street Journal corpus. Our goal is to compare the performance of these crowdsourced judgments to the annotations supplied by trained linguists for the Penn Treebank project in order to indicate the viability of this approach for annotation projects that involve contextual disambiguation. The results of our experiments on a sample of the Wall Street Journal corpus show that invoking majority agreement between multiple human workers can yield PP attachments with fairly high precision. This confirms that a crowdsourcing approach to syntactic annotation holds promise for the generation of training corpora in new domains and genres where high-quality annotations are not available and difficult to obtain.
The development of a multilingual terminology is a very long and costly process. We present the creation of a multilingual terminological database called GRISP covering multiple technical and scientific fields from various open resources. A crucial aspect is the merging of the different resources which is based in our proposal on the definition of a sound conceptual model, different domain mapping and the use of structural constraints and machine learning techniques for controlling the fusion process. The result is a massive terminological database of several millions terms, concepts, semantic relations and definitions. The accuracy of the concept merging between several resources have been evaluated following several methods. This resource has allowed us to improve significantly the mean average precision of an information retrieval system applied to a large collection of multilingual and multidomain patent documents. New specialized terminologies, not specifically created for text processing applications, can be aggregated and merged to GRISP with minimal manual efforts.
Particular uses of PNs with sense extension are focussed on and inspected taking into account the presence of PNs in lexical semantic databases and electronic corpora. Methodology to select ad include PNs in semantic databases is described; the use of PNs in corpora of Italian Language is examined and evaluated, analyzing the behaviour of a set of PNs in different periods of time. Computational resources can facilitate our study in this field in an effective way by helping codify, translate and handle particular cases of polysemy, but also guiding in metaphorical and metonymic sense recognition, supported by the ontological classification of the lexical semantic entities. The relationship between the abstract and the concrete, which is at the basis of the Conceptual Metaphor perspective, can be considered strictly related to the variation of the ontological values found in our analysis of the PNs and their belonging classes which are codified in the ItalWordNet database.
WWe propose applying standardized linguistic annotation to terms included in labels of knowledge representation schemes (taxonomies or ontologies), hypothesizing that this would help improving ontology-based semantic annotation of texts. We share the view that currently used methods for including lexical and terminological information in such hierarchical networks of concepts are not satisfactory, and thus put forward ― as a preliminary step to our annotation goal ― a model for modular representation of conceptual, terminological and linguistic information within knowledge representation systems. Our CTL model is based on two recent initiatives that describe the representation of terminologies and lexicons in ontologies: the Terminae method for building terminological and ontological models from text (Aussenac-Gilles et al., 2008), and the LexInfo metamodel for ontology lexica (Buitelaar et al., 2009). CTL goes beyond the mere fusion of the two models and introduces an additional level of representation for the linguistic objects, whereas those are no longer limited to lexical information but are covering the full range of linguistic phenomena, including constituency and dependency. We also show that the approach benefits linguistic and semantic analysis of external documents that are often to be linked to semantic resources for enrichment with concepts that are newly extracted or inferred.
As the number of language resources accessible on the Internet increases, many efforts have been made for combining language resources and language processing tools to create new services. However, existing language resource coordination frameworks cannot manage issues of intellectual property associated with language resources, which make it difficult for most end-users to get supports for their intercultural collaborations because they always have to deal with the issues by themselves. In this paper, we aim at constructing a new language service management architecture on the Language Grid, which enables language resource providers to control access to their resources in accordance with their own policies. Furthermore, we apply the proposed architecture to the operating Language Grid in order to validate the effectiveness of the architecture. As a result, several service management models utilizing the monitoring and access constraints are occurring to satisfy various requirements from language resource providers. These models can handle paid-for language resources as well as free language resources. Finally, we discuss further challenging issues of combining language resources under each different policies.
In this paper, we present the results of an experiment with utilizing a stochastic morphosyntactic tagger as a pre-processing module of a rule-based chunker and partial parser for Croatian in order to raise its overall chunking and partial parsing accuracy on Croatian texts. In order to conduct the experiment, we have manually chunked and partially parsed 459 sentences from the Croatia Weekly 100 kw newspaper sub-corpus taken from the Croatian National Corpus, that were previously also morphosyntactically disambiguated and lemmatized. Due to the lack of resources of this type, these sentences were designated as a temporary chunking and partial parsing gold standard for Croatian. We have then evaluated the chunker and partial parser in three different scenarios: (1) chunking previously morphosyntactically untagged text, (2) chunking text that was tagged using the stochastic morphosyntactic tagger for Croatian and (3) chunking manually tagged text. The obtained F1-scores for the three scenarios were, respectively, 0.874 (P: 0.825, R: 0.930), 0.891 (P: 0.856, R: 0.928) and 0.914 (P: 0.904, R: 0.925). The paper provides the description of language resources and tools used in the experiment, its setup and discussion of results and perspectives for future work.
We propose a methodology for a novel type of discourse annotation whose model is tuned to the analysis of a text as narrative. This is intended to be the basis of a story bank resource that would facilitate the automatic analysis of narrative structure and content. The methodology calls for annotators to construct propositions that approximate a reference text, by selecting predicates and arguments from among controlled vocabularies drawn from resources such as WordNet and VerbNet. Annotators then integrate the propositions into a conceptual graph that maps out the entire discourse; the edges represent temporal, causal and other relationships at the level of story content. Because annotators must identify the recurring objects and themes that appear in the text, they also perform coreference resolution and word sense disambiguation as they encode propositions. We describe a collection experiment and a method for determining inter-annotator agreement when multiple annotators encode the same short story. Finally, we describe ongoing work toward extending the method to integrate the annotators interpretations of character agency (the goals, plans and beliefs that are relevant, yet not explictly stated in the text).
It is recognized that many evaluation metrics of machine translation in use that focus on surface word level suffer from their lack of tolerance of linguistic variance, and the incorporation of linguistic features can improve their performance. To this end, WordNet is therefore widely utilized by recent evaluation metrics as a thesaurus for identifying synonym pairs. On this basis, word pairs in similar meaning, however, are still neglected. We investigate the significance of this particular word group to the performance of evaluation metrics. In our experiments we integrate eight different measures of lexical semantic similarity into an evaluation metric based on standard measures of unigram precision, recall and F-measure. It is found that a knowledge-based measure proposed by Wu and Palmer and a corpus-based measure, namely Latent Semantic Analysis, lead to an observable gain in correlation with human judgments of translation quality, in an extent to which better than the use of WordNet for synonyms.
In this paper we outline the design and present a sample of the REBECA bilingual lexical-conceptual database constructed by linking two monolingual lexical resources in which a set of lexicalized concepts of the North-American English database, the Princeton WordNet (WN.Pr) synsets, is aligned with its corresponding set of lexicalized concepts of the Brazilian Portuguese database, the Brazilian Portuguese WordNet synsets under construction, by means of the MultiNet-based interlingual schema, the concepts of which are the ones represented by the Princeton WordNet synsets. Implemented in the Prot{\'e}g{\'e}-OWL editor, the alignment of the two databases illustrates how wordnets can be turned into ontolexicons. At the current stage of development, the wheeled-vehicle conceptual domain was modeled to develop and to test REBECAs design and contents, respectively. The collection of 205 ontological concepts worked out, i.e. REBECA{\'{}}s alignment indexes, is exemplified in the wheeled- vehicle conceptual domain, e.g. [CAR], [RAILCAR], etc., and it was selected in the WN.Pr database, version 2.0. Future work includes the population of the database with more lexical data and other conceptual domains so that the intricacies of adding more concepts and devising the spreading or pruning the relationships between them can be properly evaluated.
This paper attempts to participate in the ongoing discussion in search of a suitable model for the computational treatment of Greek morphology. Focusing on the unsupervised morphology learning technique, and particularly on the model of Linguistica by Goldsmith (2001), we attempt a computational treatment of specific word formation phenomena in Modern Greek (MG), such as suffixation and compounding with bound stems, through the use of various corpora. The inability of the system to accept any morphological rule as input, hence the term 'unsupervised', interferes to a great extent with its efficiency in parsing, especially in languages with rich morphology, such as MG, among others. Specifically, neither the rich allomorphy, nor the complex combinability of morphemes in MG appear to be treated efficiently through this technique, resulting in low scores of proper word segmentation (22{\%} in inflectional suffixes and 13{\%} in derivational ones), as well as the recognition of false morphemes.
We describe the implementation of an enterprise monitoring system that builds on an ontology-based information extraction (OBIE) component applied to heterogeneous data sources. The OBIE component consists of several IE modules - each extracting on a regular temporal basis a specific fraction of company data from a given data source - and a merging tool, which is used to aggregate all the extracted information about a company. The full set of information about companies, which is to be extracted and merged by the OBIE component, is given in the schema of a domain ontology, which is guiding the information extraction process. The monitoring system, in case it detects changes in the extracted and merged information on a company with respect to the actual state of the knowledge base of the underlying ontology, ensures the update of the population of the ontology. As we are using an ontology extended with temporal information, the system is able to assign time intervals to any of the object instances. Additionally, detected changes can be communicated to end-users, who can validate and possibly correct the resulting updates in the knowledge base.
One of the essential functions of natural language is to talk about spatial relationships between objects. Linguistic constructs can express highly complex, relational structures of objects, spatial relations between them, and patterns of motion through spaces relative to some reference point. Learning how to map this information onto a formal representation from a text is a challenging problem. At present no well-defined framework for automatic spatial information extraction exists that can handle all of these issues. In this paper we introduce the task of spatial role labeling and propose an annotation scheme that is language-independent and facilitates the application of machine learning techniques. Our framework consists of a set of spatial roles based on the theory of holistic spatial semantics with the intent of covering all aspects of spatial concepts, including both static and dynamic spatial relations. We illustrate our annotation scheme with many examples throughout the paper, and in addition we highlight how to connect to spatial calculi such as region connection calculus and also how our approach fits into related work.
The field of opinion mining has emerged in recent years as an exciting challenge for computational linguistics: investigating how humans express subjective judgments through linguistic means paves the way for automatic recognition and summarization of opinionated texts, with the possibility of determining the polarities and strengths of opinions asserted. Sentiment lexicons are basic resources for investigating the orientation of a text that can be performed considering polarized words included in it but they encode the polarity of word types instead that the polarity of word tokens. The expression of an opinion through the choice of lexical items is context-sensitive and sentiment lexicons could be integrated with syntagmatic patterns that emerge as significant with statistical analyses. In this paper it will be proposed a corpus analysis of adverbially modified ambiguous (e.g. fast, rich) and objective adjectives (e.g. chemical, political) - that can be occasionally exploited to express a subjective judgments -. Comparing polarity encoded in sentiment lexicons and the results of a logistic regression analysis, the role of adverbial cues for polarity detection will be evaluated on the basis of a small sample of sentences manually annotated.
In this paper we report on the progress in the creation of an Ontology-based lexicon for Bulgarian. We have started with the concept set from an upper ontology (DOLCE). Then it was extended with concepts selected from the OntoWordNet, which correspond to Core WordNet and EuroWordNet Basic concepts. The underlying idea behind the ontology-based lexicon is its organization via two semantic relations - equivalence and subsumption. These relations reflect the distribution of lexical unit senses with respect to the concepts in the ontology. The lexical unit candidates for concept mapping have been selected from two large and well-developed lexical resources for Bulgarian - a machine readable explanatory dictionary and a morphological lexicon. In the initial step, the lexical units were handled that have equivalent senses to the concepts in the ontology (2500 at the moment). Then, in the second stage, we are proceeding with lexical units selected on their frequency distribution in a large Bulgarian corpus. This step is the more challenging one, since it might require also additions of concepts to the ontology. The main applications of the lexicon are envisaged to be the semantic annotation and semantic IR for Bulgarian.
This paper describes recent efforts at Linguistic Data Consortium at the University of Pennsylvania to create manual transcripts as a shared resource for human language technology research and evaluation. Speech recognition and related technologies in particular call for substantial volumes of transcribed speech for use in system development, and for human gold standard references for evaluating performance over time. Over the past several years LDC has developed a number of transcription approaches to support the varied goals of speech technology evaluation programs in multiple languages and genres. We describe each transcription method in detail, and report on the results of a comparative analysis of transcriber consistency and efficiency, for two transcription methods in three languages and five genres. Our findings suggest that transcripts for planned speech are generally more consistent than those for spontaneous speech, and that careful transcription methods result in higher rates of agreement when compared to quick transcription methods. We conclude with a general discussion of factors contributing to transcription quality, efficiency and consistency.
Anaphora resolution is still a challenging research field in natural language processing, lacking a algorithm that correctly resolves anaphoric pronouns. Anaphoric zero pronouns pose an even greater challenge, since this category is not lexically realised. Thus, their resolution is conditioned by their prior identification stage. This paper reports on the distribution of zero pronouns in Romanian in various genres: encyclopaedic, legal, literary, and news-wire texts. For this purpose, the RoZP corpus has been created, containing almost 50000 tokens and 800 zero pronouns which are manually annotated. The distribution patterns are compared across genres, and exceptional cases are presented in order to facilitate the methodological process of developing a future zero pronoun identification and resolution algorithm. The evaluation results emphasise that zero pronouns appear frequently in Romanian, and their distribution depends largely on the genre. Additionally, possible features are revealed for their identification, and a search scope for the antecedent has been determined, increasing the chances of correct resolution.
Our purpose is to propose and discuss the latest version of an integrated method for dialogue analysis, annotation and evaluation., using a set of different pragmatic parameters. The annotation scheme Pr.A.Ti.D was built up on task-oriented dialogues. Dialogues are part of the CLIPS corpus of spoken Italian, which consists of spoken material stratified as regard as the diatopic variation. A description of the multilevel annotation scheme is provided, discussing some problems of its design and formalisation in a DTD for Xml mark-up. A further goal was to extend the use of Pr.A.Ti.D to other typologies of task-oriented texts and to verify the necessity and the amount of possible changes to the scheme, in order to make it more general and less oriented to specific purposes: a test on map task dialogues and consequent modifications of the scheme are presented. The application of the scheme allowed us to extract pragmatic indexes, typical of each kind of text types, and to perform both a qualitative and quantitative analysis of texts. Finally, in a linguistic perspective, a comparative analyses of conversational and communicative styles in dialogues performed by speakers belonging to different linguistic cultures and areas is proposed.
Grammars play an important role in many Natural Language Processing (NLP) applications. The traditional approach to creating grammars manually, besides being labor-intensive, has several limitations. With the availability of large scale syntactically annotated treebanks, it is now possible to automatically extract an approximate grammar of a language in any of the existing formalisms from a corresponding treebank. In this paper, we present a basic approach to extract grammars from dependency treebanks of two Indian languages, Hindi and Telugu. The process of grammar extraction requires a generalization mechanism. Towards this end, we explore an approach which relies on generalization of argument structure over the verbs based on their syntactic similarity. Such a generalization counters the effect of data sparseness in the treebanks. A grammar extracted using this system can not only expand already existing knowledge bases for NLP tasks such as parsing, but also aid in the creation of grammars for languages where none exist. Further, we show that the grammar extraction process can help in identifying annotation errors and thus aid in the task of the treebank validation.
This paper proposes statistical analysis methods for improvement of terminology entry compounding. Terminology entry compounding is a mechanism that identifies matching entries across multiple multilingual terminology collections. Bilingual or trilingual term entries are unified in compounded multilingual entry. We suggest that corpus analysis can improve entry compounding results by analysing contextual terms of given term in the corpus data. Proposed algorithm is described. It is implemented in an experimental setup. Results of experiment on compounding of Latvian and Lithuanian terminology resources are provided. These results encourage further research for different language pairs and in different domains.
Linguistic Data Consortium (LDC) at the University of Pennsylvania has participated as a data provider in a variety of governmentsponsored programs that support development of Human Language Technologies. As the number of projects increases, the quantity and variety of the data LDC produces have increased dramatically in recent years. In this paper, we describe the technical infrastructure, both hardware and software, that LDC has built to support these complex, large-scale linguistic data creation efforts at LDC. As it would not be possible to cover all aspects of LDCs technical infrastructure in one paper, this paper focuses on recent development. We also report on our plans for making our custom-built software resources available to the community as open source software, and introduce an initiative to collaborate with software developers outside LDC. We hope that our approaches and software resources will be useful to the community members who take on similar challenges.
This is an overall description of ADESSE (''``Base de datos de verbos, Alternancias de Di{\'a}tesis y Esquemas Sintactico-Sem{\'a}nticos del Espa{\~n}ol''''), an online database (http://adesse.uvigo.es/) with syntactic and semantic information for all clauses in a corpus of Spanish. The manually annotated corpus has 1.5 million words, 159,000 clauses and 3,450 different verb lemmas. ADESSE is an expanded version of BDS (''``Base de datos sint{\'a}cticos del espa{\~n}ol actual''''), which contains the grammatical features of verbs and verb-arguments in the corpus. ADESSE has added semantic features such as verb sense, verb class and semantic role of arguments to make possible a detailed syntactic and semantic corpus-based characterization of verb valency. Each verb entry in the database is described in terms of valency potential and valency realizations (diatheses). The former includes a set of semantic roles of participants in a particular event type and a classification into a conceptual hierarchy of process types. Valency realizations are described in terms of correspondences of voice, syntactic functions and categories, and semantic roles. Verbs senses are discriminated at two levels: a more abstract level linked to a valency potential, and more specific verb senses taking into account particular lexical instantiations of arguments.
The availability of large collections of text have made it possible to build language models that incorporate counts of billions of n-grams. This paper proposes two new methods of efficiently storing large language models that allow O(1) random access and use significantly less space than all known approaches. We introduce two novel data structures that take advantage of the distribution of n-grams in corpora and make use of various numbers of minimal perfect hashes to compactly store language models containing full frequency counts of billions of n-grams using 2.5 Bytes per n-gram and language models of quantized probabilities using 2.26 Bytes per n-gram. These methods allow language processing applications to take advantage of much larger language models than previously was possible using the same hardware and we additionally describe how they can be used in a distributed environment to store even larger models. We show that our approaches are simple to implement and can easily be combined with pruning and quantization to achieve additional reductions in the size of the language model.
The goal of DARPAs Machine Reading (MR) program is nothing less than making the worlds natural language corpora available for formal processing. Most text processing research has focused on locating mission-relevant text (information retrieval) and on techniques for enriching text by transforming it to other forms of text (translation, summarization) ― always for use by humans. In contrast, MR will make knowledge contained in text available in forms that machines can use for automated processing. This will be done with little human intervention. Machines will learn to read from a few examples and they will read to learn what they need in order to answer questions or perform some reasoning task. Three independent Reading Teams are building universal text engines which will capture knowledge from naturally occurring text and transform it into the formal representations used by Artificial Intelligence. An Evaluation Team is selecting and annotating text corpora with task domain concepts, creating model reasoning systems with which the reading systems will interact, and establishing question-answer sets and evaluation protocols to measure progress toward this goal. We describe development of the MR evaluation framework, including test protocols, linguistic resources and technical infrastructure.
The Text Analysis Conference (TAC) is a series of Natural Language Processing evaluation workshops organized by the National Institute of Standards and Technology. The Knowledge Base Population (KBP) track at TAC 2009, a hybrid descendant of the TREC Question Answering track and the Automated Content Extraction (ACE) evaluation program, is designed to support development of systems that are capable of automatically populating a knowledge base with information about entities mined from unstructured text. An important component of the KBP evaluation is the Entity Linking task, where systems must accurately associate text mentions of unknown Person (PER), Organization (ORG), and Geopolitical (GPE) names to entries in a knowledge base. Linguistic Data Consortium (LDC) at the University of Pennsylvania creates and distributes linguistic resources including data, annotations, system assessment, tools and specifications for the TAC KBP evaluations. This paper describes the 2009 resource creation efforts, with particular focus on the selection and development of named entity mentions for the Entity Linking task evaluation.
In this paper, we present a detailed and critical analysis of the behaviour of the CasEN named entity recognition system during the French Ester2 evaluation campaign. In this project, CasEN has been confronted with the task of detecting and categorizing named entities in manual and automatic transcriptions of radio broadcastings. At first, we give a general presentation of the Ester2 campaign. Then, we describe our system, based on transducers. Next, we depict how systems were evaluated during this campaign and we report the main official results. Afterwards, we investigate in details the influence of some annotation biases which have significantly affected the estimation of the performances of systems. At last, we conduct an in-depth analysis of the effective errors of the CasEN system, providing us with some useful indications about phenomena that gave rise to errors (e.g. metonymy, encapsulation, detection of right boundaries) and are as many challenges for named entity recognition systems.
This work presents a method of linking verbs and their valency frames in VerbaLex database developed at the Centre for NLP at the Faculty of Informatics Masaryk University to the frames in Berkeley FrameNet. While completely manual work may take a long time, the proposed semi-automatic approach requires a smaller amount of human effort to reach sufficient results. The method of linking VerbaLex frames to FrameNet frames consists of two phases. The goal of the first one is to find an appropriate FrameNet frame for each frame in VerbaLex. The second phase includes assigning FrameNet frame elements to the deep semantic roles in VerbaLex. In this work main emphasis is put on the exploitation of ontologies behind VerbaLex and FrameNet. Especially, the method of linking FrameNet frame elements with VerbaLex semantic roles is built using the information provided by the ontology of semantic types in FrameNet. Based on the proposed technique, a semi-automatic linking tool has been developed. By linking FrameNet to VerbaLex, we are able to find a non-trivial subset of the interlingual FrameNet frames (including their frame-to-frame relations), which could be used as a core for building FrameNet in Czech.
This paper aims to develop a framework for automatic CG tagging. We investigated two main algorithms, CRF and Statistical alignment model based on information theory (SAM). We found that SAM gives the best results both in word level and sentence level. We got the accuracy 89.25{\%} in word level and 82.49{\%} in sentence level. Combining both methods can be suited for both known and unknown word.
In this paper we present preliminary work conducted on semi-automatic induction of inflectional paradigms from non annotated corpora using the open-source tool Linguistica (Goldsmith 2001) that can be utilized without any prior knowledge of the language. The aim is to induce morphology information from corpora such as to compare languages and foresee the difficulty to develop morphosyntactic lexica. We report on a series of corpus-based experiments run with Linguistica in Romance languages (Catalan, French, Italian, Portuguese, and Spanish), Germanic languages (Dutch, English and German), and Slavic language Polish. For each language we obtained interesting clusters of stems sharing the same suffixes. They can be seen as mini inflectional paradigms that include productive derivative suffixes. We ranked results depending on the size of the paradigms (maximum number of suffixes per stem) per language. Results show that it is useful to get a first idea of the role and complexity of inflection and derivation in a language, to compare results with other languages, and that it could be useful to build lexicographic resources from scratch. Still, special post-processing is needed to face the two principal drawbacks of the tool: no clear distinction between inflection and derivation, and not taking allomorphy into account.
In this paper, we present a modeling of a syntactic lexicon for Arabic verbs. The structure of the lexicon is based on the recently introduced ISO standard called the Lexical Markup Framework. This standard enables us to describe the lexical information in a versatile way using general guidelines and make possible to share the resources developed in compliance with it. We discuss the syntactic information associated to verbs and the model we propose to structure and represent the entries within the lexicon. To study the usability of the lexicon in a real application, we designed a rule-based system that translates a LMF syntactic resource into Type Description Language compliant resource. The rules are mapping information from LMF entries and types to TDL types. The generated lexicon is used as input for a previously written HPSG grammar for Arabic built within the Language Knowledge Builder platform. Finally, we discuss improvements in parsing results and possible perspectives of this work.
India is considered a linguistic ocean with 4 language families and 22 scheduled national languages, and 100 un-scheduled languages reported by the 2001 census. This puts tremendous pressures on the Indian government to not only have comprehensive language policies, but also to create resources for their maintenance and development. In the age of information technology, there is a greater need to have a fine balance between allocation of resources to each language keeping in view the political compulsions, electoral potential of a linguistic community and other issues. In this connection, the government of India through various ministries and a think tank consisting of eminent linguistics and policy makers has done a commendable job despite the obvious roadblocks. This paper describes the Indian governments policies towards language development and maintenance in the age of technology through the Ministry of HRD through its various agencies and the Ministry of Communications {\&} Information Technology (MCIT) through its dedicated program called TDIL (Technology Development for Indian Languages). The paper also describes some of the recent activities of the TDIL in general and in particular, an innovative corpora project called ILCI - Indian Languages Corpora Initiative.
The paper presents results of an experiment dealing with sentiment analysis of Croatian text from the domain of finance. The goal of the experiment was to design a system model for automatic detection of general sentiment and polarity phrases in these texts. We have assembled a document collection from web sources writing on the financial market in Croatia and manually annotated articles from a subset of that collection for general sentiment. Additionally, we have manually annotated a number of these articles for phrases encoding positive or negative sentiment within a text. In the paper, we provide an analysis of the compiled resources. We show a statistically significant correspondence (1) between the overall market trend on the Zagreb Stock Exchange and the number of positively and negatively accented articles within periods of trend and (2) between the general sentiment of articles and the number of polarity phrases within those articles. We use this analysis as an input for designing a rule-based local grammar system for automatic detection of polarity phrases and evaluate it on held out data. The system achieves F1-scores of 0.61 (P: 0.94, R: 0.45) and 0.63 (P: 0.97, R: 0.47) on positive and negative polarity phrases.
This paper presents and evaluates an original approach to automatically align bitexts at the word level. It relies on a syntactic dependency analysis of the source and target texts and is based on a machine-learning technique, namely inductive logic programming (ILP). We show that ILP is particularly well suited for this task in which the data can only be expressed by (translational and syntactic) relations. It allows us to infer easily rules called syntactic alignment rules. These rules make the most of the syntactic information to align words. A simple bootstrapping technique provides the examples needed by ILP, making this machine learning approach entirely automatic. Moreover, through different experiments, we show that this approach requires a very small amount of training data, and its performance rivals some of the best existing alignment systems. Furthermore, cases of syntactic isomorphisms or non-isomorphisms between the source language and the target language are easily identified through the inferred rules.
We present the named entity annotation task within the on-going project of the National Corpus of Polish. To the best of our knowledge, this is the first attempt at a large-scale corpus annotation of Polish named entities. We describe the scope and the TEI-inspired hierarchy of named entities admitted for this task, as well as the TEI-conformant multi-level stand-off annotation format. We also discuss some methodological strategies including the annotation of embedded, coordinated and discontinuous names. Our annotation platform consists of two main tools interconnected by converting facilities. A rule-based natural language processing platform SProUT is used for the automatic pre-annotation of named entities, due to the previously created Polish extraction grammars adapted to the annotation task. A customizable graphical tree editor TrEd, extended to our needs, provides an ergonomic environment for manual correction of annotations. Despite some difficult cases encountered in the early annotation phase, about 2,600 named entities in 1,800 corpus sentences have presently been annotated, which allowed to validate the project methodology and tools.
Textual entailment has been recognized as a generic task that captures major semantic inference needs across many natural language processing applications. However, to date, textual entailment has not been considered in a cross-corpus setting, nor for user generated content. Given the emergence of Medicine 2.0, medical blogs are becoming an increasingly accepted source of information. However, given the characteristics of blogs( which tend to be noisy and informal; or contain a interspersing of subjective and factual sentences) a potentially large amount of irrelevant information may be present. Given the potential noise, the overarching problem with respect to information extraction from social media is achieving the correct level of sentence filtering - as opposed to document or blog post level. Specifically for the task of medical intelligence gathering. In this paper, we propose an approach to textual entailment with uses the text from one source of user generated content (T text) for sentence-level filtering within a new and less amenable one (H text), when the underlying domain, tasks or semantic information is the same, or overlaps.
The performance of most NLP applications relies upon the quality of linguistic resources. The creation, maintenance and enrichment of those resources are a labour-intensive task, especially when no tools are available. In this paper we present the NLP architecture OAL, designed to assist computational linguists in the whole process of the development of resources in an industrial context: from corpora compilation to quality assurance. To add new words more easily to the morphosyntactic lexica, a guesser that lemmatizes and assigns morphosyntactic tags as well as inflection paradigms to a new word has been developed. Moreover, different control mechanisms are set up to check the coherence and consistency of the resources. Today OAL manages resources in five European languages: French, English, Spanish, Italian and Polish. Chinese and Portuguese are in process. The development of OAL has followed an incremental strategy. At present, semantic lexica, a named entities guesser and a named entities phonetizer are being developed.
In this paper we discuss noun compounding, a highly generative, productive process, in three distinct languages: Czech, English and Zulu. Derivational morphology presents a large grey area between regular, compositional and idiosyncratic, non-compositional word forms. The structural properties of compounds in each of the languages are reviewed and contrasted. Whereas English compounds are head-final and thus left-branching, Czech and Zulu compounds usually consist of a leftmost governing head and a rightmost dependent element. Semantic properties of compounds are discussed with special reference to semantic relations between compound members which cross-linguistically show universal patterns, but idiosyncratic, language specific compounds are also identified. The integration of compounds into lexical resources, and WordNets in particular, remains a challenge that needs to be considered in terms of the compounds syntactic idiosyncrasy and semantic compositionality. Experiments with processing compounds in Czech, English and Zulu are reported and partly evaluated. The obtained partial lists of the Czech, English and Zulu compounds are also described.
The production of gold standard corpora is time-consuming and costly. We propose an alternative: the {\^a}silver standard corpus (SSC), a corpus that has been generated by the harmonisation of the annotations that have been delivered from a selection of annotation systems. The systems have to share the type system for the annotations and the harmonisation solution has use a suitable similarity measure for the pair-wise comparison of the annotations. The annotation systems have been evaluated against the harmonised set (630.324 sentences, 15,956,841 tokens). We can demonstrate that the annotation of proteins and genes shows higher diversity across all used annotation solutions leading to a lower agreement against the harmonised set in comparison to the annotations of diseases and species. An analysis of the most frequent annotations from all systems shows that a high agreement amongst systems leads to the selection of terms that are suitable to be kept in the harmonised set. This is the first large-scale approach to generate an annotated corpus from automated annotation systems. Further research is required to understand, how the annotations from different systems have to be combined to produce the best annotation result for a harmonised corpus.
We introduce the kddo1 ontology and semantically annotated kdd09cma1 corpus from the field of knowledge discovery in database (KDD) research. The corpus is based on the abstracts for the papers accepted into the KDD-2009 conference. Each abstract has its concept mentions identified and, where possible, linked to the appropriate concept in the ontology. The ontology is based on a human generated and readable semantic wiki focused on concepts and relationships for the domain along with other related topics, papers and researchers from information sciences. To our knowledge this is the first ontology and interlinked corpus for a subdiscipline within computing science. The dataset enables the evaluation of supervised approaches to semantic annotation of documents that contain a large number of high-level concepts relative the number of named entity mentions. We plan to continue to evolve the ontology based on the discovered relations within the corpus and to extend the corpus to cover other research paper abstracts from the domain. Both resources are publicly available at http://www.gabormelli.com/Projects/kdd/data/.
This paper presents the evaluation of the PIT Corpus of multi-party dialogues recorded in a Wizard-of-Oz environment. An evaluation has been performed with two different foci: First, a usability evaluation was used to take a look at the overall ratings of the system. A shortened version of the SASSI questionnaire, namely the SASSISV, and the well established AttrakDiff questionnaire assessing the hedonistic and pragmatic dimension of computer systems have been analysed. In a second evaluation, the user's gaze direction was analysed in order to assess the difference in the user's (gazing) behaviour if interacting with the computer versus the other dialogue partner. Recordings have been performed in different setups of the system, e.g. with and without avatar. Thus, the presented evaluation further focuses on the difference in the interaction caused by deploying an avatar. The quantitative analysis of the gazing behaviour has resulted in several encouraging significant differences. As a possible interpretation it could be argued that users are more attentive towards systems with an avatar - the difference a face makes.
This article discusses the treatment of collocations in the context of a long-term project on the development of multilingual NLP tools. Besides classical two-word collocations, we will focus on the case of complex collocations (3 words or more) for which a recursive design is presented in the form of collocation of collocations. Although comparatively less numerous than two-word collocations, the complex collocations pose important challenges for NLP. The article discusses how these collocations are retrieved from corpora, inserted and stored in a lexical database, how the parser uses such knowledge and what are the advantages offered by a recursive approach to complex collocations.
The EC-funded project DICIT developed distant-talking interfaces for interactive TV. The final DICIT prototype system processes multimodal user input by speech and remote control. It was designed to understand both natural language and command-and-control-style speech input. We conducted an evaluation campaign to examine the usability and performance of the prototype. The task-oriented evaluation involved naive test persons and consisted of a subjective part with a usability questionnaire and an objective part. We used three groups of objective metrics to assess the system: one group related to speech component performance, one related to interface design and user awareness, and a final group related to task-based effectiveness and usability. These metrics were acquired with a dedicated transcription and annotation tool. The evaluation revealed a quite positive subjective assessments of the system and reasonable objective results. We report how the objective metrics helped us to determine problems in specific areas and to distinguish design-related issues from technical problems. The metrics computed over modality-specific groups also show that speech input gives a usability advantage over remote control for certain types of tasks.
EcoLexicon, a multilingual knowledge resource on the environment, provides an internally coherent information system covering a wide range of specialized linguistic and conceptual needs. Data in our terminological knowledge base (TKB) are primarily hosted in a relational database which is now linked to an ontology in order to apply reasoning techniques and enhance user queries. The advantages of ontological reasoning can only be obtained if conceptual description is based on systematic criteria and a wide inventory of non-hierarchical relations, which confer dynamism to knowledge representation. Thus, our research has mainly focused on conceptual modelling and providing a user-friendly multimodal interface. The dynamic interface, which combines conceptual (networks and definitions), linguistic (contexts, concordances) and graphical information offers users the freedom to surf it according to their needs. Furthermore, dynamism is also present at the representational level. Contextual constraints have been applied to reconceptualise versatile concepts that cause a great deal of information overload.
Languages are born, evolve and, eventually, die. During this evolution their spelling rules (and sometimes the syntactic and semantic ones) change, putting old documents out of use. In Portugal, a pair of political agreements with Brazil forced relevant changes on the way the Portuguese language is written. In this article we will detail these two Orthographic Agreements (one in the thirties and the other more recently, in the nineties), and the challenges present on the automatic migration of old documents spelling to their actual one. We will reveal Bigorna, a toolkit for the classification of language variants, their comparison and the conversion of texts in different language versions. These tools will be explained together with examples of migration issues. As Birgorna relies on a set of conversion rules we will also discuss how to infer conversion rules from a set of documents (texts with different ages). The document concludes with a brief evaluation on the conversion and classification tool results and their relevance in the current Portuguese language scenario.
We propose to create a grid virtual organization for human language technologies, at first chiefly with the task of enabling linguistic researches to use existing distributed computing facilities of the European grid infrastructure for more efficient processing of large data sets. After a brief overview of modern grid computing, a number of common use-cases of natural language processing tasks running on the grid are presented, notably corpus annotation with morpho-syntactic tagging (600+ million-word corpus annotated in less than a day), {\$}n{\$}-gram statistics processing of a corpus and creation of grid-backed web-accessible services with annotation and term-extraction as examples. Implementation considerations and common problems of using grid for this type of tasks are laid out. We conclude with an outline of a simple action plan for evolving the infrastructure created for these experiments into a fully functional Human Language Technology grid Virtual Organization with the goal of making the power of European grid infrastructure available to the linguistic community.
Availability of labeled language resources, such as annotated corpora and domain dependent labeled language resources is crucial for experiments in the field of Natural Language Processing. Most often, due to lack of resources, manual verification and annotation of electronic text material is a prerequisite for the development of NLP tools. In the context of under-resourced language, the lack of copora becomes a crucial problem because most of the research efforts are supported by organizations with limited funds. Using free, multilingual and highly structured corpora like Wikipedia to produce automatically labeled language resources can be an answer to those needs. This paper introduces NLGbAse, a multilingual linguistic resource built from the Wikipedia encyclopedic content. This system produces structured metadata which make possible the automatic annotation of corpora with syntactical and semantical labels. A metadata contains semantical and statistical informations related to an encyclopedic document. To validate our approach, we built and evaluated a Named Entity Recognition tool, trained with Wikipedia corpora annotated by our system.
A variety of methods exist for extracting terms and relations between terms from a corpus, each of them having strengths and weaknesses. Rather than just using the joint results, we apply different extraction methods in a way that the results of one method are input to another. This gives us the leverage to find terms and relations that otherwise would not be found. Our goal is to create a semantic model of a domain. To that end, we aim to find the complete terminology of the domain, consisting of terms and relations such as hyponymy and meronymy, and connected to generic wordnets and ontologies. Terms are ranked by domain-relevance only as a final step, after terminology extraction is completed. Because term relations are a large part of the semantics of a term, we estimate the relevance from its relation to other terms, in addition to occurrence and document frequencies. In the KYOTO project, we apply language-neutral terminology extraction from a parsed corpus for seven languages.
This paper gives guidelines of how to annotate Propbank instances using a dedicated editor, Jubilee. Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate. Propbank annotation also requires the choice of a sense ID for each predicate. Jubilee facilitates this annotation process by displaying several resources of syntactic and semantic information simultaneously: the syntactic structure of a sentence is displayed in the main frame, the available senses with their corresponding argument structures are displayed in another frame, all available Propbank arguments are displayed for the annotators choice, and example annotations of each sense of the predicate are available to the annotator for viewing. Easy access to each of these resources allows the annotator to quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation. Jubilee has been successfully adapted to many Propbank projects in several universities. The tool runs platform independently, is light enough to run as an X11 application and supports multiple languages such as Arabic, Chinese, English, Hindi and Korean.
In natural language relationships between entities can asserted within a single sentence or over many sentences in a document. Many information extraction systems are constrained to extracting binary relations that are asserted within a single sentence (single-sentence relations) and this limits the proportion of relations they can extract since those expressed across multiple sentences (inter-sentential relations) are not considered. The analysis in this paper focuses on finding the distribution of inter-sentential and single-sentence relations in two corpora used for the evaluation of Information Extraction systems: the MUC6 corpus and the ACE corpus from 2003. In order to carry out this analysis we had to manually mark up all the management succession relations described in the MUC6 corpus. It was found that inter-sentential relations constitute 28.5{\%} and 9.4{\%} of the total number of relations in MUC6 and ACE03 respectively. This places upper bounds on the recall of information extraction systems that do not consider relations that are asserted across multiple sentences (71.5{\%} and 90.6{\%} respectively).
Identification of transliterations is aimed at enriching multilingual lexicons and improving performance in various Natural Language Processing (NLP) applications including Cross Language Information Retrieval (CLIR) and Machine Translation (MT). This paper describes work aimed at using the widely applied graphical models approach of Dynamic Bayesian Networks (DBNs) to transliteration identification. The task of estimating transliteration similarity is not very different from specific identification tasks where DBNs have been successfully applied; it is also possible to adapt DBN models from the other identification domains to the transliteration identification domain. In particular, we investigate the applicability of a DBN framework initially proposed by Filali and Bilmes (2005) to learn edit distance estimation parameters for use in pronunciation classification. The DBN framework enables the specification of a variety of models representing different factors that can affect string similarity estimation. Three DBN models associated with two of the DBN classes originally specified by Filali and Bilmes (2005) have been tested on an experimental set up of Russian-English transliteration identification. Two of the DBN models result in high transliteration identification accuracy and combining the models leads to even much better transliteration identification accuracy.
Recent years have brought a significant growth in the volume of research in sentiment analysis, mostly on highly subjective text types (movie or product reviews). The main difference these texts have with news articles is that their target is clearly defined and unique across the text. Following different annotation efforts and the analysis of the issues encountered, we realised that news opinion mining is different from that of other text types. We identified three subtasks that need to be addressed: definition of the target; separation of the good and bad news content from the good and bad sentiment expressed on the target; and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. Furthermore, we distinguish three different possible views on newspaper articles ― author, reader and text, which have to be addressed differently at the time of analysing sentiment. Given these definitions, we present work on mining opinions about entities in English language news, in which we apply these concepts. Results showed that this idea is more appropriate in the context of news opinion mining and that the approaches taking this into consideration produce a better performance.
Over the last several years, speech-based question answering (QA) has become very popular in contrast to pure search engine based approaches on a desktop. Open-domain QA systems are now much more powerful and precise, and they can be used in speech applications. Speech-based question answering systems often rely on predefined grammars for speech understanding. In order to improve the coverage of such complex AI systems, we reused speech patterns used to generate textual entailment patterns. These can make multimodal question understanding more robust. We exemplify this in the context of a domain-specific dialogue scenario. As a result, written text input components (e.g., in a textual input field) can deal with more flexible input according to the derived textual entailment patterns. A multimodal QA dialogue spanning over several domains of interest, i.e., personal address book entries, questions about the music domain and politicians and other celebrities, demonstrates how the textual input mode can be used in a multimodal dialogue shell.
I describe the construction of a corpus for research on relative clause extraposition in German based on the treebank T{\"u
This paper presents two corpora produced within the RPM2 project: a multi-document summarization corpus and a sentence compression corpus. Both corpora are in French. The first one is the only one we know in this language. It contains 20 topics with 20 documents each. A first set of 10 documents per topic is summarized and then the second set is used to produce an update summarization (new information). 4 annotators were involved and produced a total of 160 abstracts. The second corpus contains all the sentences of the first one. 4 annotators were asked to compress the 8432 sentences. This is the biggest corpus of compressed sentences we know, whatever the language. The paper provides some figures in order to compare the different annotators: compression rates, number of tokens per sentence, percentage of tokens kept according to their POS, position of dropped tokens in the sentence compression phase, etc. These figures show important differences from an annotator to the other. Another point is the different strategies of compression used according to the length of the sentence.
As the data for more and more languages is finding its way into digital form, with an increasing amount of this data being posted to the Web, it has become possible to collect language data from the Web and create large multilingual resources, covering hundreds or even thousands of languages. ODIN, the Online Database of INterlinear text (Lewis, 2006), is such a resource. It currently consists of nearly 200,000 data points for over 1,000 languages, the data for which was harvested from linguistic documents on the Web. We identify a number of issues with language identification for such broad-coverage resources including the lack of training data, ambiguous language names, incomplete language code sets, and incorrect uses of language names and codes. After providing a short overview of existing language code sets maintained by the linguistic community, we discuss what linguists and the linguistic community can do to make the process of language identification easier.
We describe results of a word sense annotation task using WordNet, involving half a dozen well-trained annotators on ten polysemous words for three parts of speech. One hundred sentences for each word were annotated. Annotators had the same level of training and experience, but interannotator agreement (IA) varied across words. There was some effect of part of speech, with higher agreement on nouns and adjectives, but within the words for each part of speech there was wide variation. This variation in IA does not correlate with number of senses in the inventory, or the number of senses actually selected by annotators. In fact, IA was sometimes quite high for words with many senses. We claim that the IA variation is due to the word meanings, contexts of use, and individual differences among annotators. We find some correlation of IA with sense confusability as measured by a sense confusion threshhold (CT). Data mining for association rules on a flattened data representation indicating each annotator's sense choices identifies outliers for some words, and systematic differences among pairs of annotators on others.
Resource-poor languages may suffer from a lack of any of the basic resources that are fundamental to computational linguistics, including an adequate digital lexicon. Given the relatively small corpus of texts that exists for such languages, extending the lexicon presents a challenge. Languages with complex morphology present a special case, however, because individual words in these languages provide a great deal of information about the grammatical properties of the roots that they are based on. Given a morphological analyzer, it is even possible to extract novel roots from words. In this paper, we look at the case of Tigrinya, a Semitic language with limited lexical resources for which a morphological analyzer is available. It is shown that this analyzer applied to the list of more than 200,000 Tigrinya words that is extracted by a web crawler can extend the lexicon in two ways, by adding new roots and by inferring some of the derivational constraints that apply to known roots.
This study attempts to pinpoint the factors that restrict reliable word sense annotation, focusing on the influence of the number of senses annotators use and the semantic granularity of those senses. Both of these factors may be possible causes of low interannotator agreement (ITA) when tagging with fine-grained word senses, and, consequently, low WSD system performance (Ng et al., 1999; Snyder {\&} Palmer, 2004; Chklovski {\&} Mihalcea, 2002). If number of senses is the culprit, modifying the task to show fewer senses at a time could improve annotator reliability. However, if overly nuanced distinctions are the problem, then more general, coarse-grained distinctions may be necessary for annotator success and may be all that is needed to supply systems with the types of distinctions that people make. We describe three experiments that explore the role of these factors in annotation performance. Our results indicate that of these two factors, only the granularity of the senses restricts interannotator agreement, with broader senses resulting in higher annotation reliability.
We present an evaluation framework to enable developers of information seeking, transaction based spoken dialogue systems to compare the robustness of natural language understanding (NLU) approaches across varying levels of word error rate and contrasting domains. We develop statistical and semantic parsing based approaches to dialogue act identification and concept retrieval. Voice search is used in each approach to ultimately query the database. Included in the framework is a method for developers to bootstrap a representative pseudo-corpus, which is used to estimate NLU performance in a new domain. We illustrate the relative merits of these NLU techniques by contrasting our statistical NLU approach with a semantic parsing method over two contrasting applications, our CheckItOut library system and the deployed Lets Go Public! system, across four levels of word error rate. We find that with respect to both dialogue act identification and concept retrieval, our statistical NLU approach is more likely to robustly accommodate the freer form, less constrained utterances of CheckItOut at higher word error rates than is possible with semantic parsing.
Ontology population from text is becoming increasingly important for NLP applications. Ontologies in OWL format provide for a standardized means of modeling, querying, and reasoning over large knowledge bases. Populated from natural language texts, they offer significant advantages over traditional export formats, such as plain XML. The development of text analysis systems has been greatly facilitated by modern NLP frameworks, such as the General Architecture for Text Engineering (GATE). However, ontology population is not currently supported by a standard component. We developed a GATE resource called the OwlExporter that allows to easily map existing NLP analysis pipelines to OWL ontologies, thereby allowing language engineers to create ontology population systems without requiring extensive knowledge of ontology APIs. A particular feature of our approach is the concurrent population and linking of a domainand NLP-ontology, including NLP-specific features such as safe reasoning over coreference chains.
We present an approach to automatically identifying the arguments of discourse connectives based on data from the Penn Discourse Treebank. Of the two arguments of connectives, called Arg1 and Arg2, we focus on Arg1, which has proven more challenging to identify. Our approach employs a sentence-based representation of arguments, and distinguishes ''``intra-sentential connectives'''', which take both their arguments in the same sentence, from ''``inter-sentential connectives'''', whose arguments are found in different sentences. The latter are further distinguished by paragraph position into ''``ParaInit'''' connectives, which appear in a paragraph-initial sentence, and ''``ParaNonInit'''' connectives, which appear elsewhere. The paper focusses on predicting Arg1 of Inter-sentential ParaNonInit connectives, presenting a set of scope-based filters that reduce the search space for Arg1 from all the previous sentences in the paragraph to a subset of them. For cases where these filters do not uniquely identify Arg1, coreference-based heuristics are employed. Our analysis shows an absolute 3{\%} performance improvement over the high baseline of 83.3{\%} for identifying Arg1 of Inter-sentential ParaNonInit connectives.
India is a multilingual country where machine translation and cross lingual search are highly relevant problems. These problems require large resources- like wordnets and lexicons- of high quality and coverage. Wordnets are lexical structures composed of synsets and semantic relations. Synsets are sets of synonyms. They are linked by semantic relations like hypernymy (is-a), meronymy (part-of), troponymy (manner-of) etc. IndoWordnet is a linked structure of wordnets of major Indian languages from Indo-Aryan, Dravidian and Sino-Tibetan families. These wordnets have been created by following the expansion approach from Hindi wordnet which was made available free for research in 2006. Since then a number of Indian languages have been creating their wordnets. In this paper we discuss the methodology, coverage, important considerations and multifarious benefits of IndoWordnet. Case studies are provided for Marathi, Sanskrit, Bodo and Telugu, to bring out the basic methodology of and challenges involved in the expansion approach. The guidelines the lexicographers follow for wordnet construction are enumerated. The difference between IndoWordnet and EuroWordnet also is discussed.
This paper presents a corpus of annotated motion events and their event structure. We consider motion events triggered by a set of motion evoking words and contemplate both literal and figurative interpretations of them. Figurative motion events are extracted into the same event structure but are marked as figurative in the corpus. To represent the event structure of motion, we use the FrameNet annotation standard, which encodes motion in over 70 frames. In order to acquire a diverse set of texts that are different from FrameNet's, we crawled blog and news feeds for five different domains: sports, newswire, finance, military, and gossip. We then annotated these documents with an automatic FrameNet parser. Its output was manually corrected to account for missing and incorrect frames as well as missing and incorrect frame elements. The corpus, UTD-MotionEvent, may act as a resource for semantic parsing, detection of figurative language, spatial reasoning, and other tasks.
This paper describes the Language Technology Resource Center (LTRC), a United States Government website for providing information and tools for users of languages (e.g., translators, analysts, systems administrators, researchers, developers, etc.) The LTRC provides information on a broad range of products and tools. It also provides a survey where product developers and researchers can provide the U.S. Government and the public with information about their work. A variety of reports are generated, including reports of all tools of a given type in a given language or dialect. Information is provided about standards, professional organizations, online resources, and other resources. The LTRC was developed and is run by MITRE, a Federally Funded Research and Development Center (FFRDC), on behalf of the U.S. Government. One of the major challenges for the future is to identify and provide information on the many new tools that are appearing. International collaboration is critical to cover this number of tools.
This paper presents the results of a terminological work conducted by the authors on a Digital Archives Net of the Italian National Research Council (CNR) in the field of Computer Science. In particular, the research tends to analyse the use of certain terms in Computer Science in order to verify their change over the time with the aim of retrieving from the net the very essence of documentation. Its main source is a reference corpus made up of 13,500 documents which collects the scientific productions of CNR. This study is divided in three sections: 1) an introductory one dedicated to the data extracted from the scientific documentation; 2) the second section is devoted to the description of the contents managed by the PUMA system; 3) the third part contains a statistical representation of terms extracted from archive: some comparison tables between the occurrences of the most used terms in the scientific documentation will be created and diagrams with percentages about the most frequently used terms will be displayed too. Indexes and concordances will allow to reflect on the use of certain terms in this field and give possible keys for having access to the extraction of knowledge.
This paper presents the PolNet-Polish WordNet project which aims at building a linguistically oriented ontology for Polish compatible with other WordNet projects such as Princeton WordNet, EuroWordNet and other similarly organized ontologies. The main idea behind this kind of ontologies is to use words related by synonymy to construct formal representations of concepts. In the paper we sketch the PolNet project methodology and implementation. We present data obtained so far, as well as the WQuery tool for querying and maintaining PolNet. WQuery is a query language that make use of data types based on synsets, word senses and various semantic relations which occur in wordnet-like lexical databases. The tool is particularly useful to deal with complex querying tasks like searching for cycles in semantic relations, finding isolated synsets or computing overall statistics. Both data and tools presented in this paper have been applied within an advanced AI system POLINT-112-SMS with emulated natural language competence, where they are used in the understanding subsystem.
15 years have gone by and ELRA continues embracing the needs of the HLT community to design its services and to implement them through its operational body, ELDA. The needs of the community have become much more ambitious...Larger language resources (LR), better quality ones (how do we reach a compromise between price ― maybe free ― and quality?), more annotations, at different levels and for different modalities...easy access to these LRs and solved IPR issues, appropriate and adaptable licensing schemas...large activity in HLT evaluation, both in terms of setting up the evaluation and in helping produce all necessary data, protocols, specifications as well as conducting the whole process...producing the LRs researchers and developers need, LRs for a wide variety of activities and technologies...for development, for training, for evaluation...Disseminating all knowledge in the field, whether generated at ELRA or elsewhere...keeping the community up to date with what goes on regularly (LREC conferences, LangTech, Newsletters, HLT Evaluation Portal, etc.). Needless to say, part of ELRAs evolution implies facing and anticipating the realities of the new Internet and data exchange era and remaining a LR backbone...looking into new models of LR data centres and platforms, LR access and exchange via web services, new models for infrastructures and repositories with even higher collaboration to make it happen. ELRA/ELDA participate in a number of international projects focused on this new production and sharing schema that will be detailed in the current paper.
Amazigh language and culture may well be viewed to have known an unprecedented booming in Morocco : more than a hundred- which are published by the Royal Institute of Amazigh Culture (IRCAM), an institution created in 2001 to preserve, promote and endorse Amazigh culture in all its dimensions. Crucially, publications in the Amazigh language would not have seen light without the valiant attempts to upgrade the language on the linguistic and technological levels. The central thrust of this contribution is to provide a vista about the whole range of actions carried out by IRCAM. Of prime utility to this presentation is what was accomplished to supply Amazigh with the necessary tools and corpora without which the Amazigh language would emphatically fail to have a place in the world of NITCs. After a brief description of the prime specificities that characterise the standardisation of Amazigh in Morocco, a retrospective on the basic computer tools now available for the processing of Amazigh will be set out. It is concluded that the homogenisation of a considerable number of corpora should, by right, be viewed as a strategic move and an incontrovertible prerequisite to the computerisation of Amazigh,
LRs remain expensive to create and thus rare relative to demand across languages and technology types. The accidental re-creation of an LR that already exists is a nearly unforgivable waste of scarce resources that is unfortunately not so easy to avoid. The number of catalogs the HLT researcher must search, with their different formats, make it possible to overlook an existing resource. This paper sketches the sources of this problem and outlines a proposal to rectify along with a new vision of LR cataloging that will to facilitates the documentation and exploitation of a much wider range of LRs than previously considered.
The manual transcription of human gesture behavior from video for linguistic analysis is a work-intensive process that results in a rather coarse description of the original motion. We present a novel approach for transcribing gestural movements: by overlaying an articulated 3D skeleton onto the video frame(s) the human coder can replicate original motions on a pose-by-pose basis by manipulating the skeleton. Our tool is integrated in the ANVIL tool so that both symbolic interval data and 3D pose data can be entered in a single tool. Our method allows a relatively quick annotation of human poses which has been validated in a user study. The resulting data are precise enough to create animations that match the original speaker's motion which can be validated with a realtime viewer. The tool can be applied for a variety of research topics in the areas of conversational analysis, gesture studies and intelligent virtual agents.
Building a comprehensive pronunciation lexicon is a crucial element in the success of any speech recognition engine. The first stage of lexicon design involves the compilation of a comprehensive word list that keeps the Out-Of-Vocabulary (OOV) word rate to a minimum. The second stage involves providing optimized phonemic representations for all lexical items on the list. The research presented here focuses on the first stage of lexicon design ― word list compilation, and describes the methodologies employed in the collection of a pronunciation lexicon designed for the purpose of American English voice message transcription using speech recognition. The lexicon design used is based on a topic domain structure with a target of 90{\%} word coverage for each domain. This differs somewhat from standard approaches where probable words from textual corpora are extracted. This paper raises four issues involved in lexicon design for the transcription of spontaneous voice messages: the inclusion of interjections and other characteristics common to spontaneous speech; the identification of unique messaging terminology; the relative ratio of proper nouns to common words; and the overall size of the lexicon.
This paper describes changing needs among the communities that exploit language resources and recent LDC activities and publications that support those needs by providing greater volumes of data and associated resources in a growing inventory of languages with ever more sophisticated annotation. Specifically, it covers the evolving role of data centers with specific emphasis on the LDC, the publications released by the LDC in the two years since our last report and the sponsored research programs that provide LRs initially to participants in those programs but eventually to the larger HLT research communities and beyond.
After receiving his Ph.D. from MIT in 1962, Fred taught at Cornell from 1962 to 1972, worked at IBM from 1972 to 1993, and taught at Johns Hopkins from 1993 to 2010. Fred’s many technical accomplishments during this long and productive career can be seen as episodes in two epic narratives, which, like the Iliad and the Odyssey, are related but have separate themes and story lines. The theme of the ﬁrst epic is the return of Information Theory to center stage in speech and language processing; and the theme of the second epic is the development of a new relationship between science and engineering in speech recognition, computational linguistics, and artiﬁcial intelligence (AI) more generally. 
Good morning. I want to thank the ACL for awarding me the 2010 Lifetime Achievement Award. I’m honored to be included in the ranks of my respected colleagues who have received this award previously. I want to talk to you this morning about the evolution of some ideas that I think are important, with a little bit of historical and biographical context thrown in. I hope you’ll ﬁnd in what I say not only an appreciation for some of the ideas and where they came from, but also a trajectory that continues forward and suggests some solutions to problems not yet solved. 
University of Barcelona By providing a better understanding of paraphrase and coreference in terms of similarities and differences in their linguistic nature, this article delimits what the focus of paraphrase extraction and coreference resolution tasks should be, and to what extent they can help each other. We argue for the relevance of this discussion to Natural Language Processing. 1. Introduction Paraphrase extraction1 and coreference resolution have applications in Question Answering, Information Extraction, Machine Translation, and so forth. Paraphrase pairs might be coreferential, and coreference relations are sometimes paraphrases. The two overlap considerably (Hirst 1981), but their deﬁnitions make them signiﬁcantly different in essence: Paraphrasing concerns meaning, whereas coreference is about discourse referents. Thus, they do not always coincide. In the following example, b and d are both coreferent and paraphrastic, whereas a, c, e, f, and h are coreferent but not paraphrastic, and g and i are paraphrastic but not coreferent. (1) [Tony]a went to see [the ophthalmologist]b and got [his]c eyes checked. [The eye doctor]d told [him]e that [his]f [cataracts]g were getting worse. [His]h mother also suffered from [cloudy vision]i. The discourse model built for Example (1) contains six entities (i.e., Tony, the eye doctor, Tony’s eyes, Tony’s cataracts, Tony’s mother, cataracts). Because a, c, e, f, and h all point to Tony, we say that they are coreferent. In contrast, in paraphrasing, we do not need to build a discourse entity to state that g and i are paraphrase pairs; we restrict ourselves to semantic content and this is why we check for sameness of meaning between cataracts and cloudy vision alone, regardless of whether they are a referential unit in a discourse. Despite the differences, it is possible for paraphrasing and coreference to co-occur, as in the case of b and d. NLP components dealing with paraphrasing and coreference seem to have great potential to improve understanding and generation systems. As a result, they have been the focus of a large amount of work in the past couple of decades (see the surveys by ∗ CLiC, Department of Linguistics, Gran Via 585, 08007 Barcelona, Spain. E-mail: mrecasens@ub.edu. ∗∗ CLiC, Department of Linguistics, Gran Via 585, 08007 Barcelona, Spain. E-mail: marta.vila@ub.edu. 
Ralph Weischedel† Raytheon BBN Technologies We propose a novel string-to-dependency algorithm for statistical machine translation. This algorithm employs a target dependency language model during decoding to exploit long distance word relations, which cannot be modeled with a traditional n-gram language model. Experiments show that the algorithm achieves signiﬁcant improvement in MT performance over a state-ofthe-art hierarchical string-to-string system on NIST MT06 and MT08 newswire evaluation sets. 1. Introduction n-gram Language Models (LMs) have been widely used in current Statistical Machine Translation (SMT) systems. Because they treat a sentence as a ﬂat string of tokens, a drawback of traditional n-gram LMs is that they cannot model long range word relations, such as predicate–argument attachments, that are critical to translation quality. We propose a hierarchical string-to-dependency translation model that exploits a dependency LM while decoding (as opposed to during reranking n-best output) to score alternative translations based on their structural soundness. In order to generate the structured output (dependency trees) required for dependency LM scoring, translation rules in our system represent the target side as dependency structures. We restrict the target side of the rules to well-formed dependency structures to weed out bad translation rules and enable efﬁcient decoding through dynamic programming. Due to the ﬂexibility of well-formed dependency structures, such structures can cover a large set of non-constituent transfer rules (Marcu et al. 2006) that have been shown useful for MT. For comparison purposes, as our baseline, we replicated the Hiero decoder (Chiang 2005), a state-of-the-art hierarchical string-to-string model. Our experiments show that the string-to-dependency decoder signiﬁcantly improves MT performance. Overall, the ∗ 10 Moulton Street, Cambridge, MA 02138. E-mail: libinshen@gmail.com. ∗∗ 10 Moulton Street, Cambridge, MA 02138. E-mail: jxu@bbn.com. † 10 Moulton Street, Cambridge, MA 02138. E-mail: weisched@bbn.com. Submission received: 6 March 2009; revised submission received: 1 December 2009; accepted for publication: 18 March 2010. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 4  improvement in BLEU score is around 2 BLEU points on NIST Arabic-to-English and Chinese-to-English newswire test sets. Section 2 brieﬂy discusses previous approaches to SMT in order to motivate our work. Section 3 provides an overview of our string-to-dependency translation system. Section 4 provides a complete description of our system, including formal deﬁnitions of well-formed dependency structures and their operations, as well as proofs about their key properties. Section 5 describes the implementation details, which include rule extraction, decoding, using dependency LM scores, and using labels in translation rules. We discuss experimental results in Section 6, compare our work with related work in Section 7, and draw conclusions in Section 8.  2. Previous Approaches to SMT Phrase-based systems (Koehn, Och, and Marcu 2003; Och 2003) had dominated SMT until recently. Such systems typically treat the input as a sequence of phrases (word n-grams), reorder them, and produce a translation for the reordered sentence based on translation options of each source phrase. A prominent feature of such systems is the use of an n-gram LM to measure the quality of translation hypotheses. A drawback of such systems is that the lack of structural information in the output makes it impossible to score translation hypotheses based on their structural soundness. The Hiero system (Chiang 2007) was a major breakthrough in SMT. Translation rules in Hiero contain non-terminals (NTs), as well as words, which allow the input to be translated in a hierarchical manner. Because both the source and target sides of its translation rules are strings with NTs, Hiero can be viewed as a hierarchical stringto-string model. Despite the hierarchical nature of its decoder, Hiero lacks the ability to measure translation quality based on structural relations such as predicate–argument agreement. Yamada and Knight (2001) proposed a syntax-based translation model that transfers a source parse tree into a target string. This method depends on the quality of source side parsing, and ignores target information during source side analysis. Mi, Huang, and Liu (2008) later proposed a translation model that takes the source parse forest as MT input to reduce translation errors due to imperfect source side analysis. Galley et al. (2004) proposed an MT model which produces target parse trees for string inputs in order to exploit the syntactic structure of the target language. Galley et al. (2006) formalized this approach with tree transducers (Graehl and Knight 2004) by using context-free parse trees to represent the target side. However, it was later shown by Marcu et al. (2006) and Wang, Knight, and Marcu (2007) that coverage could be a big issue for the constituent based rules, even though the translation rule set was already very large. Carreras and Collins (2009) introduced a string-to-tree MT model based on spinal Tree Adjoining Grammar (TAG) (Joshi and Schabes 1997; Shen, Champollion, and Joshi 2008). In this model, a translation rule is composed of a source string and a target elementary tree. Target hypothesis trees are combined with the adjoining operation, and there are no NT slots for substitution as in LTAG-spinal parsing (Shen and Joshi 2005, 2008; Carreras, Collins, and Koo 2008). Without the constraint of NT slots, the adjoining operation allows very ﬂexible composition, so that the search space becomes much larger. One has to carefully prune the search space. DeNeefe and Knight (2009) proposed another TAG-based MT model. In their implementation, a TAG grammar was transformed to an equivalent Tree Insertion 650  Shen, Xu, and Weischedel  String-to-Dependency Statistical Machine Translation  Grammar (TIG). In this way, they do not have an explicit adjoining operation in their system, and as such reduce the search space in decoding. Sub-trees are combined with NT substitution. Many researchers followed the tree-to-tree approach (Shieber and Schabes 1990) to take advantage of structural knowledge on both sides—for example, as in the papers by Hajicˇ et al. (2002), Eisner (2003), Ding and Palmer (2005), and Quirk, Menezes, and Cherry (2005). Although tree-to-tree models can represent rich structural information of the input and the output, they have not signiﬁcantly improved MT performance, possibly due to a much larger grammar and search space. On the other hand, Smith and Eisner (2006) showed the necessity of allowing loose transformations between the trees, which made tree-to-tree models even more complicated.  3. Overview of String-to-Dependency Translation Our system is designed to address problems with existing SMT approaches. It is novel in two respects. First, it uses a dependency LM to model long-distance relations. Second, it uses well-formed dependency structures to represent translation hypotheses to achieve an effective trade-off between model coverage and decoding complexity.  3.1 Dependency-Based Translation and Language Models  Our system generates target dependency trees as output and exploits a dependency LM in scoring translation hypotheses. As described before, the goal of using a dependency LM is to exploit long-distance word dependencies and as such model the quality of the output more accurately. Figure 1 shows an example dependency tree. Each arrow points from a child to its parent. In this example, the word ﬁnd is the root. For the purpose of comparison, we ﬁrst show how a simpliﬁed SMT system uses an n-gram LM to score translation hypotheses:  Se = argmax P(Se|Sf )w1 P(Sf |Se )w2 P(Se )w3  (1)  Se  where w1, w2, and w3 are feature weights. Sf is the input and Se’s are outputs. P(Se|Sf ) is the probability of the target string given the source, and P(Sf |Ss) is the probability of the source given the target. P(Se) is the prior probability of the target string Se using an n-gram LM.  Figure 1 The dependency tree for sentence the boy will ﬁnd it interesting. 651  Computational Linguistics  Volume 36, Number 4  In comparison, the scoring function in our system is:  D = argmax P(D|Sf )w1 P(Sf |D)w2 P(D)w3  (2)  D  where P(D) is the dependency LM score of target dependency tree D. We will show how to compute P(D) in Section 5.4. We can rewrite Equation (2) with a linear model:  n  D = argmax wiFi(Sf , D)  (3)  D i=1  where n = 3, F1 = log P(D|Sf ), F2 = log P(Sf |D), and F3 = log P(D). In practice, we use both a dependency LM and a traditional n-gram LM (also known as a string LM), as well as several other features, in our decoder. Section 5.6 lists all the features used in our decoder.  3.2 Well-Formed Dependency Structures A central question in our system design is: What kinds of dependency structures are allowed in translation rules? One extreme would be to allow any arbitrary multiple level treelets, as in Ding and Palmer (2005) and Quirk, Menezes, and Cherry (2005). One can deﬁne translation rules on any fragment of a parse/dependency tree. It offers maximum coverage of translation patterns, but suffers from data sparseness and a large search space. The other extreme would be to allow only complete (CFG) constituents. This offers a more robust model and a small search space, but excludes many useful transfer rules. In our system, the target side hypotheses are restricted to well-formed dependency structures (see Section 4 for formal deﬁnitions) for a trade-off between rule coverage, model robustness, and decoding complexity. In short, a well-formed dependency structure is either (1) a single rooted tree, with each child being a complete sub-tree, or (2) a sequence of siblings, each being a complete sub-tree. Well-formed dependency structures are very ﬂexible and can represent a variety of non-constituent rules in addition to rules that are complete constituents. For example, the following translation  Chinese-to-English hong −−−−−−−−−−−−−→ the red  is obviously useful for Chinese-to-English MT, but cannot be represented in some treebased translation systems since the red is a partial constituent. However, it is a valid dependency structure in our system.  4. Formalism We ﬁrst formally deﬁne the well-formed dependency structures, which are used to represent target hypotheses. Then, we deﬁne the operations to build well-formed dependency structures from the bottom up in decoding.  652  Shen, Xu, and Weischedel  String-to-Dependency Statistical Machine Translation  4.1 Well-Formed Dependency Structures and Categories In order to exclude undesirable structures and reduce the search space, we only allow Se whose dependency structure D is well formed, which we will deﬁne subsequently. The well-formedness requirement will be applied to partial decoding results. Based on the results of previous work (DeNeefe et al. 2007), we keep two kinds of dependency structures, ﬁxed and ﬂoating. Fixed structures consist of a sub-root with children, each of which must be a complete constituent. We call them ﬁxed dependency structures because the head is known or ﬁxed. Floating structures consist of a number of consecutive sibling nodes of a common head, but the head itself is unspeciﬁed. Each of the siblings must be a complete constituent. Floating structures can represent many linguistically meaningful non-constituent structures: for example, like the red, a modiﬁer of a noun. Only those two kinds of dependency structures are well-formed structures in our system. In the rest of this section, we will provide formal deﬁnitions of well-formed structures and combinatory operations over them, so that we can easily manipulate them in decoding. Examples will be provided along with the formal deﬁnitions to aid understanding. Consider a sentence S = w1w2...wn. Let d1d2...dn represent the parent word IDs for each word. For example, d4 = 2 means that w4 depends on w2. If wi is a root, we deﬁne di = 0.  Deﬁnition 1 A dependency structure didi+1...dj, or di..j for short, is ﬁxed on head h, where h ∈ [i, j], or ﬁxed for short, if and only if it meets the following conditions 1. dh ∈/ [i, j] 2. ∀k ∈ [i, j] and k = h, dk ∈ [i, j] 3. ∀k ∈/ [i, j], dk = h or dk ∈/ [i, j] We say the category of di..j is (−, h, −), where − means this ﬁeld is undeﬁned.  Deﬁnition 2 A dependency structure di...dj is ﬂoating with children C, for a non-empty set C ⊆ {i, ..., j}, or ﬂoating for short, if and only if it meets the following conditions 1. ∃h ∈/ [i, j], s.t.∀k ∈ C, dk = h 2. ∀k ∈ [i, j] and k ∈/ C, dk ∈ [i, j] 3. ∀k ∈/ [i, j], dk ∈/ [i, j] We say the category of di..j is (C, −, −) if j < h, which means that children are on the left side of the head, or (−, −, C) otherwise. A category is composed of the three ﬁelds (A, h, B), where h is used to represent the head, and ﬁelds A and B represent left and right dependents of the head, respectively. A dependency structure is well-formed if and only if it is either ﬁxed or ﬂoating.  653  Computational Linguistics  Volume 36, Number 4  Examples We represent dependency structures with graphs. Figure 2 shows examples of ﬁxed structures, Figure 3 shows examples of ﬂoating structures, and Figure 4 shows illformed dependency structures. The structures in Figures 2 and 3 are well-formed. Figure 4(a) is ill-formed because boy does not have its child word the in the tree. Figure 4(b) is ill-formed because it is not a continuous segment due to the missing it. As for the example the red mentioned earlier, it is a well-formed ﬂoating dependency structure. It is easy to see that a ﬂoating structure whose child set C has only one element is also a ﬁxed structure. Actually, this is a desirable property on which we will introduce meta category operations later. However, for the sake of convenience, we would like to assign a single category to each well-formed structure.  Figure 2 Fixed dependency structures. Figure 3 Floating dependency structures. Figure 4 Ill-formed dependency structures. 654  Shen, Xu, and Weischedel  String-to-Dependency Statistical Machine Translation  Deﬁnition 3 Let structure T be well formed. Category cat of T is deﬁned as follows    (−, h, −) if T is ﬁxed on h  cat(T)  =    (C, −, −) (−, −, C)  if T is ﬂoating with children C on the left side AND |C| > 1 if T is ﬂoating with children C on the right side AND |C| > 1  cat is well-deﬁned according to Deﬁnitions 1 and 2.  4.2 Operations One of the purposes of introducing ﬂoating dependency structures is that siblings having a common parent will become a well-deﬁned entity, although they are not considered a constituent. We always build well-formed partial structures on the target side in decoding. Furthermore, we combine partial dependency structures in a way such that we can obtain all possible well-formed dependency structures (but no ill-formed ones) during bottom–up decoding. The solution is to employ categories (introduced earlier). Each well-formed dependency structure has a category. We can apply four combinatory operations over the categories. If we can combine two categories with a certain category operation, we can use a corresponding tree operation to combine two dependency structures. The category of the combined dependency structure is the result of the combinatory category operations.  Operations on Well-Formed Dependency Structures There are four types of operations on well-formed dependency structures. Instead of providing formal deﬁnitions, we use ﬁgures to illustrate these operations to make them easy to understand. Figure 1 shows a traditional dependency tree. Figure 5 shows the four operations for combining partial dependency structures, which are left adjoining (LA), right adjoining (RA), left concatenation (LC), and right concatenation (RC). We always combine two well-formed structures in one of the four ways, and obtain a larger well-formed structure. Two structures can be combined by adjoining, which is similar to the traditional dependency formalism. We can adjoin either a ﬁxed structure or a ﬂoating structure to the head of a ﬁxed structure. Completed siblings can be combined via concatenation. We can concatenate two ﬁxed structures, one ﬁxed structure with one ﬂoating structure, or two ﬂoating structures in the same direction. The ﬂexibility of the order of operation allows us to take advantage of various translation fragments encoded in transfer rules. Figure 6 shows alternative ways of applying operations on well-formed structures to build larger structures in a bottom– up style. Numbers represent the order of operation. The fact that the same dependency structure can have multiple derivations means that we can utilize various rules learned from different training samples. Such ﬂexibility is important for MT. 655  Computational Linguistics Figure 5 Operations over well-formed structures.  Volume 36, Number 4  Figure 6 Two alternative derivations of an example dependency tree.  Meta Operations on Categories We ﬁrst introduce three meta category operations, which will later be used to deﬁne category operations. Two of the meta operations are unary operations, left raising (LR) and right raising (RR), and one is the binary operation uniﬁcation (UF).  Deﬁnition 4 Meta Category Operations r LR((−, h, −)) = ({h}, −, −) r RR((−, h, −)) = (−, −, {h}) r UF((A1, h1, B1), (A2, h2, B2)) = NORM((A1 A2, h1 h2, B1 B2)) First, the raising operations are used to turn a completed ﬁxed structure into a ﬂoating structure, according to Theorem 1.  Theorem 1 A ﬁxed structure with category (−, h, −) for span [i, j] is also a ﬂoating structure with children {h} if there are no outside words depending on word h, which means that  ∀k ∈/ [i, j], dk = h  (4)  Proof It sufﬁces to show that all the three conditions of ﬂoating structures hold. Conditions 1 and 2 immediately follow from conditions 1 and 2 of the ﬁxed structure, respectively. Condition 3 is met according to Equation (4) and condition 3 of the ﬁxed structure.  656  Shen, Xu, and Weischedel  String-to-Dependency Statistical Machine Translation  Therefore, we can always raise a ﬁxed structure if we assume it is complete, that is,  Equation (4) holds.  Uniﬁcation is well-deﬁned if and only if we can unify all three elements and the  result is a valid ﬁxed or ﬂoating category. For example, we can unify a ﬁxed structure  with a ﬂoating structure or two ﬂoating structures in the same direction, but we cannot  unify two ﬁxed structures.    h1  if h2 = −  h1 h2 =  h2  if h1 = −  undeﬁned otherwise    A1  if A2 = −  A1  A2  =   A2 A1  ∪  A2  if A1 = − otherwise     (−, h, −) if h = −  (A, −, −) if h = −, B = −  NORM((A,  h,  B))  =    (−, −, B) undeﬁned  if h = −, A = − otherwise  Operations on Categories Now we deﬁne category operations. For the sake of convenience, we use the same names for category operations and dependency structure operations. We can easily use the meta category operations to deﬁne the four combinatory category operations. The deﬁnition of the operations is as follows. Deﬁnition 5 Combinatory category operations LA((A1, −, −), (−, h2, −)) = UF((A1, −, −), (−, h2, −)) LA((−, h1, −), (−, h2, −)) = UF(LR((−, h1, −)), (−, h2, −)) LC((A1, −, −), (A2, −, −)) = UF((A1, −, −), (A2, −, −)) LC((A1, −, −), (−, h2, −)) = UF((A1, −, −), LR((−, h2, −))) LC((−, h1, −), (A2, −, −)) = UF(LR((−, h1, −)), (A2, −, −)) LC((−, h1, −), (−, h2, −)) = UF(LR((−, h1, −)), LR((−, h2, −))) RA((−, h1, −), (−, −, B2)) = UF((−, h1, −), (−, −, B2)) RA((−, h1, −), (−, h2, −)) = UF((−, h1, −), RR((−, h2, −))) RC((−, −, B1), (−, −, B2)) = UF((−, −, B1), (−, −, B2)) RC((−, h1, −), (−, −, B2)) = UF(RR((−, h1, −)), (−, −, B2)) RC((−, −, B1), (−, h2, −)) = UF((−, −, B1), RR((−, h2, −))) RC((−, h1, −), (−, h2, −)) = UF(RR((−, h1, −)), RR((−, h2, −)))  657  Computational Linguistics  Volume 36, Number 4  Based on the deﬁnitions of dependency structure operations and category operations, one can verify the one-to-one correspondence. This correspondence can be formally stated in the following theorem.  Theorem 2 Suppose X and Y are well-formed dependency structures and OP(cat(X), cat(Y)) is welldeﬁned. We have  cat(OP(X, Y)) = OP(cat(X), cat(Y))  (5)  Proof The proof of the theorem is rather routine, so we just give a sketch here. One can show it by induction on the number of nodes in dependency structures. It sufﬁces to show that Equation (5) holds for all the operations. Actually, the category operations are designed to meet this requirement; the three ﬁelds of a category represent the head and the children on both sides. With category operations, we can easily track the types of dependency structures and constrain operations in decoding.  Soundness and Completeness Now we show the soundness and completeness of the operations on dependency structures. If we follow the operations deﬁned herein, we will build all the well-formed structures and only the well-formed structures.  Theorem 3 (Soundness) Let X and Y be two well-deﬁned dependency structures, and OP an operation over X and Y. It can be shown that OP(X, Y) is also a well-deﬁned dependency structure.  Proof Theorem 3 immediately follows Theorem 2.  Theorem 4 (Completeness) Let Z be a well-deﬁned dependency structure with at least two nodes. It can be shown that there exist well-formed structures X, Y and an operation OP, such that Z = OP(X, Y).  Proof If Z is ﬁxed on h, without losing generality, we assume g is the leftmost child (or rightmost if there is no left child) of h. We detach g from h, and obtain two sub-trees X and Y which are rooted on g and h respectively. It can be veriﬁed that X and Y are well-formed, and Z = LA(X, Y). If Z is ﬂoating with children {c1, c2, ..., cn}, where n > 1, we can split it into two ﬂoating structures with children {c1} and {c2, ..., cn}, respectively. It is easy to verify that they are the sub-structures X and Y that we are looking for.  658  Shen, Xu, and Weischedel  String-to-Dependency Statistical Machine Translation  5. Implementation 5.1 Translation Rules Translation rules are central to an MT system. In our system, each rule translates a source sub-string into a target dependency structure. The target side of the translation rules constitutes a tree grammar. One way to deﬁne a tree grammar is in the way we described earlier. Two wellformed structures can be combined into a larger one with adjoining or concatenation, and there is no non-terminal slot for substitution. This is similar to tree grammars without substitution, such as the original TAG (Joshi, Levy, and Takahashi 1975) and LTAG-spinal (Shen, Champollion, and Joshi 2008). A corresponding MT model was proposed in Carreras and Collins (2009). Search space is a major problem for such an approach, as we described earlier. In our system, we introduced NT substitution to combat the search problem. The NT slots for substitution come from what we have observed in training data. Combination of well-formed dependency structures can only happen on NT slots. By replacing NT slots with well-formed structures, we implicitly adjoin or concatenate sub-structures based on the dependency information stored in rules. We extract the translation rules from the training data containing word-to-word alignment and target parse trees, which we will explain in the next section. A similar strategy was employed by DeNeefe and Knight (2009). They turned a TAG into an equivalent TIG. In addition to these extracted rules, we also have special rules to adjoin or concatenate two neighboring hypotheses. Each of the special rules has two NT slots, but they vary on target dependency structures. They are comparable to the glue rules in Chiang (2005). To formalize translation rules and grammars, a string-to-dependency grammar G is a 4-tuple G = R, X, Tf , Te where R is a set of transfer rules. X is the only non-terminal type.1 Tf is a set of terminals (words) in the source language, and Te is a set of terminals in the target language. A string-to-dependency transfer rule R ∈ R is a 4-tuple R = Sf , Se, D, A where Sf ∈ (Tf ∪ {X})+ is a source string, Se ∈ (Te ∪ {X})+ is a target string, D represents the dependency structure for Se, and A is the alignment between Sf and Se. Non-terminal alignments in A must be one-to-one. We ignore the left hand side for both source and target, since there is only one NT type.  5.2 Rule Extraction Now we explain how we extract string-to-dependency rules from parallel training data. The procedure is similar to Chiang (2007) except that we maintain tree structures on the target side, instead of strings. Given sentence-aligned bilingual training data, we ﬁrst use GIZA++ (Och and Ney 2003) to generate word level alignment. We use a statistical CFG parser to parse  
Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-speciﬁc algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature. 1. Introduction The last two decades have seen a rising wave of interest among computational linguists and cognitive scientists in corpus-based models of semantic representation (Grefenstette 1994; Lund and Burgess 1996; Landauer and Dumais 1997; Schu¨ tze 1997; Sahlgren 2006; Bullinaria and Levy 2007; Grifﬁths, Steyvers, and Tenenbaum 2007; Pado´ and Lapata 2007; Lenci 2008; Turney and Pantel 2010). These models, variously known as vector spaces, semantic spaces, word spaces, corpus-based semantic models, or, using the term we will adopt, distributional semantic models (DSMs), all rely on some version of the distributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degree of semantic similarity between two words (or other linguistic units) can be modeled ∗ Center for Mind/Brain Sciences (CIMeC), University of Trento, C.so Bettini 31, 38068 Rovereto (TN), Italy. E-mail: marco.baroni@unitn.it. ∗∗ Department of Linguistics T. Bolelli, University of Pisa, Via Santa Maria 36, 56126 Pisa (PI), Italy. E-mail: alessandro.lenci@ling.unipi.it. Submission received: 11 January 2010; revised submission received: 15 April 2010; accepted for publication: 1 June 2010. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 4  as a function of the degree of overlap among their linguistic contexts. Conversely, the format of distributional representations greatly varies depending on the speciﬁc aspects of meaning they are designed to model. The most straightforward phenomenon tackled by DSMs is what Turney (2006b) calls attributional similarity, which encompasses standard taxonomic semantic relations such as synonymy, co-hyponymy, and hypernymy. Words like dog and puppy, for example, are attributionally similar in the sense that their meanings share a large number of attributes: They are animals, they bark, and so on. Attributional similarity is typically addressed by DSMs based on word collocates (Grefenstette 1994; Lund and Burgess 1996; Schu¨ tze 1997; Bullinaria and Levy 2007; Pado´ and Lapata 2007). These collocates are seen as proxies for various attributes of the concepts that the words denote. Words that share many collocates denote concepts that share many attributes. Both dog and puppy may occur near owner, leash, and bark, because these words denote properties that are shared by dogs and puppies. The attributional similarity between dog and puppy, as approximated by their contextual similarity, will be very high. DSMs succeed in tasks like synonym detection (Landauer and Dumais 1997) or concept categorization (Almuhareb and Poesio 2004) because such tasks require a measure of attributional similarity that favors concepts that share many properties, such as synonyms and co-hyponyms. However, many other tasks require detecting different kinds of semantic similarity. Turney (2006b) deﬁnes relational similarity as the property shared by pairs of words (e.g, dog–animal and car–vehicle) linked by similar semantic relations (e.g., hypernymy), despite the fact that the words in one pair might not be attributionally similar to those in the other pair (e.g., dog is not attributionally similar to car, nor is animal to vehicle). Turney generalizes DSMs to tackle relational similarity and represents pairs of words in the space of the patterns that connect them in the corpus. Pairs of words that are connected by similar patterns probably hold similar relations, that is, they are relationally similar. For example, we can hypothesize that dog–tail is more similar to car–wheel than to dog–animal, because the patterns connecting dog and tail (of, have, etc.) are more like those of car–wheel than like those of dog–animal (is a, such as, etc.). Turney uses the relational space to implement tasks such as solving analogies and harvesting instances of relations. Although they are not explicitly expressed in these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and focus on learning one relation type at a time (e.g., ﬁnding parts). Although semantic similarity, either attributional or relational, has the lion’s share in DSMs, similarity is not the only aspect of meaning that is addressed by distributional approaches. For instance, the notion of property plays a key role in cognitive science and linguistics, which both typically represent concepts as clusters of properties (Jackendoff 1990; Murphy 2002). In this case, the task is not to ﬁnd out that dog is similar to puppy or cat, but that it has a tail, it is used for hunting, and so on. Almuhareb (2006), Baroni and Lenci (2008), and Baroni et al. (2010) use the words co-occurring with a noun to approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction). The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674  Baroni and Lenci  Distributional Memory  a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004). Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being explored in linguistics, for instance in the study of semantic change (Sagi, Kaufmann, and Clark 2009), lexical variation (Peirsman and Speelman 2009), and for the analysis of multiword expressions (Alishahi and Stevenson 2008). The wealth and variety of semantic issues that DSMs are able to tackle conﬁrms the importance of looking at distributional data to explore meaning, as well as the maturity of this research ﬁeld. However, if we looked from a distance at the whole ﬁeld of DSMs we would see that, besides the general assumption shared by all models that information about the context of a word is an important key in grasping its meaning, the elements of difference overcome the commonalities. For instance, DSMs geared towards attributional similarity represent words in the contexts of other (content) words, thereby looking very different from models that represent word pairs in terms of patterns linking them. In turn, both these models differ from those used to explore concept properties or argument alternations. The typical approach in the ﬁeld has been a local one, in which each semantic task (or set of closely related tasks) is treated as a separate problem, that requires its own corpus-derived model and algorithm, both optimized to achieve the best performance in a given task, but lacking generality, since they resort to task-speciﬁc distributional representations, often complemented by additional taskspeciﬁc resources. As a consequence, the landscape of DSMs looks more like a jigsaw puzzle in which different parts have been completed and the whole ﬁgure starts to emerge from the fragments, but it is not clear yet how to put everything together and compose a coherent picture. We argue that the “one semantic task, one distributional model” approach represents a great limit of the current state of the art. From a theoretical perspective, corpusbased models hold promise as large-scale simulations of how humans acquire and use conceptual and linguistic information from their environment (Landauer and Dumais 1997). However, existing DSMs lack exactly the multi-purpose nature that is a hallmark of human semantic competence. The common view in cognitive (neuro)science is that humans resort to a single semantic memory, a relatively stable long-term knowledge database, adapting the information stored there to the various tasks at hand (Murphy 2002; Rogers and McClelland 2004). The fact that DSMs need to go back to their environment (the corpus) to collect ad hoc statistics for each semantic task, and the fact that different aspects of meaning require highly different distributional representations, cast many shadows on the plausibility of DSMs as general models of semantic memory. From a practical perspective, going back to the corpus to train a different model for each application is inefﬁcient, and it runs the risk of overﬁtting the model to a speciﬁc task, while losing sight of its adaptivity—a highly desirable feature for any intelligent system. Think, by contrast, of WordNet (Fellbaum 1998), a single, general purpose 675  Computational Linguistics  Volume 36, Number 4  network of semantic information that has been adapted to all sorts of tasks, many of them certainly not envisaged by the resource creators. We think that it is not by chance that no comparable resource has emerged from DSM development. In this article, we want to show that a uniﬁed approach is not only a desirable goal, but it is also a feasible one. With this aim in mind, we introduce Distributional Memory (DM), a generalized framework for distributional semantics. Differently from other current proposals that share similar aims, we believe that the lack of generalization in corpus-based semantics stems from the choice of representing co-occurrence statistics directly as matrices—geometrical objects that model distributional data in terms of binary relations between target items (the matrix rows) and their contexts (the matrix columns). This results in the development of ad hoc models that lose sight of the fact that different semantic spaces actually rely on the same kind of underlying distributional information. DM instead represents corpus-extracted co-occurrences as a third-order tensor, a ternary geometrical object that models distributional data in terms of word– link–word tuples. Matrices are then generated from the tensor in order to perform semantic tasks in the spaces they deﬁne. Crucially, these on-demand matrices are derived from the same underlying resource (the tensor) and correspond to different “views” of the same data, extracted once and for all from a corpus. DM is tested here on what we believe to be the most varied array of semantic tasks ever addressed by a single distributional model. In all cases, we compare the performance of several DM implementations to state-of-the-art results. While some of the ad hoc models that were developed to tackle speciﬁc tasks do outperform our most successful DM implementation, the latter is never too far from the top, without any task-speciﬁc tuning. We think that the advantage of having a general model that does not need to be retrained for each new task outweighs the (often minor) performance advantage of the task-speciﬁc models. The article is structured as follows. After framing our proposal within the general debate on co-occurrence modeling in distributional semantics (Section 2), we introduce the DM framework in Section 3 and compare it to other uniﬁed approaches in Section 4. Section 5 pertains to the speciﬁc implementations of the DM framework we will test experimentally. The experiments are reported in Section 6. Section 7 concludes by summarizing what we have achieved, and discussing the implications of these results for corpus-based distributional semantics. 2. Modeling Co-occurrence in Distributional Semantics Corpus-based semantics aims at characterizing the meaning of linguistic expressions in terms of their distributional properties. The standard view models such properties in terms of two-way structures, that is, matrices coupling target elements (either single words or whatever other linguistic constructions we try to capture distributionally) and contexts. In fact, the formal deﬁnition of semantic space provided by Pado´ and Lapata (2007) is built around the notion of a matrix M|B|×|T|, with B the set of basis elements representing the contexts used to compare the distributional similarity of the target elements T. This binary structure is inherently suitable for approaches that represent distributional data in terms of unstructured co-occurrence relations between an element and a context. The latter can be either documents (Landauer and Dumais 1997; Grifﬁths, Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from the target (Lund and Burgess 1996; Schu¨ tze 1997; Rapp 2003; Bullinaria and Levy 2007). We will refer to such models as unstructured DSMs, because they do not use the linguistic structure of texts to compute co-occurrences, and only record whether the target occurs  676  Baroni and Lenci  Distributional Memory  in or close to the context element, without considering the type of this relation. For instance, an unstructured DSM might derive from a sentence like The teacher eats a red apple that eat is a feature shared by apple and red, just because they appear in the same context window, without considering the fact that there is no real linguistic relation linking eat and red, besides that of linear proximity. In structured DSMs, co-occurrence statistics are collected instead in the form of corpus-derived triples: typically, word pairs and the parser-extracted syntactic relation or lexico-syntactic pattern that links them, under the assumption that the surface connection between two words should cue their semantic relation (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Almuhareb and Poesio 2004; Turney 2006b; Pado´ and Lapata 2007; Erk and Pado´ 2008; Rothenha¨usler and Schu¨ tze 2009). Distributional triples are also used in computational lexicography to identify the grammatical and collocational behavior of a word and to deﬁne its semantic similarity spaces. For instance, the Sketch Engine1 builds “word sketches” consisting of triples extracted from parsed corpora and formed by two words linked by a grammatical relation (Kilgarriff et al. 2004). The number of shared triples is then used to measure the attributional similarity between word pairs. Structured models take into account the crucial role played by syntactic structures in shaping the distributional properties of words. To qualify as context of a target item, a word must be linked to it by some (interesting) lexico-syntactic relation, which is also typically used to distinguish the type of this co-occurrence. Given the sentence The teacher eats a red apple, structured DSMs would not consider eat as a legitimate context for red and would distinguish the object relation connecting eat and apple as a different type of co-occurrence from the modiﬁer relation linking red and apple. On the other hand, structured models require more preliminary corpus processing (parsing or extraction of lexico-syntactic patterns), and tend to be more sparse (because there are more triples than pairs). What little systematic comparison of the two approaches has been carried out (Pado´ and Lapata 2007; Rothenha¨usler and Schu¨ tze 2009) suggests that structured models have a slight edge. In our experiments in Section 6.1 herein, the performance of unstructured and structured models trained on the same corpus is in general comparable. It seems safe to conclude that structured models are at least not worse than unstructured models—an important conclusion for us, as DM is built upon the structured DSM idea. Structured DSMs extract a much richer array of distributional information from linguistic input, but they still represent it in the same way as unstructured models. The corpus-derived ternary data are mapped directly onto a two-way matrix, either by dropping one element from the tuple (Pado´ and Lapata 2007) or, more commonly, by concatenating two elements. The two words can be concatenated, treating the links as basis elements, in order to model relational similarity (Pantel and Pennacchiotti 2006; Turney 2006b). Alternatively, pairs formed by the link and one word are concatenated as basis elements to measure attributional similarity among the other words, treated as target elements (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Almuhareb and Poesio 2004; Rothenha¨usler and Schu¨ tze 2009). In this way, typed DSMs obtain ﬁner-grained features to compute distributional similarity, but, by couching distributional information as two-way matrices, they lose the high expressive power of corpusderived triples. We believe that falling short of fully exploiting the potential of ternary  
Ulrike Padó† Vico Research and Consulting GmbH We present a vector space–based model for selectional preferences that predicts plausibility scores for argument headwords. It does not require any lexical resources (such as WordNet). It can be trained either on one corpus with syntactic annotation, or on a combination of a small semantically annotated primary corpus and a large, syntactically analyzed generalization corpus. Our model is able to predict inverse selectional preferences, that is, plausibility scores for predicates given argument heads. We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task (prediction of human plausibility judgments), gauging the inﬂuence of different parameters and comparing our model against other model classes. We obtain consistent beneﬁts from using the disambiguation and semantic role information provided by a semantically tagged primary corpus. As for parameters, we identify settings that yield good performance across a range of experimental conditions. However, frequency remains a major inﬂuence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items. 1. Introduction Selectional preferences or selectional constraints describe knowledge about possible and plausible ﬁllers for a predicate’s argument positions. They model the fact that there is often a semantically coherent set of concepts that can ﬁll a given argument position. Selectional preferences can help for many text analysis tasks which involve comparing different attachment decisions. Examples include syntactic disambiguation (Hindle and Rooth 1993; Toutanova et al. 2005), word sense disambiguation (WSD, ∗ Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ∗∗ E-mail: pado@cl.uni-heidelberg.de. † E-mail: ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 4  McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sentence reading times (Padó, Crocker, and Keller 2009). All these applications rely on the availability of broad-coverage, reliable selectional preferences for predicates and their argument positions. Given the immense effort necessary for manual semantic lexicon building and its associated reliability problems (see, e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences acquire selectional preferences automatically from large corpora. The simplest strategy is to extract triples (v, r, a) of a predicate, role, and argument headword (or ﬁller) from a corpus, and then to compute selectional preference as relative frequencies. However, due to the Zipﬁan nature of word frequencies, the ﬁrst step on its own results in a very sparse list of headwords, in particular for less frequent predicates. As an example, the verb anglicize only appears with nine direct objects in the 100-million word British National Corpus (BNC, Burnard 1995). Only one of them, name, appears more than once. Many highly plausible ﬁllers are missing from the list, such as word or spelling. In order to make sensible predictions for triples that are unseen at training time, it is crucial to add a generalization step that infers a degree of preference for new, unseen headwords for a given predicate and role.1 The result is, in the ideal case, an assignment to every possible headword of some degree of compatibility (or plausibility) with the predicate’s preferences. In the case of anglicize, the desired result would be a high plausibility for words like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, ﬁrst introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selectional preferences. Our model is fully distributional and does not require any knowledge sources beyond a large corpus where subjects and objects can be identiﬁed with reasonable accuracy. Its key point is to use vector space similarity (Lund and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen  
Looking for a book that lucidly sorts out XML, XSLT and XMI, GATE and UIMA, WordFreak, OpenNLP, and the Stanford NLP Toolkit? Looking for meticulous guidance via batteries of stylesheets and shell scripts, but also keen on exploiting specialpurpose rule languages such as JAPE, or pre-conﬁgured text analytics engines such as ANNIE? Looking for a coherent set of exercises (covering different technical angles and varying in task complexity) and a series of illustrative screenshots that break down the understanding of annotation workﬂows into easy-to-digest atomic thematic chunks? Preferring the hands-on “how-to” over dry and winding theory debates and formally based methodological discussions? Interested to get in touch (again) with Shakespeare’s Sonnet 130 (the text example running throughout the entire book)? Well, Graham Wilcock’s Introduction to Linguistic Annotation and Text Analytics might be a perfect match to enjoy all this. This volume, the third published lecture in Morgan & Claypool’s Synthesis Lectures on Human Language Technologies series, consists of six chapters. The ﬁrst one lays the XML-focused meta language foundations and provides additional insights into XML parsing and validation tools, as well as format-switching XML transformation routines using XSL Transformations (XSLT). Chapter 4 continues this technical thread as it elaborates on frameworks for interchanging annotations between different formats using XSLT, as well as the UML-based XML Metadata Interchange (XMI) format, an emerging standard to support the interchange of annotations produced by different tools. Introducing the WordFreak annotation tool, the second chapter sheds light on relevant linguistic annotation layers ranging from formal sentence splitting and tokenization via part-of-speech tagging, syntactic constituency parsing, and semantic predicate– argument analysis up to discourse phenomena such as co-reference resolution. Although this chapter focuses primarily on manual annotation (there is also a subsection at the end where WordFreak is linked with OpenNLP), the following one features automatic statistical annotation procedures. Easy-to-plug-in OpenNLP is contrasted here with stand-alone Stanford NLP tools at all major levels of the linguistic food chain, namely, sentence and token splitting, chunking and parsing, named entity recognition, and co-reference resolution. Increasing the level of abstraction at the systems level, the author then advances to comparing GATE and UIMA, two alternative architectures for text analytics. His emphasis is on the proper conﬁguration and task-speciﬁc customization (e.g., by introducing the JAPE rule language in GATE and the regex annotator in UIMA, both serving to improve named entity recognition). Again, practical exercises  Computational Linguistics  Volume 36, Number 4  are discussed in detail running through all levels of linguistic processing (integrating gazetteers/dictionaries, POS tagging, NP chunking and full parsing, named entity recognition, co-reference resolution, etc.). The ﬁnal chapter concludes with a survey of commercially available tools doing text analytics (e.g., alias-i’s LingPipe or IBM’s LanguageWare). Furthermore, an advanced treatment of named entity recognition (for job titles) and co-reference resolution is provided using the open-source frameworks of rule-based GATE and UIMA solutions (both incorporating customized dictionaries) and statistically based OpenNLP. The book assumes little prior knowledge, although regular expression, JAVA, and XML basics will certainly be helpful. It is targeted at undergraduate level (not necessarily computer science) students who wish to gather experience in reusing, modifying, and customizing existing linguistic annotation tools and text analytics architectures. Throughout the book, the author directs the reader to open-source, freely available, platform-independent, and easily downloadable software resources and repositories. So students ﬁnd themselves embedded in a stimulating playground where tooling is the message. The book is comprehensively written, well-structured, and very easy to follow, in particular when students have the exercises run simultaneously on their personal machines. The author has a clearly designed didactical master plan in mind which is realized by a large number of exercises with increasing, but never particularly high, complexity (see, e.g., the fourth chapter that deals with a nice set of transformation problems involving WordFreak–OpenNLP [XML–plain text], GATE–WordFreak [XML– XML], and WordFreak–UIMA [XML–XMI] tool [language] pairs). These exercises are reasonably chosen and solutions are technically well prepared and exhaustively explained with an admirable degree of clarity. However, the more advanced and experienced reader might ﬁnd the continuous pampering by way of overly ﬁne-grained thematic exposition and visualization a bit excessive, perhaps even wearisome. There is, for example, also no division into easy, medium, and hard problems to offer challenging tasks at different levels of students’ understanding. If the book is used in the context of more advanced teaching, it should be complemented by much more technical standard reference textbooks providing complementary theoretical background information on empirical NLP, (supervised) machine learning methods for NLP, computational corpus linguistics, and so forth. Yet, for getting used to mapping routines, workﬂows, and software underlying linguistic meta data annotation, this tutorial ﬁlls certainly a gap for students who come across these topics for the ﬁrst time and enjoy the all-embracing tutorial approach of the author. Any real complaints? Just a minor remark: all (URL) references are almost unreadable (Web links were not removed from the printed version of the book). There is also a dedicated Web site which contains copies of the material from this book.1 This book review was edited by Pierre Isabelle. Udo Hahn is Professor of Computational Linguistics and Language Technology at FriedrichSchiller-Universita¨t Jena (Germany) and Head of the Jena University Language & Information Engineering (JULIE) Lab. He works on text analytics (semantic search technologies, information extraction, text mining, and text summarization), with focus on biomedical language processing. Hahn’s address is Computerlinguistik, Friedrich-Schiller-Universita¨t Jena, D-07743 Jena, Germany; e-mail: udo.hahn@uni-jena.de; URL: http://www.julielab.de/. 
This book comes with “batteries included” (a reference to the phrase often used to explain the popularity of the Python programming language). It is the companion book to an impressive open-source software library called the Natural Language Toolkit (NLTK), written in Python. NLTK combines language processing tools (tokenizers, stemmers, taggers, syntactic parsers, semantic analyzers) and standard data sets (corpora and tools to access the corpora in an efﬁcient and uniform manner). Although the book builds on the NLTK library, it covers only a relatively small part of what can be done with it. The combination of the book with NLTK, a growing system of carefully designed, maintained, and documented code libraries, is an extraordinary resource that will dramatically inﬂuence the way computational linguistics is taught. The book attempts to cater to a large audience: It is a textbook on computational linguistics for science and engineering students; it also serves as practical documentation for the NLTK library, and it ﬁnally attempts to provide an introduction to programming and algorithm design for humanities students. I have used the book and its earlier on-line versions to teach advanced undergraduate and graduate students in computer science in the past eight years. The book adopts the following approach: r It is ﬁrst a practical approach to computational linguistics. It provides readers with practical skills to solve concrete tasks related to language. r It is a hands-on programming text: The ultimate goal of the book is to empower students to write programs that manipulate textual data and perform empirical experiments on large corpora. Importantly, NLTK includes a large set of corpora—this is one of the most useful and game-changing contributions of the toolkit. r It is principled: It exposes the theoretical underpinnings—both computational and linguistic—of the algorithms and techniques that are introduced. r It attempts to strike a pragmatic balance between theory and applications. The goal is to introduce “just enough theory” to ﬁt in a single semester course for advanced undergraduates, while still leaving room for practical programming and experimentation. r It aims to make working with language pleasurable.  Computational Linguistics  
Statistical Machine Translation provides a comprehensive and clear introduction to the most prominent techniques employed in the ﬁeld of the same name (SMT). This textbook is aimed at students or researchers interested in a thorough entry-point to the ﬁeld, and it does an excellent job of providing basic understanding for each of the many pieces of a statistical translation system. I consider this book to be an essential addition to any advanced undergraduate course or graduate course on SMT. The book is divided into three parts: Foundations, Core Methods, and Advanced Topics. Foundations (75 pages) covers an introduction to translation, working with text, and probability theory. Core Methods (170 pages) covers the main components of a standard phrase-based SMT system. Advanced Topics (125 pages) covers discriminative training and linguistics in SMT, including an in-depth discussion of syntactic SMT. The text as a whole assumes a certain familiarity with natural language processing; though the Foundations section provides an effort to ﬁll in the gaps, the book’s focus is decidedly translation. As such, students unfamiliar with NLP may sometimes need to consult a general NLP text. The book aims to provide a thorough introduction to each component of a statistical translation system, and it deﬁnitely succeeds in doing so. Supplementing this core material for each chapter is a highly inclusive Further Reading section. These sections provide brief narratives highlighting many relevant papers and alternative techniques for each topic addressed in the chapter. I suspect many readers will ﬁnd these literature pointers to be quite valuable, from students wishing to dive deeper, to experienced SMT researchers wishing to get started in a new sub-ﬁeld. Each chapter also closes with a short list of exercises. Many of these are very challenging (accurately indicated by a star-rating system), and involve getting your hands dirty with tools downloaded from the Web. The usefulness of these exercises will depend largely on the instructor’s tastes; I view them as a bonus rather than a core feature of the book. 1. Chapters 1–3: Foundations The ﬁrst three chapters provide foundational knowledge for the rest of the book. Introduction provides an overview of the book and a brief history of machine translation, along with a discussion of applications and an expansive list of resources. The overview’s structure takes the form of a summary of each chapter. This structure provides an effective preview of what will be covered and in what order, but it does not focus on typical introduction material; for example, there is no one place set aside to convince the reader that SMT is a good idea, or to introduce concisely the main philosophies behind the ﬁeld. The history section is enjoyable, and I was glad to see a cautionary  Computational Linguistics  Volume 36, Number 4  note regarding machine translation’s history of high hopes and disappointments. The applications section provides an excellent overview of where SMT sees actual use, and helps the reader understand why translations do not always need to be prefect. Words, Sentences, Corpora provides a whirlwind tour of NLP basics, brieﬂy touching on a broad set of topics including Zipf’s law, parts-of-speech, morphology, and a number of grammar formalisms. To give an idea of just how brief coverage can be, the section on grammar covers four formalisms in ﬁve pages. Nonetheless, these descriptions should be helpful when the concepts re-appear later in the book. This chapter closes with a discussion of parallel corpora and sentence alignment. As these are central to the business of SMT, I feel they might have been better placed in a translation-focused chapter. Probability Theory covers the basic statistics needed to understand the ideas throughout the book. This chapter is clear, and provides strong intuitions on important issues such as conditional probability. There is a surprisingly large emphasis on binomial and normal distributions, considering SMT’s heavy reliance on categorical distributions; however, these are needed to discuss signiﬁcance testing and some language modeling techniques covered later. 2. Chapters 4–8: Core Methods The next ﬁve chapters provide detailed descriptions of each of the major components of a phrase-based SMT system. Word-based Methods discusses the ﬁve IBM translation models, with a brief detour to discuss the noisy channel model that motivates the IBM approach. This chapter is best taken as a complement to Brown et al. (1993) and Kevin Knight’s (1999) tutorial on the same subject, rather than a replacement. It provides strong intuitions on what each IBM model covers and how each model works, including the clearest descriptions I have seen of IBM:3–5. However, it does sometimes make them seem a little mysterious. For example, there is no attempt to explain why IBM:1 always arrives at a global maximum, or to generalize when one can apply the mathematical simpliﬁcation that reduces IBM:1’s exponential sum over products to a polynomial product over sums. One glaring omission from this chapter is a discussion of the alignment HMM (Vogel, Ney, and Tillmann 1996). This elegant model is widely used and widely extended, and I had expected to see it covered in detail. 
This introductory book is a systematic and up-to-date overview of fundamental and focused knowledge of Chinese NLP for both practitioners and a general audience. The Chinese language has the largest number of native speakers in the world, with over one billion people speaking some style of Chinese. The globalization and the development of the Internet have signiﬁcantly increased the participation of Chinese-speaking users in global business and social life, accounting for the highest growth rate in on-line population over the past decade. Despite its increasing importance, the uniqueness of Chinese renders the computer processing of the language distinctive and challenging. With a clear awareness of many differences between Chinese and other languages in morphology, syntax, and semantics, the authors focus the subject matter on morphological analysis, which means they choose to pay close attention to the essential processing techniques that lay down the foundation of any advanced Chinese NLP system. For many readers who are puzzled by “why on earth we have to bother with Chinese NLP given the availability of Englishlanguage processing technologies,” this book is suitable puzzle-solving material; for undergraduate and postgraduate students interested in the ﬁeld, knowledge regarding the fundamentals of computer processing of Chinese language can be acquired; for a veteran already who knows everything in Chinese NLP, this is a useful and lightweight reference book. Overall, this book has a large potential audience of readers who are interested in Chinese NLP at all levels. The book consists of eight chapters of main material, an appendix of linguistic resources, and a bibliography. Chapter 1 gives an introduction by raising the unique characteristics of Chinese. A couple of interesting examples are used to explain the problem of ambiguity caused by the lack of clear delimiters between words in Chinese text. Then it proceeds to illustrate how other types of difﬁculties are produced at morphological, syntactic, and semantic levels. From these discussions, Chapter 1 highlights and concludes that “The main difference in NLP between Chinese and other languages takes place in the ﬁrst stage. This lays down the objective of the book, namely, to introduce the basic techniques in Chinese morphological analysis.” Chapter 2 describes the basic forms of morphological construction in Chinese written script, providing the necessary preparation for morphological analysis in the later chapters. It introduces the concepts of characters, morphemes, and words, as well as the typical morphological processes of word formation. In particular, the authors  Computational Linguistics  Volume 36, Number 4  provide sufﬁcient details on the nature of the compounding that plays a predominant role in Chinese word formation and appears much more prevalent than that in English. A number of examples make the content easy to follow, especially for international learners who are not familiar with Chinese. Given these primary linguistic characteristics, Chapter 3 particularizes the challenges that arise from a wide spectrum of morphological problems of Chinese. Compared to an alphabetic system like English, the difﬁculties of Chinese morphological processing not only result from its representation and writing conventions, such as a large character set, multiple co-existing variants of character sets and encoding standards, dialectal variations, special genres of punctuation, and so on, but also from its underlying linguistic heritage and distinctions. For example, the lack of formal morphological markers may render part-of-speech tagging troublesome due to few part-of-speech clues and multiple parts of speech for the same word; the indeterminacy may be worsened by homophony and homography. The authors emphasize the inﬂuences of different kinds of ambiguities and out-of-vocabulary (OOV) words and suggest that external knowledge and contextual information are important to ambiguity resolution. Chapter 4 reviews Chinese word segmentation, a major challenge unique to Chinese and a few other Asian languages. A tree-based taxonomy is used to classify segmentation algorithms into character- and word-based approaches. Besides the rudimentary methods, emphasis is given to those methods using statistics and machine learning that take the advantage of large-scale annotated corpora to solve the segmentation ambiguity issue. Three main Chinese word segmentation standards are introduced, together with the performance measures and benchmark data widely used for evaluation. As one of the major challenges in Chinese word segmentation, unknown word identiﬁcation (UWI) is discussed separately in Chapter 5, probably because of its larger impact on accuracy and greater difﬁculty. Unknown words account for 60% of word segmentation errors. Proper nouns representing person, place, and organization names that are missing from the dictionary are common sources of unknown words. New names may appear on a daily basis, tending to intensify the OOV problem. This chapter describes various means for UWI from both generic and speciﬁc point of views. The general statistical methods based on co-occurrence statistics are depicted, followed by the techniques for identifying speciﬁc types of proper nouns based on their particular formation patterns. Two general methods worthy of review but missing are classbased language models (Fu and Luke 2004) and discriminative Markov models (Zhou 2005), both of which make signiﬁcant improvements by leveraging various types of features. Chapter 6 turns the reader’s attention from word structure to meaning; it familiarizes them with the gist of the basic semantic concepts, relations, and resources, and helps build up knowledge for resolving deeper NLP problems such as word sense disambiguation, parsing, and language understanding. Three major contemporary Chinese thesauri, namely CILIN, HowNet, and CCD, are surveyed in comparison with the wellknown English WordNet. For example, HowNet has three unique features compared to WordNet: (1) its concept deﬁnitions are based on sememes, which are speciﬁed in a structured mark-up language; (2) the semantic role relations can be connected across part-of-speech categories such as between nouns and verbs; and (3) the concepts are represented in both Chinese and English. Though structurally compatible, CCD differs from WordNet with some important extensions, such as more types of nouns, ﬁner relations, and examples of collocations selected from real corpora with quantitative 778 
This book gives a short but comprehensive overview of the ﬁeld of spoken dialogue systems, outlining the issues involved in building and evaluating this type of system and making liberal use of techniques and examples from a wide range of implemented systems. It provides an excellent review of the research, with particularly relevant discussions of error handling and system evaluation, and is suitable both as an introduction to this research area and as a survey of current state-of-the-art techniques. The book is structured into seven chapters. Chapter 1 provides an introduction to the research area and brieﬂy introduces the topics covered in the book. The end of the chapter consists of a list of links to tools and components that can be used for dialogue system development, which—although currently useful—seems likely to go out of date quickly. Chapter 2 addresses the task of dialogue management, beginning by describing simple graph- and frame-based methods for dialogue control, and continuing with a discussion of VoiceXML. The chapter ends with an extended discussion of recent work in statistical approaches to dialogue control and modeling. It is unfortunate that the discussion of other methods such as the information state approach and plan-based models is postponed to Chapter 4, but otherwise this chapter provides a good summary of both classic and recent approaches. Chapter 3 discusses error handling, which is divided into three processes: error detection, error prediction (i.e., the online prediction of errors based on dialogue features), and error recovery. After surveying a range of previous approaches to these three subtasks, the authors go on to discuss several more recent, data-driven approaches. Error handling is both a vital component of any spoken dialogue system designed for realworld use and an active area of current research, so this compact summary of techniques and issues is welcome. Chapter 4 contains case studies illustrating a range of dialogue control strategies and models. It begins with a description of the information state approach, and then moves on to discuss plan-based approaches as exempliﬁed in the TRAINS and TRIPS projects. This is followed by a discussion of two systems that employ software agents for dialogue management: the Queen’s Communicator and the AthosMail system. Finally, two systems which make use of statistical models are presented: the Microsoft Bayesian receptionist, which models conversation as decision making under uncertainty, and the DIHANA system, which employs corpus-based dialogue management. The case studies in this chapter provide detailed examples of a range of techniques, along with  Computational Linguistics  Volume 36, Number 4  some discussion of the advantages and disadvantages of each approach, adding to the relevance of the book for developers of future dialogue systems. Chapter 5 discusses four aspects of spoken dialogue systems that go beyond the straightforward information exchange scenarios considered in the preceding chapters. The ﬁrst section covers aspects of collaborative planning along with Jokinen’s (2009) Constructive Dialogue Modeling approach. The section on adaptation and user modeling provides a brief but useful survey of approaches to this task. The discussion of multimodality is longer and more concrete, but concentrates almost entirely on multimodal input processing; it would have been helpful to include a similar summary of the issues involved in multimodal output generation. The ﬁnal section on “natural interaction” brieﬂy discusses two topics: non-verbal communication for embodied conversational agents and multimodal corpus annotation. Chapter 6—the longest in the book—gives a thorough treatment of the issues involved in evaluating spoken dialogue systems, beginning with an historical overview. It continues with a discussion of terminology and techniques and describes a wide range of subjective and objective evaluation measures that have been applied to the evaluation task. Next, two frameworks are presented that are designed to provide a general methodology for evaluation: PARADISE (Walker et al. 1997) and Quality of Experience (Mo¨ ller 2005). This is followed by a discussion of concepts from HCI-style usability evaluations and how they can be applied to spoken dialogue systems, and then a section dealing with semiautomatic evaluation and the role of standardization. The ﬁnal section discusses challenges that arise when evaluating advanced dialogue systems incorporating multimodality and adaptivity, and when evaluating systems designed for real-world applications. Finally, Chapter 7 brieﬂy discusses conversational dialogue systems (i.e., companion/chatbot systems), whose emphasis is on social communication rather than the information exchange tasks considered for most of the book. It also addresses the relationship between commercial and academic approaches to spoken dialogue. It is interesting to note that both of the authors have also written books of their own that address aspects of spoken dialogue systems: McTear (2004) gives a comprehensive overview of the research area, including a series of hands-on tutorials on building systems using tools such as the CSLU toolkit and VoiceXML, and Jokinen (2009) provides a detailed description of a particular style of dialogue management, Constructive Dialogue Modeling. 
The subtitle of Vladimir Pericliev’s book, An Introduction and Some Examples, is a succinct and accurate description of its contents. Pericliev argues brieﬂy for the usefulness of computer-aided techniques in linguistic discovery, contrasting it with the intuitionist approach which has characterized linguistic discovery throughout much of its history. The bulk of the book is devoted to examples of software-aided linguistic discovery drawn from his own work. Chapter 1 starts by sketching out the current state of discovery techniques in linguistic theory, categorizing scientiﬁc discovery into three main approaches: the intuitionist approach, the chance approach, and the problem-solving approach. Discoveries by intuition and by chance remain the purview of humans, but clearly the problemsolving approach can beneﬁt from the application of computational techniques. Chapter 2 presents the KINSHIP program, which performs “parsimonious discrimination” in order to determine the minimal set of features which are necessary to discriminate all of a language’s kinship terms. The program is used to discover feature geometries, superior to existing human-discovered ones, which describe the kinship terminology of languages like English and Bulgarian. Chapter 3 extends the ideas used in KINSHIP to a program called MPD (maximal parsimonious discrimination), which is then applied to a variety of other tasks, some of which are unconnected to linguistics. Of these applications, the most interesting is the use of MPD to determine the segment proﬁles which uniquely identify languages in the UPSID-451 database (consisting of segment inventories from 451 languages, selected to provide broad coverage of the world’s language families) (Maddieson and Precoda 1991). Although Pericliev discusses his results at considerable length, it is not clear what the theoretical usefulness of these proﬁles might be. What does it really tell us about French to know that it is the only language in the database to contain the phoneme [œ˜ ]? Of more practical interest was Pericliev’s discussion of the process of converting the UPSID data into a featural representation to make it amenable to processing, describing how to represent underspeciﬁed segments and how to deal with transcription variations. This sort of necessary preprocessing constitutes an important and underemphasized part of the process of machine-aided linguistic discovery. The study of UPSID does produce some interesting, though not unexpected, results. For instance, when a proﬁle contains more than one unique segment, the majority of these segments share a common feature, and 85.8% of the unique segments have some sort of secondary articulation. Chapters 4 and 5 present two more pieces of software developed by Pericliev: UNIV and AUTO. The UNIV software is inspired by Greenberg’s universals (Greenberg 1966),  Computational Linguistics  Volume 36, Number 4  and automates the haphazard process by which universals have been identiﬁed in the past. Given a vector of features for each language being studied, UNIV identiﬁes all universal patterns which hold above a user-speciﬁed threshold. Such universals can be unrestricted or statistical and they can be stand-alone or implicational. Once a set of universals has been identiﬁed, the results are fed through AUTO (for "AUthoring TOol"), which assembles boilerplate text into a journal article; given the vast number of (often trivial) universals which UNIV discovers, this can be useful. UNIV is ﬁrst applied to two data sets: one of kinship terms, and the other the word-order data used by Greenberg himself. The most interesting result is that Greenberg’s set of word-order universals was neither complete nor fully supported by the data. UNIV is then applied to the UPSID-451 database. UNIV identiﬁes a large number of previously unnoted universals, most of which are rather low-level and of little inherent theoretical interest. However, the low-level machine-discovered generalizations can then be used as the basis for more interesting manually created generalizations. The UNIV analysis also serves to reﬁne earlier claims made by Maddieson (1984) and Gamkrelidze (1978). Chapter 6 is devoted to MINTYP, which is a program for determining the minimum typology to account for an observed set of universals. The search for such typologies is discussed by Greenberg (1966) and by Hawkins (1983). MINTYP takes a system of universals and a set of logically admissible types, and eliminates any superﬂuous universals which can be implied by stronger universals in order to determine the smallest set of universals which still accounts for the observed data. This approach is able to distill Greenberg’s set of universals into as few as four composite universals. Like UNIV and KINSHIP, MINTYP follows Pericliev’s basic approach: Reduce the data to a set of features, and then ﬁnd the patterns which most economically cover the observed feature distribution. Chapter 7 turns this featural approach to the problem of genetic language classiﬁcation with the RECLASS software. Pericliev extends the featural approach to include Swadesh-type word-lists, for which he describes a method for calculating a similarity metric based on phonological features of words in the list. A set of languages from different families is selected for study, a similarity metric is calculated for pairs of languages, and unrelated languages whose similarity is signiﬁcantly greater than expected are given further attention. In Pericliev’s test case, the initial feature data consists of kinship terminology, which revealed an unexpected similarity between the Kaingang languages of Brazil and various Polynesian languages. He pursues this similarity ﬁrst by using features based on word-list similarities and then by looking at other structural features, arguing at length for the plausibility of a genetic connection. Of all the results described in the book, this is probably the most interesting, because it represents a discovery made by Pericliev’s machine-aided approach which is unlikely ever to have been found by the haphazard manual process of discovery. The main weakness of the book is that all the software described in the book was developed or co-developed by Pericliev himself, so the various programs all risk seeming like variations on a single theme. The book would have beneﬁted by including examples of software from others working in the ﬁeld, which might differ from the feature-coverage approach favored by Pericliev. That being said, Pericliev’s essential point is a valid one: Machine-aided discovery has a tremendous untapped potential for analyzing data sets which are too large to be amenable to human inspection. The success of this approach is best exempliﬁed by his machine-aided discovery of a possible genetic relationship which would otherwise have eluded human discovery. 
University of Lancaster Pauline Ziman† PHS Consulting, Ltd. In his article “Ancient symbols and computational linguistics” (Sproat 2010), Professor Sproat raised two concerns over a method that we have proposed for analyzing small data sets of symbols using entropy (Lee, Jonathan, and Ziman 2010): ﬁrst, that the method is unable to detect random but non-equiprobable systems; and second, that it misclassiﬁes kudurru texts. We address these concerns in the following response. 1. Random Systems Random systems can contain unigrams drawn from an equiprobable or from a nonequiprobable distribution. For small data sets, random but equiprobable systems are likely to have a non-equiprobable actual frequency of unigram occurrence due to the sample size. A method for determining whether a data set is unlikely to be random but equiprobable was given in Lee, Jonathan, and Ziman (2010). For a given script set, ﬁrst order entropy (E1) summarizes the frequencies at which unigrams occur. E1 is maximized when all unigrams occur with equal probability. In written language, unigrams occur with unequal probabilities—for example, the letters e and t occur more frequently in English than the letters x and z, thereby lending some degree of predictability to the occurrence of a particular unigram, and reducing the value of E1. Random script sets drawn from a non-equiprobable distribution could have the same actual frequencies of unigram occurrence as a written language script set. However, whereas there is unigram-to-unigram dependence in a language, there is no such dependency in a random system. For example, q tends to be followed by u in English. The digram qu would therefore occur more often than other digrams starting with q. This second-order dependency is captured in the second-order entropy, E2. Thus it is one of the fundamental outcomes of Shannon’s theory that the dependency in ∗ School of Biosciences, Geoffrey Pope Building, University of Exeter, Stocker Road, Exeter EX4 4QD, UK. E-mail: R.Lee@exeter.ac.uk. ∗∗ Department of Mathematics and Statistics, University of Lancaster, Lancaster LA1 4YF, UK. E-mail: p.jonathan@lancaster.ac.uk. † PHS Consulting Limited, Pryors Hayes Farm, Willington Road, Oscroft, Tarvin, Chester CH3 8NL, UK. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 4 
Tara Institute of Fundamental Research Mayank N. Vahia∗∗ Tara Institute of Fundamental Research Hrishikesh Joglekar† Ronojoy Adhikari‡ The Institute of Mathematical Sciences Iravatham Mahadevan§ Indus Research Centre 1. Introduction In a recent Last Words column (Sproat 2010), Richard Sproat laments the reviewing practices of “general science journals” after dismissing our work and that of Lee, Jonathan, and Ziman (2010) as “useless” and “trivially and demonstrably wrong.” Although we expect such categorical statements to have already raised some red ﬂags in the minds of readers, we take this opportunity to present a more accurate description of our work, point out the straw man argument used in Sproat (2010), and provide a more complete characterization of the Indus script debate. A separate response by Lee and colleagues in this issue provides clariﬁcation of issues not covered here. 2. The Indus Script Debate The Indus script refers to the approximately 4,000 inscriptions on seals, miniature tablets, pottery, stoneware, copper plates, tools, weapons, and wood left behind by ∗ Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195, USA. E-mail: rao@cs.washington.edu. ∗∗ Department of Astronomy and Astrophysics, Tara Institute of Fundamental Research, Mumbai 400005, India. † 14 Dhus Wadi, Thakurdwar, Mumbai 400002, India. ‡ The Institute of Mathematical Sciences, Chennai 600113, India. § Indus Research Centre, Roja Muthiah Research Library, Chennai 600113, India. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 4  the Indus civilization, which ﬂourished ca. 2600–1900 BCE in South Asia. The existing inscriptions (see Figure 1(a) for examples) are relatively short, the average length being 5 signs and the longest inscription on a single surface being 17 signs. The number of different symbols in the script is estimated to be about 400. This large number of symbols, coupled with archaeological evidence indicating extensive use of the script for a variety of purposes, led scholars to suggest that the script was probably a logosyllabic form of writing, each sign representing a word or syllable (Parpola 1994; Possehl 1996). In 2004, Sproat and colleagues published in the Electronic Journal of Vedic Studies an article whose title makes the unconditional pronouncement “The Collapse of the Indus script thesis: The myth of a literate Harappan civilization” (Farmer, Sproat, and Witzel 2004). The article goes on to list arguments for why the authors believe the Indus script is nonlinguistic (the arguments are said to amount to a “proof” [Farmer 2005]). They propose that the script is a collection of religious or political symbols. Sproat (2010) states that their arguments “have been accepted by many archaeologists and linguists,” without actually citing who these “many archaeologists and linguists” are. In fact, a number of respected scholars, not just those who have “spent most of their careers trying to decipher the symbols” (Sproat 2010), have voiced strong disagreement (Kenoyer 2004; Possehl 2004; Mahadevan 2009). Several have published point-by-point rebuttals (Parpola 2005; Vidale 2007; McIntosh 2008). Parpola, who is widely regarded as the leading authority on the Indus script, writes that the arguments of Sproat and co-workers “can be easily controverted” and goes on to expose the inadequacies of each of these arguments (Parpola 2005, 2008). McIntosh, in a recent book on the ancient Indus valley, also discusses the evidence against the arguments of Sproat and colleagues (McIntosh 2008, pages 372–374). Vidale, a well-known archaeologist, notes that the paper (Farmer, Sproat, and Witzel 2004) “is constructed by repeatedly advancing hypotheses and sometimes wild speculation presented as serious scientiﬁc evidence” and concludes by saying: “I see no acceptable scientiﬁc demonstration of the non-scriptural nature of the Indus sign system; therefore, I see no collapse of such ‘thesis’” (Vidale 2007, page 362). 3. Fallacies Resolved Under a section entitled “The Fallacies,” Sproat (2010) describes a result from our article in Science (Rao et al. 2009a) which presents evidence against the thesis of Farmer, Sproat, and Witzel (2004). In our article, we show that the conditional entropy of the Indus script is similar to various linguistic sequences. The impression conveyed by Sproat (2010) is that we are claiming such similarity by itself is sufﬁcient to prove that the Indus script, or indeed any symbol system, is linguistic. We do not make such a claim; instead, we only note in Rao et al. (2009a) that our result increases the evidence for the linguistic hypothesis, when one takes into account other language-like properties of the script (see detailed explanation in Section 4 herein). To set up his criticism of our work, Sproat (2010) presents Figure 1A from our Science paper but never mentions the results presented in Figure 1B in the same paper. Nor does he describe our more recent block entropy result (Rao 2010b), even though he cites this paper (this new result extends the conditional entropy work). Both of these results include data from demonstrably nonlinguistic sequences, namely, DNA, protein sequences, and Fortran code. To present our work as “simple experiments involving randomly generated texts” is, to say the least, a gross misrepresentation of our work. 796  Rao et al.  Entropy, the Indus Script, and Language  Figure 1 (a) Examples of the Indus script. Three square stamp seals, each with an Indus text at the top. Last image: three rectangular seals and three miniature tablets with inscriptions (image credit: J. M. Kenoyer/Harappa.com). (b) Block entropy scaling of the Indus script compared to natural languages and other sequences. Symbols were signs for the Indus script; bases for DNA; amino acids for proteins; change in pitch for music; characters for English; words for English, Tagalog, and Fortran; symbols in abugida (alphasyllabic) scripts for Tamil and Sanskrit; and symbols in the cuneiform script for Sumerian (see Rao et al. 2009a; Rao 2010a for details). The values for music are from Schmitt and Herzel (1997). To compare sequences over different alphabet sizes L, the logarithm in the entropy calculation was taken to base L (417 for Indus, 4 for DNA, etc.). The resulting normalized block entropy is plotted as a function of block size. Error bars denote one standard deviation above/below mean entropy and are negligibly small except for block size 6. (c) Log likelihood under a ﬁrst-order Markov model for the Indus corpus for four texts (A through D) found in foreign lands compared to average log likelihood for a random set of 50 Indus region texts not included in the training data (error bar denotes ±1 standard error of mean). The unusual sequencing of signs in the foreign texts, noted earlier by Parpola (1994), is reﬂected here in their signiﬁcantly lower likelihood values. 797  Computational Linguistics  Volume 36, Number 4  To correct this misrepresentation, we present in Figure 1(b) the block entropy result (adapted from Rao 2010b). Block entropy generalizes Shannon entropy (Shannon 1948, 1951) and the measure of bigram conditional entropy used in Rao et al. (2009a) to blocks of N symbols. Block entropy for block size N is deﬁned as:  HN = − pi(N) log pi(N)  (1)  i  where pi(N) are the probabilities of sequences (blocks) of N symbols. Thus, for N = 1, block entropy is simply the standard unigram entropy and for N = 2, it is the entropy of bigrams. Block entropy is useful because it provides a measure of the amount of ﬂexibility allowed by the syntactic rules generating the analyzed sequences (Schmitt and Herzel 1997): The more restrictive the rules, the smaller the number of syntactically correct combinations of symbols and lower the entropy. Correlations between symbols are reﬂected in a sub-linear growth of HN with N (e.g., H2 < 2H1). Figure 1(b) plots the block entropies of various types of symbol sequences as the block size is increased from N = 1 to N = 6 symbols. To counter the problems posed by the small sample size of the Indus corpus (about 1,550 lines of text and 7,000 sign occurrences), we employed a Bayesian entropy estimation technique known as the NSB estimator (Nemenman, Shafee, and Bialek 2002), which has been shown to provide good estimates of entropy for undersampled discrete data. Details regarding the NSB parameter settings and the data sets used for Figure 1(b) can be found in Rao (2010a). As seen in Figure 1(b), the block entropies of the Indus texts remain close to those of a variety of natural languages and far from the entropies for unordered and rigidly ordered sequences (Max Ent and Min Ent, respectively). Also shown in the plot for comparison are the entropies for a computer program written in the formal language Fortran, a music sequence (Beethoven’s Sonata no. 32; data from Schmitt and Herzel [1997]), and two sample biological sequences (DNA and proteins). The biological sequences and music have noticeably higher block entropies than the Indus script and natural languages; the Fortran code has lower block entropies. Does the similarity in block entropies with linguistic systems in Figure 1(b) prove that the Indus script is linguistic? We do not believe so. In fact, we contend that barring a full decipherment, one cannot prove either the linguistic or nonlinguistic thesis, unlike Sproat and colleagues who have previously claimed to have “proof” for the nonlinguistic hypothesis (Farmer, Sproat, and Witzel 2004, pages 34 and 37; Farmer 2005). What we do claim, as we state in our Science paper and as explained in more detail subsequently, is that results such as the similarity in entropy in Figure 1(b) increase the evidence for the linguistic hypothesis, given other language-like properties of the Indus script. However, Sproat, Liberman, and Shalizi (in a blog by Liberman [2009]) and Sproat at EMNLP’09 undertake the exercise of knocking down the straw man (“similarity in conditional entropy by itself implies language”) and present artiﬁcial counterexamples (e.g., having Zipﬁan distribution) with conditional independence for bigrams (Sproat 2010). First, such an exercise misses the point: as stated earlier, we do not claim that entropic similarity by itself is a sufﬁcient condition for language. Second, these “counterexamples” ignore the fact that the unigram and bigram entropies are markedly different for both the Indus script and the linguistic systems, as is obvious from comparing Figures 1 and S1 in our Science paper (Rao et al. 2009a). More importantly, these artiﬁcial examples fail to exhibit the scaling of block entropies beyond unigrams and bigrams exhibited by the Indus script and linguistic systems in Figure 1(b).  798  Rao et al.  Entropy, the Indus Script, and Language  Sproat (2010) criticizes our classiﬁcation of “Type 1” and “Type 2” nonlinguistic systems (corresponding to systems near Max Ent and Min Ent, respectively, in Figure 1(b)), saying these do not characterize any natural nonlinguistic systems. It is clear from Figure 1(b) that there do exist natural “Type 1” nonlinguistic sequences (DNA, protein sequences). The analogous result for conditional entropy was given in Figure 1B in Rao (2010b), which was omitted in Sproat (2010). As for “Type 2” systems, Vidale (2007) provides a number of examples of ancient nonlinguistic systems from Central and South Asia whose properties are in line with such systems. Section 6 herein discusses these systems as well as the speciﬁc cases of Vincˇa and kudurru sequences mentioned in Sproat (2010). Sproat, Farmer, and colleagues have objected to the use of artiﬁcial data sets in Rao et al. (2009a) to demarcate the Max Ent and Min Ent limits: This objection is a red herring and does not change the result that the Indus script is entropically similar to linguistic scripts. Finally, the allusion in Sproat (2010) that we may be “confused” about the difference between “random” and “equiprobable” is unwarranted and not worthy of comment here. The related issue of artiﬁcially generated examples with quasi-Zipﬁan distributions has already been discussed in this response. We conclude by noting here that the extension of our original conditional entropy result to block entropies directly addresses the objections of Pereira (2009), who stressed the need to go beyond bigram statistics, which Figure 1(b) does for N up to 6. Beyond N = 6, the entropy estimates become less reliable due to the small sample size of the Indus corpus.  4. Inductive Inference The correct way of interpreting the block entropy result in Figure 1(b) (and likewise the conditional entropy result) is to view it within an inductive framework (rather than in a deductive sense as Sproat and others do in Liberman [2009]). Given that we cannot answer the ontological question “Does the Indus script represent language?” without a true decipherment, we formulate the question as an epistemological problem, namely, one of estimating the posterior probability of the hypothesis HL that an unknown symbol sequence represents language, given various properties P1, P2, P3, . . . of the unknown sequence. Using a Bayesian formalism, the posterior P(HL|P1, P2, P3, . . .) is proportional to P(P1, P2, P3, . . . |HL)P(HL). Building on prior work (Hunter 1934; Knorozov, Volchok, and Gurov 1968; Mahadevan 1977; Parpola 1994), we have sought to quantitatively characterize various properties P1, P2, P3, . . . of the Indus script (Yadav et al. 2008a, 2008b, 2010; Rao et al. 2009a, 2009b; Rao 2010b). In each case, we compare these properties with those of linguistic systems to ascertain whether the property tilts the evidence towards or away from the linguistic hypothesis HL. We ﬁnd these properties to be as follows: (1) Linearity: The Indus texts are linearly written, like the vast majority of linguistic scripts (and unlike nonlinguistic systems such as medieval heraldry, Boy Scout merit badges, or highway/airport signs, systems frequently mentioned by Sproat and colleagues). (2) Directionality: There is clear evidence for directionality in the script: Texts were usually written from right to left, a fact that can be inferred, for example, from a sign being overwritten by another on its left on pottery (Lal 1966). Directionality is a universal characteristic of linguistic systems but not necessarily of nonlinguistic systems (e.g., heraldry, Boy Scout badges). (3) Use of Diacritical Marks: Indus symbols are often modiﬁed by the addition of speciﬁc sets of marks over, around, or inside a symbol. Multiple symbols are sometimes combined  799  Computational Linguistics  Volume 36, Number 4  (“ligatured”) to form a single glyph. This is similar to linguistic scripts, including later Indian scripts which use such ligatures and diacritical marks above, below, or around a symbol to modify the sound of a root consonant or vowel symbol. (4) Zipf–Mandelbrot Law: The script obeys the Zipf–Mandelbrot law, a power-law distribution on ranked data, which is often considered a necessary (though not sufﬁcient) condition for language (Yadav et al. 2010). (5) Syntactic Structure: The script exhibits distinct language-like syntactic structure including equivalence classes of symbols with respect to positional preference, classes of symbols that function as beginners and enders, symbol clusters that prefer particular positions within texts, etc. (Hunter 1934; Parpola 1994; Yadav et al. 2008a, 2008b). This structure is evident in both short texts as well as longer texts that are up to 17 symbols long. (6) Diverse usage: The script was used on a wide range of media (from seals, tablets, and pottery to copper plates, tools, clay tags, and at least one large wooden board), suggesting a diverse usage similar to linguistic scripts, and unlike nonlinguistic systems such as pottery markings, deity symbols on boundary stones, and so on, whose use is typically limited to one type of medium. (7) Use in Foreign Lands: Indus texts have been discovered as far west as Mesopotamia and the Persian Gulf. These texts typically use the same signs as texts found in the Indus region but alter their ordering. As shown in Figure 1(c), these “foreign” texts have low likelihood values compared to Indus region texts, even after taking into account regional variation across the Indus region (see error bar in Figure 1(c)) (Rao et al. 2009b; Rao 2010b). This suggests that, like other linguistic scripts, the Indus script may have been versatile enough to represent different subject matter or a different language in foreign regions. Note that although one may ﬁnd a nonlinguistic system that exhibits one of these properties (e.g., Zipﬁan distribution) and another that exhibits a different property (e.g., ligaturing), it would be highly unusual for a nonlinguistic system to exhibit a conﬂuence of all of these properties. To these properties, we add the property in Figure 1(b) that the Indus script shows the same type of entropic scaling as linguistic systems. To estimate the prior probability P(HL), one could take into account, as a number of scholars have (Vidale 2007; Parpola 2008; Mahadevan 2009), the archaeological evidence regarding the cultural sophistication of the Indus civilization, contact with other literate societies, and the extensive use of the script for trade and other purposes. These factors suggest that P(HL) is higher than chance. Considering the properties discussed previously and our estimate of P(HL), the product P(P1, P2, P3, . . . |HL)P(HL) suggests a higher posterior probability for the linguistic hypothesis than the nonlinguistic alternative. Given our current data and knowledge about the script, we believe this is the kind of statement one can make about the Indus script, rather than statements about the “collapse” of one thesis or another (Farmer, Sproat, and Witzel 2004). To claim to have “proof” of the nonlinguistic thesis (Farmer, Sproat, and Witzel 2004, pages 34 and 37; Farmer 2005) would amount to showing a posterior probability of zero for the linguistic hypothesis. This is clearly not possible given our current state of knowledge about the script and the lack of an accepted decipherment. Could the result in Figure 1(b) be an artifact of our particular entropy estimation method? We do not think so. A similar block entropy result was obtained independently by Schmitt and Herzel (1997) using an entirely different entropy estimation method (see Figure 8 in their paper). The overall result is also conﬁrmed by other methods, as discussed by Schmitt and Herzel: “This order—DNA, music, human language, computer language—when ordered by decreasing entropy, is conﬁrmed by the calculation of the Lempel-Ziv complexity (Lempel and Ziv 1976) which also serves as an estimation of the entropy of the source” (Schmitt and Herzel 1997, page 376).  800  Rao et al.  Entropy, the Indus Script, and Language  5. Comparison with Ancient Nonlinguistic Systems Sproat contends that results such as the similarity in entropy scaling in Figure 1(b) are “useless” without analyzing a sizeable number of “ancient nonlinguistic systems” (Sproat 2010). As mentioned earlier, Sproat ignores the fact that the results already include nonlinguistic systems: DNA and protein sequences (perhaps the two “most ancient” nonlinguistic systems!) as well as man-made sequences (Fortran code and music in Figure 1(b)). We believe entropic results such as Figure 1(b) to be both interesting and useful. An analogy may be apt here: If, in the dim surroundings of a jungle, you notice something moving and then spot some stripes, your belief that what is lurking is a tiger will likely go up, even though it could also be a zebra, a man wearing a tiger costume, or any of a number of possibilities. The observation you made that the object under consideration has stripes is certainly not “useless” in this case, just because you haven’t ascertained whether antelopes or elephants in the jungle also have stripes. In other words, we now know that various types of symbol sequences, from natural sequences such as DNA and proteins to man-made systems such as music and Fortran, occupy quite different entropic ranges compared to linguistic systems (Figure 1(b); Figure 8 in Schmitt and Herzel [1997]). Given this knowledge, the ﬁnding that Indus sequences occupy the same entropic range as linguistic sequences, although not proving that the Indus script is linguistic, certainly increases the posterior probability of the linguistic hypothesis, just as the observation of stripes increases the posterior probability of the “tiger” hypothesis in our earlier example.1 As to where ancient nonlinguistic systems may lie among the entropic ranges in Figure 1(b), we discuss this in the next section. 6. Countless Nonlinguistic Sign Systems? Sproat and colleagues have stated that the properties observed in the Indus script are also seen in “countless nonlinguistic sign systems” (Farmer, Sproat, and Witzel 2004, page 21). Let us consider some of these nonlinguistic systems (Sproat 2010; Farmer, Sproat, and Witzel 2004). Medieval European heraldry, Boy Scout merit badges, and airport/highway signs are not linear juxtapositions of symbols that can be up to 17 symbols long, as we ﬁnd in the case of the Indus script, nor do they exhibit a conﬂuence of script-like properties as enumerated herein. We invite the reader to compare examples of heraldry (Parker 1894), Boy Scout badges (Boy Scouts of America 2010), and airport/highway signs with the Indus script sequences in Figure 1(a) and judge for themselves whether such a comparison bears merit. Another nonlinguistic system mentioned in Sproat (2010) is the Vincˇa sign system, which refers to the markings on pottery and other artifacts from the Vincˇa culture of southeastern Europe of ca. 6000–4000 BCE. Sproat believes there is order in the Vincˇa system and states that we “mis-cite” Winn. To set the record straight, here is what Winn has to say in his article in a section on Sign Groups (Winn 1990, page 269): Neither the order nor the direction of the signs in these (sign) groups is generally determinable: judging by the frequent lack of arrangement, precision in the order probably was unimportant . . . Miniature vessels also possess sign-like clusters (Figure 12.2j), which are characteristically disarranged. 
In the last issue of this journal, I presented a piece that called into question some of the techniques reported in two papers in high-proﬁle journals that purported to provide statistical evidence for the linguistic status of some ancient symbol systems (Sproat 2010a). Not surprisingly, the authors of those two papers took issue with a number of my claims, and have requested the opportunity to respond. The two responses, taken together, are rather lengthy and as a result it is not possible, given limitations of space, for me to address each and every one of their criticisms. I will therefore focus on what I consider to be the most important objections. 2. Rao et al.’s Response The essential claim of Rao and his colleagues (henceforth “Rao”) is that I have misrepresented the claims of Rao et al. (2009), and painted an incomplete picture of their work. They also take the opportunity to discuss at some length a wide range of evidence that they feel supports the Indus script hypothesis. I want to make two things clear at the outset. First, my critique was about the paper that appeared in Science in 2009. Although I mentioned some of Rao’s later work, it was not a critique of that later work. It seemed reasonable to assume that the paper in Science was intended to be taken seriously and to stand on its own merits. If so, then I felt— and still feel—that the paper had serious problems. A problematic paper that cannot stand on its own does not become unproblematic even if subsequent research were to show the conclusions to be correct. To give a stark example, if someone should eventually demonstrate rigorously that cottontop tamarins are capable of learning “regular” grammars, that would have no bearing on the questions currently surrounding Marc Hauser’s 2002 publication in Cognition (Johnson 2010). Second, it was not my purpose in my piece to use the pages of Computational Linguistics as a forum for promoting the non-script theory for the Indus. Indeed I spent exactly one short paragraph on this, where I discussed our work in Farmer, Sproat, and Witzel (2004) as a way of setting the stage for the current discussion. In contrast, Rao spends a signiﬁcant portion of his response discussing the Indus symbols, the case for the script theory, and the case against the non-script theory, and repeatedly refers to our 2004 paper and claimed problems with our arguments. The desire to keep my discussion ∗ Center for Spoken Language Understanding, Oregon Health & Science University, 20000 NW Walker Rd, Beaverton, OR, 97006, USA. E-mail: rws@xoba.com. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 4  of the background short was what prompted me not to expand on my claim that our arguments had “been accepted by many archaeologists and linguists.” Rao notes that I do not actually cite “who these ’many archaeologists and linguists’ are.” Perhaps they do not exist? But they do: Andrew Lawler, a science reporter who in 2004 interviewed a large number of people on both sides of the debate notes that “many others are convinced that Farmer, Witzel, and Sproat have found a way to move away from sterile discussions of decipherment, and they ﬁnd few ﬂaws in their arguments” (Lawler 2004, page 2029), and quotes the Sanskrit scholar George Thompson and University of Pennsylvania Professor Emeritus of Indian studies Frank Southworth. 2.1 Misunderstandings about Nonlinguistic Symbols A large part of the discussion in Rao’s response centers, as it should, on the question of the “Type 1” (random) and “Type 2” (rigidly ordered) models, which I argued did not accurately characterize any nonlinguistic symbol systems. At the core of this debate is the Old European sign system of the Vincˇa, which is claimed to be a good instance of Type 1, and the Mesopotamian deity symbols found on kudurru stones, which are claimed to be good instances of Type 2. To support the claims for Vincˇa, Rao provides a quote from Winn (1990), which notes that the signs do not seem to have any ordering, and are “characteristically disarranged.” But if one sees the broader context of this quote, it becomes clear that Winn is talking here about a subset of the corpus, which is found on small vessels and spindle whorls. He contrasts the inscriptions discussed in the quote given by Rao with “the more arranged format of tablets and seals” (page 270), and then again (page 276) where he notes that “[d]ifferences in complexity of sign usage is denoted by sign ordering, which occurs on tablets and other objects, such as the plaque from Gradesˇnica.” On page 263 he refers to the “discovery of tablets with script-like content at Tartaria in 1961,” a clear indicator that the Vincˇa tablets would seem to involve some form of sign ordering. It is clear then, that Winn is not saying that the system is in general unordered, only that a subset of the corpus is. Winn also suggests that the small vessels in particular probably had a ritualistic function (page 276). Furthermore, the text that was ellipted in Rao’s quote suggests that order was probably unimportant on the spindle whorls “since concepts or mnemonic aids could be inferred or interpreted by an individual cognizant of the culture-speciﬁc content of the signs” (page 269), suggesting a situation where the Vincˇa “reader” could interpret the jumbled signs since they knew the structure of the system. It is unfortunate that most of Winn’s interesting work has been published in obscure locations, and is therefore not something the average reader would run across. Fortunately though, Winn maintains a Web page at http://www.prehistory.it/ftp/winn.htm, where one can see what the sign texts look like and where Winn also notes (page 8) that “the distinction between signs on pottery and the more organized signs on tablets and other artifacts may signify functional differences or different levels of usage and formality.” Winn’s description does not support the claim that Vincˇa sign texts were generally without structure. But suppose, counterfactually, it did. Would Rao et al.’s “Type 1” be a good model of the system? Their Type 1 data is modeled by a random process that generates texts “based on the assumption that each sign has an equal probability of following any other” (Rao et al. 2009, Supplement, page 2), or in other words a system that is random and equiprobable. But where does Winn say that the system is equiprobable? The most Rao’s excerpted quote could be taken to mean is that the system is random, but Winn certainly does not say that the signs occur with equal frequency. A much more 808  Sproat  Reply to Rao et al. and Lee et al.  plausible model of a random ancient sign system would be one where the symbols have a non-equiprobable distribution: as I argued in my article “symbols represent things, and the things they represent typically do not occur with equal probability.” Further, we know that many things, from nonlinguistic systems such as kudurrus (see the following) to populations of sets of species, have a Zipﬁan distribution, so a reasonable model of such a system might well be one in which the unigram probability follows a ZipfMandelbrot distribution. But to have presented Type 1 in this way would have been problematic: As I discussed in my piece, such random texts easily “fool” the bigram conditional entropy measure that Rao et al. proposed in their Science paper. Rao objects to the statement in my piece that at least some of the discussion surrounding their paper depends on the confusion between “random” and “random and equiprobable,” deeming that statement “unwarranted and not worthy of comment.” Yet their model of Vincˇa as a random equiprobable system depends precisely on this confusion. At the other extreme are the kudurru texts, which are claimed to be good examples of Type 2, a rigidly ordered system. In support, Rao quotes Slanski (2003), who says “to a certain extent, the divine symbols were deployed upon the Entitlement naruˆ s (kudurrus) according to the deities’ relative positions in the pantheon. The symbols for the higher gods of the pantheon . . . are generally found upon or toward the top and most prominent part of the monument.” Crucial phrases here are “to a certain extent” and “the symbols for the higher gods . . . are generally found.” No doubt these are partly true; indeed it is very often true that the “sun disk,” “crescent moon,” and “star” symbols occur at the beginnings of texts—though ordering within those three varies. But the kudurru texts are still far from rigid, as we showed in Farmer, Sproat, and Witzel (2004) and as my own sample from Seidl shows. Over and above this, the quoted excerpts would only seem to support a hierarchy, not a model “based on the assumption that each sign has a unique successor sign” or even one where based on “variations of this theme where each sign could be followed by, for example, 2 or 3 other signs” (Rao et al. 2009, Supplement, page 3). All the hierarchy implies is that a symbol Y that is lower on the hierarchy should come after a symbol X that is higher on the hierarchy. But for any given X there may be several symbols lower on the hierarchy, which means that X could be followed directly by any of these, assuming that no concepts from the intervening elements are “mentioned” in the “text.” It is puzzling to me that, given these considerations and my own description of my close examination of the kudurru corpus, Rao still suggests that “we expect the entropy of the kudurru sequences to be lower than linguistic systems and perhaps slightly above the minimum entropy (Min Ent) range in Figure 1(b).” Surely one would like to examine the data and see whether such a statement is likely to be supported. In any case there seems to be little point in arguing about this. I have the data for the kudurrus that I transcribed from Seidl and I will be happy to make them available to anyone who wants them. As the reader will have seen in Lee et al.’s reply, and as I discuss in Section 3, Lee et al. took a different attitude to the deity symbols than Rao has. Rather than argue from “ﬁrst principles” about what the corpus might look like, they actually went to the trouble of replicating my experiment and developed their own corpus from Seidl’s work (Rob Lee, personal communication): They concluded that by their measures kudurrus would also count as (logographic) writing (an issue I get back to subsequently)—suggesting a much less rigid ordering than Rao et al. seem to believe. Rao contrasts the hierarchy that is certainly partly true of kudurru texts with the situation in linguistic systems, which “have no such hierarchy imposed on characters or words.” Interestingly, although this is generally true, there are pockets of such hierarchies that do occur in language. A good example is from Egyptian (Allen 2000), 809  Computational Linguistics  Volume 36, Number 4  where there is a hierarchy that determines the order in which terms denoting human or divine entities are written. Thus terms for gods will occur in writing before terms for humans or other things, even if the divine terms would be spoken after the other terms. This occurs a lot in royal names, so that for example Tutankhamen, whose name means ‘living image of Amen’, is written in cartouches with the god Amen’s name ﬁrst. In my piece, I made the point that relevant nonlinguistic symbol systems abound, a point that Rao seems to be calling into question. As a minor point, they object to my use of Boy Scout merit badges and highway signs as relevant to the discussion, inviting the reader to examine these and see “whether such a comparison bears merit.” Unfortunately this is due to a misinterpretation of what I meant by these comparisons, and it is perfectly possible that I did not explain my point well enough. I was not discussing the forms of the individual signs, but rather the fact that for merit badges and highway signs the symbols can be combined into “texts” that are invariably linearly arranged. Boy Scout badges are worn on sashes, where they are neatly arranged in rows. Informational highway signs, such as indicators for gas stations, accommodations, and food, frequently occur together and when they do they are arranged linearly. Of course the individual signs are not linearly composed any more than the Indus symbols are linearly composed or, in general, the basic symbols in any script. A more serious objection surrounds my claim that to have done the job properly, Rao et al. (2009) would have to have compared a wider set of linguistic and nonlinguistic systems, but I also suggested this would depend upon having a good set of corpora for real nonlinguistic systems, something which I claimed nobody has done the legwork of compiling. Rao claims that this ignores the work of Vidale (2007), who included a description of ten ancient South Asian nonlinguistic systems in his rebuttal to Farmer, Sproat, and Witzel (2004). This is misleading at best. If the reader will care to reread what I said in my article, it is clear that what I meant was that electronic corpora do not exist on which one could carry out the kinds of statistical tests that Rao and colleagues wish to perform. I believe that remains true. Vidale does indeed discuss ten systems, discusses their distribution and use, and lists numbers of signs in each. But by “legwork,” I was not referring to discussion of such systems, but rather the development of corpora of such systems whereby one could actually perform serious statistical analyses and compare among a range of linguistic and nonlinguistic systems. As far as I am aware, there is no such set of corpora. As Rao highlights, Vidale notes the “systematic, large-scale redundancy” of symbol distributions of some of the systems he examines. Perhaps, but as we have seen in the discussion of Vincˇa symbols and kudurrus earlier, translating such broad statements into precise statistical characterizations of the kind necessary for Rao’s methods is tricky at best. Rao also makes a point of noting Vidale’s observation that the number of distinct signs in the systems he examines is 44, “a far cry from the 400 or so signs in the Indus script.” To that statistic one could add kudurru symbols, which number about 60 in Seidl’s (1989) catalog. In fact, Vidale challenges Farmer, Witzel and me to “identify another South Asian system datable to the 3rd millennium B.C. of nonlinguistic signs amounting to 400 basic signs or more” (page 344). Nonlinguistic systems with hundreds of symbols certainly exist: heraldry (where the sign set is open-ended; see any text on heraldry such as Slater [2002]) would be one such example, though of course that does not meet the time and space restrictions required by Vidale. But then, what is the basis of those restrictions? A symbol system has as many symbols as it needs. Chinese writing has thousands of characters because it has chosen to represent linguistic units at the level of the morpheme; the Greek alphabet is much smaller due to its decision to represent phonemic segments. If the Indus symbols are nonlinguistic, then the vocabulary 810  Sproat  Reply to Rao et al. and Lee et al.  size of over 400 symbols would merely reﬂect that it represented a rich underlying set of concepts. Proponents of the script thesis always like to discuss the advanced state of Indus civilization, arguing that such an advanced civilization could easily have invented writing, and would be hard pressed to do without it. The Indus certainly was one of the most advanced civilizations of the third millenium BCE: Would it therefore not be reasonable to suppose that they could have invented a rich nonlinguistic symbol system, even if their less advanced neighbors could not? I end the discussion of Type 1 and Type 2 systems by noting that Rao characterizes the objection to the use of artiﬁcial data sets as “a red herring” that “does not change the result that the Indus script is entropically similar to linguistic scripts.” But within the ﬁrst two paragraphs of the Science paper, Rao et al. state: “We compared the statistical structure of sequences of signs in the Indus script with those from a representative group of linguistic and nonlinguistic systems. Two major types of nonlinguistic systems are those that do not exhibit much sequential structure (type 1 systems) and those that follow rigid sequential order (type 2 systems).” These (artiﬁcial) systems are clearly central to their argument yet, as I have argued, they are poor models of any relevant nonlinguistic system. Objections hardly seem to constitute a red herring. Rao takes strong issue with the fact that whereas I discuss Figure 1A from their Science paper, nowhere do I discuss their Figure 1B, nor do I do more than mention their later work. On the latter point, let me reiterate: My article was about the paper that appeared in Science, not any later work. I was not, for example, making any claims about the block entropy calculations in Rao (2010): I simply have not analyzed those. But as I said, I assume that at the point the Science paper appeared, the methods proposed were intended to be taken seriously on their own, and the evidence presented was intended to be taken to support those methods. It is therefore perfectly reasonable for me to have failed to discuss later work. What about Figure 1B? It is true that I did not present this ﬁgure, which shows high entropy for DNA and proteins, includes Sanskrit among the linguistic examples, and shows that Fortran has a somewhat lower entropy than the natural language samples they include. My question about DNA and proteins would be: Why are these relevant? Although they are possibly “the two ‘most ancient’ nonlinguistic systems,” the topic of discussion surely is symbol systems that were products of the human mind. Fortran is of course a different matter, but here the question that has always puzzled me is why Fortran was not included in Figure 1A: Why did Rao and colleagues not show the entropy growth curve for Fortran in 1A, as Rao (2010) did and as they also show in their Figure 1(b) in their response? I suspect I know: Using their original bigram conditional entropy measure, Figure 1A, Fortran would have shown a growth curve that overlapped signiﬁcantly with the linguistic systems, compromising the visual impact of the plot. But I had no direct evidence for this, so rather than speculate in my piece (as I have here), I preferred to omit discussion. Unlike Rao, I did not view their Figure 1B as including crucial data that I was somehow hiding. While we are on this topic though, the block entropy in their new Figure 1(b) still leaves one wondering: Fortran is lower than their linguistic examples, but not much lower. The same is true of the Figure 1B in their Science paper. How would one expect the plot to look if they had included a larger range of languages and text types? I return to this point subsequently. In Figure 1(b) in their response, Rao includes music (a Beethoven piece), which, like DNA and protein, is very close to the maximum entropy curve. They later criticize me for ignoring “the fact that the results already include nonlinguistic systems: DNA and protein sequences . . . as well as man-made sequences (Fortran code and music in 811  Computational Linguistics  Volume 36, Number 4  Figure 1[b]).” DNA, protein, and Fortran we have just discussed. It seems odd to accuse me of ignoring the case of music, as that data did not appear at all in the Science paper; indeed it did not appear in Rao (2010), where he ﬁrst introduced the block entropy calculations. It would have required unusual prescience on my part to discuss data that, until now, have not appeared in print. Finally, while we are on the topic of types of symbol systems, I note that Rao makes a point of enumerating the list of properties that they deem relevant for considering the linguistic status of the Indus symbols. This is of course related to the issue of how to interpret the claims of their work, which we take up in Section 2.2. Rather than enumerate the properties they describe, I will simply present the script-like properties of another system, namely, the Mesopotamian deity symbols, a system we know was not linguistic. I also made some of these points, but more brieﬂy, in my piece: (1) Deity symbols are frequently linearly written and in the cases where linearity is less clear, the symbols are written around the top of a stone in an apparent concentric circle pattern (see examples in Seidl [1989]). One sees such non-linear arrangements with scripts too: The Etruscan Magliano disk, and many rune stones, have text wrapped around the border of the stone. (2) There is clear evidence for the directionality of deity symbols: To the extent that “more important” gods were depicted ﬁrst, these occur at the left/top of the text. (3) Deity symbols are often ligatured together: One symbol may be joined with another. (4) The deity symbols obey a Zipﬁan distribution. (5) Deity symbols clearly have language-like properties in that certain symbols display positional preference (symbols for the more important gods coming earlier in the text), and certain glyphs have an afﬁnity for each other—for example, some glyphs such as the “horned crown” seem to like to be joined together with the “symbol base.” (6) Deity symbols were used largely on standing stones, but were also used in other contexts such as the “necklace” depicted in the Asˇsˇurnas.irpal II bas relief shown in my piece. (7) Deity symbols are pictographic, like many real scripts—Egyptian, Luwian, Mayan. This would seem like a fairly compelling list of script-like properties: Contrary to what Rao suggests, it is certainly possible to ﬁnd very script-like nonlinguistic symbol systems.  2.2 Misunderstandings about Claims Rao, in the response to my article, as well as elsewhere, makes the point that nowhere do they claim that the conditional entropy measure(s) are sufﬁcient by themselves to demonstrate that the Indus symbols were linguistic. Indeed they do not. The last sentence of the Science article reads: “Given the prior evidence for syntactic structure in the Indus script, our results increase the probability that the script represents language, complementing other arguments that have been made explicitly or implicitly in favor of the linguistic hypothesis.” It is worth stressing at the outset that in my piece I do not claim that Rao and colleagues make the claim that conditional entropy settles the matter—though it is surely true that this was the way many people interpreted their results. The question is, how is one supposed to interpret their claim that their results “increase the probability that the script represents language”? I can think of three possible interpretations. One is as a loose general statement to the effect that there is already lots of evidence for the script thesis, and this additional piece of evidence adds to that body. The next two interpretations are more formal. A second interpretation is that they are assuming some sort of “deductive” model whereby for a set of features Fi, one is computing the probability of the linguistic hypothesis (HL), versus the nonlinguistic 812  Sproat  Reply to Rao et al. and Lee et al.  hypothesis (HNL), given these features. Implicitly then, we are combining the features into a classiﬁer that computes P(HL|Fi) versus P(HNL|Fi) for each feature, and produces an overall prediction. It is worth noting that Lee et al.’s paper does indeed use the statistical measures Lee and colleagues develop in this deductive sense. The third interpretation is the “inductive” one that Rao presents in his response, which he claims is the “correct” way to interpret their work. In that interpretation, a set of feature values are observed, and the task is to induce which of two underlying models—HL or HNL—is the one likely to have generated the observation. The reader will surely recognize that the second and third interpretations correspond pretty directly to discriminative versus generative models. Both of these approaches are used in a variety of classiﬁcation tasks, and both are perfectly legitimate ways to view the problem. The choice of the generative model as the correct interpretation of their work, and its formalization as they have given it in their response to my piece, is the ﬁrst time this particular interpretation has been explicitly stated. At the very least, if they had wanted people to understand this point, it would have been good to make it clear at the outset by stating the model in the Science paper. One can hardly fault critics for misunderstanding which of two equally viable interpretations was the one intended. But no matter: How are my criticisms, and those of others Rao cites, relevant to what they do claim, under the various interpretations? Do my criticisms, as Rao would claim, miss the point? Am I building straw men and destroying them? I do not think so. Under the “deductive” interpretation my critiques would appear to be highly relevant. I argue, for example, that a memoryless random process with a non-equiprobable unigram distribution is classiﬁed as “linguistic” by the bigram conditional entropy feature from the Science paper.1 This implies that there are distributions that are misclassiﬁed by the feature. How many such misclassiﬁed systems are there among plausible nonlinguistic symbol systems? We do not know, because Rao et al. did not do a fair comparison of a wide range of linguistic and nonlinguistic systems, and demonstrate statistically how well the classiﬁer works. So we do not know whether in fact this feature “increases the probability” of the Indus symbols being a linguistic script. Needless to say, this problem with the deductive interpretation also implies the identical problem with the looser ﬁrst interpretation I outlined above. For the “inductive” generative interpretation, the argument is a bit trickier because we need to consider the probability of observing a feature, given one of the two hypotheses. For some features, such as the feature of linear arrangements of symbols, it seems plausible that one has a higher expectation of seeing linearly arranged symbols on the hypothesis that something is a true script, compared with the hypothesis of a nonlinguistic system. For bigram conditional entropy, again we really do not know. Again, not enough examples of human-created nonlinguistic systems were presented in the Science paper to be able to estimate the probabilities. And there is another issue: None of Rao and colleagues’ work provides a statistical measure by which one can assert that something is inside the “linguistic” range. Rather, they present plots and invite the reader to eyeball them, hardly a rigorous demonstration. As I noted herein, Fortran does not appear to have a conditional entropy that is much lower than the linguistic examples. What would one expect the “linguistic range” to look like if a larger number of languages were included and, equally importantly, a variety of genres ranging from  
University of Southern California Word alignment is a critical procedure within statistical machine translation (SMT). Brown et al. (1993) have provided the most popular word alignment algorithm to date, one that has been implemented in the GIZA (Al-Onaizan et al. 1999) and GIZA++ (Och and Ney 2003) software and adopted by nearly every SMT project. In this article, we investigate whether this algorithm makes search errors when it computes Viterbi alignments, that is, whether it returns alignments that are sub-optimal according to a trained model. 1. Background Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Brown et al. (1993) align an English/French sentence pair by positing a probabilistic model by which an English sentence is translated into French.1 The model provides a set of non-deterministic choices. When a particular sequence of choices is applied to an English input sentence e1...el, the result is a particular French output sentence f1...fm. In the Brown et al. models, a decision sequence also implies a speciﬁc word alignment vector a1...am. We say aj = i when French word fj was produced by English word ei during the translation. Here is a sample sentence pair (e, f) and word alignment a: e: NULL0 Mary1 did2 not3 slap4 the5 green6 witch7 f : Mary1 no2 dio´ 3 una4 bofetada5 a6 la7 bruja8 verde9 a: [ 1 3 4 5 5 0 5 7 6 ] Notice that the English sentence contains a special NULL word (e0) that generates “spurious” target words (in this case, a6). The Brown et al. (1993) models are manyto-one, meaning that each English word can produce several French children, but each ∗ Information Sciences Institute, University of Southern California, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: sravi@isi.edu. ∗∗ Information Sciences Institute, University of Southern California, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. 
The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language— words, phrases, and sentences—is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications. In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the ﬁeld of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. We also discuss the strategies used for evaluating paraphrase generation techniques and brieﬂy explore some future trends in paraphrase generation. 1. Introduction Although everyone may be familiar with the notion of paraphrase in its most fundamental sense, there is still room for elaboration on how paraphrases may be automatically generated or elicited for use in language processing applications. In this survey, we make an attempt at such an elaboration. An important outcome of this survey is the discovery that there are a large variety of paraphrase generation methods, each with widely differing sets of characteristics, in terms of performance as well as ease of deployment. We also ﬁnd that although many paraphrase methods are developed with a particular application in mind, all methods share the potential for more general applicability. Finally, we observe that the choice of the most appropriate method for an application depends on proper matching of the characteristics of the produced paraphrases with an appropriate method. It could be argued that it is premature to survey an area of research that has shown promise but has not yet been tested for a long enough period (and in enough systems). However, we believe this argument actually strengthens the motivation for a survey ∗ Department of Computer Science and Institute for Advanced Computer Studies, A.V. Williams Bldg, University of Maryland, College Park, MD 20742, USA. E-mail: nmadnani@umiacs.umd.edu. ∗∗ Department of Computer Science and Institute for Advanced Computer Studies, A.V. Williams Bldg, University of Maryland, College Park, MD 20742, USA. E-mail: bonnie@umiacs.umd.edu. Submission received: 16 December 2008; revised submission received: 30 November 2009; accepted for publication: 7 March 2010. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 3  that can encourage the community to use paraphrases by providing an applicationindependent, cohesive, and condensed discussion of data-driven paraphrase generation techniques. We should also acknowledge related work that has been done on furthering the community’s understanding of paraphrases. Hirst (2003) presents a comprehensive survey of paraphrasing focused on a deep analysis of the nature of a paraphrase. The current survey focuses instead on delineating the salient characteristics of the various paraphrase generation methods with an emphasis on describing how they could be used in several different NLP applications. Both these treatments provide different but valuable perspectives on paraphrasing. The remainder of this section formalizes the concept of a paraphrase, scopes out the coverage of this survey’s discussion, and provides broader context and motivation by discussing applications in which paraphrase generation has proven useful, along with examples. Section 2 brieﬂy describes the tasks of paraphrase recognition and textual entailment and their relationship to paraphrase generation and extraction. Section 3 forms the major contribution of this survey by examining various corpora-based techniques for paraphrase generation, organized by corpus type. Section 4 examines recent work done to construct various types of paraphrase corpora and to elicit human judgments for such corpora. Section 5 considers the task of evaluating the performance of paraphrase generation and extraction techniques. Finally, Section 6 provides a brief glimpse of the future trends in paraphrase generation and Section 7 concludes the survey with a summary. 1.1 What is a Paraphrase? The concept of paraphrasing is most generally deﬁned on the basis of the principle of semantic equivalence: A paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form. Paraphrases may occur at several levels. Individual lexical items having the same meaning are usually referred to as lexical paraphrases or, more commonly, synonyms, for example, hot, warm and eat, consume . However, lexical paraphrasing cannot be restricted strictly to the concept of synonymy. There are several other forms such as hyperonymy, where one of the words in the paraphrastic relationship is either more general or more speciﬁc than the other, for example, reply, say and landlady, hostess . The term phrasal paraphrase refers to phrasal fragments sharing the same semantic content. Although these fragments most commonly take the form of syntactic phrases ( work on, soften up and take over, assume control of ) they may also be patterns with linked variables, for example, Y was built by X, X is the creator of Y . Two sentences that represent the same semantic content are termed sentential paraphrases, for example, I ﬁnished my work, I completed my assignment . Although it is possible to generate very simple sentential paraphrases by simply substituting words and phrases in the original sentence with their respective semantic equivalents, it is signiﬁcantly more difﬁcult to generate more interesting ones such as He needed to make a quick decision in that situation, The scenario required him to make a split-second judgment . Culicover (1968) describes some common forms of sentential paraphrases. 1.2 Scope of Discussion The idea of paraphrasing has been explored in conjunction with, and employed in, a large number of natural language processing applications. Given the difﬁculty inherent 342  Madnani and Dorr  Generating Phrasal and Sentential Paraphrases  in surveying such a diverse task, an unfortunate but necessary remedy is to impose certain limits on the scope of our discussion. In this survey, we will be restricting our discussion to only automatic acquisition of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More speciﬁcally, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note that we only discuss those paraphrasing techniques that can generate the types of paraphrases under examination in this survey: phrasal and sentential. 1.3.1 Query and Pattern Expansion. One of the most common applications of paraphrasing is the automatic generation of query variants for submission to information retrieval systems or of patterns for submission to information extraction systems. Culicover (1968) describes one of the earliest theoretical frameworks for query keyword expansion using paraphrases. One of the earliest works to implement this approach (Spa¨rckJones and Tait 1984) generates several simple variants for compound nouns in queries submitted to a technical information retrieval system. For example: Original : circuit details Variant 1 : details about the circuit Variant 2 : the details of circuits  
When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately. We refer to this task as disentanglement. We present a corpus of Internet Relay Chat dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. We propose a graph-based clustering model for disentanglement, using lexical, timing, and discourse-based features. The model’s predicted disentanglements are highly correlated with manual annotations. We conclude by discussing two extensions to the model, speciﬁcity tuning and conversation start detection, both of which are promising but do not currently yield practical improvements. 1. Motivation Simultaneous conversations seem to arise naturally in both informal social interactions and multi-party typed chat. Aoki et al.’s (2006) study of voice conversations among 8–10 people found an average of 1.76 conversations (ﬂoors) active at a time, and a maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical conversation, therefore, does not form a contiguous segment of the chatroom transcript, but is frequently broken up by interposed utterances from other conversations. Disentanglement (also called thread detection [Shen et al. 2006], thread extraction [Adams and Martell 2008], and thread/conversation management [Traum 2004]) is the clustering task of dividing a transcript into a set of distinct conversations. It is an essential prerequisite for any kind of higher-level dialogue analysis. For instance, consider the multi-party exchange in Figure 1. Contextually, it is clear that this corresponds to two conversations, and Felicia’s1 response excellent is intended for Chanel and Regine. A straightforward reading of the transcript, however, might interpret it as a response to Gale’s statement immediately preceding. ∗ Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912. E-mail: melsner@cs.brown.edu. ∗∗ Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912. E-mail: ec@cs.brown.edu. 
Sentence compression holds promise for many applications ranging from summarization to subtitle generation. The task is typically performed on isolated sentences without taking the surrounding context into account, even though most applications would operate over entire documents. In this article we present a discourse-informed model which is capable of producing document compressions that are coherent and informative. Our model is inspired by theories of local coherence and formulated within the framework of integer linear programming. Experimental results show signiﬁcant improvements over a state-of-the-art discourse agnostic approach. 1. Introduction Recent years have witnessed increasing interest in sentence compression. The task encompasses automatic methods for shortening sentences with minimal information loss while preserving their grammaticality. The popularity of sentence compression is largely due to its relevance for applications. Summarization is a case in point here. Most summarizers to date aim to produce informative summaries at a given compression rate. If we can have a compression component that reduces sentences to a minimal length and still retains the most important content, then we should be able to pack more information content into a ﬁxed size summary. In other words, sentence compression would allow summarizers to increase the overall amount of information extracted without increasing the summary length (Lin 2003; Zajic et al. 2007). It could also be used as a post-processing step in order to render summaries more coherent and less repetitive (Mani, Gates, and Bloedorn 1999). Beyond summarization, a sentence compression module could be used to display text on small screen devices such as PDAs (Corston-Oliver 2001) or as a reading aid for the blind (Grefenstette 1998). Sentence compression could also beneﬁt information retrieval by eliminating extraneous information from the documents indexed by the ∗ Department of Computer Science, University of Illinois at Urbana-Champaign, 201 N Goodwin Ave, Urbana, IL 61801, USA. E-mail: clarkeje@illinois.edu. ∗∗ School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 10 September 2008; revised submission received: 27 October 2009; accepted for publication: 6 March 2010. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 3  retrieval engine. This way it would be possible to store less information in the index without dramatically affecting retrieval performance (Olivers and Dolan 1999). In theory, sentence compression may involve several rewrite operations such as deletion, substitution, insertion, and word reordering. In practice, however, the task is commonly deﬁned as a word deletion problem: Given an input sentence of words x = x1, x2, . . . , xn, the aim is to produce a compression by removing any subset of these words (Knight and Marcu 2002). Many sentence compression models aim to learn deletion rules from a parsed parallel corpus of source sentences and their target compressions (Knight and Marcu 2002; Turner and Charniak 2005; Galley and McKeown 2007; Cohn and Lapata 2009). For example, Knight and Marcu (2002) learn a synchronous context-free grammar (Aho and Ullman 1969) from such a corpus. The grammar rules have weights (essentially probabilities estimated using maximum likelihood) and are used to ﬁnd the best compression from the set of all possible compressions for a given sentence. Other approaches exploit syntactic information without making explicit use of a parallel grammar—for example, by learning which words or constituents to delete from a parse tree (Riezler et al. 2003; Nguyen et al. 2004; McDonald 2006; Clarke and Lapata 2008). Despite differences in formulation and training requirements (some approaches require a parallel corpus, whereas others do not), existing models are similar in that they compress sentences in isolation without taking their surrounding context into account. This is in marked contrast with common practice in summarization. Professional abstractors often rely on contextual cues while creating summaries (EndresNiggemeyer 1998). This is true of automatic summarization systems too, which consider the position of a sentence in a document and how it relates to its surrounding sentences (Kupiec, Pedersen, and Chen 1995; Barzilay and Elhadad 1997; Marcu 2000; Teufel and Moens 2002). Determining which information is important in a sentence is not merely a function of its syntactic position (e.g., deleting the verb or the subject of a sentence is less likely). A variety of contextual factors can play a role, such as the discourse topic, whether the sentence introduces new entities or events that have not been mentioned before, or the reader’s background knowledge. A sentence-centric view of compression is also at odds with most relevant applications which aim to create a shorter document rather than a single sentence. The resulting document must not only be grammatical but also coherent if it is to function as a replacement for the original. However, this cannot be guaranteed without knowledge of how the discourse progresses from sentence to sentence. To give a simple example, a contextually aware compression system could drop a word or phrase from the current sentence, simply because it is not mentioned anywhere else in the document and is therefore deemed unimportant. Or it could decide to retain it for the sake of topic continuity. In this article we are interested in creating a compression model that is appropriate for both documents and sentences. Luckily, a variety of discourse theories have been developed over the years (e.g., Mann and Thompson, 1988; Grosz, Weinstein, and Joshi 1995; Halliday and Hasan 1976) and have found application in summarization (Barzilay and Elhadad 1997; Marcu 2000; Teufel and Moens 2002) and other text generation applications (Scott and de Souza 1990; Kibble and Power 2004). In creating a contextsensitive compression model we are faced with three important questions: (1) Which type of discourse information is useful for compression? (2) Is it amenable to automatic processing (there is little hope for interfacing our compression model with applications if discourse-level cues cannot be identiﬁed robustly)? and (3) How are sentence- and document-based information best integrated in a uniﬁed modeling framework? 412  Clarke and Lapata  Discourse Constraints for Document Compression  In building our compression model we borrow insights from two popular models of discourse, Centering Theory (Grosz, Weinstein, and Joshi 1995) and lexical chains (Morris and Hirst 1991). Both approaches capture local coherence—the way adjacent sentences bind together to form a larger discourse. They also both share the view that discourse coherence revolves around discourse entities and the way they are introduced and discussed. We ﬁrst automatically augment our documents with annotations pertaining to centering and lexical chains, which we subsequently use to inform our compression model. The latter is an extension of the integer linear programming formulation proposed by Clarke and Lapata (2008). In a nutshell, sentence compression is modeled as an optimization problem. Given a long sentence, a compression is formed by retaining the words that maximize a scoring function coupled with a small number of constraints ensuring that the resulting output is grammatical. The constraints are encoded as linear inequalities whose solution is found using integer linear programming (ILP; Winston and Venkataramanan 2003; Vanderbei 2001). Discourse-level information can be straightforwardly incorporated by slightly changing the compression objective— we now wish to compress entire documents rather than isolated sentences—and augmenting the constraint set with discourse-speciﬁc constraints. We use our model to compress whole documents (rather than sentences sequentially) and evaluate whether the resulting text is understandable and informative using a question-answering task. We show that our method yields signiﬁcant improvements over discourse agnostic state-of-the-art compression models (McDonald 2006; Clarke and Lapata 2008). The remainder of this article is organized as follows. Section 2 provides an overview of related work. In Section 3 we present the ILP framework and compression model we employ in our experiments. We introduce our discourse-related extensions in Sections 4 and 5. Section 6 discusses our experimental set-up and evaluation methodology. Our results are presented in Section 7. Discussion of future work concludes the paper. 2. Related Work Sentence compression has been extensively studied across different modeling paradigms and has received both generative and discriminative formulations. Most generative approaches (Knight and Marcu 2002; Turner and Charniak 2005; Galley and McKeown 2007) are instantiations of the noisy-channel model, whereas discriminative formulations include decision-tree learning (Knight and Marcu 2002), maximum entropy (Riezler et al. 2003), support vector machines (Nguyen et al. 2004), and largemargin learning (McDonald 2006; Cohn and Lapata 2009). These models are trained on a parallel corpus and learn either which constituents to delete or which words to place adjacently in the compression output. Relatively few approaches dispense with the parallel corpus and generate compressions in an unsupervised manner using either a scoring function (Hori and Furui 2004; Clarke and Lapata 2008) or compression rules that are approximated from a non-parallel corpus such as the Penn Treebank (Turner and Charniak 2005). The majority of sentence compression approaches only look at sentences in isolation without taking into account any discourse information. However, there are two notable exceptions. Jing (2000) uses information from the local context as evidence for and against the removal of phrases during sentence compression. The idea here is that words or phrases which have more links to the surrounding context are more indicative of its topic, and thus should not be dropped. The topic is not explicitly identiﬁed; instead the importance of each phrase is determined by the number of lexical links within the local context. A link is created between two words if they are repetitions, 413  Computational Linguistics  Volume 36, Number 3  morphologically related, or associated in WordNet (Fellbaum 1998) through a lexical relation (e.g., hyponymy, synonymy). Links have weights—for example, repetition is considered more important than hypernymy. Each word is assigned a context weight based on the number of links to the local context and the importance of each relation type. Phrases are scored by the sum of their children’s context scores. The decision to drop a phrase is inﬂuenced by several factors, besides the local context, such as the phrase’s grammatical role and previous evidence from a parallel corpus. Daume´ III and Marcu (2002) generalize sentence compression to document compression. Given a document D = w1, w2, . . . , wn the goal is to produce a summary, S, by dropping any subset of words from D. Their system uses the discourse structure of a document and the syntactic structure of each of its sentences in order to decide which words to drop. Speciﬁcally, they extend Knight and Marcu’s (2002) noisy-channel model so that it can be applied to entire documents. In its simpler sentence compression instantiation, the noisy-channel model has two components, a language model and a channel model, both of which act on probabilistic context-free grammar (PCFG) representations. Daume´ III and Marcu deﬁne a noisy-channel model over syntax and discourse trees. Following Rhetorical Structure Theory (RST; Mann and Thompson 1988), they represent documents by trees whose leaves correspond to elementary discourse units (edus) and whose nodes specify how these and larger units (e.g., multi-sentence segments) are linked to each other by rhetorical relations (e.g., Contrast, Elaboration). Discourse units are further characterized in terms of their text importance: nuclei denote central segments, whereas satellites denote peripheral ones. Their model therefore learns not only which syntactic constituents to drop but also which discourse units are unimportant. While Daume´ III and Marcu (2002) present a hybrid summarizer that can simultaneously delete words and sentences from a document, the majority of summarization systems to date simply select and present to the user the most important sentences in a text (see Mani [2001] for a comprehensive overview of the methods used to achieve this). Discourse-level information plays a prominent role here as the overall document organization can indicate whether a sentence should be included in the summary. A variety of approaches have focused on cohesion (Halliday and Hasan 1976) and the way it is expressed in discourse. The term broadly describes a variety of linguistic devices responsible for making the elements of a text appear uniﬁed or connected. Examples include word repetition, anaphora, ellipsis, and the use of synonyms or superordinates. The underlying assumption is that sentences connected to many other sentences are likely to carry salient information and should therefore be included in the summary (Sjorochod’ko 1972). In exploiting cohesion for summarization, it is necessary to somehow represent cohesive ties. For instance, Boguraev and Kennedy (1997) represent cohesion in terms of anaphoric relations, whereas Barzilay and Elhadad (1997) operationalize cohesion via lexical chains—sequences of related words spanning a topical unit (Morris and Hirst 1991). Besides repetition, they also examine semantic relations based on synonymy, antonymy, hypernymy, and holonymy (we discuss their approach in more detail in Section 4.1). Other approaches characterize the document in terms of discourse structure and rhetorical relations. Documents are commonly represented as trees (Mann and Thompson 1988; Corston-Oliver 1998; Ono, Sumita, and Miike 1994; Carlson et al. 2001) and the position of a sentence in a tree is indicative of its importance. To give an example, Marcu (2000) proposes a summarization algorithm based on RST. Assuming that nuclei are more salient than satellites, the importance of sentential or clausal units can be determined based on tree depth. Alternatively, discourse structure can be represented as a graph (Wolf and Gibson 2004) and sentence importance is determined in 414  Clarke and Lapata  Discourse Constraints for Document Compression  graph-theoretic terms, by using graph connectivity measures such as in-degree or PageRank (Brin and Page 1998). Although a great deal of research in summarization has focused on global properties of discourse structure, there is evidence that local coherence may also be useful without the added complexity of computing discourse representations. (Unfortunately, discourse parsers have yet to achieve levels of performance comparable to syntactic parsers.) Teufel and Moens (2002) identify discourse relations on a sentence-by-sentence basis without presupposing an explicit discourse structure. Inspired by Centering Theory (Grosz, Weinstein, and Joshi 1995)—a theory of local discourse structure that models the interaction of referential continuity and salience of discourse entities—Ora˘san (2003) proposes a summarization algorithm that extracts sentences with at least one entity in common. The idea here is that summaries containing sentences referring to the same entity will be more coherent. Other work has relied on centering not so much to create summaries but to assess whether they are readable (Barzilay and Lapata 2008). Our approach differs from previous sentence compression approaches in three key respects. First, we present a compression model that is contextually aware; decisions on whether to remove or retain a word (or phrase) are informed by its discourse properties (e.g., whether it introduces a new topic, or whether it is semantically related to the previous sentence). Unlike Jing (2000) we explicitly identify topically important words and assume speciﬁc representations of discourse structure. Secondly, in contrast to Daume´ III and Marcu (2002) and other summarization work, we adopt a less global and more shallow representation of discourse based on Centering Theory and lexical chains. One of our aims is to exploit discourse features that can be computed efﬁciently and relatively cheaply. Thirdly, our compression model can be applied to isolated sentences as well as to entire documents. We claim the latter is more in the spirit of realworld applications where the goal is to generate a condensed and coherent text. Unlike Daume´ III and Marcu (2002) our model can delete words but not sentences, although it could be used to compress documents of any type, even summaries. 3. The Compression Model Our model is an extension of the approach put forward in Clarke and Lapata (2008) where they formulate sentence compression as an optimization problem. Given a long sentence, a compression is created by retaining the words that maximize a scoring function. The latter is essentially a language model coupled with a few constraints ensuring that the resulting output is grammatical. The language model and the constraints are encoded as linear inequalities whose solution is found using ILP.1 Their model is a good point of departure for studying document-based compression. As it does not require a parallel corpus, it can be ported across domains and text genres, while delivering state-of-the-art results (see Clarke and Lapata [2008] for details). Importantly, discourse-level information can be easily incorporated in two ways: Firstly, by applying the compression objective to entire documents rather than individual sentences; and secondly, by augmenting the constraint set with discourserelated information. This is not the case for other approaches (e.g., those based on the noisy channel model) where compression is modeled by grammar rules indicating which constituents to delete in a syntactic context. Moreover, ILP delivers a globally  
Stuart M. Shieber† School of Engineering and Applied Sciences Harvard University Tree-Local Multi-Component Tree-Adjoining Grammar (TL-MCTAG) is an appealing formalism for natural language representation because it arguably allows the encapsulation of the appropriate domain of locality within its elementary structures. Its multicomponent structure allows modeling of lexical items that may ultimately have elements far apart in a sentence, such as quantiﬁers and wh-words. When used as the base formalism for a synchronous grammar, its ﬂexibility allows it to express both the close relationships and the divergent structure necessary to capture the links between the syntax and semantics of a single language or the syntax of two different languages. Its limited expressivity provides constraints on movement and, we posit, may have generated additional popularity based on a misconception about its parsing complexity. Although TL-MCTAG was shown to be equivalent in expressivity to TAG when it was ﬁrst introduced, the complexity of TL-MCTAG is still not well understood. This article offers a thorough examination of the problem of TL-MCTAG recognition, showing that even highly restricted forms of TL-MCTAG are NP-complete to recognize. However, in spite of the provable difﬁculty of the recognition problem, we offer several algorithms that can substantially improve processing efﬁciency. First, we present a parsing algorithm that improves on the baseline parsing ∗ School of Engineering and Applied Sciences, Harvard University, 38 Plymouth St., Cambridge, MA 02141. E-mail: nesson@seas.harvard.edu. ∗∗ Department of Information Engineering, University of Padua, via Gradenigo 6/A, 1-35131 Padova, Italy. E-mail: satta@dei.unipd.it. † School of Engineering and Applied Sciences, Harvard University, Maxwell Dworkin Laboratory, 33 Oxford Street, Cambridge, MA 02138. E-mail: shieber@seas.harvard.edu. Submission received: 4 November 2008; revised submission received: 13 November 2009; accepted for publication: 18 March 2010. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 3  method and runs in polynomial time when both the fan-out and rank of the input grammar are bounded. Second, we offer an optimal, efﬁcient algorithm for factorizing a grammar to produce a strongly equivalent TL-MCTAG grammar with the rank of the grammar minimized.  1. Introduction Tree-Local Multi-Component Tree-Adjoining Grammar (TL-MCTAG) is an appealing formalism for natural language representation because it arguably allows the encapsulation of the appropriate domain of locality within its elementary structures (Kallmeyer and Romero 2007). Its ﬂexible multicomponent structure allows modeling of lexical items that may ultimately have elements far apart in a sentence, such as quantiﬁers and wh-words. Its limited expressivity provides constraints on movement and, we posit, may have generated additional popularity based on a misconception about its parsing complexity. TL-MCTAG can model highly structurally divergent but closely related elementary structures, such as the syntax and the semantics of a single word or construction or the syntax of a single word or construction and its translation into another language, with a pair of elementary trees. This ﬂexibility permits conceptually simple, highly expressive, and tightly coupled modeling of the relationship between the syntax and semantics of a language or the syntax and semantics of two languages. As a result, it has frequently been put to use in a growing body of research into incorporating semantics into the TreeAdjoining Grammar (TAG) framework (Kallmeyer and Joshi 2003; Han 2006; Nesson and Shieber 2006, 2007). It is also under investigation as a possible base formalism for use in synchronous-grammar based machine translations systems (Nesson 2009). Similar pairing of elementary structures of the TAG formalism is too constrained to capture the inherent divergence in structure between different languages or even between the syntax and semantics of a language. Pairing of more expressive formalisms is too ﬂexible to provide appropriate constraints and has unacceptable consequences for processing efﬁciency. Although TL-MCTAG was ﬁrst introduced by Weir (1988) and shown at that time to be equivalent in expressivity to TAG, the complexity of TL-MCTAG is still not well understood. Perhaps because of its equivalence to TAG, questions of processing efﬁciency have not been adequately addressed. This article offers a thorough examination of the problem of TL-MCTAG recognition, showing that even highly restricted forms of TL-MCTAG are NP-complete to recognize. However, in spite of the provable difﬁculty of the recognition problem, we offer several algorithms that can substantially improve processing efﬁciency. First, we present a parsing algorithm that improves on the baseline parsing method and runs in polynomial time when both the fan-out (the maximum number of trees in a tree set) and rank (the maximum number of trees that may be substituted or adjoined into a given tree) of the input grammar are bounded. Second, we offer an optimal, efﬁcient algorithm for factorizing a grammar to produce a strongly equivalent TL-MCTAG grammar with the rank of the grammar minimized. 1.1 Summary of Results TAG is a mildly context-sensitive grammar formalism widely used in natural language processing. Multicomponent TAG (MCTAG) refers to a group of formalisms that 444  Nesson, Satta, and Shieber  Complexity, Parsing, and Factorization of TL-MCTAG  generalize TAG by allowing elementary structures to be sets of TAG trees. One member of the MCTAG formalism group is Tree-Local MCTAG (TL-MCTAG), in which all trees from a single elementary tree set are constrained to adjoin or substitute into a single tree in another elementary tree set. Weir (1988) shows that this constraint is sufﬁcient to guarantee that TL-MCTAG has weak generative capacity equivalent to the polynomially parsable TAG. Recent work on the complexity of several TAG variants has demonstrated indirectly that the universal recognition problem for TL-MCTAG is NP-hard. This result calls into question the practicality of systems that employ TL-MCTAG as the formalism for expressing a natural language grammar. In this article we present a more ﬁne-grained analysis of the processing complexity of TL-MCTAG. We demonstrate (Section 3) that even under restricted deﬁnitions where either the rank or the fan-out of the grammar is bounded, the universal recognition problem is NP-complete. We deﬁne a novel variant of multi-component TAG formalisms that treats the elementary structures as vectors of trees rather than as unordered sets (Section 4). We demonstrate that this variant of the deﬁnition of the formalism (the vector deﬁnition) is consistent with the linguistic applications of the formalism presented in the literature. Universal recognition of the vector deﬁnition of TL-MCTAG is NP-complete when both the rank and fan-out are unbounded. However, when the rank is bounded, the universal recognition problem is polynomial in both the length of the input string and the grammar size. We present a novel parsing algorithm for TL-MCTAG (Section 5) that accommodates both the set and vector deﬁnitions of TL-MCTAG. Although no algorithms for parsing TL-MCTAG have previously been published, the standard method for parsing linear context-free rewriting systems (LCFRS)–equivalent formalisms can be applied directly to TL-MCTAG to produce a quite inefﬁcient baseline algorithm in which the polynomial degree of the length of the input string depends on the input grammar. We offer an alternative parser for TL-MCTAG in which the polynomial degree of the length of the input string is constant, though the polynomial degree of the grammar size depends on the input grammar. This alternative parsing algorithm is more appealing than the baseline algorithm because it performs universal recognition of TL-MCTAG (vector deﬁnition) with constant polynomial degree in both the length of the input string and the grammar size when rank is bounded. It may not be generally desirable to impose an arbitrary rank bound on TL-MCTAGs to be used for linguistic applications. However, it is possible given a TL-MCTAG to minimize the rank of the grammar. In the penultimate section of the paper (Section 6) we offer a novel and efﬁcient algorithm for transforming an arbitrary TL-MCTAG into a strongly equivalent TL-MCTAG where the rank is minimized.  1.2 Related Work Our work on TL-MCTAG complexity bears comparison to that of several others. Kallmeyer (2009) provides a clear and insightful breakdown of the different characteristics of MCTAG variants and the effect of these characteristics on expressivity and complexity. That work clariﬁes the deﬁnitions of MCTAG variants and the relationship between them rather than presenting new complexity results. However, it suggests the possibility of proving results such as ours in its assertion that, after a standard TAG parse, a check of whether particular trees belong to the same tree set cannot be performed in polynomial time. Kallmeyer also addresses the problem of parsing 445  Computational Linguistics  Volume 36, Number 3  MCTAG, although not speciﬁcally for TL-MCTAG. The method proposed differs from ours in that MCTAGs are parsed ﬁrst as a standard TAG, with any conditions on tree or set locality checked on the derivation forest as a second step. No speciﬁc algorithm is presented for performing the check of tree-locality on a TAG derivation forest, so it is difﬁcult to directly compare the methods. However, that method cannot take advantage of the gains in efﬁciency produced by discarding inappropriate partial parses at the time that they are ﬁrst considered. Aside from Kallmeyer’s work, little attention has been paid to the problem of directly parsing TL-MCTAG. Søgaard, Lichte, and Maier (2007) present several proofs regarding the complexity of the recognition problem for some linguistically motivated extensions of TAG that are similar to TL-MCTAG. Their work shows the NP-hardness of the recognition problem for these variants and, as an indirect result, also demonstrates the NP-hardness of TLMCTAG recognition. This work differs from ours in that it does not directly show the NP-hardness of TL-MCTAG recognition and does not further locate and constrain the source of the NP-hardness of the problem to the rank of the input grammar, nor does it provide mitigation through rank reduction of the grammar or by other means. Our work on TL-MCTAG factorization is thematically though not formally related to the body of work on induction of TAGs from a treebank exempliﬁed by Chen and Shanker (2004). The factorization performed in their work is done on the basis of syntactic constraints rather than with the goal of reducing complexity. Working from a treebank of actual natural language sentences, their work does not have the beneﬁt of explicitly labeled adjunction sites but rather must attempt to reconstruct a derivation from complete derived trees. The factorization problem we address is more closely related to work on factorizing synchronous context-free grammars (CFGs) (Gildea, Satta, and Zhang 2006; Zhang and Gildea 2007) and on factorizing synchronous TAGs (Nesson, Satta, and Shieber 2008). Synchronous grammars are a special case of multicomponent grammars, so the problems are quite similar to the TL-MCTAG factorization problem. However, synchronous grammars are fundamentally set-local rather than tree-local formalisms, which in some cases simpliﬁes their analysis. In the case of CFGs, the problem reduces to one of identifying problematic permutations of non-terminals (Zhang and Gildea 2007) and can be done efﬁciently by using a sorting algorithm to binarize any non-problematic permutations until only the intractable correspondences remain (Gildea, Satta, and Zhang 2006). This method is unavailable in the TAG case because the elementary structures may have depth greater than one and therefore the concept of adjacency relied upon in their work is inapplicable. The factorization algorithm of Nesson, Satta, and Shieber (2008) is the most closely related to this one but is not directly applicable to TL-MCTAG because each link is presumed to have exactly two locations and all adjunctions occur in a set-local rather than tree-local manner. 2. Technical Background A tree-adjoining grammar consists of a set of elementary tree structures of arbitrary depth, which are combined by the operations of adjunction and substitution. Auxiliary trees are elementary trees in which the root and a frontier node, called the foot node and distinguished by the diacritic ∗, are labeled with the same nonterminal A. The adjunction operation entails splicing in an auxiliary tree in an internal node within an elementary tree also labeled with nonterminal A. Trees without a foot node, which serve as a base case for derivations and may combine with other trees by substitution, 446  Nesson, Satta, and Shieber  Complexity, Parsing, and Factorization of TL-MCTAG  are called initial trees. Examples of the adjunction and substitution operations are given in Figure 1. For further background, we refer the reader to the survey by Joshi and Schabes (1997). A TAG derivation can be fully speciﬁed by a derivation tree, which records how the elementary structures are combined using the TAG operations to form the derived tree. The nodes of the derivation tree are labeled by the names of the elementary trees and the edges are labeled by the addresses at which the child trees substitute or adjoin. In contrast to CFGs, the derivation and derived trees are distinct. We depart from the traditional deﬁnition in notation only by specifying adjunction sites explicitly with numbered links in order to simplify the presentation of the issues raised by multi-component adjunctions. Each link may be used only once in a derivation. Adjunctions may only occur at nodes marked with a link. A numbered link at a single site in a tree speciﬁes that a single adjunction is available at that site. An obligatory adjunction constraint indicates that at least one link at a given node must be used (Joshi, Levy, and Takahashi, 1975; Vijay-Shanker and Joshi 1985). We notate obligatory adjunction constraints by underlining the label of the node to which the constraint applies. Because we use explicit links, the edges in the derivation tree are labeled with the number of the link used rather than the traditional label of the address at which the operation takes place. Multiple adjunction refers to permitting an unbounded number of adjunctions to occur at a single adjunction site (Vijay-Shanker 1987; Shieber and Schabes 1994). In the standard deﬁnition of TAG, multiple adjunction is disallowed to ensure that each derivation tree unambiguously speciﬁes a single derived tree (Vijay-Shanker 1987). Because each available adjunction is explicitly notated with a numbered link, our notation implicitly disallows multiple adjunction but permits a third possibility: bounded multiple adjunction. Bounded multiple adjunction permits the formalism to obtain some of the potential linguistic advantages of allowing multiple adjunction while preventing unbounded multiple adjunction. The usual constraint of allowing only one adjunction at a given adjunction site may be enforced in our link notation by permitting only one link at a particular link site to be used. MCTAG generalizes TAG by allowing the elementary items to be sets of trees rather than single trees (Joshi and Schabes 1997). The basic operations are the same but all trees in a set must adjoin (or substitute) into another tree set in a single step in the derivation. To allow for multi-component adjunction, a numbered link may appear on two or more nodes in a tree, signifying that the adjoining trees must be members of the same tree set. Any tree in a set may adjoin at any link location if it meets other adjunction or substitution conditions such as a matching node label. Thus a single multicomponent  Figure 1 An example of TAG operations substitution and adjunction used here to model natural language syntax. 447  Computational Linguistics  Volume 36, Number 3  Figure 2 An example of the way in which two tree sets may produce several different derived trees when combined under the standard deﬁnition of multicomponent TAG. link may give rise to many distinct derived trees even when the link is always used by the same multicomponent tree set. An example is given in Figure 2. This standard deﬁnition of multicomponent adjunction we will call the set deﬁnition for contrast with a variation we introduce in Section 4. A derivation tree for a multicomponent TAG is the same as for TAG except that the nodes are labeled with the names of elementary tree sets. An MCTAG is tree-local if tree sets are required to adjoin within a single elementary tree (Weir 1988). Using the numbered link notation introduced earlier for adjunction sites, a tree-local MCTAG (TL-MCTAG) is one in which the scope of the link numbers is a single elementary tree. An example TL-MCTAG operation is given in Figure 3. In contrast, an MCTAG is set-local if the trees from a single tree set are required to adjoin within a single elementary tree set and an MCTAG is non-local if the trees from a single tree set may adjoin to trees that are not within a single tree set. In a set-local MCTAG the scope of a link is a single elementary tree set, and in a non-local MCTAG the scope of a link is the entire grammar. Weir (1988) noted in passing that TL-MCTAG has generative capacity equivalent to TAG; a combination of well-chosen additional constraints and additions of duplicates of trees to the grammar can produce a weakly equivalent TAG. Alternatively, a featurebased TAG where the features enforce the same constraints may be used. Although the generative capacity of the formalism is not increased, any such conversion from TLMCTAG to TAG may require an exponential increase in the size of the grammar as we prove in Section 3. 3. Complexity We present several complexity results for TL-MCTAG. Søgaard, Lichte, and Maier (2007) show indirectly that TL-MCTAG membership is NP-hard. For clarity, we present a direct  Figure 3 An example TL-MCTAG operation demonstrating the use of TL-MCTAG to model wh-question syntax. 448  Nesson, Satta, and Shieber  Complexity, Parsing, and Factorization of TL-MCTAG  proof here. We then present several novel results demonstrating that the hardness result holds under signiﬁcant restrictions of the formalism. For a TL-MCTAG G we write |G| to denote the size of G, deﬁned as the total number of nodes appearing in all elementary trees in the tree sets of the grammar. Fan-out, f , measures the number of trees in the largest tree set in the grammar. We show that even when the fan-out is bounded to a maximum of two, the NP-hardness result still holds. The rank, r, of a grammar is the maximum number of derivational children possible for any tree in the grammar, or in other words, the maximum number of links in any tree in the grammar. We show that when rank is bounded, the NP-hardness result also holds. A notable aspect of all of the proofs given here is that they do not make use of the additional expressive power provided by the adjunction operation of TAG. Put simply, the trees in the tree sets used in our constructions meet the constraints of Tree Insertion Grammar (TIG), a known context-free–equivalent formalism (Schabes and Waters 1995). As a result, we can conclude that the increase in complexity stems from the multi-component nature of the formalism rather than from the power added by an unconstrained adjunction operation. 3.1 Universal Recognition of TL-MCTAG is NP-Complete In this section we prove that universal recognition of TL-MCTAG is NP-complete when neither the rank nor the fan-out of the grammar is bounded. Recall the 3SAT decision problem, which is known to be NP-complete. Let V = {v1, . . . , vp} be a set of variables and C = {c1, . . . , cn} be a set of clauses. Each clause in C is a disjunction of three literals over the alphabet of all literals LV = {v1, v1, . . . , vp, vp}. We represent each clause by a set of three literals. The language 3SAT is deﬁned as the set of all conjunctive formulas over the members of C that are satisﬁable. Theorem 1 The universal recognition problem for TL-MCTAG with unbounded rank and fan-out is NP-hard. Proof Let V, C be an arbitrary instance of the 3SAT problem.1 We use the derivations of the grammar to guess the truth assignments for V and use the tree sets to keep track of the dependencies among different clauses in C. Two tree sets are constructed for each variable, one corresponding to an assignment of true to the variable and one corresponding to an assignment of false. The links in the single initial tree permit only one of these two sets to be used. The tree set for a particular truth assignment for a particular variable vi makes it possible to introduce, by means of another adjunction, terminal symbols taken from the set {1, . . . , n} that correspond to each clause in C that would be satisﬁed by the given assignment to vi. In this way, the string w = 1 · · · n can be generated if and only if all clauses are satisﬁed by the truth assignment to some variable they contain.  
Ben Taskar† University of Pennsylvania Word-level alignment of bilingual text is a critical resource for a growing variety of tasks. Probabilistic models for word alignment present a fundamental trade-off between richness of captured constraints and correlations versus efﬁciency and tractability of inference. In this article, we use the Posterior Regularization framework (Grac¸a, Ganchev, and Taskar 2007) to incorporate complex constraints into probabilistic models during learning without changing the efﬁciency of the underlying model. We focus on the simple and tractable hidden Markov model, and present an efﬁcient learning algorithm for incorporating approximate bijectivity and symmetry constraints. Models estimated with these constraints produce a signiﬁcant boost in performance as measured by both precision and recall of manually annotated alignments for six language pairs. We also report experiments on two different tasks where word alignments are required: phrase-based machine translation and syntax transfer, and show promising improvements over standard methods. 1. Introduction The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1–5) for statistical machine translation and the concept of “word-byword” alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004; Chiang et al. 2005]) as well as for ∗ INESC-ID Lisboa, Spoken Language Systems Lab, R. Alves Redol 9, 1000-029 LISBOA, Portugal. E-mail: joao.graca@l2f.inesc-id.pt. ∗∗ University of Pennsylvania, Department of Computer and Information Science, Levine Hall, 3330 Walnut Street, Philadelphia, PA 19104-6309. E-mail: kuzman@cis.upenn.edu. † University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu. Submission received: 1 August 2009; revised submission received: 24 December 2009; accepted for publication: 10 March 2010. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 3  MT system combination (Matusov, Uefﬁng, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008). IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deﬁcient, intractable models that require local heuristic search and are difﬁcult to implement and extend. Many researchers use the GIZA++ software package (Och and Ney 2003) as a black box, selecting IBM Model 4 as a compromise between alignment quality and efﬁciency. All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word). Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86–98%). This leads to the common practice of post-processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003). In this article we focus on the HMM word alignment model (Vogel, Ney, and Tillmann 1996), using a novel unsupervised learning framework that signiﬁcantly boosts its performance. The new training framework, called Posterior Regularization (Grac¸a, Ganchev, and Taskar 2007), incorporates prior knowledge in the form of constraints on the model’s posteriors. The constraints are expressed as inequalities on the expected value under the posterior distribution of user-deﬁned features. Although the base model remains unchanged, learning guides the model to satisfy these constraints. We propose two such constraints: (i) bijectivity: one word should not translate to many words; and (ii) symmetry: directional alignments should agree. Both of these constraints signiﬁcantly improve the performance of the model both in precision and recall, with the symmetry constraint generally producing more accurate alignments. Section 3 presents the Posterior Regularization (PR) framework and describes how to encode such constraints in an efﬁcient manner, requiring only repeated inference in the original model to enforce the constraints. Section 4 presents a detailed evaluation of the alignments produced. The constraints over posteriors consistently and signiﬁcantly outperform the unconstrained HMM model, evaluated against manual annotations. Moreover, this training procedure outperforms the more complex IBM Model 4 nine times out of 12. We examine the inﬂuence of constraints on the resulting posterior distributions and ﬁnd that they are especially effective for increasing alignment accuracy for rare words. We also demonstrate a new methodology to avoid overﬁtting using a small development corpus. Section 5 evaluates the new framework on two different tasks that depend on word alignments. Section 5.1 focuses on MT and shows that the better alignments also lead to better translation systems, adding to similar evidence presented in Ganchev, Grac¸a, and Taskar (2008). Section 5.2 shows that the alignments we produce are better suited for transfer of syntactic dependency parse annotations. An implementation of this work (Grac¸a, Ganchev, and Taskar 2009) is available under a GPL license.1  
Graeme Blackwood∗ University of Cambridge Eduardo R. Banga∗∗ University of Vigo William Byrne∗ University of Cambridge In this article we describe HiFST, a lattice-based decoder for hierarchical phrase-based translation and alignment. The decoder is implemented with standard Weighted Finite-State Transducer (WFST) operations as an alternative to the well-known cube pruning procedure. We ﬁnd that the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in fewer search errors, better parameter optimization, and improved translation performance. The direct generation of translation lattices in the target language can improve subsequent rescoring procedures, yielding further gains when applying long-span language models and Minimum Bayes Risk decoding. We also provide insights as to how to control the size of the search space deﬁned by hierarchical rules. We show that shallow-n grammars, low-level rule catenation, and other search constraints can help to match the power of the translation system to speciﬁc language pairs. 1. Introduction Hierarchical phrase-based translation (Chiang 2005) is one of the current promising approaches to statistical machine translation (SMT). Hiero SMT systems are based on probabilistic synchronous context-free grammars (SCFGs) whose translation rules ∗ University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K. E-mail: {ad465,gwb24,wjb31}@eng.cam.ac.uk. ∗∗ University of Vigo, Department of Signal Processing and Communications, 36310 Vigo, Spain. E-mail: {giglesia,erbanga}@gts.tsc.uvigo.es. Submission received: 30 October 2009; revised submission received: 24 February 2010; accepted for publication: 10 April 2010. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 3  can be extracted automatically from word-aligned parallel text. These grammars can produce a very rich space of candidate translations and, relative to simpler phrasebased systems (Koehn, Och, and Marcu 2003), the power of Hiero is most evident in translation between dissimilar languages, such as English and Chinese (Chiang 2005, 2007). Hiero is able to learn and apply complex patterns in movement and translation that are not possible with simpler systems. Hiero can also be used to good effect on “simpler” problems, such as translation between English and Spanish (Iglesias et al. 2009c), even though there is not the same need for the full complexity of movement and translation. If gains in using Hiero are small, however, the computational and modeling complexity involved are difﬁcult to justify. Such concerns would vanish if there were reliable methods to match Hiero complexity for speciﬁc translation problems. Loosely put, it would be a good thing if the complexity of a system was somehow proportional to the improvement in translation quality the system delivers. Another notable current trend in SMT is system combination. Minimum Bayes Risk decoding is widely used to rescore and improve hypotheses produced by individual systems (Kumar and Byrne 2004; Tromble et al. 2008; de Gispert et al. 2009), and more aggressive system combination techniques which synthesize entirely new hypotheses from those of contributing systems can give even greater translation improvements (Rosti et al. 2007; Sim et al. 2007). It is now commonplace to note that even the best available individual SMT system can be signiﬁcantly improved upon by such techniques. This puts a burden on the underlying SMT systems which is somewhat unusual in NLP. The requirement is not merely to produce a single hypothesis that is as good as possible. Ideally, the SMT systems should generate large collections of candidate hypotheses that are simultaneously diverse and of good quality. Relative to these concerns, previously published descriptions of Hiero have noted certain limitations. Spurious ambiguity (Chiang 2005) was described as a situation where the decoder produces many derivations that are distinct yet have the same model feature vectors and give the same translation. This can result in n-best lists with very few different translations which is problematic for the minimum-error-rate training algorithm ... This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all distinct hypotheses to a ﬁxed depth by means of k-best hypothesis lists. If enumeration was not necessary, or if the lists could be arbitrarily deep, there might still be many duplicate derivations, but at least the hypothesis space would not be impoverished. Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu 1997; Setiawan et al. 2009). For our purposes we say that overgeneration occurs when different derivations based on the same set of rules give rise to different translations. An example is given in Figure 1. This process is not necessarily a bad thing in that it allows new translations to be synthesized from rules extracted from training data; a strong target language model, such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses. Overgeneration does complicate translation, however, in that many hypotheses are introduced only to be subsequently discarded. The situation is further complicated by search errors. Any search procedure which relies on pruning during search is at risk of search errors and the risk is made worse if the grammars tend to introduce many similar scoring hypotheses. In particular we have found that cube pruning is very prone to search errors, that is, the hypotheses produced by cube pruning are not the top scoring hypotheses which should be found under the Hiero grammar (Iglesias et al. 2009b). 506  de Gispert et al.  Hierarchical Translation with WFSTs and Shallow-n Grammars  Figure 1 Example of multiple translation sequences from a simple grammar fragment showing variability in reordering in translation of the source sequence abc. These limitations are clearly related to each other. Moreover, they become more problematic as the amount of parallel text grows. As the number of rules in the grammar increases, the grammars become more expressive, but the ability to search them does not improve. This leads to a widening gap between the expressive power of the grammar and the ability to search it to ﬁnd good and diverse hypotheses. In this article we describe the following two reﬁnements to Hiero which are intended to address some of the limitations in its original formulation. Lattice-based hierarchical translation We describe how the cube pruning procedure can be replaced by standard operations with Weighted Finite State Transducers (WFSTs) so that Hiero uses translation lattices rather than n-best lists in search. We ﬁnd that keeping partial translation hypotheses in lattice form greatly reduces search errors. In some instances it is possible to perform translation without any pruning at all so that search errors are completely eliminated. Consistent with the observation by Chiang (2005), this leads to improvements in minimum error rate training. Furthermore, the direct generation of translation lattices can improve gains from subsequent language model and Minimum Bayes Risk (MBR) rescoring. Shallow-n grammars and additional nonterminal categories Nonterminals can be incorporated into hierarchical translation rules for the purpose of tuning the size of the Hiero search space for individual language pairs. Shallow-n grammars are described and shown to control the level of rule nesting, low-level rule catenation, and the minimum and maximum spans of individual translation rules. In translation experiments we ﬁnd that a shallow-1 grammar (one level of rule nesting) is sufﬁciently expressive for Arabic-to-English translation, but that a shallow-3 grammar is required in Chinese-to-English translation to match the performance of a full Hiero system that allows arbitrary rule nesting. These nonterminals are introduced to control the Hiero search space and do not require estimation from annotated—or parsed—parallel text, as can be required by translation systems based on linguistically motivated grammars. We use this approach as the basis of a general approach to SMT modeling. To control overgeneration, we revisit the synchronous context-free grammar deﬁned by hierarchical rules and take a shallow-1 grammar as a starting point. We then increase the complexity of the rules until the desired translation quality is found. 507  Computational Linguistics  Volume 36, Number 3  With these reﬁnements we ﬁnd that hierarchical phrase-based translation can be efﬁciently carried out with no (or minimal) search errors in large-data tasks and can achieve state-of-the-art translation performance. There are many beneﬁts to formulating Hiero translation in terms of WFSTs. Following the manner in which Knight and Al-Onaizan (1998), Kumar, Deng, and Byrne (2006), and Graehl, Knight, and May (2008) elucidate other machine translation models, we can use WFST operations to make the operations of the Hiero decoder very clear. The simplicity of the analysis makes it possible to focus on the underlying grammars and avoid the complexities of heuristic search procedures. Once the decoder is formulated, implementation is mostly straightforward using standard WFST techniques developed for language processing (Mohri, Pereira, and Riley 2002). What difﬁculties arise are due to using ﬁnite state techniques with grammars which are not themselves ﬁnite state. We will show, however, that the basic operations which need to be performed, such as extracting sufﬁcient statistics for minimum error rate training, can be done relatively easily and naturally. 1.1 Overview In Section 2 we describe HiFST, which is a hierarchical phrase-based translation system based on the OpenFST WFST libraries (Allauzen et al. 2007). We describe how translation lattices can be generated over the Cocke-Younger-Kasami (CYK) grid used for parsing under Hiero. We also review some modeling issues needed for practical translation, such as the efﬁcient handling of source language deletions and the extraction of statistics for minimum error rate training. This requires running HiFST in “alignment mode” (Section 2.3) to ﬁnd all the rule derivations that generate a given set of translation hypotheses. In Section 3 we investigate parameters that control the size and nature of the hierarchical phrase-based search space as deﬁned by hierarchical translation rules. To efﬁciently explore the largest possible space and avoid pruning in search, we introduce ways to easily adapt the grammar to the reordering needs of each language pair. We describe the use of additional nonterminal categories to limit the degree of rule nesting, and can directly control the minimum or maximum span each translation rule can cover. In Section 4 we report detailed translation results for Arabic-to-English and Chinese-to-English, and review translation results for Spanish-to-English and Finnishto-English translation. In these experiments we contrast the performance of lattice-based and cube pruning hierarchical decoding and we measure the impact on processing time and translation performance due to changes in search parameters and grammar conﬁgurations. We demonstrate that it is easy and feasible to compute the marginal instead of the Viterbi probabilities when using WFSTs, and that this yields gains in translation performance. And ﬁnally, we show that lattice-based translation performs signiﬁcantly better than k-best lists for the task of combining translation hypotheses generated from alternative morphological segmentations of the data via lattice-based MBR decoding. 2. Hierarchical Translation and Alignment with WFSTs Hierarchical phrase-based rules deﬁne a synchronous context-free grammar (CFG) and a particular search space of translation candidates. Table 1 shows the type of rules included in a standard hierarchical phrase-based grammar, where T denotes the terminals (words) and ∼ is a bijective function that relates the source and target nonterminals of 508  de Gispert et al.  Hierarchical Translation with WFSTs and Shallow-n Grammars  Table 1 Rules contained in the standard hierarchical grammar.  standard hierarchical grammar  S→ X,X S→ S X,S X X→ γ,α,∼ , γ, α ∈ {X ∪ T}+  glue rule 1 glue rule 2 hiero rules  each rule (Chiang 2007). This function is deﬁned if there are at least two nonterminals, and for clarity of presentation may be omitted in general rule discussions. When γ, α ∈ {T}+, that is, the rule contains no nonterminals, the rule is a simple lexical phrase pair. The HiFST translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman 1998). Parsing follows the description of Chiang (2005, 2007); it maintains back-pointers and employs hypothesis recombination without pruning. The underlying model is a probabilisitic synchronous CFG consisting of a set R = {Rr} of rules Rr : Nr → γr,αr / pr, with “glue” rules, S → X,X and S → S X,S X . N denotes the set of nonterminal categories (examples are given in Section 3), and pr denotes the rule probability, typically transformed to a cost cr; unless otherwise noted we use the tropical semiring, so cr = − log pr. T denotes the terminals (words), and the grammar builds parses based on strings γ, α ∈ {N ∪ T}+. Each cell in the CYK grid is speciﬁed by a nonterminal symbol and position in the CYK grid: (N, x, y), which spans sxx+y−1 on the source sentence. In effect, the source language sentence is parsed using a CFG with rules N → γ. The generation of translations is a second step that follows parsing. For this second step, we describe a method to construct word lattices with all possible translations that can be produced by the hierarchical rules. Construction proceeds by traversing the CYK grid along the back-pointers established in parsing. In each cell (N, x, y) in the CYK grid, we build a target language word lattice L(N, x, y). This lattice contains every translation of sxx+y−1 from every derivation headed by N. These lattices also contain the translation scores on their arc weights. The ultimate objective is the word lattice L(S, 1, J) which corresponds to all the analyses that cover the source sentence sJ1. Once this is built, we can apply a target language model to L(S, 1, J) to obtain the ﬁnal target language translation lattice (Allauzen, Mohri, and Roark 2003).  2.1 Lattice Construction over the CYK Grid  In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), that  is, for r ∈ R(N, x, y), the rule N → γr,αr was used in at least one derivation involving  that cell.  For each rule Rr, r ∈ R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived  from the target side of the rule αr by concatenating lattices corresponding to the ele-  ments αri is a  of αr = αr1...αr|αr|. If an nonterminal, it refers to  αri is a terminal, a cell (N , x , y )  creating lower in  its the  lattice is straightforward. If grid identiﬁed by the back-  pointer BP(N, x, y, r, i); in this case, the lattice used is L(N , x , y ). Taken together,  L(N, x, y, r) =  L(N, x, y, r, i)  (1)  i=1..|αr |  509  Computational Linguistics  Volume 36, Number 3  Figure 2 Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3. The grid is represented here in two dimensions (x, y). In practice only the ﬁrst column accepts both nonterminals (S, X). For this reason it is divided into two subcolumns.  L(N, x, y, r, i) =  A(αi) if αi ∈ T L(N , x , y ) otherwise  (2)  where A(t), t ∈ T returns a single-arc acceptor which accepts only the symbol t. The lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in R(N, x, y):  L(N, x, y) =  L(N, x, y, r)  (3)  r∈R(N,x,y)  Lattice union and concatenation are performed using the ⊕ and ⊗ WFST operations, respectively, as described by Allauzen et al. (2007). If a rule Rr has a cost cr, it is applied to the exit state of the lattice L(N, x, y, r) prior to the operation of Equation (3).  2.1.1 An Example of Phrase-based Translation. Figure 2 illustrates this process for a three- word source sentence s1s2s3 under monotonic phrase-based translation. The left-hand side shows the state of the CYK grid after parsing using the rules R1 to R5. These include three standard phrases, that is, rules with only terminals (R1, R2, R3), and the glue rules (R4, R5). Arrows represent back-pointers to lower-level cells. We are interested in the uppermost S cell (S, 1, 3), as it represents the search space of translation hypotheses covering the whole source sentence. Two rules (R4, R5) are in this cell, so the lattice L(S, 1, 3) will be obtained by the union of the two lattices found by the back-pointers of these two rules. This process is explicitly derived in the right-hand side of Figure 2.  2.1.2 An Example of Hierarchical Translation. Figure 3 shows a hierarchical scenario for the same sentence. Three rules, R6, R7, R8, are added to the example of Figure 2, thus providing two additional derivations. This makes use of sublattices already produced in the creation of L(S, 1, 3, 5) and L(X, 1, 3, 1) in Figure 2; these are shown within {}.  510  de Gispert et al.  Hierarchical Translation with WFSTs and Shallow-n Grammars  Figure 3 Translation as in Figure 2 but with additional rules R6,R7,R8. Lattices previously derived appear within {}. 2.2 A Procedure for Lattice Construction Figure 4 presents an algorithm to build the lattice for every cell. The algorithm uses memoization: If a lattice for a requested cell already exists, it is returned (line 2); otherwise it is constructed via Equations (1)–(3). For every rule, each element of the target side (lines 3,4) is checked as terminal or nonterminal (Equation (2)). If it is a terminal element (line 5), a simple acceptor is built. If it is a nonterminal (line 6), the lattice associated to its back-pointer is returned (lines 7 and 8). The complete lattice L(N, x, y, r) for each rule is built by Equation (1) (line 9). The lattice L(N, x, y) for this cell is then found by union of all the component rules (line 10, Equation (3)); this lattice is then reduced by standard WFST operations (lines 11, 12, 13). It is important at this point to remove any epsilon arcs which may have been introduced by the various WFST union, concatenation, and replacement operations (Allauzen et al. 2007). We now address several important aspects of efﬁcient implementation.  Figure 4 Recursive lattice construction from a CYK grid. 511  Computational Linguistics  Volume 36, Number 3  Figure 5 Delayed translation WFST with derivations from Figures 2 and 3 before (left) and after minimization (right).  2.2.1 Delayed Translation. Equation (2) leads to the recursive construction of lattices in upper levels of the grid through the union and concatenation of lattices from lower levels. If Equations (1) and (3) are actually carried out over fully expanded word lattices, the memory required by the upper lattices will increase exponentially. To avoid this, we use special arcs that serve as pointers to the low-level lattices. This effectively builds a skeleton for the desired lattice and delays the creation of the ﬁnal word lattice until a single replacement operation is carried out in the top cell (S, 1, J). To make this exact, we deﬁne a function g(N, x, y) which returns a unique tag for each lattice in each cell, and use it to redeﬁne Equation (2). With the back-pointer (N , x , y ) = BP(N, x, y, r, i), these special arcs are introduced as  L(N, x, y, r, i) =  A(αi) if αi ∈ T A(g(N , x , y )) otherwise  (4)  The resulting lattices L(N, x, y) are a mix of target language words and lattice pointers (Figure 5, left). Each still represents the entire search space of all translation hypotheses covering the span, however. Importantly, operations on these lattices—such as lossless size reduction via determinization and minimization (Mohri, Pereira, and Riley 2002)—can still be performed. Owing to the existence of multiple hierarchical rules which share the same low-level dependencies, these operations can greatly reduce the size of the skeleton lattice; Figure 5 shows the effect on the translation example. This process is carried out for the lattice at every cell, even at the lowest level where there are only sequences of word terminals. As stated, size reductions can be signiﬁcant. Not all redundancy is removed, however, because duplicate paths may arise through the concatenation and union of sublattices with different spans. At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices. A single FST replace operation (Allauzen et al. 2007) recursively substitutes all pointers by their lower-level lattices until no pointers are left, thus producing the complete target word lattice for the whole source sentence. The use of the lattice pointer arc was inspired by the “lazy evaluation” techniques developed by Mohri, Pereira, and Riley (2000), closely related to Recursive Transition Networks (Woods 1970; Mohri 1997). Its implementation uses the infrastructure provided by the OpenFST libraries for delayed composition.  2.2.2 Top-level Pruning and Search Pruning. The ﬁnal translation lattice L(S, 1, J) can be quite large after the pointer arcs are expanded. We therefore apply a word-based  512  de Gispert et al.  Hierarchical Translation with WFSTs and Shallow-n Grammars  Figure 6 Transducers for ﬁltering up to one (left) or two (right) consecutive deletions. language model via WFST composition (Allauzen et al. 2007) and perform likelihoodbased pruning based on the combined translation and language model scores. We call this top-level pruning because it is performed over the topmost lattice. Pruning can also be performed on the sublattices in each cell during search. One simple strategy is to monitor the number of states in the determinized lattices L(N, x, y). If this number is above a threshold, we expand any pointer arcs and apply a word-based language model via composition. The resulting lattice is then reduced by likelihoodbased pruning, after which the LM scores are removed. These pruning strategies can be very selective, for example allowing the pruning threshold to depend on the height of the cell in the grid. In this way the risk of search errors can be controlled. The same n-gram language model can be used for top-level pruning and search pruning, although different WFST realizations are required. For top-level pruning, a standard implementation as described by Allauzen et al. (2007) is appropriate. For search pruning, the WFST must allow for incomplete language model histories, because many sublattice paths are incomplete translation hypotheses which do not begin with a sentence-start marker. The language model acceptor is constructed so that initial substrings of length less than the language model order are assigned no weight under the language model. 2.2.3 Constrained Source Word Deletion. As a practical matter it can be useful to allow SMT systems to delete some source words rather than to enforce their translation. Deletions can be allowed in Hiero by including in the grammar a set of special deletion rules of the type: X→ s,NULL . Unconstrained application of these rules can lead to overly large and complex search spaces, however. We therefore limit the number of consecutive source word deletions as we explore each cell of the CYK grid. This is done by standard composition with an unweighted transducer that maps any word to itself, and up to k NULL tokens to arcs. In Figure 6 this simple transducer for k = 1 and k = 2 is drawn. Composition of the lattice in each cell with this transducer ﬁlters out all translations with more than k consecutive deleted words. 2.3 Hierarchical Phrase-Based Alignment with WFSTs We now describe a method to apply our decoder in alignment mode. The objective in alignment is to recover all the derivations which can produce a given translation. We do this rather than keep track of the rules used during translation, because we ﬁnd it faster and more efﬁcient ﬁrst to generate translations and then, by running the system as an aligner with a constrained target space, to extract all the relevant derivations with their costs. As will be discussed in Section 2.3.1, this is useful for minimum error training, where the contribution of each feature to the overall hypothesis cost is required for system optimization. 513  Computational Linguistics  Volume 36, Number 3  Figure 7 Transducer encoding simultaneously rule derivations R2R1R3R4 and R1R5R6, and the translation t5t8. The input sentence is s1s2s3 and the grammar considered here contains the following rules: R1: S→ X,X , R2: S→ S X,S X , R3: X→ s1,t5 , R4: X→ s2 s3,t8 , R5: X→ s1 X s3,X t8 and R6: X→ s2,t5 .  Conceptually, we would like to create a transducer that represents the mapping from all possible rule derivations to all possible translations, and then compose this transducer with an acceptor for the translations of interest. Creating this transducer which maps derivations to translations is not feasible for large translation grammars, so we instead keep track of rules as they are used to generate a particular translation output. We introduce two modiﬁcations into lattice construction over the CYK grid described in Section 2.2: 1. In each cell transducers are constructed which map rule sequences to the target language translation sequences they produce. In each transducer the output strings are all possible translations of the source sentence span covered by that cell; the input strings are all the rule derivations that generate those translations. The rule derivations are expressed as sequences of rule indices r given the set of rules R = {Rr}. 2. As these transducers are built they are composed with acceptors for subsequences of the reference translations so that any translations not present in the given set of reference translations are removed. In effect, this replaces the general target language model used in translation with an unweighted acceptor which accepts only speciﬁc sentences.  For alignment, Equations (1) and (2) are redeﬁned as  L(N, x, y, r) = AT(r, )  L(N, x, y, r, i)  (5)  i=1..|αr|  L(N, x, y, r, i) =  AT( , αi) if αi ∈ T L(N , x , y ) otherwise  (6)  where AT(r, t), Rr ∈ R, t ∈ T returns a single-arc transducer which accepts the symbol r in the input language (rule indices) and the symbol t in the output language (target words). The weight assigned to each arc is the same in alignment as in translation. With these deﬁnitions the goal lattice L(S, 1, J) is now a transducer with rule indices in the input symbols and target words in the output symbols. A simple example is given in Figure 7 where two rule derivations for the translation t5t8 are represented by the transducer. As we are only interested in those rule derivations that generate the given target references, we can discard non-desired translations via standard FST composition of  514  de Gispert et al.  Hierarchical Translation with WFSTs and Shallow-n Grammars  Figure 8 Construction of a substring acceptor. An acceptor for the strings t1t2t4 and t3t4 (left) and its substring acceptor (right). In alignment the substring acceptor can be used to ﬁlter out undesired partial translations via standard FST composition operations. the lattice transducer with the given reference acceptor. In principle, this would be done in the uppermost cell of the CYK, once the complete source sentence has been covered. However, keeping track of all possible rule derivations and all possible translations until the last cell may not be computationally feasible for many sentences. It is more desirable to carry out this ﬁltering composition in lower-level cells while constructing the lattice over the CYK grid so as to avoid storing an increasing number of undesired translations and derivations in the lattice. The lattice in each cell should contain translations formed only from substrings of the references. To achieve this we build an unweighted substring acceptor that accepts all substrings of each target reference string. For instance, given the reference string t1t2 . . . tJ, we build an acceptor for all substrings ti . . . tj, where 1 ≤ i ≤ j ≤ J. This is applied to lattices in all cells (x, y) that do not span the whole sentence. In the uppermost cell we compose with the reference acceptor which accepts only complete reference strings. Given a lattice of target references, the unweighted substring acceptor is built as: 1. change all non-initial states into ﬁnal states 2. add one initial state and add arcs from it to all other states Figure 8 shows an example of a substring acceptor for the two references t1t2t4 and t3t4. The substring acceptor also accepts an empty string, accounting for those rules that delete source words, which in other words translate into NULL. In some instances the ﬁnal composition with the reference acceptor might return an empty lattice. If this happens there is no rule sequence in the grammar that can generate the given source and target sentences simultaneously. We have described the use of transducers to encode mappings from rule derivations to translations. These transducers are somewhat impoverished relative to parse trees and parse forests, which are more commonly used to encode this relationship. It is easy to map from a parse tree to one of these transducers but the reverse essentially requires re-parsing to recreate the tree structure. The structures of the parse trees associated with a translation are not needed by many algorithms, however. In particular, parameter optimization by MERT requires only the rules involved in translation. Our approach keeps only what is needed by such algorithms. This approach also has practical advantages such as the ability to align directly to k-best lists or lattices of reference translations. 515  Computational Linguistics  Volume 36, Number 3  Figure 9 One arc from a rule acceptor that assigns a vector of K feature weights to each rule (top) and the result of composition with the transducer of Figure 7 (after weight-pushing) (bottom). The components of the ﬁnal K-dimensional weight vector agree with the feature weights of the rule sequence, for example ck = c2,k + c1,k + c3,k + c4,k for k = 1 . . . K.  2.3.1 Extracting Feature Values from Alignments. As described by Chiang (2007), scores are  associated with hierarchical translation rules through a factoring into features within a  log-linear model (Och and Ney 2002). We assume that we have a collection of K features  and that the cost cr of each rule Rr is cr =  K k=1  λkcr,k,  where  cr,k  is  the  value  of  the  kth  feature value for the rth rule and λk is the weight assigned to the kth feature for all rules.  For a parse which  be written as  K k=1  makes use of the  λk  N n=1  crn,k.  The  rules Rr1 quantity  .  . . RrN , its cost  N n=1  crn,k  is  the  N n=1  crn  can  therefore  contribution by the kth  feature to the overall translation score for that parse. These are the quantities which  need to be extracted from alignment lattices for use in procedures such as minimum  error rate training for estimation of the feature weights λk. The procedure described in Section 2.3 produces alignment lattices with scores  consistent with the total parse score. Further steps must be taken to factor this over-  all score to identify the contribution due to individual features or translation rules.  We introduce a rule acceptor which accepts sequences of rule indices, such as the  input sequences of the alignment transducer, and assigns weights in the form of  K-dimensional vectors. Each component of the weight vector corresponds to the feature value for that rule. Arcs have the form 0 R−r/→wr 0 where wr = [cr,1, . . . , cr,K]. An example  of composition with this rule acceptor is given in Figure 9 to illustrate how feature scores  are mapped to components of the weight vector. The same operations can be applied to  the (unweighted) alignment transducer on a much larger scale to extract the statistics  needed for minimum error rate training.  We typically apply this procedure in the tropical semiring (Viterbi likelihoods), so  that only the best rule derivation that generated each translation candidate is taken  into account when extracting feature contributions for MERT. However, given the  alignment transducer L, this could also be performed in the log semiring (marginal  likelihoods), taking into account the feature contributions from all rule derivations, for  each translation candidate. This would be adequate if the translation system also car-  ried out decoding in the log semiring, an experiment which is partially explored in  Section 4.4.  We note that the contribution of the language model to the overall translation score  cannot be calculated in this scheme, since the language model score cannot be factored  in terms of rules. To obtain the language model contribution, we simply carry out  WFST composition (Allauzen et al. 2007) between an unweighted acceptor of the target  516  de Gispert et al.  Hierarchical Translation with WFSTs and Shallow-n Grammars  Figure 10 Hierarchical translation grammar example and two parse trees with different levels of rule nesting for the input sentence s1s2s3s4. sentences and the n-gram language model used in translation. After determinization, the cost of each path in the acceptor is then the desired LM feature contribution. 3. Shallow-n Translation Grammars: Translation Search Space Reﬁnements In this section we describe shallow-n grammars in order to reduce Hiero overgeneration and adapt the grammar complexity to speciﬁc language pairs; the ultimate goal is to deﬁne the most constrained grammar that is capable of generating the desired movement and translation, so that decoding can be performed without search errors. Hiero can provide varying degrees of complexity in movement and translation. Consider the example shown in Figure 10, which shows a hierarchical grammar deﬁned by six rules. For the input sentence s1s2s3s4, there are two possible parse trees as shown; the rule derivations for each tree are R1R4R3R5 and R2R1R3R5R6. Along with each tree is shown the translation generated and the phrase-level alignment. Comparing the two trees and alignments, the leftmost tree makes use of more reordering when translating from source to target through the nested application of the hierarchical rules R3 and R4. For some language pairs this level of reordering may be required in translation, but for other language pairs it may lead to overgeneration of unwanted hypotheses. Suppose the grammar in this example is modiﬁed as follows: 1. A nonterminal X0 is introduced into hierarchical translation rules R3:X→ X0 s3,t5 X0 R4:X→ X0 s4,t3 X0 2. Rules for lexical phrases are applied to X0 R5:X0→ s1 s2,t1 t2 R6:X0→ s4,t7 These modiﬁcations exclude parses in which hierarchical translation rules generate other hierarchical rules, except at the 0th level which generate lexical phrases. Consequently the left most tree of Figure 10 cannot be generated and t5t1t2t7 is the only allowable translation of s1s2s3s4. We call this a ‘shallow-1’ grammar: The maximum 517  Computational Linguistics  Volume 36, Number 3  degree of rule nesting allowed is 1 and only the glue rule can be applied above this level. The use of additional nonterminal categories is an elegant way to easily control important aspects that can have a strong impact on the search space. A shallow-n translation grammar can be formally deﬁned as: 1. the usual nonterminal S 2. a set of nonterminals {X0, . . . , XN} 3. two glue rules: S → XN,XN and S → S XN,S XN 4. hierarchical translation rules for levels n = 1, . . . , N: R: Xn→ γ,α,∼ , γ, α ∈ {{Xn−1} ∪ T}+ with the requirement that α and γ contain at least one Xn−1 5. translation rules which generate lexical phrases: R: X0→ γ,α , γ, α ∈ T+  3.1 Avoiding Some Spurious Ambiguity The added requirement in condition (4) in the deﬁnition of shallow-n grammars is included to avoid some instances in which multiple parses lead to the same translation. It is not absolutely necessary but it can be added without any loss in representational capability. To see the effect of this constraint, consider the following example with a source sentence s1 s2 and a shallow-1 grammar deﬁned by these four rules: R1: S→ X1,X1 R2: X1→ s1 s2,t2 t1 R3: X1→ s1 X0,X0 t1 R4: X0→ s2,t2 There are two derivations R1R2 and R1R3R4 which yield identical translations. However R2 would not be allowed under the constraint introduced here because it does not rewrite an X1 to an X0.  3.2 Structured Long-Distance Movement  The basic formulation of shallow-n grammars allows only the upper-level nonterminal category S to act within the glue rule. This can prevent some useful long-distance movement, as might be needed to translate Arabic sentences in Verb-Subject-Object order into English. It often happens that the initial Arabic verb requires long-distance movement, but the subject which follows can be translated in monotonic order. For instance, consider the following Romanized Arabic sentence:  TAlb  AlwzrA’ AlmjtmEyn Alywm fy dm$q  <lY ...  (CALLED) (the ministers) (gathered) (today) (in Damascus) (FOR) ...  where the verb ’TAlb’ must be translated into English so that it follows the translations of the ﬁve subsequent Arabic words ’AlwzrA’ AlmjtmEyn Alywm fy dm$q’, which  518  de Gispert et al.  Hierarchical Translation with WFSTs and Shallow-n Grammars  are themselves translated monotonically. A shallow-1 grammar cannot generate this movement except in the relatively unlikely case that the ﬁve words following the verb can be translated as a single phrase. A more powerful approach is to deﬁne grammars which allow low-level rules to form movable groups of phrases. Additional nonterminals {Mk} are introduced to allow successive generation of k nonterminals XN−1 in monotonic order for both languages, where K1 ≤ k ≤ K2. These act in the same manner as the glue rule does in the uppermost level. Applying Mk nonterminals at the N–1 level allows one hierarchical rule to perform a long-distance movement over the tree headed by Mk. We further reﬁne the deﬁnition of shallow-n grammars by specifying the allowable values of k for the successive productions of nonterminals XN−1. There are many possible ways to formulate and constrain these grammars. If K2 = 1, then the grammar is equivalent to the previous deﬁnition of shallow-n grammars, because monotonic production is only allowed by the glue rule of level N. If K1 = 1 and K2 > 1, then the search space deﬁned by the grammar is greater than the standard shallow-n grammar as it includes structured long-distance movement. Finally, if K1 > 1 then the search space is different from standard shallow-n as the n level is only used for long-distance movement. Introduction of Mk nonterminals redeﬁnes shallow-n grammars as: 1. the usual nonterminal S 2. a set of nonterminals {X0, . . . , XN} 3. a set of nonterminals {MK1 , . . . , MK2 } for K1 = 1, 2; K1 ≤ K2 4. two glue rules: S → XN,XN and S → S XN,S XN 5. hierarchical translation rules for level N: R: XN→ γ,α,∼ , γ, α ∈ {{MK1 , . . . , MK2 } ∪ T}+ with the requirement that α and γ contain at least one Mk 6. hierarchical translation rules for levels n = 1, . . . , N − 1: R: Xn→ γ,α,∼ , γ, α ∈ {{Xn−1} ∪ T}+ with the requirement that α and γ contain at least one Xn−1 7. translation rules which generate lexical phrases: R: X0→ γ,α , γ, α ∈ T+ 8. rules which generate k nonterminals XN−1: if K1 == 2 : R: Mk→ XN−1 Mk−1,XN−1 Mk−1 , for k = 3, . . . , K2 R: M2→ XN−1 XN−1,XN−1 XN−1,I if K1 == 1 : R: Mk→ XN−1 Mk−1,XN−1 Mk−1 , for k = 2, . . . , K2 R: M1→ XN−1,XN−1 where I denotes the identity function that enforces monotonocity in the nonterminals. For example, with a shallow-1 grammar, M3 leads to the monotonic production of three nonterminals X0, which leads to the production of three lexical phrase pairs; these can be moved with a hierarchical rule of level 1. This is graphically represented by the leftmost tree in Figure 11. With a shallow-2 grammar, M2 leads to the monotonic production of 519  Computational Linguistics  Volume 36, Number 3  Figure 11 Movement allowed by two grammars: shallow-1, with K1 = 1, K2 = 3 (left), and shallow-2, with K1 = 1, K2 = 3 (right). Both grammars allow movement of the bracketed term as a unit. Shallow-1 requires that translation within the object moved be monotonic whereas shallow-2 allows up to two levels of reordering. two nonterminals X1, a movement represented by the rightmost tree in Figure 11. This movement cannot be achieved with a shallow-1 grammar. 3.3 Minimum and Maximum Rule Span It is useful to deﬁne two parameters which further control the application of hierarchical translation rules in generating the search space. Parameters hmax and hmin specify the maximum and minimum height at which any hierarchical translation rule can be applied in the CYK grid. In other words, a hierarchical rule can only be applied in cell (x, y) if hmin≤ y ≤hmax. Note that these parameters can also be set independently for each nonterminal category. 3.4 Verb Movement Grammars for Arabic-to-English Translation Following the discussion which motivated this section, we wish to model movement of Arabic verbs when translating into English. We add to the shallow-n grammars a verb restriction so that the hierarchical translation rules (5) apply only if the source language string γ contains a verb. This encourages translations in which the Arabic verb is moved at the uppermost level N. 3.5 Grammars Used for SMT Experiments We now deﬁne the hierarchical grammars for the translation experiments which we describe next. Shallow-1,2,3 : Shallow-n grammars for n = 1, 2, 3. These grammars do not incorporate any monotonicity constraints, that is K1 = K2 = 1. Shallow-1, K1 = 1, K2 = 3 : hierarchical rules with one nonterminal can reorder a monotonic production of up to three target language phrases of level 0. 520  de Gispert et al.  Hierarchical Translation with WFSTs and Shallow-n Grammars  Shallow-1, K1 = 1, K2 = 3, vo : hierarchical rules with one nonterminal can reorder a monotonic catenation of up to three target language phrases of level 0, but only if one of the source terminals is tagged as a verb. Shallow-2, K1 = 2, K2 = 3, vo : two levels of reordering with monotonic production of up to three target language phrases of level 1, but only if one of the source terminals is tagged as a verb. 4. Translation Experiments In this section we report on hierarchical phrase-based translation experiments with WFSTs. We focus mainly on the NIST Arabic-to-English and Chinese-to-English translation tasks; some results for other language pairs are summarized in Section 4.6. Translation performance is evaluated using the BLEU score (Papineni et al. 2001) as implemented for the NIST 2009 evaluation.1 The experiments are organized as follows: - Lattice-based and cube pruning hierarchical decoding (Section 4.2) - Grammar conﬁgurations and search parameters and their effect on translation performance and processing time (Section 4.3) - Marginalization over translation derivations (Section 4.4) - Combining translation lattices obtained from alternative morphological decompositions of the input (Section 4.5) 4.1 Experimental Framework For Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08 (and MT09) Arabic Constrained Data track (∼150M words per language). In addition to reporting results on the MT08 set itself, we make use of a development set mt02-05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form a validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. For Chinese-to-English translation we use all available parallel text for the GALE 2008 evaluation;2 this is approximately 250M words per language. We report translation results on the NIST MT08 set, a development set tune-nw, and a validation set test-nw. These tuning and test sets contain translations produced by the GALE program and portions of the newswire sections of MT02 through MT05. The tune-nw set has 1,755 sentences, and test-nw set is similar. The parallel texts for both language pairs are aligned using MTTK (Deng and Byrne 2008). We extract hierarchical rules from the aligned parallel texts using the constraints developed by Chiang (2007). We further ﬁlter the extracted rules by count and pattern as described by Iglesias et al (2009a). The following features are extracted from the parallel data and used to assign scores to translation rules: source-to-target and targetto-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007).  
Aiti Aw† Institute for Infocomm Research Haizhou Li‡ Institute for Infocomm Research Linguistic knowledge plays an important role in phrase movement in statistical machine translation. To efﬁciently incorporate linguistic knowledge into phrase reordering, we propose a new approach: Linguistically Annotated Reordering (LAR). In LAR, we build hard hierarchical skeletons and inject soft linguistic knowledge from source parse trees to nodes of hard skeletons during translation. The experimental results on large-scale training data show that LAR is comparable to boundary word-based reordering (BWR) (Xiong, Liu, and Lin 2006), which is a very competitive lexicalized reordering approach. When combined with BWR, LAR provides complementary information for phrase reordering, which collectively improves the BLEU score signiﬁcantly. To further understand the contribution of linguistic knowledge in LAR to phrase reordering, we introduce a syntax-based analysis method to automatically detect constituent movement in both reference and system translations, and summarize syntactic reordering patterns that are captured by reordering models. With the proposed analysis method, we conduct a comparative analysis that not only provides the insight into how linguistic knowledge affects phrase movement but also reveals new challenges in phrase reordering. 1. Introduction The phrase-based approach is a widely accepted formalism in statistical machine translation (SMT). It segments the source sentence into a sequence of phrases (not necessarily syntactic phrases), then translates and reorders these phrases in the target. The reason for the popularity of phrasal SMT is its capability of non-compositional translations and ∗ 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: dyxiong@i2r.a-star.edu.sg. ∗∗ 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: mzhang@i2r.a-star.edu.sg. † 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: aaiti@i2r.a-star.edu.sg. ‡ 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: hli@i2r.a-star.edu.sg. Submission received: 24 October 2008; revised submission received: 12 March 2010; accepted for publication: 21 April 2010. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 3  local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models signiﬁcantly outperform the distortion-based reordering model as reported, only using lexical information (e.g., boundary words) is not adequate to move phrases to appropriate positions. Consider the following Chinese example with its English translation:  [VP [PP (while) (develop) (related) (legislation) ] [VP [VV  (consider)] [NP [DNP [NP (results)]]]]1  (this)  (referendum)] [DEG (of)]] [NP  consider the results of this referendum while developing related legislation  In this example, boundary words and are able to decide that the translation of the PP phrase ... should be postponed until some phrase that succeeds it is translated. But they cannot provide further information about exactly which succeeding phrase should be translated ﬁrst. If high-level linguistic knowledge, such as the syntactic context VP→PP VP, is given, the position of the PP phrase can be easily determined since the pre-verbal modiﬁer PP in Chinese is frequently translated into a post-verbal counterpart in English. In this article, we focus on linguistically motivated phrase reordering, which integrates high-level linguistic knowledge in phrase reordering. We adopt a two-step strategy. In the ﬁrst step, we establish a hierarchical skeleton in phrasal SMT by incorporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT. In the second step, we inject soft linguistic information into nodes of the skeleton. There are two signiﬁcant advantages to using BTG in phrasal SMT. First, BTG is able to generate hierarchical structures.2 This not only enhances phrasal SMT’s capability for hierarchical and long-distance reordering but also establishes a platform for phrasal SMT to incorporate knowledge from linguistic structure. Second, phrase reordering is restricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight or inverted) of nodes in any binary branching structure, it is broadly veriﬁed that the ITG constraint has good coverage of word reorderings on various language pairs (Wu, Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable task. After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject soft linguistic information into the nodes of the skeleton. We annotate each BTG node  
Long queries often suffer from low recall in Web search due to conjunctive term matching. The chances of matching words in relevant documents can be increased by rewriting query terms into new terms with similar statistical properties. We present a comparison of approaches that deploy user query logs to learn rewrites of query terms into terms from the document space. We show that the best results are achieved by adopting the perspective of bridging the “lexical chasm” between queries and documents by translating from a source language of user queries into a target language of Web documents. We train a state-of-the-art statistical machine translation model on query-snippet pairs from user query logs, and extract expansion terms from the query rewrites produced by the monolingual translation system. We show in an extrinsic evaluation in a real-world Web search task that the combination of a query-to-snippet translation model with a query language model achieves improved contextual query expansion compared to a state-ofthe-art query expansion model that is trained on the same query log data. 1. Introduction Information Retrieval (IR) applications have been notoriously resistant to improvement attempts by Natural Language Processing (NLP). With a few exceptions for specialized tasks,1 the contribution of part-of-speech taggers, syntactic parsers, or ontologies of nouns or verbs has been inconclusive. In this article, instead of deploying NLP tools or ontologies, we apply NLP ideas to IR problems. In particular, we take a viewpoint that looks at the problem of the word mismatch between queries and documents in Web search as a problem of translating from a source language of user queries into a target language of Web documents. We concentrate on the task of query expansion by query rewriting. This task consists of adding expansion terms with similar statistical properties to the original query in order to increase the chances of matching words in relevant documents, and also to decrease the ambiguity of the query that is inherent in natural language. We focus on a comparison of models that learn to generate query ∗ Brandschenkestrasse 110, 8002 Zu¨ rich, Switzerland. E-mail: riezler@gmail.com. ∗∗ 1600 Amphitheatre Parkway, Mountain View, CA. E-mail: yliu@google.com. 
Few archaeological ﬁnds are as evocative as artifacts inscribed with symbols. Whenever an archaeologist ﬁnds a potsherd or a seal impression that seems to have symbols scratched or impressed on the surface, it is natural to want to “read” the symbols. And if the symbols come from an undeciphered or previously unknown symbol system it is common to ask what language the symbols supposedly represent and whether the system can be deciphered. Of course the ﬁrst question that really should be asked is whether the symbols are in fact writing. A writing system, as linguists usually deﬁne it, is a symbol system that is used to represent language. Familiar examples are alphabets such as the Latin, Greek, Cyrillic, or Hangul alphabets, alphasyllabaries such as Devanagari or Tamil, syllabaries such as Cherokee or Kana, and morphosyllabic systems like Chinese characters. But symbol systems that do not encode language abound: European heraldry, mathematical notation, labanotation (used to represent dance), and Boy Scout merit badges are all examples of symbol systems that represent things, but do not function as part of a system that represents language. Whether an unknown system is writing or not is a difﬁcult question to answer. It can only be answered deﬁnitively in the afﬁrmative if one can develop a veriﬁable decipherment into some language or languages. Statistical techniques have been used in decipherment for years, but these have always been used under the assumption that the system one is dealing with is writing, and the techniques are used to uncover patterns or regularities that might aid in the decipherment. Patterns of symbol distribution might suggest that a symbol system is not linguistic: For example, odd repetition patterns might make it seem that a symbol system is unlikely to be writing. But until recently nobody had argued that statistical techniques could be used to determine that a system is linguistic.1 It was therefore quite a surprise when, in April 2009, there appeared in Science a short article by Rajesh Rao of the University of Washington and colleagues at two research institutes in India that purported to provide such a measure (Rao et al. 2009a). Rao et al.’s claim, which we will describe in more detail in the next section, was that ∗ Center for Spoken Language Understanding, Oregon Health & Science University, 20000 NW Walker Rd, Beaverton, OR, 97006, USA. E-mail: rws@xoba.com. 
Johanna D. Moore† University of Edinburgh Generating responses that take user preferences into account requires adaptation at all levels of the generation process. This article describes a multi-level approach to presenting user-tailored information in spoken dialogues which brings together for the ﬁrst time multi-attribute decision models, strategic content planning, surface realization that incorporates prosody prediction, and unit selection synthesis that takes the resulting prosodic structure into account. The system selects the most important options to mention and the attributes that are most relevant to choosing between them, based on the user model. Multiple options are selected when each offers a compelling trade-off. To convey these trade-offs, the system employs a novel presentation strategy which straightforwardly lends itself to the determination of information structure, as well as the contents of referring expressions. During surface realization, the prosodic structure is derived from the information structure using Combinatory Categorial Grammar in a way that allows phrase boundaries to be determined in a ﬂexible, data-driven fashion. This approach to choosing pitch accents and edge tones is shown to yield prosodic structures with signiﬁcantly higher acceptability than baseline prosody prediction models in an expert evaluation. These prosodic structures are then shown to enable perceptibly more natural synthesis using a unit selection voice that aims to produce the target tunes, in comparison to two baseline synthetic voices. An expert evaluation and f0 analysis conﬁrm the superiority of the generator-driven intonation and its contribution to listeners’ ratings. 1. Introduction In an evaluation of nine spoken dialogue information systems, developed as part of the DARPA Communicator program, the information presentation phase of the dialogues ∗ 1712 Neil Ave., Columbus, OH 43210, USA. Web: http://www.ling.ohio-state.edu/~mwhite/. ∗∗ 10 Crichton Street, Edinburgh, Scotland EH8 1AB, UK. Web: http://www.cstr.ed.ac.uk/ssi/people/robert.html. † 10 Crichton Street, Edinburgh, Scotland EH8 1AB, UK. Web: http://www.hcrc.ed.ac.uk/~jmoore/. Submission received: 31 January 2008; revised submission received: 19 May 2009; accepted for publication: 24 September 2009. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 2  Figure 1 Typical information presentation phase of a Communicator dialogue. was found to be the primary contributor to dialogue duration (Walker, Passonneau, and Boland 2001). During this phase, the typical system sequentially presents the set of options that match the user’s constraints, as shown in Figure 1. The user can then navigate through these options and reﬁne them by offering new constraints. When multiple options are returned, this process can be exacting, leading to reduced user satisfaction. As Walker et al. (2004) observe, having to access the set of available options sequentially makes it hard for the user to remember information relevant to making a decision. To reduce user memory load, we need alternative strategies for sequential presentation. In particular, we require better algorithms for: 1. selecting the most relevant subset of options to mention, as well as the attributes that are most relevant to choosing among them; and 2. determining how to organize and express the descriptions of the selected options and attributes, in ways that are both easy to understand and memorable.1 In this article, we describe how we have addressed these points in the FLIGHTS2 system, reviewing and extending the description given in Moore et al. (2004). FLIGHTS follows previous work (Carberry, Chu-Carroll, and Elzer 1999; Carenini and Moore 2000; Walker et al. 2002) in applying decision-theoretic models of user preferences to the generation of tailored descriptions of the most relevant available options. Multiattribute decision theory provides a detailed account of how models of user preferences can be used in decision making (Edwards and Barron 1994). Such preference models have been shown to enable systems to present information in ways that are concise and 
Readability assessment is an important NLP issue with much application in the domain of language education. The capability to automatically judge the readability of a text would greatly help language teachers and learners, who currently spend a great deal of time skimming through texts looking for a text at an appropriate reading level. Substantial previous work has been done over the past decades (Klare 1963, DuBay 2004a, 2004b). Early work generated measures based on simple text statistics by assuming that these reﬂect the text reading level. For example, Kincaid, Fishburne, and Rodgers (1975) assumed that the lengths of words and sentences represent their respective difﬁculty. Chall and Dale (1995) used a manually constructed list of words assumed to capture the difﬁculty of vocabulary. These measures are easy to use but difﬁcult to apply to languages other than English, because some features, such as word length, are speciﬁc to alphabetic writing. Such methods, however, do not compete with recent methods based on more sophisticated handling of language statistics. CollinsThompson and Callan (2004) proposed a classiﬁcation model by constructing different language models for different school grades (Si and Callan 2001), and Schwarm and Ostendorf (2005) applied a support vector machine (SVM). Both of these methods outperform classical methods and are less language-dependent. These new methods, however, have a serious problem when implementation is attempted for multiple languages: the lack of training corpora. Large amounts of training data annotated with 12 school grades have not been at all easy to obtain on a reasonable scale. Another possibility might have been to manually construct such training data, ∗ University of Tokyo Cross Field, 13F Akihabara Daibiru, 1-18-13 Kanda Chiyoda-ku, Tokyo, Japan. E-mail: kumiko@kumish.net. Submission received: 7 October 2008; revised submission received: 17 September 2009; accepted for publication: 19 December 2009. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 2  but humans are generally unable to precisely judge the level of a given text among 12 arbitrary levels. The corpora therefore have to be constructed from academic texts used in schools. The amount of such data, however, is limited, and its use is usually strictly limited by copyrights. Thus, it is crucial to devise a new method or approach that allows readability assessment by using only generally available corpora, such as newspapers. Given a single text, it is hard to attribute an absolute readability level from among 12 levels, but given two texts, there should be a better chance of judging which of them is more difﬁcult. This intuition led to the new model presented in this article. Our idea is based on sorting, which is implemented in two stages: r A comparator is generated by an SVM. This comparator judges the relative readability of two given texts. r Given a set of texts, the texts are sorted by the comparator with a sorting algorithm. In our case, we used a robust binary insertion sort, as explained in further detail later in this article. The ﬁrst step requires a training corpus, but because the comparator only judges which of two texts is more difﬁcult, the texts of a corpus need only be labeled according to two different levels. Two sets of texts—one difﬁcult, the other easy—are far easier to obtain than a training corpus annotated for 12 different levels. Overall, our model of readability thus differs from previous regression or classiﬁcation models. Applying this new method, we also present an application, called Terrace, which is a system that retrieves a text with readability similar to that of a given input text. Terrace was originally motivated by a faculty request made by teachers of multiple foreign languages. The system currently works for English and Japanese, and the languages will be extended to include Chinese and French. Note that we do not claim that our model and method is better than existing methods. Although our method does compete well with previous methods, the classiﬁcation approach used in any given scenario should remain the most natural, relevant method. The intention of this article is simply to propose an alternative way of handling readability assessment, especially when adequate training corpora annotated with multiple levels are not available. 2. Related Work Readability, in general, describes the ease with which a text document can be read and understood. Readability is studied in at least two different domains, those of coherence (Barzilay and Lapata 2008) and language learning. Readability in this article signiﬁes the latter, for both a mother tongue and a second language. Even within this domain, substantial previous work has been done (Klare 1963; DuBay 2004a, 2004b). DuBay (2004a) writes that: By the 1980s, there were 200 formulas and over a thousand studies published on the readability formulas attesting to their strong theoretical and statistical validity (p. 2). Every method of readability assessment extracts some features from a text and maps the feature space to some readability norm. There are the two viewpoints regarding features and the mapping of feature values to readability, and correspondingly there are two kinds of work in this domain. 204  Tanaka-Ishii, Tezuka, and Terada  Sorting Texts by Readability  Regarding the ﬁrst type, many researchers have reported how various features affect the readability of text in terms of vocabulary, syntax, and discourse relations. Recently, Pitler and Nenkova (2008) presented an impressive veriﬁcation of the effects of each kind of feature and found that vocabulary and discourse relations are prominent, although other features are not negligible. The focus of the current work, however, is not on what feature set to consider, so we use the same features throughout the article, as explained further in Section 3.1. Rather, the focus of this article is on mapping the extracted feature values to a readability norm. So far, two models have been used for this: regression and classiﬁcation. In regression, readability is given by a score based on a linearly weighted sum of feature values. Early methods, from the Wannetka formula (Washburne and Vogel 1928), to the recent methods of Flesch–Kincaid (Kincaid, Fishburne, and Rodgers 1975) and Dale–Chall (Chall and Dale 1995), are of this kind. Elaboration of such regression methods in a more modern context could proceed through a generalized linear model based on estimation of the weights by machine learning, although we have not found such an approach within the literature of readability assessment for language learning. Our proposal is compared with such an enhanced version of regression in Section 8. In classiﬁcation, readability is segmented by academic grades, and the assessment is conducted as a classiﬁcation task. The ﬁrst is implemented by means of statistical classiﬁcation modeling, as reported in Collins-Thompson and Callan (2004) and Si and Callan (2001). The authors used a language model (unigrams) and a naive Bayes classiﬁer by presuming different language models for each reading level. A language model Mi is constructed for each level of readability i by using different corpora for each level. The readability of a given text T is assessed using the formula L(Mi|T) = Σw∈TC(w) log Pr(w|Mi), where w denotes a word in text T, C(w) denotes the frequency of w, and Pr(w|Mi) denotes the probability of w under Mi. The second is based on an SVM (Schwarm and Ostendorf 2005) and the authors also studied the effect of statistical features, such as n-grams and syntactic features. In these papers, the readability norms are represented by means of scores and classes of readability. That is, given a single text, the system assigns a value corresponding to a school grade. The result is easy to understand, and various applications have been constructed with this type of scoring. This solution only works, however, when a sufﬁcient amount of training data with annotations regarding multiple levels is provided. Usually, the availability of training data in readability assessment is limited, even for school grading. This is due to the inherent difﬁculty of classifying the readability of a text into 12 grades, making it difﬁcult to uniformly construct a large set of training data. Moreover, the copyright issue is more serious for academic texts.1 Given this situation, when readability assessment is modeled by regression or classiﬁcation, a research team wanting to apply these previous methods faces the problem of assembling training data, as we did for over a year. In this article, the readability norm is designed in a completely different way: Given two texts, a comparator judges which is more difﬁcult. By applying this comparator, a set of texts is sorted. The readability of a text is assessed by searching for its position  
Nelleke Oostdijk† Radboud University Nijmegen Peter-Arno Coppen‡ Radboud University Nijmegen While developing an approach to why-QA, we extended a passage retrieval system that uses offthe-shelf retrieval technology with a re-ranking step incorporating structural information. We get signiﬁcantly higher scores in terms of MRR@150 (from 0.25 to 0.34) and success@10. The 23% improvement that we reach in terms of MRR is comparable to the improvement reached on different QA tasks by other researchers in the ﬁeld, although our re-ranking approach is based on relatively lightweight overlap measures incorporating syntactic constituents, cue words, and document structure. 1. Introduction About 5% of all questions asked to QA systems are why-questions (Hovy, Hermjakob, and Ravichandran 2002). Why-questions need a different approach than factoid questions, because their answers are explanations that usually cannot be stated in a single phrase. Recently, research (Verberne 2006; Higashinaka and Isozaki 2008) has been directed at QA for why-questions (why-QA). In earlier work on answering why-questions on the basis of Wikipedia, we found that the answers to most why-questions are passages of text that are at least one sentence and at most one paragraph in length (Verberne et al. 2007b). Therefore, we aim at developing a system that takes as input a whyquestion and gives as output a ranked list of candidate answer passages. In the current article, we propose a three-step setup for a why-QA system: (1) a question-processing module that transforms the input question to a query; (2) an offthe-shelf retrieval module that retrieves and ranks passages of text that share content ∗ Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands. E-mail: s.verberne@let.ru.nl. ∗∗ Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands. E-mail: l.boves@let.ru.nl. † Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands. E-mail: n.oostdijk@let.ru.nl. ‡ Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands. E-mail: p.a.coppen@let.ru.nl. Submission received: 30 July 2008; revised submission received: 18 February 2009; accepted for publication: 4 September 2009. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 2  with the input query; and (3) a re-ranking module that adapts the scores of the retrieved passages using structural information from the input question and the retrieved passages. In the ﬁrst part of this article, we focus on step 2, namely, passage retrieval. The classic approach to ﬁnding passages in a text collection that share content with an input query is retrieval using a bag-of-words (BOW) model (Salton and Buckley 1988). BOW models are based on the assumption that text can be represented as an unordered collection of words, disregarding grammatical structure. Most BOW-based models use statistical weights based on term frequency, document frequency, passage length, and term density (Tellex et al. 2003). Because BOW approaches disregard grammatical structure, systems that rely on a BOW model have their limitations in solving problems where the syntactic relation between words or word groups is crucial. The importance of syntax for QA is sometimes illustrated by the sentence Ruby killed Oswald, which is not an answer to the question Who did Oswald kill? (Bilotti et al. 2007). Therefore, a number of researchers in the ﬁeld investigated the use of structural information on top of a BOW approach for answer retrieval and ranking (Tiedemann 2005; Quarteroni et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2008). These studies show that although the BOW model makes the largest contribution to the QA system results, adding structural (syntactic information) can give a signiﬁcant improvement. In the current article, we hypothesize that for the relatively complex problem of why-QA, a signiﬁcant improvement—at least comparable to the improvement gained for factoid QA—can be gained from the addition of structural information to the ranking component of the QA system. We ﬁrst evaluate a passage retrieval system for why-QA based on standard BOW ranking (step 1 and 2 in our set-up). Then we perform an analysis of the strengths and weaknesses of the BOW model for retrieving and ranking candidate answers. In view of the observed weaknesses of the BOW model, we choose our feature set to be applied to the set of candidate answer passages in the re-ranking module (step 3 in our set-up). The structural features that we propose are based on the idea that some parts of the question and the answer passage are more important for relevance ranking than other parts. Therefore, our re-ranking features are overlap-based: They tell us which parts of a why-question and its candidate answers are the most salient for ranking the answers. We evaluate our initial and adapted ranking strategies using a set of why-questions and a corpus of Wikipedia documents, and we analyze the contribution of both the BOW model and the structural features. The main contributions of this article are: (1) we address the relatively new problem of why-QA and (2) we analyze the contribution of overlap-based structural information to the problem of answer ranking. The paper is organized as follows. In Section 2, related work is discussed. Section 3 presents the BOW-based passage retrieval method for why-QA, followed by a discussion of the strengths and weaknesses of the approach in Section 4. In Section 5, we extend our system with a re-ranking component based on structural overlap features. A discussion of the results and our conclusions are presented in Sections 6 and 7, respectively. 2. Related Work We distinguish related work in two directions: research into the development of systems for why-QA (Section 2.1), and research into combining structural and BOW features for QA (Section 2.2). 230  Verberne et al.  What Is Not in the Bag of Words for Why-QA?  2.1 Research into Why-QA In related work (Verberne et al. 2007a), we focused on selecting and ranking explanatory passages for why-QA with the use of rhetorical structures. We developed a system that employs the discourse relations in a manually annotated document collection: the RST Treebank (Carlson, Marcu, and Okurowski 2003). This system matches the input question to a text span in the discourse tree of the document and it retrieves as answer the text span that has a speciﬁc discourse relation to this question span. We evaluated our method on a set of 336 why-questions formulated to seven texts from the WSJ corpus. We concluded that discourse structure can play an important role in why-QA, but that systems relying on these structures can only work if candidate answer passages have been annotated with discourse structure. Automatic parsers for creating full rhetorical structures are currently unavailable. Therefore, a more practical approach appears to be necessary for work in why-QA, namely, one which is based on automatically created annotations. Higashinaka and Isozaki (2008) focus on the problem of ranking candidate answer paragraphs for Japanese why-questions. They assume that a document retrieval module has returned the top 20 documents for a given question. They extract features for content similarity, causal expressions, and causal relations from two annotated corpora and a dictionary. Higashinaka and Isozaki evaluate their ranking method using a set of 1,000 why-questions that were formulated to a newspaper corpus by a text analysis expert. 70.3% of the reference answers for these questions are ranked in the top 10 by their system, and MRR1 was 0.328. Although the approach of Higashinaka and Isozaki is very interesting, their evaluation collection has the same ﬂaw as the one used by Verberne et al. (2007a): Both collections consist of questions formulated to a pre-selected answer text. Questions elicited in response to newspaper texts tend to be unrepresentative of questions asked in a real QA setting. In the current work, therefore, we work with a set of questions formulated by users of an online QA system (see Section 3.1).  2.2 Combining Structural and Bag-of-Words Features for QA Tiedemann (2005) investigates syntactic information from dependency structures in passage retrieval for Dutch factoid QA. He indexes his corpus at different text layers (BOW, part-of-speech, dependency relations) and uses the same layers for question analysis and query creation. He optimizes the query parameters for the passage retrieval task by having a genetic algorithm apply the weights to the query terms. Tiedemann ﬁnds that the largest weights are assigned to the keywords from the BOW layer and to the keywords related to the predicted answer type (such as ‘person’). The baseline approach, using only the BOW layer, gives an MRR of 0.342. Using the optimized IR settings with additional layers, MRR improves to 0.406. Quarteroni et al. (2007) consider the problem of answering deﬁnition questions. They use predicate–argument structures (PAS) for improved answer ranking. They ﬁnd that PAS as a stand-alone representation is inferior to parse tree representations, but that together with the BOW it yields higher accuracy. Their results show a signiﬁcant  
Kevin Knight† USC/Information Sciences Institute Daniel Marcu‡ Language Weaver, Inc. This article shows that the structure of bilingual material from standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems. We present three modiﬁcations to the MT training data to improve the accuracy of a state-of-theart syntax MT system: re-structuring changes the syntactic structure of training parse trees to enable reuse of substructures; re-labeling alters bracket labels to enrich rule application context; and re-aligning uniﬁes word alignment across sentences to remove bad word alignments and reﬁne good ones. Better structures, labels, and word alignments are learned by the EM algorithm. We show that each individual technique leads to improvement as measured by BLEU, and we also show that the greatest improvement is achieved by combining them. We report an overall 1.48 BLEU improvement on the NIST08 evaluation set over a strong baseline in Chinese/English translation. 1. Background Syntactic methods have recently proven useful in statistical machine translation (SMT). In this article, we explore different ways of exploiting the structure of bilingual material for syntax-based SMT. In particular, we ask what kinds of tree structures, tree labels, and word alignments are best suited for improving end-to-end translation accuracy. We begin with structures from standard parsing and alignment tools, then use the EM algorithm to revise these structures in light of the translation task. We report an overall +1.48 BLEU improvement on a standard Chinese-to-English test. ∗ 6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: wwang@languageweaver.com. ∗∗ 4676 Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: jonmay@isi.edu. † 4676 Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: knight@isi.edu. ‡ 6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: dmarcu@languageweaver.com. Submission received: 6 November 2008; revised submission received: 10 September 2009; accepted for publication: 1 January 2010. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 2  We carry out our experiments in the context of a string-to-tree translation system. This system accepts a Chinese string as input, and it searches through a multiplicity of English tree outputs, seeking the one with the highest score. The string-to-tree framework is motivated by a desire to improve target-language grammaticality. For example, it is common for string-based MT systems to output sentences with no verb. By contrast, the string-to-tree framework forces the output to respect syntactic requirements—for example, if the output is a syntactic tree whose root is S (sentence), then the S will generally have a child of type VP (verb phrase), which will in turn contain a verb. Another motivation is better treatment of function words. Often, these words are not literally translated (either by themselves or as part of a phrase), but rather they control what happens in the translation, as with case-marking particles or passive-voice particles. Finally, much of the re-ordering we ﬁnd in translation is syntactically motivated, and this can be captured explicitly with syntax-based translation rules. Tree-to-tree systems are also promising, but in this work we concentrate only on target-language syntax. The target-language generation problem presents a difﬁcult challenge, whereas the source sentence is ﬁxed and usually already grammatical. To prepare training data for such a system, we begin with a bilingual text that has been automatically processed into segment pairs. We require that the segments be single sentences on the English side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the English side of the bilingual data. This model assigns a probability P(e) to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training data. Second, we build a syntax-based translation model that we can use to produce candidate English trees from Chinese strings. Following previous work in noisy-channel  Figure 1 A sample learning case for the syntax-based machine translation system described in this article. 248  Wang et al.  Re-structuring, Re-labeling, and Re-aligning  SMT (Brown et al. 1993), our model operates in the English-to-Chinese direction—  we envision a generative top–down process by which an English tree is gradually  transformed (by probabilistic rules) into an observed Chinese string. We represent a  collection of such rules as a tree transducer (Knight and Graehl 2005). In order to  construct this transducer from parsed and word-aligned data, we use the GHKM rule  extraction algorithm of Galley et al. (2004). This algorithm computes the unique set of  minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the  minimal rules extracted from the example (tree, string, alignment) triple in Figure 1.  Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and  deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example  form a derivation tree.  We collect all rules over the entire bilingual corpus, and we normalize rule counts  in  this  way:  P(rule)  =  count(rule) count(LHS-root(rule))  .  When  we  apply  these  probabilities  to  derive  an  English sentence e and a corresponding Chinese sentence c, we wind up computing the  joint probability P(e, c). We smooth the rule counts with Good–Turing smoothing (Good  1953).  This extraction method assigns each unaligned Chinese word to a default rule in  the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese  words to participate in multiple translation rules. In this case, we obtain a derivation  forest of minimal rules. Galley et al. show how to use EM to count rules over deriva-  tion forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley  et al. in collecting composed rules, namely, compositions of minimal rules. These larger  rules have been shown to substantially improve translation accuracy (Galley et al. 2006;  DeNeefe et al. 2007). Figure 3 shows some of the additional rules.  With these models, we can decode a new Chinese sentence by enumerating and  scoring all of the English trees that can be derived from it by rule. The score is a  weighted product of P(e) and P(e, c). To search efﬁciently, we employ the CKY dynamic-  programming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This  algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix,  we store the non-terminal symbol at the root of the English tree being built up. We also  Figure 2 Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure. 249  Computational Linguistics  Volume 36, Number 2  Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transducer rules must be broken down, or binarized, into rules that contain at most two variables—more efﬁcient search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington 2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further efﬁciency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The ﬁnal score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reﬂected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this article, we take a broad view and investigate how changes to syntactic structures, syntactic labels, and word alignments can lead to substantial improvements in translation quality on top of a strong baseline. We design our methods around problems that arise in MT data whose parses and alignments use some Penn Treebankstyle annotations. We believe that some of the techniques will apply to other annotation schemes, but conclusions here are limited to Penn Treebank-style trees. The rest of this article is structured as follows. Section 2 describes the corpora and model conﬁgurations used in our experiments. In each of the next three sections we present a technique for modifying the training data to improve syntax MT accuracy: tree re-structuring in Section 3, tree re-labeling in Section 4, and re-aligning in Section 5. In each of these three sections, we also present experiment results to show the impact of each individual technique on end-to-end MT accuracy. Section 6 shows the improvement made by combining all three techniques. We conclude in Section 7. 
Sometimes I am amazed by how much the ﬁeld of computational linguistics has changed in the past 15 to 20 years. In the mid 1990s, I was working at a research institute where language and speech technologists worked in relatively close quarters. Speech technology seemed on the verge of a major breakthrough; this was around the time that Bill Gates was quoted in Business Week as saying that speech was not just the future of Windows, but the future of computing itself. At the same time, language technology was, well, nowhere. Bill Gates certainly wasn’t championing language technology in those days. And while the possible applications of speech technology seemed endless (who would use a keyboard in 2010, when speech-driven user interfaces would have replaced traditional computers?), the language people were thinking hard about possible applications for their admittedly somewhat immature technologies. Predicting the future is a tricky thing. No major breakthrough came for speech technology—I am still typing this. However, language technology did change almost beyond recognition. Perhaps one of the main reasons for this has been the explosive growth of the Internet, which helped language technology in two different ways. On the one hand it instigated the development and reﬁnement of techniques needed for searching in document collections of unprecedented size, on the other it resulted in a large increase of freely available text data. Recently, language technology has been particularly successful for tasks where huge amounts of textual data is available to which statistical machine learning techniques can be applied (Halevy, Norvig, and Pereira 2009). As a result of these developments, mainstream computational linguistics is now a successful, application-oriented discipline which is particularly good at extracting information from sequences of words. But there is more to language than that. For speakers, words are the result of a complex speech production process; for listeners, they are what starts off the similarly complex comprehension process. However, in many current applications no attention is given to the processes by which words are produced nor to the processes by which they can be understood. Language is treated as a product not as a process, in the terminology of Clark (1996). In addition, we use language not only as a vehicle for factual information exchange; speakers may realise all sorts of other intentions with their words: They may want to convince others to do or buy something, they may want to induce a particular emotion in the addressee, and so forth. These days, most of computational linguistics (with a few notable exceptions, more about which subsequently) has little to ∗ Tilburg Centre for Creative Computing (TiCC), Communication and Cognition research group, Tilburg University, The Netherlands. E-mail: e.j.krahmer@uvt.nl. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 2  say about how people produce and comprehend language, nor about what the possible effects of language could be. It wasn’t always like this; early work in computational linguistics took a different (and more ambitious) perspective. Winograd (1980), to give one more or less random example, explicitly treated language understanding and production as cognitive processes, which interacted with other cognitive modules such as visual perception; Hovy (1990), to give another, presented a computational model that generated different texts from the same underlying facts, depending on pragmatic, interpersonal constraints. It is interesting to observe that Winograd and Hovy built on both computational and psychological research, something which is increasingly rare in the ﬁeld of computational linguistics, a point made convincingly by Reiter (2007). By now, it is generally accepted that the problems that Winograd, Hovy, and others tried to tackle are very complex, and that the current emphasis on more well-delimited problems is probably a good thing. However, it is not difﬁcult to come up with computational applications for which a better understanding would be required of language as a process and the effects language may have on a user (interactive virtual agents which try to persuade a user to do something, for example). To learn more about how speakers and addressees manage to accurately produce and comprehend complex and potentially ambiguous sentences in real time, and how they may use these sentences for a whole range of intentions, we have to turn to psycholinguistics and social psychology, respectively. So let us sample some of the recent ﬁndings in these ﬁelds, and see if and how they might beneﬁt computational linguistics. Interestingly, we will ﬁnd many places where more attention to what goes on in computational linguistics would beneﬁt psychologists as well. 2. Language Use and Its Social Impact Social psychologists study persons and the relations they have with others and with groups. Various social psychologists have concentrated on language (although perhaps not as many as you would expect given the importance of language for social interactions). A number of different approaches can be discerned, one of which concentrates on the psychological functions of function words (Chung and Pennebaker 2007). Function words are understood here to include pronouns, prepositions, articles, conjunctions, and auxiliary verbs.  2.1 On the Psychological Functions of Pronouns One reliable ﬁnding of this perspective is that ﬁrst person singular pronouns are associated with negative affective states. For example, in one study it was found that currently depressed students used I and me more often than students who were not currently depressed, and of the latter group those who had known periods of depression used them more frequently than those who had never had such an episode (Rude, Gortner, and Pennebaker 2004). Another study found that suicidal poets used ﬁrst person singular pronouns in their poems more frequently than non-suicidal poets (Stirman and Pennebaker 2001). Of course, whether a speaker tends to use I or we more frequently is also indicative of self- versus other-centeredness. An analysis of blogs following the events of September 11 revealed that bloggers’ use of I and me dropped in the hours following the attack, while simultaneously their use of we and us increased (Cohn, Mehl, and Pennebaker 2004); this switch is interpreted by the authors as indicating that people 286  Krahmer  Last Words  were focusing less on themselves during this period, but instead focusing more on their friends and families. In a completely different study of adult speakers (both male and female) who underwent testosterone therapy, it was found that as testosterone levels dropped, so did their use of I pronouns, while simultaneously the use of non-I pronouns increased (Pennebaker et al. 2004). Pennebaker and colleagues report comparable effects of age, gender, status, and culture on personal pronoun use (Chung and Pennebaker 2007). Their corpus (or “archive”, as they call it) contains over 400,000 text ﬁles, from many different authors and collected over many years. It is interesting to observe that Pennebaker was an early adapter of computers for his analyses, simply because performing them manually was too timeconsuming. The general approach in these analyses is to determine beforehand what the “interesting” words are and then simply to count them in the relevant texts, without taking the linguistic context into account. This obviously creates errors: The relative frequency of ﬁrst-person pronouns may be indicative of depression, as we have just seen, but a sentence such as I love life seems a somewhat implausible cue for a depressed state of mind. Chung and Pennebaker (2007, page 345) themselves give the example of mad, which is counted as an anger and negative emotion word, and they point out that this is wrong for I’m mad about my lover. Clearly, standard methods from computational linguistics could be used to address this problem, for instance by looking at words in context and n-grams. Another problem which Chung and Pennebaker mention, and which will be familiar to many computational linguists, is the problem of deciding which are the interesting words to count. Here techniques such as feature construction and selection could be of help. As I will argue in what follows, the observations of Pennebaker and colleagues are potentially interesting for computational linguistics as well, but let us ﬁrst look at another relevant set of psychological ﬁndings.  2.2 On the Psychological Functions of Interpersonal Language A different strand of research on language and social psychology focuses on interpersonal verbs (a subset of what computational linguists more commonly refer to as transitive verbs): verbs which express relations between people (Semin 2009). In their model of interpersonal language (the Linguistic Categorization Model), Semin and Fiedler (1988) make a distinction between different kinds of verbs and their position on the concrete–abstract dimension. Descriptive action verbs (Romeo kisses Juliet) are assumed to be the most concrete, since they refer to a single, observable event. This is different for state verbs (Romeo loves Juliet), which describe psychological states instead of single perceptual events, and are therefore more abstract. Most abstract, according to Semin and Fiedler, are adjectives (Romeo is romantic), because they generalize over speciﬁc events and objects and only refer to characteristics of the subject. The thing to note is that the same event can, in principle, be referred to in all these different forms; a speaker has the choice of using a more concrete or a more abstract way to refer to an event (e.g., John can be described as, from more to less concrete, hitting a person, hating a person, or being aggressive). Interestingly, the abstractness level a speaker opts for tells us something about that speaker. This has been found, for instance, in the communication of ingroup (think of people with the same cultural identity or supporting the same soccer team) and outgroup (different identity, different team) behavior. There is considerable evidence that speakers describe negative ingroup and positive outgroup behavior in more concrete terms (e.g., using action verbs), thereby indicating that the behavior is more incidental, whereas positive ingroup 287  Computational Linguistics  Volume 36, Number 2  and negative outgroup behaviors are described in relatively abstract ways (e.g., more frequently using adjectives), suggesting a more enduring characteristic (see, e.g., Maass et al. 1989). Maass and colleagues showed this phenomenon, which they dubbed the Linguistic Intergroup Bias, for different Contrada (neighborhoods) participating in the famous Palio di Siena horse races, reporting about their own performance and that of the other neighborhoods. Moreover, Wigboldus, Semin, and Spears (2000) have shown that addressees do indeed pick up these implications, and Douglas and Sutton (2006) reveal that speakers who describe the behavior of others in relatively abstract terms are perceived to have biased attitudes and motives as opposed to speakers who describe this behavior in more concrete ways. It has been argued that concrete versus abstract language is not only relevant for, for example, the communication of stereotypes, but also has more fundamental effects, for instance inﬂuencing the way people perceive the world (Stapel and Semin 2007). In a typical experiment, Stapel and Semin ﬁrst subtly prime participants with either abstract or concrete language. This can be done using scrambled sentences, where participants are given four words (romantic is lamp Romeo) with the instruction to form a grammatical sentence from three of them, or by giving participants a word-search puzzle where the words to search for are the primes. After this, participants perform a seemingly unrelated task, where their perceptual focus (either on the global picture or on the details) is measured. Stapel and Semin show that processing abstract language (adjectives) results in a more global perception, whereas processing concrete language (descriptive action verbs) leads to more perceptual attention to details. At this point, a computational linguist (and probably other linguists as well) might start to wonder about the comparison between verbs and adjectives, and by the claim that adjectives are abstract. What about adjectives like blonde, young, and thin? These seem to be much more concrete than adjectives such as aggressive or honest. And what about nouns? There a distinction between concrete (ofﬁce chair) and abstract (hypothesis) seems to exist as well. This raises the question whether it is the differences in interpersonal language use or the more general distinction between concrete and abstract language which causes the observed effects on perception; a recent series of experiments suggests it is the latter (Krahmer and Stapel 2009).  2.3 What Can Computational Linguists Learn? The social psychological ﬁndings brieﬂy described here could have an impact on computational linguistics, with potential applications for both text understanding and generation. So far, it seems fair to say that most computational linguists have concentrated so much on trying to understand text or on generating coherent texts that the subtle effects that language may have on the reader were virtually ignored. Function words were originally not the words computational linguists found most interesting. They were considered too frequent; early Information Retrieval applications listed function words on “stop lists”—lists of words that should be ignored during processing—and many IR applications still do. The work of Pennebaker and colleagues indicates that pronouns (as well as other function words) do carry potentially relevant information, for instance about the mental state of the author of a document. Interestingly, for computational applications such as opinion mining and sentiment analysis (Pang and Lee 2008) as well as author attribution and stylometry (Holmes 1998), function words have been argued to be relevant as well, but it seems that research on the social psychology of language has made little or no impact on this ﬁeld. 288  Krahmer  Last Words  Consider sentiment analysis, for instance, which is the automatic extraction of “opinion-oriented” information (e.g., whether an author feels positive or negative about a certain product) from text. This is a prime example of an emerging research area in computational linguistics which moves beyond factual information exchange (although the preferred approach to this problem very much ﬁts with the paradigm sketched by Halevy et al. [2009]: take a large set of data and apply machine learning to it). Pang and Lee (2008) offer an extensive overview of research related to sentiment analysis, but do not discuss any of the psychological studies mentioned herein (in fact, of the 332 papers they cite, only one or two could conceivably be interpreted as psychological in the broadest interpretation). What is especially interesting is that their discussion of why sentiment analysis is difﬁcult echoes the discussion of Chung and Pennebaker (2007) on the problems of counting words (by sheer coincidence they even discuss essentially the same example: madden). These ﬁndings may also have ramiﬁcations for the generation of documents. If you develop an application which automatically produces texts from non-textual data, you might want to avoid excessive use of the ﬁrst-person pronoun, lest your readers think your computer is feeling down. If you want your readers to skim over the details of what is proposed in a generated text, use abstract language. In addition, you may want to use action verbs when describing your own accomplishments, and adjectives to refer to those of others (but do it in a subtle way, because people might notice). 3. Language Comprehension and Production While the link between computational linguistics and social psychology has seldom been explored, there has been somewhat more interaction with psycholinguistics. Perhaps most of this interaction has involved natural language understanding. Various early parsing algorithms were inspired by human sentence processing, which is hardly surprising: human listeners are remarkably efﬁcient in processing and adequately responding to potentially highly ambiguous sentences. Later, when large data sets of parsed sentences became available, the focus in computational linguistics shifted to developing statistical models of language processing. Interestingly, recent psycholinguistic sentence processing models are inspired in turn by statistical techniques from computational linguistics (Chater and Manning 2006; Crocker in press; Jurafsky 2003; Pado, Crocker, and Keller 2009). 3.1 On Producing Referring Expressions The situation is somewhat different for natural language generation, although superﬁcially the same kind of interaction can be observed here (albeit with a few years delay). The seminal work by Dale and Reiter (1995) on the generation of referring expressions was explicitly inspired by psycholinguistic work. Dale and Reiter concentrated on the generation of distinguishing descriptions, such as the large black dog, which single out one target object by ruling out the distractors (typically a set of other domestic animals of different sizes and colors). Given that the number of distractors may be quite large and given that each target can be referred to in multiple ways, one of the main issues in this area is how to keep the search manageable. Current algorithms for referring expression generation, building on the foundations laid by Dale and Reiter, are good at quickly computing which set of properties uniquely characterizes a target among a 289  Computational Linguistics  Volume 36, Number 2  set of distractors. Some of these algorithms are capable of generating distinguishing descriptions that human judges ﬁnd more helpful and better formulated than humanproduced distinguishing descriptions for the same targets (Gatt, Belz, and Kow 2009). To some this might suggest that the problem is solved. This conclusion, however, would be too hasty. Most of the algorithms use some very unrealistic assumptions which limit their applicability. Interestingly, these assumptions can be traced back directly to classic psycholinguistic work on the production of referring expressions (Olson 1970). Clark and Bangerter (2004) criticize a number of the unstated assumptions in Olson’s approach: Reference is treated as a one-step process (a speaker plans and produces a complete description, and nothing else, in one go) and during that process the speaker does not take the prior interaction with the addressee into account. By merely substituting computer for speaker these comments are directly applicable to most current generation algorithms as well. The problem, unfortunately, is that recent psycholinguistic research suggests that these assumptions are wrong. Often this research looks at how speakers produce referring expressions while interacting with an addressee, and one thing that is often found is that speakers adapt to their conversational partners while producing referring expressions (Clark and Wilkes-Gibbs 1986; Brennan and Clark 1996; Metzing and Brennan 2003). This kind of “entrainment” or “alignment” (Pickering and Garrod 2004) may apply at the level of lexical choice; if a speaker refers to a couch using the word sofa instead of the more common couch, the addressee is more likely to use sofa instead of couch as well later on in the dialogue (Branigan et al. in press). But the speaker and addressee may also form a general “conceptual pact” on how to refer to some object, deciding together, for instance, to refer to a tangram ﬁgure as the tall ice skater. Although adaptation itself is uncontroversial, psycholinguists argue about the extent to which speakers are capable of taking the perspective of the addressee into account (Kronmu¨ ller and Barr 2007; Brennan and Hanna 2009; Brown-Schmidt 2009), with some researchers presenting evidence that speakers may have considerable difﬁculty doing this (Horton and Keysar 1996; Keysar, Lin, and Barr 2003). In Wardlow Lane et al. (2006) people are instructed to refer to simple targets (geometrical ﬁgures that may be small or larger) in the context of three distractor objects, two of which are visible to both speaker and addressee (shared) whereas the other is visible to the speaker only (privileged). If speakers would be able to take the addressees’ perspective into account when referring, the privileged distractor should not play a role in determining which properties to include in the distinguishing description. However, Wardlow Lane and colleagues found that speakers do regularly take the privileged distractor into account (for instance adding a modiﬁer small when referring to the target, even though all the shared objects are small and only the privileged one is large). Interestingly, speakers do this more often when explicitly told that they should not leak information about the privileged object, which Wardlow Lane et al. interpret as an ironic processing effect of the kind observed by Dostoevsky (“Try to pose for yourself this task: not to think of a polar bear, and you will see that the cursed thing will come to mind every minute”). Another interesting psycholinguistic ﬁnding is that speakers often include more information in their referring expressions than is strictly needed for identiﬁcation (Arts 2004; Engelhardt, Bailey, and Ferreira 2006), for instance referring to a dog as the large black curly haired dog in a situation where there is only one large black dog. Again, that speakers are not always “Gricean” (“be as informative as required, but not more informative”) is generally agreed upon, but there is an ongoing debate about why and how speakers overspecify, some arguing that it simpliﬁes the search of the speaker 290  Krahmer  Last Words  (Engelhardt, Bailey, and Ferreira 2006) whereas others suggest that overspeciﬁed references are particularly beneﬁcial for the addressee (Paraboni, van Deemter, and Masthoff 2007). 3.2 What Can Computational Linguists Learn? Why are these psycholinguistic ﬁndings about the way human speakers refer relevant for generation algorithms? First of all, human-likeness is an important evaluation criterion, so algorithms that are good at emulating human referring expressions are likely to outperform algorithms that are not. Moreover, it is interesting to observe that generating overspeciﬁed expressions is computationally cheaper than producing minimal ones (Dale and Reiter 1995). In a similar vein, it can be argued that alignment and adaptation may reduce the search space of the generation algorithm, because they limit the number of possibilities that have to be considered. It is worth emphasizing that psycholinguistic theories have little to say about how speakers quickly decide which properties, from the large set of potential ones, to use in a referring expression. In addition, whereas notions such as adaptation, alignment, and overspeciﬁcation are intuitively appealing, it has turned out to be remarkably difﬁcult to specify how these processes operate exactly. In fact, a common criticism is that they would greatly beneﬁt from “explicit computational modeling” (Brown-Schmidt and Tanenhaus 2004). Of course, solving choice problems and computational modeling are precisely what computational linguistics has to offer. So although generation algorithms may beneﬁt a lot from incorporating insights from psycholinguistics, they in turn have the potential to further research in psycholinguistics as well.  4. Discussion In this brief, highly selective, and somewhat biased overview of work on language in several areas of psychology, we have seen that words may give valuable information about the person who produces them (but how to select and count them is tricky), that abstract or concrete language may tell you something about the opinions and attitudes a speaker has and may even inﬂuence how you perceive things (but the linguistic intuitions about what is abstract, and what concrete, need some work), and that speakers are remarkably efﬁcient when producing referring expressions, in part because they adapt to their addressee and do not necessarily try to be as brief as possible (but making these intuitive notions precise is difﬁcult). Psychological ﬁndings such as these are not merely intriguing, but could be of real use for computational linguistic applications related to document understanding or generation (and, conversely, techniques and insights from computational linguistics could be helpful for psychologists as well). Of course, some computational linguists do extensively rely on psychological ﬁndings for building their applications (you know who you are), just as some psychologists use sophisticated computational and statistical models rather than human participants for their studies (this is especially true in psycholinguistics). But these are exceptions, and certainly do not belong to mainstream computational linguistics or psychology. Which raises one obvious question: Why isn’t there more interaction between these two communities? There seem to be at least three reasons for this. First, and most obvious, many researchers are not aware of what happens outside their own specialized ﬁeld. The articles in psychology are fairly accessible (usually no complex statistical models or overformalized algorithms there), but many computational linguists may feel that it 291  Computational Linguistics  Volume 36, Number 2  would be a better investment of their limited time to read some more of the 17,000 (and counting) journal, conference, and workshop papers they have not yet read in the invaluable ACL Anthology. For psychologists presumably similar considerations apply, with the additional complication that many of the anthology papers require a substantial amount of technical prior knowledge. In addition, it might be that the different publication cultures are a limiting factor here as well: for psychologists, journals are the main publication outlet; for them most non-journal publications have a low status and hence might be perceived as not worth exploring. Another perhaps more interesting reason is that psychologists and computational linguists have subtly different general objectives. Psychologists want to get a better understanding of people; how their social context determines their language behavior, how they produce and comprehend sentences, and so on. Their models are evaluated in terms of whether there is statistical evidence for their predictions in actual human behavior. Computational linguists evaluate their models (“algorithms”) on large collections of human-produced data; one model is better than another if it accounts for more of the data. Of course, a model can perform well when evaluated on human data, but be completely unrealistic from a psychological point of view. If a computational linguist develops a referring expression generation algorithm (or a machine translation system or an automatic summarizer) which accounts for the data in a way which is psychologically unrealistic, the work will generally not be of interest to psychologists. Conversely, if psychological insights are difﬁcult to formalize or require complex algorithms or data structures, computational linguists are likely not to be enthusiastic about applying them. Obviously, this hinders cross-pollination of ideas as well. Third, and somewhat related to the previous point, it sometimes seems that computational linguists see trees where psychologists see a forest. Psychologists appear to be most interested in showing a general effect (and are particularly appreciative of clever and elegant experimental designs which reveal these effects); if merely counting words already gives you a statistically reliable effect, then why bother with a more complicated way of counting n-grams and worrying about back-off smoothing to deal with data sparseness? Doing so would conceivably give you a better estimate of the signiﬁcance and size of your effect, but would probably not change your story in any fundamental way. Computational linguists, by contrast, evaluate their models on (often shared) datasets (and tend to be more impressed by technical prowess—e.g., new statistical machine learning models—or by smart ways of automatically collecting large quantities of data); each data point that is processed incorrectly by their model offers a potential advantage for someone else’s model. In view of observations such as these, it is perhaps not surprising that computational linguists and psychologists have remained largely unaware of each other’s work so far. Predicting the future is a tricky thing, but it seems not unlikely that most computational linguists and psychologists will continue going their own way in the future. Nevertheless, I hope to have shown here that both communities could beneﬁt from the occasional foray into the others’ territory. For psychologists, the tools and techniques developed by computational linguists could further their research, by helping to make their models and theories more explicit and hence easier to test and compare. For computational linguists, insights from both the social psychology of language and from psycholinguists could contribute to a range of applications, from opinion mining to text understanding and generation. Obviously, this contribution could be on the level of “words”, but a more substantial contribution is conceivable as well. As we have seen, psychologists are particularly strong in explanatory theories (on affect, on interaction, etc.) and perhaps taking these as starting points for our applications (e.g., 292  Krahmer  Last Words  on affective and interactive generation) could make them theoretically more interesting and empirically more adequate.  Acknowledgments Thanks to Robert Dale for inviting me to write a Last Words piece on this topic and for his useful comments on an earlier version. This piece grew out of discussions I had over the years with both computational linguists and psychologists, including Martijn Balsters, Kees van Deemter, Albert Gatt, Roger van Gompel, Erwin Marsi, Diederik Stapel, Marc Swerts, Marie¨t Theune, and Ad Vingerhoets. Needless to say, I alone am responsible for the simpliﬁcations and opinions in this work. I received ﬁnancial support from the Netherlands Organization for Scientiﬁc Research (NWO), via the Vici project “Bridging the gap between computational linguistics and psycholinguistics: The case of referring expressions” (277-70-007), which is gratefully acknowledged. References Arts, Anja. 2004. Overspeciﬁcation in Instructive Texts. Unpublished Ph.D. thesis, Tilburg University. Branigan, Holly P., Martin J. Pickering, Jamie Pearson, and Janet F. McLean. (in press). Linguistic alignment between humans and computers. Journal of Pragmatics. Brennan, Susan and Herbert H. Clark. 1996. Conceptual pacts and lexical choice in conversation. Journal of Experimental Psychology, 22(6):1482–1493. Brennan, Susan E. and Joy E. Hanna. 2009. Partner-speciﬁc adaptation in dialog. Topics in Cognitive Science, 1(2):274–291. Brown-Schmidt, S. and M. Tanenhaus. 2004. Priming and alignment: Mechanism or consequence? commentary on Pickering and Garrod 2004. Behavioral and Brain Sciences, 27:193–194. Brown-Schmidt, Sarah. 2009. Partner-speciﬁc interpretation of maintained referential precedents during interactive dialog. Journal of Memory and Language, 61:171–190. Chater, Nick and Christopher D. Manning. 2006. Probabilistic models of language processing and acquisition. Trends in Cognitive Science, 10:335–344. Chung, C. K. and James W. Pennebaker. 2007. The psychological function of function words. In K. Fiedler, editor, Social Communication: Frontiers of Social  Psychology. Psychology Press, New York, pages 343–359. Clark, Herbert H. 1996. Using Language. Cambridge University Press, Cambridge, UK. Clark, Herbert H. and Adrian Bangerter. 2004. Changing ideas about reference. In Ira A. Noveck and Dan Sperber, editors, Experimental Pragmatics. Palgrave Macmillan, Basingstoke, pages 25–49. Clark, Herbert H. and Deanna Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22:1–39. Cohn, M., M. Mehl, and James W. Pennebaker. 2004. Linguistic markers of psychological change surrounding September 11, 2001. Psychological Science, 15:687–693. Crocker, Matthew W. (in press). Computational psycholinguistics. In Alex Clark, Chris Fox, and Shalom Lappin, editors, Computational Linguistics and Natural Language Processing Handbook. Wiley Blackwell, London, UK. Dale, Robert and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 18:233–263. Douglas, Karen and Robbie Sutton. 2006. When what you say about others says something about you: Language abstraction and inferences about describers’ attitudes and goals. Journal of Experimental Social Psychology, 42: 500–508. Engelhardt, Paul E., Karl G. D. Bailey, and Fernanda Ferreira. 2006. Do speakers and listeners observe the Gricean Maxim of Quantity? Journal of Memory and Language, 54:554–573. Gatt, Albert, Anja Belz, and Eric Kow. 2009. The tuna-reg challenge 2009: Overview and evaluation results. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG), pages 174–182, Athens. Halevy, Alon, Peter Norvig, and Fernando Pereira. 2009. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24:8–12. Holmes, David I. 1998. The evolution of stylometry in humanities scholarship. Literary and Linguistic Computing, 13:111–117.  293  Computational Linguistics  Volume 36, Number 2  Horton, W. S. and B. Keysar. 1996. When do speakers take into account common ground? Cognition, 59:91–117. Hovy, Eduard H. 1990. Pragmatics and natural language generation. Artiﬁcial Intelligence, 43:153–197. Jurafsky, Dan. 2003. Probabilistic modeling in psycholinguistics: Linguistic comprehension and production. In Rens Bod, Jennifer Hay, and Stefanie Jannedy, editors, Probabilistic Linguistics. MIT Press, Cambridge, MA, pages 39–96. Keysar, B., S. Lin, and D. J. Barr. 2003. Limits on theory of mind use in adults. Cognition, 89:25–41. Krahmer, Emiel and Diederik Stapel. 2009. Abstract language, global perception: How language shapes what we see. In Proceedings of the Annual Meeting of the Cognitive Science Society, pages 286–291, Amsterdam. Kronmu¨ ller, E. and Dale Barr. 2007. Perspective-free pragmatics: Broken precedents and the recovery-frompreemption hypothesis. Journal of Memory and Language, 56:436–455. Maass, A., D. Salvi, L. Arcuri, and G. Semin. 1989. Language use in intergroup contexts: The linguistic intergroup bias. Journal of Personality and Social Psychology, 57:981–993. Metzing, Charles A. and Susan E. Brennan. 2003. When conceptual pacts are broken: Partner effects on the comprehension of referring expressions. Journal of Memory and Language, 49:201–213. Olson, David R. 1970. Language and thought: Aspects of a cognitive theory of semantics. Psychological Review, 77:257–273. Pado, Ulrike, Matthew W. Crocker, and Frank Keller. 2009. A probabilistic model of semantic plausibility in sentence processing. Cognitive Science, 33:794–838. Pang, B. and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2:1–135. Paraboni, Ivandre´, Kees van Deemter, and Judith Masthoff. 2007. Generating referring expressions: Making referents  easy to identity. Computational Linguistics, 33:229–254. Pennebaker, James W., C. Groom, D. Loew, and J. Dabbs. 2004. Testosterone as a social inhibitor: Two case studies of the effect of testosterone treatment on language. Journal of Abnormal Psychology, 113:172–175. Pickering, Martin and Simon Garrod. 2004. Towards a mechanistic psychology of dialogue. Behavioural and Brain Sciences, 27:169–226. Reiter, Ehud. 2007. The shrinking horizons of computational linguistics. Computational Linguistics, 33:283–287. Rude, S., E. Gortner, and James W. Pennebaker. 2004. Language use of depressed and depression-vulnerable college students. Cognition and Emotion, 18:1121–1133. Semin, Gu¨ n. 2009. Language and social cognition. In F. Strack and J. Fo¨ rster, editors, Social Cognition: The Basis of Human Interaction. Psychology Press, London, pages 269–290. Semin, Gu¨ n and Klaus Fiedler. 1988. The cognitive functions of linguistic categories in describing persons: Social cognition and language. Journal of Personality and Social Psychology, 34:558–568. Stapel, Diederik and Gu¨ n Semin. 2007. The magic spell of language. Journal of Personality and Social Psychology, 93:23–33. Stirman, Shannon and James W. Pennebaker. 2001. Word use in the poetry of suicidal and nonsuicidal poets. Psychosomatic Medicine, 63:517–522. Wardlow Lane, Liane, Michelle Groisman, and Victor S. Ferreira. 2006. Don’t talk about pink elephants! : Speakers’ control over leaking private information during language production. Psychological Science, 17:273–277. Wigboldus, Danie¨l, Gu¨ n Semin, and Russell Spears. 2000. How do we communicate stereotypes? linguistic bases and inferential consequences. Journal of Personality and Social Psychology, 78:5–18. Winograd, Terry. 1980. What does it mean to understand language? Cognitive Science, 4:209–241. 
Tim Miller∗ University of Minnesota Lane Schwartz∗ University of Minnesota Human syntactic processing shows many signs of taking place within a general-purpose short-term memory. But this kind of memory is known to have a severely constrained storage capacity — possibly constrained to as few as three or four distinct elements. This article describes a model of syntactic processing that operates successfully within these severe constraints, by recognizing constituents in a right-corner transformed representation (a variant of left-corner parsing) and mapping this representation to random variables in a Hierarchical Hidden Markov Model, a factored time-series model which probabilistically models the contents of a bounded memory store over time. Evaluations of the coverage of this model on a large syntactically annotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategy based on this model, suggest this model may be cognitively plausible. 1. Introduction It is an interesting possibility that human syntactic processing may occur entirely within a general-purpose short-term memory. Like other short-term memory processes, syntactic processing is susceptible to degradation if short-term memory capacity is loaded, for example, when readers are asked to retain lists of words while reading (Just and Carpenter 1992); and memory of words and syntax degrades over time within and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse information about referents from other sentences (Ericsson and Kintsch 1995). But short-term memory is known to have severe capacity limitations of perhaps no more than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem ∗ Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455. E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu. ∗∗ Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street, Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg. Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for publication: 27 May 2009. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 1  too austere to process the rich tree-like phrase structure commonly invoked to explain word-order regularities in natural language. This article aims to show that they are not. The article describes a comprehension model, based on a right-corner transform—a reversible tree transform related to the left-corner transform of Johnson (1998a)—that associates familiar phrase structure trees with the contents of a memory store of three to four partially completed constituents over time. Coverage results on the large syntactically annotated Penn Treebank corpus show a vast majority of naturally occurring sentences can be recognized using a memory store containing a maximum of only three incomplete constituents, and nearly all sentences can be recognized using four, consistent with estimates of human short-term memory capacity. This transform reduces memory usage in incremental (left to right) processing by transforming right-branching constituent structures into left-branching structures, allowing child constituents to be composed with parent constituents before either have been completely recognized. But because this composition identiﬁes an incomplete child as the awaited portion of an incomplete parent, it implicitly predicts that this child constituent will be the rightmost (i.e., last) child of the parent, before this child has been completely recognized. Parsing accuracy results on the Penn Treebank using a Hierarchical Hidden Markov Model (Murphy and Paskin 2001)—essentially a probabilistic pushdown automaton with a bounded pushdown store—show that this prediction can be reliably learned from training data. The remainder of this article is organized as follows: Section 2 describes some related models of human syntactic processing using a bounded memory store; Section 3 describes a Hierarchical Hidden Markov Model (HHMM) framework for statistical parsing using this bounded store of incomplete constituents; Section 4 describes the right-corner transform and how it relates conventional phrase structure to incomplete constituents in a bounded memory store; Section 5 describes an experiment to estimate the level of coverage of the Penn Treebank corpus that can be achieved using this transform with various memory limits, given a linguistically motivated binarization of this corpus; and Section 6 gives accuracy results of this bounded-memory model trained on this corpus, given that some amount of incremental prediction (as described earlier) must be involved. 2. Bounded-Memory Parsing One of the earliest bounded-memory parsing models is that of Marcus (1980). This model maintains a bounded store of complete but unattached constituents as a buffer, and operates on them using a variety of specialized memory manipulation operations, deferring certain attachment decisions until the contents of this buffer indicate it is safe to do so. (In contrast, the model described in this article maintains a store of incomplete constituents using ordinary stack-like push and pop operations, deﬁned to allow constituents to be composed before being completely recognized.) The Marcus parser provides a bounded-memory explanation for human difﬁculties in processing garden path sentences: for example, the horse raced past the barn fell, with intended interpretation [NP the horse [RC (which was) raced past the barn]] fell (Bever 1970), in which raced seems like the main verb of the sentence until the word fell is encountered. But this explanation due to memory exhaustion is not compatible with observations of unproblematic parsing of sentences such as these when contextual information is provided in advance: for example, two men on horseback had a race; one went by the meadow, and the other went by the barn (Crain and Steedman 1985). 2  Schuler et al.  Parsing Using Human-Like Memory Constraints  Ades and Steedman (1982) introduce the idea of composing incomplete constituents to reduce storage demands in incremental processing using Combinatorial Categorial Grammar (CCG), avoiding the need to maintain large buffers of complete but unattached constituents. The right-corner transform described in this article composes incomplete constituents in very much the same way, but CCG is essentially a competence model, in that it seeks to unify lexical category representations used in processing with learned generalizations about argument structure, whereas the model described herein is exclusively a performance model, allowing generalizations about lexical argument structures to be learned in some other representation, then combined with probabilistic information about parsing strategies to yield a set of derived incomplete constituents. As a result, the model described in this article has a freer hand to satisfy strict working memory bounds, which may not permit some of the alternative composition operations proposed in the CCG account, thought to be associated with available prosody and quantiﬁer scope analyses.1 Johnson-Laird (1983) and Abney and Johnson (1991) propose a pure processing account of memory capacity limits in parsing ordinary phrase structure trees. The Johnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, of which the right-corner transform introduced in this article is a variant, in order to bring memory usage for most parsable sentences to within seven or so active or awaited phrase structure constituents. This account may be used to explain human processing difﬁculties in processing triply center-embedded sentences like the rat that the cat that the dog chased killed ate the malt, with intended interpretation [NP the rat that [NP the cat that [NP the dog] chased] killed] ate the malt (Chomsky and Miller 1963). But this explanation does not account for examples of triply center-embedded sentences that typically do not cause processing problems: [NP that [NP the food that [NP John] ordered] tasted good] pleased him (Gibson 1991). Moreover, the apparent competition between comprehension of center-embedded object relatives and retention of unrelated words in general-purpose memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at least, can be) used to store incomplete constituents during comprehension. This would predict three or four elements of reliable storage, rather than seven (Cowan 2001). The transform-based model described in this article exploits a conception of chunking (Miller 1956) to combine pairs of active and awaited constituents from the Abney and Johnson analysis, connected by recognized structure, in order to operate within estimates of human short-term memory bounds. Because of these counterexamples to the memory-exhaustion explanation of garden path and center-embedding difﬁculties, recent work has turned to explanations other than memory exhaustion for these phenomena. Lewis and Vasishth (2005) attribute processing errors to activation interference among stored constituents that have similar syntactic and semantic roles. Hale’s surprisal (2001) and entropic model (2006) link human processing difﬁculties to signiﬁcant changes in the relative probability of competing hypotheses in incremental parsing, such that if activation is taken to be a mechanism for probability estimation, processing difﬁculties may be ascribed to the relatively slow speed of activation change within the brain (or to collapsing activation when probabilities grow too small, as in the case of garden path sentences). These models explain many processing difﬁculties without invoking memory limits, and are 
Many NLP applications entail that texts are classiﬁed based on their semantic distance (how similar or different the texts are). For example, comparing the text of a new document to that of documents of known topics can help identify the topic of the new text. Typically, a distributional distance is used to capture the implicit semantic distance between two pieces of text. However, such approaches do not take into account the semantic relations between words. In this article, we introduce an alternative method of measuring the semantic distance between texts that integrates distributional information and ontological knowledge within a network ﬂow formalism. We ﬁrst represent each text as a collection of frequency-weighted concepts within an ontology. We then make use of a network ﬂow method which provides an efﬁcient way of explicitly measuring the frequency-weighted ontological distance between the concepts across two texts. We evaluate our method in a variety of NLP tasks, and ﬁnd that it performs well on two of three tasks. We develop a new measure of semantic coherence that enables us to account for the performance difference across the three data sets, shedding light on the properties of a data set that lends itself well to our method. 1. Introduction Many natural language tasks can be cast as a problem of comparing texts in terms of their semantic distance. For example, given a suitable text distance measure, document classiﬁcation can be performed by comparing the text of a new document to the text of various documents whose topics are known. The new document is then labelled with the topic of the document whose text is most similar to it. In general, the texts to be compared may be full documents, as in this example, or may be portions of documents, or even collections of documents. Using text comparison to perform semantic classiﬁcation has been adopted in a variety of natural language processing (NLP) tasks, from document classiﬁcation (Scott and Matwin 1998; Rennie 2001; Al-Mubaid and Umair 2006), to prepositional phrase attachment (Pantel and Lin 2000), to spelling correction (Budanitsky and Hirst 2001). ∗ Department of Computer Science, University of Toronto, 6 King’s College Road, Toronto, Ontario M5S 3G4, Canada. E-mail: vyctsang@cs.toronto.edu. ∗∗ Department of Computer Science, University of Toronto, 6 King’s College Road, Toronto, Ontario M5S 3G4, Canada. E-mail: suzanne@cs.toronto.edu. Submission received: 16 December 2007; revised submission received: 18 June 2008; accepted for publication: 20 August 2008. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 1  Distributional methods for semantic distance are widely used and highly successful in comparing texts that are represented as bags of words with associated frequencies of occurrence (Lee 2001; Weeds, Weir, and McCarthy 2004; Pedersen, Banerjee, and Patwardhan 2005). In document classiﬁcation, for example, the text of a document may be represented as a word frequency vector, which is compared using a distributional distance measure to each of the word frequency vectors of the texts of the documents of known topics. In this way, distributional distance between word vectors captures the semantic distance between two texts that is implicitly encoded in the set of words used in each. Semantic distance can also be measured more explicitly, by using the relations in an ontology as the direct encoding of semantic association. However, such approaches have generally been limited to calculating the distance between two individual concepts, rather than capturing the distance between two sets of concepts corresponding to two texts. Numerous measures have been proposed, for example, for capturing the distance between two concepts in WordNet, typically relying on the synonymy (synset) and hyponymy (is-a) relations (Wu and Palmer 1994; Resnik 1995; Jiang and Conrath 1997, among others). Using such an ontological measure to compare two texts (collections of words instead of single words) might involve mapping each word of a text to its appropriate concept(s) in the ontology, and then calculating the aggregate distance between the two resulting sets of concepts across the ontological relations. For example, one might calculate the semantic distance between the two texts as the average, minimum, maximum, or summed ontological distance between the individual elements of the two sets of concepts (Corley and Mihalcea 2005). Observe that each of these approaches to text comparison—distributional and ontological—encodes information not contained in the other. Distributional distance captures important information about frequency of occurrence of the words that constitute the target text, whereas ontological distance captures essential semantic knowledge that has been encoded in the relations of an ontology. In response, previous work has attempted to combine distributional and ontological information in computing semantic distance. For example, researchers have developed measures of semantic distance between texts that apply distributional distances to concept vectors of frequencies rather than to word vectors (McCarthy 2000; Mohammad and Hirst 2006). However, these approaches only make pairwise comparisions between the elements of the concept vectors, and do not take into account the important ontological relations among the concepts. In order to capture such relations, other methods have instead integrated distributional information into an ontological method. However, such approaches have heretofore been limited to measuring distance between two individual concepts. For example, some ontological measures use corpus frequencies of words to yield concept weights that are taken into account in measuring the distance between two concepts (Resnik 1995; Jiang and Conrath 1997). What has been missing is an approach to semantic distance between two texts—two sets of words—that can truly integrate distributional and ontological (relational) information, drawing more fully on their complementary advantages for text comparison. In this article, we describe a new graph-based distance measure that achieves the desired integration of distributional and ontological factors in measuring semantic distance between two sets of concepts (mapped from two texts). An ontology is treated as a graph in the usual manner, in which the concepts are nodes and the relations are edges. A text is represented as a subgraph of the ontology, by mapping the words in the text into their corresponding concepts, which are weighted according to the word frequencies. We call the resulting set of frequency-weighted concepts a semantic proﬁle. 32  Tsang and Stevenson  A Graph-Theoretic Framework for Semantic Distance  By exploiting the relational structure of the ontology, we can explicitly measure the ontological distance over the paths between two proﬁles. Using the frequencies on the concept nodes, we weight these paths according to the frequency distribution of words in the two texts. The resulting calculation yields a frequency-weighted ontological distance between the two sets of concepts. Thus, we view a text not as a set of items to be compared individually to those in another set (with those individual distances then somehow combined, e.g., as in Corley and Mihalcea [2005]), but rather as a distribution of “mass” within a graph that encodes the semantic relations across the two sets, and use a weighted graph-based approach that captures the aggregate distance between the two frequency masses. To our knowledge, this is the ﬁrst method to integrate ontological and distributional information in the graphical calculation of text distance. This article describes the use of the new measure in several different types of NLP text comparison tasks, in order to explore the situations in which such an approach can be effective. Given the novelty of the approach, the task-based evaluation is not intended as the last word on the usefulness of the method, but rather as a ﬁrst suite of experiments across different types of text comparison tasks to illuminate some of the strengths and weaknesses of such an approach to text distance. We thus analyze the results in detail to identify future directions for further illuminating when and to what extent the method might be useful. The analysis reveals that our method is not consistently successful across our sample tasks. We hypothesize that, because ontological relations play an integral role in our semantic distance measure, the measure is less effective when the semantic proﬁle for a text (the set of corresponding concepts) lacks semantic coherence. Other work has explored ways to measure the semantic coherence of a set of concepts in terms of their connectedness within an ontology (Gurevych et al. 2003). Because a semantic proﬁle in our work includes both ontological (relational) and distributional (frequency) knowledge, we require a measure of semantic coherence that takes both into account. We develop a novel measure of semantic coherence called proﬁle density that captures both the ontological and distributional coherence of a set of frequencyweighted concepts, and apply it to the data sets used in the different tasks to better understand the performance of our semantic distance measure. Our distance measure is cast as a graphical text comparison task within a network ﬂow framework as described in Section 2. In Section 3, we give an overview of our exploration of the method on three types of text comparison problems. The following three sections present experimental results and analysis of applying our method to the various tasks: verb alternation detection (Section 4), name disambiguation (Section 5), and document classiﬁcation (Section 6). In Section 7, we describe our proﬁle density measure and use it to analyze the properties of the data sets that lead to the performance differential across the tasks. We conclude the paper with a description of related work in text comparison and graph-theoretic NLP approaches (Section 8) and a discussion of some future directions for our research (Section 9). 2. The Network Flow Method As noted previously, we treat an ontology as a graph and represent a text as a semantic proﬁle—a collection of nodes in the graph (concepts in the ontology), each having a weight (its frequency). For example, in Figure 1, a small text consisting of the words {cheese, wheat}, with frequencies of 4 and 10, respectively, is represented as a small weighted subgraph in an ontology by uniformly distributing the word frequencies among the associated concepts. In this way, a text is a weighted subgraph within a 33  Computational Linguistics  Volume 36, Number 1  Figure 1 A small text represented as a collection of weighted nodes in a fragment of WordNet. larger graph (with the thickness of the boxes in the ﬁgure indicating weight), and two such weighted subgraphs are connected via a set of paths in the graph. Our goal is to measure the distance between two subgraphs (representing two texts to be compared), taking into account both the ontological distance between the component concepts and their frequency distributions. To achieve this, we measure the amount of “effort” required to transform one proﬁle to match the other graphically: The more similar they are, the less effort it takes to transform one into the other. (This view is similar to that motivating the use of “earth mover’s distance” in computer vision [Levina and Bickel 2001].) In Section 2.1, we ﬁrst give the intuitive motivation for the approach in terms of the properties of semantic distance that we want to capture by considering transport effort. We then present the mathematical formulation of our graph-based method as a minimum cost ﬂow (MCF) problem in Section 2.2, and describe the formulation of our task within this network ﬂow framework in Section 2.3. In Section 2.4, we return to the properties we identify in Section 2.1 to explain how they are reﬂected in the MCF formulation. 2.1 An Intuitive Overview In Figure 2(a), we show a diagrammatic representation of an ontology (the large open triangle) with two proﬁles, one indicated with ﬁlled squares and the other with ﬁlled triangles. The location of a ﬁlled shape indicates the location of a proﬁle concept in the ontology, and its size indicates its frequency within the proﬁle. We omit edges between the nodes to simplify the diagram, but note that we assume we have a hierarchical, connected ontology; hyponymy links are sufﬁcient. Our goal is to calculate the similarity between the two proﬁles by determining how much effort is required to transport, along the ontological links, the frequency mass from all of the squares to “ﬁll” the available space in the triangles. The amount of mass to move and the amount of space available are indicated by the sizes of the squares and triangles, respectively. The degree of effort required to transport one to the other indicates the degree of semantic distance. 34  Tsang and Stevenson  A Graph-Theoretic Framework for Semantic Distance  Figure 2 Two subgraphs (one represented by squares, the other, triangles) with varying degrees of overlap and, therefore, similarity within an ontology. Figure (b) differs from Figure (a) in terms of the ontological distance between the square and the triangle clusters. Figure (c) differs from Figure (a) in terms of the size of the individual squares. The transport effort is determined by both the amount of mass to move and the graphical distance over which it must travel. First consider graphical (ontological) distance between the proﬁles. Assume the calculated distance between the two proﬁles in Figure 2(a) is d. In Figure 2(b), the triangle proﬁle is exactly the same. By contrast, although the square proﬁle has the same internal properties (same frequency distribution and graphical structure), its location is further from the triangles. Because the two proﬁles occupy more distant portions of the ontological space, they are less semantically similar than in Figure 2(a). As desired, the extra ontological distance over which the square frequency mass must be transported to the triangles will cause the calculated distance in Figure 2(b) to be larger than d. Next consider the effect of varying the frequency distribution over the proﬁle nodes. Again, in Figure 2(c), the triangle proﬁle is exactly the same as in Figure 2(a). However, whereas the nodes of the square proﬁle in Figure 2(c) are in the same locations as in Figure 2(a), their distributional properties are different. The bulk of the frequency distribution is now shifted closer to the nodes of the triangle proﬁle. Because the two proﬁles have more distributional weight located closer within the ontology, this indicates that the semantic space they occupy is more similar than in Figure 2(a). Correspondingly, because much of the mass of the square proﬁle needs to travel less far to ﬁll the space of the triangle nodes, the calculated distance in Figure 2(c) will be less than d. 35  Computational Linguistics  Volume 36, Number 1  It is worth noting explicitly that this notion of semantic distance as transport effort of concept frequency over the relations (edges) of an ontology differs signiﬁcantly from an approach to semantic distance that utilizes concept vectors of frequency. By crucially utilizing the relations between concepts in calculating semantic distance, our approach can determine the distance between texts that use related but non-equivalent concepts. For example, our measure will ﬁnd greater similarity between a text that discusses milk and one that discusses cheese than between one that discusses milk and one that discusses bread. A vector distance would ﬁnd each of these equally dissimilar, because there are no concepts in common, and there is no way to relate milk to cheese.1 The intuitive examples in Figure 2 show that calculating semantic distance as transport effort captures in a well-motivated way both the ontological distance between the proﬁles and their weighting by the distributional amounts of the concept nodes. In the next subsection, we describe a mathematical formulation that captures the relevant properties of our problem in a network ﬂow framework. Network ﬂow methods are often used in computer science for modelling such transport effort, for example, in communication or transportation networks.  2.2 Minimum Cost Flow Our intuitive transport effort examples above can be viewed as a supply–demand problem, in which we ﬁnd the minimum cost ﬂow (MCF) from the supply proﬁle to the demand proﬁle to meet the requirements of the latter. Mathematically, let G = (N, E) be a connected graph representing an ontology, where N is the set of nodes representing the individual concepts, and E is the set of edges representing the relations between the concepts. (Most ontologies are connected; in the case of a forest, adding an arbitrary root node yields a connected graph.) Each edge has a cost c : E → R, which is the ontological distance of the edge. Each node i ∈ N is associated with a value b(i) such that b : N → R indicates its available supply (b(i) > 0), its demand (b(i) < 0), or neither (b(i) = 0). The goal is to ﬁnd a ﬂow from supply nodes to demand nodes that satisﬁes the supply/demand constraints of each node and minimizes the overall “transport cost.” First, we have to deﬁne a function to describe the ﬂow entering i via an incoming edge (h, i) and exiting i via an outgoing edge (i, j). Let INi be the set of edges (h, i) with a ﬂow entering node i; similarly, let OUTi be the set of edges (i, j) with a ﬂow exiting node i. Then, the ﬂow entering and exiting node i is captured by x : E → R such that we can observe the combined incoming ﬂow, (h,i)∈INi x(h, i), from the entering edges INi, as well as the combined outgoing ﬂow, (i,j)∈OUTi x(i, j), via the exiting edges OUTi (see Figure 3). A valid ﬂow, x, must be found such that the net ﬂow at each node—the difference between its exiting ﬂow and its entering ﬂow—equals its speciﬁed supply or demand constraints. For example, in Figure 2 where the squares represent the supply and the triangles represent the demand, a solution for x would allow us to transport all the weight at the squares to ﬁll the triangles, via a set of routes connecting them. 
We present an approach to the automatic creation of extractive summaries of literary short stories. The summaries are produced with a speciﬁc objective in mind: to help a reader decide whether she would be interested in reading the complete story. To this end, the summaries give the user relevant information about the setting of the story without revealing its plot. The system relies on assorted surface indicators about clauses in the short story, the most important of which are those related to the aspectual type of a clause and to the main entities in a story. Fifteen judges evaluated the summaries on a number of extrinsic and intrinsic measures. The outcome of this evaluation suggests that the summaries are helpful in achieving the original objective. 1. Introduction In the last decade, automatic text summarization has become a popular research topic with a curiously restricted scope of applications. A few innovative research directions have emerged, including headline generation (Soricut and Marcu 2007), summarization of books (Mihalcea and Ceylan 2007), personalized summarization (D´ıaz and Gerva´s 2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), summarization of speech (Fuentes et al. 2005), dialogues (Zechner 2002), evaluative text (Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks 2007). In addition, more researchers have been venturing past purely extractive summarization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006). By and large, however, most research in text summarization still revolves around texts characterized by rigid structure. The better explored among such texts are news articles (Barzilay and McKeown 2005), medical documents (Elhadad et al. 2005), legal documents (Moens 2007), and papers in the area of computer science (Teufel and Moens 2002; Mei and Zhai 2008). Although summarizing these genres is a formidable challenge in itself, it excludes a continually increasing number of informal documents available electronically. Such documents, ranging from novels to personal Web pages, offer a wealth of information that merits the attention of the text summarization community. ∗ School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa, Ontario K1N 6N5, Canada. E-mail: ankazant@site.uottawa.ca. ∗∗ School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa, Ontario K1N 6N5, Canada. E-mail: szpak@site.uottawa.ca. Submission received: 3 April 2007; revised submission received: 20 January 2009; accepted for publication: 29 July 2009. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 1  We attempt to make a step in this direction by devising an approach to summarizing a relatively unexplored genre: literary short stories. Well-structured documents, such as news articles, exhibit a number of characteristics that help identify some of the important passages without performing in-depth semantic analysis. These characteristics include predictable location of typical items in a document and in its well-delineated parts, cue words, and template-like structure that often characterizes a genre (e.g., scientiﬁc papers). This is not the case in literature. Quite the contrary—to write ﬁction in accordance with a template is a sure way to write poor prose. One also cannot expect to ﬁnd portions of text that summarize the main idea behind a story, and even less so to ﬁnd them in the same location. In addition, the variety of literary devices (the widespread use of metaphor and ﬁgurative language, leaving things unsaid and relying on the reader’s skill of reading between the lines, frequent use of dialogue, etc.) makes summarizing ﬁction a very distinct task. It is a contribution of this work to demonstrate that summarizing short ﬁction is feasible using state-of-the-art tools in natural language technology. In the case of our corpus, this is also done without deep semantic resources or knowledge bases, although such resources would be of great help. We leverage syntactic information and shallow semantics (provided by a gazetteer) to produce indicative summaries of short stories that people ﬁnd helpful and that outperform naive baselines and two state-of-the-art generic summarizers. We have restricted the scope of this potentially vast project in several ways. In the course of this work we concentrate on producing summaries of short stories suitable for a particular purpose: to help a reader form adequate expectations about the complete story and decide whether she would be interested in reading it. To this end, the summary includes important elements of the setting of a story, such as the place and the main characters, presented as excerpts from the complete story. The assumption behind this deﬁnition is this: If a reader knows when and where the story takes place and who its main characters are, she should be able to make informed decisions about it. With such a deﬁnition of short story summaries, re-telling the plot in the summary is not among the objectives of this work; in fact, doing so is undesirable. We have introduced this limitation for two reasons. There is an “ideological” side of the decision: Not many people want to know what happens in a story before reading it, even if this may help them decide that the story is worth reading. There also is a practical side, namely the complexity of the problem: Summarizing the plot would be considerably more difﬁcult (see Section 2 for a review of related work). We hope to tackle this issue in the future. For now, creating indicative summaries of short stories is challenge enough. The summaries in Figures 1–3 illustrate our approach in the context of a naive lead baseline and a ceiling. Figure 1 shows an example of an automatically produced summary that meets the aforementioned criteria. A reader can see that the story is set in a restaurant where the customers are tended to by two waitresses: the fair Aileen who “wins hearts” and “the-bag-o’-meal” plain-faced Tildy. If the reader chooses to pursue the story, she will ﬁnd the description of an accident of paramount importance to Tildy: One day she is kissed by a customer in public! The event is more than ﬂattering to usually under-appreciated Tildy. It causes a complete change in how she views herself. The story then unfolds to reveal that the customer was drunk on the day in question and that he returned to apologize several days later. This apology is a severe blow to Tildy and an abrupt end of many a dream that the incident had spurred in her head. The story ends with Aileen trying to comfort her crying friend by saying “He ain’t anything 72  Kazantseva and Szpakowicz  Summarizing Short Stories  Figure 1 Example of a summary produced by the system. of a gentleman or he wouldn’t ever of apologized.” Yet, the summary in Figure 1 does not reveal these facts. For comparison, Figure 2 shows a summary obtained by taking the same number of sentences from the beginning of the story. As the reader can see, such a trivial approach is not sufﬁcient to create a useful summary. Figure 3 shows a manually created “ideal” summary. We experimented with a corpus of 47 stories from the 19th and early 20th century written by renowned writers, including O. Henry, Jerome K. Jerome, Anton Chekhov, and Guy de Maupassant. The stories, with the exception of a few fairy tales, are classical examples of short social ﬁction. The corpus was collected from Project Gutenberg (www.gutenberg.org) and only contains stories in English. The average length of a story is 3,333 tokens and the target compression rate expressed in the number of sentences is 94%. In order to create summaries of short stories that satisfy our stated criteria (henceforth indicative summaries), the system searches each story for sentences that focus on important entities and relate the background of the story (as opposed to events). Correspondingly, processing has two stages. Initially, the summarizer identiﬁes two types of important entities: main characters and locations. This is achieved using a gazetteer, resolving anaphoric expressions and then identifying frequently mentioned  Figure 2 Example of a lead baseline summary. 73  Computational Linguistics  Volume 36, Number 1  Figure 3 Example of a manual summary. entities. Next, the system selects sentences that set out the background of the story and focus on one of the important entities. In order to separate the background of a story from the plot (i.e., events), we rely on the notion of aspect.1 We approximate the aspectual type of a clause using either machine learning or manually produced rules. This is achieved by relying on an array of verb-related features, such as tense, lexical aspect of the main verb, presence of temporal expressions, and so on. Finally, the system composes a summary out of the selected sentences. Our task remains a signiﬁcant challenge despite its limited scope. To produce such indicative summaries successfully, one needs to consider many facets of the problem. An informative data representation, computational complexity, and usability of the ﬁnal product are only some of them. Because the project is at the stage of an advanced feasibility study, it has not been possible to do justice to all aspects of the problem. Therefore, we concentrated on several speciﬁc issues and left many more to future work and to fellow researchers. Firstly, we sought to identify characteristics of short stories that could be helpful in creating summaries. We devised an informative and practical data representation that could be reproduced without too much cost or effort. Secondly, we restricted ourselves to identifying the most informative portions of the stories and paid much less attention to readability and coherence of the resulting summaries. Even though readability is an important property, we hypothesized that informativeness is yet more important. Once the task of identifying informative passages has been accomplished, one can work on achieving coherence and readability. In the end, the emphasis was on the creation of extractive summaries using established tools and methods and on the identiﬁcation of genre-speciﬁc properties that can help summarization. The novelty of the task and the absence of agreed-upon measures for evaluating summaries of literary prose call for a thorough evaluation using a variety of metrics. That is why we conduct three distinct evaluation experiments. The summaries are 
Newly coined words pose problems for natural language processing systems because they are not in a system’s lexicon, and therefore no lexical information is available for such words. A common way to form new words is lexical blending, as in cosmeceutical, a blend of cosmetic and pharmaceutical. We propose a statistical model for inferring a blend’s source words drawing on observed linguistic properties of blends; these properties are largely based on the recognizability of the source words in a blend. We annotate a set of 1,186 recently coined expressions which includes 515 blends, and evaluate our methods on a 324-item subset. In this ﬁrst study of novel blends we achieve an accuracy of 40% on the task of inferring a blend’s source words, which corresponds to a reduction in error rate of 39% over an informed baseline. We also give preliminary results showing that our features for source word identiﬁcation can be used to distinguish blends from other kinds of novel words. 1. Lexical Blends Neologisms—newly coined words or new senses of an existing word—are constantly being introduced into a language (Algeo 1980; Lehrer 2003), often for the purpose of naming a new concept. Domains that are culturally prominent or that are rapidly advancing, such as electronic communication and the Internet, often contain many neologisms, although novel words arise throughout a language (Ayto 1990, 2006; Knowles and Elliott 1997). Consequently, any natural language processing (NLP) system operating on recently produced text will encounter new words. Because lexical resources are often a key component of an NLP system, performance of the entire system will likely suffer due to missing lexical information for neologisms. Ideally, an NLP system could identify neologisms as such, and then infer various aspects of their syntactic or semantic properties necessary for the computational task at hand. Recent approaches to this kind of lexical acquisition task typically infer the target lexical information from statistical distributional properties of the terms. However, this technique is generally not ∗ Department of Computer Science, University of Toronto, 6 King’s College Rd., Toronto, ON M5S 3G4, Canada, E-mail: pcook@cs.toronto.edu. ∗∗ Department of Computer Science, University of Toronto, 6 King’s College Rd., Toronto, ON M5S 3G4, Canada, E-mail: suzanne@cs.toronto.edu. Submission received: 4 November 2008; revised submission received: 23 May 2009; accepted for publication: 24 June 2009. © 2010 Association for Computational Linguistics  Computational Linguistics  Volume 36, Number 1  applicable to neologisms, which are relatively infrequent due to their recent introduction into the language. Fortunately, linguistic observations regarding neologisms— namely, that they are formed through speciﬁc word formation processes—can give insights for automatically learning their lexical properties. New words come about through a variety of means, including derivational morphology, compounding, and borrowing from another language (Algeo 1980; Bauer 1983; Plag 2003). Computational work on neologisms has largely focused on particular word formation processes, and has exploited information about the formation process to learn aspects of the semantic properties of words (Means 1988; Nadeau and Turney 2005; Baker and Brew 2008, for example). Subtractive word formations—words formed from partial orthographic or phonological content from existing words—have received a fair amount of attention recently in computational linguistics, particularly under the heading of inferring the long form of acronyms, especially in the bio-medical domain (e.g., Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example). Lexical blends—the focus of this study—also known as blends, are another common type of subtractive word formation. Most blends are formed by combining a preﬁx of one source word with a sufﬁx of another source word, as in brunch (breakfast and lunch). There may be overlap in the contribution of the source words, as in fantabulous ( fantastic and fabulous). It is also possible that one or both source words are entirely present, for example, gaydar ( gay radar) and jetiquette ( jet etiquette). We refer to blends such as these as simple two-word sequential blends, and focus on this common type of blend in this article. Blends in which (part of) a word is inserted within another (e.g., entertoyment, a blend of entertainment and toy) and blends formed from more than two source words (e.g., nofriendo from no, friend, and Nintendo) are rare. In Algeo’s (1991) study of new words, approximately 5% were blends. However, in our analysis of 1,186 words taken from a popular neologisms Web site, approximately 43% were blends. Clearly, computational techniques are needed that can augment lexicons with knowledge of novel blends. The precise nature and intended use of a computational lexicon will determine the degree of processing required of a novel blend. In some cases it may sufﬁce for the lexical entry for a blend to simply consist of its source words. For example, a system that employs a measure of distributional similarity may beneﬁt from replacing occurrences of a blend—likely a recently coined and hence low frequency item—by its source words, for which distributional information is likely available. In other cases, further semantic reasoning about the blend and its source words may be required (e.g., determining the semantic relationship between the source words as an approximation of the meaning of the blend). However, any approach to handling blends will need to recognize that a novel word is a blend and identify its source words. These two tasks are the focus of this article. Speciﬁcally, we draw on linguistic knowledge of how blends are formed as the basis for automatically determining the source words of a blend. Language users create blends that tend to be interpretable by others. Tapping into properties of blends believed to contribute to the recognizability of their source words— and hence the interpretability of the resulting blend—we develop statistical measures that indicate whether a word pair is likely the source words for a given blend. Moreover, the fact that a novel word is determined to have a “good” source word pair may be evidence that it is in fact a blend, because we are unlikely to ﬁnd two words that are a “good” source word pair for a non-blend. Thus, the statistical measures we develop for source word identiﬁcation may also be useful in recognizing a novel word as a blend. 130  Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English To our knowledge, the only computational treatment of blends is our earlier work that presents preliminary statistical methods and results for the two tasks of recognizing an unknown word as a blend and identifying its source words (Cook and Stevenson 2007). Here we extend that work in a number of important directions. We expand the statistical features to better capture co-occurrence patterns of the source words that can indicate the likelihood of their combination into a blend. We present experimental results conﬁrming that the extended features provide a substantial improvement over the earlier work on source word identiﬁcation. We further propose a means, based on linguistic factors of the source words, for pruning the number of word pairs that are considered for a blend. This ﬁltering heuristic greatly reduces the number of candidate source words processed, while giving modest gains in performance, even though this method excludes the correct word pair from consideration for a number of blends. We then consider the use of a much larger lexicon of candidate source words, which could potentially improve performance greatly as it contains the correct source word pair for many more blends than a smaller lexicon. We also make improvements to the earlier experimental data set and methods. In our earlier study, we use a data set consisting of a list of blends extracted from a dictionary. In the current work, we annotate a set of 324 blends (with their source words) from a recent database of neologisms, to enable a more legitimate testing of our method, on truly novel blends. Experiments on this new data set show that the recent blends differ from established blends in terms of their statistical properties, and emphasize the need for further resources of neologisms. We also experiment with a machine learning approach to combine the information from the statistical features in a more sophisticated manner than in our previous work. Finally, we perform more extensive experiments on distinguishing blends from other kinds of novel words. 2. A Statistical Model of Lexical Blends We present statistical features that are used to automatically infer the source words of a word known to be a lexical blend, and show that the same features can be used to distinguish blends from other types of neologisms. First, given a blend, we generate all word pairs that could have formed the blend. This set is termed the candidate set, and the word pairs it contains are referred to as candidate pairs (Section 2.1). Next, we extract a number of linguistically motivated statistical features for each candidate pair, as well as ﬁlter from the candidate sets those pairs that are unlikely to be source words due to their linguistic properties (Section 2.2). Later, we explain how we use the features to rank the candidate pairs according to how likely they are the source words for that blend. Interestingly, the “goodness” of a candidate pair is also related to how likely the word is actually a blend. 2.1 Candidate Sets To create the candidate set for a blend, we ﬁrst generate each preﬁx–sufﬁx pair such that the blend is composed of the preﬁx followed by the sufﬁx. (In this work, preﬁx and sufﬁx refer to the beginning or ending of a string, regardless of whether those portions are afﬁxes.) We restrict the preﬁxes and sufﬁxes to be of length two or more. This heuristic reduces the size of the candidate sets, yet generally does not exclude a blend’s source 131  Computational Linguistics  Volume 36, Number 1  words from its candidate set since it is uncommon for a source word to contribute less than two letters. For example, for brunch (breakfast+lunch) we consider the following preﬁx–sufﬁx pairs: br, unch; bru, nch; brun, ch. For each preﬁx–sufﬁx pair, we then ﬁnd in a lexicon all words beginning with the preﬁx and all words ending in the sufﬁx, ignoring hyphens and whitespace, and take the Cartesian product of the preﬁx words and sufﬁx words to form a list of candidate word pairs. The candidate set for the blend is the union of the candidate word pairs for all its preﬁx–sufﬁx pairs. Note that in this example, the candidate pair brute crunch would be included twice: once for the preﬁx–sufﬁx pair br, unch; and once again for bru, nch. Unlike in our previous study, we remove all such duplicate pairs from the ﬁnal candidate set. A candidate set for architourist, a blend of architecture and tourist, is given in Table 1.  2.2 Statistical Features Our statistical features are motivated by properties of blends observed in corpus-based studies, and by cognitive factors in human interpretation of blends, particularly relating to how easily humans can recognize a blend’s source words. All the features are formulated to give higher values for more likely candidate pairs. We organize the features into four groups—frequency; length, contribution, and phonology; semantics; and syllable structure—and describe each feature group in the following subsections.  2.2.1 Frequency. Various frequency properties of the source words inﬂuence how easily a language user recognizes the words that form a blend. Because blends are most usefully coined when the source words can be readily deduced, we hypothesize that frequencybased features will be useful in identifying blends and their source words. We propose ten features that draw on the frequency of candidate source words. Lehrer (2003) presents a study in which humans are asked to give the source words for blends. Among her ﬁndings are that frequent source words are more easily recognizable. Our ﬁrst two features—the frequency of each candidate word, freq(w1) and freq(w2)—reﬂect this ﬁnding. Lehrer also ﬁnds that the recognizability of a source word is further affected by both the number of words in its neighborhood— the set of words which begin/end with the preﬁx/sufﬁx which that source word  Table 1 A candidate set for architourist, a blend of architecture and tourist.  archimandrite archipelago architect architect architectural architectural architecturally architecturally architecture architecture archives archivist  tourist tourist behaviourist tourist behaviourist tourist behaviourist tourist behaviourist tourist tourist tourist  132  Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English  contributes—and the frequencies of those words. (Gries [2006] reports a similar ﬁnding.) Our next two features capture this insight:  freq(w1 )  freq(w2 )  (1)  freq(preﬁx)  freq(sufﬁx)  where freq(preﬁx) is the sum of the frequency of all words beginning with preﬁx, and similarly for freq(sufﬁx). These four features were used in our previous study; the following six features are new in this study. Because we observe that blends are often formed from two words that co-occur in language use, our previous study (Cook and Stevenson 2007) included a feature, the pointwise mutual information of w1 and w2, to reﬂect this. However, this feature provides only a weak indication that there is a semantic relation between two words sufﬁcient to lead to them being blended. Here we propose six new features that capture various co-occurrence frequencies as follows. A blend’s source words often correspond to a common sequence of words, for example, camouﬂanguage is camouﬂaged language. We therefore include two features based on Dice’s co-efﬁcient to capture the frequency with which the source words occur consecutively:  2 × freq(w1 w2)  2 × freq(w2 w1)  (2)  freq(w1) + freq(w2) freq(w1) + freq(w2)  Because many blends can be paraphrased by a conjunctive phrase—for example, broccoﬂower is broccoli and cauliﬂower—we also use a feature that reﬂects how often the candidate words are used in this way:  2 × ( freq(w1 and w2) + freq(w2 and w1))  (3)  freq(w1 and) + freq(and w1) + freq(w2 and) + freq(and w2)  Furthermore, some blends can be paraphrased by a noun modiﬁed by a prepositional phrase, for example, a nicotini is a martini with nicotine. Lauer (1995) suggests eight prepositional paraphrases for identifying the semantic relationship between the modiﬁer and head in a noun compound. Using the same paraphrases, the following feature measures how often two candidate source words occur with any of the following prepositions P between them: about, at, for, from, in, of, on, with:  2 × ( freq(w1 P w2) + freq(w2 P w1))  (4)  freq(w1 P) + freq(P w1) + freq(w2 P) + freq(P w2)  where freq(w P v) is the sum of the frequency of w and v occurring with each of the eight prepositions between w and v, and freq(w P) is the sum of the frequency of w occurring with each of the eight prepositions immediately following w.  133  Computational Linguistics  Volume 36, Number 1  Because the previous three features target the source words occurring in very speciﬁc patterns, we also count the candidate source words occurring in any of the patterns in an effort to avoid data sparseness problems.  2 × ( freq(w1 w2) + freq(w2 w1) + freq(w1 and w2)  +freq(w2 and w1) + freq(w1 P w2) + freq(w2 P w1))  (5)  freq(w1) + freq(w2)  Finally, because the above patterns are very speciﬁc, and do not capture general cooccurrence information which may also be useful in identifying a blend’s source words, we include the following feature which counts the candidate source words co-occurring within a ﬁve-word window.  2 × freq(w1, w2 in a 5 word window)  (6)  freq(w1) + freq(w2)  2.2.2 Length, Contribution, and Phonology. Ten features tap into properties of the orthographic or phonetic composition of the source words and blend. In our previous work on blends, we found such features unhelpful in source word identiﬁcation. Here, we propose revised versions of our old features, and a few new ones. Note that although we use information about the phonological and/or syllabic structure of the source words, we do not assume such knowledge for the blend itself, since it is a neologism for which such lexical information is typically unavailable. The ﬁrst word in a conjunct tends to be shorter than the second, and this also seems to be the case for the source words in blends (Kelly 1998; Gries 2004). The ﬁrst three features therefore capture this tendency based on the graphemic, phonemic, and syllabic length of w2 relative to w1, respectively: lengraphemes(w2 ) (7) lengraphemes(w1 ) + lengraphemes(w2 ) lenphonemes(w2 ) (8) lenphonemes(w1 ) + lenphonemes(w2 ) lensyllables(w2 ) (9) lensyllables(w1 ) + lensyllables(w2 ) A blend and its second source word also tend to be similar in length, possibly because, similar to compounds, the second source word of a blend is often the head; therefore it is this word that determines the overall phonological structure of the resulting blend (Kubozono 1990). The following feature captures this property using graphemic length 134  Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English  as an approximation to phonemic length, because as stated previously, we assume no phonological information about the blend.  
Gian Piero Zarri’s book summarizes more than a decade of his research on knowledge representation for narrative text. The centerpiece of Zarri’s work is the Narrative Knowledge Representation Language (NKRL), which he describes and compares to other competing theories. In addition, he discusses how to model the meaning of narrative text by giving many real-world examples. NKRL provides three different components or capabilities: (a) a representation system, (b) inferencing, and (c) an implementation. It is implemented via a Java-based system that shows how a representational theory can be applied to narrative texts. The book consists of ﬁve chapters and two appendices. Chapter 1 introduces the basic principles of NKRL. The chapter ﬁrst deﬁnes the focus on nonﬁction narratives by contrasting the domain with ﬁctional narratives, for example, a novel. Zarri chooses n-ary predicates in order to represent events formally. He argues for a neo-Davidsonian knowledge representation following Schank (1980), Schubert (1976), and others, and at the same time he sets his approach apart from the knowledge representation proposals one can ﬁnd in Semantic Web representation languages such as RDF and OWL. However, Zarri emphasizes that NKRL, despite its similarity to conceptual graphs (Sowa 1999), is more focused on practical applications. The chapter concludes by introducing so-called templates in an attempt to demonstrate the practical usefulness of NKRL. Chapter 2 provides an in-depth description of NKRL. Four connected components are introduced: r The deﬁnitional component provides a hierarchy of abstract concepts (e.g., artifact, company, activity) called HClass (hierarchy of classes). r The descriptive component is a hierarchy of event types called HTemp (hierarchy of templates) commonly found in the domain of non-ﬁction narratives (e.g., moving an object, producing a task or activity). r The factual component describes the concrete instantiation of an event. For example, the sentence Berlex Laboratories have performed an evaluation of a given compound would be represented as [PRODUCE: [ SUBJ BERLEX LABORATORIES, OBJ: ASSESSMENT 1, TOPIC: COMPOUND 27]]  Computational Linguistics  Volume 36, Number 1  r The enumerative component links the values of the event description from the factual component to unique instantiations of these participants of an object, such as in COMPOUND 27. This chapter also contains a comparison of NKRL and other formalisms that deal with the representation of temporal information, such as TimeML (Pustejovsky et al. 2003) or Discourse Representation Theory (DRT; Kamp and Reyle 1993). A detailed description is given of how NKRL represents temporal information based on Allen’s (1984) interval calculus and how NKRL approaches the problem of underspeciﬁed or coarse temporal information such as in around December 25, 2005. Chapter 3 adds more information about the semantics and the ontologies in NKRL. A set of predeﬁned conceptual structures is introduced and numerous examples taken from real-world narratives are provided. The deﬁnitional component introduced in Chapter 2 is ﬂeshed out and the distinction between sortal and non-sortal concepts (e.g., CHAIR versus GOLD) is described. This hierarchy is quite similar to other so-called upper-level ontologies such as CYC or SUMO (Guha and Lenat 1991; Pease, Niles, and Li 2002), which are introduced to the reader in more detail in the beginning of this chapter. In addition to the conceptual hierarchy HClass, the descriptive component HTemp holds a set of often-used templates. Each template is described with a speciﬁc example that shows how different slots of the templates may be ﬁlled. Chapter 4 covers in more detail how inferences can be drawn within the implemented system. The NKRL system provides several query tools for retrieving information from the knowledge base encoded in NKRL annotations, as described by the previous chapters. The query tools comprise querying by search patterns, uniﬁcation / ﬁltering operations, and indexing temporal information. The indexing of temporal information considers different levels of temporal information including a temporal perspective. The temporal perspective is used to represent information about when an event starts or ends in addition to whether it is observed by somebody. Chapter 5 provides the author’s conclusions, suggesting technological and theoretical enhancement to the current version of NKRL. Appendix A contains a detailed description of the NKRL software and Appendix B discusses the treatment of a particular linguistic phenomenon within NKRL: plural entities. The book offers a unique combination of different tools for modeling narrative information. It contains valuable discussions of important questions such as whether n-ary predicates should be used. However, some of these discussions would have beneﬁted from a more in-depth treatment. The comparison with TimeML (Pustejovsky et al. 2003), for example, only partly covers recent developments and does not mention software that utilizes TimeML. For example, the TARSQI toolset,1 not mentioned by Zarri, allows the user to extract events and temporal expressions while temporal links are derived and consistency checks can be run via a constraint propagation component. Zarri often compares and contrasts his work with the representation languages used for the Semantic Web, such as RDF and OWL. He rightly points out similarities while addressing shortcomings of the Semantic Web technology (e.g., restriction to ternary predicates). But he overlooks an important point: RDF and OWL were not created for the semantic representation of non-ﬁctional narratives—the focus of this book. Halevy, Norvig, and Pereira (2009), for instance, point out that one needs to distinguish between  
Gian Piero Zarri’s book summarizes more than a decade of his research on knowledge representation for narrative text. The centerpiece of Zarri’s work is the Narrative Knowledge Representation Language (NKRL), which he describes and compares to other competing theories. In addition, he discusses how to model the meaning of narrative text by giving many real-world examples. NKRL provides three different components or capabilities: (a) a representation system, (b) inferencing, and (c) an implementation. It is implemented via a Java-based system that shows how a representational theory can be applied to narrative texts. The book consists of ﬁve chapters and two appendices. Chapter 1 introduces the basic principles of NKRL. The chapter ﬁrst deﬁnes the focus on nonﬁction narratives by contrasting the domain with ﬁctional narratives, for example, a novel. Zarri chooses n-ary predicates in order to represent events formally. He argues for a neo-Davidsonian knowledge representation following Schank (1980), Schubert (1976), and others, and at the same time he sets his approach apart from the knowledge representation proposals one can ﬁnd in Semantic Web representation languages such as RDF and OWL. However, Zarri emphasizes that NKRL, despite its similarity to conceptual graphs (Sowa 1999), is more focused on practical applications. The chapter concludes by introducing so-called templates in an attempt to demonstrate the practical usefulness of NKRL. Chapter 2 provides an in-depth description of NKRL. Four connected components are introduced: r The deﬁnitional component provides a hierarchy of abstract concepts (e.g., artifact, company, activity) called HClass (hierarchy of classes). r The descriptive component is a hierarchy of event types called HTemp (hierarchy of templates) commonly found in the domain of non-ﬁction narratives (e.g., moving an object, producing a task or activity). r The factual component describes the concrete instantiation of an event. For example, the sentence Berlex Laboratories have performed an evaluation of a given compound would be represented as [PRODUCE: [ SUBJ BERLEX LABORATORIES, OBJ: ASSESSMENT 1, TOPIC: COMPOUND 27]]  Computational Linguistics  Volume 36, Number 1  r The enumerative component links the values of the event description from the factual component to unique instantiations of these participants of an object, such as in COMPOUND 27. This chapter also contains a comparison of NKRL and other formalisms that deal with the representation of temporal information, such as TimeML (Pustejovsky et al. 2003) or Discourse Representation Theory (DRT; Kamp and Reyle 1993). A detailed description is given of how NKRL represents temporal information based on Allen’s (1984) interval calculus and how NKRL approaches the problem of underspeciﬁed or coarse temporal information such as in around December 25, 2005. Chapter 3 adds more information about the semantics and the ontologies in NKRL. A set of predeﬁned conceptual structures is introduced and numerous examples taken from real-world narratives are provided. The deﬁnitional component introduced in Chapter 2 is ﬂeshed out and the distinction between sortal and non-sortal concepts (e.g., CHAIR versus GOLD) is described. This hierarchy is quite similar to other so-called upper-level ontologies such as CYC or SUMO (Guha and Lenat 1991; Pease, Niles, and Li 2002), which are introduced to the reader in more detail in the beginning of this chapter. In addition to the conceptual hierarchy HClass, the descriptive component HTemp holds a set of often-used templates. Each template is described with a speciﬁc example that shows how different slots of the templates may be ﬁlled. Chapter 4 covers in more detail how inferences can be drawn within the implemented system. The NKRL system provides several query tools for retrieving information from the knowledge base encoded in NKRL annotations, as described by the previous chapters. The query tools comprise querying by search patterns, uniﬁcation / ﬁltering operations, and indexing temporal information. The indexing of temporal information considers different levels of temporal information including a temporal perspective. The temporal perspective is used to represent information about when an event starts or ends in addition to whether it is observed by somebody. Chapter 5 provides the author’s conclusions, suggesting technological and theoretical enhancement to the current version of NKRL. Appendix A contains a detailed description of the NKRL software and Appendix B discusses the treatment of a particular linguistic phenomenon within NKRL: plural entities. The book offers a unique combination of different tools for modeling narrative information. It contains valuable discussions of important questions such as whether n-ary predicates should be used. However, some of these discussions would have beneﬁted from a more in-depth treatment. The comparison with TimeML (Pustejovsky et al. 2003), for example, only partly covers recent developments and does not mention software that utilizes TimeML. For example, the TARSQI toolset,1 not mentioned by Zarri, allows the user to extract events and temporal expressions while temporal links are derived and consistency checks can be run via a constraint propagation component. Zarri often compares and contrasts his work with the representation languages used for the Semantic Web, such as RDF and OWL. He rightly points out similarities while addressing shortcomings of the Semantic Web technology (e.g., restriction to ternary predicates). But he overlooks an important point: RDF and OWL were not created for the semantic representation of non-ﬁctional narratives—the focus of this book. Halevy, Norvig, and Pereira (2009), for instance, point out that one needs to distinguish between  
We like to think that the progress of science follows an exponential curve, boldly shooting upwards, ever more speedily. In reality, there is a ﬁrmament of darkness (that’s our daily slog) sparsely spotted with dim lights (those are discoveries). Fittingly, discoverers are also few, just as in every ﬁeld of human endeavor. For every great leader, artist, or scientist there is a multitude of followers, and this is as it should be. There can only be so many winners—and only so much winning research. Anecdotal evidence has it that one project in a hundred gives back: Its results justify the investment.1 And yet, our cocky attitude: We tend to count ourselves among that one hundredth. We need success. We simply must excel. That is, after all, how scientists are judged. Those of us who have sat on hiring or promotion committees know how evaluation works; surely, we all get to be at the receiving end. One needs many publications for a thriving career, and we publish if we show how we go one better on someone else’s result. So, we cannot afford a setback, a path wrongly taken, a poor result. There is no room for that in our line of work. This already sounds dismal, but it gets worse. People have come to equate every negative result with failure. Suppose you have set up an experiment carefully and in good faith, but still it comes up short. That’s not a positive outcome. Maybe your intuition has let you down. Maybe this cannot work. Wait, maybe you can prove that it cannot work? No, forget it. You have already failed. Don’t waste any more time. Cut your losses and move on. Well, reconsider! You can miss a truly fundamental lesson. In 1887, Michelson and Morley set out to prove the existence of ether, thought to be a conduit for light. They could not prove it: they failed. But—their null result prompted a line of research which culminated in the theory of special relativity. House of Peers Peer review breeds mediocrity. Galileos, Brunos, Modiglianis, and van Goghs go against the grain; the scorn of their peers and the indifference of the public crush them. An obliging self-promoter will fare much better. Peer review also censures failure. A ∗ School of Information Technology and Engineering, University of Ottawa, 800 King Edward Avenue, Ottawa, Ontario, K1N 6N5, Canada. Institute of Computer Science, Polish Academy of Sciences, Ordona 21, 01-237 Warszawa, Poland. E-mail: szpak@site.uottawa.ca. 
This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efﬁcient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger. 
We study self-training with products of latent variable grammars in this paper. We show that increasing the quality of the automatically parsed data used for self-training gives higher accuracy self-trained grammars. Our generative self-trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without selftraining. Additionally, we show that multiple self-trained grammars can be combined in a product model to achieve even higher accuracy. The product model is most effective when the individual underlying grammars are most diverse. Combining multiple grammars that were self-trained on disjoint sets of unlabeled data results in a ﬁnal test accuracy of 92.5% on the WSJ test set and 89.6% on our Broadcast News test set. 
Syntactic consistency is the preference to reuse a syntactic construction shortly after its appearance in a discourse. We present an analysis of the WSJ portion of the Penn Treebank, and show that syntactic consistency is pervasive across productions with various lefthand side nonterminals. Then, we implement a reranking constituent parser that makes use of extra-sentential context in its feature set. Using a linear-chain conditional random ﬁeld, we improve parsing accuracy over the generative baseline parser on the Penn Treebank WSJ corpus, rivalling a similar model that does not make use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies. 
We present a uniﬁed view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 
In this paper, we develop multilingual supervised latent Dirichlet allocation (MLSLDA), a probabilistic generative model that allows insights gleaned from one language’s data to inform how the model captures properties of other languages. MLSLDA accomplishes this by jointly modeling two aspects of text: how multilingual concepts are clustered into thematically coherent topics and how topics associated with text connect to an observed regression variable (such as ratings on a sentiment scale). Concepts are represented in a general hierarchical framework that is ﬂexible enough to express semantic ontologies, dictionaries, clustering constraints, and, as a special, degenerate case, conventional topic models. Both the topics and the regression are discovered via posterior inference from corpora. We show MLSLDA can build topics that are consistent across languages, discover sensible bilingual lexical correspondences, and leverage multilingual corpora to better predict sentiment. Sentiment analysis (Pang and Lee, 2008) offers the promise of automatically discerning how people feel about a product, person, organization, or issue based on what they write online, which is potentially of great value to businesses and other organizations. However, the vast majority of sentiment resources and algorithms are limited to a single language, usually English (Wilson, 2008; Baccianella and Sebastiani, 2010). Since no single language captures a majority of the content online, adopting such a limited approach in an increasingly global community risks missing important details and trends that might only be available when text in multiple languages is taken into account.  Up to this point, multiple languages have been addressed in sentiment analysis primarily by transferring knowledge from a resource-rich language to a less rich language (Banea et al., 2008), or by ignoring differences in languages via translation into English (Denecke, 2008). These approaches are limited to a view of sentiment that takes place through an English-centric lens, and they ignore the potential to share information between languages. Ideally, learning sentiment cues holistically, across languages, would result in a richer and more globally consistent picture. In this paper, we introduce Multilingual Supervised Latent Dirichlet Allocation (MLSLDA), a model for sentiment analysis on a multilingual corpus. MLSLDA discovers a consistent, uniﬁed picture of sentiment across multiple languages by learning “topics,” probabilistic partitions of the vocabulary that are consistent in terms of both meaning and relevance to observed sentiment. Our approach makes few assumptions about available resources, requiring neither parallel corpora nor machine translation. The rest of the paper proceeds as follows. In Section 1, we describe the probabilistic tools that we use to create consistent topics bridging across languages and the MLSLDA model. In Section 2, we present the inference process. We discuss our set of semantic bridges between languages in Section 3, and our experiments in Section 4 demonstrate that this approach functions as an effective multilingual topic model, discovers sentiment-biased topics, and uses multilingual corpora to make better sentiment predictions across languages. Sections 5 and 6 discuss related research and discusses future work, respectively.  45 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 45–55, MIT, Massachusetts, USA, 9-11 October 2010. c 2010 Association for Computational Linguistics  
Discovering and summarizing opinions from online reviews is an important and challenging task. A commonly-adopted framework generates structured review summaries with aspects and opinions. Recently topic models have been used to identify meaningful review aspects, but existing topic models do not identify aspect-speciﬁc opinion words. In this paper, we propose a MaxEnt-LDA hybrid model to jointly discover both aspects and aspect-speciﬁc opinion words. We show that with a relatively small amount of training data, our model can effectively identify aspect and opinion words simultaneously. We also demonstrate the domain adaptability of our model. 
This paper presents a two-stage approach to summarizing multiple contrastive viewpoints in opinionated text. In the ﬁrst stage, we use an unsupervised probabilistic approach to model and extract multiple viewpoints in text. We experiment with a variety of lexical and syntactic features, yielding signiﬁcant performance gains over bag-of-words feature sets. In the second stage, we introduce Comparative LexRank, a novel random walk formulation to score sentences and pairs of sentences from opposite viewpoints based on both their representativeness of the collection as well as their contrastiveness with each other. Experimental results show that the proposed approach can generate informative summaries of viewpoints in opinionated text. 
In the 1980s, plot units were proposed as a conceptual knowledge structure for representing and summarizing narrative stories. Our research explores whether current NLP technology can be used to automatically produce plot unit representations for narrative text. We create a system called AESOP that exploits a variety of existing resources to identify affect states and applies “projection rules” to map the affect states onto the characters in a story. We also use corpus-based techniques to generate a new type of affect knowledge base: verbs that impart positive or negative states onto their patients (e.g., being eaten is an undesirable state, but being fed is a desirable state). We harvest these “patient polarity verbs” from a Web corpus using two techniques: co-occurrence with Evil/Kind Agent patterns, and bootstrapping over conjunctions of verbs. We evaluate the plot unit representations produced by our system on a small collection of Aesop’s fables. 
Recent times have seen a tremendous growth in mobile based data services that allow people to use Short Message Service (SMS) to access these data services. In a multilingual society it is essential that data services that were developed for a speciﬁc language be made accessible through other local languages also. In this paper, we present a service that allows a user to query a FrequentlyAsked-Questions (FAQ) database built in a local language (Hindi) using Noisy SMS English queries. The inherent noise in the SMS queries, along with the language mismatch makes this a challenging problem. We handle these two problems by formulating the query similarity over FAQ questions as a combinatorial search problem where the search space consists of combinations of dictionary variations of the noisy query and its top-N translations. We demonstrate the effectiveness of our approach on a real-life dataset. 
We present a machine learning approach for the task of ranking previously answered questions in a question repository with respect to their relevance to a new, unanswered reference question. The ranking model is trained on a collection of question groups manually annotated with a partial order relation reﬂecting the relative utility of questions inside each group. Based on a set of meaning and structure aware features, the new ranking model is able to substantially outperform more straightforward, unsupervised similarity measures. 
The PECO framework is a knowledge representation for formulating clinical questions. Queries are decomposed into four aspects, which are Patient-Problem (P), Exposure (E), Comparison (C) and Outcome (O). However, no test collection is available to evaluate such framework in information retrieval. In this work, we ﬁrst present the construction of a large test collection extracted from systematic literature reviews. We then describe an analysis of the distribution of PECO elements throughout the relevant documents and propose a language modeling approach that uses these distributions as a weighting strategy. In our experiments carried out on a collection of 1.5 million documents and 423 queries, our method was found to lead to an improvement of 28% in MAP and 50% in P@5, as compared to the state-of-the-art method.  decisions about patient care. Practice EBM means integrating individual clinical expertise with the best available external clinical evidence from systematic research. It involves tracking down the best evidence from randomized trials or meta-analyses with which to answer clinical questions. Richardson et al. (1995) identiﬁed the following four aspects as the key elements of a well-built clinical question: • Patient-problem: what are the patient characteristics (e.g. age range, gender, etc.)? What is the primary condition or disease? • Exposure-intervention: what is the main intervention (e.g. drug, treatment, duration, etc.)? • Comparison: what is the exposure compared to (e.g. placebo, another drug, etc.)? • Outcome: what are the clinical outcomes (e.g. healing, morbidity, side effects, etc.)?  
In this paper, we present a novel approach to Web search result clustering based on the automatic discovery of word senses from raw text, a task referred to as Word Sense Induction (WSI). We ﬁrst acquire the senses (i.e., meanings) of a query by means of a graphbased clustering algorithm that exploits cycles (triangles and squares) in the co-occurrence graph of the query. Then we cluster the search results based on their semantic similarity to the induced word senses. Our experiments, conducted on datasets of ambiguous queries, show that our approach improves search result clustering in terms of both clustering quality and degree of diversiﬁcation. 
Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation that makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e. paraphrases) with only monolingual knowledge of the source language. Evaluations demonstrate that this approach can yield substantial improvements in translation quality. 
In this paper, we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features. Rather than directly using treebank categories as in previous studies, we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed, word-aligned parallel corpus, based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side. In our model, each X nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints. 
We propose a language-independent approach for improving statistical machine translation for morphologically rich languages using a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process. Our model extends the classic phrase-based model by means of (1) word boundary-aware morpheme-level phrase extraction, (2) minimum error-rate training for a morpheme-level translation model using word-level BLEU, and (3) joint scoring with morpheme- and word-level language models. Further improvements are achieved by combining our model with the classic one. The evaluation on English to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically signiﬁcant improvements over the classic model based on BLEU and human judgments. 
As a prerequisite to translation of poetry, we implement the ability to produce translations with meter and rhyme for phrase-based MT, examine whether the hypothesis space of such a system is ﬂexible enough to accomodate such constraints, and investigate the impact of such constraints on translation quality. 
We describe a new scalable algorithm for semi-supervised training of conditional random ﬁelds (CRF) and its application to partof-speech (POS) tagging. The algorithm uses a similarity graph to encourage similar ngrams to have similar POS tags. We demonstrate the efﬁcacy of our approach on a domain adaptation task, where we assume that we have access to large amounts of unlabeled data from the target domain, but no additional labeled data. The similarity graph is used during training to smooth the state posteriors on the target domain. Standard inference can be used at test time. Our approach is able to scale to very large problems and yields signiﬁcantly improved target domain accuracy. 
This paper focuses on the task of inserting punctuation symbols into transcribed conversational speech texts, without relying on prosodic cues. We investigate limitations associated with previous methods, and propose a novel approach based on dynamic conditional random ﬁelds. Different from previous work, our proposed approach is designed to jointly perform both sentence boundary and sentence type prediction, and punctuation prediction on speech utterances. We performed evaluations on a transcribed conversational speech domain consisting of both English and Chinese texts. Empirical results show that our method outperforms an approach based on linear-chain conditional random ﬁelds and other previous approaches. 
Many sequence labeling tasks in NLP require solving a cascade of segmentation and tagging subtasks, such as Chinese POS tagging, named entity recognition, and so on. Traditional pipeline approaches usually suffer from error propagation. Joint training/decoding in the cross-product state space could cause too many parameters and high inference complexity. In this paper, we present a novel method which integrates graph structures of two subtasks into one using virtual nodes, and performs joint training and decoding in the factorized state space. Experimental evaluations on CoNLL 2000 shallow parsing data set and Fourth SIGHAN Bakeoff CTB POS tagging data set demonstrate the superiority of our method over cross-product, pipeline and candidate reranking approaches. 
We deﬁne the crouching Dirichlet, hidden Markov model (CDHMM), an HMM for partof-speech tagging which draws state prior distributions for each local document context. This simple modiﬁcation of the HMM takes advantage of the dichotomy in natural language between content and function words. In contrast, a standard HMM draws all prior distributions once over all states and it is known to perform poorly in unsupervised and semisupervised POS tagging. This modiﬁcation signiﬁcantly improves unsupervised POS tagging performance across several measures on ﬁve data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 
The problem of automatically classifying the gender of a blog author has important applications in many commercial domains. Existing systems mainly use features such as words, word classes, and POS (part-ofspeech) n-grams, for classification learning. In this paper, we propose two new techniques to improve the current result. The first technique introduces a new class of features which are variable length POS sequence patterns mined from the training data using a sequence pattern mining algorithm. The second technique is a new feature selection method which is based on an ensemble of several feature selection criteria and approaches. Empirical evaluation using a real-life blog data set shows that these two techniques improve the classification accuracy of the current state-ofthe-art methods significantly. 
This paper studies the effects of training data on binary text classification and postulates that negative training data is not needed and may even be harmful for the task. Traditional binary classification involves building a classifier using labeled positive and negative training examples. The classifier is then applied to classify test instances into positive and negative classes. A fundamental assumption is that the training and test data are identically distributed. However, this assumption may not hold in practice. In this paper, we study a particular problem where the positive data is identically distributed but the negative data may or may not be so. Many practical text classification and retrieval applications fit this model. We argue that in this setting negative training data should not be used, and that PU learning can be employed to solve the problem. Empirical evaluation has been conducted to support our claim. This result is important as it may fundamentally change the current binary classification paradigm. 
Automated essay scoring is one of the most important educational applications of natural language processing. Recently, researchers have begun exploring methods of scoring essays with respect to particular dimensions of quality such as coherence, technical errors, and relevance to prompt, but there is relatively little work on modeling organization. We present a new annotated corpus and propose heuristic-based and learning-based approaches to scoring essays along the organization dimension, utilizing techniques that involve sequence alignment, alignment kernels, and string kernels. 
Models of latent document semantics such as the mixture of multinomials model and Latent Dirichlet Allocation have received substantial attention for their ability to discover topical semantics in large collections of text. In an effort to apply such models to noisy optical character recognition (OCR) text output, we endeavor to understand the effect that character-level noise can have on unsupervised topic modeling. We show the effects both with document-level topic analysis (document clustering) and with word-level topic analysis (LDA) on both synthetic and real-world OCR data. As expected, experimental results show that performance declines as word error rates increase. Common techniques for alleviating these problems, such as ﬁltering low-frequency words, are successful in enhancing model quality, but exhibit failure trends similar to models trained on unprocessed OCR output in the case of LDA. To our knowledge, this study is the ﬁrst of its kind. 
We present three novel methods of compactly storing very large n-gram language models. These methods use substantially less space than all known approaches and allow n-gram probabilities or counts to be retrieved in constant time, at speeds comparable to modern language modeling toolkits. Our basic approach generates an explicit minimal perfect hash function, that maps all n-grams in a model to distinct integers to enable storage of associated values. Extensions of this approach exploit distributional characteristics of n-gram data to reduce storage costs, including variable length coding of values and the use of tiered structures that partition the data for more efﬁcient storage. We apply our approach to storing the full Google Web1T n-gram set and all 1-to-5 grams of the Gigaword newswire corpus. For the 1.5 billion n-grams of Gigaword, for example, we can store full count information at a cost of 1.66 bytes per n-gram (around 30% of the cost when using the current stateof-the-art approach), or quantized counts for 1.41 bytes per n-gram. For applications that are tolerant of a certain class of relatively innocuous errors (where unseen n-grams may be accepted as rare n-grams), we can reduce the latter cost to below 1 byte per n-gram. 
Syntax-based translation models should in principle be efﬁcient with polynomially-sized search space, but in practice they are often embarassingly slow, partly due to the cost of language model integration. In this paper we borrow from phrase-based decoding the idea to generate a translation incrementally left-to-right, and show that for tree-to-string models, with a clever encoding of derivation history, this method runs in averagecase polynomial-time in theory, and lineartime with beam search in practice (whereas phrase-based decoding is exponential-time in theory and quadratic-time in practice). Experiments show that, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++). 
Strong indications of perspective can often come from collocations of arbitrary length; for example, someone writing get the government out of my X is typically expressing a conservative rather than progressive viewpoint. However, going beyond unigram or bigram features in perspective classiﬁcation gives rise to problems of data sparsity. We address this problem using nonparametric Bayesian modeling, speciﬁcally adaptor grammars (Johnson et al., 2006). We demonstrate that an adaptive na¨ıve Bayes model captures multiword lexical usages associated with perspective, and establishes a new state-of-the-art for perspective classiﬁcation results using the Bitter Lemons corpus, a collection of essays about mid-east issues from Israeli and Palestinian points of view. 
In many applications, replacing a complex word form by its stem can reduce sparsity, revealing connections in the data that would not otherwise be apparent. In this paper, we focus on preﬁx verbs: verbs formed by adding a preﬁx to an existing verb stem. A preﬁx verb is considered compositional if it can be decomposed into a semantically equivalent expression involving its stem. We develop a classiﬁer to predict compositionality via a range of lexical and distributional features, including novel features derived from web-scale Ngram data. Results on a new annotated corpus show that preﬁx verb compositionality can be predicted with high accuracy. Our system also performs well when trained and tested on conventional morphological segmentations of preﬁx verbs. 
We show that jointly performing semantic role labeling (SRL) on bitext can improve SRL results on both sides. In our approach, we use monolingual SRL systems to produce argument candidates for predicates in bitext at ﬁrst. Then, we simultaneously generate SRL results for two sides of bitext using our joint inference model. Our model prefers the bilingual SRL result that is not only reasonable on each side of bitext, but also has more consistent argument structures between two sides. To evaluate the consistency between two argument structures, we also formulate a log-linear model to compute the probability of aligning two arguments. We have experimented with our model on Chinese-English parallel PropBank data. Using our joint inference model, F1 scores of SRL results on Chinese and English text achieve 79.53% and 77.87% respectively, which are 1.52 and 1.74 points higher than the results of baseline monolingual SRL combination systems respectively. 
This paper presents a method for the automatic discovery of MANNER relations from text. An extended deﬁnition of MANNER is proposed, including restrictions on the sorts of concepts that can be part of its domain and range. The connections with other relations and the lexico-syntactic patterns that encode MANNER are analyzed. A new feature set specialized on MANNER detection is depicted and justiﬁed. Experimental results show improvement over previous attempts to extract MANNER. Combinations of MANNER with other semantic relations are also discussed. 
Polysemy is a major characteristic of natural languages. Like words, syntactic forms can have several meanings. Understanding the correct meaning of a syntactic form is of great importance to many NLP applications. In this paper we address an important type of syntactic polysemy – the multiple possible senses of tense syntactic forms. We make our discussion concrete by introducing the task of Tense Sense Disambiguation (TSD): given a concrete tense syntactic form present in a sentence, select its appropriate sense among a set of possible senses. Using English grammar textbooks, we compiled a syntactic sense dictionary comprising common tense syntactic forms and semantic senses for each. We annotated thousands of BNC sentences using the deﬁned senses. We describe a supervised TSD algorithm trained on these annotations, which outperforms a strong baseline for the task. 
Information-extraction (IE) research typically focuses on clean-text inputs. However, an IE engine serving real applications yields many false alarms due to less-well-formed input. For example, IE in a multilingual broadcast processing system has to deal with inaccurate automatic transcription and translation. The resulting presence of non-target-language text in this case, and non-language material interspersed in data from other applications, raise the research problem of making IE robust to such noisy input text. We address one such IE task: entity-mention detection. We describe augmenting a statistical mention-detection system in order to reduce false alarms from spurious passages. The diverse nature of input noise leads us to pursue a multi-faceted approach to robustness. For our English-language system, at various miss rates we eliminate 97% of false alarms on inputs from other Latin-alphabet languages. In another experiment, representing scenarios in which genre-speciﬁc training is infeasible, we process real ﬁnancial-transactions text containing mixed languages and data-set codes. On these data, because we do not train on data like it, we achieve a smaller but signiﬁcant improvement. These gains come with virtually no loss in accuracy on clean English text. 
Seed sampling is critical in semi-supervised learning. This paper proposes a clusteringbased stratified seed sampling approach to semi-supervised learning. First, various clustering algorithms are explored to partition the unlabeled instances into different strata with each stratum represented by a center. Then, diversity-motivated intra-stratum sampling is adopted to choose the center and additional instances from each stratum to form the unlabeled seed set for an oracle to annotate. Finally, the labeled seed set is fed into a bootstrapping procedure as the initial labeled data. We systematically evaluate our stratified bootstrapping approach in the semantic relation classification subtask of the ACE RDC (Relation Detection and Classification) task. In particular, we compare various clustering algorithms on the stratified bootstrapping performance. Experimental results on the ACE RDC 2004 corpus show that our clusteringbased stratified bootstrapping approach achieves the best F1-score of 75.9 on the subtask of semantic relation classification, approaching the one with golden clustering. 
Multi-category bootstrapping algorithms were developed to reduce semantic drift. By extracting multiple semantic lexicons simultaneously, a category’s search space may be restricted. The best results have been achieved through reliance on manually crafted negative categories. Unfortunately, identifying these categories is non-trivial, and their use shifts the unsupervised bootstrapping paradigm towards a supervised framework. We present NEG-FINDER, the ﬁrst approach for discovering negative categories automatically. NEG-FINDER exploits unsupervised term clustering to generate multiple negative categories during bootstrapping. Our algorithm effectively removes the necessity of manual intervention and formulation of negative categories, with performance closely approaching that obtained using negative categories deﬁned by a domain expert. 
Existing graph-based ranking methods for keyphrase extraction compute a single importance score for each word via a single random walk. Motivated by the fact that both documents and words can be represented by a mixture of semantic topics, we propose to decompose traditional random walk into multiple random walks speciﬁc to various topics. We thus build a Topical PageRank (TPR) on word graph to measure word importance with respect to different topics. After that, given the topic distribution of the document, we further calculate the ranking scores of words and extract the top ranked ones as keyphrases. Experimental results show that TPR outperforms state-of-the-art keyphrase extraction methods on two datasets under various evaluation metrics. 
In this paper, we investigate how modeling content structure can beneﬁt text analysis applications such as extractive summarization and sentiment analysis. This follows the linguistic intuition that rich contextual information should be useful in these tasks. We present a framework which combines a supervised text analysis application with the induction of latent content structure. Both of these elements are learned jointly using the EM algorithm. The induced content structure is learned from a large unannotated corpus and biased by the underlying text analysis task. We demonstrate that exploiting content structure yields signiﬁcant improvements over approaches that rely only on local context.1 
This work concerns automatic topic segmentation of email conversations. We present a corpus of email threads manually annotated with topics, and evaluate annotator reliability. To our knowledge, this is the ﬁrst such email corpus. We show how the existing topic segmentation models (i.e., Lexical Chain Segmenter (LCSeg) and Latent Dirichlet Allocation (LDA)) which are solely based on lexical information, can be applied to emails. By pointing out where these methods fail and what any desired model should consider, we propose two novel extensions of the models that not only use lexical information but also exploit ﬁner level conversation structure in a principled way. Empirical evaluation shows that LCSeg is a better model than LDA for segmenting an email thread into topical clusters and incorporating conversation structure into these models improves the performance signiﬁcantly. 
Several recent discourse parsers have employed fully-supervised machine learning approaches. These methods require human annotators to beforehand create an extensive training corpus, which is a time-consuming and costly process. On the other hand, unlabeled data is abundant and cheap to collect. In this paper, we propose a novel semi-supervised method for discourse relation classiﬁcation based on the analysis of cooccurring features in unlabeled data, which is then taken into account for extending the feature vectors given to a classiﬁer. Our experimental results on the RST Discourse Treebank corpus and Penn Discourse Treebank indicate that the proposed method brings a signiﬁcant improvement in classiﬁcation accuracy and macro-average F-score when small training datasets are used. For instance, with training sets of c.a. 1000 labeled instances, the proposed method brings improvements in accuracy and macro-average F-score up to 50% compared to a baseline classiﬁer. We believe that the proposed method is a ﬁrst step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. 
 Language is sensitive to both semantic and pragmatic effects. To capture both effects, we model language use as a cooperative game between two players: a speaker, who generates an utterance, and a listener, who responds with an action. Speciﬁcally, we consider the task of generating spatial references to objects, wherein the listener must accurately identify an object described by the speaker. We show that a speaker model that acts optimally with respect to an explicit, embedded listener model substantially outperforms one that is trained to directly generate spatial descriptions. 
For resource-limited language pairs, coverage of the test set by the parallel corpus is an important factor that affects translation quality in two respects: 1) out of vocabulary words; 2) the same information in an input sentence can be expressed in different ways, while current phrase-based SMT systems cannot automatically select an alternative way to transfer the same information. Therefore, given limited data, in order to facilitate translation from the input side, this paper proposes a novel method to reduce the translation difﬁculty using source-side lattice-based paraphrases. We utilise the original phrases from the input sentence and the corresponding paraphrases to build a lattice with estimated weights for each edge to improve translation quality. Compared to the baseline system, our method achieves relative improvements of 7.07%, 6.78% and 3.63% in terms of BLEU score on small, medium and largescale English-to-Chinese translation tasks respectively. The results show that the proposed method is effective not only for resourcelimited language pairs, but also for resourcesufﬁcient pairs to some extent. 
This paper studies the problem of mining entity translation, speciﬁcally, mining English and Chinese name pairs. Existing efforts can be categorized into (a) a transliterationbased approach leveraging phonetic similarity and (b) a corpus-based approach exploiting bilingual co-occurrences, each of which suffers from inaccuracy and scarcity respectively. In clear contrast, we use unleveraged resources of monolingual entity co-occurrences, crawled from entity search engines, represented as two entity-relationship graphs extracted from two language corpora respectively. Our problem is then abstracted as ﬁnding correct mappings across two graphs. To achieve this goal, we propose a holistic approach, of exploiting both transliteration similarity and monolingual co-occurrences. This approach, building upon monolingual corpora, complements existing corpus-based work, requiring scarce resources of parallel or comparable corpus, while signiﬁcantly boosting the accuracy of transliteration-based work. We validate our proposed system using real-life datasets. 
This paper studies two issues, non-isomorphic structure translation and target syntactic structure usage, for statistical machine translation in the context of forest-based tree to tree sequence translation. For the first issue, we propose a novel non-isomorphic translation framework to capture more non-isomorphic structure mappings than traditional tree-based and tree-sequence-based translation methods. For the second issue, we propose a parallel space searching method to generate hypothesis using tree-to-string model and evaluate its syntactic goodness using tree-to-tree/tree sequence model. This not only reduces the search complexity by merging spurious-ambiguity translation paths and solves the data sparseness issue in training, but also serves as a syntax-based target language model for better grammatical generation. Experiment results on the benchmark data show our proposed two solutions are very effective, achieving significant performance improvement over baselines when applying to different translation models. 
We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a ﬁner granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model framework, and ﬁnd that it yields consistent improvements over a wide range of baselines. 
There is considerable interest in interdisciplinary combinations of automatic speech recognition (ASR), machine learning, natural language processing, text classiﬁcation and information retrieval. Many of these boxes, especially ASR, are often based on considerable linguistic resources. We would like to be able to process spoken documents with few (if any) resources. Moreover, connecting black boxes in series tends to multiply errors, especially when the key terms are out-ofvocabulary (OOV). The proposed alternative applies text processing directly to the speech without a dependency on ASR. The method ﬁnds long (∼ 1 sec) repetitions in speech, and clusters them into pseudo-terms (roughly phrases). Document clustering and classiﬁcation work surprisingly well on pseudoterms; performance on a Switchboard task approaches a baseline using gold standard manual transcriptions. 
In situated dialogue humans often utter linguistic expressions that refer to extralinguistic entities in the environment. Correctly resolving these references is critical yet challenging for artiﬁcial agents partly due to their limited speech recognition and language understanding capabilities. Motivated by psycholinguistic studies demonstrating a tight link between language production and human eye gaze, we have developed approaches that integrate naturally occurring human eye gaze with speech recognition hypotheses to resolve exophoric references in situated dialogue in a virtual world. In addition to incorporating eye gaze with the best recognized spoken hypothesis, we developed an algorithm to also handle multiple hypotheses modeled as word confusion networks. Our empirical results demonstrate that incorporating eye gaze with recognition hypotheses consistently outperforms the results obtained from processing recognition hypotheses alone. Incorporating eye gaze with word confusion networks further improves performance. 
In this paper we address two key challenges for extractive multi-document summarization: the search problem of ﬁnding the best scoring summary and the training problem of learning the best model parameters. We propose an A* search algorithm to ﬁnd the best extractive summary up to a given length, which is both optimal and efﬁcient to run. Further, we propose a discriminative training algorithm which directly maximises the quality of the best summary, rather than assuming a sentence-level decomposition as in earlier work. Our approach leads to signiﬁcantly better results than earlier techniques across a number of evaluation metrics. 
Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier’s entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sievebased approaches could be applied to other NLP tasks. 
We present a simple, robust generation system which performs content selection and surface realization in a uniﬁed, domain-independent framework. In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-speciﬁc systems both in terms of BLEU scores and human evaluation. 
The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions. Based on an integer linear programming formulation, the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met. Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modiﬁcations. 
We employ statistical methods to analyze, generate, and translate rhythmic poetry. We ﬁrst apply unsupervised learning to reveal word-stress patterns in a corpus of raw poetry. We then use these word-stress patterns, in addition to rhyme and discourse models, to generate English love poetry. Finally, we translate Italian poetry into English, choosing target realizations that conform to desired rhythmic patterns. 
We address the modeling, parameter estimation and search challenges that arise from the introduction of reordering models that capture non-local reordering in alignment modeling. In particular, we introduce several reordering models that utilize (pairs of) function words as contexts for alignment reordering. To address the parameter estimation challenge, we propose to estimate these reordering models from a relatively small amount of manuallyaligned corpora. To address the search challenge, we devise an iterative local search algorithm that stochastically explores reordering possibilities. By capturing non-local reordering phenomena, our proposed alignment model bears a closer resemblance to stateof-the-art translation model. Empirical results show signiﬁcant improvements in alignment quality as well as in translation performance over baselines in a large-scale ChineseEnglish translation task. 
We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignment model. We deﬁne translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we ﬁnd that rule extraction from posteriors gives translation improvements. We also ﬁnd that grammars with rules with only one nonterminal, when extracted from posteriors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices. 
Hierarchical phrase-based (HPB) translation provides a powerful mechanism to capture both short and long distance phrase reorderings. However, the phrase reorderings lack of contextual information in conventional HPB systems. This paper proposes a contextdependent phrase reordering approach that uses the maximum entropy (MaxEnt) model to help the HPB decoder select appropriate reordering patterns. We classify translation rules into several reordering patterns, and build a MaxEnt model for each pattern based on various contextual features. We integrate the MaxEnt models into the HPB model. Experimental results show that our approach achieves signiﬁcant improvements over a standard HPB system on large-scale translation tasks. On Chinese-to-English translation, the absolute improvements in BLEU (caseinsensitive) range from 1.2 to 2.1. 
We present the ﬁrst evaluation of the utility of automatic evaluation metrics on surface realizations of Penn Treebank data. Using outputs of the OpenCCG and XLE realizers, along with ranked WordNet synonym substitutions, we collected a corpus of generated surface realizations. These outputs were then rated and post-edited by human annotators. We evaluated the realizations using seven automatic metrics, and analyzed correlations obtained between the human judgments and the automatic scores. In contrast to previous NLG meta-evaluations, we ﬁnd that several of the metrics correlate moderately well with human judgments of both adequacy and ﬂuency, with the TER family performing best overall. We also ﬁnd that all of the metrics correctly predict more than half of the signiﬁcant systemlevel differences, though none are correct in all cases. We conclude with a discussion of the implications for the utility of such metrics in evaluating generation in the presence of variation. A further result of our research is a corpus of post-edited realizations, which will be made available to the research community. 
Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Many different methods have been proposed, yet comparisons are difﬁcult to make since there is little consensus on evaluation framework, and many papers evaluate against only one or two competitor systems. Here we evaluate seven different POS induction systems spanning nearly 20 years of work, using a variety of measures. We show that some of the oldest (and simplest) systems stand up surprisingly well against more recent approaches. Since most of these systems were developed and tested using data from the WSJ corpus, we compare their generalization abilities by testing on both WSJ and the multilingual Multext-East corpus. Finally, we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype-driven learner. In most cases, the prototype-driven learner outperforms the unsupervised system used to initialize it, yielding state-of-the-art results on WSJ and improvements on nonEnglish corpora. 
Domain adaptation, the problem of adapting a natural language processing system trained in one domain to perform well in a different domain, has received signiﬁcant attention. This paper addresses an important problem for deployed systems that has received little attention – detecting when such adaptation is needed by a system operating in the wild, i.e., performing classiﬁcation over a stream of unlabeled examples. Our method uses Adistance, a metric for detecting shifts in data streams, combined with classiﬁcation margins to detect domain shifts. We empirically show effective domain shift detection on a variety of data sets and shift conditions. 
A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. 
Minimum Error Rate Training is the algorithm for log-linear model parameter training most used in state-of-the-art Statistical Machine Translation systems. In its original formulation, the algorithm uses N-best lists output by the decoder to grow the Translation Pool that shapes the surface on which the actual optimization is performed. Recent work has been done to extend the algorithm to use the entire translation lattice built by the decoder, instead of N-best lists. We propose here a third, intermediate way, consisting in growing the translation pool using samples randomly drawn from the translation lattice. We empirically measure a systematic improvement in the BLEU scores compared to training using N-best lists, without suffering the increase in computational complexity associated with operating with the whole lattice. 
In modern machine translation practice, a statistical phrasal or hierarchical translation system usually relies on a huge set of translation rules extracted from bi-lingual training data. This approach not only results in space and efﬁciency issues, but also suffers from the sparse data problem. In this paper, we propose to use factorized grammars, an idea widely accepted in the ﬁeld of linguistic grammar construction, to generalize translation rules, so as to solve these two problems. We designed a method to take advantage of the XTAG English Grammar to facilitate the extraction of factorized rules. We experimented on various setups of low-resource language translation, and showed consistent signiﬁcant improvement in BLEU over state-ofthe-art string-to-dependency baseline systems with 200K words of bi-lingual training data. 
Production of parallel training corpora for the development of statistical machine translation (SMT) systems for resource-poor languages usually requires extensive manual effort. Active sample selection aims to reduce the labor, time, and expense incurred in producing such resources, attaining a given performance benchmark with the smallest possible training corpus by choosing informative, nonredundant source sentences from an available candidate pool for manual translation. We present a novel, discriminative sample selection strategy that preferentially selects batches of candidate sentences with constructs that lead to erroneous translations on a held-out development set. The proposed strategy supports a built-in diversity mechanism that reduces redundancy in the selected batches. Simulation experiments on English-to-Pashto and Spanish-to-English translation tasks demonstrate the superiority of the proposed approach to a number of competing techniques, such as random selection, dissimilarity-based selection, as well as a recently proposed semisupervised active learning strategy. 
We examine effects that empty categories have on machine translation. Empty categories are elements in parse trees that lack corresponding overt surface forms (words) such as dropped pronouns and markers for control constructions. We start by training machine translation systems with manually inserted empty elements. We ﬁnd that inclusion of some empty categories in training data improves the translation result. We expand the experiment by automatically inserting these elements into a larger data set using various methods and training on the modiﬁed corpus. We show that even when automatic prediction of null elements is not highly accurate, it nevertheless improves the end translation result. 
Conventional wisdom dictates that synchronous context-free grammars (SCFGs) must be converted to Chomsky Normal Form (CNF) to ensure cubic time decoding. For arbitrary SCFGs, this is typically accomplished via the synchronous binarization technique of (Zhang et al., 2006). A drawback to this approach is that it inﬂates the constant factors associated with decoding, and thus the practical running time. (DeNero et al., 2009) tackle this problem by deﬁning a superset of CNF called Lexical Normal Form (LNF), which also supports cubic time decoding under certain implicit assumptions. In this paper, we make these assumptions explicit, and in doing so, show that LNF can be further expanded to a broader class of grammars (called “scope3”) that also supports cubic-time decoding. By simply pruning non-scope-3 rules from a GHKM-extracted grammar, we obtain better translation performance than synchronous binarization.  
In this article, an original view on how to improve phrase translation estimates is proposed. This proposal is grounded on two main ideas: ﬁrst, that appropriate examples of a given phrase should participate more in building its translation distribution; second, that paraphrases can be used to better estimate this distribution. Initial experiments provide evidence of the potential of our approach and its implementation for effectively improving translation performance. 
Word alignment plays a central role in statistical MT (SMT) since almost all SMT systems extract translation rules from word aligned parallel training data. While most SMT systems use unsupervised algorithms (e.g. GIZA++) for training word alignment, supervised methods, which exploit a small amount of human-aligned data, have become increasingly popular recently. This work empirically studies the performance of these two classes of alignment algorithms and explores strategies to combine them to improve overall system performance. We used two unsupervised aligners, GIZA++ and HMM, and one supervised aligner, ITG, in this study. To avoid language and genre speciﬁc conclusions, we ran experiments on test sets consisting of two language pairs (Chinese-to-English and Arabicto-English) and two genres (newswire and weblog). Results show that the two classes of algorithms achieve the same level of MT performance. Modest improvements were achieved by taking the union of the translation grammars extracted from different alignments. Signiﬁcant improvements (around 1.0 in BLEU) were achieved by combining outputs of different systems trained with different alignments. The improvements are consistent across languages and genres. 
We present a new syntactic parser that works left-to-right and top down, thus maintaining a fully-connected parse tree for a few alternative parse hypotheses. All of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence. Thus they only ﬁnd a complete fully connected parse at the very end. In contrast, both subjective and experimental evidence show that people understand a sentence word-to-word as they go along, or close to it. The constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact. Our parser achieves a new best result for topdown parsers of 89.4%,a 20% error reduction over the previous single-parser best result for parsers of this type of 86.8% (Roark, 2001). The improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming. 
We introduce a novel training algorithm for unsupervised grammar induction, called Zoomed Learning. Given a training set T and a test set S, the goal of our algorithm is to identify subset pairs Ti, Si of T and S such that when the unsupervised parser is trained on a training subset Ti its results on its paired test subset Si are better than when it is trained on the entire training set T . A successful application of zoomed learning improves overall performance on the full test set S. We study our algorithm’s effect on the leading algorithm for the task of fully unsupervised parsing (Seginer, 2007) in three different English domains, WSJ, BROWN and GENIA, and show that it improves the parser F-score by up to 4.47%. 
Parser disambiguation with precision grammars generally takes place via statistical ranking of the parse yield of the grammar using a supervised parse selection model. In the standard process, the parse selection model is trained over a hand-disambiguated treebank, meaning that without a signiﬁcant investment of effort to produce the treebank, parse selection is not possible. Furthermore, as treebanking is generally streamlined with parse selection models, creating the initial treebank without a model requires more resources than subsequent treebanks. In this work, we show that, by taking advantage of the constrained nature of these HPSG grammars, we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources. 
It is well known that parsing accuracies drop signiﬁcantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difﬁculty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance. 
This paper approaches the scope learning problem via simplified shallow semantic parsing. This is done by regarding the cue as the predicate and mapping its scope into several constituents as the arguments of the cue. Evaluation on the BioScope corpus shows that the structural information plays a critical role in capturing the relationship between a cue and its dominated arguments. It also shows that our parsing approach significantly outperforms the state-of-the-art chunking ones. Although our parsing approach is only evaluated on negation and speculation scope learning here, it is portable to other kinds of scope learning. 
We describe a model for the lexical analysis of Arabic text, using the lists of alternatives supplied by a broad-coverage morphological analyzer, SAMA, which include stable lemma IDs that correspond to combinations of broad word sense categories and POS tags. We break down each of the hundreds of thousands of possible lexical labels into its constituent elements, including lemma ID and part-of-speech. Features are computed for each lexical token based on its local and document-level context and used in a novel, simple, and highly efﬁcient two-stage supervised machine learning algorithm that overcomes the extreme sparsity of label distribution in the training data. The resulting system achieves accuracy of 90.6% for its ﬁrst choice, and 96.2% for its top two choices, in selecting among the alternatives provided by the SAMA lexical analyzer. We have successfully used this system in applications such as an online reading helper for intermediate learners of the Arabic language, and a tool for improving the productivity of Arabic Treebank annotators. 
In many NLP systems, there is a unidirectional ﬂow of information in which a parser supplies input to a semantic role labeler. In this paper, we build a system that allows information to ﬂow in both directions. We make use of semantic role predictions in choosing a single-best parse. This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones. Our system penalizes parses that give rise to low-scoring semantic roles. To explore the consequences of this we perform two experiments. First, we use a baseline generative model to produce n-best parses, which are then re-ordered by our semantic model. Second, we use a modiﬁed version of our semantic role labeler to predict semantic roles at parse time. The performance of this modiﬁed labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser’s packed chart. For both experiments, the resulting semantic predictions are then used to select parses. Finally, we feed the selected parses produced by each experiment to the full version of our semantic role labeler. We ﬁnd that SRL performance can be improved over this baseline by selecting parses with likely semantic roles. 
Graph-based methods have gained attention in many areas of Natural Language Processing (NLP) including Word Sense Disambiguation (WSD), text summarization, keyword extraction and others. Most of the work in these areas formulate their problem in a graph-based setting and apply unsupervised graph clustering to obtain a set of clusters. Recent studies suggest that graphs often exhibit a hierarchical structure that goes beyond simple ﬂat clustering. This paper presents an unsupervised method for inferring the hierarchical grouping of the senses of a polysemous word. The inferred hierarchical structures are applied to the problem of word sense disambiguation, where we show that our method performs signiﬁcantly better than traditional graph-based methods and agglomerative clustering yielding improvements over state-of-the-art WSD systems based on sense induction. 
While a signiﬁcant amount of research has been devoted to textual entailment, automated entailment from conversational scripts has received less attention. To address this limitation, this paper investigates the problem of conversation entailment: automated inference of hypotheses from conversation scripts. We examine two levels of semantic representations: a basic representation based on syntactic parsing from conversation utterances and an augmented representation taking into consideration of conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical ﬁndings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations. 
Problems stemming from domain adaptation continue to plague the statistical natural language processing community. There has been continuing work trying to ﬁnd general purpose algorithms to alleviate this problem. In this paper we argue that existing general purpose approaches usually only focus on one of two issues related to the difﬁculties faced by adaptation: 1) difference in base feature statistics or 2) task differences that can be detected with labeled data. We argue that it is necessary to combine these two classes of adaptation algorithms, using evidence collected through theoretical analysis and simulated and real-world data experiments. We ﬁnd that the combined approach often outperforms the individual adaptation approaches. By combining simple approaches from each class of adaptation algorithm, we achieve state-of-the-art results for both Named Entity Recognition adaptation task and the Preposition Sense Disambiguation adaptation task. Second, we also show that applying an adaptation algorithm that ﬁnds shared representation between domains often impacts the choice in adaptation algorithm that makes use of target labeled data. 
Using multi-layer neural networks to estimate the probabilities of word sequences is a promising research area in statistical language modeling, with applications in speech recognition and statistical machine translation. However, training such models for large vocabulary tasks is computationally challenging which does not scale easily to the huge corpora that are nowadays available. In this work, we study the performance and behavior of two neural statistical language models so as to highlight some important caveats of the classical training algorithms. The induced word embeddings for extreme cases are also analysed, thus providing insight into the convergence issues. A new initialization scheme and new training techniques are then introduced. These methods are shown to greatly reduce the training time and to signiﬁcantly improve performance, both in terms of perplexity and on a large-scale translation task. 
Almost all Chinese language processing tasks involve word segmentation of the language input as their first steps, thus robust and reliable segmentation techniques are always required to make sure those tasks wellperformed. In recent years, machine learning and sequence labeling models such as Conditional Random Fields (CRFs) are often used in segmenting Chinese texts. Compared with traditional lexicon-driven models, machine learned models achieve higher F-measure scores. But machine learned models heavily depend on training materials. Although they can effectively process texts from the same domain as the training texts, they perform relatively poorly when texts from new domains are to be processed. In this paper, we propose to use χ2 statistics when training an SVM-HMM based segmentation model to improve its ability to recall OOV words and then use bootstrapping strategies to maintain its ability to recall IV words. Experiments show the approach proposed in this paper enhances the domain portability of the Chinese word segmentation model and prevents drastic decline in performance when processing texts across domains. 
We present a novel approach to distributionalonly, fully unsupervised, POS tagging, based on an adaptation of the EM algorithm for the estimation of a Gaussian mixture. In this approach, which we call Latent-Descriptor Clustering (LDC), word types are clustered using a series of progressively more informative descriptor vectors. These descriptors, which are computed from the immediate left and right context of each word in the corpus, are updated based on the previous state of the cluster assignments. The LDC algorithm is simple and intuitive. Using standard evaluation criteria for unsupervised POS tagging, LDC shows a substantial improvement in performance over state-of-the-art methods, along with a several-fold reduction in computational cost. 
We define a probabilistic morphological analyzer using a data-driven approach for Syriac in order to facilitate the creation of an annotated corpus. Syriac is an under-resourced Semitic language for which there are no available language tools such as morphological analyzers. We introduce novel probabilistic models for segmentation, dictionary linkage, and morphological tagging and connect them in a pipeline to create a probabilistic morphological analyzer requiring only labeled data. We explore the performance of models with varying amounts of training data and find that with about 34,500 labeled tokens, we can outperform a reasonable baseline trained on over 99,000 tokens and achieve an accuracy of just over 80%. When trained on all available training data, our joint model achieves 86.47% accuracy, a 29.7% reduction in error rate over the baseline. 
This paper examines tagging models for spontaneous English speech transcripts. We analyze the performance of state-of-the-art tagging models, either generative or discriminative, left-to-right or bidirectional, with or without latent annotations, together with the use of ToBI break indexes and several methods for segmenting the speech transcripts (i.e., conversation side, speaker turn, or humanannotated sentence). Based on these studies, we observe that: (1) bidirectional models tend to achieve better accuracy levels than left-toright models, (2) generative models seem to perform somewhat better than discriminative models on this task, and (3) prosody improves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed signiﬁcantly improve performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal. 
This paper proposes a fast and simple unsupervised word segmentation algorithm that utilizes the local predictability of adjacent character sequences, while searching for a leasteffort representation of the data. The model uses branching entropy as a means of constraining the hypothesis space, in order to efﬁciently obtain a solution that minimizes the length of a two-part MDL code. An evaluation with corpora in Japanese, Thai, English, and the ”CHILDES” corpus for research in language development reveals that the algorithm achieves an accuracy, comparable to that of the state-of-the-art methods in unsupervised word segmentation, in a signiﬁcantly reduced computational time. 
We show that the standard beam-search algorithm can be used as an efﬁcient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a signiﬁcant speed improvement. Such decoding is enabled by: (1) separating full word features from partial word features so that feature templates can be instantiated incrementally, according to whether the current character is separated or appended; (2) deciding the POS-tag of a potential word when its ﬁrst character is processed. Early-update is used with perceptron training so that the linear model gives a high score to a correct partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 
Part-of-speech (POS) tag distributions are known to exhibit sparsity — a word is likely to take a single predominant tag in a corpus. Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy. However, in existing systems, this expansion come with a steep increase in model complexity. This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments. In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training. Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex state-of-the art systems.1 
We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate the effectiveness of various features and machine learners for this task. While a simple bag-of-words approach provides a solid baseline, we ﬁnd that adding information from dialogue structure and inter-utterance dependency provides some increase in performance; learners that account for sequential dependencies (CRFs) show the best performance. We report our results from testing using a corpus of chat dialogues derived from online shopping customer-feedback data. 
This paper proposes a unified framework for zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, anaphoricity determination and antecedent identification. In particular, all the three sub-tasks are addressed using tree kernel-based methods with appropriate syntactic parse tree structures. Experimental results on a Chinese zero anaphora corpus show that the proposed tree kernel-based methods significantly outperform the feature-based ones. This indicates the critical role of the structural information in zero anaphora resolution and the necessity of tree kernel-based methods in modeling such structural information. To our best knowledge, this is the first systematic work dealing with all the three sub-tasks in Chinese zero anaphora resolution via a unified framework. Moreover, we release a Chinese zero anaphora corpus of 100 documents, which adds a layer of annotation to the manually-parsed sentences in the Chinese Treebank (CTB) 6.0. 
This paper proposes a method for automatically inserting commas into Japanese texts. In Japanese sentences, commas play an important role in explicitly separating the constituents, such as words and phrases, of a sentence. The method can be used as an elemental technology for natural language generation such as speech recognition and machine translation, or in writing-support tools for non-native speakers. We categorized the usages of commas and investigated the appearance tendency of each category. In this method, the positions where commas should be inserted are decided based on a machine learning approach. We conducted a comma insertion experiment using a text corpus and conﬁrmed the effectiveness of our method. 
Unknown words are a hindrance to the performance of hand-crafted computational grammars of natural language. However, words with incomplete and incorrect lexical entries pose an even bigger problem because they can be the cause of a parsing failure despite being listed in the lexicon of the grammar. Such lexical entries are hard to detect and even harder to correct. We employ an error miner to pinpoint words with problematic lexical entries. An automated lexical acquisition technique is then used to learn new entries for those words which allows the grammar to parse previously uncovered sentences successfully. We test our method on a large-scale grammar of Dutch and a set of sentences for which this grammar fails to produce a parse. The application of the method enables the grammar to cover 83.76% of those sentences with an accuracy of 86.15%. 
The reliable extraction of knowledge from text requires an appropriate treatment of the time at which reported events take place. Unfortunately, there are very few annotated data sets that support the development of techniques for event time-stamping and tracking the progression of time through a narrative. In this paper, we present a new corpus of temporally-rich documents sourced from English Wikipedia, which we have annotated with TIMEX2 tags. The corpus contains around 120000 tokens, and 2600 TIMEX2 expressions, thus comparing favourably in size to other existing corpora used in these areas. We describe the preparation of the corpus, and compare the proﬁle of the data with other existing temporally annotated corpora. We also report the results obtained when we use DANTE, our temporal expression tagger, to process this corpus, and point to where further work is required. The corpus is publicly available for research purposes. 
We present PEM, the ﬁrst fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, ﬂuency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 
Extant Statistical Machine Translation (SMT) systems are very complex softwares, which embed multiple layers of heuristics and embark very large numbers of numerical parameters. As a result, it is difﬁcult to analyze output translations and there is a real need for tools that could help developers to better understand the various causes of errors. In this study, we make a step in that direction and present an attempt to evaluate the quality of the phrase-based translation model. In order to identify those translation errors that stem from deﬁciencies in the phrase table (PT), we propose to compute the oracle BLEU-4 score, that is the best score that a system based on this PT can achieve on a reference corpus. By casting the computation of the oracle BLEU-1 as an Integer Linear Programming (ILP) problem, we show that it is possible to efﬁciently compute accurate lower-bounds of this score, and report measures performed on several standard benchmarks. Various other applications of these oracle decoding techniques are also reported and discussed. 
Automatic evaluation of Machine Translation (MT) quality is essential to developing highquality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate ‘A because B’ as ‘B because A.’ Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not signiﬁcantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefﬁcients modiﬁed with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 
Electronic dictionaries covering all natural language levels are very relevant for the human use as well as for the automatic processing use, namely those constructed with respect to international standards. Such dictionaries are characterized by a complex structure and an important access time when using a querying system. However, the need of a user is generally limited to a part of such a dictionary according to his domain and expertise level which corresponds to a specialized dictionary. Given the importance of managing a unified dictionary and considering the personalized needs of users, we propose an approach for generating personalized views starting from a normalized dictionary with respect to Lexical Markup Framework LMF-ISO 24613 norm. This approach provides the re-use of already defined views for a community of users by managing their profiles information and promoting the materialization of the generated views. It is composed of four main steps: (i) the projection of data categories controlled by a set of constraints (related to the user‟s profiles), (ii) the selection of values with consistency checking, (iii) the automatic generation of the query‟s model and finally, (iv) the refinement of the view. The proposed approach was consolidated by carrying out an experiment on an LMF normalized Arabic dictionary. 
In this paper, we consider the problem of generating candidate corrections for the task of correcting errors in text. We focus on the task of correcting errors in preposition usage made by non-native English speakers, using discriminative classiﬁers. The standard approach to the problem assumes that the set of candidate corrections for a preposition consists of all preposition choices participating in the task. We determine likely preposition confusions using an annotated corpus of nonnative text and use this knowledge to produce smaller sets of candidates. We propose several methods of restricting candidate sets. These methods exclude candidate prepositions that are not observed as valid corrections in the annotated corpus and take into account the likelihood of each preposition confusion in the non-native text. We ﬁnd that restricting candidates to those that are observed in the non-native data improves both the precision and the recall compared to the approach that views all prepositions as possible candidates. Furthermore, the approach that takes into account the likelihood of each preposition confusion is shown to be the most effective. 
Conﬁdence-Weighted linear classiﬁers (CW) and its successors were shown to perform well on binary and multiclass NLP problems. In this paper we extend the CW approach for sequence learning and show that it achieves state-of-the-art performance on four noun phrase chucking and named entity recognition tasks. We then derive few algorithmic approaches to estimate the prediction’s correctness of each label in the output sequence. We show that our approach provides a reliable relative correctness information as it outperforms other alternatives in ranking label-predictions according to their error. We also show empirically that our methods output close to absolute estimation of error. Finally, we show how to use this information to improve active learning. 
The research question treated in this paper is centered on the idea of exploiting rich resources of one language to enhance the performance of a mention detection system of another one. We successfully achieve this goal by projecting information from one language to another via a parallel corpus. We examine the potential improvement using various degrees of linguistic information in a statistical framework and we show that the proposed technique is effective even when the target language model has access to a signiﬁcantly rich feature set. Experimental results show up to 2.4F improvement in performance when the system has access to information obtained by projecting mentions from a resource-richlanguage mention detection system via a parallel corpus. 
Named-entity recognition (NER) is an important task required in a wide variety of applications. While rule-based systems are appealing due to their well-known “explainability,” most, if not all, state-of-the-art results for NER tasks are based on machine learning techniques. Motivated by these results, we explore the following natural question in this paper: Are rule-based systems still a viable approach to named-entity recognition? Specifically, we have designed and implemented a high-level language NERL on top of SystemT, a general-purpose algebraic information extraction system. NERL is tuned to the needs of NER tasks and simpliﬁes the process of building, understanding, and customizing complex rule-based named-entity annotators. We show that these customized annotators match or outperform the best published results achieved with machine learning techniques. These results conﬁrm that we can reap the beneﬁts of rule-based extractors’ explainability without sacriﬁcing accuracy. We conclude by discussing lessons learned while building and customizing complex rule-based annotators and outlining several research directions towards facilitating rule development. 
We present a novel approach to relation extraction that integrates information across documents, performs global inference and requires no labelled text. In particular, we tackle relation extraction and entity identiﬁcation jointly. We use distant supervision to train a factor graph model for relation extraction based on an existing knowledge base (Freebase, derived in parts from Wikipedia). For inference we run an efﬁcient Gibbs sampler that leads to linear time joint inference. We evaluate our approach both for an indomain (Wikipedia) and a more realistic outof-domain (New York Times Corpus) setting. For the in-domain setting, our joint model leads to 4% higher precision than an isolated local approach, but has no advantage over a pipeline. For the out-of-domain data, we beneﬁt strongly from joint modelling, and observe improvements in precision of 13% over the pipeline, and 15% over the isolated baseline. 
In this paper we introduce the new task of social event extraction from text. We distinguish two broad types of social events depending on whether only one or both parties are aware of the social contact. We annotate part of Automatic Content Extraction (ACE) data, and perform experiments using Support Vector Machines with Kernel methods. We use a combination of structures derived from phrase structure trees and dependency trees. A characteristic of our events (which distinguishes them from ACE events) is that the participating entities can be spread far across the parse trees. We use syntactic and semantic insights to devise a new structure derived from dependency trees and show that this plays a role in achieving the best performing system for both social event detection and classiﬁcation tasks. We also use three data sampling approaches to solve the problem of data skewness. Sampling methods improve the F1-measure for the task of relation detection by over 20% absolute over the baseline. 
In this paper, we focus on the opinion target extraction as part of the opinion mining task. We model the problem as an information extraction task, which we address based on Conditional Random Fields (CRF). As a baseline we employ the supervised algorithm by Zhuang et al. (2006), which represents the state-of-the-art on the employed data. We evaluate the algorithms comprehensively on datasets from four different domains annotated with individual opinion target instances on a sentence level. Furthermore, we investigate the performance of our CRF-based approach and the baseline in a single- and cross-domain opinion target extraction setting. Our CRF-based approach improves the performance by 0.077, 0.126, 0.071 and 0.178 regarding F-Measure in the single-domain extraction in the four domains. In the crossdomain setting our approach improves the performance by 0.409, 0.242, 0.294 and 0.343 regarding F-Measure over the baseline.  are: a) Opinion question answering - i.e. with questions regarding an entity as in “What do the people like / dislike about X?”. b) Recommender systems - i.e. if the system shall only recommend entities which have received good reviews regarding a certain aspect. c) Opinion summarization - i.e. if one wants to create an overview of all positive / negative opinions regarding aspect Y of entity X and cluster them accordingly. All of these tasks have in common that in order to fulﬁll them, the opinion mining system must be capable of identifying what the opinions in the individual sentences are about, hence extract the opinion targets. Our goal in this work is to extract opinion targets from user-generated discourse, a discourse type which is quite frequently encountered today, due to the explosive growth of Web 2.0 community websites. Typical sentences which we encounter in this discourse type are shown in the following examples. The opinion targets which we aim to extract are underlined in the sentences, the corresponding opinion expressions are shown in italics.  
In this paper, we investigate structured models for document-level sentiment classiﬁcation. When predicting the sentiment of a subjective document (e.g., as positive or negative), it is well known that not all sentences are equally discriminative or informative. But identifying the useful sentences automatically is itself a difﬁcult learning problem. This paper proposes a joint two-level approach for document-level sentiment classiﬁcation that simultaneously extracts useful (i.e., subjective) sentences and predicts document-level sentiment based on the extracted sentences. Unlike previous joint learning methods for the task, our approach (1) does not rely on gold standard sentence-level subjectivity annotations (which may be expensive to obtain), and (2) optimizes directly for document-level performance. Empirical evaluations on movie reviews and U.S. Congressional ﬂoor debates show improved performance over previous approaches. 
In this paper, we introduce a method that automatically builds text classiﬁers in a new language by training on already labeled data in another language. Our method transfers the classiﬁcation knowledge across languages by translating the model features and by using an Expectation Maximization (EM) algorithm that naturally takes into account the ambiguity associated with the translation of a word. We further exploit the readily available unlabeled data in the target language via semisupervised learning, and adapt the translated model to better ﬁt the data distribution of the target language. 
In this paper we develop an approach to tackle the problem of verb selection for learners of English as a second language (ESL) by using features from the output of Semantic Role Labeling (SRL). Unlike existing approaches to verb selection that use local features such as n-grams, our approach exploits semantic features which explicitly model the usage context of the verb. The verb choice highly depends on its usage context which is not consistently captured by local features. We then combine these semantic features with other local features under the generalized perceptron learning framework. Experiments on both indomain and out-of-domain corpora show that our approach outperforms the baseline and achieves state-of-the-art performance.1 
In this paper, we conducted a systematic comparative analysis of language in different contexts of bursty topics, including web search, news media, blogging, and social bookmarking. We analyze (1) the content similarity and predictability between contexts, (2) the coverage of search content by each context, and (3) the intrinsic coherence of information in each context. Our experiments show that social bookmarking is a better predictor to the bursty search queries, but news media and social blogging media have a much more compelling coverage. This comparison provides insights on how the search behaviors and social information sharing behaviors of users are correlated to the professional news media in the context of bursty events. 
Even the entire Web corpus does not explicitly answer all questions, yet inference can uncover many implicit answers. But where do inference rules come from? This paper investigates the problem of learning inference rules from Web text in an unsupervised, domain-independent manner. The SHERLOCK system, described herein, is a ﬁrst-order learner that acquires over 30,000 Horn clauses from Web text. SHERLOCK embodies several innovations, including a novel rule scoring function based on Statistical Relevance (Salmon et al., 1971) which is effective on ambiguous, noisy and incomplete Web extractions. Our experiments show that inference over the learned rules discovers three times as many facts (at precision 0.8) as the TEXTRUNNER system which merely extracts facts explicitly stated in Web text. 
Determining whether two terms in text have an ancestor relation (e.g. Toyota and car) or a sibling relation (e.g. Toyota and Honda) is an essential component of textual inference in NLP applications such as Question Answering, Summarization, and Recognizing Textual Entailment. Signiﬁcant work has been done on developing stationary knowledge sources that could potentially support these tasks, but these resources often suffer from low coverage, noise, and are inﬂexible when needed to support terms that are not identical to those placed in them, making their use as general purpose background knowledge resources difﬁcult. In this paper, rather than building a stationary hierarchical structure of terms and relations, we describe a system that, given two terms, determines the taxonomic relation between them using a machine learning-based approach that makes use of existing resources. Moreover, we develop a global constraint optimization inference process and use it to leverage an existing knowledge base also to enforce relational constraints among terms and thus improve the classiﬁer predictions. Our experimental evaluation shows that our approach signiﬁcantly outperforms other systems built upon existing well-known knowledge sources. 
Although many algorithms have been developed to harvest lexical resources, few organize the mined terms into taxonomies. We propose (1) a semi-supervised algorithm that uses a root concept, a basic level concept, and recursive surface patterns to learn automatically from the Web hyponym-hypernym pairs subordinated to the root; (2) a Web based concept positioning procedure to validate the learned pairs’ is-a relations; and (3) a graph algorithm that derives from scratch the integrated taxonomy structure of all the terms. Comparing results with WordNet, we ﬁnd that the algorithm misses some concepts and links, but also that it discovers many additional ones lacking in WordNet. We evaluate the taxonomization power of our method on reconstructing parts of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested. 
In contrast with the booming increase of internet data, state-of-art QA (question answering) systems, otherwise, concerned data from speciﬁc domains or resources such as search engine snippets, online forums and Wikipedia in a somewhat isolated way. Users may welcome a more general QA system for its capability to answer questions of various sources, integrated from existed specialized sub-QA engines. In this framework, question classiﬁcation is the primary task. However, the current paradigms of question classiﬁcation were focused on some speciﬁed type of questions, i.e. factoid questions, which are inappropriate for the general QA. In this paper, we propose a new question classiﬁcation paradigm, which includes a question taxonomy suitable to the general QA and a question classiﬁer based on MLN (Markov logic network), where rule-based methods and statistical methods are uniﬁed into a single framework in a fuzzy discriminative learning approach. Experiments show that our method outperforms traditional question classiﬁcation approaches. 
Recurrent event queries (REQ) constitute a special class of search queries occurring at regular, predictable time intervals. The freshness of documents ranked for such queries is generally of critical importance. REQ forms a signiﬁcant volume, as much as 6% of query trafﬁc received by search engines. In this work, we develop an improved REQ classiﬁer that could provide signiﬁcant improvements in addressing this problem. We analyze REQ queries, and develop novel features from multiple sources, and evaluate them using machine learning techniques. From historical query logs, we develop features utilizing query frequency, click information, and user intent dynamics within a search session. We also develop temporal features by time series analysis from query frequency. Other generated features include word matching with recurrent event seed words and time sensitivity of search result set. We use Naive Bayes, SVM and decision tree based logistic regression model to train REQ classiﬁer. The results on test data show that our models outperformed baseline approach signiﬁcantly. Experiments on a commercial Web search engine also show signiﬁcant gains in overall relevance, and thus overall user experience. 
With the proliferation of user-generated articles over the web, it becomes imperative to develop automated methods that are aware of the ideological-bias implicit in a document collection. While there exist methods that can classify the ideological bias of a given document, little has been done toward understanding the nature of this bias on a topical-level. In this paper we address the problem of modeling ideological perspective on a topical level using a factored topic model. We develop efﬁcient inference algorithms using Collapsed Gibbs sampling for posterior inference, and give various evaluations and illustrations of the utility of our model on various document collections with promising results. Finally we give a Metropolis-Hasting inference algorithm for a semi-supervised extension with decent results. 
We present a novel approach for (written) dialect identiﬁcation based on the discriminative potential of entire words. We generate Swiss German dialect words from a Standard German lexicon with the help of hand-crafted phonetic/graphemic rules that are associated with occurrence maps extracted from a linguistic atlas created through extensive empirical ﬁeldwork. In comparison with a charactern-gram approach to dialect identiﬁcation, our model is more robust to individual spelling differences, which are frequently encountered in non-standardized dialect writing. Moreover, it covers the whole Swiss German dialect continuum, which trained models struggle to achieve due to sparsity of training data. 
The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks ranging from the acquisition of synonyms and paraphrases to word sense disambiguation and textual entailment. Vector-based models are typically directed at representing words in isolation and thus best suited for measuring similarity out of context. In his paper we propose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 
We introduce tiered clustering, a mixture model capable of accounting for varying degrees of shared (context-independent) feature structure, and demonstrate its applicability to inferring distributed representations of word meaning. Common tasks in lexical semantics such as word relatedness or selectional preference can beneﬁt from modeling such structure: Polysemous word usage is often governed by some common background metaphoric usage (e.g. the senses of line or run), and likewise modeling the selectional preference of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneﬁcial and where it can be detrimental. 
We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model signiﬁcantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task. 
Inducing a grammar directly from text is one of the oldest and most challenging tasks in Computational Linguistics. Signiﬁcant progress has been made for inducing dependency grammars, however the models employed are overly simplistic, particularly in comparison to supervised parsing models. In this paper we present an approach to dependency grammar induction using tree substitution grammar which is capable of learning large dependency fragments and thereby better modelling the text. We deﬁne a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach signiﬁcantly improves the state-of-the-art, when measured by head attachment accuracy. 
We reveal a previously unnoticed connection between dependency parsing and statistical machine translation (SMT), by formulating the dependency parsing task as a problem of word alignment. Furthermore, we show that two well known models for these respective tasks (DMV and the IBM models) share common modeling assumptions. This motivates us to develop an alignment-based framework for unsupervised dependency parsing. The framework (which will be made publicly available) is ﬂexible, modular and easy to extend. Using this framework, we implement several algorithms based on the IBM alignment models, which prove surprisingly effective on the dependency parsing task, and demonstrate the potential of the alignment-based approach. 
This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or speciﬁc meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and deﬁnes how these meanings can be combined to analyze complete sentences. We use higher-order uniﬁcation to deﬁne a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efﬁciently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 
We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages. Our method uses a single set of manually-speciﬁed language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages. During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules. We also automatically reﬁne the syntactic categories given in our coarsely tagged input. Across six languages our approach outperforms state-of-theart unsupervised methods by a signiﬁcant margin.1 
Mining sentiment from user generated content is a very important task in Natural Language Processing. An example of such content is threaded discussions which act as a very important tool for communication and collaboration in the Web. Threaded discussions include e-mails, e-mail lists, bulletin boards, newsgroups, and Internet forums. Most of the work on sentiment analysis has been centered around ﬁnding the sentiment toward products or topics. In this work, we present a method to identify the attitude of participants in an online discussion toward one another. This would enable us to build a signed network representation of participant interaction where every edge has a sign that indicates whether the interaction is positive or negative. This is different from most of the research on social networks that has focused almost exclusively on positive links. The method is experimentally tested using a manually labeled set of discussion posts. The results show that the proposed method is capable of identifying attitudinal sentences, and their signs, with high accuracy and that it outperforms several other baselines.  us to build a signed network representation of participant interaction in which the interaction between two participants is represented using a positive or a negative edge. Even though using signed edges in social network studies is clearly important, most of the social networks research has focused only on positive links between entities. Some work has recently investigated signed networks (Leskovec et al., 2010; Kunegis et al., 2009), however this work was limited to a few number of datasets in which users were allowed to explicitly add negative, as well as positive, relations. This work will pave the way for research efforts to examine signed social networks in more detail. It will also allow us to study the relation between explicit relations and the text underlying those relation. Although similar, identifying sentences that display an attitude in discussions is different from identifying opinionated sentences. A sentence in a discussion may bear opinions about a deﬁnite target (e.g., price of a camera) and yet have no attitude toward the other participants in the discussion. For instance, in the following discussion Alice’s sentence has her opinion against something, yet no attitude toward the recipient of the sentence, Bob.  
We propose two hashing-based solutions to the problem of fast and effective personal names spelling correction in People Search applications. The key idea behind our methods is to learn hash functions that map similar names to similar (and compact) binary codewords. The two methods differ in the data they use for learning the hash functions - the ﬁrst method uses a set of names in a given language/script whereas the second uses a set of bilingual names. We show that both methods give excellent retrieval performance in comparison to several baselines on two lists of misspelled personal names. More over, the method that uses bilingual data for learning hash functions gives the best performance. 
Determining whether a textual phrase denotes a functional relation (i.e., a relation that maps each domain element to a unique range element) is useful for numerous NLP tasks such as synonym resolution and contradiction detection. Previous work on this problem has relied on either counting methods or lexico-syntactic patterns. However, determining whether a relation is functional, by analyzing mentions of the relation in a corpus, is challenging due to ambiguity, synonymy, anaphora, and other linguistic phenomena. We present the LEIBNIZ system that overcomes these challenges by exploiting the synergy between the Web corpus and freelyavailable knowledge resources such as Freebase. It ﬁrst computes multiple typed functionality scores, representing functionality of the relation phrase when its arguments are constrained to speciﬁc types. It then aggregates these scores to predict the global functionality for the phrase. LEIBNIZ outperforms previous work, increasing area under the precisionrecall curve from 0.61 to 0.88. We utilize LEIBNIZ to generate the ﬁrst public repository of automatically-identiﬁed functional relations. 
The rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation. In this paper, we present a multi-level generative model that reasons jointly about latent topics and geographical regions. High-level topics such as “sports” or “entertainment” are rendered differently in each geographic region, revealing topic-speciﬁc regional distinctions. Applied to a new dataset of geotagged microblogs, our model recovers coherent topics and their regional variants, while identifying geographic areas of linguistic consistency. The model also enables prediction of an author’s geographic location from raw text, outperforming both text regression and supervised topic models. 
This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for nonprojective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efﬁcient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets. 
Schedule  14:00 - 15:30 First part 15:30 - 16:00 Coffee break 16:00 - 17:30 Second part 
 Outline • Part I – Introduction – Paraphrase Identification – Paraphrase Extraction • Part II – Paraphrase Generation – Applications of Paraphrases – Evaluation of Paraphrases – Conclusions and Future work 
Honorifics in Japanese plays an incredibly important role in all walks of social life. The demand to transform regular expressions in Japanese into honorifics automatically has increased rapidly especially in business situations. This paper reviews existing studies and proposes a system to fill this demand with more practicable functions. The experiment shows the effectiveness of our strategy. 
This presentation introduces a Python module (PyCWN) for accessing and processing Chinese lexical resources. In particular, our focus is put on the Chinese Wordnet (CWN) that has been developed and released by CWN group at Academia Sinica. PyCWN provides the access to Chinese Wordnet (sense and relation data) under the Python environment. The presenation further demonstrates how this module applies to a variety of lexical processing tasks as well as the potentials for multilingual lexical processing. 
We present a tool for annotation of se­ mantic inter­sentential discourse rela­ tions on the tectogrammatical layer of the Prague Dependency Treebank (PDT). We present the way of helping the annotators by several useful features implemented in the annotation tool, such as a possibility to combine surface and deep syntactic representation of sen­ tences during the annotation, a possibili­ ty to define, display and connect arbi­ trary groups of nodes, a clause­based compact depiction of trees, etc. For studying differences among parallel an­ notations, the tool offers a simultaneous depiction of parallel annotations of the data. 
Have2eat is a popular mobile application available for iPhone and Android-based devices that helps users to ﬁnd and assess nearby restaurants. It lists restaurants located around the device and provides a quick highlight about the opinions expressed by online reviewers. Have2eat summarizes textual reviews by extracting relevant sentences and by automatically generating detailed ratings about speciﬁc aspects of the restaurant. A compact one-screen digest allows users to quickly access the information they need, expand to full review pages, and report their experience online by entering ratings and comments. 
 wider audiences as they allow users to ask  COMUNICA is a voice QA system for Brazilian Portuguese with search capabilities for consulting both structured and unstructured datasets. One of the goals of this work is to help address digital inclusion by providing an alternative way to accessing written information, which users can employ regardless of available computational resources or computational literacy.  questions in their own native language and especially if this includes spoken language, sometimes without the need even for a computer (e.g. using the phone). This paper describes COMUNICA, a voice QA system for Brazilian Portuguese with search capabilities for consulting both structured and unstructured datasets. The domain chosen to evaluate the system is that of municipal information from the FAMURS database.2 One of the goals of this work is to help address digital inclusion by providing a way to over-  
Online learning calls for instant assessment and feedback. YanFa is a system developed to score online EnglishChinese translation exercises with intelligent feedback for Chinese non-English majors. With the aid of HowNet and Cilin—Chinese Synonym Set (Extended Version), the system adopts the hybrid approach to scoring student translation semantically. It compares student translation with model translation by Synonym Matching, Sentence-pattern Matching and Word Similarity Calculating respectively. The experiment results show that the correlation ratio between the scores given by the system and by human raters is 0.58, which indicates that the algorithm is able to fulfill the task of automated scoring. YanFa is also able to provide feedback on syntactic mistakes made by students through interacting with them. It asks students to analyze the English sentence elements. Then it compares the student analyses with those of the parser and points out the parts which might lead to their wrong understanding as well as their wrong translating.  
This paper presents HCAMiner, a system focusing on detecting how concepts are linked across multiple documents. A traditional search involving, for example, two person names will attempt to find documents mentioning both these individuals. This research focuses on a different interpretation of such a query: what is the best concept chain across multiple documents that connects these individuals? A new robust framework is presented, based on (i) generating concept association graphs, a hybrid content representation, (ii) performing concept chain queries (CCQ) to discover candidate chains, and (iii) subsequently ranking chains according to the significance of relationships suggested. These functionalities are implemented using an interactive visualization paradigm which assists users for a better understanding and interpretation of discovered relationships. 
This demonstration presents a highperformance syntactic and semantic dependency parser. The system consists of a pipeline of modules that carry out the tokenization, lemmatization, part-of-speech tagging, dependency parsing, and semantic role labeling of a sentence. The system’s two main components draw on improved versions of a state-of-the-art dependency parser (Bohnet, 2009) and semantic role labeler (Bjo¨rkelund et al., 2009) developed independently by the authors. The system takes a sentence as input and produces a syntactic and semantic annotation using the CoNLL 2009 format. The processing time needed for a sentence typically ranges from 10 to 1000 milliseconds. The predicate–argument structures in the ﬁnal output are visualized in the form of segments, which are more intuitive for a user. 
PanLex is a lemmatic translation resource which combines a large number of translation dictionaries and other translingual lexical resources. It currently covers 1353 language varieties and 12M expressions, but aims to cover all languages and up to 350M expressions. This paper describes the resource and current applications of it, as well as lextract, a new effort to expand the coverage of PanLex via semi-automatic dictionary scraping. 
Antelogue is a pronoun resolution prototype designed to be released as off-the-shelf software to be used autonomously or integrated with larger anaphora resolution or other NLP systems. It has modules to handle pronouns in both text and dialogue. In Antelogue, the problem of pronoun resolution is addressed as a two-step process: a) acquiring information about properties of words and the entities they represent and b) determining an algorithm that utilizes these features to make resolution decisions. A hybrid approach is implemented that combines known statistical and machine learning techniques for feature acquisition and a symbolic algorithm for resolution. 
 tion between the primitives is explained by the se-  In this paper, we propose a lexical senses representation system called E-HowNet, in which the lexical senses are deﬁned by basic concepts. As a result, the meanings of expressions are more speciﬁc than those derived by using primitives. We also design an ontology to express the taxonomic relations between concepts and the attributes of concepts. To establish the taxonomic relations between word senses, we introduce a strategy that constructs the E-HowNet ontology automatically. We then implement the lexical ontology as a Web application1 to demonstrate the taxonomy and the search functions for querying key-terms and E-HowNet expressions in the lexicon, which contains more than 88,000 lexical senses.  mantic role “manner”. For further details, readers may refer to the E-HowNet technical report (CKIP 2009). With a well-established entity-relation model, semantic composition is applicable from the morphological level to the sentential level in EHowNet. Semantic compositionality, together with syntactic information, contributes enormously to natural language understanding. The remainder of this paper is organized as follows. We describe the major features of EHowNet in Section 2 and introduce the E-HowNet ontology in Section 3. Then, we present our online E-HowNet system in Section 4. Section 5 contains some concluding remarks. To achieve the goal of semantic compositionality and to extend the advantage from HowNet, the following features are implemented in E-HowNet. a) Multi-level deﬁnitions and semantic decom-  
The system presented is a web application designed to aid linguistic research with data collection and online publishing. It is a service mainly for linguists and language experts working with language description of less-documented and less-resourced languages. When the central concern is in-depth linguistic analysis, maintaining and administering software can be a burden. Cloud computing offers an alternative. At present mainly used for archiving, we extend linguistic web applications to allow creation, search and storage of interlinear annotated texts. By combining a conceptually appealing online glosser with an SQL database and a wiki, we make the online publication of linguistic data an easy task also for noncomputationally oriented researchers. 
The presentation will mainly cover (1) What is HowNet? HowNet is an on-line common-sense knowledgebase unveiling inter-conceptual relationships and interattribute relationships of concepts as connoting in lexicons of the Chinese and their English equivalents. (2) How it functions in the computation of meaning and as a NLP platform? The presentation will show 9 HowNet-based application tools. All of them are not merely demonstration of some methodology or algorithm, but are real application tools that can be tested by users themselves. Apart from the tools that are specially designed to deal with Chinese, most of the tools are bilingual, even the WSD tool. 
The mwetoolkit is a tool for automatic extraction of Multiword Expressions (MWEs) from monolingual corpora. It both generates and validates MWE candidates. The generation is based on surface forms, while for the validation, a series of criteria for removing noise are provided, such as some (language independent) association measures.1 In this paper, we present the use of the mwetoolkit in a standard conﬁguration, for extracting MWEs from a corpus of general-purpose English. The functionalities of the toolkit are discussed in terms of a set of selected examples, comparing it with related work on MWE extraction. 
There exists a well-established and almost unanimously adopted measure of tagger performance, namely, accuracy. Although it is perfectly adequate for small tagsets and typical approaches to disambiguation, we show that it is deﬁcient when applied to rich morphological tagsets and propose various extensions designed to better correlate with the real usefulness of the tagger. 
The use of semantic information to improve IR is a long-standing goal. This paper presents a novel Document Expansion method based on a WordNet-based system to ﬁnd related concepts and words. Expansion words are indexed separately, and when combined with the regular index, they improve the results in three datasets over a state-of-the-art IR engine. Considering that many IR systems are not robust in the sense that they need careful ﬁnetuning and optimization of their parameters, we explored some parameter settings. The results show that our method is specially effective for realistic, non-optimal settings, adding robustness to the IR engine. We also explored the effect of document length, and show that our method is specially successful with shorter documents. 
Machine-learned ranking techniques automatically learn a complex document ranking function given training data. These techniques have demonstrated the effectiveness and ﬂexibility required of a commercial web search. However, manually labeled training data (with multiple absolute grades) has become the bottleneck for training a quality ranking function, particularly for a new domain. In this paper, we explore the adaptation of machine-learned ranking models across a set of geographically diverse markets with the market-speciﬁc pairwise preference data, which can be easily obtained from clickthrough logs. We propose a novel adaptation algorithm, PairwiseTrada, which is able to adapt ranking models that are trained with multi-grade labeled training data to the target market using the target-market-speciﬁc pairwise preference data. We present results demonstrating the efﬁcacy of our technique on a set of commercial search engine data. 
The treatment of factual data has been widely studied in different areas of Natural Language Processing (NLP). However, processing subjective information still poses important challenges. This paper presents research aimed at assessing techniques that have been suggested as appropriate in the context of subjective - Opinion Question Answering (OQA). We evaluate the performance of an OQA with these new components and propose methods to optimally tackle the issues encountered. We assess the impact of including additional resources and processes with the purpose of improving the system performance on two distinct blog datasets. The improvements obtained for the different combination of tools are statistically significant. We thus conclude that the proposed approach is adequate for the OQA task, offering a good strategy to deal with opinionated questions. 
In this paper, we propose an approach to automatically detect sentiments on Twitter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover, we leverage sources of noisy labels as our training data. These noisy labels were provided by a few sentiment detection websites over twitter data. In our experiments, we show that since our features are able to capture a more abstract representation of tweets, our solution is more effective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources. 
We propose a methodology for investigating how well NLP systems handle meaning preserving syntactic variations. We start by presenting a method for the semi automated creation of a benchmark where entailment is mediated solely by meaning preserving syntactic variations. We then use this benchmark to compare a semantic role labeller and two grammar based RTE systems. We argue that the proposed methodology (i) supports a modular evaluation of the ability of NLP systems to handle the syntax/semantic interface and (ii) permits focused error mining and error analysis. 
Query expansion consists in extending user queries with related terms in order to solve the lexical gap problem in Information Retrieval and Question Answering. The main difﬁculty lies in identifying relevant expansion terms in order to prevent query drift. We propose to use deﬁnition clusters built from a combination of English lexical resources for query expansion. We apply the technique of pseudo relevance feedback to obtain expansion terms from deﬁnition clusters. We show that this expansion method outperforms both local feedback, based on the document collection, and expansion with WordNet synonyms, for the task of document retrieval in Question Answering. 
We present in this paper a formal approach for the representation of multimodal information. This approach, thanks to the to use of typed feature structures and hypergraphs, generalizes existing ones (typically annotation graphs) in several ways. It ﬁrst proposes an homogenous representation of different types of information (nodes and relations) coming from different domains (speech, gestures). Second, it makes it possible to specify constraints representing the interaction between the different modalities, in the perspective of developing multimodal grammars. 
This paper presents a framework for combining semantic relations extracted from text to reveal even more semantics that otherwise would be missed. A set of 26 relations is introduced, with their arguments deﬁned on an ontology of sorts. A semantic parser is used to extract these relations from noun phrases and verb argument structures. The method was successfully used in two applications: rapid customization of semantic relations to arbitrary domains and recognizing entailments. 
We address the problem of unsupervised and language-pair independent alignment of symmetrical and asymmetrical parallel corpora. Asymmetrical parallel corpora contain a large proportion of 1-to-0/0-to-1 and 1-to-many/many-to-1 sentence correspondences. We have developed a novel approach which is fast and allows us to achieve high accuracy in terms of F1 for the alignment of both asymmetrical and symmetrical parallel corpora. The source code of our aligner and the test sets are freely available. 
There has been relatively little work focused on determining the formality level of individual lexical items. This study applies information from large mixedgenre corpora, demonstrating that significant improvement is possible over simple word-length metrics, particularly when multiple sources of information, i.e. word length, word counts, and word association, are integrated. Our best hybrid system reaches 86% accuracy on an English near-synonym formality identiﬁcation task, and near perfect accuracy when comparing words with extreme formality differences. We also test our word association method in Chinese, a language where word length is not an appropriate metric for formality. 
This paper presents a methodology for a quantitative and qualitative evaluation of Textual Entailment systems. We take advantage of the decomposition of Text Hypothesis pairs into monothematic pairs, i.e. pairs where only one linguistic phenomenon at a time is responsible for entailment judgment, and propose to run TE systems over such datasets. We show that several behaviours of a system can be explained in terms of the correlation between the accuracy on monothematic pairs and the accuracy on the corresponding original pairs. 
We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French. The architectures are based on PCFGs with latent variables, graph-based dependency parsing and transition-based dependency parsing, respectively. We also study the inuence of three types of lexical information: lemmas, morphological features, and word clusters. The results show that all three systems achieve competitive performance, with a best labeled attachment score over 88%. All three parsers benet from the use of automatically derived lemmas, while morphological features seem to be less important. Word clusters have a positive effect primarily on the latent variable parser. 
As unlexicalized parsing lacks word token information, it is important to investigate novel parsing features to improve the accuracy. This paper studies a set of tree topological (TT) features. They quantitatively describe the tree shape dominated by each non-terminal node. The features are useful in capturing linguistic notions such as grammatical weight and syntactic branching, which are factors important to syntactic processing but overlooked in the parsing literature. By using an ensemble classifierbased model, TT features can significantly improve the parsing accuracy of our unlexicalized parser. Further, the ease of estimating TT feature values makes them easy to be incorporated into virtually any mainstream parsers. 
This paper proposes an approach to improve graph-based dependency parsing by using decision history. We introduce a mechanism that considers short dependencies computed in the earlier stages of parsing to improve the accuracy of long dependencies in the later stages. This relies on the fact that short dependencies are generally more accurate than long dependencies in graph-based models and may be used as features to help parse long dependencies. The mechanism can easily be implemented by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 
We conduct a series of Part-of-Speech (POS) Tagging experiments using Expectation Maximization (EM), Variational Bayes (VB) and Gibbs Sampling (GS) against the Chinese Penn Treebank. We want to first establish a baseline for unsupervised POS tagging in Chinese, which will facilitate future research in this area. Secondly, by comparing and analyzing the results between Chinese and English, we highlight some of the strengths and weaknesses of each of the algorithms in POS tagging task and attempt to explain the differences based on some preliminary linguistics analysis. Comparing to English, we find that all algorithms perform rather poorly in Chinese in 1-to-1 accuracy result but are more competitive in many-to-1 accuracy. We attribute one possible explanation of this to the algorithms’ inability to correctly produce tags that match the desired tag count distribution. 
This article delves into the scoring function of the statistical paraphrase generation model. It presents an algorithm for exact computation and two applicative experiments. The ﬁrst experiment analyses the behaviour of a statistical paraphrase generation decoder, and raises some issues with the ordering of n-best outputs. The second experiment shows that a major boost of performance can be obtained by embedding a true score computation inside a Monte-Carlo sampling based paraphrase generator. 
Unknown words are a major issue for large-scale grammars of natural language. We propose a machine learning based algorithm for acquiring lexical entries for all forms in the paradigm of a given unknown word. The main advantages of our method are the usage of word paradigms to obtain valuable morphological knowledge, the consideration of different contexts which the unknown word and all members of its paradigm occur in and the employment of a full-blown syntactic parser and the grammar we want to improve to analyse these contexts and provide elaborate syntactic constraints. We test our algorithm on a large-scale grammar of Dutch and show that its application leads to an improved parsing accuracy. 
Word co-occurrence networks are one of the most common linguistic networks studied in the past and they are known to exhibit several interesting topological characteristics. In this article, we investigate the global topological properties of word co-occurrence networks and, in particular, present a detailed study of their spectrum. Our experiments reveal certain universal trends found across the networks for seven different languages from three different language families, which are neither reported nor explained by any of the previous studies and models of word-cooccurrence networks. We hypothesize that since word co-occurrences are governed by syntactic properties of a language, the network has much constrained topology than that predicted by the previously proposed growth model. A deeper empirical and theoretical investigation into the evolution of these networks further suggests that they have a coreperiphery structure, where the core hardly evolves with time and new words are only attached to the periphery of the network. These properties are fundamental to the nature of word co-occurrence across languages. 
Creating correct, semantic representations of questions is essential for applications that can use formal reasoning to answer them. However, even within a restricted domain, it is hard to anticipate all the possible ways that a question might be phrased, and engineer reliable processing modules to produce a correct semantic interpretation for the reasoner. In our work on posing questions to a biology knowledge base, we address this brittleness in two ways: First, we exploit the DIRT paraphrase database to introduce alternative phrasings of a question; Second, we defer word sense and semantic role commitment until question answering. Resulting ambiguities are then resolved by interleaving additional interpretation with question-answering, allowing the combinatorics of alternatives to be controlled and domain knowledge to guide paraphrase and sense selection. Our evaluation suggests that the resulting system is able to understand exam-style questions more reliably. 
This paper studies two methods for training hierarchical MT rules independently of word alignments. Bilingual chart parsing and EM algorithm are used to train bitext correspondences. The ﬁrst method, rule arithmetic, constructs new rules as combinations of existing and reliable rules used in the bilingual chart, signiﬁcantly improving the translation accuracy on the German-English and Farsi-English translation task. The second method is proposed to construct additional rules directly from the chart using inside and outside probabilities to determine the span of the rule and its non-terminals. The paper also presents evidence that the rule arithmetic can recover from alignment errors, and that it can learn rules that are difﬁcult to learn from bilingual alignments. 
In this paper we look at the problem of cleansing noisy text using a statistical machine translation model. Noisy text is produced in informal communications such as Short Message Service (SMS), Twitter and chat. A typical Statistical Machine Translation system is trained on parallel text comprising noisy and clean sentences. In this paper we propose an unsupervised method for the translation of noisy text to clean text. Our method has two steps. For a given noisy sentence, a weighted list of possible clean tokens for each noisy token are obtained. The clean sentence is then obtained by maximizing the product of the weighted lists and the language model scores. 
We present a new reordering model estimated as a standard n-gram language model with units built from morphosyntactic information of the source and target languages. It can be seen as a model that translates the morpho-syntactic structure of the input sentence, in contrast to standard translation models which take care of the surface word forms. We take advantage from the fact that such units are less sparse than standard translation units to increase the size of bilingual context that is considered during the translation process, thus effectively accounting for mid-range reorderings. Empirical results on French-English and GermanEnglish translation tasks show that our model achieves higher translation accuracy levels than those obtained with the widely used lexicalized reordering model. 
Traditionally Sanskrit is written without blank, sentences can make thousands of characters without any separation. A critical edition takes into account all the different known versions of the same text in order to show the differences between any two distinct versions, in term of words missing, changed or omitted. This paper describes the Sanskrit characteristics that make text comparisons different from other languages, and will present different methods of comparison of Sanskrit texts which can be used for the elaboration of computer assisted critical edition of Sanskrit texts. It describes two sets of methods used to obtain the alignments needed. The ﬁrst set is using the L.C.S., the second one the global alignment algorithm. One of the methods of the second set uses a classical technique in the ﬁeld of artiﬁcial intelligence, the A* algorithm to obtain the suitable alignment. We conclude by comparing our different results in term of adequacy as well as complexity. 
In this paper, we present hybrid decoding — a novel statistical machine translation (SMT) decoding paradigm using multiple SMT systems. In our work, in addition to component SMT systems, system combination method is also employed in generating partial translation hypotheses throughout the decoding process, in which smaller hypotheses generated by each component decoder and hypotheses combination are used in the following decoding steps to generate larger hypotheses. Experimental results on NIST evaluation data sets for Chinese-to-English machine translation (MT) task show that our method can not only achieve signiﬁcant improvements over individual decoders, but also bring substantial gains compared with a state-of-the-art word-level system combination method. 
Global ranking, a new information retrieval (IR) technology, uses a ranking model for cases in which there exist relationships between the objects to be ranked. In the ranking task, the ranking model is defined as a function of the properties of the objects as well as the relations between the objects. Existing global ranking approaches address the problem by “learning to rank”. In this paper, we propose a global ranking framework that solves the problem via data fusion. The idea is to take each retrieved document as a pseudo-IR system. Each document generates a pseudo-ranked list by a global function. The data fusion algorithm is then adapted to generate the final ranked list. Taking a biomedical information extraction task, namely, interactor normalization task (INT), as an example, we explain how the problem can be formulated as a global ranking problem, and demonstrate how the proposed fusion-based framework outperforms baseline methods. By using the proposed framework, we improve the performance of the top 1 INT system by 3.2% using the official evaluation metric of the BioCreAtIvE challenge. In addition, by employing the standard ranking quality measure, NDCG, we demonstrate that * Corresponding author  the proposed framework can be cascaded with different local ranking models and improve their ranking results. 
In this paper the development of an opinion summarization system that works on Bengali News corpus has been described. The system identifies the sentiment information in each document, aggregates them and represents the summary information in text. The present sys-tem follows a topic-sentiment model for sentiment identification and aggregation. Topic-sentiment model is designed as discourse level theme identification and the topic-sentiment aggregation is achieved by theme clustering (k-means) and Document level Theme Relational Graph representation. The Document Level Theme Relational Graph is finally used for candidate summary sentence selection by standard page rank algorithms used in Information Retrieval (IR). As Bengali is a resource constrained language, the building of annotated gold standard corpus and acquisition of linguistics tools for lexico-syntactic, syntactic and discourse level features extraction are described in this paper. The reported accuracy of the Theme detection technique is 83.60% (precision), 76.44% (recall) and 79.85% (F-measure). The summarization system has been evaluated with Precision of 72.15%, Recall of 67.32% and Fmeasure of 69.65%. 
Automated identiﬁcation of diverse sentiment types can be beneﬁcial for many NLP systems such as review summarization and public media analysis. In some of these systems there is an option of assigning a sentiment value to a single sentence or a very short text. In this paper we propose a supervised sentiment classiﬁcation framework which is based on data from Twitter, a popular microblogging service. By utilizing 50 Twitter tags and 15 smileys as sentiment labels, this framework avoids the need for labor intensive manual annotation, allowing identiﬁcation and classiﬁcation of diverse sentiment types of short texts. We evaluate the contribution of different feature types for sentiment classiﬁcation and show that our framework successfully identiﬁes sentiment types of untagged sentences. The quality of the sentiment identiﬁcation was also conﬁrmed by human judges. We also explore dependencies and overlap between different sentiment types represented by smileys and Twitter hashtags. 
Recent work on distributional methods for similarity focuses on using the context in which a target word occurs to derive context-sensitive similarity computations. In this paper we present a method for computing similarity which builds vector representations for words in context by modeling senses as latent variables in a large corpus. We apply this to the Lexical Substitution Task and we show that our model signiﬁcantly outperforms typical distributional methods. 
Due to the lack of annotated data sets, there are few studies on machine learning based approaches to extract named entities (NEs) in clinical text. The 2009 i2b2 NLP challenge is a task to extract six types of medication related NEs, including medication names, dosage, mode, frequency, duration, and reason from hospital discharge summaries. Several machine learning based systems have been developed and showed good performance in the challenge. Those systems often involve two steps: 1) recognition of medication related entities; and 2) determination of the relation between a medication name and its modifiers (e.g., dosage). A few machine learning algorithms including Conditional Random Field (CRF) and Maximum Entropy have been applied to the Named Entity Recognition (NER) task at the first step. In this study, we developed a Support Vector Machine (SVM) based method to recognize medication related entities. In addition, we systematically investigated various types of features for NER in clinical text. Evaluation on 268 manually annotated discharge summaries from i2b2 challenge showed that the SVM-based NER system achieved the best F-score of 90.05% (93.20% Precision, 87.12% Recall), when semantic features generated from a rule-based system were included.  has many applications in general language domain such as identifying person names, locations, and organizations. NER is crucial for biomedical literature mining as well (Hirschman, Morgan, & Yeh, 2002; Krauthammer & Nenadic, 2004) and many studies have focused on biomedical entities, such as gene/protein names. There are mainly two types of approaches to identify biomedical entities: rule-based and machine learning based approaches. While rule-based approaches use existing biomedical knowledge/resources, machine learning (ML) based approaches rely much on annotated training data. The advantage of rule-based approaches is that they usually can achieve stable performance across different data sets due to the verified resources, while machine learning approaches often report better results when the training data are good enough. In order to harness the advantages of both approaches, the combination of them, called the hybrid approach, has often been used as well. CRF and SVM are two common machine learning algorithms that have been widely used in biomedical NER (Takeuchi & Collier, 2003; Kazama, Makino, Ohta, & Tsujii, 2002; Yamamoto, Kudo, Konagaya, & Matsumoto, 2003; Torii, Hu, Wu, & Liu, 2009; Li, Savova, & Kipper-Schuler, 2008). Some studies reported better results using CRF (Li, Savova, & Kipper-Schuler, 2008), while others showed that the SVM was better (Tsochantaridis, Joachims, & Hofmann, 2005) in NER. Keerthi & Sundararajan (Keerthi & Sundararajan, 2007) conducted some experiments and demonstrated that CRF and SVM were quite close in performance, when identical feature functions were used.  
Prepositions in English are a well-known challenge for language learners, and the computational analysis of preposition usage has attracted signiﬁcant attention. Such research generally starts out by developing models of preposition usage for native English based on a range of features, from shallow surface evidence to deep linguistically-informed properties. While we agree that ultimately a combination of shallow and deep features is needed to balance the preciseness of exemplars with the usefulness of generalizations to avoid data sparsity, in this paper we explore the limits of a purely surfacebased prediction of prepositions. Using a web-as-corpus approach, we investigate the classiﬁcation based solely on the relative number of occurrences for target n-grams varying in preposition usage. We show that such a surface-based approach is competitive with the published state-of-the-art results relying on complex feature sets. Where enough data is available, in a surprising number of cases it thus is possible to obtain sufﬁcient information from the relatively narrow window of context provided by n-grams which are small enough to frequently occur but large enough to contain enough predictive information about preposition usage.  
Several sets of explanatory variables – including shallow, language modeling, POS, syntactic, and discourse features – are compared and evaluated in terms of their impact on predicting the grade level of reading material for primary school students. We ﬁnd that features based on in-domain language models have the highest predictive power. Entity-density (a discourse feature) and POS-features, in particular nouns, are individually very useful but highly correlated. Average sentence length (a shallow feature) is more useful – and less expensive to compute – than individual syntactic features. A judicious combination of features examined here results in a signiﬁcant improvement over the state of the art. 
In statistical machine translation, decoding without any reordering constraint is an NP-hard problem. Inversion Transduction Grammars (ITGs) exploit linguistic structure and can well balance the needed ﬂexibility against complexity constraints. Currently, translation models with ITG constraints usually employs the cube-time CYK algorithm. In this paper, we present a shift-reduce decoding algorithm that can generate ITG-legal translation from left to right in linear time. This algorithm runs in a reduce-eager style and is suited to phrase-based models. Using the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can signiﬁcantly improve both the accuracy and the speed on different test sets. 
A new approach has been developed for acquiring bilingual web pages from the result pages of search engines, which is composed of two challenging tasks. The first task is to detect web records embedded in the result pages automatically via a clustering method of a sample page. Identifying these useful records through the clustering method allows the generation of highly effective features for the next task which is high-quality bilingual web page acquisition. The task of high-quality bilingual web page acquisition is a classification problem. One advantage of our approach is that it is search engine and domain independent. The test is based on 2516 records extracted from six search engines automatically and annotated manually, which gets a high precision of 81.3% and a recall of 94.93%. The experimental results indicate that our approach is very effective. 
The amount of information in medical publications continues to increase at a tremendous rate. Systematic reviews help to process this growing body of information. They are fundamental tools for evidence-based medicine. In this paper, we show that automatic text classification can be useful in building systematic reviews for medical topics to speed up the reviewing process. We propose a per-question classification method that uses an ensemble of classifiers that exploit the particular protocol of a systematic review. We also show that when integrating the classifier in the human workflow of building a review the per-question method is superior to the global method. We test several evaluation measures on a real dataset. 
This paper presents a fuzzy set theory based approach to Chinese sentence-level sentiment classification. Compared with traditional topic-based text classification techniques, the fuzzy set theory provides a straightforward way to model the intrinsic fuzziness between sentiment polarity classes. To approach fuzzy sentiment classification, we first propose a fine-to-coarse strategy to estimate sentence sentiment intensity. Then, we define three fuzzy sets to represent the respective sentiment polarity classes, namely positive, negative and neutral sentiments. Based on sentence sentiment intensities, we further build membership functions to indicate the degrees of an opinionated sentence in different fuzzy sets. Finally, we determine sentence-level polarity under maximum membership principle. We show that our approach can achieve promising performance on the test set for Chinese opinion analysis pilot task at NTCIR-6. 
Out-of-vocabulary (OOV) words present a signiﬁcant challenge for Machine Translation. For low-resource languages, limited training data increases the frequency of OOV words and this degrades the quality of the translations. Past approaches have suggested using stems or synonyms for OOV words. Unlike the previous methods, we show how to handle not just the OOV words but rare words as well in an Example-based Machine Translation (EBMT) paradigm. Presence of OOV words and rare words in the input sentence prevents the system from ﬁnding longer phrasal matches and produces low quality translations due to less reliable language model estimates. The proposed method requires only a monolingual corpus of the source language to ﬁnd candidate replacements. A new framework is introduced to score and rank the replacements by efﬁciently combining features extracted for the candidate replacements. A lattice representation scheme allows the decoder to select from a beam of possible replacement candidates. The new framework gives statistically significant improvements in English-Chinese and English-Haitian translation systems. 
Precision-oriented search results such as those typically returned by the major search engines are vulnerable to issues of polysemy. When the same term refers to different things, the dominant sense is preferred in the rankings of search results. In this paper, we propose a novel technique in the context of web search that utilizes contextual terms provided by users for query disambiguation, making it possible to prefer other senses without altering the original query. 
We present GENSEM, a tool for generating input semantic representations for two sentence generators based on the same reversible Tree Adjoining Grammar. We then show how GENSEM can be used to produced large and controlled benchmarks and test the relative performance of these generators. 
Verb sufﬁxes and verb complexes of morphologically rich languages carry a lot of information. We show that this information if harnessed for the task of shallow parsing can lead to dramatic improvements in accuracy for a morphologically rich language- Marathi1. The crux of the approach is to use a powerful morphological analyzer backed by a high coverage lexicon to generate rich features for a CRF based sequence classiﬁer. Accuracy ﬁgures of 94% for Part of Speech Tagging and 97% for Chunking using a modestly sized corpus (20K words) vindicate our claim that for morphologically rich languages linguistic insight can obviate the need for large amount of annotated corpora. 
Humans are very good at judging the strength of relationships between two terms, a task which, if it can be automated, would be useful in a range of applications. Systems attempting to solve this problem automatically have traditionally either used relative positioning in lexical resources such as WordNet, or distributional relationships in large corpora. This paper proposes a new approach, whereby relationships are derived from natural language text by using existing nlp tools, then integrated into a large scale semantic network. Spreading activation is then used on this network in order to judge the strengths of all relationships connecting the terms. In comparisons with human measurements, this approach was able to obtain results on par with the best purpose built systems, using only a relatively small corpus extracted from the web. This is particularly impressive, as the network creation system is a general tool for information collection and integration, and is not speciﬁcally designed for tasks of this type. 
State-of-the-art approaches for unsupervised keyphrase extraction are typically evaluated on a single dataset with a single parameter setting. Consequently, it is unclear how effective these approaches are on a new dataset from a different domain, and how sensitive they are to changes in parameter settings. To gain a better understanding of state-of-the-art unsupervised keyphrase extraction algorithms, we conduct a systematic evaluation and analysis of these algorithms on a variety of standard evaluation datasets. 
In this paper, we propose a novel framework to enrich Translation Memory (TM) systems with Statistical Machine Translation (SMT) outputs using ranking. In order to offer the human translators multiple choices, instead of only using the top SMT output and top TM hit, we merge the N-best output from the SMT system and the k-best hits with highest fuzzy match scores from the TM system. The merged list is then ranked according to the prospective post-editing effort and provided to the translators to aid their work. Experiments show that our ranked output achieve 0.8747 precision at top 1 and 0.8134 precision at top 5. Our framework facilitates a tight integration between SMT and TM, where full advantage is taken of TM while high quality SMT output is availed of to improve the productivity of human translators. 
Hierarchical phrase-based models provide a powerful mechanism to capture non-local phrase reorderings for statistical machine translation (SMT). However, many phrase reorderings are arbitrary because the models are weak on determining phrase boundaries for patternmatching. This paper presents a novel approach to learn phrase boundaries directly from word-aligned corpus without using any syntactical information. We use phrase boundaries, which indicate the beginning/ending of phrase reordering, as soft constraints for decoding. Experimental results and analysis show that the approach yields signiﬁcant improvements over the baseline on large-scale Chineseto-English translation. 
In the ﬁeld of multi-document summarization, the Pyramid method has become an important approach for evaluating machine-generated summaries. The method is based on the manual annotation of text spans with the same meaning in a set of human model summaries. In this paper, we present an unsupervised, probabilistic topic modeling approach for automatically identifying such semantically similar text spans. Our approach reveals some of the structure of model summaries and identiﬁes topics that are good approximations of the Summary Content Units (SCU) used in the Pyramid method. Our results show that the topic model identiﬁes topic-sentence associations that correspond to the contributors of SCUs, suggesting that the topic modeling approach can generate a viable set of candidate SCUs for facilitating the creation of Pyramids. 
This paper proposes a novel extractive summarization method for contact center dialogues. We use a particular type of hidden Markov model (HMM) called Class Speaker HMM (CSHMM), which processes operator/caller utterance sequences of multiple domains simultaneously to model domain-speciﬁc utterance sequences and common (domainwide) sequences at the same time. We applied the CSHMM to call summarization of transcripts in six different contact center domains and found that our method signiﬁcantly outperforms competitive baselines based on the maximum coverage of important words using integer linear programming. 
This paper proposes a supervised learning method to recognize expressions that show a relation between two named entities, e.g., person, location, or organization. The method uses two novel features, 1) whether the candidate words inherently express relations and 2) how the candidate words are inﬂuenced by the past relations of two entities. These features together with conventional syntactic and contextual features are organized as a tree structure and are fed into a boosting-based classiﬁcation algorithm. Experimental results show that the proposed method outperforms conventional methods. 
Previous works tend to compute the similarity between two sentences based on the comparison of their nearest meanings. However, the nearest meanings do not always represent their actual meanings. This paper presents a method which computes the similarity between two sentences based on a comparison of their actual meanings. This is achieved by transforming an existing most-outstanding corpus-based measure into a knowledge-based measure, which is then integrated with word sense disambiguation. The experimental results on a standard data set show that the proposed method outperforms the baseline and the improvement achieved is statistically significant at 0.025 levels. 
We introduce the novel problem of automatic related work summarization. Given multiple articles (e.g., conference/journal papers) as input, a related work summarization system creates a topic-biased summary of related work speciﬁc to the target paper. Our prototype Related Work Summarization system, ReWoS, takes in set of keywords arranged in a hierarchical fashion that describes a target paper’s topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones. Our initial results show an improvement over generic multi-document summarization baselines in a human evaluation. 
Re-ranking for Information Retrieval aims to elevate relevant feedbacks and depress negative ones in initial retrieval result list. Compared to relevance feedback-based re-ranking method widely adopted in the literature, this paper proposes a new method to well use three features in known negative feedbacks to identify and depress unknown negative feedbacks. The features include: 1) the minor (lower-weighted) terms in negative feedbacks; 2) hierarchical distance (HD) among feedbacks in a hierarchical clustering tree; 3) obstinateness strength of negative feedbacks. We evaluate the method on the TDT4 corpus, which is made up of news topics and their relevant stories. And experimental results show that our new scheme substantially outperforms its counterparts. 1. INTRODUCTION When we start out an information retrieval journey on a search engine, the first step is to enter a query in the search box. The query seems to be the most direct reflection of our information needs. However, it is short and often out of standardized syntax and terminology, resulting in a large number of negative feedbacks. Some researches focus on exploring long-term query logs to acquire query intent. This may be helpful for obtaining information relevant to specific interests but not to daily real-time query intents. Especially it is extremely difficult to determine whether the interests and which of them should be involved into certain queries. Therefore, given a query, it is important to “locally” ascertain its intent by using the real-time feedbacks.  Intuitively it is feasible to expand the query using the most relevant feedbacks (Chum et al., 2007). Unfortunately search engines just offer “farraginous” feedbacks (viz. pseudo-feedback) which may involve a great number of negative feedbacks. And these negative feedbacks never honestly lag behind relevant ones in the retrieval results, sometimes far ahead because of their great literal similarity to query. These noisy feedbacks often mislead the process of learning query intent. For so long, there had no effective approaches to confirm the relevance of feedbacks until the usage of the web click-through data (Joachims et al., 2003). Although the data are sometimes incredible due to different backgrounds and habits of searchers, they are still the most effective way to specify relevant feedbacks. This arouses recent researches about learning to rank based on supervised or semi-supervised machine learning methods, where the click-through data, as the direct reflection of query intent, offer reliable training data to learning the ranking functions. Although the learning methods achieve substantial improvements in ranking, it can be found that lots of “obstinate” negative feedbacks still permeate retrieval results. Thus an interesting question is why the relevant feedbacks are able to describe what we really need, but weakly repel what we do not need. This may attribute to the inherent characteristics of pseudo-feedback, i.e. their high literal similarity to queries. Thus no matter whether query expansion or learning to rank, they may fall in the predicament that “favoring” relevant feedbacks may result in “favoring” negative ones, and that “hurting” negative feedbacks may result in “hurting” relevant ones. However, there are indeed some subtle differences between relevant and negative feedbacks, e.g. the minor terms (viz. low-weighted terms in texts). Although these terms are often ignored in  436 Coling 2010: Poster Volume, pages 436–444, Beijing, August 2010  relevance measurement because their little effect recently that negative ones begin to receive some  on mining relevant feedbacks that have the same attention. Zhang (Zhang et al., 2009) utilize the  topic or kernel, they are useful in distinguishing irrelevance distribution to estimate the true rele-  relevant feedbacks from negative ones. As a re- vance model. Their work gives the evidence that  sult, these minor terms provides an opportunity negative feedbacks are useful in the ranking  to differentiate the true query intent from its process. However, their work focuses on gener-  counterpart intents (called “opposite intents” ating a better description of query intent to attract  thereafter in this paper). And the “opposite in- relevant information, but ignoring that negative  tents” are adopted to depress negative feedbacks feedbacks have the independent effect on repel-  without “hurting” the ranks of relevant feedbacks. ling their own kind. That is, if we have a king,  In addition, hierarchical clustering tree is helpful we will not refuse a queen. In contrast, Wang  to establish the natural similarity correlation (Wang et al., 2008) benefit from the independent  among information. So this paper adopts the hi- effect from the negative feedbacks. Their method  erarchical distance among feedbacks in the tree represents the opposite of query intent by using  to enhance the “opposite intents” based division negative feedbacks and adopts that to discount  of relevant and negative feedbacks. Finally, an the relevance of each pseudo-feedback to a query.  obstinateness factor is also computed to deal However, their work just gives a hybrid repre-  with some obstinate negative feedbacks in the sentation of opposite intent which may overlap  top list of retrieval result list. In fact, Teevan much with the relevance model. Although an-  (Teevan et al., 2008) observed that most search- other work (Wang et al., 2007) of them filters  ers tend to browse only a few feedbacks in the query terms from the opposite intent, such filter-  first one or two result pages. So our method fo- ing makes little effect because of the sparsity of  cuses on improving the precision of highly the query terms in pseudo-feedback.  ranked retrieval results.  Other related work includes query expansion,  The rest of the paper is organized as follows. term extraction and text clustering. In fact, query  Section 2 reviews the related work. Section 3 expansion techniques are often the chief benefi-  describes our new irrelevance feedback-based ciary of click-through data (Chum et al., 2007).  re-ranking scheme and the HD measure. Section However, the query expansion techniques via  4 introduces the experimental settings while Sec- clicked feedbacks fail to effectively repel nega-  tion 5 reports experimental results. Finally, Sec- tive ones. This impels us to focus on un-clicked  tion 6 draws the conclusion and indicates future feedbacks. Cao (Cao et al., 2008) report the ef-  work.  fectiveness of selecting good expansion terms for  2. RELATED WORK  pseudo-feedback. Their work gives us a hint about the shortcomings of the one-sided usage of  Our work is motivated by information search behaviors, such as eye-tracking and click through (Joachims, 2003). Thereinto, the click-through behavior is most widely used for acquiring query intent. Up to present, several interesting fea-  high-weighted terms. Lee (Lee et al., 2008) adopt a cluster-based re-sampling method to emphasize the core topic of a query. Their repeatedly feeding process reveals the hierarchical relevance of pseudo-feedback.  tures, such as click frequency and hit time on 3. RE-RANKING SCHEME  click graph (Craswell et al., 2007), have been  extracted from click-through data to improve 3.1 Re-ranking Scheme  search results. However, although effective on query learning, they fail to avoid the thorny problem that even when the typed query and the click-through data are the same, their intents may not be the same for different searchers. A considerable number of studies have explored pseudo-feedback to learn query intent, thus refining page ranking. However, most of them focus on the relevant feedbacks. It is until  The re-ranking scheme, as shown in Figure 1, consists of three components: acquiring negative feedbacks, measuring irrelevance feedbacks and re-ranking pseudo-feedback. Given a query and its search engine results, we start off the re-ranking process after a trigger point. The point may occur at the time when searchers click on “next page” or any hyperlink.  437  All feedbacks before the point are assumed to 3.2 Representing Opposite Intent  have been seen by searchers. Thus the un-clicked feedbacks before the point will be treated as the known negative feedbacks because they attract no attention of searchers. This may be questioned because searchers often skip some hyperlinks that have the same contents as before, even if the links are relevant to their interests. However, such skip normally reflects the true searching intent because novel relevant feedbacks always have more attractions after all.  It is necessary for the representation of opposite intent to obey two basic rules: 1) the opposite intent should be much different from the query intent; and 2) it should reflect the independent effect of negative feedbacks. Given a query, it seems easy to represent its opposite intent by using a vector of high-weighted terms of negative feedbacks. However, the vector is actually a “close relative” of query intent because the terms often have  much overlap with that of relevant feedbacks.  And the overlapping terms are exactly the source  of the highly ranked negative feedbacks. Thus  we should throw off the overlapping terms and  focus on the rest instead.  In this paper, we propose two simple facilities  in representing opposite intent. One is a vector of  the weighted terms (except query terms) occur-  ring in the known negative feedbacks, named as  Figure 1. Re-ranking scheme  O(−q) , while another further filters out the high-weighted terms occurring in the known  Another crucial step after the trigger point is to relevant feedbacks, named as O(−q − r) . Although  generate the opposite intent by using the known O(−q) filters out query terms, the terms are so  negative feedbacks. But now we temporarily sparse that they contribute little to opposite intent  leave the issue to Section 3.2 and assume that we learning. Thus, we will not explore O(−q) fur-  have obtained a good representation of the oppo- ther in this paper (Our preliminary experiments  site intent, and meanwhile that of query intent confirm our reasoning). In contrast, O(−q − r) not  has been composed of the highly weighted terms only differs from the representation of query in-  in the known relevant feedbacks and query terms. tent due to its exclusion of query terms but also  Thus, given an unseen pseudo-feedback, we can emphasize the low-weighted terms occurring in  calculate its overall ranking score predisposed to negative feedbacks due to exclusion of  the opposite intent as follows: R _ score = O _ score − α ⋅ I _ score  high-weighted terms occurring in the known (1) relevant feedbacks.  where the O_score is the relevance score to the  opposite intent, I_score is that to the query intent 3.3 Employing Opposite Intent  and α is a weighting factor. On the basis, we re-rank the unseen feedbacks in ascending order. That is, the feedback with the largest score appears at the bottom of the ranked list. It is worthwhile to emphasize that although the overall ranking score, i.e. R_score, looks similar to Wang (Wang et al., 2008) who adopts the inversely discounted value (i.e. the relevance score is calculated as I _ score - α ⋅ O _ score ) to re-rank feedbacks in descending order, they are actually quite different because our overall ranking score as shown in Equation (1) is designed to depress negative feedbacks, thereby achieving the similar effect to filtering.  Another key issue in our re-ranking scheme is how to measure the relevance of all the feedbacks to the opposite intent, i.e. O_score, thereby the ranking score R_score. For simplicity, we only consider Boolean measures in employing opposite intent to calculate the ranking score R_score. Assume that given a query, there are N known relevant feedbacks and N known negative ones. First, we adopt query expansion to acquire the representation of query intent. This is done by pouring all terms of the N relevant feedbacks and query terms into a bag of words, where all the occurring weights of each term are  438  accumulated, and extracting n top-weighted  terms to represent the query intent as I(+q + r) .  Then, we use the N negative feedbacks to rep-  resent the n-dimensional opposite intents  O(−q − r) . For any unseen pseudo-feedback u, we  also represent it using an n-dimensional vector  V (u) which contains its n top-weighted terms. In  all the representation processes, the TFIDF  weighting is adopted.  Thus, for an unseen pseudo-feedback u, the  relevance scores to the query intent and the op-  posite intent can be measured as: I _ score(u) = B{ V (u), I (+q + r) } (2) O _ score(u) = B{ V (u), O(−q − r) }  where B{∗,∗} indicates Boolean calculation:  ∑ B{X ,Y} = b{xi ,Y}, xi ∈ X  ⎧1, b{xi ,Y} = ⎩⎨0,  if xi ∈ Y if xi ∉ Y  (3)  In particular, we simply set the factor α , as  mentioned in Equation (1), to 1 so as to balance  the effect of query intent and its opposite intent  on the overall ranking score. The intuition is that  if an unseen pseudo-feedback has more overlap-  ping terms with O(−q − r) than I (+q + r) , it will  has higher probability of being depressed as an  negative feedback.  Two alternatives to the above Boolean meas-  ure are to employ the widely-adopted VSM co-  sine measure and Kullback-Liebler (KL) diver-  gence (Thollard et al., 2000). However, such  term-weighting alternatives will seriously elimi-  nate the effect of low-weighted terms, which is  core of our negative feedback-based re-ranking  scheme.  3.4 Hierarchical Distance (HD) Measure  The proposed method in Section 3.3 ignores two key issues. First, given a query, although search engine has thrown away most opposite intents, it is unavoidable that the pseudo-feedback still involves more than one opposite intent. However, the representation O(−q − r) has the difficulty in highlighting all the opposite intents because the feature fusion of the representation smoothes the independent characteristics of each opposite intent. Second, given several opposite intents, they have different levels of effects on the negative score O _ score(u) . And the effects cannot be measured by the unilateral score.  Figure 2. Weighted distance calculation  To solve the issues, we propose a hierarchical  distance based negative measure, abbr. HD,  which measures the distances among feedbacks  in a hierarchical clustering tree, and involves  them into hierarchical division of relevance score.  Given two random leaves u and v in the tree,  their HD score is calculated as:  HD _ score(u,v) = rel(u,v)  (4)  W (u,v)  where rel(∗,∗) indicates textual similarity, W (∗,∗)  indicates the weighted distance in the tree, which  is calculated as:  ∑ W (u,v) = wi (u,v)  (5)  i∈m  where m is the total number of the edges between two leaves, wi(∗,∗) indicates the weight of the i-th edge. In this paper, we adopt CLUTO to generate the hierarchical binary tree, and simply let each wi(∗,∗) equal 1. Thus the W (∗,∗) becomes to be the number of edges m, for example, the W ( j,k) equals 5 in Figure 2. On the basis, given an unseen feedback u, we can acquire its modified re-ranking score R ′_ score by following steps. First, we regard each known negative feedback as an opposite intent, following the two generative rules (mentioned in section 3.2) to generate its n-dimensional representation O(−q − r) . Additionally we represent both the known relevant feedbacks and the unseen feedback u as n-dimensional term vectors. Second, we cluster these feedbacks to generate a hierarchical binary tree and calculate the HD score for each pair of (u,∗) , where ∗ denotes a leaf in the tree except u. Thus the modified ranking score is calculated as:  ∑ ∑ R ′_ score = HDI _ score(u, vi ) − HDI _ score(u, v j ) (6)  i∈N  j∈N  where vi indicates the i-th known negative feedback in the leaves, N is the total number of  439  v , v j indicates the j-th known relevant feedback, N is the total number of v . Besides, we still adopt Boolean value to measure the textual similarity rel(∗,∗) in both clustering process and ranking score calculation, thus the HD score in the formula (6) can be calculated as follows:  HD _ score(u, v) = B{ V (u), V (v) } W (u, v) (7) HD _ score(u, v) = O _ score(u) W (u, v)  3.5 Obstinateness Factor  Additionally we involve an interesting feature,  i.e. the obstinate degree, into our re-ranking  scheme. The degree is represented by the rank of  negative feedbacks in the original retrieval re-  sults. That is, the more “topping the list” an  negative feedback is, the more obstinate it is.  Therefore we propose a hypothesis that if a  feedback is close to the obstinate feedback, it  should be obstinate too. Thus given an unseen  feedback u, its relevance to an opposite intent in  HD can be modified as:  O _ score(u)′ = (1 + β ) ⋅ O _ score(u)  (8)  rnk  where rnk indicates the rank of the opposite intent in original retrieval results (Note: in HD, every known negative feedback is an opposite intent), β is a smoothing factor. Because as- cending order is used in our re-ranking process, by the weighting coefficient, i.e. (1 + β / rnk) , the feedback close to the obstinate opposite intents will be further depressed. But the coefficient is not commonly used. In HD, we firstly ascertain the feedback closest to u, and if the feedback is known to be negative, set to vmax , we will use the Equation (8) to punish the pair of (u, vmax ) alone, otherwise without any punishment.  4. EXPERIMENTAL SETTING  4.1 Data Set We evaluate our methods with two TDT collections: TDT 2002 and TDT 2003. There are 3,085 stories in the TDT 2002 collection are manually labeled as relevant to 40 news topics, 30,736 ones irrelevant to any of the topics. And 3,083 news stories in the TDT 2003 collection are labeled as relevant to another 40 news topics, 15833 ones irrelevant to them. In our evaluation,  we adopt TDT 2002 as training set, and TDT 2003 as test set. Besides, only English stories are used, both Mandarin and Arabic ones are replaced by their machine-translated versions (i.e. mttkn2 released by LDC).  Corpus  good  fair  poor  TDT 2002  26  7  7  TDT 2003  22  10  8  Table 1. Number of queries referring to different  types of feedbacks (Search engine: Lucene 2.3.2)  In our experiments, we realize a simple search engine based on Lucene 2.3.2 which applies document length to relevance measure on the basis of traditional literal term matching. To emulate the real retrieval process, we extract the title from the interpretation of news topic and regard it as a query, and then we run the search engine on the TDT sets and acquire the first 1000 pseudo-feedback for each query. All feedbacks will be used as the input of our re-ranking process, where the hand-crafted relevant stories default to the clicked feedbacks. By the search engine, we mainly obtain three types of pseudo-feedback: “good”, “fair” and “poor”, where “good” denotes that more than 5 clicked (viz. relevant) feedbacks are in the top 10, “fair” denotes more than 2 but less than 5, “poor” denotes less than 2. Table 1 shows the number of queries referring to different types of feedbacks.  4.2 Evaluation Measure  We use three evaluation measures in experiments,  P@n, NDCG@n and MAP. Thereinto, P@n de-  notes the precision of top n feedbacks. On the  basis, NDCG takes into account the influence of  position to precision. NDCG at position n is cal-  culated as:  ∑ NDCG @ n = 1 ⋅ DCG @ N =  n 2r(ui ) − 1 i=1 log(1 + i)  (9)  Zn  Zn  where i is the position in the result list, Zn is a normalizing factor and chosen so that for the  perfect list DCG at each position equals one, and  r(ui) equals 1 when ui is relevant feedback, else 0. While MAP additionally takes into account recall,  calculated as:  ∑ ∑ MAP = 1 m  m 1( i=1 Ri  k j=1ri (u j ) ⋅ ( p @ j)i )  (10)  where m is the total number of queries, so MAP  gives the average measure of precision and recall  440  for multiple queries, Ri is the total number of feedbacks relevant to query i, and k is the number of pseudo-feedback to the query. Here k is indicated to be 1000, thus Map can give the average measure for all positions of result list. 4.3 Systems We conduct experiments using four main systems, in which the search engine based on Lucene 2.3.2, regarded as the basic retrieval system, provides the pseudo-feedback for the following three re-ranking systems. Exp-sys: Query is expanded by the first N known relevant feedbacks and represented by an n-dimensional vector which consists of n distinct terms. The standard TFIDF-weighted cosine metric is used to measure the relevance of the unseen pseudo-feedback to query. And the relevance-based descending order is in use. Wng-sys: A system realizes the work of Wang (Wang et al., 2008), where the known relevant feedbacks are used to represent query intent, and the negative feedbacks are used to generate opposite intent. Thus, the relevance score of a feedback is calculated as I_scorewng- αw ⋅ O_scorewng, and the relevance-based descending order is used in re-ranking. Our-sys: A system is approximately similar to Wng-sys except that the relevance is measured by O_scoreour- α ⋅ I_scoreour and the pseudo-feedback is re-ranked in ascending order. Additionally both Wng-sys and Our-sys have three versions. We show them in Table 2, where “I” corresponds to the generation rule of query intent, “O” to that of opposite intent, Rel. means relevance measure, u is an unseen feedback, v is a known relevant feedback, v is a known negative feedback. 5. RESULTS 5.1 Main Training Result We evaluate the systems mainly in two circumstances: when both N and N equal 1 and when they equal 5. In the first case, we assume that retrieval capability is measured under given few known feedbacks; in the second, we emulate the first page turning after several feedbacks have been clicked by searchers. Besides, the approximately optimal value of n for the Exp-sys, which is trained to be 50, is adopted as the global value for all other systems. The training results are shown in Figure 3, where the Exp-sys never  gains much performance improvement when n is  greater than 50. In fairness to effects of “I” and  “O” on relevance measure, we also make n  equal 50. In addition, all the discount factors  (viz. α , α w2 and α w3) initially equal 1, and the  smoothing factor β is trained to be 0.5.  “I” n-dimensional vector for each v, Number of v in use is N  “O” Wng-sys1 Rel.  None  ∑N  R _ scorew1 = (  cos(u, v)) / N i =1  Number of v in use is N, all v combine into a n-dimensional “I” bag of words bw2  Wng-sys2 “O” Number of v in use is N , all v combine into a n-dimensional words bag bw2  Rel.  R _ scorew2 = cos(u,bw2 ) − αw2 ⋅ cos(u,bw2)  “I” Similar generation rules to Wng-sys2 except that query  Wng-sys3 “O” terms are removed from bag of words bw3 and bw3  Rel.  R _ scorew3 = cos(u,bw3) − αw3 ⋅ cos(u,bw3)  “I”  I (+q + r) in section 3.3  Our-sys1 “O”  O(−q − r) in section 3.2  Rel.  R _ score = O _ score − α ⋅ I _ score  “I”  The same generation rules to Our-sys1  “O”  Our-sys2  HD algorithm:  ∑ ∑ Rel. R ′_ score = HDI _ score(u,vi ) − HDI _ score(u,v j )  i∈N  j∈N  “I” “O”  The same generation rules to Our-sys1  Our-sys3  HD algorithm + obstinateness factor:  Rel.  O _ score(u)′ = (1 + β ) ⋅ O _ score(u)  rnk  Table 2. All versions of both Wngs and Ours  Figure 3. Parameter training of Exp-sys For each query we re-rank all the pseudo-feedback, including that defined as known, so P@20 and NDCG@20 are in use to avoid over-fitting (such as P@10 and NDCG@10 given both N and N equal 5 ). We show the main training results in Table 3, where our methods achieve much better performances than the re-ranking methods based on relevant feedback learning when N= N =5. Thereinto, our basic system, i.e. Our-sys1, at least achieves approximate 5% improvement on P@20, 3% on NDCG@20 and 1% on MAP than the optimal wng-sys (viz. wng-sys1). And obvi-  441  ously the most substantial improvements are contributed by the HD measure which even increases the P@20 of Our-sys1 by 8.5%, NDCG@20 by 13% and MAP by 9%. But it is slightly disappointing that the obstinateness factor only has little effectiveness on performance improvement, although Our-sys3 nearly wins the best retrieval results. This may stem from “soft” punishment on obstinateness, that is, for an unseen feedback, only the obstinate companion closest to the feedback is punished in relevance measure.  -  Our-sys1 Our-sys2 Exp-sys Wng-sys1 Basic  P@20 0.6603 0.8141 0.63125 0.7051 0.6588  NDCG@20 0.7614 0.8587 0.8080 0.7797 0.6944  MAP 0.6583 0.7928 0.5955 0.7010 0.6440  Table 3. Main training results  It is undeniable that all the re-ranking systems  work worse than the basic search engine when  the known feedbacks are rare, such as N = N =1.  This motivates an additional test on the higher  values of both N and N ( N = N =9), as shown  in Table 4. Thus it can be found that most of the  re-ranking systems achieve much better per-  formance than the basic search engine. An im-  portant reason for this is that more key terms can  be involved into representations of both query  intent and its opposite intent. So it seems that  more manual intervention is always reliable.  However in practice, seldom searchers are will-  ing to use an unresponsive search engine that can  only offer relatively satisfactory feedbacks after  lots of click-through and page turning. And in  fact at least two pages (if one page includes 10  pseudo-feedback) need to be turned in the train-  ing corpus when both N and N equal 9. So  we just regard the improvements benefiting from  high click-through rate as an ideal status, and  still adopt the practical numerical value of N  and N , i.e. N = N =5, to run following test.  terms consistently involved in query representa-  tion by Exp-sys.  systems Basic Exp-sys Wng-sys1 Wng-sys2 Wng-sys3 Our-sys1 Our-sys2 Our-sys3  N=N 
Because English is a low morphology language, current statistical parsers tend to ignore morphology and accept some level of redundancy. This paper investigates how costly such redundancy is for a lexicalised grammar such as CCG. We use morphological analysis to split verb inﬂectional sufﬁxes into separate tokens, so that they can receive their own lexical categories. We ﬁnd that this improves accuracy when the splits are based on correct POS tags, but that errors in gold standard or automatically assigned POS tags are costly for the system. This shows that the parser can beneﬁt from morphological analysis, so long as the analysis is correct. 
Choosing the right parameters for a word sense disambiguation task is critical to the success of the experiments. We explore this idea for prepositions, an often overlooked word class. We examine the parameters that must be considered in preposition disambiguation, namely context, features, and granularity. Doing so delivers an increased performance that signiﬁcantly improves over two state-ofthe-art systems, and shows potential for improving other word sense disambiguation tasks. We report accuracies of 91.8% and 84.8% for coarse and ﬁne-grained preposition sense disambiguation, respectively. 
Annotating scientific publications with keywords and phrases is of great importance to searching, indexing, and cataloging such documents. Unlike previous studies that focused on usercentric annotation, this paper presents our investigation of various annotation characteristics on service-centric annotation. Using a large number of publicly available annotated scientific publications, we characterized and compared the two different types of annotation processes. Furthermore, we developed an automatic approach of annotating scientific publications based on a machine learning algorithm and a set of novel features. When compared to other methods, our approach shows significantly improved performance. Experimental data sets and evaluation results are publicly available at the supplementary website1. 
In this paper, we explore a CLIR-based approach to construct large-scale Chinese-English comparable corpora, which is valuable for translation knowledge mining. The initial source and target document sets are crawled from news website and standardized uniformly. Keywords are extracted from the source document firstly, and then the extracted keywords are translated and combined as query words through certain criteria to retrieve against the index created using target document set. Meanwhile, the mapping correlations between source and target documents are developed according to the value of similarity calculated by the retrieval tool. Two methods are evaluated to filter the comparable document pairs so as to ensure the quality of the comparable corpora. Experimental results indicate that our approach is effective on the construction of ChineseEnglish comparable corpora. 
Many existing methods for bilingual lexicon learning from comparable corpora are based on similarity of context vectors. These methods suffer from noisy vectors that greatly affect their accuracy. We introduce a method for ﬁltering this noise allowing highly accurate learning of bilingual lexicons. Our method is based on the notion of in-domain terms which can be thought of as the most important contextually relevant words. We provide a method for identifying such terms. Our evaluation shows that the proposed method can learn highly accurate bilingual lexicons without using orthographic features or a large initial seed dictionary. In addition, we also introduce a method for measuring the similarity between two words in different languages without requiring any initial dictionary. 
Our goal is to propose a description model for the lexicon. We describe a software framework for representing the lexicon and its variations called Proteus. Various examples show the different possibilities offered by this tool. We conclude with a demonstration of the use of lexical resources in complex, real examples. 
In this paper, we propose languagespecific methods of sentiment analysis in morphologically rich languages. In contrast of previous works confined to statistical methods, we make use of various linguistic features effectively. In particular, we make chunk structures by using the dependence relations of morpheme sequences to restrain semantic scope of influence of opinionated terms. In conclusion, our linguistic structural methods using chunking improve the results of sentiment analysis in Korean news corpus. This approach will aid sentiment analysis of other morphologically rich languages like Japanese and Turkish. 
Information Extraction (IE) technology is facing new challenges of dealing with large-scale heterogeneous data sources from different documents, languages and modalities. Information fusion, a new emerging area derived from IE, aims to address these challenges. We specify the requirements and possible solutions to perform information fusion. The issues include redundancy removal, contradiction resolution and uncertainty reduction. We believe this is a critical step to advance IE to a higher level of performance and portability. 
We describe an effective constituent projection strategy, where constituent projection is performed on the basis of dependency projection. Especially, a novel measurement is proposed to evaluate the candidate projected constituents for a target language sentence, and a PCFG-style parsing procedure is then used to search for the most probable projected constituent tree. Experiments show that, the parser trained on the projected treebank can signiﬁcantly boost a state-of-the-art supervised parser. When integrated into a tree-based machine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 
This paper presents a comparative study on two key problems existing in extractive summarization: the ranking problem and the selection problem. To this end, we presented a systematic study of comparing different learning-to-rank algorithms and comparing different selection strategies. This is the first work of providing systematic analysis on these problems. Experimental results on two benchmark datasets demonstrate three findings: (1) pairwise and listwise learning-to-rank algorithms outperform the baselines significantly; (2) there is no significant difference among the learning-to-rank algorithms; and (3) the integer linear programming selection strategy generally outperformed Maximum Marginal Relevance and Diversity Penalty strategies. 
We present a method for producing a bird’s-eye view of statements that are expressed on Web pages on a given topic. This method aggregates statements that are relevant to the topic, and shows contradictory and contrastive relations among them. This view of contradictions and contrasts helps users acquire a top-down understanding of the topic. To realize this, we extract such statements and relations, including cross-document implicit contrastive relations between statements, in an unsupervised manner. Our experimental results indicate the effectiveness of our approach. 
We present a probabilistic generative model for learning semantic parsers from ambiguous supervision. Our approach learns from natural language sentences paired with world states consisting of multiple potential logical meaning representations. It disambiguates the meaning of each sentence while simultaneously learning a semantic parser that maps sentences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators. 
Unlike static documents, version controlled documents are continuously edited by one or more authors. Such collaborative revision process makes traditional modeling and visualization techniques inappropriate. In this paper we propose a new representation based on local spacetime smoothing that captures important revision patterns. We demonstrate the applicability of our framework using experiments on synthetic and real-world data. 
 PNCs occur in a wide range of languages (Himmelmann, 1998); the conditions for deter-  The realization of singular count nouns without an accompanying determiner inside a PP (determinerless PP, bare PP, Preposition-Noun Combination) has recently attracted some interest in computational linguistics. Yet, the relevant factors for determiner omission remain unclear, and conditions for determiner omission vary from language to language. We present a logistic regression model of determiner omission in German based on data obtained by applying annotation mining to a large, automatically and manually annotated corpus.  miner omission, however, have not been detected yet, and conditions applying to one language do not carry over to other languages. In addition, speakers only reluctantly judge the acceptability of newly coined PNCs, so that reliance to introspective judgments cannot be assumed. For English, Stvan (1998) and Baldwin et al. (2006) have claimed that either the semantics of the preposition or of the noun play a major role in determining whether a singular count noun may appear without a determiner in a PNC. Stvan (1998) assumes that nouns determine the well-formedness of PNCs (3) if the denotation of the noun occurs in a particular semantic field, while Baldwin et al. (2006) assume that certain  
This paper presents a novel approach to dialogue act recognition employing multilevel information features. In addition to features such as context information and words in the utterances, the recognition task utilizes syntactic and semantic relations acquired by information extraction methods. These features are utilized by a Bayesian network classiﬁer for our dialogue act recognition. The evaluation results show a clear improvement from the accuracy of the baseline (only with word features) with 61.9% to an accuracy of 67.4% achieved by the extended feature set. 
The optimal choice of speech understanding method depends on the amount of training data available in rapid prototyping. A statistical method is ultimately chosen, but it is not clear at which point in the increase in training data a statistical method become effective. Our framework combines multiple automatic speech recognition (ASR) and language understanding (LU) modules to provide a set of speech understanding results and selects the best result among them. The issue is how to allocate training data to statistical modules and the selection module in order to avoid overﬁtting in training and obtain better performance. This paper presents an automatic training data allocation method that is based on the change in the coefﬁcients of the logistic regression functions used in the selection module. Experimental evaluation showed that our allocation method outperformed baseline methods that use a single ASR module and a single LU module at every point while training data increase.  systems, it needs to be constructed for each system, and accordingly, training data are required for each. To collect more real training data, which will lead to higher performance, it is more desirable to use a prototype system than that based on the Wizard-of-Oz (WoZ) method where real ASR errors cannot be observed, and to use a more accurate speech understanding module. That is, in the bootstrapping phase, spoken dialogue systems need to operate before sufﬁcient real data have been collected. We have been addressing the issue of rapid prototyping on the basis of the “Multiple Language model for ASR and Multiple language Understanding (MLMU)” framework (Katsumaru et al., 2009). In MLMU, the most reliable speech understanding result is selected from candidates produced by various combinations of multiple ASR and LU modules using hand-crafted grammar and statistical models. A grammar-based method is still effective at an early stage of system development because it does not require training data; Schapire et al. (2005) also incorporated humancrafted prior knowledge into their boosting algorithm. By combining multiple understanding modules, complementary results can be obtained by different kinds of ASR and LU modules.  
We present a transformation scheme that mediates between description logics (DL) or RDF-encoded ontologies and type hierarchies in feature logics (FL). The DL-to-FL direction is illustrated by an implemented ofﬂine procedure that maps ontologies with large, dynamically maintained instance data to named entity (NE) and information extraction (IE) resources encoded in typed feature structures. The FL-to-DL translation is exempliﬁed by a (currently manual) translation of so-called MRS (Minimal Recursion Semantics) representations into OWL instances that are based on OWL classes, generated from the the type hierarchy of a deep linguistic grammar. The paper will identify parts of knowledge which can be translated from one formalism into the other without loosing information and parts which can only be approximated. The work described here is important for the Semantic Web to become a reality, since semantic annotations of natural language documents (DL) can be automatically generated by shallow and deep natural language parsing systems (FL). 
Implicit relevance feedback has proved to be a important resource in improving search accuracy and personalization. However, researchers who rely on feedback data for testing their algorithms or other personalization related problems are loomed with problems like unavailability of data, staling up of data and so on. Given these problems, we are motivated towards creating a synthetic user relevance feedback data, based on insights from query log analysis. We call this simulated feedback. We believe that simulated feedback can be immensely beneﬁcial to web search engine and personalization research communities by greatly reducing efforts involved in collecting user feedback. The beneﬁts from ”Simulated feedback” are - it is easy to obtain and also the process of obtaining the feedback data is repeatable, customizable and does not need the interactions of the user. In this paper, we describe a simple yet effective approach for creating simulated feedback. We have evaluated our system using the clickthrough data of the users and achieved 77% accuracy in generating click-through data. 
This paper presents the novel task of best topic word selection, that is the selection of the topic word that is the best label for a given topic, as a means of enhancing the interpretation and visualisation of topic models. We propose a number of features intended to capture the best topic word, and show that, in combination as inputs to a reranking model, we are able to consistently achieve results above the baseline of simply selecting the highest-ranked topic word. This is the case both when training in-domain over other labelled topics for that topic model, and cross-domain, using only labellings from independent topic models learned over document collections from different domains and genres. 
 (such as adjectival modiﬁcation) as edges. Each  We present a new method, based on graph theory, for bilingual lexicon extraction without relying on resources with limited availability like parallel corpora. The graphs we use represent linguistic relations between words such as adjectival modiﬁcation. We experiment with a number of ways of combining different linguistic relations and present a novel method, multi-edge extraction (MEE), that is both modular and scalable. We evaluate MEE on adjectives, verbs and nouns and show that it is superior to cooccurrence-based extraction (which does not use linguistic analysis). Finally, we publish a reproducible baseline to establish an evaluation benchmark for bilingual lexicon extraction. 
Statistical word alignment often suffers from data sparseness. Part-of-speeches are often incorporated in NLP tasks to reduce data sparseness. In this paper, we attempt to mitigate such problem by reflecting alignment tendency between part-of-speeches to statistical word alignment. Because our approach does not rely on any language-dependent knowledge, it is very simple and purely statistic to be applied to any language pairs. End-to-end evaluation shows that the proposed method can improve not only the quality of statistical word alignment but the performance of statistical machine translation. 
We describe a new information fusion approach to integrate facts extracted from cross-media objects (videos and texts) into a coherent common representation including multi-level knowledge (concepts, relations and events). Beyond standard information fusion, we exploited video extraction results and significantly improved text Information Extraction. We further extended our methods to multi-lingual environment (English, Arabic and Chinese) by presenting a case study on cross-lingual comparable corpora acquisition based on video comparison. 
In this paper, we present an unsupervised hybrid model which combines statistical, lexical, linguistic, contextual, and temporal features in a generic EMbased framework to harvest bilingual terminology from comparable corpora through comparable document alignment constraint. The model is configurable for any language and is extensible for additional features. In overall, it produces considerable improvement in performance over the baseline method. On top of that, our model has shown promising capability to discover new bilingual terminology with limited usage of dictionaries. 
This paper introduces several extractive approaches for automatic image tagging, relying exclusively on information mined from texts. Through evaluations on two datasets, we show that our methods exceed competitive baselines by a large margin, and compare favorably with the stateof-the-art that uses both textual and image features. 
An unsupervised discriminative training procedure is proposed for estimating a language model (LM) for machine translation (MT). An English-to-English synchronous context-free grammar is derived from a baseline MT system to capture translation alternatives: pairs of words, phrases or other sentence fragments that potentially compete to be the translation of the same source-language fragment. Using this grammar, a set of impostor sentences is then created for each English sentence to simulate confusions that would arise if the system were to process an (unavailable) input whose correct English translation is that sentence. An LM is then trained to discriminate between the original sentences and the impostors. The procedure is applied to the IWSLT Chinese-to-English translation task, and promising improvements on a state-ofthe-art MT system are demonstrated. 
This paper presents a novel featurebased semantic role labeling (SRL) method which uses both constituent and dependency syntactic views. Comparing to the traditional SRL method relying on only one syntactic view, the method has a much richer set of syntactic features. First we select several important constituent-based and dependency-based features from existing studies as basic features. Then, we propose a statistical method to select discriminative combined features which are composed by the basic features. SRL is achieved by using the SVM classifier with both the basic features and the combined features. Experimental results on Chinese Proposition Bank (CPB) show that the method outperforms the traditional constituent-based or dependency-based SRL methods. 
We investigate the effectiveness of different linguistic cues for distinguishing literal and non-literal usages of potentially idiomatic expressions. We focus specifically on features that generalize across different target expressions. While idioms on the whole are frequent, instances of each particular expression can be relatively infrequent and it will often not be feasible to extract and annotate a sufﬁcient number of examples for each expression one might want to disambiguate. We experimented with a number of different features and found that features encoding lexical cohesion as well as some syntactic features can generalize well across idioms. 
Various strategies have been proposed to enhance web search through utilizing individual user information. However, considering the well acknowledged recurring queries and repetitive clicks among users, it is still an open issue whether using individual user information is a proper direction of efforts in improving the web search. In this paper, we first quantitatively demonstrate that individual user information is more beneficial than common user information. Then we statistically compare the benefit of individual and common user information through Kappa statistic. Finally, we calculate potential for personalization to present an overview of what queries can benefit more from individual user information. All these analyses are conducted on both English AOL log and Chinese Sogou log, and a bilingual perspective statistics consistently confirms our findings. 
Sentence-level aligned parallel texts are important resources for a number of natural language processing (NLP) tasks and applications such as statistical machine translation and cross-language information retrieval. With the rapid growth of online parallel texts, efﬁcient and robust sentence alignment algorithms become increasingly important. In this paper, we propose a fast and robust sentence alignment algorithm, i.e., FastChampollion, which employs a combination of both length-based and lexiconbased algorithm. By optimizing the process of splitting the input bilingual texts into small fragments for alignment, FastChampollion, as our extensive experiments show, is 4.0 to 5.1 times as fast as the current baseline methods such as Champollion (Ma, 2006) on short texts and achieves about 39.4 times as fast on long texts, and Fast-Champollion is as robust as Champollion. 
Tibetan word segmentation is essential for Tibetan information processing. People mainly use the basic machine matching method which is based on dictionary to segment Tibetan words at present, because there is no segmented Tibetan corpus which can be used for training in Tibetan word segmentation. But the method based on dictionary is not fit to Tibetan number identification. This paper studies the characteristics of Tibetan numbers, and then, proposes a method to identify Tibetan numbers based on classification of number components. The method first tags every number component according to the class it belongs to while segmenting, and then updates the tag series according to some predefined rules. At last adjacent number components are combined to form a Tibetan number if they meet a certain requirement. In the testing result from 7938K Tibetan corpus, the identification accuracy is 99.21%. 
 Gildea and Jurafsky (2002) first tackled SRL as an independent task, which is divided into  We propose a novel MLN-based method that collectively conducts SRL on groups of news sentences. Our method is built upon a baseline SRL, which uses no parsers and leverages redundancy. We evaluate our method on a manually labeled news corpus and demonstrate that news redundancy significantly improves the performance of the baseline, e.g., it improves the F-score from 64.13% to 67.66%. *  several sub-tasks such as argument identification, argument classification, global inference, etc. Some researchers (Xue and Palmer, 2004; Koomen et al., 2005; Cohn and Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005; Toutanova et al., 2008) used a pipelined approach to attack the task. Some others resolved the sub-tasks simultaneously. For example, some work (Musillo and Merlo, 2006; Merlo and Musillo, 2008) integrated syntactic parsing and SRL into a single model, and another (Riedel and Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009)  
While ITG has many desirable properties for word alignment, it still suffers from the limitation of one-to-one matching. While existing approaches relax this limitation using phrase pairs, we propose a ITG formalism, which even handles units of non-contiguous words, using both simple and hierarchical phrase pairs. We also propose a parameter estimation method, which combines the merits of both supervised and unsupervised learning, for the ITG formalism. The ITG alignment system achieves significant improvement in both word alignment quality and translation performance. 
Visually and phonologically similar characters are major contributing factors for errors in Chinese text. By defining appropriate similarity measures that consider extended Cangjie codes, we can identify visually similar characters within a fraction of a second. Relying on the pronunciation information noted for individual characters in Chinese lexicons, we can compute a list of characters that are phonologically similar to a given character. We collected 621 incorrect Chinese words reported on the Internet, and analyzed the causes of these errors. 83% of these errors were related to phonological similarity, and 48% of them were related to visual similarity between the involved characters. Generating the lists of phonologically and visually similar characters, our programs were able to contain more than 90% of the incorrect characters in the reported errors. 
Phrase-based statistical MT (SMT) is a milestone in MT. However, the translation model in the phrase based SMT is structure free which greatly limits its reordering capacity. To address this issue, we propose a non-lexical headmodifier based reordering model on word level by utilizing constituent based parse tree in source side. Our experimental results on the NIST ChineseEnglish benchmarking data show that, with a very small size model, our method significantly outperforms the baseline by 1.48% bleu score. 
Recent kernel-based PPI extraction systems achieve promising performance because of their capability to capture structural syntactic information, but at the expense of computational complexity. This paper incorporates dependency information as well as other lexical and syntactic knowledge in a feature-based framework. Our motivation is that, considering the large amount of biomedical literature being archived daily, feature-based methods with comparable performance are more suitable for practical applications. Additionally, we explore the difference of lexical characteristics between biomedical and newswire domains. Experimental evaluation on the AIMed corpus shows that our system achieves comparable performance of 54.7 in F1-Score with other state-of-the-art PPI extraction systems, yet the best performance among all the feature-based ones. 
 performance especially when estimating ratings  In this paper, we propose a review selection approach towards accurate estimation of feature ratings for services on participatory websites where users write textual reviews for these services. Our approach selects reviews that comprehensively talk about a feature of a service by using information distance of the reviews on the feature. The rating estimation of the feature for these selected reviews using machine learning techniques provides more accurate results than that for other reviews. The average of these estimated feature ratings also better represents an accurate overall rating for the feature of the service, which provides useful feedback for other users to choose their satisfactory services. 
We present several ways of measuring the inter-annotator agreement in the ongoing annotation of semantic inter-sentential discourse relations in the Prague Dependency Treebank (PDT). Two ways have been employed to overcome limitations of measuring the agreement on the exact location of the start/end points of the relations. Both methods – skipping one tree level in the start/end nodes, and the connective-based measure – are focused on a recognition of the existence and of the type of the relations, rather than on fixing the exact positions of the start/end points of the connecting arrows. 
News Comments on the web express readers’ attitudes or opinions about an event or object in the corresponding news article. And opinion target extraction from news comments is very important for many useful Web applications. However, many sentences in the comments are irregular and informal, and sometimes the opinion targets are implicit. Thus the task is very challenging and it has not been investigated yet. In this paper, we propose a new approach to uniformly extracting explicit and implicit opinion targets from news comments by using Centering Theory. The approach uses global information in news articles as well as contextual information in adjacent sentences of comments. Our experimental results verify the effectiveness of the proposed approach. 
We use robust and fast Finite-State Machines (FSMs) to solve scriptural translation problems. We describe a phonetico-morphotactic pivot UIT (universal intermediate transcription), based on the common phonetic repository of Indo-Pak languages. It is also extendable to other language groups. We describe a finitestate scriptural translation model based on finite-state transducers and UIT. We report its performance on Hindi, Urdu, Punjabi and Seraiki corpora. For evaluation, we design two classification scales based on the word and sentence accuracies for translation system classifications. We also show that subjective evaluations are vital for real life usage of a translation system in addition to objective evaluations. 
Text documents are complex high dimensional objects. To effectively visualize such data it is important to reduce its dimensionality and visualize the low dimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore dimensionality reduction methods that draw upon domain knowledge in order to achieve a better low dimensional embedding and visualization of documents. We consider the use of geometries speciﬁed manually by an expert, geometries derived automatically from corpus statistics, and geometries computed from linguistic resources. 
The Varro toolkit is a system for identifying and counting a major class of regularity in treebanks and annotated natural language data in the form of treestructures: frequently recurring unordered subtrees. This software has been designed for use in linguistics to be maximally applicable to actually existing treebanks and other stores of tree-structurable natural language data. It minimizes memory use so that moderately large treebanks are tractable on commonly available computer hardware. This article introduces condensed canonically ordered trees as a data structure for efﬁciently discovering frequently recurring unordered subtrees. 
This paper investigates the new problem of automatic sense induction for instance names using automatically extracted attribute sets. Several clustering strategies and data sources are described and evaluated. We also discuss the drawbacks of the evaluation metrics commonly used in similar clustering tasks. The results show improvements in most metrics with respect to the baselines, especially for polysemous instances. 
Most existing techniques for combining multiple alignment tables can combine only two alignment tables at a time, and are based on heuristics (Och and Ney, 2003), (Koehn et al., 2003). In this paper, we propose a novel mathematical formulation for combining an arbitrary number of alignment tables using their power mean. The method frames the combination task as an optimization problem, and ﬁnds the optimal alignment lying between the intersection and union of multiple alignment tables by optimizing the parameter p: the afﬁnely extended real number deﬁning the order of the power mean function. The combination approach produces better alignment tables in terms of both F-measure and BLEU scores. 
Traditional 1-best translation pipelines suffer a major drawback: the errors of 1best outputs, inevitably introduced by each module, will propagate and accumulate along the pipeline. In order to alleviate this problem, we use compact structures, lattice and forest, in each module instead of 1-best results. We integrate both lattice and forest into a single tree-to-string system, and explore the algorithms of lattice parsing, lattice-forest-based rule extraction and decoding. More importantly, our model takes into account all the probabilities of different steps, such as segmentation, parsing, and translation. The main advantage of our model is that we can make global decision to search for the best segmentation, parse-tree and translation in one step. Medium-scale experiments show an improvement of +0.9 BLEU points over a state-of-the-art forest-based baseline. 
 been under development and new projects start every year. PWN database contains about 150000  In this paper, an automatic method for Persian WordNet construction based on Prenceton WordNet 2.1 (PWN) is introduced. The proposed approach uses Persian and English corpora as well as a bilingual dictionary in order to make a mapping between PWN synsets and Persian words. Our method calculates a score for each candidate synset of a given Persian word and for each of its translation, it selects the synset with maximum score as a link to the Persian word. The manual evaluation on selected links proposed by our method on 500 randomly selected Persian words, shows about 76.4% quality respect to precision measure. By augmenting the Persian WordNet with the un-ambiguous words, the total accuracy of automatically extracted Persian WordNet is about 82.6% which outperforms the previously semi-automated generated Persian WordNet by about 12.6%.  words organized in over 115000 synsets. Manual construction of WordNet is a time consuming task and requires linguistic knowledge. A number of automatic methods were proposed for constructing WordNet for other languages that use PWN and other existing lexical resources. In order to help the development of WordNets for other languages rather than English, especially for European one, a project named EuroWordNet was found (Vossen, 1999), in which a number of automatic methods for construction of such databases were proposed (Farreres et al., 1998). There have been some other efforts to create a WordNet for Persian language (Famian, 2007; Rouhizadeh et al., 2008; Shamsfard, 2008) but there exists no Persian WordNet yet that covers all Persian words in dictionary and comparable with PWN. These projects have tried to construct Persian WordNet in the manually or semi automatic manner. In (Shamsfard, 2008) a semi automatic method is proposed in which for each Persian word a number of PWN synsets are sug-  
Entity sense disambiguation becomes difﬁcult with few or even zero training instances available, which is known as imbalanced learning problem in machine learning. To overcome the problem, we create a new set of reliable training instances from dictionary, called dictionarybased prototypes. A hierarchical classiﬁcation system with a tree-like structure is designed to learn from both the prototypes and training instances, and three different types of classiﬁers are employed. In addition, supervised dimensionality reduction is conducted in a similarity-based space. Experimental results show our system outperforms three baseline systems by at least 8.3% as measured by macro F1 score. 
The goal of this work is to produce a classifier that can distinguish subjective sentences from objective sentences for the Urdu language. The amount of labeled data required for training automatic classifiers can be highly imbalanced especially in the multilingual paradigm as generating annotations is an expensive task. In this work, we propose a cotraining approach for subjectivity analysis in the Urdu language that augments the positive set (subjective set) and generates a negative set (objective set) devoid of all samples close to the positive ones. Using the data set thus generated for training, we conduct experiments based on SVM and VSM algorithms, and show that our modified VSM based approach works remarkably well as a sentence level subjectivity classifier. 
We propose a method for the task of identifying the general positions of users in online debates, i.e., support or oppose the main topic of an online debate, by exploiting local information in their remarks within the debate. An online debate is a forum where each user post an opinion on a particular topic while other users state their positions by posting their remarks within the debate. The supporting or opposing remarks are made by directly replying to the opinion, or indirectly to other remarks (to express local agreement or disagreement), which makes the task of identifying users’ general positions difﬁcult. A prior study has shown that a linkbased method, which completely ignores the content of the remarks, can achieve higher accuracy for the identiﬁcation task than methods based solely on the contents of the remarks. In this paper, we show that utilizing the textual content of the remarks into the link-based method can yield higher accuracy in the identiﬁcation task. 
In this paper, we present a two-stage approach to acquire Japanese unknown morphemes from text with full POS tags assigned to them. We ﬁrst acquire unknown morphemes only making a morphologylevel distinction, and then apply semantic classiﬁcation to acquired nouns. One advantage of this approach is that, at the second stage, we can exploit syntactic clues in addition to morphological ones because as a result of the ﬁrst stage acquisition, we can rely on automatic parsing. Japanese semantic classiﬁcation poses an interesting challenge: proper nouns need to be distinguished from common nouns. It is because Japanese has no orthographic distinction between common and proper nouns and no apparent morphosyntactic distinction between them. We explore lexico-syntactic clues that are extracted from automatically parsed text and investigate their effects. 
Lexicalized Well-Founded Grammar (LWFG) is a recently developed syntacticsemantic grammar formalism for deep language understanding, which balances expressiveness with provable learnability results. The learnability result for LWFGs assumes that the semantic composition constraints are learnable. In this paper, we show what are the properties and principles the semantic representation and grammar formalism require, in order to be able to learn these constraints from examples, and give a learning algorithm. We also introduce a LWFG parser as a deductive system, used as an inference engine during LWFG induction. An example for learning a grammar for noun compounds is given. 
This paper proposes a method for evaluating grammatical error detection methods to maximize the learning effect obtained by grammatical error detection. To achieve this, this paper sets out the following two hypotheses — imperfect, rather than perfect, error detection maximizes learning effect; and precisionoriented error detection is better than a recall-oriented one in terms of learning effect. Experiments reveal that (i) precisionoriented error detection has a learning effect comparable to that of feedback by a human tutor, although the ﬁrst hypothesis is not supported; (ii) precision-oriented error detection is better than recall-oriented in terms of learning effect; (iii) -measure is not always the best way of evaluating error detection methods. 
We present novel kernels based on structured and unstructured features for reranking the N-best hypotheses of conditional random ﬁelds (CRFs) applied to entity extraction. The former features are generated by a polynomial kernel encoding entity features whereas tree kernels are used to model dependencies amongst tagged candidate examples. The experiments on two standard corpora in two languages, i.e. the Italian EVALITA 2009 and the English CoNLL 2003 datasets, show a large improvement on CRFs in F-measure, i.e. from 80.34% to 84.33% and from 84.86% to 88.16%, respectively. Our analysis reveals that both kernels provide a comparable improvement over the CRFs baseline. Additionally, their combination improves CRFs much more than the sum of the individual contributions, suggesting an interesting kernel synergy. 
In this paper we propose a novel algorithm for opinion summarization that takes account of content and coherence, simultaneously. We consider a summary as a sequence of sentences and directly acquire the optimum sequence from multiple review documents by extracting and ordering the sentences. We achieve this with a novel Integer Linear Programming (ILP) formulation. Our proposed formulation is a powerful mixture of the Maximum Coverage Problem and the Traveling Salesman Problem, and is widely applicable to text generation and summarization tasks. We score each candidate sequence according to its content and coherence. Since our research goal is to summarize reviews, the content score is deﬁned by opinions and the coherence score is developed in training against the review document corpus. We evaluate our method using the reviews of commodities and restaurants. Our method outperforms existing opinion summarizers as indicated by its ROUGE score. We also report the results of human readability experiments. 
Position information has been proved to be very effective in document summarization, especially in generic summarization. Existing approaches mostly consider the information of sentence positions in a document, based on a sentence position hypothesis that the importance of a sentence decreases with its distance from the beginning of the document. In this paper, we consider another kind of position information, i.e., the word position information, which is based on the ordinal positions of word appearances instead of sentence positions. An extractive summarization model is proposed to provide an evaluation framework for the position information. The resulting systems are evaluated on various data sets to demonstrate the effectiveness of the position information in different summarization tasks. Experimental results show that word position information is more effective and adaptive than sentence position information. 
Supervised semantic role labeling (SRL) systems are generally claimed to have accuracies in the range of 80% and higher (Erk and Pado´, 2006). These numbers, though, are the result of highly-restricted evaluations, i.e., typically evaluating on hand-picked lemmas for which training data is available. In this paper we consider performance of such systems when we evaluate at the document level rather than on the lemma level. While it is wellknown that coverage gaps exist in the resources available for training supervised SRL systems, what we have been lacking until now is an understanding of the precise nature of this coverage problem and its impact on the performance of SRL systems. We present a typology of ﬁve different types of coverage gaps in FrameNet. We then analyze the impact of the coverage gaps on performance of a supervised semantic role labeling system on full texts, showing an average oracle upper bound of 46.8%. 
The aim of this study is to use the word-space model to measure the semantic loads of single verbs, proﬁle verbal lexicon acquisition, and explore the semantic information on Chinese resultative verb compounds (RVCs). A distributional model based on Academia Sinica Balanced Corpus (ASBC) with Latent Semantic Analysis (LSA) is built to investigate the semantic space variation depending on the semantic loads/speciﬁcity. The between group comparison of age-related changes in verb style is then conducted to suggest the inﬂuence of semantic space on verbal acquisition. Finally, it demonstrates how meaning exploring on RVCs is done with semantic space. 
We present a novel algorithm for detecting errors in MT, speciﬁcally focusing on content words that are deleted during MT. We evaluate it in the context of cross-lingual question answering (CLQA), where we try to correct the detected errors by using a better (but slower) MT system to retranslate a limited number of sentences at query time. Using a query-dependent ranking heuristic enabled the system to direct scarce MT resources towards retranslating the sentences that were most likely to beneﬁt CLQA. The error detection algorithm identiﬁed spuriously deleted content words with high precision. However, retranslation was not an effective approach for correcting them, which indicates the need for a more targeted approach to error correction in the future. 
A weakly supervised method uses anonymized search queries to induce a ranking among class labels extracted from unstructured text for various instances. The accuracy of the extracted class labels exceeds that of previous methods, over evaluation sets of instances associated with Web search queries. 
This article presents an original lexical unit extraction system for Chinese. The method is based on an incremental process driven by an association score featuring a minimal resources statistically aided linguistic approach. We also introduce a linguistics-based lexical unit deﬁnition and use it to describe an evaluation protocol dedicated to the task. The experimental results on a domain speciﬁc corpus show that the method performs better than other approaches. The extraction results, evaluated on a random sample of the working corpus, show a recall of 68.4 % and precision of 37.1 %. 
We demonstrate the use of context features, namely, names of places, and unlabelled data for the detection of personal name language of origin. While some early work used either rule-based methods or n-gram statistical models to determine the name language of origin, we use the discriminative classiﬁcation maximum entropy model and view the task as a classiﬁcation task. We perform bootstrapping of the learning using list of names out of context but with known origin and then using expectation-maximisation algorithm to further train the model on a large corpus of names of unknown origin but with context features. Using a relatively small unlabelled corpus we improve the accuracy of name origin recognition for names written in Chinese from 82.7% to 85.8%, a signiﬁcant reduction in the error rate. The improvement in F -score for infrequent Japanese names is even greater: from 77.4% without context features to 82.8% with context features. 
Texts are replete with gaps, information omitted since authors assume a certain amount of background knowledge. We define the process of enrichment that fills these gaps. We describe how enrichment can be performed using a Background Knowledge Base built from a large corpus. We evaluate the effectiveness of various openly available background knowledge bases and we identify the kind of information necessary for enrichment. 
In this paper we present a new algorithm for the Person Cross Document Coreference task. We show that accurate results require a way to adapt the parameters of the similarity function – metrics and threshold – to the ontological constraints obeyed by individuals. The technique we propose dynamically changes the initial weights computed when the context is analyzed. The weight recomputation is necessary in order to resolve clusters borders, which are inevitably blurred by a static approach. The results show a significant gain in accuracy. 
We present an evaluation framework for plagiarism detection.1 The framework provides performance measures that address the speciﬁcs of plagiarism detection, and the PAN-PC-10 corpus, which contains 64 558 artiﬁcial and 4 000 simulated plagiarism cases, the latter generated via Amazon’s Mechanical Turk. We discuss the construction principles behind the measures and the corpus, and we compare the quality of our corpus to existing corpora. Our analysis gives empirical evidence that the construction of tailored training corpora for plagiarism detection can be automated, and hence be done on a large scale. 
With OWL (Web Ontology Language) established as a standard for encoding ontologies on the Semantic Web, interest has begun to focus on the task of verbalising OWL code in controlled English (or other natural language). Current approaches to this task assume that axioms in OWL can be mapped to sentences in English. We examine three potential problems with this approach (concerning logical sophistication, information structure, and size), and show that although these could in theory lead to insuperable difﬁculties, in practice they seldom arise, because ontology developers use OWL in ways that favour a transparent mapping. This result is evidenced by an analysis of patterns from a corpus of over 600,000 axioms in about 200 ontologies. 
We go beyond simple propositional meaning extraction and present experiments in determining which propositions in text the author believes. We show that deep syntactic parsing helps for this task. Our best feature combination achieves an Fmeasure of 64%, a relative reduction in Fmeasure error of 21% over not using syntactic features. 
Studies of discourse relations have not, in the past, attempted to characterize what serves as evidence for them, beyond lists of frozen expressions, or markers, drawn from a few well-deﬁned syntactic classes. In this paper, we describe how the lexicalized discourse relation annotations of the Penn Discourse Treebank (PDTB) led to the discovery of a wide range of additional expressions, annotated as AltLex (alternative lexicalizations) in the PDTB 2.0. Further analysis of AltLex annotation suggests that the set of markers is openended, and drawn from a wider variety of syntactic types than currently assumed. As a ﬁrst attempt towards automatically identifying discourse relation markers, we propose the use of syntactic paraphrase methods. 
This paper shows that incorporating linguistically motivated features to ensure correct animacy and number agreement in an averaged perceptron ranking model for CCG realization helps improve a state-ofthe-art baseline even further. Traditionally, these features have been modelled using hard constraints in the grammar. However, given the graded nature of grammaticality judgements in the case of animacy we argue a case for the use of a statistical model to rank competing preferences. Though subject-verb agreement is generally viewed to be syntactic in nature, a perusal of relevant examples discussed in the theoretical linguistics literature (Kathol, 1999; Pollard and Sag, 1994) points toward the heterogeneous nature of English agreement. Compared to writing grammar rules, our method is more robust and allows incorporating information from diverse sources in realization. We also show that the perceptron model can reduce balanced punctuation errors that would otherwise require a post-ﬁlter. The full model yields signiﬁcant improvements in BLEU scores on Section 23 of the CCGbank and makes many fewer agreement errors. 
This paper looks at the web as a corpus and at the effects of using web counts to model language, particularly when we consider them as a domain-speciﬁc versus a general-purpose resource. We ﬁrst compare three vocabularies that were ranked according to frequencies drawn from general-purpose, specialised and web corpora. Then, we look at methods to combine heterogeneous corpora and evaluate the individual and combined counts in the automatic extraction of noun compounds from English general-purpose and specialised texts. Better n-gram counts can help improve the performance of empirical NLP systems that rely on n-gram language models. 
Previous research in cross-document entity coreference has generally been restricted to the ofﬂine scenario where the set of documents is provided in advance. As a consequence, the dominant approach is based on greedy agglomerative clustering techniques that utilize pairwise vector comparisons and thus require O(n2) space and time. In this paper we explore identifying coreferent entity mentions across documents in high-volume streaming text, including methods for utilizing orthographic and contextual information. We test our methods using several corpora to quantitatively measure both the efﬁcacy and scalability of our streaming approach. We show that our approach scales to at least an order of magnitude larger data than previous reported methods. 
We study correlation of rankings of text summarization systems using evaluation methods with and without human models. We apply our comparison framework to various well-established contentbased evaluation measures in text summarization such as coverage, Responsiveness, Pyramids and ROUGE studying their associations in various text summarization tasks including generic and focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. The research is carried out using a new content-based evaluation framework called FRESA to compute a variety of divergences among probability distributions. 
We present a library of implemented HPSG analyses for argument optionality based on typological studies of this phenomenon in the world’s languages, developed in the context of a grammar customization system that pairs a crosslinguistic core grammar with extensions for non-universal phenomena on the basis of user input of typological properties. Our analyses are compatible with multiple intersecting phenomena, including person, number, gender, tense, aspect and morphological rule formulation. We achieve 80-100% coverage on test suites from 10 natural languages. 
 statement can be speciﬁed as follows:  We present an adaptation technique for statistical machine translation, which applies the well-known Bayesian learning paradigm for adapting the model parameters. Since state-of-the-art statistical machine translation systems model the translation process as a log-linear combination of simpler models, we present the formal derivation of how to apply such paradigm to the weights of the log-linear combination. We show empirical results in which a small amount of adaptation data is able to improve both the non-adapted system and a system which optimises the abovementioned weights on the adaptation set only, while gaining both in reliability and speed. 
This paper presents a constraint-based graph partitioning approach to coreference resolution solved by relaxation labeling. The approach combines the strengths of groupwise classiﬁers and chain formation methods in one global method. Experiments show that our approach signiﬁcantly outperforms systems based on separate classiﬁcation and chain formation steps, and that it achieves the best results in the state of the art for the same dataset and metrics. 
Sentiment analysis systems can beneﬁt from the translation of sentiment information. We present a novel, graph-based approach using SimRank, a well-established graph-theoretic algorithm, to transfer sentiment information from a source language to a target language. We evaluate this method in comparison with semantic orientation using pointwise mutual information (SO-PMI), an established unsupervised method for learning the sentiment of phrases. 
This paper presents a survey of research in controlled natural languages that can be used as high-level knowledge representation languages. Over the past 10 years or so, a number of machine-oriented controlled natural languages have emerged that can be used as high-level interface languages to various kinds of knowledge systems. These languages are relevant to the area of computational linguistics since they have two very interesting properties: ﬁrstly, they look informal like natural languages and are therefore easier to write and understand by humans than formal languages; secondly, they are precisely deﬁned subsets of natural languages and can be translated automatically (and often deterministically) into a formal target language and then be used for automated reasoning. We present and compare the most mature of these novel languages, show how they can balance the disadvantages of natural languages and formal languages for knowledge representation, and discuss how domain specialists can be supported writing speciﬁcations in controlled natural language. 
Retrieval evaluation without relevance judgments is a hard but also very meaningful work. In this paper, we use clustering technique to improve the performance of judgment free retrieval evaluation. By using one system to represent all the systems that are similar to it, we can largely reduce the negative effect of similar retrieval results in Retrieval evaluation. Experimental results demonstrated that our method outperformed all the previous judgment free evaluation methods significantly. Its overall average performance outperformed the best previous result by 20.5%. Besides, our work is a general framework that can be applied to any other judgment free evaluation method for performance improvement. 
In this paper, we propose a method for mediatory summarization, which is a novel technique for facilitating users’ assessments of the credibility of information on the Web. A mediatory summary is generated by extracting a passage from Web documents; this summary is generated on the basis of its relevance to a given query, fairness, and density of keywords, which are features of the summaries constructed to determine the credibility of information on the Web. We demonstrate the eﬀectiveness of the generated mediatory summary in comparison with the summaries of Web documents produced by Web search engines. 
Document keywords are associated to documents as summarized versions of the documents’ content. Considering that the number of documents is quickly growing every day, the availability of these keywords is very important. Although, usually keywords are manually written. This motivated us to work on an approach to change this manual procedure for an automatic one. This paper presents a language independent approach that extracts the most relevant Multiword Expressions and single words from documents and propose them to describe the core content of each document. 
We study a novel shallow information extraction problem that involves extracting sentences of a given set of topic categories from medical forum data. Given a corpus of medical forum documents, our goal is to extract two related types of sentences that describe a biomedical case (i.e., medical problem descriptions and medical treatment descriptions). Such an extraction task directly generates medical case descriptions that can be useful in many applications. We solve the problem using two popular machine learning methods Support Vector Machines (SVM) and Conditional Random Fields (CRF). We propose novel features to improve the accuracy of extraction. Experiment results show that we can obtain an accuracy of up to 75%. 
This work presents a study to bridge topic modeling and personalized search. A probabilistic topic model is used to extract topics from user search history. These topics can be seen as a roughly summary of user preferences and further treated as feedback within the KL-Divergence retrieval model to estimate a more accurate query model. The topics more relevant to current query contribute more in updating the query model which helps to distinguish between relevant and irrelevant parts and ﬁlter out noise in user search history. We designed task oriented user study and the results show that: (1) The extracted topics can be used to cluster queries according to topics. (2) The proposed approach improves ranking quality consistently for queries matching user past interests and is robust for queries not matching past interests. 
In this paper we address methodological issues in the evaluation of a projectionbased framework for dependency parsing in which annotations for a source language are transfered to a target language using word alignments in a parallel corpus. The projected trees then constitute the training data for a data-driven parser in the target language. We discuss two problems that arise in the evaluation of such cross-lingual approaches. First, the annotation scheme underlying the source language annotations – and hence the projected target annotations and predictions of the parser derived from them – is likely to differ from previously existing gold standard test sets devised speciﬁcally for the target language. Second, the standard procedure of cross-validation cannot be performed in the absence of parallel gold standard annotations, so an alternative method has to be used to assess the generalization capabilities of the projected parsers. 
In this paper, we propose a novel dependency-based bracketing transduction grammar for statistical machine translation, which converts a source sentence into a target dependency tree. Different from conventional bracketing transduction grammar models, we encode target dependency information into our lexical rules directly, and then we employ two different maximum entropy models to determine the reordering and combination of partial dependency structures, when we merge two neighboring blocks. By incorporating dependency language model further, large-scale experiments on Chinese-English task show that our system achieves signiﬁcant improvements over the baseline system on various test sets even with fewer phrases. 
We present a simple algorithm for clustering semantic patterns based on distributional similarity and use cluster memberships to guide semi-supervised pattern discovery. We apply this approach to the task of relation extraction. The evaluation results demonstrate that our novel bootstrapping procedure significantly outperforms a standard bootstrapping. Most importantly, our algorithm can effectively prevent semantic drift and provide semi-supervised learning with a natural stopping criterion. 
In this paper, we describe a SVM classification framework of session detection task on both Chinese and English query logs. With eight features on the aspects of temporal and content information extracted from pairs of successive queries, the classification models achieve significantly superior performance than the statof-the-art method. Additionally, we find through ROC analysis that there exists great discrimination power variability among different features and within the same feature across different users. To fully utilize this variability, we build local models for individual users and combine their predictions with those from the global model. Experiments show that the local models do make significant improvements to the global model, although the amount is small. 
We present a theoretical and empirical comparative analysis of the two dominant categories of approaches in Chinese word segmentation: word-based models and character-based models. We show that, in spite of similar performance overall, the two models produce different distribution of segmentation errors, in a way that can be explained by theoretical properties of the two models. The analysis is further exploited to improve segmentation accuracy by integrating a word-based segmenter and a character-based segmenter. A Bootstrap Aggregating model is proposed. By letting multiple segmenters vote, our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff. 
We study the use of Conﬁdence Measures (CM) for erroneous constituent discrimination in an Interactive Predictive Parsing (IPP) framework. The IPP framework allows to build interactive tree annotation systems that can help human correctors in constructing error-free parse trees with little effort (compared to manually postediting the trees obtained from an automatic parser). We show that CMs can help in detecting erroneous constituents more quickly through all the IPP process. We present two methods for precalculating the conﬁdence threshold (globally and per-interaction), and observe that CMs remain highly discriminant as the IPP process advances. 
This paper presents a novel method for acquiring a set of query patterns to retrieve documents containing important information about an entity. Given an existing Wikipedia category that contains the target entity, we extract and select a small set of query patterns by presuming that formulating search queries with these patterns optimizes the overall precision and coverage of the returned Web information. We model this optimization problem as a weighted maximum satisﬁability (weighted Max-SAT) problem. The experimental results demonstrate that the proposed method outperforms other methods based on statistical measures such as frequency and point-wise mutual information (PMI), which are widely used in relation extraction. 
 This paper proposes a semi-supervised approach for WSD in Word-Class based selectional preferences. The approach exploits syntagmatic and paradigmatic semantic redundancy in the semantic system and uses association computation and minimum description length for the task of WSD. Experiments on Predicate-Object collocations and Subject-Predicate collocations with polysemous predicates in Chinese show that the proposed approach achieves a precision which is 8% higher than the semanticassociation based baseline. The semisupervised nature of the approach makes it promising for constructing large scale selectional preference knowledge base.  
Active Learning (AL) is a selective sampling strategy which has been shown to be particularly cost-efﬁcient by drastically reducing the amount of training data to be manually annotated. For the annotation of natural language data, cost efﬁciency is usually measured in terms of the number of tokens to be considered. This measure, assuming uniform costs for all tokens involved, is, from a linguistic perspective at least, intrinsically inadequate and should be replaced by a more adequate cost indicator, viz. the time it takes to manually label selected annotation examples. We here propose three different approaches to incorporate costs into the AL selection mechanism and evaluate them on the MUC7T corpus, an extension of the MUC7 newspaper corpus that contains such annotation time information. Our experiments reveal that using a costsensitive version of semi-supervised AL, up to 54% of true annotation time can be saved compared to random selection. 
We present a general methodology for extracting multi-word expressions (of various types), along with their translations, from small parallel corpora. We automatically align the parallel corpus and focus on misalignments; these typically indicate expressions in the source language that are translated to the target in a noncompositional way. We then use a large monolingual corpus to rank and ﬁlter the results. Evaluation of the quality of the extraction algorithm reveals signiﬁcant improvements over na¨ıve alignment-based methods. External evaluation shows an improvement in the performance of machine translation that uses the extracted dictionary. 
This paper proposes a novel topic model, Citation-Author-Topic (CAT) model that addresses a semantic search task we deﬁne as expert search – given a research area as a query, it returns names of experts in this area. For example, Michael Collins would be one of the top names retrieved given the query Syntactic Parsing. Our contribution in this paper is two-fold. First, we model the cited author information together with words and paper authors. Such extra contextual information directly models linkage among authors and enhances the author-topic association, thus produces more coherent author-topic distribution. Second, we provide a preliminary solution to the task of expert search when the learning repository contains exclusively research related documents authored by the experts. When compared with a previous proposed model (Johri et al., 2010), the proposed model produces high quality author topic linkage and achieves over 33% error reduction evaluated by the standard MAP measurement. 
We present a web-based algorithm for the task of POS tagging of unknown words (words appearing only a small number of times in the training data of a supervised POS tagger). When a sentence s containing an unknown word u is to be tagged by a trained POS tagger, our algorithm collects from the web contexts that are partially similar to the context of u in s, which are then used to compute new tag assignment probabilities for u. Our algorithm enables fast multi-domain unknown word tagging, since, unlike previous work, it does not require a corpus from the new domain. We integrate our algorithm into the MXPOST POS tagger (Ratnaparkhi, 1996) and experiment with three languages (English, German and Chinese) in seven in-domain and domain adaptation scenarios. Our algorithm provides an error reduction of up to 15.63% (English), 18.09% (German) and 13.57% (Chinese) over the original tagger. 
Hindi and Urdu share a common phonology, morphology and grammar but are written in different scripts. In addition, the vocabularies have also diverged significantly especially in the written form. In this paper we show that we can get reasonable quality translations (we estimated the Translation Error rate at 18%) between the two languages even in absence of a parallel corpus. Linguistic resources such as treebanks, part of speech tagged data and parallel corpora with English are limited for both these languages. We use the translation system to share linguistic resources between the two languages. We demonstrate improvements on three tasks and show: statistical machine translation from Urdu to English is improved (0.8 in BLEU score) by using a Hindi-English parallel corpus, Hindi part of speech tagging is improved (upto 6% absolute) by using an Urdu part of speech corpus and a Hindi-English word aligner is improved by using a manually word aligned UrduEnglish corpus (upto 9% absolute in FMeasure). 
In this paper we present a novel phrase structure parsing approach with the help of dependency structure. Different with existing phrase parsers, in our approach the inference procedure is guided by dependency structure, which makes the parsing procedure flexibly. The experimental results show our approach is much more accurate. With the help of golden dependency trees, F1 score of our parser achieves 96.08% on Penn English Treebank and 90.61% on Penn Chinese Treebank. With the help of N-best dependency trees generated by modified MSTParser, F1 score achieves 90.54% for English and 83.93% for Chinese. 
The overwhelming amounts of multimedia contents have triggered the need for automatically detecting the semantic concepts within the media contents. With the development of photo sharing websites such as Flickr, we are able to obtain millions of images with usersupplied tags. However, user tags tend to be noisy, ambiguous and incomplete. In order to improve the quality of tags to annotate web images, we propose an approach to build Semantic Fields for annotating the web images. The main idea is that the images are more likely to be relevant to a given concept, if several tags to the image belong to the same Semantic Field as the target concept. Semantic Fields are determined by a set of highly semantically associated terms with high tag co-occurrences in the image corpus and in different corpora and lexica such as WordNet and Wikipedia. We conduct experiments on the NUSWIDE web image corpus and demonstrate superior performance on image annotation as compared to the state-ofthe-art approaches. 
In this paper, we present an investigation into the use of cue phrases as a basis for dialogue act classiﬁcation. We deﬁne what we mean by cue phrases, and describe how we extract them from a manually labelled corpus of dialogue. We describe one method of evaluating the usefulness of such cue phrases, by applying them directly as a classiﬁer to unseen utterances. Once we have extracted cue phrases from one corpus, we determine if these phrases are general in nature, by applying them directly as a classiﬁcation mechanism to a different corpus to that from which they were extracted. Finally, we experiment with increasingly restrictive methods for selecting cue phrases, and demonstrate that there are a small number of core cue phrases that are useful for dialogue act classiﬁcation. 
Search with synonyms is a challenging problem for Web search, as it can easily cause intent drifting. In this paper, we propose a practical solution to this issue, based on co-clicked query analysis, i.e., analyzing queries leading to clicking the same documents. Evaluation results on Web search queries show that synonyms obtained from this approach considerably outperform the thesaurus based synonyms, such as WordNet, in terms of keeping search intent. 
Recent years have witnessed a large body of research works on cross-domain sentiment classification problem, where most of the research endeavors were based on a supervised learning strategy which builds models from only the labeled documents or only the labeled sentiment words. Unfortunately, such kind of supervised learning method usually fails to uncover the full knowledge between documents and sentiment words. Taking account of this limitation, in this paper, we propose an iterative reinforcement learning approach for cross-domain sentiment classification by simultaneously utilizing documents and words from both source domain and target domain. Our new method can make full use of the reinforcement between documents and words by fusing four kinds of relationships between documents and words. Experimental results indicate that our new method can improve the performance of cross-domain sentiment classification dramatically. 
Word relation features, which encode relation information between words, are supposed to be effective features for sentiment classification. However, the use of word relation features suffers from two issues. One is the sparse-data problem and the lack of generalization performance; the other is the limitation of using word relations as additional features to unigrams. To address the two issues, we propose a generalized word relation feature extraction method and an ensemble model to efficiently integrate unigrams and different type of word relation features. Furthermore, aimed at reducing the computation complexity, we propose two fast feature selection methods that are specially designed for word relation features. A range of experiments are conducted to evaluate the effectiveness and efficiency of our approaches. 
Translation rule extraction is an important issue in syntax-based Statistical Machine Translation (SMT). Recent studies show that rule coverage is one of the key factors affecting the success of syntaxbased systems. In this paper, we first present a simple and effective method to improve rule coverage by using multiple parsers in translation rule extraction, and then empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 
This paper presents a new approach to improving relation extraction based on minimally supervised learning. By adding some limited closed-world knowledge for conﬁdence estimation of learned rules to the usual seed data, the precision of relation extraction can be considerably improved. Starting from an existing baseline system we demonstrate that utilizing limited closed world knowledge can effectively eliminate ”dangerous” or plainly wrong rules during the bootstrapping process. The new method improves the reliability of the conﬁdence estimation and the precision value of the extracted instances. Although recall suffers to a certain degree depending on the domain and the selected settings, the overall performance measured by F-score considerably improves. Finally we validate the adaptability of the best ranking method to a new domain and obtain promising results. 
We describe a Chinese temporal annotation experiment that produced a sizable data set for the TempEval-2 evaluation campaign. We show that while we have achieved high inter-annotator agreement for simpler tasks such as identiﬁcation of events and time expressions, temporal relation annotation proves to be much more challenging. We show that in order to improve the inter-annotator agreement it is important to strategically select the annotation targets, and the selection of annotation targets should be subject to syntactic, semantic and discourse constraints. 
In this work, we model the writing revision process of English as a Second Language (ESL) students with syntaxdriven machine translation methods. We compare two approaches: tree-tostring transformations (Yamada and Knight, 2001) and tree-to-tree transformations (Smith and Eisner, 2006). Results suggest that while the tree-totree model provides a greater coverage, the tree-to-string approach oﬀers a more plausible model of ESL learners’ revision writing process. 
Empty categories represent an important source of information in syntactic parses annotated in the generative linguistic tradition, but empty category recovery has only started to receive serious attention until very recently, after substantial progress in statistical parsing. This paper describes a uniﬁed framework in recovering empty categories in the Chinese Treebank. Our results show that given skeletal gold standard parses, the empty categories can be detected with very high accuracy. We report very promising results for empty category recovery for automatic parses as well. 
We show that unsupervised part of speech tagging performance can be signiﬁcantly improved using likely substitutes for target words given by a statistical language model. We choose unambiguous substitutes for each occurrence of an ambiguous target word based on its context. The part of speech tags for the unambiguous substitutes are then used to ﬁlter the entry for the target word in the word–tag dictionary. A standard HMM model trained using the ﬁltered dictionary achieves 92.25% accuracy on a standard 24,000 word corpus. 
In this paper, we investigate the problem of entity identiﬁcation and relation extraction from encyclopedia articles, and we propose a joint discriminative probabilistic model with arbitrary graphical structure to optimize all relevant subtasks simultaneously. This modeling offers a natural formalism for exploiting rich dependencies and interactions between relevant subtasks to capture mutual beneﬁts, as well as a great ﬂexibility to incorporate a large collection of arbitrary, overlapping and nonindependent features. We show the parameter estimation algorithm of this model. Moreover, we propose a new inference method, namely collective iterative classiﬁcation (CIC), to ﬁnd the most likely assignments for both entities and relations. We evaluate our model on real-world data from Wikipedia for this task, and compare with current state-of-the-art pipeline and joint models, demonstrating the effectiveness and feasibility of our approach. 
We present the ﬁrst known empirical results on sequence labeling based on maximum margin Markov networks (M 3N ), which incorporate both kernel methods to efﬁciently deal with high-dimensional feature spaces, and probabilistic graphical models to capture correlations in structured data. We provide an efﬁcient algorithm, the stochastic gradient descent (SGD), to speedup the training procedure of M 3N . Using ofﬁcial dataset for noun phrase (NP) chunking as a case study, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than the structured sequential minimal optimization (structured SMO). Our model compares favorably with current state-of-the-art sequence labeling approaches. More importantly, our model can be easily applied to other sequence labeling tasks. 
In this paper, we introduce our recent work on Chinese HPSG grammar development through treebank conversion. By manually defining grammatical constraints and annotation rules, we convert the bracketing trees in the Penn Chinese Treebank (CTB) to be an HPSG treebank. Then, a large-scale lexicon is automatically extracted from the HPSG treebank. Experimental results on the CTB 6.0 show that a HPSG lexicon was successfully extracted with 97.24% accuracy; furthermore, the obtained lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English. 
This paper is a case study on cross-lingual induction of lexical resources for deep, broad-coverage syntactic analysis of German. We use a parallel corpus to induce a classiﬁer for German participles which can predict their syntactic category. By means of this classiﬁer, we induce a resource of adverbial participles from a huge monolingual corpus of German. We integrate the resource into a German LFG grammar and show that it improves parsing coverage while maintaining accuracy. 
This paper focuses on the Web-based English-Chinese OOV term translation pattern, and emphasizes particularly on the translation selection strategy based on the fusion of multiple features and the ranking mechanism based on Ranking Support Vector Machine (Ranking SVM). By utilizing the CoNLL2003 corpus for the English Named Entity Recognition (NER) task and selected new terms, the experiments based on different data sources show the consistent results. Our OOV term translation model can “filter” the most possible translation candidates with better ability. From the experimental results for combining our OOV term translation model with English-Chinese CrossLanguage Information Retrieval (CLIR) on the data sets of Text Retrieval Evaluation Conference (TREC), it can be found that the obvious performance improvement for both query translation and retrieval can also be obtained. 
This paper presents two pivot strategies for statistical machine transliteration, namely system-based pivot strategy and model-based pivot strategy. Given two independent source-pivot and pivot-target name pair corpora, the model-based strategy learns a direct sourcetarget transliteration model while the system-based strategy learns a sourcepivot model and a pivot-target model, respectively. Experimental results on benchmark data show that the systembased pivot strategy is effective in reducing the high resource requirement of training corpus for low-density language pairs while the model-based pivot strategy performs worse than the system-based one. 
Treebank annotation is a labor-intensive and time-consuming task. In this paper, we show that a simple statistical ranking model can signiﬁcantly improve treebanking efﬁciency by prompting human annotators, well-trained in disambiguation tasks for treebanking but not necessarily grammar experts, to the most relevant linguistic disambiguation decisions. Experiments were carried out to evaluate the impact of such techniques on annotation efﬁciency and quality. The detailed analysis of outputs from the ranking model shows strong correlation to the human annotator behavior. When integrated into the treebanking environment, the model brings a signiﬁcant annotation speed-up with improved inter-annotator agreement.† 
An important task of opinion mining is to extract people’s opinions on features of an entity. For example, the sentence, “I love the GPS function of Motorola Droid” expresses a positive opinion on the “GPS function” of the Motorola phone. “GPS function” is the feature. This paper focuses on mining features. Double propagation is a state-of-the-art technique for solving the problem. It works well for medium-size corpora. However, for large and small corpora, it can result in low precision and low recall. To deal with these two problems, two improvements based on part-whole and “no” patterns are introduced to increase the recall. Then feature ranking is applied to the extracted feature candidates to improve the precision of the top-ranked candidates. We rank feature candidates by feature importance which is determined by two factors: feature relevance and feature frequency. The problem is formulated as a bipartite graph and the well-known web page ranking algorithm HITS is used to find important features and rank them high. Experiments on diverse real-life datasets show promising results.  
Given the increasing need to process massive amounts of textual data, efﬁciency of NLP tools is becoming a pressing concern. Parsers based on lexicalised grammar formalisms, such as TAG and CCG, can be made more efﬁcient using supertagging, which for CCG is so effective that every derivation consistent with the supertagger output can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. 
Metaphorical and contextual affect detection from open-ended text-based dialogue is challenging but essential for the building of effective intelligent user interfaces. In this paper, we report updated developments of an affect detection model from text, including affect detection from one particular type of metaphorical affective expression and affect detection based on context. The overall affect detection model has been embedded in an intelligent conversational AI agent interacting with human users under loose scenarios. Evaluation for the updated affect detection component is also provided. Our work contributes to the conference themes on sentiment analysis and opinion mining and the development of dialogue and conversational agents. 
We propose an event-enriched model to alleviate the semantic deficiency problem in the IR-style text processing and apply it to sentence ordering for multi-document news summarization. The ordering algorithm is built on event and entity coherence, both locally and globally. To accommodate the eventenriched model, a novel LSA-integrated two-layered clustering approach is adopted. The experimental result shows clear advantage of our model over event-agonistic models. 
Temporal expressions in texts contain significant temporal information. Understanding temporal information is very useful in many NLP applications, such as information extraction, documents summarization and question answering. Therefore, the temporal expression normalization which is used for transforming temporal expressions to temporal information has absorbed many researchers’ attentions. But previous works, whatever the hand-crafted rules-based or the machine-learnt rules-based, all can not address the actual problem about temporal reference in real texts effectively. More specifically, the reference time choosing mechanism employed by these works is not adaptable to the universal implicit times in normalization. Aiming at this issue, we introduce a new reference time choosing mechanism for temporal expression normalization, called reference time dynamic-choosing, which assigns the appropriate reference times to different classes of implicit temporal expressions dynamically when normalizing. And then, the solution to temporal expression defuzzification by scenario dependences among temporal expressions is discussed. Finally, we evaluate the system on a substantial corpus collected by Chinese news articles and obtained more promising results than compared methods.  
Existing works indicate that the absence of explicit discourse connectives makes it difﬁcult to recognize implicit discourse relations. In this paper we attempt to overcome this difﬁculty for implicit relation recognition by automatically inserting discourse connectives between arguments with the use of a language model. Then we propose two algorithms to leverage the information of these predicted connectives. One is to use these predicted implicit connectives as additional features in a supervised model. The other is to perform implicit relation recognition based only on these predicted connectives. Results on Penn Discourse Treebank 2.0 show that predicted discourse connectives help implicit relation recognition and the ﬁrst algorithm can achieve an absolute average f-score improvement of 3% over a state of the art baseline system. 
This paper presents a novel semisupervised learning algorithm called Active Deep Networks (ADN), to address the semi-supervised sentiment classification problem with active learning. First, we propose the semi-supervised learning method of ADN. ADN is constructed by Restricted Boltzmann Machines (RBM) with unsupervised learning using labeled data and abundant of unlabeled data. Then the constructed structure is finetuned by gradient-descent based supervised learning with an exponential loss function. Second, we apply active learning in the semi-supervised learning framework to identify reviews that should be labeled as training data. Then ADN architecture is trained by the selected labeled data and all unlabeled data. Experiments on five sentiment classification datasets show that ADN outperforms the semi-supervised learning algorithm and deep learning techniques applied for sentiment classification. 
The field of information retrieval still strives to develop models which allow semantic information to be integrated in the ranking process to improve performance in comparison to standard bag-ofwords based models. A conceptual model has been adopted in generalpurpose retrieval which can comprise a range of concepts, including linguistic terms, latent concepts and explicit knowledge concepts. One of the drawbacks of this model is that the computational cost is significant and often intractable in modern test collections. Therefore, approaches utilising conceptbased models for re-ranking initial retrieval results have attracted a considerable amount of study. This method enjoys the benefits of reduced document corpora for semantic space construction and improved ranking results. However, fitting such a model to a smaller collection is less meaningful than fitting it into the whole corpus. This paper proposes a dual-space model which incorporates external knowledge to enhance the space produced by the latent concept method. This model is intended to produce global consistency across the semantic space: similar entries are likely to have the same re-ranking scores with respect to the latent and manifest concepts. To illustrate the effectiveness of the proposed method, experiments were conducted using test collections across different languages. The results demon-  strate that the method can comfortably achieve improvements in retrieval performance. 
In this paper, we focus on the challenge of automatically converting a constituency treebank (source treebank) to ﬁt the standard of another constituency treebank (target treebank). We formalize the conversion problem as an informed decoding procedure: information from original annotations in a source treebank is incorporated into the decoding phase of a parser trained on a target treebank during the parser assigning parse trees to sentences in the source treebank. Experiments on two Chinese treebanks show signiﬁcant improvements in conversion accuracy over baseline systems, especially when training data used for building the parser is small in size. 
This paper studies the problem of imposing a known hierarchical structure onto an unstructured spoken document, aiming to help browse such archives. We formulate our solutions within a dynamic-programming-based alignment framework and use minimum errorrate training to combine a number of global and hierarchical constraints. This pragmatic approach is computationally efﬁcient. Results show that it outperforms a baseline that ignores the hierarchical and global features and the improvement is consistent on transcripts with different WERs. Directly imposing such hierarchical structures onto raw speech without using transcripts yields competitive results. 
 We present a probabilistic, salience-based approach to the interpretation of pointing gestures together with spoken utterances. Our mechanism models dependencies between spatial and temporal aspects of gestures and features of utterances. For our evaluation, we collected a corpus of requests which optionally included pointing. Our results show that pointing information improves interpretation accuracy. 
The Right Frontier Constraint (RFC), as a constraint on the attachment of new constituents to an existing discourse structure, has important implications for the interpretation of anaphoric elements in discourse and for Machine Learning (ML) approaches to learning discourse structures. In this paper we provide strong empirical support for SDRT’s version of RFC. The analysis of about 100 doubly annotated documents by ﬁve different naive annotators shows that SDRT’s RFC is respected about 95% of the time. The qualitative analysis of presumed violations that we have performed shows that they are either click-errors or structural misconceptions. 
Multi-word expressions constitute a signiﬁcant portion of the lexicon of every natural language, and handling them correctly is mandatory for various NLP applications. Yet such entities are notoriously hard to deﬁne, and are consequently missing from standard lexicons and dictionaries. Multi-word expressions exhibit idiosyncratic behavior on various levels: orthographic, morphological, syntactic and semantic. In this work we take advantage of the morphological and syntactic idiosyncrasy of Hebrew noun compounds and employ it to extract such expressions from text corpora. We show that relying on linguistic information dramatically improves the accuracy of compound extraction, reducing over one third of the errors compared with the best baseline. 
In cross-language information retrieval it is often important to align words that are similar in meaning in two corpora written in different languages. Previous research shows that using context similarity to align words is helpful when no dictionary entry is available. We suggest a new method which selects a subset of words (pivot words) associated with a query and then matches these words across languages. To detect word associations, we demonstrate that a new Bayesian method for estimating Point-wise Mutual Information provides improved accuracy. In the second step, matching is done in a novel way that calculates the chance of an accidental overlap of pivot words using the hypergeometric distribution. We implemented a wide variety of previously suggested methods. Testing in two conditions, a small comparable corpora pair and a large but unrelated corpora pair, both written in disparate languages, we show that our approach consistently outperforms the other systems. 
While subjectivity related research in other languages has increased, most of the work focuses on single languages. This paper explores the integration of features originating from multiple languages into a machine learning approach to subjectivity analysis, and aims to show that this enriched feature set provides for more effective modeling for the source as well as the target languages. We show not only that we are able to achieve over 75% macro accuracy in all of the six languages we experiment with, but also that by using features drawn from multiple languages we can construct high-precision meta-classiﬁers with a precision of over 83%. 
Plagiarism, the unacknowledged reuse of text, does not end at language boundaries. Cross-language plagiarism occurs if a text is translated from a fragment written in a different language and no proper citation is provided. Regardless of the change of language, the contents and, in particular, the ideas remain the same. Whereas different methods for the detection of monolingual plagiarism have been developed, less attention has been paid to the crosslanguage case. In this paper we compare two recently proposed cross-language plagiarism detection methods (CL-CNG, based on character n-grams and CL-ASA, based on statistical translation), to a novel approach to this problem, based on machine translation and monolingual similarity analysis (T+MA). We explore the effectiveness of the three approaches for less related languages. CL-CNG shows not be appropriate for this kind of language pairs, whereas T+MA performs better than the previously proposed models. 
In this work we present the results of experimental work on the development of lexical class-based lexica by automatic means. Our purpose is to assess the use of linguistic lexical-class based information as a feature selection methodology for the use of classifiers in quick lexical development. The results show that the approach can help reduce the human effort required in the development of language resources significantly. 
We propose a series of learned arc ﬁlters to speed up graph-based dependency parsing. A cascade of ﬁlters identify implausible head-modiﬁer pairs, with time complexity that is ﬁrst linear, and then quadratic in the length of the sentence. The linear ﬁlters reliably predict, in context, words that are roots or leaves of dependency trees, and words that are likely to have heads on their left or right. We use this information to quickly prune arcs from the dependency graph. More than 78% of total arcs are pruned while retaining 99.5% of the true dependencies. These ﬁlters improve the speed of two state-ofthe-art dependency parsers, with low overhead and negligible loss in accuracy. 
This paper considers the problem of document-level multi-way sentiment detection, proposing a hierarchical classiﬁer algorithm that accounts for the inter-class similarity of tagged sentiment-bearing texts. This type of classiﬁer also provides a natural mechanism for reducing the feature space of the problem. Our results show that this approach improves on state-of-the-art predictive performance for movie reviews with three-star and fourstar ratings, while simultaneously reducing training times and memory requirements. 
A novel and robust approach to improving statistical machine translation ﬂuency is developed within a minimum Bayesrisk decoding framework. By segmenting translation lattices according to conﬁdence measures over the maximum likelihood translation hypothesis we are able to focus on regions with potential translation errors. Hypothesis space constraints based on monolingual coverage are applied to the low conﬁdence regions to improve overall translation ﬂuency. 
A great deal of information on the Web is represented in both textual and structured form. The structured form is machinereadable and can be used to augment the textual data. We call this augmentation – the annotation of texts with relations that are included in the structured data – self-annotation. In this paper, we introduce self-annotation as a new supervised learning approach for developing and implementing a system that extracts ﬁnegrained relations between entities. The main beneﬁt of self-annotation is that it does not require manual labeling. The input of the learned model is a representation of the free text, its output structured relations. Thus, the model, once learned, can be applied to any arbitrary free text. We describe the challenges for the self-annotation process and give results for a sample relation extraction system. To deal with the challenge of ﬁnegrained relations, we implement and evaluate both shallow and deep linguistic analysis, focusing on German. 
In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, parsing and training times are still relatively long. To determine why, we analyzed the time usage of a dependency parser. We illustrate that the mapping of the features onto their weights in the support vector machine is the major factor in time complexity. To resolve this problem, we implemented the passive-aggressive perceptron algorithm as a Hash Kernel. The Hash Kernel substantially improves the parsing times and takes into account the features of negative examples built during the training. This has lead to a higher accuracy. We could further increase the parsing and training speed with a parallel feature extraction and a parallel parsing algorithm. We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers, phrase structrue parsers, and machine translation. 
Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-ﬂedged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, linearization and morphologization. Multilevel annotated corpora are increasingly available for multiple languages. We take advantage of them and propose a multilingual deep stochastic sentence realizer that mirrors the state-ofthe-art research in semantic parsing. The realizer uses an SVM learning algorithm. For each pair of adjacent levels of annotation, a separate decoder is deﬁned. So far, we evaluated the realizer for Chinese, English, German, and Spanish. 
We generalize the task of ﬁnding question paraphrases in a question repository to a novel formulation in which known questions are ranked based on their utility to a new, reference question. We manually annotate a dataset of 60 groups of questions with a partial order relation reﬂecting the relative utility of questions inside each group, and use it to evaluate meaning and structure aware utility functions. Experimental evaluation demonstrates the importance of using structural information in estimating the relative usefulness of questions, holding the promise of increased usability for social QA sites. 
Multi-document summarization aims to produce a concise summary that contains salient information from a set of source documents. In this field, sentence ranking has hitherto been the issue of most concern. Since documents often cover a number of topic themes with each theme represented by a cluster of highly related sentences, sentence clustering was recently explored in the literature in order to provide more informative summaries. Existing clusterbased ranking approaches applied clustering and ranking in isolation. As a result, the ranking performance will be inevitably influenced by the clustering result. In this paper, we propose a reinforcement approach that tightly integrates ranking and clustering by mutually and simultaneously updating each other so that the performance of both can be improved. Experimental results on the DUC datasets demonstrate its effectiveness and robustness. 
We describe a novel approach to coreference resolution which implements a global decision via hypergraph partitioning. In constrast to almost all previous approaches, we do not rely on separate classiﬁcation and clustering steps, but perform coreference resolution globally in one step. Our hypergraph-based global model implemented within an endto-end coreference resolution system outperforms two strong baselines (Soon et al., 2001; Bengtson & Roth, 2008) using system mentions only. 
Relation extraction is the task of recognizing semantic relations among entities. Given a particular sentence supervised approaches to Relation Extraction employed feature or kernel functions which usually have a single sentence in their scope. The overall aim of this paper is to propose methods for using knowledge and resources that are external to the target sentence, as a way to improve relation extraction. We demonstrate this by exploiting background knowledge such as relationships among the target relations, as well as by considering how target relations relate to some existing knowledge resources. Our methods are general and we suggest that some of them could be applied to other NLP tasks. 
 Semantic role labeling (SRL) and word sense disambiguation (WSD) are two fundamental tasks in natural language processing to ﬁnd a sentence-level semantic representation. To date, they have mostly been modeled in isolation. However, this approach neglects logical constraints between them. We therefore exploit some pipeline systems which verify the automatic all word sense disambiguation could help the semantic role labeling and vice versa. We further propose a Markov logic model that jointly labels semantic roles and disambiguates all word senses. By evaluating our model on the OntoNotes 3.0 data, we show that this joint approach leads to a higher performance for word sense disambiguation and semantic role labeling than those pipeline approaches. 
In this paper, we propose an unsupervised approach for identifying bipolar person names in a set of topic documents. We employ principal component analysis (PCA) to discover bipolar word usage patterns of person names in the documents and show that the signs of the entries in the principal eigenvector of PCA partition the person names into bipolar groups spontaneously. Empirical evaluations demonstrate the efficacy of the proposed approach in identifying bipolar person names of topics. 
This paper proposes a multi-label approach to detect emotion causes. The multi-label model not only detects multi-clause causes, but also captures the long-distance information to facilitate emotion cause detection. In addition, based on the linguistic analysis, we create two sets of linguistic patterns during feature extraction. Both manually generalized patterns and automatically generalized patterns are designed to extract general cause expressions or specific constructions for emotion causes. Experiments show that our system achieves a performance much higher than a baseline model. 
Event Anaphora Resolution is an important task for cascaded event template extraction and other NLP study. In this paper, we provide a first systematic study of resolving pronouns to their event verb antecedents for general purpose. First, we explore various positional, lexical and syntactic features useful for the event pronoun resolution. We further explore tree kernel to model structural information embedded in syntactic parses. A composite kernel is then used to combine the above diverse information. In addition, we employed a twin-candidate based preferences learning model to capture the pair wise candidates’ preference knowledge. Besides we also look into the incorporation of the negative training instances with anaphoric pronouns whose antecedents are not verbs. Although these negative training instances are not used in previous study on anaphora resolution, our study shows that they are very useful for the final resolution through random sampling strategy. Our experiments demonstrate that it’s meaningful to keep certain training data as development data to help SVM select a more accurate hyper plane which provides significant improvement over the default setting with all training data. 
In this paper, we propose an unsupervised approach to automatically synthesize Wikipedia articles in multiple languages. Taking an existing high-quality version of any entry as content guideline, we extract keywords from it and use the translated keywords to query the monolingual web of the target language. Candidate excerpts or sentences are selected based on an iterative ranking function and eventually synthesized into a complete article that resembles the reference version closely. 16 English and Chinese articles across 5 domains are evaluated to show that our algorithm is domainindependent. Both subjective evaluations by native Chinese readers and ROUGE-L scores computed with respect to standard reference articles demonstrate that synthesized articles outperform existing Chinese versions or MT texts in both content richness and readability. In practice our method can generate prototype texts for Wikipedia that facilitate later human authoring. 
It has been known that a combination of multiple kernels and addition of various resources are the best options for improving effectiveness of kernel-based PPI extraction methods. These supplements, however, involve extensive kernel adaptation and feature selection processes, which attenuate the original benefits of the kernel methods. This paper shows that we are able to achieve the best performance among the stateof-the-art methods by using only a single kernel, convolution parse tree kernel. In-depth analyses of the kernel reveal that the keys to the improvement are the tree pruning method and consideration of tree kernel decay factors. It is noteworthy that we obtained the performance without having to use any additional features, kernels or corpora. 
Text mining for global health surveillance is an emerging technology that is gaining increased attention from public health organisations and governments. The lack of multilingual resources such as WordNets speciﬁcally targeted at this task have so far been a major bottleneck. This paper reports on a major upgrade to the BioCaster Web monitoring system and its freely available multilingual ontology; improving its original design and extending its coverage of diseases from 70 to 336 in 12 languages.  mortality and socio-economic disruption (Cox et al., 2003). Furthermore, outbreaks of livestock diseases, such as foot-and-mouth disease or equine inﬂuenza can have a devastating impact on industry, commerce and human health (Blake et al., 2003). The challenge is to enhance vigilance and control the emergence of outbreaks. Whilst human analysis remains essential to spot complex relationships, automated analysis has a key role to play in ﬁltering the vast volume of data in real time and highlighting unusual trends using reliable predictor indicators.  
This paper addresses two problems that commonly arise in parsing with precisionoriented, rule-based models of grammar: lack of speed and lack of robustness. First, we show how we can reduce parsing times by restricting the number of tasks the parser will carry out, based on a generative model of rule applications. Second, we show that a combination of search space restriction and radically overgenerating robustness rules lead to a more robust parser, with only a small penalty in precision. Applying both the robustness rules and a fragment fallback strategy showed better recall than just giving fragment analyses, with equal precision. Results are reported on a medium-sized HPSG grammar for German. 1 
We present a framework where auxiliary MT systems are used to provide lexical predictions to a main SMT system. In this work, predictions are obtained by means of pivoting via auxiliary languages, and introduced into the main SMT system in the form of a low order language model, which is estimated on a sentenceby-sentence basis. The linear combination of models implemented by the decoder is thus extended with this additional language model. Experiments are carried out over three different translation tasks using the European Parliament corpus. For each task, nine additional languages are used as auxiliary languages to obtain the triangulated predictions. Translation accuracy results show that improvements in translation quality are obtained, even for large data conditions. 
We present a method for translating semantic relationships between languages where relationships are deﬁned as pattern clusters. Given a pattern set which represents a semantic relationship, we use the web to extract sample term pairs of this relationship. We automatically translate the obtained term pairs using multilingual dictionaries and disambiguate the translated pairs using web counts. Finally we discover the set of most relevant target language patterns for the given relationship. The obtained pattern set can be utilized for extraction of new relationship examples for the target language. We evaluate our method on 11 diverse target languages. To assess the quality of the discovered relationships, we use an automatically generated cross-lingual SAT analogy test, WordNet relationships, and concept-speciﬁc relationships, achieving high precision. The proposed framework allows fully automated cross-lingual relationship mining and construction of multilingual pattern dictionaries without relying on parallel corpora. 
This paper investigates the impact of using different temporal algebras for learning temporal relations between events. Speciﬁcally, we compare three intervalbased algebras: Allen (1983) algebra, Bruce (1972) algebra, and the algebra derived from the TempEval-07 campaign. These algebras encode different granularities of relations and have different inferential properties. They in turn behave differently when used to enforce global consistency constraints on the building of a temporal representation. Through various experiments on the TimeBank/AQUAINT corpus, we show that although the TempEval relation set leads to the best classiﬁcation accuracy performance, it is too vague to be used for enforcing consistency. By contrast, the other two relation sets are similarly harder to learn, but more useful when global consistency is important. Overall, the Bruce algebra is shown to give the best compromise between learnability and expressive power. 
To speed up the process of categorizing learner errors and obtaining data for languages which lack error-annotated data, we describe a linguistically-informed method for generating learner-like morphological errors, focusing on Russian. We outline a procedure to select likely errors, relying on guiding stem and sufﬁx combinations from a segmented lexicon to match particular error categories and relying on grammatical information from the original context. 
Coreference resolution is a classic NLP problem and has been studied extensively by many researchers. Most existing studies, however, are generic in the sense that they are not focused on any specific text. In the past few years, opinion mining became a popular topic of research because of a wide range of applications. However, limited work has been done on coreference resolution in opinionated text. In this paper, we deal with object and attribute coreference resolution. Such coreference resolutions are important because without solving it a great deal of opinion information will be lost, and opinions may be assigned to wrong entities. We show that some important features related to opinions can be exploited to perform the task more accurately. Experimental results using blog posts demonstrate the effectiveness of the technique. 
The integration of facts derived from information extraction systems into existing knowledge bases requires a system to disambiguate entity mentions in the text. This is challenging due to issues such as non-uniform variations in entity names, mention ambiguity, and entities absent from a knowledge base. We present a state of the art system for entity disambiguation that not only addresses these challenges but also scales to knowledge bases with several million entries using very little resources. Further, our approach achieves performance of up to 95% on entities mentioned from newswire and 80% on a public test set that was designed to include challenging queries. 
Syntactic reordering on the source-side is an effective way of handling word order differences. The (DE) construction is a ﬂexible and ubiquitous syntactic structure in Chinese which is a major source of error in translation quality. In this paper, we propose a new classiﬁer model — discriminative latent variable model (DPLVM) — to classify the DE construction to improve the accuracy of the classiﬁcation and hence the translation quality. We also propose a new feature which can automatically learn the reordering rules to a certain extent. The experimental results show that the MT systems using the data reordered by our proposed model outperform the baseline systems by 6.42% and 3.08% relative points in terms of the BLEU score on PB-SMT and hierarchical phrase-based MT respectively. In addition, we analyse the impact of DE annotation on word alignment and on the SMT phrase table. 
Twitter, as one of the most popular micro-blogging services, provides large quantities of fresh information including real-time news, comments, conversation, pointless babble and advertisements. Twitter presents tweets in chronological order. Recently, Twitter introduced a new ranking strategy that considers popularity of tweets in terms of number of retweets. This ranking method, however, has not taken into account content relevance or the twitter account. Therefore a large amount of pointless tweets inevitably flood the relevant tweets. This paper proposes a new ranking strategy which uses not only the content relevance of a tweet, but also the account authority and tweet-specific features such as whether a URL link is included in the tweet. We employ learning to rank algorithms to determine the best set of features with a series of experiments. It is demonstrated that whether a tweet contains URL or not, length of tweet and account authority are the best conjunction.1 
Previous methods on improving translation quality by employing multiple SMT models usually carry out as a secondpass decision procedure on hypotheses from multiple systems using extra features instead of using features in existing models in more depth. In this paper, we propose translation model generalization (TMG), an approach that updates probability feature values for the translation model being used based on the model itself and a set of auxiliary models, aiming to enhance translation quality in the firstpass decoding. We validate our approach on translation models based on auxiliary models built by two different ways. We also introduce novel probability variance features into the log-linear models for further improvements. We conclude that our approach can be developed independently and integrated into current SMT pipeline directly. We demonstrate BLEU improvements on the NIST Chinese-toEnglish MT tasks for single-system decodings, a system combination approach and a model combination approach.1 
We present Mixture Model-based Minimum Bayes Risk (MMMBR) decoding, an approach that makes use of multiple SMT systems to improve translation accuracy. Unlike existing MBR decoding methods defined on the basis of single SMT systems, an MMMBR decoder reranks translation outputs in the combined search space of multiple systems using the MBR decision rule and a mixture distribution of component SMT models for translation hypotheses. MMMBR decoding is a general method that is independent of specific SMT models and can be applied to various commonly used search spaces. Experimental results on the NIST Chinese-to-English MT evaluation tasks show that our approach brings significant improvements to single system-based MBR decoding and outperforms a stateof-the-art system combination method. 1 
We consider the task of summarizing a cluster of related sentences with a short sentence which we call multi-sentence compression and present a simple approach based on shortest paths in word graphs. The advantage and the novelty of the proposed method is that it is syntaxlean and requires little more than a tokenizer and a tagger. Despite its simplicity, it is capable of generating grammatical and informative summaries as our experiments with English and Spanish data demonstrate. 
We propose a simple but effective method for enriching dictionary deﬁnitions with images based on image searches. Various query expansion methods using synonyms/hypernyms (or related words) are evaluated. We demonstrate that our method is effective in obtaining highprecision images that complement dictionary entries, even for words with abstract or multiple meanings. 
We present a novel graph-based summarization framework (Opinosis) that generates concise abstractive summaries of highly redundant opinions. Evaluation results on summarizing user reviews show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method. The summaries are readable, reasonably well-formed and are informative enough to convey the major opinions. 
This paper proposes a novel semisupervised word alignment technique called EMDC that integrates discriminative and generative methods. A discriminative aligner is used to ﬁnd high precision partial alignments that serve as constraints for a generative aligner which implements a constrained version of the EM algorithm. Experiments on small-size Chinese and Arabic tasks show consistent improvements on AER. We also experimented with moderate-size Chinese machine translation tasks and got an average of 0.5 point improvement on BLEU scores across ﬁve standard NIST test sets and four other test sets. 
 This paper makes three significant extensions to a noisy channel speller designed for standard written text to target the challenging domain of search queries. First, the noisy channel model is subsumed by a more general ranker, which allows a variety of features to be easily incorporated. Second, a distributed infrastructure is proposed for training and applying Web scale n-gram language models. Third, a new phrase-based error model is presented. This model places a probability distribution over transformations between multi-word phrases, and is estimated using large amounts of query-correction pairs derived from search logs. Experiments show that each of these extensions leads to significant improvements over the stateof-the-art baseline methods.  
Surface realisation with grammars integrating ﬂat semantics is known to be NP complete. In this paper, we present a new algorithm for surface realisation based on Feature Based Tree Adjoining Grammar (FTAG) which draws on the observation that an FTAG can be translated into a Regular Tree Grammar describing its derivation trees. We carry out an extensive testing of several variants of this algorithm using an automatically produced testsuite and compare the results obtained with those obtained using GenI, another FTAG based surface realiser. 
We describe an approach to automatically learn reordering rules to be applied as a preprocessing step in phrase-based machine translation. We learn rules for 8 different language pairs, showing BLEU improvements for all of them, and demonstrate that many important order transformations (SVO to SOV or VSO, headmodiﬁer, verb movement) can be captured by this approach. 
Recent studies have shown the potential beneﬁts of leveraging resources for resource-rich languages to build tools for similar, but resource-poor languages. We examine what constitutes “similarity” by comparing traditional phylogenetic language groups, which are motivated largely by genetic relationships, with language groupings formed by clustering methods using typological features only. Using data from the World Atlas of Language Structures (WALS), our preliminary experiments show that typologically-based clusters look quite different from genetic groups, but perform as good or better when used to predict feature values of member languages. 
In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other treebanks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2–5% F1. 
We describe a new unsupervised approach for synonymy discovery by aligning paraphrases in monolingual domain corpora. For that purpose, we identify phrasal terms that convey most of the concepts within domains and adapt a methodology for the automatic extraction and alignment of paraphrases to identify paraphrase casts from which valid synonyms are discovered. Results performed on two different domain corpora show that general synonyms as well as synonymic expressions can be identiﬁed with a 67.27% precision. 
Given a movie comment, does it contain a spoiler? A spoiler is a comment that, when disclosed, would ruin a surprise or reveal an important plot detail. We study automatic methods to detect comments and reviews that contain spoilers and apply them to reviews from the IMDB (Internet Movie Database) website. We develop topic models, based on Latent Dirichlet Allocation (LDA), but using linguistic dependency information in place of simple features from bag of words (BOW) representations. Experimental results demonstrate the effectiveness of our technique over four movie-comment datasets of different scales. 
Plagiarism is the use of the language and thoughts of another work and the representation of them as one's own original work. Various levels of plagiarism exist in many domains in general and in academic papers in particular. Therefore, diverse efforts are taken to automatically identify plagiarism. In this research, we developed software capable of simple plagiarism detection. We have built a corpus (C) containing 10,100 academic papers in computer science written in English and two test sets including papers that were randomly chosen from C. A widespread variety of baseline methods has been developed to identify identical or similar papers. Several methods are novel. The experimental results and their analysis show interesting findings. Some of the novel methods are among the best predictive methods. 
We present an approach to model hidden attributes in the compositional semantics of adjective-noun phrases in a distributional model. For the representation of adjective meanings, we reformulate the pattern-based approach for attribute learning of Almuhareb (2006) in a structured vector space model (VSM). This model is complemented by a structured vector space representing attribute dimensions of noun meanings. The combination of these representations along the lines of compositional semantic principles exposes the underlying semantic relations in adjective-noun phrases. We show that our compositional VSM outperforms simple pattern-based approaches by circumventing their inherent sparsity problems. 
Hierarchical phrase-based machine translation can capture global reordering with synchronous context-free grammar, but has little ability to evaluate the correctness of word orderings during decoding. We propose a method to integrate word-based reordering model into hierarchical phrasebased machine translation to overcome this weakness. Our approach extends the synchronous context-free grammar rules of hierarchical phrase-based model to include reordered source strings, allowing efﬁcient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system. 
Phrase reordering is of great importance for statistical machine translation. According to the movement of phrase translation, the pattern of phrase reordering can be divided into three classes: monotone, BTG (Bracket Transduction Grammar) and hierarchy. It is a good way to use different styles of reordering models to reorder different phrases according to the characteristics of both the reordering models and phrases itself. In this paper a novel reordering model based on multi-layer phrase (PRML) is proposed, where the source sentence is segmented into different layers of phrases on which different reordering models are applied to get the final translation. This model has some advantages: different styles of phrase reordering models are easily incorporated together; when a complicated reordering model is employed, it can be limited in a smaller scope and replaced with an easier reordering model in larger scope. So this model better trade-offs the translation speed and performance simultaneously. 
It has been recognized for quite some time that sustainable data formats play an important role in the development and curation of linguistic resources. The purpose of this paper is to show how GermaNet, the German version of the Princeton WordNet, can be converted to the Lexical Markup Framework (LMF), a published ISO standard (ISO-24613) for encoding lexical resources. The conversion builds on Wordnet-LMF, which has been proposed in the context of the EU KYOTO project as an LMF format for wordnets. The present paper proposes a number of crucial modifications and a set of extensions to Wordnet-LMF that are needed for conversion of wordnets in general and for conversion of GermaNet in particular. 
We propose and implement a modiﬁcation of the Eisner (1996) normal form to account for generalized composition of bounded degree, and an extension to deal with grammatical type-raising. 
This paper 1 presents an empirical approach to mining parallel corpora. Conventional approaches use a readily available collection of comparable, nonparallel corpora to extract parallel sentences. This paper attempts the much more challenging task of directly searching for high-quality sentence pairs from the Web. We tackle the problem by formulating good search query using „Learning to Rank‟ and by filtering noisy document pairs using IBM Model 1 alignment. End-to-end evaluation shows that the proposed approach significantly improves the performance of statistical machine translation. 
Cross Document Coreference (CDC) is the task of constructing the coreference chain for mentions of a person across a set of documents. This work offers a holistic view of using document-level categories, sub-document level context and extracted entities and relations for the CDC task. We train a categorization component with an efﬁcient ﬂat algorithm using thousands of ODP categories and over a million web documents. We propose to use ranked categories as coreference information, particularly suitable for web documents that are widely different in style and content. An ensemble composite coreference function, amenable to inactive features, combines these three levels of evidence for disambiguation. A thorough feature importance study is conducted to analyze how these three components contribute to the coreference results. The overall solution is evaluated using the WePS benchmark data and demonstrate superior performance. 
This paper proposes a new approach to phrase rescoring for statistical machine translation (SMT). A set of novel features capturing the translingual equivalence between a source and a target phrase pair are introduced. These features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to discriminatively rescore the baseline MT system’s phrase library: boost good phrase translations while prune bad ones. This approach not only significantly improves machine translation quality, but also reduces the model size by a considerable margin. 
Fact collections are mostly built using semi-supervised relation extraction techniques and wisdom of the crowds methods, rendering them inherently noisy. In this paper, we propose to validate the resulting facts by leveraging global constraints inherent in large fact collections, observing that correct facts will tend to match their arguments with other facts more often than with incorrect ones. We model this intuition as a graph-ranking problem over a fact graph and explore novel random walk algorithms. We present an empirical study, over a large set of facts extracted from a 500 million document webcrawl, validating the model and showing that it improves fact quality over state-of-the-art methods. 
In this paper we propose a completely unsupervised method for open-domain entity extraction and clustering over query logs. The underlying hypothesis is that classes deﬁned by mining search user activity may signiﬁcantly differ from those typically considered over web documents, in that they better model the user space, i.e. users’ perception and interests. We show that our method outperforms state of the art (semi-)supervised systems based either on web documents or on query logs (16% gain on the clustering task). We also report evidence that our method successfully supports a real world application, namely keyword generation for sponsored search. 
We describe the implementation of reranking models for ﬁne-grained opinion analysis – marking up opinion expressions and extracting opinion holders. The reranking approach makes it possible to model complex relations between multiple opinions in a sentence, allowing us to represent how opinions interact through the syntactic and semantic structure. We carried out evaluations on the MPQA corpus, and the experiments showed signiﬁcant improvements over a conventional system that only uses local information: for both tasks, our system saw recall boosts of over 10 points. 
Adaptor grammars are a framework for expressing and performing inference over a variety of non-parametric linguistic models. These models currently provide state-of-the-art performance on unsupervised word segmentation from phonemic representations of child-directed unsegmented English utterances. This paper investigates the applicability of these models to unsupervised word segmentation of Mandarin. We investigate a wide variety of different segmentation models, and show that the best segmentation accuracy is obtained from models that capture interword “collocational” dependencies. Surprisingly, enhancing the models to exploit syllable structure regularities and to capture tone information does improve overall word segmentation accuracy, perhaps because the information these elements convey is redundant when compared to the inter-word dependencies. 
This paper presents a ﬁrst efﬁcient implementation of a weighted deductive CYK parser for Probabilistic Linear ContextFree Rewriting Systems (PLCFRS), together with context-summary estimates for parse items used to speed up parsing. LCFRS, an extension of CFG, can describe discontinuities both in constituency and dependency structures in a straightforward way and is therefore a natural candidate to be used for data-driven parsing. We evaluate our parser with a grammar extracted from the German NeGra treebank. Our experiments show that datadriven LCFRS parsing is feasible with a reasonable speed and yields output of competitive quality. 
In this paper we consider the problem of building a system to predict readability of natural-language documents. Our system is trained using diverse features based on syntax and language models which are generally indicative of readability. The experimental results on a dataset of documents from a mix of genres show that the predictions of the learned system are more accurate than the predictions of naive human judges when compared against the predictions of linguistically-trained expert human judges. The experiments also compare the performances of different learning algorithms and different types of feature sets when used for predicting readability. 
Sense annotation and lexicon building are costly affairs demanding prudent investment of resources. Recent work on multilingual WSD has shown that it is possible to leverage the annotation work done for WSD of one language (SL) for another (TL), by projecting Wordnet and sense marked corpus parameters of SL to TL. However, this work does not take into account the cost of manually cross-linking the words within aligned synsets. Further, it does not answer the question of “Can better accuracy be achieved if a user is willing to pay additional money?” We propose a measure for cost-beneﬁt analysis which measures the “value for money” earned in terms of accuracy by investing in annotation effort and lexicon building. Two key ideas explored in this paper are (i) the use of probabilistic crosslinking model to reduce manual crosslinking effort and (ii) the use of selective sampling to inject a few training examples for hard-to-disambiguate words from the target language to boost the accuracy. 
While extensive studies on relation extraction have been conducted in the last decade, statistical systems based on supervised learning are still limited because they require large amounts of training data to achieve high performance. In this paper, we develop a cross-lingual annotation projection method that leverages parallel corpora to bootstrap a relation detector without signiﬁcant annotation efforts for a resource-poor language. In order to make our method more reliable, we introduce three simple projection noise reduction methods. The merit of our method is demonstrated through a novel Korean relation detection task. 
This paper describes a feasibility study of n-gram-based evaluation metrics for automatic keyphrase extraction. To account for near-misses currently ignored by standard evaluation metrics, we adapt various evaluation metrics developed for machine translation and summarization, and also the R-precision evaluation metric from keyphrase evaluation. In evaluation, the R-precision metric is found to achieve the highest correlation with human annotations. We also provide evidence that the degree of semantic similarity varies with the location of the partially-matching component words. 
Text Understanding systems often commit to a single best interpretation of a sentence before analyzing subsequent text. This interpretation is chosen by resolving ambiguous alternatives to the one with the highest conﬁdence, given the context available at the time of commitment. Subsequent text, however, may contain information that changes the conﬁdence of alternatives. This may especially be the case with multiple redundant texts on the same topic. Ideally, systems would delay choosing among ambiguous alternatives until more text has been read. One solution is to maintain multiple candidate interpretations of each sentence until the system acquires disambiguating evidence. Unfortunately, the number of alternatives explodes quickly. In this paper, we propose a packed graphical (PG) representation that can efﬁciently represent a large number of alternative interpretations along with dependencies among them. We also present an algorithm for combining multiple PG representations to help resolve ambiguity and prune alternatives when the time comes to commit to a single interpretation. Our controlled experiments show that by delaying ambiguity resolution until multiple texts have been read, our prototype’s accuracy is higher than when committing to interpretations sentence-by-sentence.  
This paper investigates the use and the prediction potential of semantic similarity measures for automatic generation of links across different documents and passages. First, the correlation between the way people link content and the results produced by standard semantic similarity measures is investigated. The relation between semantic similarity and the length of the documents is then also analysed. Based on these ﬁndings a new method for link generation is formulated and tested. 
This paper proposes a dependency-driven scheme to dynamically determine the syntactic parse tree structure for tree kernel-based anaphoricity determination in coreference resolution. Given a full syntactic parse tree, it keeps the nodes and the paths related with current mention based on constituent dependencies from both syntactic and semantic perspectives, while removing the noisy information, eventually leading to a dependency-driven dynamic syntactic parse tree (D-DSPT). Evaluation on the ACE 2003 corpus shows that the D-DSPT outperforms all previous parse tree structures on anaphoricity determination, and that applying our anaphoricity determination module in coreference resolution achieves the so far best performance. 
This paper describes how to cluster together the phrases of a phrase-based statistical machine translation (SMT) system, using information in the phrase table itself. The clustering is symmetric and recursive: it is applied both to sourcelanguage and target-language phrases, and the clustering in one language helps determine the clustering in the other. The phrase clusters have many possible uses. This paper looks at one of these uses: smoothing the conditional translation model (TM) probabilities employed by the SMT system. We incorporated phrase-cluster-derived probability estimates into a baseline loglinear feature combination that included relative frequency and lexically-weighted conditional probability estimates. In ChineseEnglish (C-E) and French-English (F-E) learning curve experiments, we obtained a gain over the baseline in 29 of 30 tests, with a maximum gain of 0.55 BLEU points (though most gains were fairly small). The largest gains came with medium (200-400K sentence pairs) rather than with small (less than 100K sentence pairs) amounts of training data, contrary to what one would expect from the paraphrasing literature. We have only begun to explore the original smoothing approach described here. 
Context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community, e.g. (Fung, 1998; Rapp, 1999). Surprisingly, none of those works have systematically investigated the impact of the many parameters controlling their approach. The present study aims at doing just this. As a testcase, we address the task of translating terms of the medical domain by exploiting pages mined from Wikipedia. One interesting outcome of this study is that signiﬁcant gains can be obtained by using an association measure that is rarely used in practice. 
We present a constituent parsing-based reordering technique that improves the performance of the state-of-the-art English-to-Japanese phrase translation system that includes distortion models by 4.76 BLEU points. The phrase translation model with reordering applied at the pre-processing stage outperforms a syntax-based translation system that incorporates a phrase translation model, a hierarchical phrase-based translation model and a tree-to-string grammar. We also show that combining constituent reordering and the syntax model improves the translation quality by additional 0.84 BLEU points. 
Polarity shifting marked by various linguistic structures has been a challenge to automatic sentiment classification. In this paper, we propose a machine learning approach to incorporate polarity shifting information into a document-level sentiment classification system. First, a feature selection method is adopted to automatically generate the training data for a binary classifier on polarity shifting detection of sentences. Then, by using the obtained binary classifier, each document in the original polarity classification training data is split into two partitions, polarity-shifted and polarity-unshifted, which are used to train two base classifiers respectively for further classifier combination. The experimental results across four different domains demonstrate the effectiveness of our approach. 
Previous work on bilingual lexicon extraction from comparable corpora aimed at ﬁnding a good representation for the usage patterns of source and target words and at comparing these patterns efﬁciently. In this paper, we try to work it out in another way: improving the quality of the comparable corpus from which the bilingual lexicon has to be extracted. To do so, we propose a measure of comparability and a strategy to improve the quality of a given corpus through an iterative construction process. Our approach, being general, can be used with any existing bilingual lexicon extraction method. We show here that it leads to a signiﬁcant improvement over standard bilingual lexicon extraction methods. 
In this paper, we focus on object feature 1 based review summarization. Different from most of previous work with linguistic rules or statistical methods, we formulate the review mining task as a joint structure tagging problem. We propose a new machine learning framework based on Conditional Random Fields (CRFs). It can employ rich features to jointly extract positive opinions, negative opinions and object features for review sentences. The linguistic structure can be naturally integrated into model representation. Besides linear-chain structure, we also investigate conjunction structure and syntactic tree structure in this framework. Through extensive experiments on movie review and product review data sets, we show that structure-aware models outperform many state-of-the-art approaches to review mining. 
This paper addresses the problem of dynamic model parameter selection for loglinear model based statistical machine translation (SMT) systems. In this work, we propose a principled method for this task by transforming it to a test data dependent development set selection problem. We present two algorithms for automatic development set construction, and evaluated our method on several NIST data sets for the Chinese-English translation task. Experimental results show that our method can effectively adapt log-linear model parameters to different test data, and consistently achieves good translation performance compared with conventional methods that use a ﬁxed model parameter setting across different data sets. 
In this paper we present a simplified shallow semantic parsing approach to learning the scope of negation (SoN). This is done by formulating it as a shallow semantic parsing problem with the negation signal as the predicate and the negation scope as its arguments. Our parsing approach to SoN learning differs from the state-of-the-art chunking ones in two aspects. First, we extend SoN learning from the chunking level to the parse tree level, where structured syntactic information is available. Second, we focus on determining whether a constituent, rather than a word, is negated or not, via a simplified shallow semantic parsing framework. Evaluation on the BioScope corpus shows that structured syntactic information is effective in capturing the domination relationship between a negation signal and its dominated arguments. It also shows that our parsing approach much outperforms the state-of-the-art chunking ones. 
Several researchers have proposed semi-supervised learning methods for adapting event extraction systems to new event types. This paper investigates two kinds of bootstrapping methods used for event extraction: the document-centric and similarity-centric approaches, and proposes a filtered ranking method that combines the advantages of the two. We use a range of extraction tasks to compare the generality of this method to previous work. We analyze the results using two evaluation metrics and observe the effect of different training corpora. Experiments show that our new ranking method not only achieves higher performance on different evaluation metrics, but also is more stable across different bootstrapping corpora. 
Previous research has demonstrated the importance of handling differences between domains such as “newswire” and “biomedicine” when porting NLP systems from one domain to another. In this paper we identify the related issue of subdomain variation, i.e., differences between subsets of a domain that might be expected to behave homogeneously. Using a large corpus of research articles, we explore how subdomains of biomedicine vary across a variety of linguistic dimensions and discover that there is rich variation. We conclude that an awareness of such variation is necessary when deploying NLP systems for use in single or multiple subdomains. 
News tweets that report what is happening have become an important real-time information source. We raise the problem of Semantic Role Labeling (SRL) for news tweets, which is meaningful for fine grained information extraction and retrieval. We present a self-supervised learning approach to train a domain specific SRL system to resolve the problem. A large volume of training data is automatically labeled, by leveraging the existing SRL system on news domain and content similarity between news and news tweets. On a human annotated test set, our system achieves state-of-the-art performance, outperforming the SRL system trained on news. 
Tree-based translation models, which exploit the linguistic syntax of source language, usually separate decoding into two steps: parsing and translation. Although this separation makes tree-based decoding simple and efﬁcient, its translation performance is usually limited by the number of parse trees offered by parser. Alternatively, we propose to parse and translate jointly by casting tree-based translation as parsing. Given a source-language sentence, our joint decoder produces a parse tree on the source side and a translation on the target side simultaneously. By combining translation and parsing models in a discriminative framework, our approach signiﬁcantly outperforms a forestbased tree-to-string system by 1.1 absolute BLEU points on the NIST 2005 Chinese-English test set. As a parser, our joint decoder achieves an F1 score of 80.6% on the Penn Chinese Treebank. 
We propose semantic role features for a Tree-to-String transducer to model the reordering/deletion of source-side semantic roles. These semantic features, as well as the Tree-to-String templates, are trained based on a conditional log-linear model and are shown to signiﬁcantly outperform systems trained based on Max-Likelihood and EM. We also show signiﬁcant improvement in sentence ﬂuency by using the semantic role features in the log-linear model, based on manual evaluation. 
This paper analyzes the contribution of semantic roles to TimeML event recognition and classiﬁcation. For that purpose, an approach using conditional random ﬁelds with a variety of morphosyntactic features plus semantic roles features is developed and evaluated. Our system achieves an F1 of 81.4% in recognition and a 64.2% in classiﬁcation. We demonstrate that the application of semantic roles improves the performance of the presented system, especially for nominal events. 
We study the problem of integrating scattered online opinions. For this purpose, we propose to exploit structured ontology to obtain well-formed relevant aspects to a topic and use them to organize scattered opinions to generate a structured summary. Particularly, we focus on two main challenges in implementing this idea, (1) how to select the most useful aspects from a large number of aspects in the ontology and (2) how to order the selected aspects to optimize the readability of the structured summary. We propose and explore several methods for solving these challenges. Experimental results on two different data sets (US Presidents and Digital Cameras) show that the proposed methods are effective for selecting aspects that can represent the major opinions and for generating coherent ordering of aspects. 
We propose an unsupervised approach utilizing only raw corpora to enhance morphological alignment involving highly inﬂected languages. Our method focuses on closed-class morphemes, modeling their inﬂuence on nearby words. Our languageindependent model recovers important links missing in the IBM Model 4 alignment and demonstrates improved end-toend translations for English-Finnish and English-Hungarian. 
We propose to analyse semantic similarity in comparable text by matching syntactic trees and labeling the alignments according to one of ﬁve semantic similarity relations. We present a Memorybased Graph Matcher (MBGM) that performs both tasks simultaneously as a combination of exhaustive pairwise classiﬁcation using a memory-based learner, followed by global optimization of the alignments using a combinatorial optimization algorithm. The method is evaluated on a monolingual treebank consisting of comparable Dutch news texts. Results show that it performs substantially above the baseline and close to the human reference. 
This paper investigates how to automatically create a dialogue control component of a listening agent to reduce the current high cost of manually creating such components. We collected a large number of listening-oriented dialogues with their user satisfaction ratings and used them to create a dialogue control component using partially observable Markov decision processes (POMDPs), which can learn a policy to satisfy users by automatically ﬁnding a reasonable reward function. A comparison between our POMDP-based component and other similarly motivated systems using human subjects revealed that POMDPs can satisfactorily produce a dialogue control component that can achieve reasonable subjective assessment. 
Texts are commonly interpreted based on the entire discourse in which they are situated. Discourse processing has been shown useful for inference-based application; yet, most systems for textual entailment – a generic paradigm for applied inference – have only addressed discourse considerations via off-the-shelf coreference resolvers. In this paper we explore various discourse aspects in entailment inference, suggest initial solutions for them and investigate their impact on entailment performance. Our experiments suggest that discourse provides useful information, which signiﬁcantly improves entailment inference, and should be better addressed by future entailment systems. 
The detailed analyses of sentence structure provided by parsers have been applied to address several information extraction tasks. In a recent bio-molecular event extraction task, state-of-the-art performance was achieved by systems building specifically on dependency representations of parser output. While intrinsic evaluations have shown signiﬁcant advances in both general and domain-speciﬁc parsing, the question of how these translate into practical advantage is seldom considered. In this paper, we analyze how event extraction performance is affected by parser and dependency representation, further considering the relation between intrinsic evaluation and performance at the extraction task. We ﬁnd that good intrinsic evaluation results do not always imply good extraction performance, and that the types and structures of different dependency representations have speciﬁc advantages and disadvantages for the event extraction task. 
Relations between entities in text have been widely researched in the natural language processing and informationextraction communities. The region connecting a pair of entities (in a parsed sentence) is often used to construct kernels or feature vectors that can recognize and extract interesting relations. Such regions are useful, but they can also incorporate unnecessary distracting information. In this paper, we propose a rulebased method to remove the information that is unnecessary for relation extraction. Protein–protein interaction (PPI) is used as an example relation extraction problem. A dozen simple rules are deﬁned on output from a deep parser. Each rule specifically examines the entities in one target interaction pair. These simple rules were tested using several PPI corpora. The PPI extraction performance was improved on all the PPI corpora. 
 2008) and finding relations in textual entailment (Burchardt and Frank 2006).  In this paper we explore the possibility of using cross lingual projections that help to automatically induce role-semantic annotations in the PropBank paradigm for Urdu, a resource poor language. This technique provides annotation projections based on word alignments. It is relatively inexpensive and has the potential to reduce human effort involved in creating semantic role resources. The projection model exploits lexical as well as syntactic information on an English-Urdu parallel corpus. We show that our method generates reasonably good annotations with an accuracy of 92% on short structured sentences. Using the automatically generated annotated corpus, we conduct preliminary experiments to create a semantic role labeler for Urdu. The results of the labeler though modest, are promising and indicate the potential of our technique to generate large scale annotations for Urdu.  Automatically identifying semantic roles is often referred to as shallow semantic parsing (Gildea and Jurafsky, 2002). For English, this process is facilitated by the existence of two main SRL annotated corpora – FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Both datasets mark almost all surface realizations of semantic roles. FrameNet has 800 semantic frames that cover 120,000 example sentences1. PropBank has annotations that cover over 113,000 predicate-argument structures. Clearly English is well supported with resources for semantic roles. However, there are other widely spoken resource poor languages that are not as privileged. The PropBank based resources available for languages like Chinese (Xue and Palmer, 2009), Korean (Palmer et al., 2006) and Spanish (Taule, 2008) are only about two-thirds the size of the English PropBank. Several alternative techniques have been explored in the literature to generate semantic role labeled corpora for resource poor languages as providing manually annotated data is time con-  
The main task we address in our research is classification of text using fine-grained attitude labels. The developed @AM system relies on the compositionality principle and a novel approach based on the rules elaborated for semantically distinct verb classes. The evaluation of our method on 1000 sentences, that describe personal experiences, showed promising results: average accuracy on the finegrained level (14 labels) was 62%, on the middle level (7 labels) – 71%, and on the top level (3 labels) – 88%. 
We present an unsupervised word segmentation model for machine translation. The model uses existing monolingual segmentation techniques and models the joint distribution over source sentence segmentations and alignments to the target sentence. During inference, the monolingual segmentation model and the bilingual word alignment model are coupled so that the alignments to the target sentence guide the segmentation of the source sentence. The experiments show improvements on Arabic-English and ChineseEnglish translation tasks. 
This paper describes a search procedure to discover optimal feature sets for dependency parsers. The search applies to the shift–reduce algorithm and the feature sets are extracted from the parser conﬁguration. The initial feature is limited to the ﬁrst word in the input queue. Then, the procedure uses a set of rules founded on the assumption that topological neighbors of signiﬁcant features in the dependency graph may also have a signiﬁcant contribution. The search can be fully automated and the level of greediness adjusted with the number of features examined at each iteration of the discovery procedure. Using our automated feature discovery on two corpora, the Swedish corpus in CoNLL-X and the English corpus in CoNLL 2008, and a single parser system, we could reach results comparable or better than the best scores reported in these evaluations. The CoNLL 2008 test set contains, in addition to a Wall Street Journal (WSJ) section, an out-of-domain sample from the Brown corpus. With sets of 15 features, we obtained a labeled attachment score of 84.21 for Swedish, 88.11 on the WSJ test set, and 81.33 on the Brown test set. 
We evaluate two dependency parsers, MSTParser and MaltParser, with respect to their capacity to recover unbounded dependencies in English, a type of evaluation that has been applied to grammarbased parsers and statistical phrase structure parsers but not to dependency parsers. The evaluation shows that when combined with simple post-processing heuristics, the parsers correctly recall unbounded dependencies roughly 50% of the time, which is only slightly worse than two grammar-based parsers speciﬁcally designed to cope with such dependencies. 
This paper proposes a co-training style algorithm called Co-STAR that acquires hyponymy relations simultaneously from structured and unstructured text. In CoSTAR, two independent processes for hyponymy relation acquisition – one handling structured text and the other handling unstructured text – collaborate by repeatedly exchanging the knowledge they acquired about hyponymy relations. Unlike conventional co-training, the two processes in Co-STAR are applied to different source texts and training data. We show the effectiveness of this algorithm through experiments on largescale hyponymy-relation acquisition from Japanese Wikipedia and Web texts. We also show that Co-STAR is robust against noisy training data. 
This paper presents a simple and efﬁcient algorithm for approximate dictionary matching designed for similarity measures such as cosine, Dice, Jaccard, and overlap coefﬁcients. We propose this algorithm, called CPMerge, for the τ overlap join of inverted lists. First we show that this task is solvable exactly by a τ -overlap join. Given inverted lists retrieved for a query, the algorithm collects fewer candidate strings and prunes unlikely candidates to efﬁciently ﬁnd strings that satisfy the constraint of the τ -overlap join. We conducted experiments of approximate dictionary matching on three large-scale datasets that include person names, biomedical names, and general English words. The algorithm exhibited scalable performance on the datasets. For example, it retrieved strings in 1.1 ms from the string collection of Google Web1T unigrams (with cosine similarity and threshold 0.7).  is a nontrivial task to ﬁnd the entry from these surface expressions appearing in text. This paper addresses approximate dictionary matching, which consists of ﬁnding all strings in a string collection V such that they have similarity that is no smaller than a threshold α with a query string x. This task has a broad range of applications, including spelling correction, ﬂexible dictionary look-up, record linkage, and duplicate detection (Henzinger, 2006; Manku et al., 2007). Formally, the task obtains a subset Yx,α ⊆ V , Yx,α = {y ∈ V sim(x, y) ≥ α}, (1) where sim(x, y) presents the similarity between x and y. A na¨ıve solution to this task is to compute similarity values |V | times, i.e., between x and every string y ∈ V . However, this solution is impractical when the number of strings |V | is huge (e.g., more than one million). In this paper, we present a simple and efﬁcient algorithm for approximate dictionary matching designed for similarity measures such as cosine, Dice, Jaccard, and overlap coefﬁcients. Our main contributions are twofold.  
During face-to-face conversation, people naturally integrate speech, gestures and higher level language interpretations to predict the right time to start talking or to give backchannel feedback. In this paper we introduce a new model called Latent Mixture of Discriminative Experts which addresses some of the key issues with multimodal language processing: (1) temporal synchrony/asynchrony between modalities, (2) micro dynamics and (3) integration of different levels of interpretation. We present an empirical evaluation on listener nonverbal feedback prediction (e.g., head nod), based on observable behaviors of the speaker. We conﬁrm the importance of combining four types of multimodal features: lexical, syntactic structure, eye gaze, and prosody. We show that our Latent Mixture of Discriminative Experts model outperforms previous approaches based on Conditional Random Fields (CRFs) and Latent-Dynamic CRFs. 
Text summarization solves the problem of extracting important information from huge amount of text data. There are various methods in the literature that aim to find out well-formed summaries. One of the most commonly used methods is the Latent Semantic Analysis (LSA). In this paper, different LSA based summarization algorithms are explained and two new LSA based summarization algorithms are proposed. The algorithms are evaluated on Turkish documents, and their performances are compared using their ROUGE-L scores. One of our algorithms produces the best scores. 
Although much work in NLP has focused on simply determining what a document means, we also must know whether or not to believe it. Fact-ﬁnding algorithms attempt to identify the “truth” among competing claims in a corpus, but fail to take advantage of the user’s prior knowledge and presume that truth itself is universal and objective rather than subjective. We introduce a framework for incorporating prior knowledge into any factﬁnding algorithm, expressing both general “common-sense” reasoning and speciﬁc facts already known to the user as ﬁrst-order logic and translating this into a tractable linear program. As our results show, this approach scales well to even large problems, both reducing error and allowing the system to determine truth respective to the user rather than the majority. Additionally, we introduce three new fact-ﬁnding algorithms capable of outperforming existing fact-ﬁnders in many of our experiments. 
We use web-scale N-grams in a base NP parser that correctly analyzes 95.4% of the base NPs in natural text. Web-scale data improves performance. That is, there is no data like more data. Performance scales log-linearly with the number of parameters in the model (the number of unique N-grams). The web-scale N-grams are particularly helpful in harder cases, such as NPs that contain conjunctions. 
This paper presents an approach to summarize single scientiﬁc papers, by extracting its contributions from the set of citation sentences written in other papers. Our methodology is based on extracting signiﬁcant keyphrases from the set of citation sentences and using these keyphrases to build the summary. Comparisons show how this methodology excels at the task of single paper summarization, and how it out-performs other multi-document summarization methods. 
In practical applications, decoding speed is very important. Modern structured learning technique adopts template based method to extract millions of features. Complicated templates bring about abundant features which lead to higher accuracy but more feature extraction time. We propose Two Dimensional Trie (2D Trie), a novel efﬁcient feature indexing structure which takes advantage of relationship between templates: feature strings generated by a template are preﬁxes of the features from its extended templates. We apply our technique to Maximum Spanning Tree dependency parsing. Experimental results on Chinese Tree Bank corpus show that our 2D Trie is about 5 times faster than traditional Trie structure, making parsing speed 4.3 times faster. 
The problem addressed in this paper is to predict a user’s numeric rating in a product review from the text of the review. Unigram and n-gram representations of text are common choices in opinion mining. However, unigrams cannot capture important expressions like “could have been better”, which are essential for prediction models of ratings. N-grams of words, on the other hand, capture such phrases, but typically occur too sparsely in the training set and thus fail to yield robust predictors. This paper overcomes the limitations of these two models, by introducing a novel kind of bag-of-opinions representation, where an opinion, within a review, consists of three components: a root word, a set of modiﬁer words from the same sentence, and one or more negation words. Each opinion is assigned a numeric score which is learned, by ridge regression, from a large, domain-independent corpus of reviews. For the actual test case of a domain-dependent review, the review’s rating is predicted by aggregating the scores of all opinions in the review and combining it with a domaindependent unigram model. The paper presents a constrained ridge regression algorithm for learning opinion scores. Experiments show that the bag-of-opinions method outperforms prior state-of-the-art techniques for review rating prediction.  
Emotion words have been well used as the most obvious choice as feature in the task of textual emotion recognition and automatic emotion lexicon construction. In this work, we explore features for recognizing word emotion. Based on RenCECps (an annotated emotion corpus) and MaxEnt (Maximum entropy) model, several contextual features and their combination have been experimented. Then PLSA (probabilistic latent semantic analysis) is used to get semantic feature by clustering words and sentences. The experimental results demonstrate the effectiveness of using semantic feature for word emotion recognition. After that, “word emotion components” is proposed to describe the combined basic emotions in a word. A signiﬁcant performance improvement over contextual and semantic features was observed after adding word emotion components as feature. 
Research in named entity recognition and mention detection has typically involved a fairly small number of semantic classes, which may not be adequate if semantic class information is intended to support natural language applications. Motivated by this observation, we examine the under-studied problem of semantic subtype induction, where the goal is to automatically determine which of a set of 92 ﬁne-grained semantic classes a noun phrase belongs to. We seek to improve the standard supervised approach to this problem using two techniques: hierarchical classiﬁcation and collective classiﬁcation. Experimental results demonstrate the effectiveness of these techniques, whether or not they are applied in isolation or in combination with the standard approach. 
Model minimization has been shown to work well for the task of unsupervised part-of-speech tagging with a dictionary. In (Ravi and Knight, 2009), the authors invoke an integer programming (IP) solver to do model minimization. However, solving this problem exactly using an integer programming formulation is intractable for practical purposes. We propose a novel two-stage greedy approximation scheme to replace the IP. Our method runs fast, while yielding highly accurate tagging results. We also compare our method against standard EM training, and show that we consistently obtain better tagging accuracies on test data of varying sizes for English and Italian. 
Active learning has been applied to different NLP tasks, with the aim of limiting the amount of time and cost for human annotation. Most studies on active learning have only simulated the annotation scenario, using prelabelled gold standard data. We present the ﬁrst active learning experiment for Word Sense Disambiguation with human annotators in a realistic environment, using ﬁne-grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classiﬁer performance when applied to a real-world task. 
Route directions are natural language (NL) statements that specify, for a given navigational task and an automatically computed route representation, a sequence of actions to be followed by the user to reach his or her goal. A corpusbased approach to generate route directions involves (i) the selection of elements along the route that need to be mentioned, and (ii) the induction of a mapping from route elements to linguistic structures that can be used as a basis for NL generation. This paper presents an Expectation-Maximization (EM) based algorithm that aligns geographical route representations with semantically annotated NL directions, as a basis for the above tasks. We formulate one basic and two extended models, the latter capturing special properties of the route direction task. Although our current data set is small, both extended models achieve better results than the simple model and a random baseline. The best results are achieved by a combination of both extensions, which outperform the random baseline and the simple model by more than an order of magnitude. 
The interpretation of a multiple-domain text corpus as a single ontology leads to misconceptions. This is because some concepts may be syntactically equal; though, they are semantically lopsided in different domains. Also, the occurrences of a domain concept in a large multipledomain corpus may not gauge correctly the concept significance. This paper tackles the mentioned problems and proposes a novel ontology builder to extract separate domain specific ontologies from such a corpus. The builder contribution is to sustain each domain specific concepts and relations to get precise answers for user questions. We extend a single ontology builder named Text2Onto to apply our thought. We fruitfully enhance it to answer, more precisely, questions on a subset of AQUAINT corpus. 
The paper describes a weakly supervised approach for decomposing words into all morphemes: stems, preﬁxes and sufﬁxes, using wordforms with marked stems as training data. As we concentrate on under-resourced languages, the amount of training data is limited and we need some amount of supervision in the form of a small number of wordforms with marked stems. In the ﬁrst stage we introduce a new Supervised Stem Extraction algorithm (SSE). Once stems have been extracted, an improved unsupervised segmentation algorithm GBUMS (GraphBased Unsupervised Morpheme Segmentation) is used to segment sufﬁx or preﬁx sequences into individual sufﬁxes and preﬁxes. The approach, experimentally validated on Turkish and isiZulu languages, gives high performance on test data and is comparable to a fully supervised method. 
Multi-document summarization has been an important problem in information retrieval. It aims to distill the most important information from a set of documents to generate a compressed summary. Given a sentence graph generated from a set of documents where vertices represent sentences and edges indicate that the corresponding vertices are similar, the extracted summary can be described using the idea of graph domination. In this paper, we propose a new principled and versatile framework for multi-document summarization using the minimum dominating set. We show that four well-known summarization tasks including generic, query-focused, update, and comparative summarization can be modeled as diﬀerent variations derived from the proposed framework. Approximation algorithms for performing summarization are also proposed and empirical experiments are conducted to demonstrate the eﬀectiveness of our proposed framework. 
Main approaches to corpus-based semantic class mining include distributional similarity (DS) and pattern-based (PB). In this paper, we perform an empirical comparison of them, based on a publicly available dataset containing 500 million web pages, using various categories of queries. We further propose a frequencybased rule to select appropriate approaches for different types of terms. 
We present a novel approach to automatic metaphor identiﬁcation in unrestricted text. Starting from a small seed set of manually annotated metaphorical expressions, the system is capable of harvesting a large number of metaphors of similar syntactic structure from a corpus. Our method is distinguished from previous work in that it does not employ any hand-crafted knowledge, other than the initial seed set, but, in contrast, captures metaphoricity by means of verb and noun clustering. Being the ﬁrst to employ unsupervised methods for metaphor identiﬁcation, our system operates with the precision of 0.79. 
 Thanks to its simplicity, social tagging system has accumulated huge amount of user contributed tags. However, user contributed tags lack explicit hierarchical structure, while many tag-based applications would beneﬁt if such a structure presents. In this work, we explore the structure of tags with a directed and easy-to-evaluate relation, named as the subsumption relation. We propose three methods to discover the subsumption relation between tags. Speciﬁcally, the tagged document’s content is used to ﬁnd the relations, which leads to better result. Besides relation discovery, we also propose a greedy algorithm to eliminate the redundant relations by constructing a Layered Directed Acyclic Graph (LayeredDAG) of tags. We perform quantitative evaluations on two real world data sets. The results show that our methods outperform hierarchical clustering-based approach. Empirical study of the constructed Layered-DAG and error analysis are also provided. 
Zulu is an indigenous language of South Africa, and one of the eleven ofﬁcial languages of that country. It is spoken by about 11 million speakers. Although it is similar in size to some Western languages, e.g. Swedish, it is considerably under-resourced. This paper presents a new open-source morphological corpus for Zulu named Ukwabelana corpus. We describe the agglutinating morphology of Zulu with its multiple preﬁxation and sufﬁxation, and also introduce our labeling scheme. Further, the annotation process is described and all single resources are explained. These comprise a list of 10,000 labeled and 100,000 unlabeled word types, 3,000 part-of-speech (POS) tagged and 30,000 raw sentences as well as a morphological Zulu grammar, and a parsing algorithm which hypothesizes possible word roots and enumerates parses that conform to the Zulu grammar. We also provide a POS tagger which assigns the grammatical category to a morphologically analyzed word type. As it is hoped that the corpus and all resources will be of beneﬁt to any person doing research on Zulu or on computer-aided analysis of languages, they will be made available in the public domain from http://www.cs.bris. ac.uk/Research/MachineLearning/ Morphology/Resources/. 
We present a novel Evaluation Metric for Morphological Analysis (EMMA) that is both linguistically appealing and empirically sound. EMMA uses a graphbased assignment algorithm, optimized via integer linear programming, to match morphemes of predicted word analyses to the analyses of a morphologically rich answer key. This is necessary especially for unsupervised morphology analysis systems which do not have access to linguistically motivated morpheme labels. Across 3 languages, EMMA scores of 14 systems have a substantially greater positive correlation with mean average precision in an information retrieval (IR) task than do scores from the metric currently used by the Morpho Challenge (MC) competition series. We compute EMMA and MC metric scores for 93 separate system-language pairs from the 2007, 2008, and 2009 MC competitions, demonstrating that EMMA is not susceptible to two types of gaming that have plagued recent MC competitions: Ambiguity Hijacking and Shared Morpheme Padding. The EMMA evaluation script is publicly available from http://www.cs.bris.ac.uk/ Research/MachineLearning/ Morphology/Resources/. 
In this paper, we describe a novel approach to computational modeling and understanding of social and cultural phenomena in multi-party dialogues. We developed a two-tier approach in which we first detect and classify certain social language uses, including topic control, disagreement, and involvement, that serve as first order models from which presence the higher level social constructs such as leadership, may be inferred. 1. Introduction We investigate the language dynamics in small group interactions across various settings. Our focus in this paper is on English online chat conversations; however, the models we are developing are more universal and applicable to other conversational situations: informal face-to-face interactions, formal meetings, moderated discussions, as well as interactions conducted in languages other than English, e.g., Urdu and Mandarin. Multi-party online conversations are particularly interesting because they become a pervasive form of communication within virtual communities, ubiquitous across all age groups. In particular, a great amount of communication online occurs in virtual chat-rooms, typically conducted using a highly informal text dialect. At the same time, the reduced-cue environment of online interaction necessitates more explicit linguistic devices to convey social and cultural nuances than is typical in face-to-face or even voice conversations. Our objective is to develop computational models of how certain social phenomena such as leadership, power, and conflict are signaled and reflected in language through the choice of lexical, syntactic, semantic and conversational forms by discourse participants. In this  paper we report the results of an initial phase of our work during which we constructed a prototype system called DSARMD-1 (Detecting Social Actions and Roles in Multiparty Dialogue). Given a representative segment of multiparty task-oriented dialogue, DSARMD-1 automatically classifies all discourse participants by the degree to which they deploy selected social language uses, such as topic control, task control, involvement, and disagreement. These are the mid-level social phenomena, which are deployed by discourse participants in order to achieve or assert higher-level social constructs, including leadership. In this work we adopted a two-tier empirical approach where social language uses are modeled through observable linguistic features that can be automatically extracted from dialogue. The high-level social constructs are then inferred from a combination of language uses attributed to each discourse participant; for example, a high degree of influence and a high degree of involvement by the same person may indicate a leadership role. In this paper we limit our discussion to the first tier only: how to effectively model and classify social language uses in multi-party dialogue. 2. Related Research Issues related to linguistic manifestation of social phenomena have not been systematically researched before in computational linguistics; indeed, most of the effort thus far was directed towards the communicative dimension of discourse. While the Speech Acts theory (Austin, 1962; Searle, 1969) provides a generalized framework for multiple levels of discourse analysis (locution, illocution and perlocution), most current approaches to dialogue focus on information content and structural components (Blaylock, 2002; Carberry & Lambert, 1999; Stolcke, et al., 2000) in dialogue; few take into account the effects that speech acts may have upon the social  1038 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038–1046, Beijing, August 2010  roles of discourse participants. Also relevant is research on modeling sequences of dialogue acts – to predict the next one (Samuel et al. 1998; Ji & Bilmes, 2006 inter alia) – or to map them onto subsequences or “dialogue games” (Carlson 1983; Levin et al., 1998), which are attempts to formalize participants’ roles in conversation (e.g., Linell, 1990; Poesio & Mikheev, 1998; Field et al., 2008). There is a body of literature in anthropology, linguistics, sociology, and communication on the relationship between language and power, as well as other social phenomena, e.g., conflict, leadership; however, existing approaches typically look at language use in situations where the social relationships are known, rather than using language predictively. For example, conversational analysis (Sacks et al., 1974) is concerned with the structure of interaction: turn-taking, when interruptions occur, how repairs are signaled, but not what they reveal about the speakers. Research in anthropology and communication has concentrated on how certain social norms and behaviors may be reflected in language (e.g., Scollon and Scollon, 2001; Agar, 1994) with few systematic studies attempting to explore the reverse, i.e., what the linguistic phenomena tell us about social norms and behaviors. 3. Data & Annotation Our initial focus has been on on-line chat dialogues. While chat data is plentiful on-line, its adaptation for research purposes presents a number of challenges that include users’ privacy issues on the one hand, and their complete anonymity on the other. Furthermore, most data that may be obtained from public chat-rooms is of limited value for the type of modeling tasks we are interested in due to its high-level of noise, lack of focus, and rapidly shifting, chaotic nature, which makes any longitudinal studies virtually impossible. To derive complex models of conversational behavior, we need the interaction to be reasonably focused on a task and/or social objectives within a group. Few data collections exist covering multiparty dialogue, and even fewer with on-line chat. Moreover, the few collections that exist were built primarily for the purpose of training dialogue act tagging and similar linguistic phenomena; few if any of these corpora are  suitable for deriving pragmatic models of conversation, including socio-linguistic phenomena. Existing resources include a multi-person meeting corpus ICSI-MRDA and the AMI Meeting Corpus (Carletta, 2007), which contains 100 hours of meetings captured using synchronized recording devices. Still, all of these resources look at spoken language rather than on-line chat. There is a parallel interest in the online chat environment, although the development of useful resources has progressed less. Some corpora exist such as the NPS Internet chat corpus (Forsyth and Martell, 2007), which has been hand-anonymized and labeled with part-of-speech tags and dialogue act labels. The StrikeCom corpus (Twitchell et al., 2007) consists of 32 multi-person chat dialogues between players of a strategic game, where in 50% of the dialogues one participant has been asked to behave ‘deceptively’. It is thus more typical that those interested in the study of Internet chat compile their own corpus on an as needed basis, e.g., Wu et al. (2002), Khan et al. (2002), Kim et al. (2007). Driven by the need to obtain a suitable dataset we designed a series of experiments in which recruited subjects were invited to participate in a series of on-line chat sessions in a specially designed secure chat-room. The experiments were carefully designed around topics, tasks, and games for the participants to engage in so that appropriate types of behavior, e.g., disagreement, power play, persuasion, etc. may emerge spontaneously. These experiments and the resulting corpus have been described elsewhere (Shaikh et al., 2010b), and we refer the reader to this source. Ultimately a corpus of 50 hours of English chat dialogue was collected comprising more than 20,000 turns and 120,000 words. In addition we also assembled a corpus of 20 hours of Urdu chat. A subset of English language dataset has been annotated at four levels: communication links, dialogue acts, local topics and meso-topics (which are essentially the most persistent local topics). Although full details of these annotations are impossible to explain within the scope of this article, we briefly describe them below. Annotated datasets were used to develop and train automatic modules that detect and classify social uses of language in discourse. It is important to note that the annota-  1039  tion has been developed to support the objectives of our project and does not necessarily conform to other similar annotation systems used in the past. • Communicative links. In a multi-party dialogue an utterance may be directed towards a specific participant, a subgroup of participants or to everyone. • Dialogue Acts. We developed a hierarchy of 15 dialogue acts for annotating the functional aspect of the utterance in discussion. The tagset we adopted is based on DAMSL (Allen & Core, 1997) and SWBD (Jurafsky et al., 1997), but compressed to 15 tags tuned significantly towards dialogue pragmatics and away from more surface characteristics of utterances (Shaikh et al., 2010a). • Local topics. Local topics are defined as nouns or noun phrases introduced into discourse that are subsequently mentioned again via repetition, synonym, or pronoun. • Topic reference polarity. Some topics, which we call meso-topics, persist through a number of turns in conversation. A selection of meso-topics is closely associated with the task in which the discourse participants are engaged. Meso-topics can be distinguished from the local topics because the speakers often make polarized statements about them. 4. Socio-linguistic Phenomena We are interested in modeling the social phenomena of Leadership and Power in discourse. These high-level phenomena (or Social Roles, SR) will be detected and attributed to discourse participants based on their deployment of selected Language Uses (LU) in multi-party dialogue. Language Uses are mid-level socio-linguistic devices that link linguistic components deployed in discourse (from lexical to pragmatic) to social constructs obtaining for and between the participants. The language uses that we are currently studying are Agenda Control, Disagreement, and Involvement (Broadwell et al., 2010). Our research so far is focused on the analysis of English-language synchronous chat, and we are looking for correlations between various metrics that can be used to detect LU in multiparty dialogue. We expect that some of these correlations may be culturally specific or language-specific, as we move into the  analysis of Urdu and Mandarin discourse in the next phase of this project. 4.1 Agenda Control in Dialogue Agenda Control is defined as efforts by a member or members of the group to advance the group’s task or goal. This is a complex LU that we will model along two dimensions: (1) Topic Control and (2) Task Control. Topic Control refers to attempts by any discourse participants to impose the topic of conversation. Task Control, on the other hand, is an effort by some members of the group to define the group’s project or goal and/or steer the group towards that goal. We believe that both behaviors can be detected using scalar measures per participant based on certain linguistic features of their utterances. For example, one hypothesis is that topic control is indicated by the rate of local topic introductions (LTI) per participant (Givon, 1983). Local topics may be defined quite simply as noun phrases introduced into discourse, which are subsequently mentioned again via repetition, synonym, pronoun, or other form of co-reference. Thus, one measure of topic control is the number of local topics introduced by each participant as percentage of all local topics in a discourse. Using an LTI index we can construct assertions about topic control in a discourse. For example, suppose the following information is discovered about the speaker LE in a multi-party discussion dialogue-11 where 90 local topics are identified: 1. LE introduces 23/90 (25.6%) of local top- ics in this dialogue. 2. The mean rate of local topic introductions is this dialogue is 14.29%, and standard deviation is 8.01. 3. LE is in the top quintile of participants for introducing new local topics We can now claim the following, with a degree of confidence (to be determined): TopicControlLTI (LE, 5, dialogue-1) We read this as follows: speaker LE exerts the highest degree of topic control in dialogue-1. Of course, LTI is just one source of evidence and we developed other metrics to complement it. We mention three of them here: 
We employ Maximum Entropy model to conduct sub-tree alignment between bilingual phrasal structure trees. Various lexical and structural knowledge is explored to measure the syntactic similarity across Chinese-English bilingual tree pairs. In the experiment, we evaluate the sub-tree alignment using both gold standard tree bank and the automatically parsed corpus with manually annotated sub-tree alignment. Compared with a heuristic similarity based method, the proposed method significantly improves the performance with only limited sub-tree aligned data. To examine its effectiveness for multilingual applications, we further attempt different approaches to apply the sub-tree alignment in both phrase and syntax based SMT systems. We then compare the performance with that of the widely used word alignment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment. 
Verb classes which integrate a wide range of linguistic properties (Levin, 1993) have proved useful for natural language processing (NLP) applications. However, the real-world use of these classes has been limited because for most languages, no resources similar to VerbNet (KipperSchuler, 2005) are available. We apply a verb clustering approach developed for English to French – a language for which no such experiment has been conducted yet. Our investigation shows that not only the general methodology but also the best performing features are transferable between the languages, making it possible to learn useful VerbNet style classes for French automatically without languagespeciﬁc tuning. 
Automated conversion has allowed the development of wide-coverage corpora for a variety of grammar formalisms without the expense of manual annotation. Analysing new languages also tests formalisms, exposing their strengths and weaknesses. We present Chinese CCGbank, a 760,000 word corpus annotated with Combinatory Categorial Grammar (CCG) derivations, induced automatically from the Penn Chinese Treebank (PCTB). We design parsimonious CCG analyses for a range of Chinese syntactic constructions, and transform the PCTB trees to produce them. Our process yields a corpus of 27,759 derivations, covering 98.1% of the PCTB. 1 Introduction An annotated corpus is typically used to develop statistical parsers for a given formalism and language. An alternative to the enormous cost of hand-annotating a corpus for a speciﬁc formalism is to convert from an existing corpus. The Penn Treebank (PTB; Marcus et al., 1994) has been converted to HPSG (Miyao et al., 2004), LFG (Cahill et al., 2002), LTAG (Xia, 1999), and CCG (Hockenmaier, 2003). Dependency corpora, e.g. the German Tiger corpus, have also been converted (Hockenmaier, 2006). The Penn Chinese Treebank (PCTB; Xue et al., 2005) provides analyses for 770,000 words of Chinese. Existing PCTB conversions have targeted TAG (Chen et al., 2005) and LFG (Burke and Lam, 2004; Guo et al., 2007). We present Chinese CCGbank, a Chinese corpus of CCG derivations automatically induced from the PCTB. Combinatory Categorial Grammar (CCG; Steedman, 2000) is a lexicalised grammar formalism offering a uniﬁed account of local and nonlocal dependencies. We harness the facilities of  
We propose a structure called dependency forest for statistical machine translation. A dependency forest compactly represents multiple dependency trees. We develop new algorithms for extracting string-todependency rules and training dependency language models. Our forest-based string-to-dependency system obtains signiﬁcant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 
A distributed system is described that reliably mines parallel text from large corpora. The approach can be regarded as cross-language near-duplicate detection, enabled by an initial, low-quality batch translation. In contrast to other approaches which require specialized metadata, the system uses only the textual content of the documents. Results are presented for a corpus of over two billion web pages and for a large collection of digitized public-domain books. 
The precise identiﬁcation of light verb constructions is crucial for the successful functioning of several NLP applications. In order to facilitate the development of an algorithm that is capable of recognizing them, a manually annotated corpus of light verb constructions has been built for Hungarian. Basic annotation guidelines and statistical data on the corpus are also presented in the paper. It is also shown how applications in the ﬁelds of machine translation and information extraction can make use of such a corpus and an algorithm. 
Syntax based reordering has been shown to be an effective way of handling word order differences between source and target languages in Statistical Machine Translation (SMT) systems. We present a simple, automatic method to learn rules that reorder source sentences to more closely match the target language word order using only a source side parse tree and automatically generated alignments. The resulting rules are applied to source language inputs as a pre-processing step and demonstrate signiﬁcant improvements in SMT systems across a variety of languages pairs including English to Hindi, English to Spanish and English to French as measured on a variety of internal test sets as well as a public test set. 
Strategic business decision making involves the analysis of market forecasts. Today, the identiﬁcation and aggregation of relevant market statements is done by human experts, often by analyzing documents from the World Wide Web. We present an efﬁcient information extraction chain to automate this complex natural language processing task and show results for the identiﬁcation part. Based on time and money extraction, we identify sentences that represent statements on revenue using support vector classiﬁcation. We provide a corpus with German online news articles, in which more than 2,000 such sentences are annotated by domain experts from the industry. On the test data, our statement identiﬁcation algorithm achieves an overall precision and recall of 0.86 and 0.87 respectively. 
Single-document summarization and multidocument summarization are very closely related tasks and they have been widely investigated independently. This paper examines the mutual influences between the two tasks and proposes a novel unified approach to simultaneous single-document and multidocument summarizations. The mutual influences between the two tasks are incorporated into a graph model and the ranking scores of a sentence for the two tasks can be obtained in a unified ranking process. Experimental results on the benchmark DUC datasets demonstrate the effectiveness of the proposed approach for both single-document and multidocument summarizations. 
Discriminating vandalism edits from non-vandalism edits in Wikipedia is a challenging task, as ill-intentioned edits can include a variety of content and be expressed in many different forms and styles. Previous studies are limited to rule-based methods and learning based on lexical features, lacking in linguistic analysis. In this paper, we propose a novel Web-based shallow syntacticsemantic modeling method, which utilizes Web search results as resource and trains topic-specific n-tag and syntactic n-gram language models to detect vandalism. By combining basic task-specific and lexical features, we have achieved high F-measures using logistic boosting and logistic model trees classifiers, surpassing the results reported by major Wikipedia vandalism detection systems. 
Question detection serves great purposes in the cQA question retrieval task. While detecting questions in standard language data corpus is relatively easy, it becomes a great challenge for online content. Online questions are usually long and informal, and standard features such as question mark or 5W1H words are likely to be absent. In this paper, we explore question characteristics in cQA services, and propose an automated approach to detect question sentences based on lexical and syntactic features. Our model is capable of handling informal online languages. The empirical evaluation results further demonstrate that our model signiﬁcantly outperforms traditional methods in detecting online question sentences, and it considerably boosts the question retrieval performance in cQA. 
A range of Natural Language Processing tasks involve making judgments about the semantic relatedness of a pair of sentences, such as Recognizing Textual Entailment (RTE) and answer selection for Question Answering (QA). A key challenge that these tasks face in common is the lack of explicit alignment annotation between a sentence pair. We capture the alignment by using a novel probabilistic model that models tree-edit operations on dependency parse trees. Unlike previous tree-edit models which require a separate alignment-ﬁnding phase and resort to ad-hoc distance metrics, our method treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. We demonstrate the robustness of our model by conducting experiments for RTE and QA, and show that our model performs competitively on both tasks with the same set of general features. 
The character-based tagging approach is a dominant technique for Chinese word segmentation, and both discriminative and generative models can be adopted in that framework. However, generative and discriminative character-based approaches are significantly different and complement each other. A simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches. Experiments on the Second SIGHAN Bakeoff show that this joint approach achieves 21% relative error reduction over the discriminative model and 14% over the generative one. In addition, closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best Fscore in four out of five corpora. 
We explore the near-synonym lexical choice problem using a novel representation of near-synonyms and their contexts in the latent semantic space. In contrast to traditional latent semantic analysis (LSA), our model is built on the lexical level of co-occurrence, which has been empirically proven to be effective in providing higher dimensional information on the subtle differences among near-synonyms. By employing supervised learning on the latent features, our system achieves an accuracy of 74.5% in a “ﬁll-in-the-blank” task. The improvement over the current state-of-the-art is statistically signiﬁcant. We also formalize the notion of subtlety through its relation to semantic space dimensionality. Using this formalization and our learning models, several of our intuitions about subtlety, dimensionality, and context are quantiﬁed and empirically tested. 
 Dynamic sentiment ambiguous  adjectives (DSAAs) like “large, small,  high, low” pose a challenging task on  sentiment analysis. This paper proposes a  knowledge -based  method  to  automatically determine the semantic  orientation of DSAAs within context.  The task is reduced to sentiment  classification of target nouns, which we  refer to sentiment expectation instead of  semantic orientation widely used in  previous researches. We mine the Web  using lexico-syntactic patterns to infer  sentiment expectation of nouns, and then  exploit character-sentiment model to  reduce noises caused by the Web data.  At sentence level, our method achieves  promising result with an f-score of  78.52% that is substantially better than  baselines. At document level, our  method outperforms previous work in  sentiment classification of product  reviews.  
As tokenization is usually ambiguous for many natural languages such as Chinese and Korean, tokenization errors might potentially introduce translation mistakes for translation systems that rely on 1-best tokenizations. While using lattices to offer more alternatives to translation systems have elegantly alleviated this problem, we take a further step to tokenize and translate jointly. Taking a sequence of atomic units that can be combined to form words in different ways as input, our joint decoder produces a tokenization on the source side and a translation on the target side simultaneously. By integrating tokenization and translation features in a discriminative framework, our joint decoder outperforms the baseline translation systems using 1-best tokenizations and lattices signiﬁcantly on both ChineseEnglish and Korean-Chinese tasks. Interestingly, as a tokenizer, our joint decoder achieves signiﬁcant improvements over monolingual Chinese tokenizers. 
For sentiment analysis, lexicons play an important role in many related tasks. In this paper, aiming to build Chinese emotion lexicons for public use, we adopted a graph-based algorithm which ranks words according to a few seed emotion words. The ranking algorithm exploits the similarity between words, and uses multiple similarity metrics which can be derived from dictionaries, unlabeled corpora or heuristic rules. To evaluate the adopted algorithm and resources, two independent judges were asked to label the top words of ranking list. It is observed that noise is almost unavoidable due to imprecise similarity metrics between words. So, to guarantee the quality of emotion lexicons, we use an iterative feedback to combine manual labeling and the automatic ranking algorithm above. We also compared our newly constructed Chinese emotion lexicons (happiness, anger, sadness, fear and surprise) with existing counterparts, and related analysis is offered. 
Nocuous ambiguity occurs when a linguistic expression is interpreted differently by different readers in a given context. We present an approach to automatically identify nocuous ambiguity that is likely to lead to misunderstandings among readers. Our model is built on a machine learning architecture. It learns from a set of heuristics each of which predicts a factor that may lead a reader to favor a particular interpretation. An ambiguity threshold indicates the extent to which ambiguity can be tolerated in the application domain. Collections of human judgments are used to train heuristics and set ambiguity thresholds, and for evaluation. We report results from applying the methodology to coordination and anaphora ambiguity. Results show that the method can identify nocuous ambiguity in text, and may be widened to cover further types of ambiguity. We discuss approaches to evaluation. 
In this paper we investigate the challenges of applying statistical machine translation to meeting conversations, with a particular view towards analyzing the importance of modeling contextual factors such as the larger discourse context and topic/domain information on translation performance. We describe the collection of a small corpus of parallel meeting data, the development of a statistical machine translation system in the absence of genre-matched training data, and we present a quantitative analysis of translation errors resulting from the lack of contextual modeling inherent in standard statistical machine translation systems. Finally, we demonstrate how the largest source of translation errors (lack of topic/domain knowledge) can be addressed by applying documentlevel, unsupervised word sense disambiguation, resulting in performance improvements over the baseline system. 
Dative variation is a widely observed syntactic phenomenon in world languages (e.g. I gave John a book and I gave a book to John). It has been shown that which surface form will be used in a dative sentence is not a completely random choice, rather, it is conditioned by a wide range of linguistic factors. Previous work by Bresnan and colleagues adopted a statistical modeling approach to investigate the probabilistic trends in English dative alternation. In this paper, we report a similar study on Mandarin Chinese. We further developed Bresnan et al.’s models to suit the complexity of the Chinese data. Our models effectively explain away a large proportion of the variation in the data, and unveil some interesting probabilistic features of Chinese grammar. Among other things, we show that Chinese dative variation is sensitive to heavy NP shift in both left and right directions. 
This paper proposes an efﬁcient online method that trains a classiﬁer with many conjunctive features. We employ kernel computation called kernel slicing, which explicitly considers conjunctions among frequent features in computing the polynomial kernel, to combine the merits of linear and kernel-based training. To improve the scalability of this training, we reuse the temporal margins of partial feature vectors and terminate unnecessary margin computations. Experiments on dependency parsing and hyponymy-relation extraction demonstrated that our method could train a classiﬁer orders of magnitude faster than kernel-based online learning, while retaining its space efﬁciency. 
Near-synonyms are useful knowledge resources for many natural language applications such as query expansion for information retrieval (IR) and paraphrasing for text generation. However, near-synonyms are not necessarily interchangeable in contexts due to their specific usage and syntactic constraints. Accordingly, it is worth to develop algorithms to verify whether near-synonyms do match the given contexts. In this paper, we consider the near-synonym substitution task as a classification task, where a classifier is trained for each near-synonym set to classify test examples into one of the near-synonyms in the set. We also propose the use of discriminative training to improve classifiers by distinguishing positive and negative features for each nearsynonym. Experimental results show that the proposed method achieves higher accuracy than both pointwise mutual information (PMI) and n-gram-based methods that have been used in previous studies. 
In distributional semantics studies, there is a growing attention in compositionally determining the distributional meaning of word sequences. Yet, compositional distributional models depend on a large set of parameters that have not been explored. In this paper we propose a novel approach to estimate parameters for a class of compositional distributional models: the additive models. Our approach leverages on two main ideas. Firstly, a novel idea for extracting compositional distributional semantics examples. Secondly, an estimation method based on regression models for multiple dependent variables. Experiments demonstrate that our approach outperforms existing methods for determining a good model for compositional distributional semantics. 
In opinion mining of product reviews, one often wants to produce a summary of opinions based on product features/attributes. However, for the same feature, people can express it with different words and phrases. To produce a meaningful summary, these words and phrases, which are domain synonyms, need to be grouped under the same feature group. This paper proposes a constrained semisupervised learning method to solve the problem. Experimental results using reviews from five different domains show that the proposed method is competent for the task. It outperforms the original EM and the state-of-the-art existing methods by a large margin. 
Supertagging is an important technique for deep syntactic analysis. A supertagger is usually trained independently of the parser using a sequence labeling method. This presents an inconsistent training objective between the supertagger and the parser. In this paper, we propose a forest-guided supertagger training method to alleviate this problem by incorporating global grammar constraints into the supertagging process using a CFGﬁlter. It also provides an approach to make the supertagger and the parser more tightly integrated. The experiment shows that using the forest-guided trained supertagger, the parser got an absolute 0.68% improvement from baseline in F-score for predicate-argument relation recognition accuracy and achieved a competitive result of 89.31% with a faster parsing speed, compared to a state-of-the-art HPSG parser. 
Entity linking refers entity mentions in a document to their representations in a knowledge base (KB). In this paper, we propose to use additional information sources from Wikipedia to find more name variations for entity linking task. In addition, as manually creating a training corpus for entity linking is laborintensive and costly, we present a novel method to automatically generate a large scale corpus annotation for ambiguous mentions leveraging on their unambiguous synonyms in the document collection. Then, a binary classifier is trained to filter out KB entities that are not similar to current mentions. This classifier not only can effectively reduce the ambiguities to the existing entities in KB, but also be very useful to highlight the new entities to KB for the further population. Furthermore, we also leverage on the Wikipedia documents to provide additional information which is not available in our generated corpus through a domain adaption approach which provides further performance improvements. The experiment results show that our proposed method outperforms the state-of-the-art approaches. 
A large body of prior research on coreference resolution recasts the problem as a two-class classiﬁcation problem. However, standard supervised machine learning algorithms that minimize classiﬁcation errors on the training instances do not always lead to maximizing the F-measure of the chosen evaluation metric for coreference resolution. In this paper, we propose a novel approach comprising the use of instance weighting and beam search to maximize the evaluation metric score on the training corpus during training. Experimental results show that this approach achieves signiﬁcant improvement over the state-of-the-art. We report results on standard benchmark corpora (two MUC corpora and three ACE corpora), when evaluated using the link-based MUC metric and the mention-based B-CUBED metric. 
This paper proposes a method that extracts paraphrases from search engine query logs. The method ﬁrst extracts paraphrase query-title pairs based on an assumption that a search query and its corresponding clicked document titles may mean the same thing. It then extracts paraphrase query-query and title-title pairs from the query-title paraphrases with a pivot approach. Paraphrases extracted in each step are validated with a binary classiﬁer. We evaluate the method using a query log from Baidu1, a Chinese search engine. Experimental results show that the proposed method is effective, which extracts more than 3.5 million pairs of paraphrases with a precision of over 70%. The results also show that the extracted paraphrases can be used to generate high-quality paraphrase patterns. 
This paper proposes a method that leverages multiple machine translation (MT) engines for paraphrase generation (PG). The method includes two stages. Firstly, we use a multi-pivot approach to acquire a set of candidate paraphrases for a source sentence S. Then, we employ two kinds of techniques, namely the selection-based technique and the decoding-based technique, to produce a best paraphrase T for S using the candidates acquired in the ﬁrst stage. Experimental results show that: (1) The multi-pivot approach is effective for obtaining plenty of valuable candidate paraphrases. (2) Both the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases. Moreover, these two techniques are complementary. (3) The proposed method outperforms a state-of-the-art paraphrase generation approach. 
Ambiguity of entity mentions and concept references is a challenge to mining text beyond surface-level keywords. We describe an effective method of disambiguating surface forms and resolving them to Wikipedia entities and concepts. Our method employs an extensive set of features mined from Wikipedia and other large data sources, and combines the features using a machine learning approach with automatically generated training data. Based on a manually labeled evaluation set containing over 1000 news articles, our resolution model has 85% precision and 87.8% recall. The performance is significantly better than three baselines based on traditional context similarities or sense commonness measurements. Our method can be applied to other languages and scales well to new entities and concepts. 
There often exist multiple corpora for the same natural language processing (NLP) tasks. However, such corpora are generally used independently due to distinctions in annotation standards. For the purpose of full use of readily available human annotations, it is signiﬁcant to simultaneously utilize multiple corpora of different annotation standards. In this paper, we focus on the challenge of constituent syntactic parsing with treebanks of different annotations and propose a collaborative decoding (or co-decoding) approach to improve parsing accuracy by leveraging bracket structure consensus between multiple parsing decoders trained on individual treebanks. Experimental results show the effectiveness of the proposed approach, which outperforms stateof-the-art baselines, especially on long sentences. 
In this paper, we consider sentence simpliﬁcation as a special form of translation with the complex sentence as the source and the simple sentence as the target. We propose a Tree-based Simpliﬁcation Model (TSM), which, to our knowledge, is the ﬁrst statistical simpliﬁcation model covering splitting, dropping, reordering and substitution integrally. We also describe an efﬁcient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. 
Many Semantic Role Labeling (SRL) combination strategies have been proposed and tested on English SRL task. But little is known about how much Chinese SRL can beneﬁt from system combination. And existing combination strategies trust each individual system’s output with the same conﬁdence when merging them into a pool of candidates. In our approach, we assign different weights to different system outputs, and add a weighted merging stage to the conventional SRL combination architecture. We also propose a method to obtain an appropriate weight for each system’s output by minimizing some error function on the development set. We have evaluated our strategy on Chinese Proposition Bank data set. With our minimum error weighting strategy, the F1 score of the combined result achieves 80.45%, which is 1.12% higher than baseline combination method’s result, and 4.90% higher than the best individual system’s result. 
Unrehearsed spoken language often contains disﬂuencies. In order to correctly interpret a spoken utterance, any such disﬂuencies must be identiﬁed and removed or otherwise dealt with. Operating on transcripts of speech which contain disﬂuencies, our particular focus here is the identiﬁcation and correction of speech repairs using a noisy channel model. Our aim is to develop a high-accuracy mechanism that can identify speech repairs in an incremental fashion, as the utterance is processed word-by-word. We also address the issue of the evaluation of such incremental systems. We propose a novel approach to evaluation, which evaluates performance in detecting and correcting disﬂuencies incrementally, rather than only assessing performance once the processing of an utterance is complete. This demonstrates some shortcomings in our basic incremental model, and so we then demonstrate a technique that improves performance on the detection of disﬂuencies as they happen. 
The Languages, Documentation and Information Management Division manages the entire document production chain. It employs 160 staff members. The translation services translate about 150,000 pages a year, 110,000 internally and 40,000 externally. About 10,000 documents are produced every year in the three official languages. As of now, more than 400,000 documents are available for downloading in English, French and Spanish. The English Translation Section has 4 permanent staff members, with 1 to 3 in-house freelance translators at any given time. The French and Spanish sections have 20 permanent staff members each, with 5 or more in-house freelance translators at any given time and 20 translators working from home on special service contracts. Approximately 25 % of work is outsourced to external translators. Working methods vary from the traditional dictation to the most sophisticated computer-assisted translation tools, depending on the type of document or the computer background of the translator. Translation support services are provided at all stages of the translation cycle. They aim to save translators and revisers time by finding sources, terminology and passages that have already been translated. Tasks range from referencing and pre-translating documents to developing computer-assisted translation (CAT) tools and providing relevant training and support. The Translation Support Section acts as the interface between the translators and computer experts and cooperates closely with the Informatics Division. Three teams share the responsibility for translation support. The CAT team, composed of a small group of experts (two computer-oriented translators and one expert in computer tools), provides high value-added services such as pre-translation of documents, management of translation memories, administration of terminology databases and development of computer applications that help translators in their work. The Support team (one and a half persons) acts as a hotline for translators (in-house and external), installs and updates the translation applications and provides training for users. The Referencing team, whose 5 members have a thorough knowledge of WTO documentation, serves about 80 translators by providing them all the necessary background documentation. For each document handed out for translation, the team typically provides a list of relevant references and, when appropriate, a comparison between the successive versions of the document to be translated. Using computer-assisted referencing tools, the team provides, for documents with occasional quotes from other documents, an electronic copy of the original with all - 1 -  passages occurring in other documents highlighted and hyperlinked to the sources in all languages. For documents that have been revised or altered, or that are largely lifted from other documents (new version, etc.), all deleted, added and displaced portions of text are marked in different colours and styles. Translators can thus focus only on new translation and use the recycled translation provided for them. Translation memories are systematically used for certain types of documents and for documents in which more than 50% is lifted from previously translated documents. Other tools are also used by translators and support staff. These include off-the-shelf terminology database management and term extraction systems, an in-house developed trilingual concordancer (i.e. a full-text search engine which displays the hit documents in the requested source and target languages and automatically aligns them), a legacy documentation search and retrieval system, a translation resources portal, etc. Most of these tools are available to the general public and may thus be used by external translators. Because of the wide range of subjects it covers and taking into account the sensitivity of certain documents and negotiations and the tight deadlines, translators consider the WTO a challenging organization to work for. But the work is generally considered rewarding because of the variety and the relevance of subjects covered, the challenging nature of texts that are often in the spotlight, the good working environment and the excellent tools made available to them. So what would be the added-value of an automatic translation system in this already highly-computerised translation environment? And what happens when an international organization such as the World Trade Organization decides to go ahead with machine translation? Why MT? Why SMT? Whereas computer-assisted translation tools have long been used at the WTO, machine translation had until recently never moved beyond testing. However, at the WTO as in other organizations, efforts are being made to streamline work processes and make the most of translation tools available on the market. The challenge was, and still is, can machine translation help a translator work faster while maintaining the same level of quality? In other words, can an automated translation system produce an output of usable quality, that is, a draft translation worth editing? With this in mind, several tests were carried out in 2009 by the Translation Support Section on rule-based machine translation systems. But these did not produced the expected results and proved to be time– and resource–consuming. However, the latest developments in the field of statistical machine translation showed that this approach to automated translation in the context of the WTO translation services was more realistic and less costly. Unlike the rule-based approaches to MT, the statistical approach could take advantage of the millions of sentences already translated by WTO translators without having to build huge specialized dictionaries and specific rules, thus reducing overhead costs. Therefore, in April 2010, the WTO invited potential partners to submit tenders outlining the approach and mechanisms by which they would support the development and implementation of its business requirements. Scope of the project and business requirements The project was to be carried out in two phases: a pilot phase, based on a limited scope of the WTO corpus and a limited number of language pairs, to assess the viability of the SMT approach and the ability of the implemented system to produce MT outputs of usable quality; and a production phase to provide a full-fledged statistical machine translation system, based on the entire WTO corpus, delivering MT outputs in the six language pairs in use at the WTO. It was understood that the production phase would be launched under the condition that the system, as implemented and fine-tuned during the pilot phase, would be able to produce MT outputs of usable quality and to handle growing amounts of data and requests. The company Simple Shift was selected among the potential suppliers to whom the Request for Proposal was sent. Simple Shift is a Geneva-based IT consulting and service company with a strong expertise in language engineering and multilingual information retrieval. - 2 -  Project Management and Implementation Methodology It was agreed with Simple Shift to use the open source statistical machine translation software Moses as the central platform for the WTO's SMT system (SMTS). Moreover, it was agreed that additional in-house software developed by Simple Shift (in particular the phrase segmenter and aligner) would be provided to the WTO, including the source code so as to guarantee the organization's autonomy. The segmenter in the SMT system installed at the WTO is purely statistical. Thus it needs to be trained on bilingual corpuses. The Canadian Hansard corpus was used for the English-French pair and the Europarl parallel corpus for the English-Spanish pair. As with all training-based systems, the difficulty was essentially to provide the computer with enough examples of all the possible sentence endings, as well as false sentence endings such as abbreviations and other symbols which include, for example, a full-stop mark. Just like the segmenter, the aligner is trained on a bilingual corpus. The aligner installed at the WTO is also trained on the Canadian Hansard and the Europarl corpus. The aligner chooses, for each source sentence, the target sentence which is the most probable translation. The system is provided with an ordered list of source sentences and a non-ordered list of target sentences. The most frequent words (articles, prepositions, etc.), which are defined in a so-called "stopword list", are removed in order to limit the index size, and thus reduce the number of possible translation options and increase the system’s performance. When the system analyzes the first word of the first source sentence, it looks up in its training material to decide which target word is the best possible translation More precisely, it looks for the source word in its training material, and then it looks at all the words in all the corresponding target sentences and calculates the probability, for each target word, that it could be the correct translation of the source word. The target word with the highest probability is chosen. Additional processes are also performed: in case of an alignment "hole" (i.e. the space separating two blocks of well-aligned sentences), the program stops and removes the first and last sentences of the well-aligned block in order to keep only the sentence pairs for which the alignment is certain. The aligner also performs a geometrical calculation of how realistic the alignment is, on the basis of the length of the source and target sentences. The result of the segmentation and alignment processes is the production of translation memories in TMX format. The content of these translation memories includes the source word or phrase, the target word or phrase and the corresponding translation probability (e.g. car ||| voiture ||| 0.8). On the basis of these TMX files, the program randomly extracts a number of source and target sentences (for example 2,000 sentences) and produces 3 files: a training file composed of all the translation memory except the 2,000 sentences; a tuning file containing 1,000 source and target sentences; a test file containing the other 1,000 source and target sentences. The result of this process is the creation of six files: the three files described above, respectively for the source and for the target languages. In this process, all the sentences which have no translation or which are entirely in capital letters are removed, as well as all sentences containing more than 40 words. Lowercasing and tokenization (i.e. separating each word from possible noise such as punctuation signs and apostrophes), language and translation models building, compression and tuning are then performed with the relevant modules included in the Moses application. The Moses environment The SMTS architecture relies on a single physical server hosting three to four virtual machines: one for the converter and node (translation system) builder; one to allow users to access and use the application; one or if possible two (depending on the performances of the physical server) to host the Moses nodes. A specific SMTS should be built and installed for each language pair and direction. Each node is encapsulated in a virtual machine (under VMware); each virtual machine includes two to four nodes, depending on the server's performances. Each node is dedicated to a language pair and direction and has its own IP address and port number. The SMTS is a full web application, requiring only a browser from the client computer. - 3 -  Input and output formats Two input and output formats will be available in the production version of the SMTS:  TXT: Input and output files will be in plain text format. This will in particular be the case when text is directly entered or copied/pasted in the web-based graphic user interface of the SMT system;  XLIFF: The input file will include the source segments, which will contain the sentences to be translated, and the target segments, which will be empty at the initial stage. Tags relating to the text layout (bold, italics, etc.) will not be kept in the translated segments. The output file will then be an XLIFF file containing the source segments of the original documents as well as the target segments as translated by the SMTS. This target file can then be opened with an XLIFF-supported bilingual editor (such as Trados Studio) for post-editing purposes. Methodology used at the WTO for output evaluation One must keep in mind that the central issue in evaluating any type of translation is that several outputs can be considered acceptable. Moreover, evaluators often do not agree on what a correct translation is. In order to avoid "reinventing the wheel", an extensive review of the literature on this topic was made. Then a list of evaluation criteria which the human translators could use was proposed and various evaluation metrics which seemed to fit the WTO's working context and project objectives were set: a statistical metric, which is rather objective but does not reflect the direct usability of the SMT output, and two human metrics, which are more subjective but were crucial in determining whether the SMT pilot project could be deployed into production. Quantitative metric: using the BLEU score To sum up, BLEU (Bilingual Evaluation Understudy) is a statistical metric of the quality of a machine translation which is calculated by comparing three elements: a series of source sentences, their translation performed by human translators (called "reference translation"), and their translation performed by a computer. The calculation is based on the "precision" metric. BLEU has several significant advantages: it correlates well with human judgment, it calculates quickly, it can deal with several reference translations for a given source sentence and there is an extensive benchmark of BLEU scores on the Internet. - 4 -  However, the BLEU scores are criticized on several points: the intelligibility or grammatical correctness is not taken into account; all words in a sentence have similar weight, whatever their frequency and semantic importance; it cannot deal with languages where words have no boundaries (although this is not an issue with the WTO's official languages).  Moses comes with a built-in BLEU score calculation module. This BLEU module can be used to calculate the BLEU score of any other MT system. It was considered useful to build a customized benchmark on WTO documents. Therefore, the BLEU score was chosen as the quantitative metric of the SMTS project to facilitate both the calculation processes and the comparison with the performance of Moses on other corpuses, or with the performance of another SMT system (typically Google Translate) on WTO corpuses.  Qualitative approach: proposed methodologies for an evaluation by human translators  The stated goal of the SMTS pilot project at the WTO is to assess whether the quality of the SMTS output is high enough for a human translator, or reviser, to post-edit instead of performing a full translation from scratch.  From this point of view, the central quality criterion should probably be the time needed to post-edit a given sentence from the SMTS output. This element is more critical than the gravity of the errors, because all errors (minor and major) will have to be corrected for the document to meet the WTO's quality standards, and it often takes about the same time to correct major and minor errors.  However, an evaluation process may only lead to three possible types of conclusion: the SMTS output is good enough to be post-edited; it is so bad that it will be impossible to improve it until it reaches the minimum quality level required; it could be used if improved to some extent.  A quality evaluation which would take into account only the time spent to post-edit SMTS sentences would make the second or third types of conclusion impossible, since there would be no information as to which problems are the most acute. This information, however, is necessary to conduct experiments to decide whether the system could be improved to some extent.  Therefore a two-fold approach was suggested. First, a simple methodology was proposed, based on the time spent to correct each sentence. If this first evaluation results in the conclusion that the SMTS output is worth editing, the evaluation process can be terminated. However, if the result indicates that the SMTS output requires too much correction time, or that almost none of the sentences could be kept for post-editing, a second round of analysis would be required to determine in more detail the types of errors met in the output. This in turn would help to establish priorities in the improvement effort. This second round of output evaluation should thus be based on a more detailed methodology.  The quantitative evaluation  The first evaluation sheet is based on the correction effort (and thus the time) required for each SMTS output sentence. The example below is built on a short series of 10 sentences extracted from a WTO document, as translated by the SMTS during the pilot project.  Number of Sentences  In %  Score per Total Sentence Score  Number of sentences which did not require any correction  
My impression from talking to other conference participants is that many of them feel a sense of awe and sometimes perplexity when hearing about the available technologies being presented – but that the situation in their own organization seems to take place in a parallel universe. The will may be present to rationalize operations by introducing workflows, terminology management systems, some degree of machine translation etc., but the chaos of existing systems presents a corporate localization department with many challenges of a quite down-to-earth nature: legacy file-storage systems, ad-hoc taxonomies, hectic and sometimes unpredictable file traffic, heterogeneous technologies, …. Wiping the slate clean and buying a comprehensive solution may be the answer for some. For others, the prospect of opening your doors to external consultants and completely re-thinking the localization workflow, and having to explain the ins and outs (and the cost) of this to an upper management that barely understands what the word “localization” means, and staff that basically likes the way things have always been done in their own particular office, must be pretty daunting. At SAS’s European Localization Center (ELC) we have developed slowly and organically over the past decade. Localization of SAS software was much less extensive in the 90’s where end users tended to have a data science or IT  background and a good command of English statistical terms. In the last decade, however, end solutions have increasingly targeted the front-office business user and the need for localization has grown in step with this. To keep track of the changes and growing volume of work, ELC has expanded and it has been necessary for all of us to develop our skill set and optimize our processes on a running basis. This presentation describes how we have tackled the seemingly mundane task of managing our translation memories, and the unexpected benefits that have arisen from this initiative. The path that we have taken does not cover new theoretical ground and is not earth-shattering in any sense. But I would like to open a window onto a solution that has evolved in the day-to-day maelstrom of the localization workspace and hope that it can inspire others to find a low-cost homegrown solution that knits into the idiosyncrasies of their particular localization workflow. TMs: the situation before…. SAS uses three different CAT tools to localize software products. Around 2004 we used a great deal of time and effort in re-organizing our storage structure for projects being sent out for localization, and translated projects coming back. The major part of this effort involved lengthy and detailed discussions about object granularity (product domain, product, project, toolkit, filetype etc.) and a suitable taxonomy for these objects. I mention this somewhat basic step because in my experience this is the absolutely most important make-or-break factor in creating an automated workflow of any kind. You can muddle by on inconsistent naming systems, but in the long-term this will continually trip you up when you try and introduce overarching systems that are scripted in a programming language. We created a good structure which was partnered with a database using the same set of taxonomies. Using the database and Perl scripts and a Web frontend we set up a fairly straightforward automated workflow. Our translators were able to trigger the building of localization toolkits, download these, translate them and upload them again. [A localization toolkit is a package containing all the files needed to deploy a localization project in a CAT tool on a local computer. The returned, completed toolkit is the same package including the translated files and the translation memory.] The system ensured that the uploaded toolkits and the files they contained were placed in the right folders. Some of the files were the translation memories. These were placed in designated folders but left in their proprietary format. After some time we wanted to set up a terminology management system, and needed to carry out term extraction on our existing translation memories. It now became a problem that our translation memories were distributed in the system and stored with each project. But pulling them together didn’t fully solve the issue because they were still in a binary format that was difficult to work with.  We decided to go the TMX way. We added an extra step to the toolkit upload process which took the uploaded project and used API’s from our CAT tools to create TMX. This is in itself was not an easy format to play around with, although now the TMs from the different tools were at least uniform. We decided that we needed a simple delimited txt format that was more amenable to scripting and we created an additional XSLT transformation that converted the TMX files into a very simple delimited format with the source and target strings only. Other metadata was stored in the file name (version, project code, filetype), and additional metadata was also obtainable through the relationships in our data structure (domain, project ship date etc.) Whenever a project was uploaded by a translator, a TMX file was created for that project/language/version and placed in a designated folder. We allowed uploaded files to overwrite any existing files for the same project/language/version, as we only wanted to store the most recently uploaded TM in these files, but wanted to allow different version numbers to endure. We were now close to a stage where we could aggregate all the TMs, but first needed to go back and trigger the TMX/txt process for all legacy projects as the only trigger we had until now was a project upload and many projects are dormant between releases. This was actually quite a laborious task involving hundreds of projects. Now we created a script that ran nightly. It combed the project folders and identified and gathered together all the txt format TMs for all the latest versions of projects. This made it possible to aggregate the files into a grand data set for each language. The rows of these data sets consisted of the source string, target string, version, ship date, project name, filetype, toolkit name, product domain. At SAS the development language is English and therefore the source language for localization is always English. In other words we had a set of language-specific TM silos containing the latest version of the TM for every product ever localized at SAS, in an easily readable format. An interesting feature of our automated workflow is that our translators can build toolkits and upload translated toolkits themselves. Within the scheduled period, until the sign-off date, translators can upload toolkits as many times as they wish. This ensures that the TM stored in the TM repository can be kept updated by translators to within a 24-hr period. Although the original reason for creating these TM repositories was to extract terminology, we quickly became aware of the possibilities embodied in these data sets which were updated nightly. now that we have TM repositories, what else can we use them for…? Like all localization centers, we receive many queries from translators, testers, reviewers, etc. about source/target strings. We added an extra step to the repository creation and also created large language-specific txt files (one for  each language) containing all the strings from the TMs. This made it very easy to quickly locate the offending string pairs and answer many kinds of query. Interest in accessing these files snowballed, and we realized that it would be of major benefit to our translators and others to make the files available in some way via Web access. The solution we came up with was to create a Web form that used a CGI script to send queries on to a Perl script. The Web page provides a set of search fields. Users enter search strings and these are sent for processing to a script which uses regular-expression pattern matching to find results. These are returned in a new Web page. We added a number of subsetting possibilities. Users could select a language, a domain or project or just specify All, and search in source and/or target strings. We also added various kinds of advanced search. This tool, which we call TM Search, was very heavily accessed and we were surprised at the wide range of users. Word spread within the company and we found that many other groups outside the localization community were also using it. We have also made the tool available to our language service providers and it has drastically reduced the volume of terminology queries previously sent to us. We also feel that it is contributing greatly to increased consistency in the translations. As well as having the chance to see how other translators within one’s own language have translated certain strings or terms, translators can also search in languages that are related to their own and see how the translations were tackled there. Customers have also become aware that this facility exists and have expressed interest in having access to it. Many SAS customers operate across national-language boundaries, and local offices in, say, Holland, Germany and France often sit using the same software product, carrying out the same process. But the product versions they use are localized to their own language. Having look-up access to the English source for the localized UI text strings they see on their screens would greatly assist cross-border communication. addressing an old pain: which TMs should I use? There was a constantly recurring issue in the localization process. SAS products are released in waves that we call ship events. For each new ship event we typically target between 10-20 products for localization to a number of languages. Products are only localized to a certain language if there is a good business case for doing so, so the constellation of languages changes from project to project. We had always tried to advise localizers about which existing translation memories would reduce the word count for the new localization projects we launched. A previous release of the same product was an obvious candidate, and if we knew of related products these were also recommended. However, quite often, half-way through the translation cycle, a translator would contact  us and tell us that they had discovered that attaching the translation memory from project XXX reduced the translatable word count by, for example, 6000 words. In short, we had an idea about which TMs may be of help, but this was based on what various people in the organization might remember, or vague communications filtering through to us that developers had re-used several components from product A, when creating product B. For new product releases we found it hard to recommend TMs to attach as reference material. This situation was compounded by the fact that for each language group we had a different set of TMs. We realized that the TM repository may provide us with a way to systematically investigate the nature of our TMs. Initially we toyed with the idea of a solution that created a purpose-built TM: purpose-built for each new project in hand. This would be done on the basis of strings from the repository. The format of the purpose-built TM could, for example, be TMX; though right now the repositories were stored in a database format, or txt delimited files. We looked into the possibility of going back to the TMX and aggregating this on a native XML server. With this in mind we invited an XML software provider to advise us on a solution. The architecture could be set up. The solution was sophisticated but somewhat expensive, and would require that we learn XML technologies like XQuery, XPath, etc. to an expert level, or commit ourselves to purchasing ongoing consultancy services from the provider which would bump the price up even higher. We took a look at the tools and technologies that we already had at our disposal. We had the SAS environment and the SAS programming language, and the necessary programming skills to attempt a solution. The solution we came up with turned out to be a fairly simple one. We decided to abandon the approach of creating purpose-built TMs for projects based on all the existing TM material. Because of the nature of our products, preserving the context of a translation is very important. We were concerned that if we allowed an automated process to pluck out strings individually from the repository, the resulting TM would encompass too many different contexts from different products and the result of pre-translations would be internally inconsistent. Also, this process would result in very many variant target strings, and our translators would have to sit and choose between these – a time-consuming task. Another consideration was technical in nature. Attaching a proprietary translation memory to a tool is straightforward. The proprietary TMs tend to be richer in data (some of this is invariably lost or modified when changing the TM to another format). Wherever possible we prefer to attach TMs written in the tool’s own format. What we were looking for was a solution that emulated the existing process. We wanted a process that would return results in the form of a short list of  TMs that would provide the greatest possible number of pre-translations. We could then use the proprietary TM formats and attach these to the new translations – a very quick and easy step. TM Discovery We called the new process TM Discovery. It consists of a number of steps that are scripted in the SAS programming language, which is perfectly suited to crunching large data volumes, but could just also have been written in many other languages. The basic algorithm is quite simple. Our localizations are packaged in toolkits. Each toolkit contains files that are the same file type. A typical file type could be a Java properties file. So, for the new release of product AAA, we wanted to discover which existing TMs would give us the greatest number of pre-translations for our AAA_properties toolkit for, say, French. This is the same as asking: are there any comparison matches between source strings in AAA_properties and English source strings in the French TM repository. The repository contains all the string pairs for all translations that have been carried out to date in French. The rows have a “product” variable and a “filetype” variable, but also a “toolkit_name” variable which is a combination of project and filetype. Imagine a string in the BBB product taken from the properties files: the toolkit_name would be BBB_properties. This is an important point because for us the toolkit, designated by the toolkit_name, represents the basic-level granularity. Below it we have individual files and eventually individual strings, and above it we have product, domain. This is important in deciding how to deal with duplicate strings, which we did as follows. If we look at source/target string pairs (as opposed to source strings only), a duplicate row was defined as a row in which the source string and target string were identical with the source string and target string from another row with the same toolkit_name. In other words all the string pairs at the BBB_properties level were unique. If two rows contained the same source string, but a differing target string these were not considered duplicates. Coming back to our new set of strings under investigation, AAA_properties, the first step is to copy the repository data set for French to a playpen and append the set of AAA_properties strings under investigation as new rows. Now a sort operation can be carried out on the source string variable. Duplicate source strings will be grouped. If a group contains a string from AAA_properties, then it is of interest to us. We keep these groups as SUBSET_1 and remove all other rows. Now we carry out a sort operation on the new SUBSET_1, but this time on the toolkit_name variable so that the resulting groups of strings are each a set of strings belonging to one toolkit. These groups are counted internally, and the largest group (excluding of course AAA_properties) is extracted and we can  call this SUBSET_2. For example, this could be a set of strings from the XXX product, called XXX_properties. SUBSET_2 is reduced to only include unique source strings. This is the top candidate TM and can be added to the top of a list of recommended TMs. A metric can be added to this: number of segments matched, number of words matched. Now we return to SUBSET_1 and using SUBSET_2, mark off all rows in SUBSET_1 that match a string in SUBSET_2, and remove these rows from SUBSET_1. The reason for this is that we are giving a privileged status to strings from XXX_properties (found in SUBSET_2). It appears that XXX_properties is the most similar TM to the set of strings we are investigating. We do not want to encounter these strings again as we continue our investigation through other TMs. They are, so to speak, already spoken for. Now we perform the same operation as before on the remainder of the strings and create a SUBSET_3. SUBSET_3 turns out to contain strings from YYY_xml and this is written to the list as the next-best candidate TM, and the metrics are worked out for the number of matches. There is thus no “measured” overlap between XXX_properties and YYY_xml. There may in fact be overlap, but this is not reflected in our metrics. We are interested in the total number of words that can be pre-translated using the smallest possible selection of TMs. Typically we find that after the first 3-4 recommended TMs, the number of matches falls to less than 50 words, and then very quickly to less than 5 words, petering out after 6-10 recommended TMs. The list of recommended TMs is then saved as a txt file and stored in a folder below the project information Web pages for the AAA project. A link to this file is added to the actual information pages. The operation is then carried out for other filetypes that are being localized to French for the AAA product. Typically we localize 3-8 filetypes for each product. Then the whole operation is performed for each other language into which the AAA product is being localized. All these operations are prepared and executed as a batch job. So, localizers and PMs open the project information Web pages for the AAA project, and under a designated tab are presented with a list of links. They are arranged by language, and within each language by filetype. Clicking on a link opens the list of recommended TMs that should be attached as reference material to a new project. discussion of results Typically, the TM Discovery returns a list of 3-4 useful TMs. The top candidate will most often be the previous version of the same toolkit. So for the AAA_properties_2.0 toolkit, the top recommended TM will be AAA_properties_1.0 – in other words the previous version of the same toolkit. This is a re-assuring result, as any other result would invalidate the process.  But often the secondary recommended TM contains substantial matches, and this would not have been predictable. For new products and sometimes even for new releases of existing localized products we discover major overlaps that we had not been aware of between the current project and an already translated product. Results are different for each language – sometimes quite different. As described above, this is due to the fact that the set of existing TMs varies from language to language. Previously we could give some indication about which TMs may be of benefit, but this was based on our knowledge of the English source, and didn’t always make sense for some languages. We are now able to provide a language-specific list of recommended TMs. The metrics provided by the TM Discovery process can be taken as a guide to the usefulness of the TM. The inter-relationship between the TMs on the list returned is a solid and reliable one. However, the metrics will never match completely the metrics obtained from the CAT tool once the project is created and the reference TMs leveraged. This is because the TM Discovery tool does not attempt to carry out fuzzy matching and there can be discrepancies caused by different handling of things like case-sensitivity, punctuation, etc., between the tool and the TM Discovery process. Results returned can indicate that TM in one tool-specific format should be used in another tool. This necessitates us using the TMX version of the TM, which for some tools requires an intermediate conversion, but this is no great problem. the cost We did not purchase any additional software or hardware. Naturally there is the time spent creating the various scripts, but this was done in stages over a fairly long period and did not create any noticeable resource problem. We have not carried out any in-depth analysis of the savings gained by having the system. The TM Search tool saves valuable time for many different types of user and has increased the internal consistency of localizations. Quicker troubleshooting means more bugs can be addressed in a release and this in turn means better quality. The TM Discovery tool displays a more immediate effect. In the word count reports obtained after attaching recommended TMs, it is possible to see how many words are pre-translated; words which may otherwise have been missed. At a cost of 5-6 words per Euro, for just one localization project, if an unknown TM relationship is discovered to the tune of 3000 words, if the project is earmarked for localization into 12 languages this means a saving of around 6,000 Euros. At ELC we manage around 250-300 localization projects annually.  process designers Krzysztof Jozefowicz is the Localization Manager at ELC, SAS's European Localization Center. He has worked in the localization industry for 15 years and has an M.Sc. in Computer Science from the Jagiellonian University in Cracow. Ronan Martin is the Terminology Manager at ELC, SAS's European Localization Center. He has worked with terminology and localization for the last 10 years, and previously as a language-learning consultant and translator. He has an M.A. in Educational Psychology (Pædagogik) from the University of Copenhagen.   
 Legal field • To contribute to the improvement of justice by way of technological innovation • Thousands of decisions  200 000/y in Canada X 10 for US • Multilingual, multiple fields • Access to specific decisions  Classification • Access to relevant content  Summarization  Translation 3  Research: Industry: Clients:  Partners Fisheries and Oceans Canada Canadian Coast Guard  NLP Solutions: TRANSLI • NLP Translation Solutions for Legal and governmental documents Large volumes of information High speed High accuracy 5  Machine Translation of Legal Information • NLP Translation Solutions for Courts and Tribunals • Statistical machine translation (SMT) Contextual approach and use of domain-specific vocabulary Domain adaptation for SMT SMT integrated to a fully automated Translation Management workflow 6  Certifications & Achievement • In order to protect the intellectual property of its innovative automatic summarization and translation technologies: – NLP Technologies has filed two patents – Trademarks for DecisionExpress™ and TRANSLI ™ • Certified technology by: 7  TRANSLI – Run-Time Translation Process  Original document Statistical Analysis Translation Parameters  Translated document Language Model 8  Translation Memories Translated Archives Dictionnaires/glossaries Internet Human Translations  How TRANSLI Works? Translation Parameters  Parallel corpus  New Language pair  Language Model  9  Need for translation at Federal Courts • 200 new judgments each week • 75% in English, 25% in French • By law, a translation in the other language must be provided • Delay of many months before it is available • Original goal: use SMT for draft before the official translation becomes available 10 10  Revised goal of TRANSLI • Remarkably good quality translations • By adding a post-editing process • MT output can be used as official translations – Reduces translation delay – Savings on the cost of translation • Pilot study conducted to determine the type of modifications done by the human editor – Done in collaboration with Federal Courts, UdeM and NLP 11 11 
 Christoph Rösener, Paul Schmidt University of Saarland Campus D-66 123 Saarbrücken, Germany  1. Introduction The paper introduces the results of several projects (ALLES (Advanced Long distance Education System)1, ILLU (Internet-Lehr-Lernmodule für die Übersetzer- und Dolmetscherausbildung)2 AUTOLEARN (AUTOmatic tutor for lifelong language LEARNing)3, and the soon starting COMENIUS project ICE3 ‘Integrating CALL in Early Education Environments’)4 addressed in [2], [3], [5], [6] whose objective was and is to create e-Learning sites for students of translation and second language learners realised as autonomous learning systems. ‘Autonomous learning’ means that the student can work with the system on the computer without any support by a human tutor. The innovation in all these projects is that intelligent (!) automatic correction is provided for students’ input. ‘Automatic correction’ could be seen on a trivial level on the basis of a simple matching of characters, e.g. as a gap filling exercise: ‘Fill in the right preposition’. This is actually the kind of IT-based language learning at the moment. Intelligent correction as provided in our systems goes far beyond that. It is meant to be the (automatic) detection of the kind of error that has been committed by the student and the issuing of a pedagogically useful diagnosis of the kind of error. This kind of correction is provided on different levels, orthographic, syntactic and even on a content level. Our automatic correction tools (AC tools) are based on sophisticated language technology. Automatic correction tools are made by computational linguists who develop modules for error detection on the basis of linguistic analyses. As the adaptation of sophisticated language technology to new applications or new learning units usually requires these specialists (computational linguists configuring grammars) a serious bottleneck for the usability of such systems exists at this point. In order to avoid this bottleneck a tool was developed that allows the teacher to create her own linguistic resources (to be used by the AC tools) and thus equip her own exercises with automatic correction, i.e. to generate automatically the required automatic correction tools. This is the second and most interesting component. The paper comes in the following sections: - A short description of the NLP tools and what they do - Automatic correction of second language learning - Automatic correction of translation - The tool for generating automatic correction tools - A summary and assessment of the results 2. The NLP-Resources for Automatic Correction  Automatic correction (AC) tools are based on natural language processing (NLP) technology. They deliver corrections on different levels of linguistic description, grammatical and orthographic correction, semantic and pragmatic adequacy of texts by checking the content of the students' productions. The major innovation in our tools is the application of ‘automatic intelligent language processing' (NLP) to language learning. The important point is that this is done without compromising the didactic  
Nonprofit translation activity driven by users and volunteer translators now represent a market force that easily rivals the mainstream translation and localization industries. While they still try to understand the drivers behind this nonprofit movement and occasionally attempt to tap in to these newly discovered “resources”, nonprofit translation efforts for good causes are growing at a phenomenal rate. This paper examines the case of The Rosetta Foundation as an example of a not-for-profit volunteer translation facilitator. The paper focuses on the motivating factors for volunteer translators. A survey was distributed to the several hundred volunteers who signed up as translators in the first few months of The Rosetta Foundation’s launch. The paper provides some background on what might well become the next generation of translation and localization and present the results of the survey. Finally, we will explore how The Rosetta Foundation, and other not-for-profit translation organisations might better motivate volunteers to contribute their skills and expertise. Next Generation Translation and Localization A spectre is haunting the translation industry - the spectre of translators taking charge. Translators are not only translating millions of words every day outside of the well-established commercial framework, they are also covering languages the mainstream industry has continued to ignore, although they are spoken by millions of people, and their work has been truly life-changing for many of their “clients”. Life-saving, health related information has long been easily available on the web to speakers of English and the other G20 languages. This information is now also becoming available in languages such as Gujarati, Hindi, Kannada, Swahili, Tamil and Telugu thanks to the efforts of volunteer translators. Industry analysts do not feel comfortable estimating the number of words translated by the companies they are selling their costly reports to (Beninatto & DePalma 2008a). However, they estimate the size of the global translation market (US$19b for 2010), calculate the compound annual average growth of the sector (14.6% for 2008-2012), and use the Herfindahl-Hirschman Index (HHI) to point out the extraordinary level of fragmentation in the industry (Beninatto & DePalma 2008b). Surprisingly, they completely ignore what surely has become one of the most dramatic phenomena in the industry over the past five years. The Tipping Point While the mainstream industry started to discuss “crowd-sourcing” in 2007 and is still trying to figure out its reaction to it, users and volunteer translators have quietly taken control (following a keynote by Jeff Howe at the Localization World conference in Seattle). Thousands of translators have long been involved in the localization of open source success stories such as Mozilla and Open Office, providing access to basic IT tools and technologies to those who do not represent a market for large multinational companies. With better collaboration tools and translation  technologies becoming freely available and internet access improving, the volunteer translation movement has now reached a tipping point. We estimate that volunteer work ranks way ahead of the Top 25 translation companies reported by Beninatto and DePalma. The number of volunteer translators and the number of words they are translating is a multiple of that reported by any commercial service provider. However, the most important aspect of their work is not its volume measured in commercial terms. It is its impact on the languages covered, on the lives of the people they translate for, on their economic well being, on their access to justice, on their environment and on their health. They use their professional experience and skills to provide access to knowledge and information to people independent of their social, economic, linguistic or geographic background; people that have been ignored by the mainstream translation and localization industry because they do not represent a viable business case. The Illusion of Control In 2003, an illegal translation of Harry Potter and the Order of the Phoenix, appeared in Venezuela soon after the release of the English version and five months before the scheduled release of the Spanish translation. The MTC India Youth Icons used to be business magnates, cricket players or actors. In 2007, the award did not go to a person but to a website: Orkut, the social networking site translated by its users into Hindi, Marathi, Bengali, Tamil and Telugu. Industry experts such as Greg Oxton in The Power and Value of On-line Communities at the AGIS’09 event have been calling on digital publishers to give up “their illusion of control”. The localization and translation decision is increasingly not being taken by the publishers but by users. It is increasingly based on real user requirements and not on carefully crafted business cases. Content and software are translated by those who really want and need it into their languages, often well ahead of versions translated by the professionals based on “solid” business cases. The user is taking charge and examples for this are many. The best known example in the commercial sector is Facebook. The site grew from 34 million international users in early 2008 to well over 500 million by the end of 2010. Facebook now “speaks” more than 100 languages – all supplied by its users. Other examples are content, sites, and applications translated by the crowd for Symantec, Sun and Microsoft. In Open Source, users have translated “their” applications into well over 100 languages. Open Office, for example, is available in around 120 languages, among them Tigrinya, Sidama and Papiamento – hardly languages backed by a “solid” business case. At Wikimania in July 2010, Google reported that they are working with volunteer translators across India, the Middle East and Africa to translate more than 16 million words for Wikipedia into Arabic, Gujarati, Hindi, Kannada, Swahili, Tamil and Telugu. Within three months, 600,000 words from more than 100 articles in the English Wikipedia were translated, growing Hindi Wikipedia by almost 20 percent. Since 2008, this effort alone produced a total number of 16 million translated words (Wikimania 2010). Reports from the commercial use of crowd-sourcing indicate that languages in the socalled “long-tail” of the language curve benefit most from crowd-sourcing efforts, suggesting that speakers of lesser-used languages are better motivated to act as volunteer translators. For example, Rickard (2009), in his paper on the crowd-sourcing translation effort for Symantec’s PC Tools application, reported that 78% of the contributions in the first three months were in the “long-tail”. Rickard divides contributors into three groups (students, power users and professional translators) and suggests that they are all motivated by different factors (e.g. seeking experience, being able to shape the product, or keeping their skills sharp). On a more general level, Shirky argues that volunteers are driven by the need to fill their “cognitive surplus” (Shirky 2010). Nonprofit Translation - Motivation The Rosetta Foundation (TRF) has listed nonprofit localisation initiatives around the world (http://www.therosettafoundation.org/index.php/en/ourcolleagues) and invites translators to  provide additional information about initiatives they are aware of. The list provides more than 30 links to a wide variety of sites inviting translators to volunteer their services. Some of these provide access to a large number of initiatives seeking translators, such as idealist.org or United Way; they are run by nonprofit translation services, such as the American Alliance Language Services, HSA Translations or ITC Global Translations; they are linked to specific projects, such as TED, Ashoka, Handicap International, Global Voices or Wiser Earth; or they position themselves as brokers of volunteer translation services for good causes, such as Grassroots.org, Translations for Progress, Translators without Borders or The Rosetta Foundation. This paper examines the case of The Rosetta Foundation as an example of a not-for-profit volunteer translation facilitator. The organization was established in 2009 to “relieve poverty, support healthcare, develop education and promote justice through access to information and knowledge across the languages of the world”. It was established and is supported by individuals and companies with many decades of experience in the localization sector who had become aware that current mainstream localization approaches actively promoted the exclusion of large numbers of people and languages from access to information and knowledge, information vital for their health, their freedom, their education and their economic well-being. The Foundation is working on a web ecosystem that aims to support nonprofit localization and translation communities, driven by the requirements of its nonprofit clients and its volunteer translator community. In 2011, The Rosetta Foundation aims to translate 40 million words. The focus here is on the motivating factors for volunteer translators. A survey was distributed to the several hundred volunteers who signed up as translators in the first few months of The Rosetta Foundation’s launch. This survey examined what relevance volunteers give to various motivating factors such as improvement of language and translation skills, acquisition of translation experience, support of the stated cause of The Rosetta Foundation, support for lesserused languages and intellectual stimulation. The survey also asked volunteers to provide feedback on what might motivate them to contribute even more, including factors such as payment, recognition on the web site, feedback on their work and free gifts. The remainder of this paper will present the results of this survey and explore how The Rosetta Foundation, and other not-forprofit volunteer-based translation organisations, might better motivate volunteers to contribute their skills and expertise. Volunteers and Theories on Motivation Shirky (2010), in his book entitled Cognitive Surplus: Creativity and Generosity in a Connected Age, posits that the era of one-way consumption of content is over and that people are so understimulated that many are now willing to create content for intellectual stimulation. However, the urge to do something with the so-called cognitive surplus is not motivated by boredom. In 1972, Deci substantiated the theory that motivation is both extrinsic and intrinsic. Extrinsic motivation refers to cases where the activity itself is a reward whereas intrinsic motivation refers to activities where the participant gets paid (Shirky 2010: 72). Deci also identified two intrinsic emotions: the desire to be autonomous and the desire to be competent. Shirky (2010) refers to the former as “personal motivation” and to the latter as “social motivation” and he points out that while we have always wanted to be autonomous, connected and competent, new social media have become an enabler for those goals (ibid: 84). Benkler and Nissenbaum (2006) identified two broad social emotions, one centring around connectedness or membership and the other around sharing and generosity. Shirky (2010: 73) also refers to a study by Frey and Goette (1999) which demonstrated that in real world situations, where money was offered as a reward for volunteering, it depressed the number of hours the average volunteer contributed. In our study of TRF volunteers, we were interested in finding out if any of these theories on motivation were evident. For example, was there evidence of personal and social motivation,  of desire for autonomy and competence, of connectedness and generosity? And, would payment have a negative impact on TRF volunteers’ willingness to contribute? Most importantly, what can we learn about motivating factors so that volunteers will continue to donate their time? The Survey In order to understand what was motivating people to sign up for and undertake unpaid translation work for TRF we conducted a survey of the volunteers who were signed up by July 2010. The link to the survey was sent by e-mail to the addresses provided by volunteers.1 Volunteers were asked to answer five questions which sought to probe their motivations for volunteering as translators and to find out what might motivate them to continue volunteering in the future. The survey was limited to five questions only in order to keep the time required for answering to a minimum. Online surveys have a notoriously low response rate. However, the response of the Rosetta Volunteer Community was an impressive 54% (139 responses out of 257 volunteers). This high response rate is most likely due to the fact that the volunteers are already highly motivated and reasonably committed to the cause. Volunteer Profile 222 volunteers signed up under the category of “professional translators”, while 35 signed up as “amateurs”. Of the professional translators, 82% were female. However, for the amateur group, the bias towards female volunteers was reduced, with only 54% being female. Volunteers were asked to indicate their level of relevant work experience and they could chose from categories “beginner”, “some”, “a bit out of practice”, “a lot”. 42% indicated that they had “a lot” of experience, while 18% indicated they were “beginners” (even though they had signed up as professional translators). Only 2% stated they were a bit out of practice, which leaves 38% as having “some” experience. The relatively high numbers of volunteers selfidentifying as having beginner level or only some experience might explain the reason for some of the motivating factors identified in the survey (see below). Those who signed up as amateurs were also asked to state if they had experience, some experience or no experience with translation. Only 23% said they had no experience, while 46% said they had some. When asked how many hours per week the translators might be able to dedicate to The Rosetta Foundation, the average was an impressive 8 hours. As some people listed values as high as 20 and 35 hours per week here, it is possible that the question (“How many hours per week do you think you could offer your translation services?”) was misunderstood and some volunteers were indicating how much time they normally spend on translation per week. The average value of 8 hours per week should, therefore, probably be reduced as this might prove to be an unreasonable expectation. Language and domain profile 182 of the professional volunteers identified English as a source language, 71 identified French, 64 German and 60 Spanish.2 Other languages mentioned as Source Languages included Arabic, Bulgarian, Belorussian, Catalan, Czech, Croatian, Chinese, Filipino, Danish, Dutch, Italian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Swedish and Ukranian, but the number who listed these languages was considerably smaller. For Target Languages, 90 listed English, 87 Spanish, 31 French and 23 German. Additional target languages mentioned were Arabic, Armenian Catalan, Croatian, Chinese, 
On one hand, fixed expressions can be extremely transparent, such as if and when and from the top down, but they still have to be recognized as fixed. As we found by the analysis of a bilingual translation corpus, non-recognition occasionally leads to rather infelicitous mistakes in translation. On the other hand, many fixed expressions (e.g. time and effort, in process, over time) cannot be found in dictionaries and some more recently established expressions, such as up line support, are even less likely to be included in dictionaries (Colson 
In recent years, it is becoming more and more clear that the localisation industry does not have the neces-­‐ sary manpower to satisfy the increasing demand for high-­‐quality translation. This has fuelled the search new and existing technologies that would increase translator throughput. As Translation Memory (TM) systems are the most commonly employed tool by translators, a number of enhancements are available to assist them in their job. One such enhancement would be to show the translator which parts of the sen-­‐ tence that needs to be translated match which parts of the fuzzy match suggested by the TM. For this information to be used, however, the translators have to carry it over to the TM translation themselves. In this paper, we present a novel methodology that can automatically detect and highlight the segments that need to be modiﬁed in a TM-­‐suggested translation. We base it on state-­‐of-­‐the-­‐art sub-­‐tree alignment technology (Zhechev, 2010) that can produce aligned phrase-­‐based-­‐tree pairs from unannotated data. Our system operates in a three-­‐step process. First, the fuzzy match selected by the TM and its translation are aligned. This lets us know which segments of the source-­‐language sentence correspond to which segments in its translation. In the second step, the fuzzy match is aligned to the input sentence that is currently being translated. This tells us which parts of the input sentence are available in the fuzzy match and which still need to be translated. In the third step, the fuzzy match is used as an intermediary, through which the alignments between the input sentence and the TM translation are established. In this way, we can detect with precision the segments in the suggested translation that the translator needs to edit and highlight them appropriately to set them apart from the segments that are already good translations for parts of the input sentence. Additionally, we can show the alignments—as detected by our system—between the input and the translation, which will make it even easier for the translator to post-­‐edit the TM suggestion. This alignment information can additionally be used to pre-­‐translate the mismatched segments, further reduc-­‐ ing the post-­‐editing load.  1. Introduc&on As the world becomes increasingly i  nterconnected, ideas, products and services need to be c  ommunicated to the w  idest audience possible. This requires localisation for a  s many languages, cultures and locales as possible, with translation being one of the main parts of the localisation process. Because of this, t  he amount of data that n  eeds professional high-­‐quality translation is continuing to increase well beyond the ca-­‐ pacity of the world’s human translators. Current e  ﬀorts in the localisation industry a  re mostly d  irected at the r  eduction of the amount of data that needs to be translated manually from scratch. Such eﬀorts mainly include the use of Translation Memory (TM) systems, where earlier transla-­‐ tions are stored in a database and oﬀered as suggestions when new data needs to be translated. As TM systems were originally limited to providing translations only for (almost) exact m  atches o  f the new data, the integration o  f Machine T  ranslation (MT)      
In this paper, we present a hybrid approach to align single words, compound words and idiomatic expressions from bilingual parallel corpora. The objective is to develop, improve and maintain automatically translation lexicons. This approach combines linguistic and statistical information in order to improve word alignment results. The linguistic improvements taken into account refer to the use of an existing bilingual lexicon, named entities recognition, grammatical tags matching and detection of syntactic dependency relations between words. Statistical information refer to the number of occurrences of repeated words, their positions in the parallel corpus and their lengths in terms of number of characters. Single-word alignment uses an existing bilingual lexicon, named entities and cognates detection and grammatical tags matching. Compound-word alignment consists in establishing correspondences between the compound words of the source sentence and the compound words of the target sentences. A syntactic analysis is applied on the source and target sentences in order to extract dependency relations between words and to recognize compound words. Idiomatic expressions alignment starts with a monolingual term extraction for each of the source and target languages, which provides a list of sequences of repeated words and a list of potential translations. These sequences are represented with vectors which indicate their numbers of occurrences and the numbers of segments in which they appear. Then, the translation relation between the source and target expressions are evaluated with a distance metric. The single and compound word aligners have been evaluated on a subset of 1103 sentences in English and French of the JOC (Official Journal of the European Community) corpus . The obtained results showed that these aligners generate a translation lexicon with 90 % of precision for single words and 84 % of precision for compound words. We evaluated the idiomatic expressions aligner on a subset of the Canadian Parliament Hansard corpus and we obtained a precision of 81%. 
The availability of huge amounts of online opinions has created a new need to develop effective query-based opinion summarizers to analyze this information in order to facilitate decision making at every level. To develop an effective opinion summarization approach, we have targeted to resolve specifically Question Irrelevancy and Discourse Incoherency problems which have been found to be the most frequently occurring problems for opinion summarization. To address these problems, we have introduced a hybrid approach by combining text schema and rhetorical relations to exploit intra-sentential rhetorical relations. To evaluate our approach, we have built a system called BlogSum and have compared BlogSum-generated summaries after applying rhetorical structuring to BlogSum-generated candidate sentences without utilizing rhetorical relations using the Text Analysis Conference (TAC) 2008 data for summary contents. Evaluation results show that our approach improves summary contents by reducing question irrelevant sentences.
Much of the work on Statistical Machine Translation (SMT) from morphologically rich languages has shown that morphological tokenization and orthographic normalization help improve SMT quality because of the sparsity reduction they contribute. In this paper, we study the effect of these processes on SMT when translating into a morphologically rich language, namely Arabic.We explore a space of tokenization schemes and normalization options. We only evaluate on detokenized and orthographically correct (enriched) output. Our results show that the best performing tokenization scheme is that of the Penn Arabic Treebank. Additionally, training on orthographically normalized (reduced) text then jointly enriching and detokenizing the output outperforms training on enriched text.
We improve our recently proposed technique for integrating Arabic verb-subject constructions in SMT word alignment (Carpuat et al., 2010) by distinguishing between matrix (or main clause) and non-matrix Arabic verb-subject constructions. In gold translations, most matrix VS (main clause verb-subject) constructions are translated in inverted SV order, while non-matrix (subordinate clause) VS constructions are inverted in only half the cases. In addition, while detecting verbs and their subjects is a hard task, our syntactic parser detects VS constructions better in matrix than in non-matrix clauses. As a result, reordering only matrix VS for word alignment consistently improves translation quality over a phrase-based SMT baseline, and over reordering all VS constructions, in both medium- and large-scale settings. In fact, the improvements obtained by reordering matrix VS on the medium-scale setting remarkably represent 44{\%} of the gain in BLEU and 51{\%} of the gain in TER obtained with a word alignment training bitext that is 5 times larger.
This paper presents an empirical study on the application of the maximum entropy approach for part-of-speech tagging of Vietnamese text, a language with special characteristics which largely distinguish it from occidental languages. Our best tagger explores and includes useful knowledge sources for tagging Vietnamese text and gives a 93.40{\%}overall accuracy and a 80.69{\%}unknown word accuracy on a test set of the Vietnamese treebank. Our tagger significantly outperforms the tagger that is being used for building the Vietnamese treebank, and as far as we are aware, this is the best tagging result ever published for the Vietnamese language.
We will demonstrate iMAGs (interactive Multilingual Access Gateways), in particular on a scientific laboratory web site and on the Greater Grenoble (La M{\'e}tro) web site.
The system demo introduces Grail, a general-purpose parser for multimodal categorial grammars, with special emphasis on recent research which makes Grail suitable for wide-coverage French syntax and semantics. These developments have been possible thanks to a categorial grammar which has been extracted semi-automatically from the Paris 7 treebank and a semantic lexicon which maps word, part-of-speech tags and formulas combinations to Discourse Representation Structures.
Description of Moz, a translation support system designed for texts exhibiting a high proportion of structured and semi-structured terminological content. The system comprises a web-based collaborative translation memory, with high recall via subsentential linguistic analysis and facilities for messaging and quality assurance. It is in production use, translating some 140,000 words per week.
General purpose, high quality and fully automatic MT is believed to be impossible. We are interested in scriptural translation problems, which are weak sub-problems of the general problem of translation. We introduce the characteristics of the weak problems of translation and of the scriptural translation problems, describe different computational approaches (finite-state, statistical and hybrid) to solve these problems, and report our results on several combinations of Indo-Pak languages and writing systems.
In Artificial Intelligence, analogy is used as a non exact reasoning technique to solve problems, for natural language processing, for learning classification rules, etc. This paper is interested in the analogical proportion, a simple form of the reasoning by analogy, and presents some of its uses in machine learning for NLP. The analogical proportion is a relation between four objects that expresses that the way to transform the first object into the second is the same as the way to transform the third in the fourth. We firstly give definitions about the general notion of analogical proportion between four objects. We give a special focus on objects structured as ordered and labeled trees, with an original definition of analogy based on optimal alignment. Secondly, we present two algorithms which deal with tree analogical matching and solving analogical equations between trees. We show their use in two applications : the learning of the syntactic tree (parsing) of a sentence and the generation of prosody for synthetic speech.
The paper describes the development of a wide-coverage type-logical grammar for French, which has been extracted from the Paris 7 treebank and received a significant amount of manual verification and cleanup. The resulting treebank is evaluated using a supertagger and performs at a level comparable to the best supertagging results for English.
Question Generation (QG) and Question Answering (QA) are some of the many challenges for natural language understanding and interfaces. As humans need to ask good questions, the potential benefits from automated QG systems may assist them in meeting useful inquiry needs. In this paper, we consider an automatic Sentence-to-Question generation task, where given a sentence, the Question Generation (QG) system generates a set of questions for which the sentence contains, implies, or needs answers. To facilitate the question generation task, we build elementary sentences from the input complex sentences using a syntactic parser. A named entity recognizer and a part of speech tagger are applied on each of these sentences to encode necessary information.We classify the sentences based on their subject, verb, object and preposition for determining the possible type of questions to be generated. We use the TREC-2007 (Question Answering Track) dataset for our experiments and evaluation.
For a long time, machine translation and professional translation vendors have had a contentious relation. However, new tools, computing platforms, and business models are changing the fundamentals of this relationship. I will review the main trends in the area while emphasizing both past causes of failure and main drivers of success.
The exploitation of large corpora to create and populate shared translation resources has been hampered in two areas: first, practical problems ({``}locked-in{''} data, ineffective exchange formats, client reservations); and second, ethical and legal problems. Recent developments, notably online collaborative translation environments (Desillets, 2007) and greater industry openness, might have been expected to highlight such issues. Yet the growing use of shared data is being addressed only gingerly. Good reasons lie behind the failure to broach the ethics of shared resources. The issues are challenging: confidentiality, ownership, copyright, authorial rights, attribution, the law, protectionism, costs, fairness, motivation, trust, quality, reliability. However, we argue that, though complex, these issues should not be swept under the carpet. The huge demand for translation cannot be met without intelligent sharing of resources (Kelly, 2009). Relevant ethical considerations have already been identified in translation and related domains, in such texts as Codes of Ethics, international conventions and declarations, and Codes of Professional Conduct; these can be useful here. We outline two case studies from current industry initiatives, highlighting their ethical implications. We identify questions which users and developers should be asking and relate these to existing debates and codes as a practical framework for their consideration.
The purpose of this work is to show how machine translation can be integrated into professional translation environments using two possible workflows. In the first workflow we demonstrate the real-time, sentence-by-sentence use of both rule-based and statistical machine translation systems with translation memory programs. In the second workflow we present a way of applying machine translation to full translation projects beforehand. We also compare and discuss the efficiency of statistical and rule-based machine translation systems, and propose some ideas about how these systems could be combined with translation memory technologies into a unified translation application.
We present two methods that merge ideas from statistical machine translation (SMT) and translation memories (TM). We use a TM to retrieve matches for source segments, and replace the mismatched parts with instructions to an SMT system to fill in the gap. We show that for fuzzy matches of over 70{\%}, one method outperforms both SMT and TM baselines.
Although Machine Translation (MT) has been attracting more and more attention from the translation industry, the quality of current MT systems still requires humans to post-edit translations to ensure their quality. The time necessary to post-edit bad quality translations can be the same or even longer than that of translating without an MT system. It is well known, however, that the quality of an MT system is generally not homogeneous across all translated segments. In order to make MT more useful to the translation industry, it is therefore crucial to have a mechanism to judge MT quality at the segment level to prevent bad quality translations from being post-edited within the translation workflow. We describe an approach to estimate translation post-editing effort at sentence level in terms of Human-targeted Translation Edit Rate (HTER) based on a number of features reflecting the difficulty of translating the source sentence and discrepancies between the source and translation sentences. HTER is a simple metric and obtaining HTER annotated data can be made part of the translation workflow. We show that this approach is more reliable at filtering out bad translations than other simple criteria commonly used in the translation industry, such as sentence length.
This paper focuses on the relationship between source text characteristics (ambiguity, complexity and style compliance) and machine-translation post-editing effort (both temporal and technical). Post-editing data is collected in a traditional translation environment and subsequently plotted against textual scores produced by a range of systems. Our findings show some strong correlation between ambiguity and complexity scores and technical post-editing effort, as well as moderate correlation between one of the style guide compliance scores and temporal post-editing effort.
This paper describes our work on building and employing Statistical Machine Translation systems for TV subtitles in Scandinavia. We have built translation systems for Danish, English, Norwegian and Swedish. They are used in daily subtitle production and translate large volumes. As an example we report on our evaluation results for three TV genres. We discuss our lessons learned in the system development process which shed interesting light on the practical use of Machine Translation technology.
This paper proposes a novel method for exploiting comparable documents to generate parallel data for machine translation. First, each source document is paired to each sentence of the corresponding target document; second, partial phrase alignments are computed within the paired texts; finally, fragment pairs across linked phrase-pairs are extracted. The algorithm has been tested on two recent challenging news translation tasks. Results show that mining for parallel fragments is more effective than mining for parallel sentences, and that comparable in-domain texts can be more valuable than parallel out-of-domain texts.
In this paper, we present the result of our work on improving the preprocessing for German-English statistical machine translation. We implemented and tested various improvements aimed at i) converting German texts to the new orthographic conventions; ii) performing a new tokenization for German; iii) normalizing lexical redundancy with the help of POS tagging and morphological analysis; iv) splitting German compound words with frequency based algorithm and; v) reducing singletons and out-of-vocabulary words. All these steps are performed during preprocessing on the German side. Combining all these processes, we reduced 10{\%} of the singletons, 2{\%} OOV words, and obtained 1.5 absolute (7{\%} relative) BLEU improvement on the WMT 2010 German to English News translation task.
Current Statistical Machine Translation (SMT) systems translate texts sentence by sentence without considering any cross-sentential context. Assuming independence between sentences makes it difficult to take certain translation decisions when the necessary information cannot be determined locally. We argue for the necessity to include crosssentence dependencies in SMT. As a case in point, we study the problem of pronominal anaphora translation by manually evaluating German-English SMT output. We then present a word dependency model for SMT, which can represent links between word pairs in the same or in different sentences. We use this model to integrate the output of a coreference resolution system into English-German SMT with a view to improving the translation of anaphoric pronouns.
Currently most state-of-the-art statistical machine translation systems present a mismatch between training and generation conditions. Word alignments are computed using the well known IBM models for single-word based translation. Afterwards phrases are extracted using extraction heuristics, unrelated to the stochastic models applied for finding the word alignment. In the last years, several research groups have tried to overcome this mismatch, but only with limited success. Recently, the technique of forced alignments has shown to improve translation quality for a phrase-based system, applying a more statistically sound approach to phrase extraction. In this work we investigate the first steps to combine forced alignment with a hierarchical model. Experimental results on IWSLT and WMT data show improvements in translation quality of up to 0.7{\%} BLEU and 1.0{\%} TER.
This paper describes a technique to exploit multiple pivot languages when using machine translation (MT) on language pairs with scarce bilingual resources, or where no translation system for a language pair is available. The principal idea is to generate intermediate translations in several pivot languages, translate them separately into the target language, and generate a consensus translation out of these using MT system combination techniques. Our technique can also be applied when a translation system for a language pair is available, but is limited in its translation accuracy because of scarce resources. Using statistical MT systems for the 11 different languages of Europarl, we show experimentally that a direct translation system can be replaced by this pivot approach without a loss in translation quality if about six pivot languages are available. Furthermore, we can already improve an existing MT system by adding two pivot systems to it. The maximum improvement was found to be 1.4{\%} abs. in BLEU in our experiments for 8 or more pivot languages.
Phrase-based systems deeply depend on the quality of their phrase tables and therefore, the process of phrase extraction is always a fundamental step. In this paper we present a general and extensible phrase extraction algorithm, where we have highlighted several control points. The instantiation of these control points allows the simulation of previous approaches, as in each one of these points different strategies/heuristics can be tested. We show how previous approaches fit in this algorithm, compare several of them and, in addition, we propose alternative heuristics, showing their impact on the final translation results. Considering two different test scenarios from the IWSLT 2010 competition (BTEC, Fr-En and DIALOG, Cn-En), we have obtained an improvement in the results of 2.4 and 2.8 BLEU points, respectively.
In this paper, we investigate different methodologies of Arabic segmentation for statistical machine translation by comparing a rule-based segmenter to different statistically-based segmenters. We also present a new method for segmentation that serves the need for a real-time translation system without impairing the translation accuracy.
Sign languages represent an interesting niche for statistical machine translation that is typically hampered by the scarceness of suitable data, and most papers in this area apply only a few, well-known techniques and do not adapt them to small-sized corpora. In this paper, we will propose new methods for common approaches like scaling factor optimization and alignment merging strategies which helped improve our baseline. We also conduct experiments with different decoders and employ state-of-the-art techniques like soft syntactic labels as well as trigger-based and discriminative word lexica and system combination. All methods are evaluated on one of the largest sign language corpora available.
Overview WFST formulation of Hiero-style translation Alternative implementation to Cube Pruning (CPH) Works with lattices rather than individual translation hypotheses Based on Google OpenFST toolkit Fast Hiero grammars language-speciﬁc grammars to avoid pruning, search errors, ... Fast linearized lattice based minimum Bayes risk decoding (LMBR) with weighted ﬁnite state transducers Joint work with Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood, Juan Pino  Department of Engineering University of Cambridge  Hierarchical Phrase-Based Translation with Weighted Finite State Transducers IWSLT 2010, Paris, France  
William Byrne, Cambridge University I will present recent work in statistical machine translation which uses Weighted Finite-State Transducers (WFSTs) to implement a variety of search and estimation algorithms. I will describe HiFST, a lattice-based decoder for hierarchical phrase-based statistical machine translation. The decoder is implemented with standard WFST operations as an alternative to the well-known cube pruning procedure. We ﬁnd that the use of WFSTs in translation leads to fewer search errors, better parameter optimization, and improved translation performance. We also ﬁnd that the direct generation of target language lattices under Hiero translation grammars can improve subsequent rescoring procedures, yielding further gains with long-span language models and Minimum Bayes Risk decoding. Resources for adding semantics to machine translation Jan Hajicˇ, Charles University in Prague Current (Statistical) Machine Translation systems rarely go beyond morphology, lemmatization, phrases or syntax. One of the possible ways to direct research in the xix  near future is use semantics in one way or the other, whether as semantics features or factors within the successful phrase-based or hierarchical systems, or in hybrid systems, or otherwise. However, semantic features have to be learnt from annotated data, at least until unsupervised learning can replace all the expensive annotation projects. In the talk, I will present the basics of the family of Prague dependency treebanks (currently available for Czech, English and Arabic), which to various extents provide combined manual annotation of syntax and semantics based on the dependency framework, but general enough to be used in systems of all types, including the classical non-hierarchical SMT systems where only word-based features can be incorporated into the model. One of the corpora available is speciﬁcally aimed at machine translation, since it is a parallel, fully manually annotated Czech-English corpus, which consists of the Penn Treebank texts (preserving also the original annotation) and its professional translation to Czech. Speciﬁc resources aimed at spoken language analysis will also be presented, even though no parallel version exists yet. These are based on the "speech reconstruction" idea by Fred Jelinek and his students, which was incorporated into a dialog corpus of Czech and English that was then developed at Charles University. The Quaero program: Multilingual and multimedia technologies Jean-Luc Gauvain, LIMSI/CNRS The goal of the Quaero programme is to promote research and industrial innova- tion for multilingual multimedia content processing and information management and access. The Core Technology Cluster (CTC) groups the research activities in Quaero which aim improve the state-of-the-art in automatic multimedia document structuring and indexing, and to develop and evaluate the core technologies. The core technologies cover text processing, translation, audio and speech processing, image and video processing, audio and video ﬁngerprinting, cross-modal processing, and search and navigation methods. Generic technologies are being developed that can be applied to a wide range of documents, along with tools to enhance portability. This talk will overview the core technologies being developed and evaluated in Quaero, with an emphasis on multilingual language technologies (NE, Q&A, MT, STT, SPKR). The talk will conclude with some examples cases of the CTC research being incorporated in application prototypes. The main applications areas are general search engines for multimedia documents; portals for personal multimedia document management; online services to access audiovisual archives and digital libraries; advanced tools for the content production and management chain; and highly personalized content-on-demand services. xx Proceedings of the 7th International Workshop on Spoken Language Translation 
William Byrne, Cambridge University I will present recent work in statistical machine translation which uses Weighted Finite-State Transducers (WFSTs) to implement a variety of search and estimation algorithms. I will describe HiFST, a lattice-based decoder for hierarchical phrase-based statistical machine translation. The decoder is implemented with standard WFST operations as an alternative to the well-known cube pruning procedure. We ﬁnd that the use of WFSTs in translation leads to fewer search errors, better parameter optimization, and improved translation performance. We also ﬁnd that the direct generation of target language lattices under Hiero translation grammars can improve subsequent rescoring procedures, yielding further gains with long-span language models and Minimum Bayes Risk decoding. Resources for adding semantics to machine translation Jan Hajicˇ, Charles University in Prague Current (Statistical) Machine Translation systems rarely go beyond morphology, lemmatization, phrases or syntax. One of the possible ways to direct research in the xix  near future is use semantics in one way or the other, whether as semantics features or factors within the successful phrase-based or hierarchical systems, or in hybrid systems, or otherwise. However, semantic features have to be learnt from annotated data, at least until unsupervised learning can replace all the expensive annotation projects. In the talk, I will present the basics of the family of Prague dependency treebanks (currently available for Czech, English and Arabic), which to various extents provide combined manual annotation of syntax and semantics based on the dependency framework, but general enough to be used in systems of all types, including the classical non-hierarchical SMT systems where only word-based features can be incorporated into the model. One of the corpora available is speciﬁcally aimed at machine translation, since it is a parallel, fully manually annotated Czech-English corpus, which consists of the Penn Treebank texts (preserving also the original annotation) and its professional translation to Czech. Speciﬁc resources aimed at spoken language analysis will also be presented, even though no parallel version exists yet. These are based on the "speech reconstruction" idea by Fred Jelinek and his students, which was incorporated into a dialog corpus of Czech and English that was then developed at Charles University. The Quaero program: Multilingual and multimedia technologies Jean-Luc Gauvain, LIMSI/CNRS The goal of the Quaero programme is to promote research and industrial innova- tion for multilingual multimedia content processing and information management and access. The Core Technology Cluster (CTC) groups the research activities in Quaero which aim improve the state-of-the-art in automatic multimedia document structuring and indexing, and to develop and evaluate the core technologies. The core technologies cover text processing, translation, audio and speech processing, image and video processing, audio and video ﬁngerprinting, cross-modal processing, and search and navigation methods. Generic technologies are being developed that can be applied to a wide range of documents, along with tools to enhance portability. This talk will overview the core technologies being developed and evaluated in Quaero, with an emphasis on multilingual language technologies (NE, Q&A, MT, STT, SPKR). The talk will conclude with some examples cases of the CTC research being incorporated in application prototypes. The main applications areas are general search engines for multimedia documents; portals for personal multimedia document management; online services to access audiovisual archives and digital libraries; advanced tools for the content production and management chain; and highly personalized content-on-demand services. xx Proceedings of the 7th International Workshop on Spoken Language Translation 
In this paper, we describe AppTek{'}s new APT machine translation system that we employed in the IWSLT 2010 evaluation campaign. This year, we participated in the Arabic-to-English and Turkish-to-English BTEC tasks. We discuss the architecture of the system, the preprocessing steps and the experiments carried out during the campaign. We show that competitive translation quality can be obtained with a system that can be turned into a real-life product without much effort.
 39 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  40 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  41 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  42 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
 47 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  48 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  49 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  50 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
This year FBK took part in the BTEC translation task, with source languages Arabic and Turkish and target language English, and in the new TALK task, source English and target French. We worked in the framework of phrase-based statistical machine translation aiming to improve coverage of models in presence of rich morphology, on one side, and to make better use of available resources through data selection techniques. New morphological segmentation rules were developed for Turkish-English. The combination of several Turkish segmentation schemes into a lattice input led to an improvement wrt to last year. The use of additional training data was explored for Arabic-English, while on the English to French task improvement was achieved over a strong baseline by automatically selecting relevant and high quality data from the available training corpora.
In this paper we explore the contribution of the use of two Arabic morphological analyzers as preprocessing tools for statistical machine translation. Similar investigations have already been reported for morphologically rich languages like German, Turkish and Arabic. Here, we focus on the case of the Arabic language and mainly discuss the use of the G-LexAr analyzer. A preliminary experiment has been designed to choose the most promising translation system among the 3 G-LexAr-based systems, we concluded that the systems are equivalent. Nevertheless, we decided to use the lemmatized output of G-LexAr and use its translations as primary run for the BTEC AE track. The results showed that G-LexAr outputs degrades translation compared to the basic SMT system trained on the un-analyzed corpus.
 69 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  70 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  71 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  72 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
 75 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  76 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  77 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  78 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
In this paper we describe the Instituto de Engenharia de Sistemas e Computadores Investigac ̧a ̃o e Desenvolvimento (INESC-ID) system that participated in the IWSLT 2010 evaluation campaign. Our main goal for this evaluation was to employ several state-of-the-art methods applied to phrase-based machine translation in order to improve the translation quality. Aside from the IBM M4 alignment model, two constrained alignment models were tested, which produced better overall results. These results were further improved by using weighted alignment matrixes during phrase extraction, rather than the single best alignment. Finally, we tested several filters that ruled out phrase pairs based on puntuation. Our system was evaluated on the BTEC and DIALOG tasks, having achieved a better overall ranking in the DIALOG task.
This paper presents the submissions of the PRHLT group for the evaluation campaign of the International Workshop on Spoken Language Translation. We focus on the development of reliable translation systems between syntactically different languages (DIALOG task) and on the efficient training of SMT models in resource-rich scenarios (TALK task).
 95 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  96 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  97 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  98 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
 101 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  102 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  103 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  104 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
This paper describes LIMSI{'}s Statistical Machine Translation systems (SMT) for the IWSLT evaluation, where we participated in two tasks (Talk for English to French and BTEC for Turkish to English). For the Talk task, we studied an extension of our in-house n-code SMT system (the integration of a bilingual reordering model over generalized translation units), as well as the use of training data extracted from Wikipedia in order to adapt the target language model. For the BTEC task, we concentrated on pre-processing schemes on the Turkish side in order to reduce the morphological discrepancies with the English side. We also evaluated the use of two different continuous space language models for such a small size of training data.
This paper describes the two systems developed by the LIUM laboratory for the 2010 IWSLT evaluation campaign. We participated to the new English to French TALK task. We developed two systems, one for each evaluation condition, both being statistical phrase-based systems using the the Moses toolkit. Several approaches were investigated.
This paper describes the MIRACL statistical Machine Translation system and the improvements that were developed during the IWSLT 2010 evaluation campaign. We participated to the Arabic to English BTEC tasks using a phrase-based statistical machine translation approach. In this paper, we first discuss some challenges in translating from Arabic to English and we explore various techniques to improve performances on a such task. Next, we present our solution for disambiguating the output of an Arabic morphological analyzer. In fact, The Arabic morphological analyzer used produces all possible morphological structures for each word, with an unique correct proposition. In this work we exploit the Arabic-English alignment to choose the correct segmented form and the correct morpho-syntactic features produced by our morphological analyzer.
This paper describes the MIT-LL/AFRL statistical MT system and the improvements that were developed during the IWSLT 2010 evaluation campaign. As part of these efforts, we experimented with a number of extensions to the standard phrase-based model that improve performance on the Arabic and Turkish to English translation tasks. We also participated in the new French to English BTEC and English to French TALK tasks. We discuss the architecture of the MIT-LL/AFRL MT system, improvements over our 2008 system, and experiments we ran during the IWSLT-2010 evaluation. Specifically, we focus on 1) cross-domain translation using MAP adaptation, 2) Turkish morphological processing and translation, 3) improved Arabic morphology for MT preprocessing, and 4) system combination methods for machine translation.
This paper describes NICT{'}s participation in the IWSLT 2010 evaluation campaign for the DIALOG translation (Chinese-English) and the BTEC (French-English) translation shared-tasks. For the DIALOG translation, the main challenge to this task is applying context information during translation. Context information can be used to decide on word choice and also to replace missing information during translation. We applied discriminative reranking using contextual information as additional features. In order to provide more choices for re-ranking, we generated n-best lists from multiple phrase-based statistical machine translation systems that varied in the type of Chinese word segmentation schemes used. We also built a model that merged the phrase tables generated by the different segmentation schemes. Furthermore, we used a lattice-based system combination model to combine the output from different systems. A combination of all of these systems was used to produce the n-best lists for re-ranking. For the BTEC task, a general approach that used latticebased system combination of two systems, a standard phrasebased system and a hierarchical phrase-based system, was taken. We also tried to process some unknown words by replacing them with the same words but different inflections that are known to the system.
 149 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  150 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  151 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  152 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
 159 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  160 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  161 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  162 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
In this paper we describe the statistical machine translation system of the RWTH Aachen University developed for the translation task of the IWSLT 2010. This year, we participated in the BTEC translation task for the Arabic to English language direction. We experimented with two state-of-theart decoders: phrase-based and hierarchical-based decoders. Extensions to the decoders included phrase training (as opposed to heuristic phrase extraction) for the phrase-based decoder, and soft syntactic features for the hierarchical decoder. Additionally, we experimented with various rule-based and statistical-based segmenters for Arabic. Due to the different decoders and the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each system. The next step would be to exploit these variations and achieve better results by combining the systems. We try different strategies for system combination and report significant improvements over the best single system.
 171 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  172 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  173 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  174 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
 177 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  178 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  179 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  180 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
We report on our participation in the IWSLT 2010 evaluation campaign. Similar to previous years, our submitted systems are based on the Moses statistical machine translation toolkit. This year, we also experimented with hierarchical phrase-based models. In addition, we utilized automatic minimum error-rate training instead of manually-guided tuning. We focused more on the BTEC Turkish-English task and explored various experimentations with unsupervised segmentation to measure their effects on the translation performance. We present the results of several contrastive experiments, including those that failed to improve the translation performance.
This paper describes the UPC-BMIC-VMU participation in the IWSLT 2010 evaluation campaign. The SMT system is a standard phrase-based enriched with novel segmentations. These novel segmentations are computed using statistical measures such as Log-likelihood, T-score, Chi-squared, Dice, Mutual Information or Gravity-Counts. The analysis of translation results allows to divide measures into three groups. First, Log-likelihood, Chi-squared and T-score tend to combine high frequency words and collocation segments are very short. They improve the SMT system by adding new translation units. Second, Mutual Information and Dice tend to combine low frequency words and collocation segments are short. They improve the SMT system by smoothing the translation units. And third, GravityCounts tends to combine high and low frequency words and collocation segments are long. However, in this case, the SMT system is not improved. Thus, the road-map for translation system improvement is to introduce new phrases with either low frequency or high frequency words. It is hard to introduce new phrases with low and high frequency words in order to improve translation quality. Experimental results are reported in the French-to-English IWSLT 2010 evaluation where our system was ranked 3rd out of nine systems.
 199 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  200 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  201 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010  202 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 
What’s Driving Machine Translation?  www.lexcelera.com  WORLDWARE CONFERENCE  Yesterday’s translation processes won’t solve today’s challenges. http://www.youtube.com/watch?v=sIFYPQjYhv8  WORLDWARE CONFERENCE  The environment is changing. WORLDWARE CONFERENCE 
The adequacy of inversion transduction grammars (ITGs) has been widely debated, and the discussion’s crux seems to be whether the search space is inclusive enough (Zens and Ney, 2003; Wellington et al., 2006; Søgaard and Wu, 2009). Parse failure rate when parses are constrained by word alignments is one metric that has been used, but no one has studied parse failure rates of the full class of ITGs on representative hand aligned corpora. It has also been noted that ITGs in Chomsky normal form induce strictly less alignments than ITGs (Søgaard and Wu, 2009). This study is the ﬁrst study that directly compares parse failure rates for this subclass and the full class of ITGs. 
This paper presents an unsupervised method for extracting parallel sentence pairs from a comparable corpus. A translation system is used to mine the comparable corpus and to detect parallel sentence pairs. An iterative process is implemented not only to increase the number of extracted parallel sentence pairs but also to improve the overall quality of the translation system. A comparison between this unsupervised method and a semi-supervised method is also presented. The unsupervised method was tested in a hard condition: no available parallel corpus to bootstrap the process and the comparable corpus contained up to 50% of non parallel data. The experiments conducted show that the unsupervised method can be really applied in the case of lacking parallel data. While preliminary experiments are conducted on French-English translation, this unsupervised method is also applied successfully to a low e-resourced language pair (French-Vietnamese). 
In most statistical machine translation (SMT) systems, bilingual segments are extracted via word alignment. However, there lacks systematic study as to what alignment characteristics can beneﬁt MT under speciﬁc experimental settings such as the language pair or the corpus size. In this paper we produce a set of alignments by directly tuning the alignment model according to alignment F-score and BLEU score in order to investigate the alignment characteristics that are helpful in translation. We report results for a phrasebased SMT system on Chinese-to-English IWSLT data, and Spanish-to-English European Parliament data. With a statistical analysis into alignment characteristics that are correlated with BLEU score, we give alignment hints to improve BLEU score using a phrase-based SMT system and different types of corpus. 
This paper describes the Language Technology Resource Center (LTRC), a U.S. Government website for providing information and tools for users of languages (e.g., translators, analysts, systems administrators, researchers, developers, etc.) The LTRC provides information on a broad range of products and tools, and provides a means for product developers and researchers to provide the U.S. Government and the public with information about their work.  relief effort. In 2002, the effort received a small amount of funding from the Office of the Secretary of Defense. In 2004, it became one of the first projects of the Language and Speech Exploitation Resources (LASER) Advanced Concept Technology Demonstration (ACTD). The program was transitioned to the U.S. Army and currently reports to the Foreign Language Program Office in the Army G2. It was transitioned to the Defense Intelligence Agency Foreign Language Management Division in 2009.  
The focus of this article is to show how client centric Machine Translation (MT) evaluation can assist in identifying the best MT solution in an Enterprise context.  should be done prior to the engine selection and may be the subject for another paper. The starting point of this article is the Enterprise with a clearly identified MT application context including content to be translated as well as an explicit MT workflow. 2 Parameters  Examining extensive lists of opaque scores (BLEU and others) does not bring value to the Enterprise market unless it can equate the results with its own daily commercial challenges. Armed with the right set of directives the decision process is made simpler as Cisco recently discovered. From the results of an external evaluation program they were able to identify a clear process for engine selection as well as creating a matrix for future needs. 
This work investigates query translation using only Wikipedia-based resources in a two step approach: analysis and disambiguation. After arguing that data mined from Wikipedia is particularly relevant to query translation, both from a lexical and a semantic perspective, we detail the implementation of the approach. In the analysis phase, lexical units are extracted from queries and associated to several possible translations using a Wikipediabased bilingual dictionary. During the second phase, one translation is chosen amongst the many candidates, based on topic homogeneity, asserted with the help of semantic information carried by categories of Wikipedia articles. We report promising results regarding translation accuracy. 
 2 Rapid and minimal post-editing  The aim of this paper is to identify postediting guidelines that need to be highlighted in the context of translation training. The analysis of a corpus of texts post-edited by 10 translation trainees reveals how they intuitively approach postediting. The corpus shows that stylistic and phraseological changes are kept to a minimum, and that most errors occur in the field of calque or translation loss. On the basis of the intuitive post-editing strategies observed, a number of recommendations are made for teaching purposes, and they include the need for error analysis of MT output. 
Machine translation evaluation methods are highly necessary in order to analyze the performance of translation systems. Up to now, the most traditional methods are the use of automatic measures such as BLEU or the quality perception performed by native human evaluations. In order to complement these traditional procedures, the current paper presents a new human evaluation based on the expert knowledge about the errors encountered at several linguistic levels: orthographic, morphological, lexical, semantic and syntactic. The results obtained in these experiments show that some linguistic errors could have more inﬂuence than other at the time of performing a perceptual evaluation. 
This paper describes a rule-based machine translation system from Breton to French intended for producing gisting translations. The paper presents a summary of the ongoing development of the system, along with an evaluation of two versions, and some reﬂection on the use of MT systems for lesser-resourced or minority languages. 
The paper presents an exploratory study of the translation processes for 12 student and 12 professional translators. We relate properties of the translators’ process data (eye movements and keystrokes) with the quality of the produced translations, using BLEU scores and human evaluation scores for ﬂuency and accuracy to assess translation quality. We also investigate how BLEU scores correlate with human scores, and how BLEU scores depend on the number of reference translations. We segment the translation process into skimming, drafting and post-editing phases, and show that the translation behavior of student and professional translators differ with respect to how they use the translation phases. We also show that students and professionals differ mainly with respect to produced translation ﬂuency. 
Weights of the various components in a standard Statistical Machine Translation model are usually estimated via Minimum Error Rate Training. With this, one ﬁnds their optimum value on a development set with the expectation that these optimal weights generalise well to other test sets. However, this is not always the case when domains differ. This work uses a perceptron algorithm to learn more robust weights to be used on out-of-domain corpora without the need for specialised data. For an Arabic-to-English translation system, the generalisation of weights represents an improvement of more than 2 points of BLEU with respect to the MERT baseline using the same information. 
The classical approach to tackle speech translation assembles a text-to-text translation system placed after a speech recogniser, yielding the so-called decoupled architecture. In this regard, there are two issues to bear in mind: ﬁrst, what is translated in the decoupled architecture is the most likely transcription of the spoken utterance; second, translation systems are sensitive to errors in the source string, and speech recognition systems are still far from being ﬂawless. In this paper we promote the use of an architecture to carry out speech translation that allows to build up the most likely translation relying upon both acoustic and translation models in a cooperative manner, that is the so-called integrated architecture. The integrated architecture is implemented in the ﬁnite-state framework by virtue of the composition of ﬁnite-state acoustic models of the source language within a stochastic ﬁnite-state transducer that would encompass source and target languages. The potential performance of the integrated architecture is assessed quantitatively in relation to the decoupled one. We conclude that while the single-best approach for both decoupled and integrated architectures show similar performance, an oracle evaluation reveals that the potential scope of the integrated architecture would offer statistically signiﬁcant differences. c 2010 European Association for Machine Translation.  
This study evaluates the impact of integrating two different collocation segmentations methods in a standard phrase-based statistical machine translation approach. The collocation segmentation techniques are implemented simultaneously in the source and target side. Each resulting collocation segmentation is used to extract translation units. Experiments are reported in the English-to-Spanish Bible task and promising results (an improvement over 0.7 BLEU absolute) are achieved in translation quality. 
In this work, we address the question of how to integrate conﬁdence measures into a interactive-predictive machine translation system and reduce user effort. Specifically, we propose to use word conﬁdence measures to aid the user in validating correct preﬁxes from the outputs given by the system. Experimental results obtained on a corpus of the Bulletin of the European Union show that conﬁdence information can help to reduce user effort. 
Post-editing (PE) is still a new activity for many translators. The lack of training, clear and consistent guidelines and international standards may cause difficulties in the transition from translation to PE. Aiming to gain a better understanding of these difficulties and using data gathered from a pilot project, this paper explores possible correlations between PE performance and previous translation experience. We test a combination of the LISA QA Model and the GALE postediting guidelines as a typology for classifying post-editing changes implemented by six post-editors for French and Spanish (three for each target language). This enables a comparison of the types of changes made for the two target languages. We also measure speed and keyboard/mouse activity and link those to translator experience. The insight gathered may be useful for devising future PE guidelines and training programmes.  contribute to the difficulties encountered by translators when post-editing. The over-riding objective of our research is to model post-editing behaviour for two languages which belong to the same language family in order to design specifications for computer-aided support of post-editing, as well as to design a training programme for new post-editors. As part of this research, we conducted a pilot project in June 2009. This was the initial step for a larger scale project, to be carried out in 2010. By analysing PE performance we hope to gather data that may help improve future PE guidelines and training. This paper is structured as follows: Section 2 provides an overview of the pilot project and its objectives and constraints. Section 3 discusses the profile of a good post-editor and introduces the typology we used for classifying the post-editing changes made during the project. Section 4 provides an analysis of the data, including number and types of changes made, productivity and keyboard/mouse data. Section 5 presents conclusions and recommendations for future work. 2 Pilot project  
 This paper proposes the use of a Deterministic Annealing Expectation-Maximization (DAEM) algorithm to estimate the wordalignments involved in the statistical translation process. This approach is aimed to overcome the problem of the local maxima in complex alignment models, thus making unnecessary to iterate with previous and simpler ones. Using the DAEM algorithm allows us to explore the power of highly expressive statistical alignment models without the experimental limitations of working with non-convex models, while, at the same time, observing consistent improvements in translation quality. Experimental results show that, by using an appropriate temperature scheduling, equal or better estimations are obtained independently of the initial parameter estimates.  
Machine Translation (MT) is the task of automatically translating a text from one language to another. In this work we describe a phrase-based Statistical Machine Translation (SMT) system that translates English sentences to Bangla. A transliteration module is added to handle outof-vocabulary (OOV) words. This is especially useful for low-density languages like Bangla for which only a limited amount of training data is available. Furthermore, a special component for handling preposition is implemented to treat systematic grammatical differences between English and Bangla. We have shown the improvement of our system through effective impacts on the BLEU, NIST and TER scores. The overall BLEU score of our system is 11.7 and for short sentences it is 23.3. 
Localisation has long been regarded as an appropriate domain for the deployment of MT. This paper reports the results of a qualitative study which investigated the activities of professional translators working in a Language Services Provider. We analyse the observed work practices and highlight issues related to translation efﬁciency, quality assessment and teamwork. These issues may impact on the successful deployment of MT in a localisation setting but have not been considered extensively by research assessing the appropriateness of MT for localisation as yet. 
Translation memory (TM) plays an important role in localisation workﬂows and is used as an efﬁcient and fundamental tool to carry out translation. In recent years, statistical machine translation (SMT) techniques have been rapidly developed, and the translation quality and speed have been signiﬁcantly improved as well. However, when applying SMT technique to facilitate post-editing in the localisation industry, we need to adapt SMT to the TM data which is formatted with special mark-up. In this paper, we explore some issues when adapting SMT to Symantec formatted TM data. Three different methods are proposed to handle the Translation Memory eXchange (TMX) markup and a comparative study is carried out between them. Furthermore, we also compare the TMX-based SMT systems with a customised SYSTRAN system through human evaluation and automatic evaluation metrics. The experimental results conducted on the French and English language pair show that the SMT can perform well using TMX as input format either during training or at runtime. 
The article presents an attempt to automate all data creation processes of a rulebased shallow-transfer machine translation system. The presented methods were tested on two fully functional translation systems Slovenian-Serbian and SlovenianMacedonian. An extensive range of evaluation tests was performed to assess the applicability of the methods. 
The paper presents EuroTermBank Terminology Add-in – an innovative multilingual terminology translation tool. This tool integrates the federated terminology of the currently over 100 EuroTermBank terminology resources in Microsoft Word environment and accesses over 2 million terminology entries in 27 European languages. The new tool provides a range of functions necessary and useful for target user groups improving productivity and effectiveness in the process of translation. User feedback shows definite interest in the new tool which is considered to be extremely useful and efficient for translation needs. Beta versions of EuroTermBank Terminology Add-in for Microsoft Word 2003 and 2007 can be freely downloaded from the EuroTermBank multilingual terminology portal. 
Statistical machine translation relies heavily on parallel corpora to train its models for translation tasks. While more and more bilingual corpora are readily available, the quality of the sentence pairs should be taken into consideration. This paper presents a novel lattice score-based data cleaning method to select proper sentence pairs from the ones extracted from a bilingual corpus by the sentence alignment methods. The proposed method is carried out as follows: ﬁrstly, an initial phrasebased model is trained on the full sentencealigned corpus; then for each of the sentence pairs in the corpus, word alignments are used to create anchor pairs and sourceside lattices; thirdly, based on the translation model, target-side phrase networks are expanded on the lattices and Viterbi searching is used to ﬁnd approximated decoding results; ﬁnally, BLEU score thresholds are used to ﬁlter out the low-score sentence pairs for the data cleaning purpose. Our experiments on the FBIS corpus showed improvements of BLEU score from 23.78 to 24.02 in Chinese-English. 
Corpus driven machine translation approaches such as Phrase-Based Statistical Machine Translation and Example-Based Machine Translation have been successful by using word alignment to ﬁnd translation fragments for matched source parts in a bilingual training corpus. However, they still cannot properly deal with systematic translation for insertion or deletion words between two distant languages. In this work, we used syntactic chunks as translation units to alleviate this problem, improve alignments and show improvement in BLEU for Korean to English and Chinese to English translation tasks. 
Source language reordering can be seen as the preprocessing task of permuting the order of the source words in such a way that the resulting permutation allows as monotone a translation process as possible. We explore a simple but effective source reordering algorithm that works as a cascade of source string transforms, each consisting of swapping the positions of a single pair of adjacent words in order to unfold a candidate pair of crossing alignments. The decision to swap a pair of words is modelled as a binary classiﬁcation task formulated as a log-linear model and trained under maximum entropy (MaxEnt). We experiment with features that consist of the local neighborhood of both words as well as lexico-syntactic representations known as supertags. Our experiments on the English-to-Dutch EuroParl translation task show that the cascaded alignment unfolding slightly improves the performance of a state-of-the-art phrase translation system that uses distance-based and lexicalized block-oriented reordering.  ﬁned using a local window of phrases, e.g., (Tillman, 2004). Hierarchical approaches, based on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005), explore yet a wider range of reorderings deﬁned by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. Finally, source sentence reordering is a preprocessing task that aims at ﬁnding a permutation of the words of the source sentence that contains the least number of crossing alignments between source words and their target sentence counterparts. This paper is concerned with the task of source language reordering as a preprocessing task. Figure 1 depicts the translation from source string S to target string T with alignment a (solid line) and the alternative of source reordering S into S followed by the translation S → T with alignment a (in dashed lines). Source reordering of S is as successfull as much as it will yield a permutation S that has as monotone an alignment a as possible with T .  
In recent years the performance of SMT increased in domains with enough training data. But under real-world conditions, it is often not possible to collect enough parallel data. We propose an approach to adapt an SMT system using small amounts of parallel in-domain data by introducing the corpus identiﬁer (corpus id) as an additional target factor. Then we added features to model the generation of the tags and features to judge a sequence of tags. Using this approach we could improve the translation performance in two domains by up to 1 BLEU point when translating from German to English. 
 The problem of language model adaptation in statistical machine translation is considered. A mixture of language models is employed, which is obtained by clustering the bilingual training data. Unsupervised clustering is guided by either the development or the test set. Different mixture weight estimation schemes are proposed and compared, at the level of either single or all source sentences. Experimental results show that, by training different speciﬁc language models weighted according to the actual input instead of using a single target language model, translation quality is improved, as measured by BLEU and TER.  
We present a general method for incorporating an “expert” model into a Statistical Machine Translation (SMT) system, in order to improve its performance on a particular “area of expertise”, and apply this method to the speciﬁc task of ﬁnding adequate replacements for Out-of-Vocabulary (OOV) words. Candidate replacements are paraphrases and entailed phrases, obtained using monolingual resources. These candidate replacements are transformed into “dynamic biphrases”, generated at decoding time based on the context of each source sentence. Standard SMT features are enhanced with a number of new features aimed at scoring translations produced by using different replacements. Active learning is used to discriminatively train the model parameters from human assessments of the quality of translations. The learning framework yields an SMT system which is able to deal with sentences containing OOV words but also guarantees that the performance is not degraded for input sentences without OOV words. Results of experiments on English-French translation show that this method outperforms previous work addressing OOV words in terms of acceptability. 
Syntactic reordering has been demonstrated to be helpful and effective for handling different word orders between source and target languages in SMT. However, in terms of hierarchial PB-SMT (HPB), does the syntactic reordering still has a significant impact on its performance? This paper introduces a reordering approach which explores the (DE) grammatical structure in Chinese. We employ the Stanford DE classiﬁer to recognise the DE structures in both training and test sentences of Chinese, and then perform word reordering to make the Chinese sentences better match the word order of English. The annotated and reordered training data and test data are applied to a re-implemented HPB system and the impact of the DE construction is examined. The experiments are conducted on the NIST 2008 evaluation data and experimental results show that the BLEU and METEOR scores are signiﬁcantly improved by 1.83/8.91 and 1.17/2.73 absolute/relative points respectively. 
We present new results from a hybrid combination of rule-based machine translation (RBMT) with a variant of statistical machine translation (SMT) that supports hierarchical structures and is therefore able to preserve more of the linguistic structures obtained from the RBMT system than versions of SMT that operate on ﬂat phrases alone. Having shown in (Chen and Eisele, 2010) for the ﬁrst time that a tighter integration of hierachical MT systems from different paradigms leads to consistent improvements for translation from German to English in various experimental settings, the current paper generalizes the approach to translation from English to German, where we observe similar improvements. These ﬁndings indicate that hybrid combinations of MT paradigms can beneﬁt from structural similarities in the underlying models, which makes us expect even stronger beneﬁts from a tight integration of different approaches. . 
 Sharon O’Brien SALIS, Dublin City University Sharon.obrien@ dcu.ie  Minako O’Hagan SALIS, Dublin City University Minako.ohagan@d cu.ie  Fred Hollowood Research & Deployment (SES), Symantec Corporation, Ireland FHollowood@symantec .com  Abstract This paper introduces a new statistical preprocessing model for Rule-Based Machine Translation (RBMT) systems. We train a Statistical Machine Translation (SMT) system using monolingual corpora. This model can transform a source input to an RBMT system into a more target-language friendly or RBMTsystem friendly “pivot” language. We apply this proposed model to translation from English to Chinese in a pilot project. Automatic evaluation scores (BLEU, TER and GTM) show that this pre-processing model can increase the quality of the output of the RBMT system, especially with an increase in the size of the training corpus. This model is applicable to language pairs which differ in grammar and language structures. 
We present a user case of the free/opensource Spanish ↔ Brazilian Portuguese Apertium machine translation system inside the localization workﬂow of Autodesk. This system, initially developed to perform general-domain translations, has been customized by Prompsit to ﬁt with Autodesk needs and by respecting the localization workﬂow as much as possible. This original scenario shows that postedited machine translation can generate immediately signiﬁcant productivity gains with publication-ready linguistic quality. 
Today, we and our customers are faced with a huge amount of continuous data streams in multiple languages and different forms and formats. Therefore, our business communications requirements and strategies demand for an effective employment of various language resources to economically and efficiently administer, master and monitor information processes and workflows across languages, cultures and time zones. We have thoroughly investigated into what language resources are mostly suited for our needs, and what are the important enablers in different translingual technical deployment scenarios that guarantee throughput, scalability, quality and successful operations and applications. Although Machine Translation (MT) is still a gadget because neither individual nor business users do share the usability and quality of MT as a real user experience, MT is an intrinsic part of our solution. With this paper we want to share and discuss our findings on MT with the community. 1. Global Communications Landscape In the last decade, private and business communications have changed dramatically with the Internet being the ultimate communications platform for everyone across time zones, languages and cultures. Translation and cultural adaptation play an ever increasing critical role in this global communications landscape and are no longer restricted to business and technical communication only. With the ubiquitous web access from differ- © 2010 European Association for Machine Translation.  ent even mobile devices such as smartphones, the need for competent and effective language services increases exponentially, and in particular these services shall be highly configurable and available from everywhere, on demand, and preferable as a pay-per-use service offering. Obviously, this new, highly proactive communications landscape with its associated demands for multiple language services cannot be handled, administered and controlled properly with traditional translation technology service set-ups because currently they are not flexible enough to account for the various communications requirements. The existing translation management systems are mainly designed to manage and monitor human-oriented tasks which in general are comprised of single, disruptive steps without direct interprocess relationship and interaction. Full automation and ambient adaptability are key to keep pace with the speed and variety of the multilingual transcultural demands, and the intrinsic characteristics of the processes from start to finish with persisted states. The term “glocalization” that is derived from the Japanese term “dochakuka” meaning “global localization” names the just described global communications landscape most appropriately. In Section 2, we outline a possible technical solution that guided our language technology investigation with a focus on translation automation in particular machine translation (MT), and show what is needed to serve the demands of a global communications landscape. How this solution might be set into operation is discussed in Section 3, which also points to some serious shortcomings that exist with state-of-the-art technical and technological MT offerings with a focus on sharable language resources. In Section 4, we conclude and list the necessary steps in terms of short-, medium- and longterm investments, and how the MT  community – developers, providers, researchers and users – should contribute to these investments. Section 5 closes with additional perspectives for the decade and beyond. 2. Technical Framework Solutions A new breed of translation management framework is needed that also accounts for the effective modeling of the processes and workflows in addition to the efficient management of highly adaptable translation automation functions. The modeling and management aspects must be taken into account in an integrated way, and allow for effective and efficient automation and scalability from personal to business orchestration (local) and choreography (global). In general, the core of such a framework is an event-driven workflow engine that offers capabilities to model, administer, monitor and test single processes and process stacks, and includes means to facilitate and continuously control human and machine feedback cycles which shall also serve as machine learning and training instances for amending and improving the services. This engine can be based on an already existing translation management system workflow engine with some thorough redesign and extensions that account for the new needs and demands of a proactive, mobile, and multilingual information society and their specific language requirements. For example, appropriate APIs and SDKs1 shall permit the integration of different third party offerings into a seamless process workflow, from modeling tasks down to the execution level, and its transparent management including the simplification of particular compulsory rules of regulation, governance and compliance. For example, a possible deployment scenario might include content authoring support with language proofing and translatability monitoring as well as terminology services to ensure consistency and compliance with predefined corporate rules and conventions, or a post-editing guidance service in tandem with a machine translation deployment application. As with other (business) processes, frameworks and tools that allow for an effective modeling and managing are needed to ensure an effective automation of various language and cultural adaptation services including their effectual qual- 
We describe the effort of the Microsoft Translator team to develop a Haitian Creole statistical machine translation engine from scratch in a matter of days. Haitian Creole presents a number of difﬁculties for devleoping an SMT system, principal among these is the lack of signiﬁcant amounts of parallel training data and an inconsistent orthography, both of which lead to data sparseness. We demonstrate, however, that it is possible to build a translation engine of reasonable quality over very little data by engaging with the native language community and reducing data sparseness in creative ways. As such, we show that MT as a technology and as a service can be deployed rapidly in crisis situations. 
This paper describes the transfer component of a syntax-based Example-based Machine Translation system. The source sentence parse tree is matched in a bottom-up fashion with the source language side of a parallel example treebank, which results in a target forest which is sent to the target language generation component. The results on a 500 sentences test set are compared with a top-down approach to transfer of the same system, with the bottom-up approach yielding much better results. 
Example-Based Machine Translation (EBMT), like other corpus based methods, requires substantial parallel training data. One way to reduce data requirements and improve translation quality is to generalize parts of the parallel corpus into translation templates. This automated generalization process requires clustering. In most clustering approaches the optimal number of clusters (N ) is found empirically on a tune set which often takes several days. This paper introduces a spectral clustering framework that automatically estimates the optimal N and removes unstable oscillating points. The new framework produces signiﬁcant improvements in low-resource EBMT settings for English-to-French (≈1.4 BLEU points), English-to-Chinese (≈1 BLEU point), and English-to-Haitian (≈2 BLEU points). The translation quality with templates created using automatically and empirically found best N were almost the same. By discarding “incoherent” points, a further boost in translation scores is observed, even above the empirically found N . 
In the wake of the January 12 earthquake in Haiti it quickly became clear that the existing emergency response services had failed but text messages were still getting through. A number of people quickly came together to establish a text-message based emergency reporting system. There was one hurdle: the majority of the messages were in Haitian Kreyol, which for the most part was not understood by the primary emergency responders, the US Military. We therefore crowdsourced the translation of messages, allowing volunteers from within the Haitian Kreyol and French-speaking communities to translate, categorize and geolocate the messages in real-time. Collaborating online, they employed their local knowledge of locations, regional slang, abbreviations and spelling variants to process more than 40,000 messages in the first six weeks alone. According the responders this saved hundreds of lives and helped direct the first food and aid to tens of thousands. The average turn-around from a message arriving in Kreyol to it being translated, categorized, geolocated and streamed back to the responders was 10 minutes. Collaboration among translators was crucial for data-quality, motivation and community contacts, enabling richer value-adding in the translation than would have been possible from any one person.
The recent emergence of crowdsourced translation à la Facebook or Twitter has exposed a raw nerve in the translation industry. Perceptions of ill-placed entitlement -- we are the professionals who have the ``right'' to translate these products -- abound. And many have felt threatened by something that carries not only a relatively newly coined term -- crowdsourcing -- but seems in and of itself completely new. Or is it?
Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation that makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e. paraphrases) with only monolingual knowledge of the source language. Evaluations demonstrate that this approach can yield substantial improvements in translation quality.
This position paper outlines our project {--} WikiBABEL {--} which will be released as an open source project for the creation of multilingual Wikipedia content, and has potential to produce parallel data as a by-product for Machine Translation systems research. We discuss its architecture, functionality and the user-experience components, and briefly present an analysis that emphasizes the resonance that the WikiBABEL design and the planned involvement with Wikipedia has with the open source communities in general and Wikipedians in particular.
•Kinds of MT Systems •Rule-based, statistical, and hybrid 2  © Mike Dillinger, 2010  Industrial‐scale translation  Introduction to MT The Problem: Industrial‐scale translation 3 Why MT? Houston, we have a problem. Communication is the lifeblood of business. Without communication, business can’t happen. - Communication with clients - Internal communication But now our clients and operations are global It’s too hard, too time consuming, and too expensive to re-write everything from scratch in each language. So, translation is inescapable. And it’s not just 20 or 30 pages, eh? 4  © Mike Dillinger, 2010  Industrial‐scale translation  © Mike Dillinger, 2010  Why MT? We need industrial-scale translation, part 1 There are more products = more content The products are more complex = more content The products have more uses = more content Products change more rapidly = more content Operations are more complex = more content Content is used in more places = more translation 5 Why MT? We need industrial-scale translation, part 2 Product development is faster = faster translation Time to market is faster = faster translation Support has to be faster = faster translation Organizations have to be flexible = faster translation We need industrial-scale translation, part 3 Oh, and by the way, You can’t spend any more money to get all of this done! =cheaper translation 6  Industrial‐scale translation  © Mike Dillinger, 2010  Industrial‐scale translation  “Industrial scale” means…  More More  Better Better  Faster Faster  Cheaper Cheaper  More Better Faster Cheaper MoreBetterFasteCrheaper  MorBeetteFrasCtehreaper  7  © Mike Dillinger, 2010  Industrial‐scale translation  A Perfect Storm The practice of global communication is falling apart:  a) Goals Publish documents b) Scale Easier access to more documents; language pairs c) Process write + translate + use + support are all independent  60% or more are not used Information overload; crisis of confidence; a few languages Each interferes with the other  8  © Mike Dillinger, 2010  Industrial‐scale Translation  The problems Scalability, cost, time yHuman translation is (usually) wonderful but it doesn’t scale well yBigger projects = more costs yBigger projects = more issues yMore languages = more costs + more issues yHuman translation is expensive yHuman translation is slow  Goals We need industrial‐ scale translation processes: more, better, faster, cheaper  9  © Mike Dillinger, 2010  Introduction to MT FAQs: What’s MT? 10  FAQs: What’s MT?  © Mike Dillinger, 2010  What’s MT? Machine Translation systems are software products that translate electronic texts (and speech) into other languages automatically. • Do you mean systems like Google Translate and Babelfish? •Yes, the basic technology is the same, but for companies we adapt the system extensively, to meet your needs. The result is very different! • Can we fire our human translators? •No. In most situations, MT requires human translators. Their job just changes so they can do more translation faster. Many translation agencies already use MT for draft translations because it saves them time and money. • We already use machine translation from Trados, right? •Trados is one good use of old machine translation technology – it’s called “translation memory”. It doesn’t work well with new sentences or new topics. Modern machine translation technology can do a much better job with new input; think of MT as “translation reasoning”. 11 What’s MT? • You guys really hate translators, don’t you? •Not at all! Some overly enthusiastic MT researchers in the old days talked about replacing humans, which scared the pants off the translators. It also embarrassed us to death. Nowadays, we even invite translators to our conferences : ) • MT is ridiculous. Only humans can really translate. You have to understand the subtleties of language and culture. •It turns out that very, very many kinds of sentences are routine enough that machines can do a great job without subtle understanding. Most useful texts are neither poetic nor sophisticated. • I’ve seen MT on the web. It’s laughable junk. •Millions of people use MT every day and very few complain. Besides, it’s free. What would a free Mercedes look like? Brand new Enterprise MT, customized to your needs is very, very different from what you see on the web. 12  FAQs: What’s MT?  © Mike Dillinger, 2010  FAQs: What’s MT?  “MT will have a negative impact on my brand”  Your site translated without MT  Your site translated with MT  © Mike Dillinger, 2010  FAQs: Who’s really using MT?  Faster Cheaper More consistent 13 Who’s really using MT? • “In-bound” Translation (from other languages to yours) •Global Public Health Information Network (Public Health, Canada) •Many military and business organizations •Internet users around the world • “Out-bound” Translation (from yours to other languages) •Symantec, Adobe, Cisco, Microsoft, Intel, European Community, etc. •Internet users around the world • “Real-time” Translation (between two languages) •Translated subtitles (news, Jay Leno), translated TV and radio broadcasts •Internet users around the world: translated chat, translated SMS 14  © Mike Dillinger, 2010  Introduction to MT Limitations: What MT can’t do 15  Limitations: What MT can’t do  - Should translators “fix” errors  Translation “quality”  in the source documents? - Should they reorganize source  documents?  Quality of target document =  f (quality of source document + quality of target sentences)  Correct source/target equivalence is still a question of trust, not of measurement.  So far, we’ve only used translators’ criteria for quality. What do end users notice and not notice? What are their criteria for quality?  Dimensions of information quality yContent quality (relevant, complete, accurate information) yDesign quality (easy to find and maintain information) yLinguistic quality (easy to understand information) yTerm consistency yStylistic simplicity yProcess quality (cost, consistency, reliability, etc.) 16  © Mike Dillinger, 2010  Limitations: What MT can’t do  How good? Which MT system should I choose?  © Mike Dillinger, 2010  How NOT to evaluate MT  17  How not to evaluate MT The usual (mis)steps: yHear salesperson say how great product X is yAsk lots of questions about “quality”, speed, interfaces, cost yGet an evaluation versions of product X and others yTranslate some of your documents yAsk translators about “quality” of translation yGet puzzled about poor output quality yDecide not to use MT  Outcomes yWasted time, effort, money yLittle understanding, little learning yNegative reputation for MT  © Mike Dillinger, 2010  Conclusion:  “These MT products are junk!”  18  Limitations: What MT can’t do  Garbage in, garbage out?  Conclusion: This software rots!  Documents  Terminology Dictionary Management Customization Standard File Types File Format Filters Style Grammar Management Customization  MT System  19  © Mike Dillinger, 2010  Limitations: What MT can’t do  Original source Emerging Markets Take Record Share of World Equity (Update1)  Raw Google (customization not possible), original source Emerging Markets Take Record Part du World Equity (Update1)  By Michael Patterson and Laura Cochrane July 3 (Bloomberg) -- Developing countries’ share of worldwide equity value climbed to a record as the fastest-growing economies lured investors amid the first global recession since World War II.  By Michael Patterson et Laura Cochrane 3 juillet (Bloomberg) - Développer la part des pays dans le monde entier la valeur a atteint un record de la plus forte croissance économique a attiré les investisseurs au milieu de la première récession mondiale depuis la Seconde Guerre mondiale.  The 22 nations <that were> classified as “emerging” by index provider MSCI Inc. comprised 24 percent of world market capitalization, up from 18 percent at the start of this year, the highest proportion since Bloomberg began compiling the data in 2003. China shares surpassed $3 trillion yesterday for the first time since August, from $1.8 trillion at the end of 2008.  Les 22 pays classés comme "émergents" de l'indice MSCI Inc fournisseur comprend 24 pour cent de la capitalisation boursière mondiale, contre 18 pour cent au début de cette année, la proportion la plus élevée depuis Bloomberg a commencé la compilation des données en 2003. La Chine partage dépassé $ 3 billion, hier, pour la première fois depuis le mois d'août, à partir de $ 1,8 billions à la fin de 2008.  The increase signals growing confidence in developing countries as equity investors, spurred by interest-rate cuts and stimulus plans, redeploy cash after the worst U.S. losses last since the Great Depression. The MSCI Emerging Markets Index rose 35 percent, beating a 2.9 percent advance in the MSCI World Index of developed economies and lifting the value of stocks to $8.6 trillion from $5.1 trillion in 2008 “Everyone is trying to jump on that bandwagon,” said Nicholas Field, who helps manage about $11 billion in emerging- market stocks at Schroders Plc in London.  L'augmentation des signaux de plus en plus confiance dans les pays en développement que les investisseurs, stimulée par des taux d'intérêt des coupures et des plans d'incitation, de redéployer en espèces après la pire des États-Unis pertes dernier depuis la Grande Dépression. L'indice MSCI Emerging Markets Index a augmenté de 35 pour cent, en battant l'avance de 2,9 pour cent dans l'indice mondial MSCI des pays développés et la levée de la valeur des stocks à 8,6 billions de $ 5.1 billions de $ en 2008 Tout le monde tente de sauter sur cette aventure», a déclaré Nicholas Field, qui permet de gérer environ 11 milliards de dollars en actions des marchés émergents à Schroders Plc à Londres.  20  © Mike Dillinger, 2010  Limitations: What MT can’t do  Adapted source Emerging Markets Take Record Share of World Equity (Update1)  Improved MT with adapted source and dic customization Les marchés émergent prennent partie record de situation nette modiale (Update1)  By Michael Patterson and Laura Cochrane July 3 (Bloomberg). Developing countries now have a recordbreaking part of world equity. Their economies are growing quickly and they lured investors, even amid the first global recession since World War II.  Par Michael Patterson et Laura Cochrane Le 3 (Bloomberg) juillet. Les pays en voie de développement ont maintenant une partie record de situation nette modiale. Leurs économies croissent rapidement et ils ont leurré des investisseurs, même entre la première récession globale depuis seconde guerre mondiale.  The 22 nations that were classified as “emerging countries” by index provider MSCI Inc. comprised 24 percent of world market capitalization. This increased from 18 percent at the start of this year. This was the highest proportion since Bloomberg began compiling the data in 2003. China's part surpassed $3 trillion yesterday for the first time since August, from $1.8 trillion at the end of 2008.  Les 22 nations qui ont été classifiées comme "pays émergents" par fournisseur de l'indice MSCI Inc. compris 24 pour cent de capitalisation de marché mondial. Cela a augmenté de 18 pour cent au début de cette année. C'était la plus haute proportion depuis que Bloomberg a commencé à compiler les données dans 2003. La partie de Chine a dépassé $3 billion hier pour la première fois depuis août, de $1.8 billion à la fin de 2008.  The increase signals growing confidence in developing countries as equity investors. Spurred by interest-rate cuts and stimulus plans, they are redeploying cash after the worst U.S. losses since the Great Depression. The MSCI Emerging Markets Index rose 35 percent. This beat a 2.9 percent advance in the MSCI World Index of developed economies and lifted the value of stocks to $8.6 trillion from $5.1 trillion in 2008. “Everyone is trying to jump on that bandwagon,” said Nicholas Field, who helps to manage about $11 billion in emerging-market stocks at Schroders Plc in London.  L'augmentation signale la confiance croissante au pays en voie de développement comme investisseurs des capitaux propres. Talonnés par les réductions de l'intérêt-taux et les plans du signal, ils redistribuent la liquidité après le plus mauvais U.S. pertes depuis la Grande Baisse. L'Indice MSCI des marchés émergents a augmenté 35 pour cent. Cela a battu un 2.9 pour cent avance dans l'Indice Mondiale MSCI d'économies développées et a soulevé la valeur de titres à $8.6 billion de $5.1 billion dans 2008. Tout le monde essaie de prendre ce train en marche, a dit Nicolas Field qui aide pour diriger approximativement $11 milliard dans les actions des marchés émergents à Schroder Plc à Londres.  21  © Mike Dillinger, 2010  Limitations: What MT can’t do  Why is MT output so bad?  ‐Source issues‐ yPoor writing in the source text yFormatting issues in the source text ‐Mismatch issues‐ yTerms and expressions that are not in the MT dictionary ySentence types that are not covered by the MT system ‐MT issues‐ yIncorrect word sense chosen yIncorrect sentence structure analysis  What’s MT good for? yDraft translations of clean source texts yFast translations of low‐value information yTranslation of very high‐ volume information yInformation extraction and decision support  22  © Mike Dillinger, 2010  Limitations: What MT can’t do  Lessons learned  yTechnology is designed and built for optimal performance in specific conditions (ex: paved road, competent driver, correct fuel) Even a wonderful, brand‐new BMW looks like junk when it’s tested outside the design specs. yUsing technology outside of its “comfort zone” requires adaptation.  Action items yUnderstand MT’s “comfort zone” yAssess which adaptations are necessary: yInput yPeople yProcess yTechnology  23  © Mike Dillinger, 2010  Limitations: What MT can’t do  The “comfort zone” for MT  Adaptations by writers Manage terminology and vocabulary explicitly Manage writing style explicitly Manage writing style explicitly Use standard file formats Write to minimize post‐editing  MT is designed for:  Adaptations of MT  Familiar (to the system) words and phrases Familiar (to the system) sentence types Literal, predictable meanings Standardized file formats Post‐editing  Add words and phrases to dictionary Extend grammatical coverage; couple translation memory Extend semantic coverage; couple translation memory Add filters and converters Extend system performance to minimize post‐editing  The adaptations converge  Very fast processing Very large volumes Good quality 24  © Mike Dillinger, 2010  Localization scenario  The status quo  Existing IT infrastructure  Existing People and Processes  Existing People and Processes  - Ill-defined authoring processes - Simple localization management processes - Informal work flow definition - Few quantitative metrics in use  Implementation Gaps LSP Implementation Gaps  Existing IT infrastructure  Existing People and Processes  Existing People and Processes  - Simple localization management processes - Informal quality control of target - No error analysis  25  © Mike Dillinger, 2010  Localization scenario  Why deploying MT seems difficult  Implementation Gaps  Existing IT infrastructure  Existing IT infrastructure  Existing People and Processes  MT  Existing People and Processes  - Ill-defined authoring processes - Simple localization management processes - Informal work flow definition - Few quantitative metrics in use  Implementation Gaps  Implementation Gaps  - term detection and  - post-editing  management  - systematic quality control  - well-defined authoring  of target  processes  - error analysis for feedback  - systematic quality control of source  - document postprocessing  - document pre-processing Who can do this?  - in-house staff?  - new hires?  - outsourced?  - Simple localization management processes - Informal quality control of target - No error analysis  26  © Mike Dillinger, 2010  ROI: Benefits of MT  Introduction to MT ROI: Benefits of MT 27 ROI: Benefits of MT Machine Translation systems provide faster and cheaper translations than humans with translation memory tools alone. • MT captures translator knowledge and effort in additional ways (memory vs. reasoning) •MT requires more disciplined writing, which leads to additional efficiency and savings •MT shifts the translator’s workload from slower, more complex tasks (translation) to faster, simpler tasks (revising) There are a range of different scenarios for translation automation, with and without MT. 28  © Mike Dillinger, 2010  ROI: Benefits of MT  Total effort (time * cost) for the same 1,000,000‐word project Area = amount of effort  Cost (USD)  $ 250,000  $ 240,000  No tools  $ 230,000  $ 220,000  $ 210,000  $ 200,000  $ 190,000  $ 180,000  $ 170,000  $ 160,000  TM only  $ 150,000  $ 140,000  MT+  $ 130,000  $ 120,000  TM+MT  $ 110,000 TM+customized MT  $ 100,000 TM+MT  $ 90,000 +better source  $ 80,000  $ 70,000 TM+MT+much  $ 60,000 better source  $ 50,000  $ 40,000  $ 30,000  $ 20,000  $ 10,000  $  - MT  
English-Manipuri language pair is one of the rarely investigated with restricted bilingual resources. The development of a factored Statistical Machine Translation (SMT) system between English as source and Manipuri, a morphologically rich language as target is reported. The role of the suffixes and dependency relations on the source side and case markers on the target side are identified as important translation factors. The morphology and dependency relations play important roles to improve the translation quality. A parallel corpus of 10350 sentences from news domain is used for training and the system is tested with 500 sentences. Using the proposed translation factors, the output of the translation quality is improved as indicated by the BLEU score and subjective evaluation.
We introduce a novel translation rule that captures discontinuous, partial constituent, and non-projective phrases from source language. Using the traversal order sequences of the dependency tree, our proposed method 1) extracts the synchronous rules in linear time and 2) combines them efficiently using the CYK chart parsing algorithm. We analytically show the effectiveness of this translation rule in translating relatively free order sentences, and empirically investigate the coverage of our proposed method.
An implementation of a non-structural Example-Based Machine Translation system that translates sentences from Arabic to English, using a parallel corpus aligned at the sentence level, is described. Source-language synonyms were derived automatically and used to help locate potential translation examples for fragments of a given input sentence. The smaller the parallel corpus, the greater the contribution provided by synonyms. Considering the degree of relevance of the subject matter of a potential match contributes to the quality of the final results.
Hebrew and Arabic are related but mutually incomprehensible languages with complex morphology and scarce parallel corpora. Machine translation between the two languages is therefore interesting and challenging. We discuss similarities and differences between Hebrew and Arabic, the benefits and challenges that they induce, respectively, and their implications for machine translation. We highlight the shortcomings of using English as a pivot language and advocate a direct, transfer-based and linguistically-informed (but still statistical, and hence scalable) approach. We report preliminary results of such a system that we are currently developing.
This paper describes a method that successfully exploits simple syntactic features for n-best translation candidate reranking using perceptrons. Our approach uses discriminative language modelling to rerank the n-best translations generated by a statistical machine translation system. The performance is evaluated for Arabic-to-English translation using NIST{'}s MT-Eval benchmarks. Whilst parse trees do not consistently help, we show how features extracted from a simple Part-of-Speech annotation layer outperform two competitive baselines, leading to significant BLEU improvements on three different test sets.
We present a novel exact solution to the approximate string matching problem in the context of translation memories, where a text segment has to be matched against a large corpus, while allowing for errors. We use suffix arrays to detect exact n-gram matches, A* search heuristics to discard matches and A* parsing to validate candidate segments. The method outperforms the canonical baseline by a factor of 100, with average lookup times of 4.3{--}247ms for a segment in a realistic scenario.
We describe an effort to improve standard reference-based metrics for Machine Translation (MT) evaluation by enriching them with Confidence Estimation (CE) features and using a learning mechanism trained on human annotations. Reference-based MT evaluation metrics compare the system output against reference translations looking for overlaps at different levels (lexical, syntactic, and semantic). These metrics aim at comparing MT systems or analyzing the progress of a given system and are known to have reasonably good correlation with human judgments at the corpus level, but not at the segment level. CE metrics, on the other hand, target the system in use, providing a quality score to the end-user for each translated segment. They cannot rely on reference translations, and use instead information extracted from the input text, system output and possibly external corpora to train machine learning algorithms. These metrics correlate better with human judgments at the segment level. However, they are usually highly biased by difficulty level of the input segment, and therefore are less appropriate for comparing multiple systems translating the same input segments. We show that these two classes of metrics are complementary and can be combined to provide MT evaluation metrics that achieve higher correlation with human judgments at the segment level.
Morphologically rich languages pose a challenge for statistical machine translation (SMT). This challenge is magnified when translating into a morphologically rich language. In this work we address this challenge in the framework of a broad-coverage English-to-Arabic phrase based statistical machine translation (PBSMT). We explore the full spectrum of Arabic segmentation schemes ranging from full word form to fully segmented forms and examine the effects on system performance. Our results show a difference of 2.61 BLEU points between the best and worst segmentation schemes indicating that the choice of the segmentation scheme has a significant effect on the performance of a PBSMT system in a large data scenario. We also show that a simple segmentation scheme can perform as good as the best and more complicated segmentation scheme. We also report results on a wide set of techniques for recombining the segmented Arabic output.
In this paper, we describe an extension to a hybrid machine translation system for handling dialect Arabic, using a decoding algorithm to normalize non-standard, spontaneous and dialectal Arabic into Modern Standard Arabic. We prove the feasibility of the approach by measuring and comparing machine translation results in terms of BLEU with and without the proposed approach. We show in our tests that on real-live broadcast input with transcriptions of dialectal speech we achieve an increase on BLEU of about 1{\%}, and on web content with dialect text of about 2{\%}.
In this paper, we present the insights gained from a detailed study of coupling a highly modular English-Hindi RBMT system with a standard phrase-based SMT system. Coupling the RBMT and SMT systems at various stages in the RBMT pipeline, we observe the effects of the source transformations at each stage on the performance of the coupled MT system. We propose an architecture that systematically exploits the structural transfer and robust generation capabilities of the RBMT system. Working with the English-Hindi language pair, we show that the coupling configurations explored in our experiments help address different aspects of the typological divergence between these languages. In spite of working with very small datasets, we report significant improvements both in terms of BLEU (7.14 and 0.87 over the RBMT and the SMT baselines respectively) and subjective evaluation (relative decrease of 17{\%} in SSER).
We describe a unified and coherent syntactic framework for supporting a semantically-informed syntactic approach to statistical machine translation. Semantically enriched syntactic tags assigned to the target-language training texts improved translation quality. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English translation task. This finding supports the hypothesis (posed by many researchers in the MT community, e.g., in DARPA GALE) that both syntactic and semantic information are critical for improving translation quality{---}and further demonstrates that large gains can be achieved for low-resource languages with different word order than English.
In this work we review and compare three additional syntactic enhancements for the hierarchical phrase-based translation model, which have been presented in the last few years. We compare their performance when applied separately and study whether the combination may yield additional improvements. Our findings show that the models are complementary, and their combination achieve an increase of 1{\%} in BLEU and a reduction of nearly 2{\%} in TER. The models presented in this work are made available as part of the Jane open source machine translation toolkit.
TER-Plus (TERp) is an extended TER evaluation metric incorporating morphology, synonymy and paraphrases. There are three new edit operations in TERp: Stem Matches, Synonym Matches and Phrase Substitutions (Paraphrases). In this paper, we propose a TERp-based augmented system combination in terms of the backbone selection and consensus decoding network. Combining the new properties of the TERp, we also propose a two-pass decoding strategy for the lattice-based phrase-level confusion network (CN) to generate the final result.The experiments conducted on the NIST2008 Chinese-to-English test set show that our TERp-based augmented system combination framework achieves significant improvements in terms of BLEU and TERp scores compared to the state-of-the-art word-level system combination framework and a TER-based combination strategy.
Lexical-Functional Grammar (LFG) f-structures (Kaplan and Bresnan, 1982) have attracted some attention in recent years as an intermediate data representation for statistical machine translation. So far, however, there are no alignment tools capable of aligning f-structures directly, and plain word alignment is used for this purpose. In this way no use is made of the structural information contained in f-structures. We present the first version of a specialized f-structure alignment open-source software.
In this paper, we present a novel approach to incorporate source-side syntactic reordering patterns into phrase-based SMT. The main contribution of this work is to use the lattice scoring approach to exploit and utilize reordering information that is favoured by the baseline PBSMT system. By referring to the parse trees of the training corpus, we represent the observed reorderings with source-side syntactic patterns. The extracted patterns are then used to convert the parsed inputs into word lattices, which contain both the original source sentences and their potential reorderings. Weights of the word lattices are estimated from the observations of the syntactic reordering patterns in the training corpus. Finally, the PBSMT system is tuned and tested on the generated word lattices to show the benefits of adding potential source-side reorderings in the inputs. We confirmed the effectiveness of our proposed method on a medium-sized corpus for Chinese-English machine translation task. Our method outperformed the baseline system by 1.67{\%} relative on a randomly selected testset and 8.56{\%} relative on the NIST 2008 testset in terms of BLEU score.
Much of the previous work on transliteration has depended on resources and attributes specific to particular language pairs. In this work, rather than focus on a single language pair, we create robust models for transliterating from all languages in a large, diverse set to English. We create training data for 150 languages by mining name pairs from Wikipedia. We train 13 systems and analyze the effects of the amount of training data on transliteration performance. We also present an analysis of the types of errors that the systems make. Our analyses are particularly valuable for building machine translation systems for low resource languages, where creating and integrating a transliteration module for a language with few NLP resources may provide substantial gains in translation performance.
We introduce a method for learning to translate out-of-vocabulary (OOV) words. The method focuses on combining sublexical/constituent translations of an OOV to generate its translation candidates. In our approach, wild-card searches are formulated based on our OOV analysis, aimed at maximizing the probability of retrieving OOVs{'} sublexical translations from existing resource of machine translation (MT) systems. At run-time, translation candidates of the unknown words are generated from their suitable sublexical translations and ranked based on monolingual and bilingual information. We have incorporated the OOV model into a state-of-the-art MT system and experimental results show that our model indeed helps to ease the negative impact of OOVs on translation quality, especially for sentences containing more OOVs (significant improvement).
The performance of current sentence alignment tools varies according to the to-be-aligned texts. We have found existing tools unsuitable for hard-to-align parallel texts and describe an alternative alignment algorithm. The basic idea is to use machine translations of a text and BLEU as a similarity score to find reliable alignments which are used as anchor points. The gaps between these anchor points are then filled using BLEU-based and length-based heuristics. We show that this approach outperforms state-of-the-art algorithms in our alignment task, and that this improvement in alignment quality translates into better SMT performance. Furthermore, we show that even length-based alignment algorithms profit from having a machine translation as a point of comparison.
This paper suggests a method for detecting cross-lingual semantic similarity using parallel PropBanks. We begin by improving word alignments for verb predicates generated by GIZA++ by using information available in parallel PropBanks. We applied the Kuhn-Munkres method to measure predicate-argument matching and improved verb predicate alignments by an F-score of 12.6{\%}. Using the enhanced word alignments we checked the set of target verbs aligned to a specific source verb for semantic consistency. For a set of English verbs aligned to a Chinese verb, we checked if the English verbs belong to the same semantic class using an existing lexical database, WordNet. For a set of Chinese verbs aligned to an English verb we manually checked semantic similarity between the Chinese verbs within a set. Our results show that the verb sets we generated have a high correlation with semantic classes. This could potentially lead to an automatic technique for generating semantic classes for verbs.
This paper presents a set of experiments on Domain Adaptation of Statistical Machine Translation systems. The experiments focus on Chinese-English and two domain-specific corpora. The paper presents a novel approach for combining multiple domain-trained translation models to achieve improved translation quality for both domain-specific as well as combined sets of sentences. We train a statistical classifier to classify sentences according to the appropriate domain and utilize the corresponding domain-specific MT models to translate them. Experimental results show that the method achieves a statistically significant absolute improvement of 1.58 BLEU (2.86{\%} relative improvement) score over a translation model trained on combined data, and considerable improvements over a model using multiple decoding paths of the Moses decoder, for the combined domain test set. Furthermore, even for domain-specific test sets, our approach works almost as well as dedicated domain-specific models and perfect classification.
This paper investigates varying the decoder weight of the language model (LM) when translating different parts of a sentence. We determine the condition under which the LM weight should be adapted. We find that a better translation can be achieved by varying the LM weight when decoding the most problematic spot in a sentence, which we refer to as a difficult segment. Two adaptation strategies are proposed and compared through experiments. We find that adapting a different LM weight for every difficult segment resulted in the largest improvement in translation quality.
The quality of statistical machine translation systems depends on the quality of the word alignments that are computed during the translation model training phase. IBM alignment models, as implemented in the GIZA++ toolkit, constitute the de facto standard for performing these computations. The resulting alignments and translation models are however very noisy, and several authors have tried to improve them. In this work, we propose a simple and effective approach, which considers alignment as a series of independent binary classification problems in the alignment matrix. Through extensive feature engineering and the use of stacking techniques, we were able to obtain alignments much closer to manually defined references than those obtained by the IBM models. These alignments also yield better translation models, delivering improved performance in a large scale Arabic to English translation task.
With the steadily increasing demand for high-quality translation, the localisation industry is constantly searching for technologies that would increase translator throughput, in particular focusing on the use of high-quality Statistical Machine Translation (SMT) supplementing the established Translation Memory (TM) technology. In this paper, we present a novel modular approach that utilises state-of-the-art sub-tree alignment and SMT techniques to turn the fuzzy matches from a TM into near-perfect translations. Rather than relegate SMT to a last-resort status where it is only used should the TM system fail to produce the desired output, for us SMT is an integral part of the translation process that we rely on to obtain high-quality results. We show that the presented system consistently produces better-quality output than the TM and performs on par or better than the standalone SMT system.
This paper examines the motivation, design, and practical results of several types of human evaluation tasks for machine translation. In addition to considering annotator performance and task informativeness over multiple evaluations, we explore the practicality of tuning automatic evaluation metrics to each judgment type in a comprehensive experiment using the METEOR-NEXT metric. We present results showing clear advantages of tuning to certain types of judgments and discuss causes of inconsistency when tuning to various judgment data, as well as sources of difficulty in the human evaluation tasks themselves.
A method is presented for incremental re-training of an SMT system, in which a local phrase table is created and incrementally updated as a file is translated and post-edited. It is shown that translation data from within the same file has higher value than other domain-specific data. In two technical domains, within-file data increases BLEU score by several full points. Furthermore, a strong recency effect is documented; nearby data within the file has greater value than more distant data. It is also shown that the value of translation data is strongly correlated with a metric defined over new occurrences of n-grams. Finally, it is argued that the incremental re-training prototype could serve as the basis for a practical system which could be interactively updated in real time in a post-editing setting. Based on the results here, such an interactive system has the potential to dramatically improve translation quality.
We propose a source-side decoding sequence language model for phrase-based statistical machine translation. This model is a reordering model in the sense that it helps the decoder find the correct decoding sequence. The model uses word-aligned bilingual training data. We show improved translation quality of up to 1.34{\%} BLEU and 0.54{\%} TER using this model compared to three other widely used reordering models.
Statistical machine translation (SMT) models have recently begun to include source context modeling, under the assumption that the proper lexical choice of the translation for an ambiguous word can be determined from the context in which it appears. Various types of lexical and syntactic features have been explored as effective source context to improve phrase selection in SMT. In the present work, we introduce lexico-syntactic descriptions in the form of supertags as source-side context features in the state-of-the-art hierarchical phrase-based SMT (HPB) model. These features enable us to exploit source similarity in addition to target similarity, as modelled by the language model. In our experiments two kinds of supertags are employed: those from lexicalized tree-adjoining grammar (LTAG) and combinatory categorial grammar (CCG). We use a memory-based classification framework that enables the efficient estimation of these features. Despite the differences between the two supertagging approaches, they give similar improvements. We evaluate the performance of our approach on an English-to-Dutch translation task, and report statistically significant improvements of 4.48{\%} and 6.3{\%} BLEU scores in translation quality when adding CCG and LTAG supertags, respectively, as context-informed features.
Machine Translation traditionally treats documents as sets of independent sentences. In many genres, however, documents are highly structured, and their structure contains information that can be used to improve translation quality. We present a preliminary approach to document translation that uses structural features to modify the behaviour of a language model, at sentence-level granularity. To our knowledge, this is the first attempt to incorporate structural information into statistical MT. In experiments on structured English/French documents from the Hansard corpus, we demonstrate small but statistically significant improvements.
In the hierarchical phrase based (HPB) translation model, in addition to hierarchical phrase pairs extracted from bi-text, glue rules are used to perform serial combination of phrases. However, this basic method for combining phrases is not sufficient for phrase reordering. In this paper, we extend the HPB model with maximum entropy based bracketing transduction grammar (BTG), which provides content-dependent combination of neighboring phrases in two ways: serial or inverse. Experimental results show that the extended HPB system achieves absolute improvements of 0.9∼1.8 BLEU points over the baseline for large-scale translation tasks.
Since most Korean postpositions signal grammatical functions such as syntactic relations, generation of incorrect Korean post-positions results in producing ungrammatical outputs in machine translations targeting Korean. Chinese and Korean belong to morphosyntactically divergent language pairs, and usually Korean postpositions do not have their counterparts in Chinese. In this paper, we propose a preprocessing method for a statistical MT system that generates more adequate Korean postpositions. We transfer syntactic relations of subject-verb-object patterns in Chinese sentences and enrich them with transferred syntactic relations in order to reduce the morpho-syntactic differences. The effectiveness of our proposed method is measured with lexical units of various granularities. Human evaluation also suggest improvements over previous methods, which are consistent with the result of the automatic evaluation.
We report findings from a user study with professional post-editors using a translation recommendation framework (He et al., 2010) to integrate Statistical Machine Translation (SMT) output with Translation Memory (TM) systems. The framework recommends SMT outputs to a TM user when it predicts that SMT outputs are more suitable for post-editing than the hits provided by the TM. We analyze the effectiveness of the model as well as the reaction of potential users. Based on the performance statistics and the users{'} comments, we find that translation recommendation can reduce the workload of professional post-editors and improve the acceptance of MT in the localization industry.
Although the scoring features of state-of-the-art Phrase-Based Statistical Machine Translation (PB-SMT) models are weighted so as to optimise an objective function measuring translation quality, the estimation of the features themselves does not have any relation to such quality metrics. In this paper, we introduce a translation quality-based feature to PB-SMT in a bid to improve the translation quality of the system. Our feature is estimated by averaging the edit-distance between phrase pairs involved in the translation of oracle sentences, chosen by automatic evaluation metrics from the N-best outputs of a baseline system, and phrase pairs occurring in the N-best list. Using our method, we report a statistically significant 2.11{\%} relative improvement in BLEU score for the WMT 2009 Spanish-to-English translation task. We also report that using our method we can achieve statistically significant improvements over the baseline using many other MT evaluation metrics, and a substantial increase in speed and reduction in memory use (due to a reduction in phrase-table size of 87{\%}) while maintaining significant gains in translation quality.
In this paper, we propose a novel model for scoring reordering in phrase-based statistical machine translation (SMT) and successfully use it for translation from Farsi into English and Arabic. The model replaces the distance-based distortion model that is widely used in most SMT systems. The main idea of the model is to penalize each new deviation from the monotonic translation path. We also propose a way for combining this model with manually created reordering rules for Farsi which try to alleviate the difference in sentence structure between Farsi and English/Arabic by changing the position of the verb. The rules are used in the SMT search as soft constraints. In the experiments on two general-domain translation tasks, the proposed penalty-based model improves the BLEU score by up to 1.5{\%} absolute as compared to the baseline of monotonic translation, and up to 1.2{\%} as compared to using the distance-based distortion model.
We propose a Chinese dependency tree reordering method for Chinese-to-Korean SMT systems through analyzing systematic differences between the Chinese and Korean languages. Translating predicate-predicate patterns in Chinese into Korean raises various issues such as long-distance reordering. This paper concentrates on syntactic reordering of predicate-predicate patterns in Chinese dependency trees through contrastively analyzing construction types in Chinese and their corresponding translations in Korean. We explore useful linguistic knowledge that assists effective syntactic reordering of Chinese dependency trees; we design two experiments with different kinds of linguistic knowledge combined with the phrase and hierarchical phrase-based SMT systems, and assess the effectiveness of our proposed methods. The experiments achieved significant improvements by resolving the long-distance reordering problem.
We present a conditional-random-field approach to discriminatively-trained phrase-based machine translation in which training and decoding are both cast in a sampling framework and are implemented uniformly in a new probabilistic programming language for factor graphs. In traditional phrase-based translation, decoding infers both a ``Viterbi'' alignment and the target sentence. In contrast, in our approach, a rich overlapping-phrase alignment is produced by a fast deterministic method, while probabilistic decoding infers only the target sentence, which is then able to leverage arbitrary features of the entire source sentence, target sentence and alignment. By using SampleRank for learning we could in principle efficiently estimate hundreds of thousands of parameters. Test-time decoding is done by MCMC sampling with annealing. To demonstrate the potential of our approach we show preliminary experiments leveraging alignments that may contain overlapping bi-phrases.
In this work we give a detailed comparison of the impact of the integration of discriminative and trigger-based lexicon models in state-of-the-art hierarchical and conventional phrase-based statistical machine translation systems. As both types of extended lexicon models can grow very large, we apply certain restrictions to discard some of the less useful information. We show how these restrictions facilitate the training of the extended lexicon models. We finally evaluate systems that incorporate both types of models with different restrictions on a large-scale translation task for the Arabic-English language pair. Our results suggest that extended lexicon models can be substantially reduced in size while still giving clear improvements in translation performance.
This paper describes successful applications of discriminative lexicon models to the statistical machine translation (SMT) systems into morphologically complex languages. We extend the previous work on discriminatively trained lexicon models to include more contextual information in making lexical selection decisions by building a single global log-linear model of translation selection. In offline experiments, we show that the use of the expanded contextual information, including morphological and syntactic features, help better predict words in three target languages with complex morphology (Bulgarian, Czech and Korean). We also show that these improved lexical prediction models make a positive impact in the end-to-end SMT scenario from English to these languages.
System combination exploits differences between machine translation systems to form a combined translation from several system outputs. Core to this process are features that reward n-gram matches between a candidate combination and each system output. Systems differ in performance at the n-gram level despite similar overall scores. We therefore advocate a new feature formulation: for each system and each small n, a feature counts n-gram matches between the system and candidate. We show post-evaluation improvement of 6.67 BLEU over the best system on NIST MT09 Arabic-English test data. Compared to a baseline system combination scheme from WMT 2009, we show improvement in the range of 1 BLEU point.
Paraphrase generation is useful for various NLP tasks. But pivoting techniques for paraphrasing have limited applicability due to their reliance on parallel texts, although they benefit from linguistic knowledge implicit in the sentence alignment. Distributional paraphrasing has wider applicability, but doesn{'}t benefit from any linguistic knowledge. We combine a distributional semantic distance measure (based on a non-annotated corpus) with a shallow linguistic resource to create a hybrid semantic distance measure of words, which we extend to phrases. We embed this extended hybrid measure in a distributional paraphrasing technique, benefiting from both linguistic knowledge and independence from parallel texts. Evaluated in statistical machine translation tasks by augmenting translation models with paraphrase-based translation rules, we show our novel technique is superior to the non-augmented baseline and both the distributional and pivot paraphrasing techniques. We train models on both a full-size dataset as well as a simulated {``}low density{''} small dataset.
Speaking both as an experienced independent technical translator and on behalf of the 11,000 members of the American Translators Association (ATA), the Association's president will explain the training, standards, objectives and working methods of the people who convey meaning from one language into another as their chosen profession. The role of machine translation in that undertaking will be discussed, but the presenter's main objective is to give the translator's view of human translation, describing a process that conveys authentic voices based on much more than words, that in order to be faithful cannot be merely generic and interchangeable, and that gives individual attention to every human utterance. The truth about translators will also be revealed: they are early and eager adopters of language technology when it serves their purpose, and they welcome any opportunity to advance the cause of communication. The goal of this presentation is to contribute to better understanding on both sides, and to help establish a productive and mutually beneficial relationship between MT professionals and the entire community of translators and interpreters represented by ATA. 
We describe a case study that presents a framework for examining whether Machine Translation (MT) output enables translation professionals to translate faster while at the same time producing better quality translations than without MT output. We seek to find decision factors that enable a translation professional, known as a Paralinguist, to determine whether MT output is of sufficient quality to serve as a {``}seed translation{''} for post-editors. The decision factors, unlike MT developers{'} automatic metrics, must function without a reference translation. We also examine the correlation of MT developers{'} automatic metrics with error annotators{'} assessments of post-edited translations.
Automated translation can assist with a variety of translation needs in government, from speeding up access to information for intelligence work to helping human translators increase their productivity. However, government entities need to have a mechanism in place so that they know whether or not they can trust the output from automated translation solutions. In this presentation, Language Weaver will present a new capability ``TrustScore'': an automated scoring algorithm that communicates how good the automated translation is, using a meaningful metric. With this capability, each translation is automatically assigned a score from 1 to 5 in the TrustScore. A score of 1 would indicate that the translation is unintelligible; a score of 3 would indicate that meaning has been conveyed and that the translated content is actionable. A score approaching 4 or higher would indicate that meaning and nuance have been carried through. This automatic prediction of quality has been validated by testing done across significant numbers of data points in different companies and on different types of content. After outlining TrustScore, and how it works, Language Weaver will discuss how a scoring mechanism like TrustScore could be used in a translation productivity workflow in government to assist linguists with day to day translation work. This would enable them to further benefit from their investments in automated translation software. Language Weaver would also share how TrustScore is used in commercial deployments to cost effectively publish information in near real time.
The present study examines from users' perspective the performance of Google's online translation service on the documents of the United Nations. Since at least 2004, United Nations has been exploring, piloting, and implementing computer assisted translation (CAT) with Trados as an officially selected vehicle. A more recent development is the spontaneous adoption of Google translation among Chinese translators as an easy, versatile, and labor-saving tool. With machine translation getting real among developers and end-users, there seems to be a need to conduct a reality check to see how well it serves its purpose. The current study examines Google translation and its degree of assistance to the Chinese professional translators at the United Nations in particular. It uses a variety of UN documents to test and evaluate the performance of Google translation from English to Chinese. The sampled UN documents consist of 3 resolutions, 2 letters, 2 provisional agendas, 1 plenary verbatim, 1 report, 1 note by the Secretariat, and 1 budget. The results vindicate Google's cutting edge in machine translation when English to Chinese is concerned, thanks to its powerful infrastructure and immense translation database. The conversion between the two languages takes only an instant, even for a fairly long piece. On top of that, Google gets terminology right more frequently and seems better able to make an intelligent guess when compared with other translation tools like MS Bing. But Google's Chinese is far from intelligible, especially at the sentence level, primarily because of serious problems with word order and sentence parsing. There are also technical problems like adding or omitting words and erroneous rendering of numbers. Nevertheless, Google translation offers translators an option to work on its rough draft for the benefit of saving time and pain in typing. The challenges of post-editing, however, may offset the time saved. Even though Google translation may not necessarily net in speed gains when it is used to assist translation, it certainly is a beneficial labor saver, including mental labor when it performs at its very best.
The Foreign Media Collaboration Framework (FMCF) is the latest approach by NASIC to provide a comprehensive system to process foreign language materials. FMCF is a Services Oriented Architecture (SOA) that provides an infrastructure to manage HLT tools, products, workflows, and services. This federated SOA solution adheres to DISA's NCES SOA Governance Model, DDMS XML for Metadata Capture/Dissemination, and IC-ISM for Security. The FMCF provides a cutting edge infrastructure that encapsulates multiple capabilities from multiple vendors in one place. This approach will accelerate HLT development, contain sustainment cost, minimize training, and brings the MT, OCR, ASR, audio/video, entity extraction, analytic tools and database under one umbrella, thus reducing the total cost of ownership.
Social media and tools for communication over the Internet have expanded a great deal in recent years. This expansion offers a diverse set of users a means to communicate more freely and spontaneously in mixed languages and genres (blogs, message boards, chat, texting, video and images). Dialectal Arabic is pervasive in written social media, however current state of the art tools made for Modern Standard Arabic (MSA) fail on Arabic dialects. COLABA enables MSA users to interpret dialects correctly. It helps find Arabic colloquial content that is currently not easily searchable and accessible to MSA queries. The COLABA team has built a suite of tools that will offer users the ability to anonymously capture online unstructured media content from blogs to comprehend, organize, and validate content from informal and colloquial genres of online communication in MSA and a variety of Arabic dialects. The DoD/Combating Terrorism Technical Support Office/Technical Support Working Group (CTTSO/TSWG) awarded the contract to Acxiom Corporation and partners from MTI/IBM, Columbia University, Janya and Wichita State University to bring joint expertise to address this challenge. The suite has several use applications: Support for language and cultural learning by making colloquial Arabic intelligible to students of MSA; Retrieval and prioritization for triage and content analysis by finding Arabic colloquial and dialect terms that today's search engines miss; by providing appropriate interpretations of colloquial Arabic, which is opaque to current analytics approaches; and by Identify named entities, events, topics, and sentiment. Enabling improved translations by MSA-trained MT systems through decreases in out-of-vocabulary terms achieved by means of colloquial term conversion to MSA.
It is common practice that linguists will do MT post-editing to improve translation accuracy and fluency. This presentation however, examines the importance of pre-editing source material to improve MT. Even when a digital source file which is literally correct is used for MT, there are still some factors that have significant effect on MT translation accuracy and fluency. Based on 35 examples from more than 20 professional journals and websites, this article is about an experiment of pre-editing source material for Chinese-English MT in the S and T domain. Pertinent examples are selected to illustrate how machine translation accuracy and fluency can be enhanced by pre-editing which includes the following four areas: to provide a straightforward sentence structure, to improve punctuation, to use straightforward wording, and to eliminate redundancy and superfluous elements.
Professional language analysts leverage a myriad of tools in their quest to produce accurate translations of foreign language material. The effectiveness of these tools ultimately affects resource allocation, information dissemination and subsequent follow-on mission planning; all three of which are vital, time-critical components in the intelligence cycle. This presentation will highlight the need for interactive tools that perform jointly in an operational environment, focusing on a dynamic suite of foreign language tools packaged into a desktop application and serving in a machine translation role. Basis Technology's Arabic/Afghan Desktop Suite (ADS) supports DOMEX, CELLEX, and HUMINT missions while being the most powerful Arabic, Dari and Pushto text analytic and processing software available. The ADS translates large scale lists of names from foreign language to English and also pinpoints place names appearing in reports with their coordinate locations on maps. With standardization output having to be more accurate than ever, the ADS ensures conformance with USG transliteration standards for Arabic script languages, including IC, BGN/PCGN, SATTS and MELTS. The ADS enables optimization of your limited resources and allows your analysts and linguists to be tasked more efficiently throughout the workflow process.
CACI has developed and delivered systems for document exploitation and processing to Government customers around the world. Many of these systems include advanced language processing capabilities in order to enable rapid triage of vast collections of foreign language documents, separating the content that requires immediate human attention from the less immediately pressing material. AppTek provides key patent-pending Machine Translation technology for this critical process, rendering material in Arabic, Farsi and other languages into an English rendition that enables both further automated processing and rapid review by monolingual analysts, to identify the documents that require immediate linguist attention. Both CACI and AppTek have been working with customers to develop capabilities that enable them, the users, to be the ones in command of making their systems learn and continuously improve. We will describe how we put this critical user requirement into the systems and the key role that the user's involvement played in this. We will also discuss some of the key components of the system and what the customer-centric evolution of the system will be, including our document translation workflow, the machine translation technology within it, and our approaches to supporting the technology and sustaining its success designed around adapting to user needs.
A panel of industry and government experts will discuss ways in which they have applied task-based evaluation for Machine Translation and other language technologies in their organizations and share ideas for new methods that could be tried in the future. As part of the discussion, the panelists will address some of the following points: What task-based evaluation means within their organization, i.e., how task-based evaluation is defined; How task-based evaluation impacts the use of MT technologies in their work environment; Whether task-based evaluation correlates with MT developers' automated metrics and if not, how do we arrive at automated metrics that do correlate with the more expensive task-based evaluation; What ``lessons-learned'' resulted from the course of performing task-based evaluation; How task-based evaluations can be generalized to multiple workflow environments.
In spite of low literacy levels in Afghanistan and the Tribal Areas of Pakistan, the Pashto and Dari regions of the World Wide Web manifest diverse content from authors with a broad range of viewpoints. We have used cross-language information retrieval (CLIR) with machine translation to explore this content, and present an informal study of the principal genres that we have encountered. The suitability and limitations of existing machine translation packages for these languages for the exploitation of this content is discussed.
Current state-of-the-art in speech recognition, machine translation, and natural language processing (NLP) technologies has allowed the development of powerful media monitoring systems that provide today's analysts with automatic tools for ingesting and searching through different types of data, such as broadcast video, web pages, documents, and scanned images. However the core human-language technologies (HLT) in these media monitoring systems are static learners, which mean that they learn from a pool of labeled data and apply the induced knowledge to operational data in the field. To enable successful and widespread deployment and adoption of HLT, these technologies need to be able to adapt effectively to new operational domains on demand. To provide the US Government analyst with dynamic tools that adapt to these changing domains, these HLT systems must support customizable lexicons. However, the lexicon customization capability in HLT systems presents another unique challenge especially in the context of multiple users of typical media monitoring system installations in the field. Lexicon customization requests from multiple users can be quite extensive, and may conflict in orthographic representation (spelling, transliteration, or stylistic consistency) or in overall meaning. To protect against spurious and inconsistent updates to the system, the media monitoring systems need to support a central terminology management capability to collect, manage, and execute customization requests across multiple users of the system. In this talk, we will describe the integration of a user-driven lexicon/dictionary customization and terminology management capability in the context of the Raytheon BBN Web Monitoring System (WMS) to allow intelligence analysts to update the Machine Translation (MT) system in the WMS with domain- and mission-specific source-to-English phrase translation rules. The Language Learning Broker (LLB) tool from the Technology Development Group (TDG) is a distributed system that supports dictionary/terminology management, personalized dictionaries, and a workflow between linguists and linguist management. LLB is integrated with the WMS to provide a terminology management capability for users to submit, review, validate, and manage customizations of the MT system through the WMS User Interface (UI). We will also describe an ongoing experiment to measure the effectiveness of this user-driven customization capability, in terms of increased translation utility, through a controlled experiment conducted with the help of intelligence analysts.
In today's post 9/11 world, the need for qualified linguists to process all the foreign language materials that are collected/confiscated overseas and at home has grown considerably. To date, a gap exists in the number of linguists needed to process all this material. To fill this gap, the government has invested in the research, development and implementation of Human Language Technologies into the linguist workflow. Most of the current DOMEX workflows incorporate HLT tools, whether that is Machine Translation, Named Entity Extraction, Name Normalization or Transliteration tools. These tools aid the linguists in processing and translating DOMEX material, cutting back on the amount of time needed to sift through all the material. In addition to the technologies used in workflow processes, we have also implemented tools for intelligence analysts, such as the Broadcast Monitoring System and Tripwire. These tools allow non-language qualified analysts to search through foreign language material and exploit that material for intelligence value. These tools implement such technologies as Speech-to-text and machine translation. Part of this effort to fill the gap in the ability to process all this information has been collaboration amongst the members of the Intelligence Community on the research and development of tools. This type of engagement allows the government to save time and money in eliminating the duplication of efforts and allows government agencies to share their ideas and expertise. Our presentation will address some of the tools that are currently in use throughout DoD; being considered for use; some of the challenges we face; and how we are making best use of the HLT development and research that is supporting our needs.
In this paper, we describe WeBiText (www.webitext.ca) and how it is being used. WeBiText is a concordancer that allows translators to search in large, high-quality multilingual web sites, in order to find solutions to translation problems. After a quick overview of the system, we present results from an analysis of its logs, which provides a picture of how the tool is being used and how well it performs. We show that it is mostly used to find solutions for short, two or three word translation problems. The system produces at least one hit for 58{\%} of the queries, and hits from at least five different web pages in 41{\%} of cases. We show that 36{\%} of the queries correspond to specialized language problems, which is much higher than what was previously reported for a similar concordancer based on the Canadian Hansard (TransSearch). We also provide a back of the envelope calculation of the current economic impact of the tool, which we estimate at {\$}1 million per year, and growing rapidly.
The presentation will focus on ongoing work to develop sentence-aligned Chinese-English data for machine translation customization. Fully automatic alignment produces noisy data (e.g., containing OCR and alignment errors), and we are looking at the question of just how noisy noisy data can be and still produce translation improvements. Related, data clean-up efforts are time- and labor-intensive and we are examining whether translation improvements justify the clean-up costs.
Language Now is a natural language processing (NLP) research and development program with a goal of improving the performance of machine translation (MT) and other NLP technologies in mission-critical applications. The Language NOW research and development program has produced the following four primary advances as Government license-free technology: 1) A consistent and simple user interface developed to allow non-technical users, regardless of language proficiency, to use NLP technology in exploiting foreign language text content. Language NOW research has produced first-of-a-kind capabilities such as detection and handling of structured data, direct processing and visualization of foreign language data with transliterations and translations. 2) A highly efficient NLP integration framework, the Abstract Scalable Language Services (ASLS). ASLS offers system developers easy implementation of an efficient integrated service oriented architecture suitable for devices ranging from handheld computers to large enterprise computer clusters. 3) Service wrappers integrating commercial, Government license-free, open source and research software that provide NLP services such as machine translation, named entity recognition, optical character recognition (OCR), transliteration and text search. 4) STatistical Engines for Language Analysis (STELAE) and Maximum Entropy Extraction Pipeline (MEEP) tools that produce customized statistical machine translation and hybrid statistical/rule-based named entity recognition engines.
Parallel corpora have traditionally been created, maintained and disseminated by translators and analysts addressing specific domains. They grow by aggregation, individual contributions taking residence in the knowledge base. While the provenance of these new terms is known, their validity is not; they must be vetted by domain and language experts in order to be considered for use in the translation process. In order to address the evolving ecosphere surrounding parallel corpora, developers and analysts need to move beyond the data limitations of the static model. This traditional model does not fully take advantage of new infiltration and exfiltration datapaths available in today's world of distributed knowledge bases. Incoming data are no longer simply textual-audio, imagery and video are all critical components in corpora utility. Corpora maintainers have access to these media types through a variety of data sources, such as automated media monitoring services, the output of any number of translation environments, and translation memory exchanges (TMXs) developed by domain and language experts. These input opportunities are often pre-vetted and ready for automated inclusion into the parallel corpora; their content should not be reduced to the strictly textual. Unfortunately, the quality of the automated alignment and segmentation systems used in these automated systems remains a concern for the bulk preprocessing needed for downstream systems. These data sources share a common characteristic, that of known provenance. They are typically a vetted source and a regular provider to the parallel corpora, whether via daily newscasts or other means. Other data sources are distributed in nature and thus offer distinct challenges to the collection, vetting and exploitation processes. One of the most exciting of such an infiltration path is crowdsourcing. A next-generation parallel corpora management system must be capable of, if not necessarily automatically incorporating crowdsourced terminology as a vetted source, facilitating manual inclusion of vetted crowdsourced terminology. This terminology may be submitted in any scale from practically any source. It may overlap or be contradictory - it almost certainly will require some degree of analysis and evaluation before inclusion. Fortunately, statistical analysis techniques are available to mitigate these concerns. One significant benefit to a crowdsourcing approach is the gains in alignment and segmentation accuracy over similar products offered by the automated systems mentioned above. Given the scalability of crowdsourcing methods, it is certainly a viable framework for bulk alignment and segmentation. Another consideration for the development of distributed parallel corpora systems is their position in the translation workflow. The outputs and exfiltration paths of such a system can be as used for such diverse purposes as addition to existing TMXs, refinement of existing MT applications (through either improvement of their learning processes or inclusion of parallel-corpora generated domain-specific lexicons), creation of sentence pairs and other products for language learning system (LLS) systems, and support for exemplar language clips such as those developed by the State Department.
This briefing addresses the development of a conversion table that will enable a translator to render Chinese names, locations, and nomenclature into proper Pinyin. As a rule, Russian Machine Translation is a robust system that provides good results. It is a mature system with extensive glossaries and can be useful for translating documents across many disciplines. However, as a result of the transliteration process, Russian MT will not convert Chinese terms from Russian into the Pinyin standard. This standard is used by most databases and the internet. Currently the MT software is performing as it was designed, but this problem impacts the accuracy of the MT making it almost useless for many purposes including data retrieval.
In this paper, we describe the methods used to develop an exchangeable translation memory bank of sentence-aligned Mandarin Chinese - English sentences. This effort is part of a larger effort, initiated by the National Virtual Translation Center (NVTC), to foster collaboration and sharing of translation memory banks across the Intelligence Community and the Department of Defense. In this paper, we describe our corpus creation process - a largely automated process - highlighting the human interventions that are still deemed necessary. We conclude with a brief discussion of how this work will affect plans for NVTC's new translation management workflow and future research to increase the performance of the automated components of the corpus creation process.
This paper presents a language vendor's perspective on the actual implementation of machine translation solutions in the translation/localization process. This lecture will be delivered at AMTA-2010 Conference, and a short video will accompany lecturer's speech.
This document describes the use of MT at GLTaC and provides an approach to determining if offering MT services is right for you. There is no single answer or approach to providing MT services so this is just one way an LSP has chosen to provide MT services.
This paper aims to give an insight into some of the challenges and opportunities from implementing machine translation in an enterprise environment. This is written from a business perspective rather than a technical one and highlights how Applied Language Solutions has designed and rolled out a series of customer specific machine translation solutions within our Enterprise.
PangeaMT is presented from our standpoint as a LSP keen to develop and implement a cost-effective translation automation strategy that is also in line with our full commitment to open standards. Moses lies at the very core of PangeaMT but we have built several pre-/post-processing modules around it, from word reordering to inline mark-up parser to TMX/XLIFF filters. These represent interesting breakthroughs in real-world, customized SMT applications.
Over the last two years, Adobe Systems has incorporated Machine Translation with post-editing into the localization workflow. Currently, the number of products using MT for localization has grown to over a dozen, and the number of languages covered is now five. Adobe is continuing to expand the number of products which use MT for localization, and is also looking beyond localization to other applications of MT technology. In this paper, we discuss some of our further use cases, and the varying requirements each use case has for quality, customization, cost, and other factors. Based on those varying requirements, we consider a range of MT solutions beyond our current model of licensed, customized commercial engines.
Avaya identified machine translation and post-editing as the next step in their strategy for global information management to deliver against the ever-present business objectives of {``}Increased Efficiency and Additional Localized Content{''}. Avaya shares how they assessed the market and selected their chosen vendor.
Although still in a nascent state as a professional translation tool, customized SMT engines already have multiple applications, each of which require clear definitions about quality and productivity. Three engine-training scenarios have emerged which are representative of real-world applications for the development and use of a customized SMT engines based on the availability of data. In the case that limited or no bilingual training data is available, a unique development process can be used to harvest and translate n-grams directly. Using this approach Asia Online and Moravia IT have successfully customized SMT engines for use in various domains. A partnership between an MT engine provider and a qualified LSP is essential to deliver quality results using this approach.
With pressure to offer content in many languages, many companies are considering machine translation for faster delivery and lower translation costs, yet MT is notorious for poor quality translation. How can you improve your content quality to make MT work for you? High quality source content eliminates many of the common roadblocks for using machine translation effectively. In this presentation, Jennifer Beaupre, Marketing Director and Kent Taylor, GM, acrolinx, will review what best practices have taught us about these topics: 1 Why is source content important when using machine translation? 2 How does source content affect translation costs? 3 How can source content improve the quality of MT output?
This paper discusses how to measure the impact of online content localized by machine translation in meeting the business need of commercial users, i.e., reducing the volume of telephone calls to the Call Center (call deflection). We address various design, conceptual and practical issues encountered in proving the value of machine translation and conclude that the approach that will give the best result is one that reconciles end-user (human evaluation) feedback with web and Call Center data.
This paper outlines the methodologies Microsoft has deployed for seamless integration of human translation into the translation workflow, and describes a variety of methods to gather and collect human translation data. Increased amounts of parallel training data help to enhance the translation quality of the statistical MT system in use at Microsoft. The presentation covers the theory, the technical methodology as well as the experiences Microsoft has with the implementation, and practical use of such a system. Included is a discussion of the factors influencing the translation quality of a statistical MT system, a short description of the feedback collection mechanism in use at Microsoft, and the metrics it observed on its MT deployments.
CA{'}s globalization team has a long term goal of reaching fully loaded costs of 10 cents per word. Fully loaded costs include the costs incurred for translation, localization QA, engineering, project management, and overall management. While translation budgets are gradually decreasing and volumes increasing, machine translation becomes an alternative source to produce more with less. This paper describes how CA Technologies tries to accomplish this long term goal with the deployment of MT systems to increase productivity with less cost, in a relatively short time.
The open source statistical machine translation toolkit Moses has recently drawn a lot of attention in the localization industry. Companies see the chance to use Moses to leverage their existing translation assets and integrate MT into their localization processes. Due to the academic origins of Moses there are some obstacles to overcome when using it in an industry setting. In this paper we discuss what these obstacles are and how they are addressed by the newly established Moses for Localization open source project. We describe the different components of the project and the benefits a company can gain from using this open source project.
In this presentation, we focus on integrating machine translation (MT) into an existing corporate localization and translation workflow. This MT extended workflow includes a customized post-editing sub-workflow together with crowdsourced, incentives based translation evaluation feedback routines that enable automated learning processes. The core of the implementation is a semantic repository that comprises the necessary information artifacts and links to language resources to organize, manage and monitor the different human and machine roles, tasks, and the entire lifecylce of the localization and translation supply chain(s).
PLuTO {--} Patent Language Translation Online {--} is a partially EU-funded commercialization project which specializes in the automatic retrieval and translation of patent documents. At the core of the PLuTO framework is a machine translation (MT) engine through which web-based translation services are offered. The fully integrated PLuTO architecture includes a translation engine coupling MT with translation memories (TM), and a patent search and retrieval engine. In this paper, we first describe the motivating factors behind the provision of such a service. Following this, we give an overview of the PLuTO framework as a whole, with particular emphasis on the MT components, and provide a real world use case scenario in which PLuTO MT services are ex- ploited.
This paper highlights the results and trends on post-editing and machine translation from the recent AMTA and SDL Automated Translation Survey. Then Continental Airlines and SDL share their experiences, and the benefits and challenges of human post-editing.
This paper describes PROMT system deployment at PayPal including: PayPal localization process challenges and requirements to a machine translation solution; Technical specifications of PROMT Translation Server Developer Edition; Linguistic customization performed by PROMT team for PayPal; Engineering Customization performed by PROMT team for PayPal; Additional customized development performed by PROMT team on behalf of PayPal; PROMT engine and PayPal productivity gains and cost savings.
