Abstract. This paper describes an alternative formalism for Thai syntax parsing based on a lexicalized tree adjoining grammar (LTAG). We first briefly present some formal background concerning LTAG, which is necessary for an understanding of LTAG and its application to Thai. Specifically, we address several issues regarding difficulties in parsing Thai sentences and how to resolve these issues using LTAG. Such difficulties arise for several reasons as follows. For one thing, Thai sentences do not contain delimiters or blanks between words while Thai words lack inflectional and derivational affixes. Moreover, inconsistent ordering relations within and across phrasal categories characterize Thai sentences as well as the fact that they sometimes contain discontinuous sentence constituents in their construction. Finally, we discuss future research on a novel almostparsing approach based on LTAG for handling compound multi-word extraction in automatic Thai word segmentation. Keywords: Thai-LTAG, Thai syntax parsing 
Keywords: Chinese word segmentation, Conditional Random Fields, Machine learning, Out-of-Vocabulary words, Web data. 
 b  Likun Qiu , Kai Zhao , and Changjian Hu  a Department of Chinese Language and Literature, Peking University, China b NEC Laboratories, China, {qiulikun, zhaokai, huchangjian}@research.nec.com.cn  Abstract. This paper proposes a hybrid model to address the task of sense guessing for Chinese unknown words. Three types of similarity, i.e., positional, syntactic and semantic similarity, are analyzed; and three models are developed accordingly. Then the three models are combined to form a hybrid one (HPPS Model). To verify the effectiveness and consistency of HPPS, experiments were conducted on ten test sets which were collected from two popular Chinese thesauruses Cilin and HowNet. In addition, extra experiments were made on a test set of 2000 words which were collected from newspaper. The experiments show that HPPS Model consistently produces 4%~6% F-score improvement over the best results reported in previous researches. Keywords: Semantic category, Unknown word, LC Principle, Semantic classification. 
Keywords: Collaborative Filtering, Single Document Summarization, Personalized Summarization, Tag Recommendation 
 b  a  Han Ren , Donghong Ji , Jing Wan , and Chong Teng  a School of Computer Science, Wuhan University 129 Luoyu Road, Wuhan 430079, China cslotus@mail.whu.edu.cn, donghong_ji@yahoo.com, tengchong@whu.edu.cn b School of Chinese Language & Literature, Wuhan University 129 Luoyu Road, Wuhan 430079, China jennifer_wanj@yahoo.com.cn  Abstract. Current researches on Question Answering concern more complex questions than factoid ones. Since definition questions are investigated by many researches, how to acquire accurate answers still becomes a core problem for definition QA. Although some systems use web knowledge bases to improve answer acquisition, we propose an approach that leverage them in an effective way. After summarizing definitions from web knowledge bases and merge them to a definition set, a two-stage retrieval model based on Probabilistic Latent Semantic Analysis is produced to seek documents and sentences in which the topic is similar to those in definition set. Then, an answer ranking model is employed to select both statistically and semantically similar sentences between sentences retrieved and sentences in definition set. Finally, sentences are ranked as answer candidates according to their scores. Experiments indicate following conclusions: 1) specific summarization technologies improves definition QA systems to a better performance; 2) topic based models can be more helpful than centroid-based models for definition QA systems in solving synonym and data sparse problems; 3) shallow semantic analysis is effective to find discriminative characteristics of definitions automatically. Keywords: web knowledge bases, question answering, definition questions, probabilistic latent semantic analysis. 
Keywords: Thematic Roles, Lexicons, Lexicon Constructing Systems, Filipino Language, Natural Language Processing. 
School of Computing, University of Utah, 50 S Central Campus Drive 3190, Salt Lake City UT 84112 jags@cs.utah.edu Abstract. One of the main purposes of a summary is to be able to replace the original document. We assume that a summary would be able to replace or act as a substitute for the document if its probability distribution is similar to that of the original document. With this hypothesis in mind, we carry out our analysis using datasets from Document Understanding Conference (DUC), studying the results of unigram probability models and also looking at Kullback-Leibler Divergence (KLD) as a measure of ‘closeness’ between probability models of the documents and their summaries. In this paper, we also discuss two summary generation approaches that were designed based on the above hypothesis (a) Summary generation by extraction of sentences based on its coverage and (b) Minimum KLD Summary Generation Method (uses a metric for reducing redundancy). Our research shows that the above summarizer, which is light and simple, can deliver good summaries comparable to other state-of-the-art systems. Keywords: multi-document summarization, frequency, unigram model, KL-Divergence 
Keywords: Question Answering, Passage Retrieval, Statistical Machine Translation Models, Query Expansion 
Abstract. Chinese semantic units fall into a continuum of connection tightness, ranging from very tight, non-compositional expressions, tight compositional words, phrases, and then to loose more or less arbitrary combinations of words. We propose an approach to measure tightness connection within this continuum, based on document frequency of segmentation patterns in a reference corpus. A variety of corpora, including search engine snippets, search engine results derived from query logs, as well as standard corpora have been investigated. Our tightness ranking on 300 phrases is quite close to their manual ranking, and non-compositional compound extraction can achieve a precision as high as 94.3% on the top 1,000 4-grams extracted from the Chinese Gigaword corpus.  Keywords: Compounds in Mandarin, Collocation, Compositionality.  
Keywords: hard syntactic rules, soft syntactic rules, effective integration, phrase-based translation 
Abstract. Pursuing on the analysis of product reviews, a bootstrapping method is proposed to find the product features and opinion words in iterative steps. Different from conventional methods, a graph model is built to link and measure the relationship between the pairs of product features and opinion words. A rule-based method is presented to get the initial seeds of product features and opinion words automatically. Our experimental results on electronic product reviews are encouraging, which prove the proposed method and techniques are effective in performing this task of feature level opinion mining. Keywords: opinion mining, product reviews, bootstrapping, graph model 
b  b,c  b  Weishi Zhang , Kai Zhao , Likun Qiu , and Changjian Hu  a School of Software, Tsinghua University, Beijing, China, zhang-ws08@mails.tsinghua.edu.cn b NEC Laboratories, China, {zhaokai, qiulikun, huchangjian}@research.nec.com.cn c Department of Chinese Language and Literature, Peking University, China.  Abstract. This paper presents a method for sentiment classification, called SESS (SElfSupervised and Syntax-Based method). SESS includes three phases. Firstly, some documents are initially classified based on a sentiment dictionary, and then the sentiments of phrases and documents are iteratively revised. This phase provides some accurately labeled data for the second phase. Secondly, a machine learning model is trained with the labeled data. Thirdly, the acquired model applies on the whole data set to get the final classification result. Moreover, to improve the quality of labeled data, the affect of compound and complex sentences on clause sentiment is examined. For three types of compound and complex sentences, i.e., coordination, concession or condition sentence, the clause sentiment is revised accordingly. Experiments show that, as an unsupervised method, SESS achieves comparative performance to state-of-the-art supervised methods on the same data. Keywords: Self-supervised, syntax-based, opinion mining, sentiment classification. 
Keywords: Dependency parsing, Hybrid approach, Modularity, Indian language parsing, Constraint based parsing. 
 a  Hiram Calvo , Kentaro Inui , and Yuji Matsumoto  a Computational Linguistics, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan {calvo, inui, matsu}@is.naist.jp b Artificial Intelligence, Center for Computing Research, National Polytechnic Institute, Mexico City, DF, 07738, Mexico hcalvo@cic.ipn.mx  Abstract. Learning Plausible Verb Arguments allows to automatically learn what kind of activities, where and how, are performed by classes of entities from sparse argument co-occurrences with a verb; this information it is useful for sentence reconstruction tasks. Calvo et al. (2009b) propose a non language-dependent model based on the Word Space Model for calculating the plausibility of candidate arguments given one verb and one argument, and compare with the single latent variable PLSI algorithm method, outperforming it. In this work we replicate their experiments with a different corpus, and explore variants to the PLSI method in order to explore further capabilities of this latter widely used technique. Particularly, we propose using an interpolated PLSI scheme that allows the combination of multiple latent semantic variables, and validate it in a task of identifying the real dependency-pair triple with regard to an artificially created one, obtaining up to 83% recall. Keywords: Plausible Verb Arguments, K-Nearest Neighbors algorithm, KNN, Distributional Thesaurus, Probabilistic Latent Semantic Indexing, PLSI. 
Keywords: context-based searching, disambiguation, simplified-traditional Chinese term alignment, lexical semantics, statistical machine translation 
 a  Siaw-Fong Chung , F.Y. August Chao , and Yi-Chen Hsieh  a Department of English, National Chengchi University, No. 64, ZhiNan Road Section 2, Wenshan District, Taipei City 11605, Taiwan {sfchung, 96551016}@nccu.edu.tw b Department of Management Information Systems, National Chengchi University, No. 64, ZhiNan Road Section 2, Wenshan District, Taipei City 11605, Taiwan fychao.tw@gmail.com  Abstract. This paper introduces the newly created VocabAnalyzer which is equipped with keyword, concordancing and n-gram functions. The VocabAnalyzer also allows the comparison of the inputted text against Jeng et al. (2002) vocabulary word list. Two case studies will be discussed in this paper. The first study compares two versions of the English Bible and the second study compares word list created by abstracts written by graduate students of various English departments in Taiwan. Both study shows that the VocabAnalyzer is beneficial for applied linguistics studies as well as for teaching and learning of English. Analyses of student writing can be carried out based on the results from this tool. Keywords: VocabAnalyzer, word list, keyword, concordancer, vocabulary 
 b  Shirley N. Dita , Rachel Edita O. Roxas , and Paul Inventado  a Department of English and Applied Linguistics, De La Salle University 2401 Taft Avenue, Manila, Philippines shirley.dita@dlsu.edu.ph b College of Computer Studies, De La Salle University 2401 Taft Avenue, Manila, Philippines {rachel.roxas, paul.inventado}@dlsu.edu.ph  Abstract. This paper aims at describing the building of the online corpora on Philippine languages as part of the online repository system called Palito. There are five components of the corpora: the top four major Philippine languages which are Tagalog, Cebuano, Ilocano and Hiligaynon and the Filipino Sign Language (FSL). The four languages are composed of 250,000-word written texts each, whereas the FSL is composed of seven thousand signs in video format. Categories of the written texts include creative writing (such as novels and stories) and religious texts (such as the Bible). Automated tools are provided for language analysis such as word count, collocates, and others. This is part of a bigger corpora building project for Philippine languages that would consider text, speech and video forms, and the corresponding development of automated tools for language analysis of these various forms. Keywords: corpora, Philippine languages, corpus-building, concordancer, sign language  
zsolan@meaningo.com Abstract. We discuss the problem of translation in the wider context of the problem of meaning in cognition and describe a structural statistical machine translation (MT) method motivated by philosophical, cognitive, and computational considerations. Our approach relies on a recently published algorithm capable of learning from a raw corpus a limited yet effective grammar that can be used to construct probabilistic parsers and language models, and on cognitively motivated heuristics for learning construction-based translation models. A pilot system has been implemented and tested successfully on simple English to Hebrew and Spanish to English translation tasks. Keywords: machine translation, grammar inference, cognitive linguistics 
 b  Alex Chengyu Fang , Wanyin Li , and Nancy Ide  a Department of Chinese, Translation, and Linguistics, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong {acfang, claireli}@cityu.edu.hk b Department of Computer Science, Vassar College, 124 Raymond Avenue, Poughkeepsie, NY 12604 ide@cs.vassar.edu  Abstract. This paper presents an early experimental work on BNC Text Categorization (TC) with Latin etymologies as features, emphasis on spoken and written texts. Two aims achieved in this study: (1) to explore discriminative new linguistic features rather than lots of noise-bringing “bag-of-words” (BoW). (2) to build up a base step to represent texts in distinct types of linguistic features with different weighting scheme rather than a plain feature vectors of BoW. The experiments disclose a notable distinct distribution pattern of Latin etymologies in spoken and written BNC texts. The performance of a home-made classifier based on the probability distribution ranges of Latin etymologies reaches a precision of 72.31% and recall of 73.22% on BNC spoken texts and precision of 73.31% and recall of 69.98% on BNC written texts. Keywords: Text Categorization, Latin Etymologies, Discriminative Features. 
Abstract. This paper proposes a method to extract constructions in a formal and mathematically rigid way using the technique of formal concept analysis (FCA). Looking at lemmas of core components of constructions as objects and semantic frames of the construction instances as attributes, in terms of the FCA, a complete lattice that represents the network structure of constructions can be obtained. We conducted the preliminary experiment of extracting the network of sub-patterns of the English ditransitive construction using a relatively small-sized corpus that was semantically tagged. The result displays the potential capability of this method, both for verifying and substantiating previous theoretical works on construction grammar and for enabling the application of such works to more practical enterprises in the ﬁeld of natural language processing. Keywords: construction grammar, formal concept analysis, symbolic thesis, linguistic ontology, ditransitive construction 
Abstract. Modeling of semantic space is a new and challenging research topic both in cognitive science and linguistics. Existing approaches can be classiﬁed into two different types according to how the calculation are done: either a word-by-word co-occurrence matrix or a word-by-context matrix (Riordan 2007). In this paper, we argue that the existing popular distributional semantic model (vector space model), does not adequately explain the age-ofacquisition data in Chinese. An alternatively measure of semantic proximity called PROX (Gaume et al, 2006) is applied instead. The application or PROX has interesting psycholinguistic implications. Unlike previous semantic space models, PROX can be trained with children’s data as well as adult data. This allows us to test the hypothesis that children’s semantic space approximates the target of acquisition: adult’s semantic space. It also allows us to compare our Chinese experiment results with French results to see to attest the universality of the approximation model. 
 a  a  Yun Jin , Qing Li , Yingshun Wu , and Young-Gil Kim  a Natural Language Processing Research Team, ETRI, 138 Gajeongno, Yuseong-gu, Daejeon, 305-764, Korea {Wkim1019, suni, kimyk}@etri.re.kr b School of Economic Information Engineering, Southwestern University of Finance and Economics, 55 Guang hua chun Road, Chengdu, 61130, China liq_t@swufe.edu.cn  Abstract. Natural language parser is usually faced ungrammatical input, such as mistyping or error POS tags. If the parser uses language dependant explicit linguistic knowledge to detect and correct grammatical errors, it is useful for parser. In this paper, we propose a method that uses the Chinese structural auxiliary knowledge to detect and correct ungrammatical Chinese parsing errors. We focus on three error types: miss segmentation, miss POS tags, and miss typing. Experimental results show that appropriate use of evident Chinese structural auxiliary knowledge indeed helps to correct parsing errors and further to improve Chinese parsing performance. Keywords: Chinese structural auxiliary, Chinese parsing, parsing error, abuse 
800 Dong Chuan Road, Shanghai 200240, China Abstract. Keyphrases extracted from articles are beneﬁcial in helping people boost browsing speed, but unfortunately keyphrases are rarely available for news articles due to the high expense of labor and time for manual annotation. This paper proposes a practical approach to extracting keyphrases for Chinese news articles using the TextRank and query log knowledge. Previous work is word based, while our approach uses phrase as its basic element. We generate phrases by employing several statistical criteria with the huge amount of queries as a training corpus. We use TextRank, a graph-based learning algorithm, for extracting keyphrases from Chinese news articles. In addition, two instructive features, lengths and positions of phrases, are incorporated into the TextRank model. Experimental results demonstrate that our methods improve the performance signiﬁcantly. Keywords: keyphrase extraction, Chinese News Articles, TextRank, query log. 
 b  Maofu Liu , Fang Fang , and Donghong Ji  a College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan 430065, P.R.China liumaofu@wust.edu.cn, fangfang0402@126.com b School of Computer, Wuhan University, Wuhan 430079, P.R.China donghong_ji2000@yahoo.com.cn  Abstract. In this paper, we propose a document re-ranking approach based on the Wikipedia articles related to the specific questions to re-order the initial retrieved documents to improve the precision of top retrieved documents in Chinese information retrieval for question answering (IR4QA) system where the questions are definition or biography type. On one hand, we compute the similarity between each document in the initial retrieved results and the related Wikipedia article. On the other hand, we do clustering analysis for the documents based on the K-Means clustering method and compute the similarity between each centroid of the clusters and the Wikipedia article. Then we integrate the two kinds of similarity with the initial ranking score as the last similarity value and re-rank the documents in descending order with this measure. Experiment results demonstrate that this approach can improve the precision of the top relevant documents effectively. Keywords: document re-ranking, Chinese IR4QA, Wikipedia, clustering analysis 
c Department of Computational Biology, University of Tokyo Kashiwa-no-ha 5-1-5, Kashiwa-shi, 277-8568 Japan d School of Computer Science, University of Manchester National Centre for Text Mining, University of Manchester Abstract. This paper presents an annotation framework wherein the management process of the annotation guidelines is integrated into the annotation process. Such an integration allows systematic management and reference of guidelines during annotation. For the evaluation of the proposed annotation system, we compare the conventional and proposed annotation frameworks, experiments using automatic guideline suggestion, and describe a unique feature of the integrated framework. Keywords: Annotation Guideline, Annotation Tool, Corpus Annotation 
Department of Linguistics, The University of Texas at Austin, Austin, TX 78712, USA jbaldrid@mail.utexas.edu Abstract. Factorial Hidden Markov Models (FHMM) support joint inference for multiple sequence prediction tasks. Here, we use them to jointly predict part-of-speech tag and supertag sequences with varying levels of supervision. We show that supervised training of FHMM models improves performance compared to standard HMMs, especially when labeled training data is scarce. Secondly, we show that an FHMM and a maximum entropy Markov model in a single step co-training setup improves the performance of both models when there is limited labeled training data. Finally, we ﬁnd that FHMMs trained from tag dictionaries rather than labeled examples also perform better than a standard HMM. Keywords: Hidden Markov Models, Bayesian Models, Categorial Grammar, Supertagging 
Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Nara, 630-0192, JAPAN Abstract. In the “Sandglass” machine translation architecture, we identify the class of monosemous Japanese functional expressions and utilize it in the task of translating Japanese functional expressions into English. We employ the semantic equivalence classes of a recently compiled large scale hierarchical lexicon of Japanese functional expressions. We then study whether functional expressions within a class can be translated into a single canonical English expression. Next, we introduce two types of ambiguities of functional expressions and identify monosemous functional expressions. In the evaluation of our translation rules for Japanese functional expressions, we directly apply those rules to monosemous functional expressions, and show that the proposed framework outperforms commercial machine translation software products. We further study how to extract rules for translating functional expressions in Japanese patent documents into English. In the result of this study, we show that translation rules manually developed based on the corpus for Japanese language grammar learners is reliable also in the patent domain. Keywords: machine translation, Japanese functional expressions, polysemy, sense disambiguation 
Keywords: Supersense, BaseNP, Named Entity, Predicate Argument Structure Analysis, Semantic Role Labeling. 
a  b  Kun Wang , Chengqing Zong , and Keh-Yih Su  a National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Room 1010, No. 95, Zhongguancun East Road, Haidian District, Beijing 100190, China {kunwang, cqzong}@nlpr.ia.ac.cn b Behavior Design Corporation, 2F, No. 5, Industry East Road IV, Science-Based Industrial Park, Hsinchu, Taiwan, ROC kysu@bdc.com.tw  Abstract. Since the traditional word-based n-gram model, a generative approach, cannot handle those out-of-vocabulary (OOV) words in the testing-set, the character-based discriminative approach has been widely adopted recently. However, this discriminative model, though is more robust to OOV words, fails to deliver satisfactory performance for those in-vocabulary (IV) words that have been observed before. Having analyzed the wordbased approach, its capability to handle the dependency between adjacent characters within a word, which is believed that the human adopts for doing segmentation, is found to account for its excellent performance for those IV words. To incorporate the intra-word characters dependency, a character-based approach with a generative model is thus proposed in this paper. The experiments conducted on the second SIGHAN Bakeoffs have shown that the proposed model not only achieves a good balance between those IV words and OOV words, but also outperforms the above-mentioned well-known approaches under the similar conditions. Keywords: Chinese Word Segmentation, Generative Model, Discriminative Model. 
 a  a  a  Xiangli Wang , Shunya Iwasawa , Yusuke Miyao , Takuya Matsuzaki ,  a  a,b,c  Kun Yu and Jun’ichi Tsujii  a Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {xiangli, iwasawa, yusuke, matuzaki, kunyu, tsujii}@is.s.u-tokyo.ac.jp b School of Computer Science, University of Manchester c National Center for Text Mining  Abstract. Data-driven parsing has been a main method for analyzing natural languages. We aim at exploring a data-driven Chinese parser, by basing it on Head-driven Phrase Structure Grammar (HPSG). Unlike for English, there is still no available Chinese HPSG framework. As the first step of our work, we design a Chinese HPSG framework, which can be used as the basis for a practical parser. In this paper, 1) we present a Chinese syntactic structure system and 2) we design a primary Chinese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 
b International Center Iwate University Ueda 3-18-8, Morioka 020-8550, Japan chidori@iwate-u.ac.jp c Japan Society for the Promotion of Science Center for the Advancement of Higher Education Tohoku University Kawauchi 41, Aoba-ku, Sendai 980-8576, Japan ajb129@hotmail.com Abstract. The Minami Hierarchy or the four-layers of embeddings within Japanese sentences has been known to give a convincing account of heterogeneous linguistic data in Japanese Grammar. However, the categorization of sentence constituents has faced serious problems. In this paper, we illustrate that the hierarchical sentential structure is on the whole tenable and attempt to represent it with Phrase Structure Grammar rules. The result is a realization of surface syntactic structure information that can serve as input to Scope Control Theory, a routine of interpretation or semantic evaluation that requires the generalizations of Minami’s hypothesis to hold true for evaluation to successfully complete. Keywords: hierarchy, Japanese, sentence structure, surface syntax, semantic evaluation 
 Keh-Jiann Chen and Shu-Ling Huang  a CKIP, Institute of Information Science, Academia Sinica kchen@iis.sinica.edu.tw b Department of Chinese Language and Literature Studies, National Hsinchu University of Education slhuang@mail.nhcue.edu.tw  Abstract. The purpose of designing the lexical semantic representation model E-HowNet is for natural language understanding. E-HowNet is a frame-based entity-relation model extended from HowNet to define lexical senses and achieving compositional semantics. The followings are major extension features of E-HowNet to achieve the goal. a) Word senses (concepts) are defined by either primitives or any well-defined concepts and conceptual relations; b) A uniform sense representation model for content words, function words and phrases; c) Semantic relations are explicitly expressed; and d) Near-canonical representations for lexical senses and phrasal senses. We demonstrate the above features and show how coarse-grained semantic composition can be carried out under the framework of E-HowNet. Possible applications of E-HowNet are also suggested. We hope that the ultimate goal of natural language understanding will be accomplished after future improvement and evolution of the current E-HowNet. Keywords: E-HowNet, compositional semantics, lexical semantic representations 
 b  Ming Zhou , Long Jiang , and Jing He  a Microsft Research Asia, Sigma Centre, Haidian, Beijing, 100109, P.R. China {mingzhou, longj}@microsoft.com b Dept. of Computer Science, Tsinghua University hejing2929@gmail.com  Abstract. We propose a novel statistical approach to automatically generate Chinese couplets and Chinese poetry. For Chinese couplets, the system takes as input the first sentence and generates as output an N-best list of second sentences using a phrase-based SMT model. A comprehensive evaluation using both human judgments and BLEU scores has been conducted and the results demonstrate that this approach is very successful. We then extended this approach to generate classic Chinese poetry using the quatrain as a case study. Given a few keywords describing a user's intention, a statistical model is used to generate the first sentence. Then a phrase-based SMT model is used to generate the other three quatrain sentences one by one. Evaluation using human judgment over individual lines as well as the quality of the generated poem as a whole demonstrates promising results. Keywords: Chinese couplets, Chinese classic poetry, automatic generation. 
Abstract. Emotion is a complicated concept, and can be represented in different ways. In this paper, we discuss two kinds of emotion representations: the enumerative representation and the compositional representation. Compared to the enumerative representation, the compositional representation is the less rigid description of an emotion. However, from the perspective of emotion classification and detection, different representations often correspond to different emotion processing task. In the enumerative representation, emotion processing can be considered as single-label classification (detecting one and only one label); in the compositional representation, the task turns into the detection of a vector. In this paper, we explore the impact of these emotion representations in emotion processing, including the trade-off of these representations and the selection of technologies to process emotion. Keywords: emotion, emotion classification, emotion processing, multi-label classification 
Keywords: Part-of-speech tagging, maximum entropy models, morphosyntactic lexicon, French, language resource development 
 c  Asif Ekbal , Md. Hasanuzzaman , and Sivaji Bandyopadhyay  a Department of Computational Linguistics, University of Heidelberg, 1m Neuenheimer Feld 325, 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de, asif.ekbal@gmail.com b West Bengal Industrial Development Corporation, Kolkata, India hasanuzzaman.im@gmail.com c Department of Computer Science and Engineering, Jadavpur University, Kolkata-700032, India sivaji_cse_ju@yahoo.com, sbandyopadhyay@cse.jdvu.ac.in  Abstract. Part of Speech (POS) tagging is the task of labeling each word in a sentence with its appropriate syntactic category called part of speech. POS tagging is a very important preprocessing task for language processing activities. In this paper, we report about our work on POS tagging for Bengali by combining different POS tagging systems using three weighted voting techniques. The individual POS taggers are based on Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM) frameworks. The POS taggers use a tag set of 27 POS tags, defined for the Indian languages. The individual system makes use of the different contextual information of the words along with the variety of word-level features that are helpful in predicting the various POS classes. The POS tagger has been trained and tested with 57,341 and 35K tokens, respectively. It has been experimentally verified that the lexicon, named entity recognizer and different word suffixes are effective in handling the unknown word problems and improve the accuracy of the POS tagger significantly. Experimental results show the effectiveness of the proposed voted POS tagger with an accuracy of 92.35%, which is an improvement of 5.29% over the least performing ME based system and 2.23% over the best performing SVM based system. Keywords: Part of Speech (POS) tagging, Statistical techniques, Voting, Bengali. 
Keywords: Adjective density, text formality, text classification, Linear Regression, Naïve Bayes. 
 b  Marie Garnier , Arnaud Rykner , and Patrick Saint-Dizier  a Université Toulouse le Mirail, 5 allées Antonio Machado, 31058 Toulouse mhl.garnier@gmail.com, arnaud.rykner@neuf.fr b IRIT, 118 route de Narbonne, 31062 Toulouse stdizier@irit.fr  Abstract. This paper presents a first step towards the automatic generation of argumentative responses to accompany the corrections proposed by a correction and writing-aid system. This system focuses on pairs of languages (e.g. French speakers writing in English), and incorporates a strong didactic orientation. We show how, in case several corrections are available, error annotations can be used to design argumentations weighing the pros and cons of each correction. Argumentation is paired with decision theory in order to help the user pick out the most appropriate correction. Argumentative responses produced manually are used to create the generation schemas required to implement the automatic generation of such texts in the future. Keywords: Error annotation, error correction, correction rules, argumentation, decision. 
 b  Azadeh Kamel Ghalibaf , Saeed Rahati , and Azam Estaji  a Department of Artificial Intelligence, Azad University of Mashhad, Ostad Yousefi - Ghasem Abad - Mashhad - Iran (0098511-6627512) Azadeh_Kamel@hotmail.com b Department of Linguistics, Ferdowsi University of Mashhad, Azadi Square- Mashhad-Iran- PostCode: 9177948974 Estaji@um.ac.ir  Abstract. Extracting semantic roles is one of the major steps in representing text meaning. It refers to finding the semantic relations between a predicate and syntactic constituents in a sentence. In this paper we present a semantic role labeling system for Persian, using memory-based learning model and standard features. We show that good semantic parsing results can be achieved with a small 1300-sentence training set. In order to extract features, we developed a shallow syntactic parser which divides the sentence into segments with certain syntactic units. The input data for both systems is drawn from Hamshahri corpus which is hand-labeled with required syntactic and semantic information. The results show an F-score of 90.3% on argument boundary detection task and an F-score of 87.4% on semantic role labeling task using Gold-standard parses. An overall system performance shows an F-score of 83.8% on complete semantic role labeling system i.e. boundary plus classification. Keywords: Semantic Role Labeling, Shallow Semantic Parsing, Shallow Syntactic Parsing, Memory-Based Learning. 
MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems Shanghai Jiao Tong University 800 Dong Chuan Road, Shanghai 200040, China bllu@sjtu.edu.cn c Microsoft Research Asia 49 Zhichun Road, Haidian District, Beijing 100080, China {zli, leizhang}@microsoft.com Abstract. We present in this paper a supervised topic model for multi-class classiﬁcation problems. To incorporate supervisory information, we jointly model documents and their labels in a graphical model called LogisticLDA, which mathematically integrates a generative model and a discriminative model in a principled way. By maximizing the posterior of document labels using logistic normal distributions, the model effectively incorporates the supervisory information to maximize inter-class distance in the topic space, while documents still enjoy the interchangeability characteristic for ease of inference. Experimental results on three benchmark datasets demonstrate that the model outperforms state-of-the-art supervised topic models. Compared with support vector machine, the model also achieves comparable performance, but meanwhile it discovers a topic space, which is valuable for dimension reduction, topic mining and document retrieval. Keywords: supervised topic model, text categorization, graph model, statistical learning 
 b  a  Rejwanul Haque , Sudip Kumar Naskar , Antal van den Bosch , and Andy Way  a CNGL, School of Computing, Dublin City University, Dublin 9, Ireland {rhaque, snaskar, away}@computing.dcu.ie b ILK Research Group, Tilburg centre for Creative Computing, Tilburg University, Tilburg, The Netherlands {Antal.vdnBosch}@uvt.nl  Abstract. The Phrase-Based Statistical Machine Translation (PB-SMT) model has recently begun to include source context modeling, under the assumption that the proper lexical choice of an ambiguous word can be determined from the context in which it appears. Various types of lexical and syntactic features such as words, parts-of-speech, and supertags have been explored as effective source context in SMT. In this paper, we show that position-independent syntactic dependency relations of the head of a source phrase can be modeled as useful source context to improve target phrase selection and thereby improve overall performance of PB-SMT. On a Dutch—English translation task, by combining dependency relations and syntactic contextual features (part-of-speech), we achieved a 1.0 BLEU (Papineni et al., 2002) point improvement (3.1% relative) over the baseline. Keywords: phrase-based SMT, syntactic dependencies, memory-based learning 
France Telecom R&D Center (Beijing), Beijing, 10080, P.R.China yuandong@orange-ftgroup.com Abstract. Alternating Structure Optimization (ASO) is a recently proposed linear Multitask Learning algorithm. Although its effective has been veriﬁed in both semi-supervised as well as supervised methods, yet they necessitate taking external resource as a prerequisite. Therefore, feasibility of employing ASO to further improve the performance merely rests on the labeled data on hand proves to be a task deserving close scrutiny. Catering to this challenging while untapped problem, this paper presents a novel application of ASO to the subtask of Shallow Semantic Parsing: Chunking. Our experiments on Chinese Treebank 5.0 present promising result in chunk analysis, and the error rate is reduced by 5.72%, proposing a profound way to further improve the performance. Keywords: Shallow Semantic Parsing, Chunking, CRFs, Multi-Task Learning, ASO. 
 c  a  Po Hu , Donghong Ji , Hai Wang , and Chong Teng  a 129 Luoyu Road, Computer School, Wuhan University, Wuhan 430079, China phu@mail.ccnu.edu.cn, donghong_ji2000@yahoo.com.cn, tchong616@126.com b 152 Luoyu Road, Department of Computer Science, Huazhong Normal University, Wuhan 430079, China c 463 Guanshan Road, School of Electronics and Information, Wuhan Polytechnic, Wuhan 430074, China whseaking@163.com  Abstract. This paper presents a novel approach to query-focused multi-document summarization. As a good biased summary is expected to keep a balance among query relevance, content salience and information diversity, the approach first makes use of both the content feature and the relationship feature to select a number of sentences via the cotraining based semi-supervised learning, which can identify the query relevant sentences beyond a single point of view. Then the ranking algorithm based on Markov chain random walks is employed on the relevant sentences by encouraging content salience and information diversity in a unified framework. The final summary focusing on the integration of relevance, salience and diversity is created after several sentences with the highest overall ranking scores are extracted. We performed experiments on DUC2007 dataset and the evaluation results show that the proposed approach can achieve significant improvement over standard baseline approaches and gain comparable performance to the state-of-the-art systems. Keywords: multi-document summarization, co-training based semi-supervised learning, graph based sentence ranking 
 b  b  Chung-chi Huang , Meng-chiech Lee , Zhe-nan Lin , and Jason S. Chang  a Institute of Information Systems and Applications, National Tsing Hua University, HsinChu, Taiwan 300, R.O.C. u901571@gmail.com b Department of Computer Science, National Tsing Hua University, HsinChu, Taiwan 300, R.O.C. {iamrabbit37, rexkimta, jason.jschang}@gmail.com  Abstract. We introduce a method for learning to assign suitable sentiment ratings to review articles. In our approach, reviews are transformed into collections of n-gram and semantic word class features aimed at maximizing the probability of classifying them into accurate ratings. The method involves automatically segmenting review articles into sentences and automatically estimating associations between features and sentiment ratings via machine learning techniques. At run-time, a simple weighting strategy is performed to give extra weights to features in potential evaluative sentences (e.g., the first, the last sentences and sentences with adverbs) from others. Experiments show that word class information alleviates data sparseness problem facing higher-level n-grams (e.g., bigrams and trigrams) and that our model using both training-time n-gram and semantic features and run-time weighting mechanism outperforms a strong baseline with surface n-gram features by 2.5% relatively. Keywords: sentiment, semantic orientations, classification, sentiment ratings, machine learning techniques. 
 Heng Ji and Dekang Lin  a Computer Science Department, Queens College and Graduate Center, City University of New York 65-30 Kissena Boulevard, Flushing, NY 11367, USA hengji@cs.qc.cuny.edu b Google, Inc. 1600 Amphitheater Parkway, Mountain View, CA 94043, USA lindek@google.com  Abstract. In this paper we present a simple approach to discover gender and animacy knowledge for person mention detection. We learn noun-gender and noun-animacy pair counts from web-scale n-grams using specific lexical patterns, and then apply confidence estimation metrics to filter noise. The selected informative pairs are then used to detect person mentions from raw texts in an unsupervised learning framework. Experiments showed that this approach can achieve high performance comparable to state-of-the-art supervised learning methods which require manually annotated corpora and gazetteers.  Keywords: Knowledge Discovery, Mention Detection, N-Grams, Gender, Animacy  
Abstract. In this paper, we discuss the summarization for supporting a user’s judgment on the credibility of information on the Web. In general, if a statement contradicts another statement, the credibility of either of the statements decreases. However, these opposing statements may coexist under certain situations, and presenting such situations is helpful for a user’s judgment. Therefore, we focus on the coexistence of these opposing statements, and attempt to develop a system to generate survey reports that contain mediatory summaries, which are deﬁned as passages extracted from Web documents in order to present situations in which these opposing statements can coexist. We describe the outline of the summarization system and describe how to improve the TextRank algorithm from the viewpoint of passage extraction for the system. From experimental results, we conﬁrmed that the methods based on the improved TextRank algorithm can extract signiﬁcant passages, which are actually considered as signiﬁcant by human assessors, with higher precision than baseline methods. Keywords: Mediatory Summary, Information Credibility, Passage Extraction, Text Summarization 
 Ji-Hye Kim and James H. Yoon  a Department of English Literature and Language, SungKyunkwan University, Myongruyndong 3-53, Chongno-gu, Seoul, South Korea jkim38ster@gmail.com b Department of Linguistics, University of Illinois at Urbana-Champaign, 4080 FLB, 707 South Mathews Ave, Urbana, IL 61801, U.S.A. jyoon@illinois.edu  Abstract. This study investigates the binding behavior of the Korean anaphor ‘caki’, which has been regarded thus far as a long-distance anaphor (LDA). However, given that even local anaphors can be bound long-distance when they function as exempt anaphors in certain languages (Pollard and Sag 1992; Kim and Yoon 2009), we investigated the binding behavior of LD-bound ‘caki’, in order to determine whether LD-bound ‘caki’ differs from LD-bound ‘caki-casin’. In the experiment, subjects were required to rate the acceptability of Korean sentences representing various types of LD binding of ‘caki’ and to determine whether the sloppy or the strict reading was more prominent in elliptical VPs containing the anaphor. The results are discussed with respect to the typology of LDAs proposed by Cole, Hermon and Huang (2001). Keywords: long-distance (LD) binding, Korean binding, anaphor binding, binding of ‘caki’, strict vs. sloppy reading, VP ellipsis, logophoricity, exempt binding 
Keywords: Bootstrapping algorithm, shared term, reliability equation, subjective evaluation, morphological analysis. 
b Department of Liberal Arts, Japan Coast Guard Academy, 5-1 Wakaba-cho, Kure, Hiroshima, 737-8512 Japan nakamura@jcga.ac.jp Abstract. This study explores the scope properties of some indeﬁnites in Japanese in terms of epsilon calculus. Different from ordinary noun phrases, quantiﬁcational noun phrases like indeﬁnites are assigned higher-order categories and/or types in syntax, and taken to denote functions from sets to truth values in semantics, which results in great difﬁculty in deriving proper interpretations of sentences with quantiﬁed expressions, given the tight syntaxsemantics relation built into theories of grammar. We simply deal with all quantiﬁed expressions as terms of type e, and treat indeﬁnites as choice functions, i.e., functions that apply to sets and arbitrarily select one of their members. Some indeﬁnites can take arbitrarily wide scopes, depending on contextual information, whereas others have limitations on the freedom of scope taking. We adopt Dynamic Syntax to implement this idea, making it possible for the scope of indeﬁnites to be left unspeciﬁed and ﬁxed in a later stage of parsing. Keywords: indeﬁnite expression, epsilon calculus, choice function, Skolemization, Dynamic Syntax 
 a,b  Lung-Hao Lee , Yu-Ting Yu , and Chu-Ren Huang  a Institute of Linguistics, Academia Sinica, 128 Academia Road, Section 2, Taipei 115, Taiwan {lunghao, tina0822, churen}@gate.sinica.edu.tw b Department of Chinese & Bilingual Studies, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong churen.huang@inet.polyu.edu.hk  Abstract. We bootstrapped Chinese WordNet with semantic domain labels of WordNet Domains for constructing a language resource called Chinese WordNet Domains. The bootstrapping methods work from three aspects: 1) Princeton WordNet alignment, 2) lexical semantic relations and 3) domain taxonomy mapping. Experimental results of our proposed bootstrapping based domain predication achieve satisfying effects. We believe the resulting Chinese WordNet Domains will be the first oriental language resource, which can be used to interoperate with the existing WordNet Domains of several languages and benefits for cross-language and domain-specific researches and applications. In addition, we also plan to release resulting Chinese WordNet Domains to the community for research purposes. Keywords: Bootstrapping, Chinese WordNet, WordNet Domains, Multi-label. 
 a,b  Sophia Yat Mei Lee , Ying Chen , and Chu-Ren Huang  a Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University, Hong Kong {sophiaym, chenying3176, churenhuang}@gmail.com b Institute of Linguistics, Academia Sinica, Taipei  Abstract. This paper presents a linguistic analysis of emotions by introducing some concrete linguistic rules in identifying the important elements of an emotion, which are the experiencer and the cause. It specifically discusses two primary emotions, happiness and surprise, in Chinese. These rules can be very useful for emotion detection and classification. We also examine the features of a cause event according to its degree of transitivity, namely agentivity, kinesis, and event participation. We find that the emotion classes (happiness vs. surprise) override the emotion verb types (change-of-state verb vs. homogeneous verb) in terms of cause event features. We believe that this study will have some implications for the linguistic account of causal events as well as the theory of emotions in general. We also hope that this work will shed lights in the automatic detection and classification of emotion in language technology. Keywords: emotion, cause event, experiencer, happiness, surprise. 
 Ying-shu Liao and Ting-gen Liao  a Department of English, National Chengchi University, No. 64, Sec. 2, ZhiNan Rd., Wensgan District, Taipei City, 11605, Taiwan (R.O.C.) 96551505@nccu.edu.tw b Department of Industrial Education and Technology, National Changhua University of Education, No.1, Jin-De Road, Changhua City, Taiwan (R.O.C.) gen0620@yahoo.com.tw  Abstract. The present study investigates how advice writers employ move sequences and modal verbs to achieve intended discourse functions. This paper aims to testify two research hypotheses: 1. Advice writers of different topics employ different moves and modal verbs to achieve discourse function, and 2. The differences may imply writers’ intentions, emotion and expectancy of effects on the readers to interpret. The corpus of five advice columns for investigation is collected from the website Creators.com. Locher’s (2006) ten moves for advice columns and Leech (2005)’s proposal of discourse function of modals are the frameworks for data analysis. The results indicate four frequent moves: advice, assessment, explanation, and general information. In addition, the columnists use different modal verbs in dealing with different issues. This study has shed light on language learning about the discourse function realized by moves and modals. Keywords: advice column, discourse function, moves, modal verbs. 
{desalle, gaume}@univ-tlse2.fr Abstract. Systematic cross-linguistic studies of verbs syntactic-semantic behaviors for typologically distant languages such as Mandarin Chinese and French are difﬁcult to conduct. Such studies are nevertheless necessary due to the crucial role that verbal constructions play in the mental lexicon. This paper addresses the problem by combining psycho-linguistics and computational methods. Psycho-linguistics provides us with a bilingual corpus that features verbal construction associated with carefully built extra-linguistic material (short video clips). Computational approaches bring us distributional semantic models (DSM) to measure the distance between linguistic elements in the extra-linguistic space. These models allows for cross-linguistic measures that we evaluate against manually annotated data. In this paper, we discuss the results, potential shortcomings involving cultural variability and how to measure such bias. Keywords: Cross-linguistic study, Distributional Semantic Models, Psycholinguistics, Extralinguistic context 
Abstract. The ability to accurately classify temporal relations between events is an important task in a large number of natural language processing and text mining applications such as question answering, summarization, and language specific information retrieval. In this paper, we propose an improved way of classifying temporal relations, using support vector machines (SVM). Along with gold-standard corpus features, the proposed method aims at exploiting useful syntactic features, which are automatically generated, to improve accuracy of the SVM classification method. Accordingly, a number of novel kernel functions are introduced and evaluated for temporal relation classification. Our evaluations clearly demonstrate that adding syntactic features results in a considerable performance improvement over the state of the art method, which merely employs gold-standard features. Keywords: Temporal Relations between Events, Classification, Information Retrieval, Text Mining 
b  a  a  a  Tzu-yi Nien , Tsun Ku , Chung-chi Huang , Mei-hua Chen , and Jason S. Chang  a Institute of Information Systems and Applications, National Tsing Hua University, HsinChu, Taiwan 300, R.O.C. {zinien, u901571, chen.meihua, jason.jschang}@gmail.com b Institute for Information Industry, 11F, No. 106, Section 2, Heping East Road, Taipei, Taiwan 106, R.O.C. cujing@iii.org.tw  Abstract. We introduce a method for learning to assign word senses to translation pairs. In our approach, this sense assignment or disambiguation problem is transformed into one on how to navigate through a sense network like WordNet aimed at distinguishing the more adequate senses from others. The method involves automatically constructing classification models for branching nodes in the network, and automatically learning to reject less probable senses, based on the translation characteristics of word senses and semanticallyrelated word groups (e.g., lexicographer files) respectively. At run-time, translation pairs are expanded with their synonyms and sense ambiguity is resolved using a greedy algorithm choosing the most likely branches based on the trained classification models. Evaluation shows that our method significantly outperforms the strong baseline of assigning most frequent sense to the translation pairs and effectively determines suitable word senses for given translation pairs, suggesting the possibility of employing our method as a computerassisted tool for speeding up the process of lexicography or of using our method to assist machine translation systems in word selection. Keywords: Word sense disambiguation, word translation classification, WordNet, machine-learning technique, maximum entropy model. 
Keywords: Generative Lexicon, Referential Module, possessive relation, Japanese genitive marker, selective binding 
Keywords: raising to object (RTO), epistemic verbs, the thetic and categorical judgments, surface-compositional, Combinatory Categorial Grammar (CCG) 
yxjia@pku.edu.cn Abstract. The automatic extraction of semantic class instance is a foundational work for many natural language processing applications. One of its crucial problems is how to validate whether a candidate instances is a true class member. Different from the common validation approaches based on the cooccurrence between instances, we present a novel approach based on concept characteristics, including category features, interference semantic classes, and collective instance. Firstly, we analyze the common error instances produced by cooccurrence-based validation from the perspective of concept, and then utilize the concept characteristics to validate the candidate instances. We conduct experiments on eight semantic classes and achieved high accuracies and recall rates, especially on open semantic classes, such as ﬁsh and singer. Keywords: semantic class, candidate validation, concept characteristics 
 Keywords: DOM lists, SEP feature, INC feature, pseudogapping schema.  
In 2004, the AMI Consortium set out to collect a multimodal meeting corpus that would give us all the raw material we needed to demonstrate a whole range of meeting support technologies, most of which we knew we hadn't thought of yet. In this keynote, I will talk about how we designed the corpus to grow an interdisciplinary community that would collectively understand not just the technologies but how groups work, and then I will describe some of the novel applications we have built using the data and are currently showing to industrial end users. Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 2–3  Developing Meeting Support Technologies: From Data to Demonstration (and Beyond) 
The tutorial presents a practical overview of automatic linguistic annotation of texts using freely available open source tools. 
This article presents a set of interactive parser-based CALL programs for North Sa´mi. The programs are based on a ﬁnite state morphological analyser and a constraint grammar parser which is used for syntactic analysis and navigating in the dialogues. The analysers provide effective and reliable handling of a wide variety of user input. In addition, relaxation of the grammatical analysis of the user input enables locating grammatical errors and reacting to the errors with appropriate feedback messages. 
While the overwhelming majority of information extraction efforts in the biomedical domain have focused on the extraction of simple binary interactions between named entity pairs, some recently published corpora provide complex, nested and typed event annotations that aim to accurately capture the diversity of biological relationships. We present the ﬁrst machine learning approach for extracting such relationships, utilizing both a graph kernel and a novel, task-speciﬁc feature set. We show that relationships can be predicted with 77% F-score, or 83% if their type and direction is disregarded. Using both gold standard and generated parses, we determine the impact of parsing on extraction performance. Finally, we convert our predicted complex relationships to binary interactions, recovering binary annotation with 62% F-score, relating the new method to the large body of work available on binary interactions. 
This paper presents how we adapted a website search engine for cross language information retrieval, using the Uplug word alignment tool for parallel corpora. We first studied the monolingual search queries posed by the visitors of the website of the Nordic council containing six different languages. In order to compare how well different types of bilingual dictionaries covered the most common queries and terms on the website we tried a collection of ordinary bilingual dictionaries, a small manually constructed trilingual dictionary and an automatically constructed trilingual dictionary, constructed from the news corpus in the website using Uplug. The precision and recall of the automatically constructed Swedish-English dictionary using Uplug were 71 and 93 percent, respectively. We found that precision and recall increase significantly in samples with high word frequency, but we could not confirm that POS-tags improve precision. The collection of ordinary dictionaries, consisting of about 200 000 words, only cover half of the top 100 search queries at the website. The automatically built trilingual dictionary combined with the small manually built trilingual dictionary consists of about 2000 words and covers 27 of the top 100 search queries. Key words: Cross language information retrieval, parallel corpora, word alignment, Swedish, Danish, Norwegian.  
 Harper, 1999; Toutanova et al., 2003). But as com-  putational power grows, and (semi)automatic anno-  State-of-the-art statistical part-of-speech tation becomes more correct over time, resulting in  taggers mainly use information on tag bi- or large almost-correct training corpora, it would be  trigrams, depending on the size of the train- interesting to see if it’s worth extending the view.  ing corpus. Some also use lexical emission  For Swedish, several statistical part-of-speech  probabilities above unigrams with beneﬁ- taggers have been trained on the Swedish  cial results. In both cases, a wider con- Stockholm-Umeå Corpus (SUC, Ejerhed et al.,  text usually gives better accuracy for a large 2006), which has become a de facto standard  training corpus, which in turn gives better for training and evaluating part-of-speech taggers.  accuracy than a smaller one. Large corpora Most of them are based on hidden Markov mod-  with validated tags, however, are scarce, so els (e.g. Carlberger and Kann, 1999; Hall, 2003;  a bootstrap technique can be used. As the Megyesi, 2002; Nivre, 2000; Sjöbergh, 2003b),  corpus grows, it is probable that a widened with bi- or trigram tag transition probabilities.  context would improve results even further.  As SUC is a balanced corpus (not just news texts)  In this paper, we looked at the contribution to accuracy of such an extended view for both tag transitions and lexical emissions, applied to both a validated Swedish source corpus and a raw bootstrap corpus. We found that the extended view was more important for tag transitions, in particular if applied to the bootstrap corpus. For lexical emission, it was also more important if applied to the bootstrap corpus than to the source corpus, although it was beneﬁcial for both. The overall best tagger had an accuracy of 98.05%.  with a fairly large tagset, it is too small to be used alone as training data for any higher-accuracy tagger, so it has also been used to bootstrap a much larger, unannotated, corpus, that can be added as training data. In previous studies, bootstrapping has proved to be a viable approach (cf. Forsbom, 2008b; Merialdo, 1994; Nivre and Grönqvist, 2001; Sjöbergh, 2003a). A recent open-source tagger, HunPos (Halácsy et al., 2007), include the range of parameters we would like to explore for extended context views for tag transition and lexical emissions. In the following, we ﬁrst describe the method, tagger and data sets used (Section 2), before de-  
Due to their linguistic and extra-linguistic nature toponyms deserve a special treatment when they are translated. The paper deals with issues related to automated translation of toponyms from English into Latvian. Translation process allows us to translate not only toponyms from a dictionary, but out-of-vocabulary toponyms as well. Translation of out-of-vocabulary toponyms is divided into three steps: source string normalization, translation, and target string normalization. Translation step implies application of translation strategies and linguistic toponym translation patterns. 10,000 UK-related toponyms from Geonames were used as a development set. The developed methods have been evaluated on a test set: the accuracy of translation is 67% for the whole test set, 58% for oneword toponymic units, and 81% for multiword toponyms. 
Annotated corpora are sets of structured text used to enable Natural Language Processing (NLP) tasks. Annotations may include tagged parts-of-speech, semantic concepts assigned to phrases, or semantic relationships between these concepts in text. Building annotated corpora is labor-intensive and presents a major obstacle to advancing machine translators, named entity recognizers (NER), part-ofspeech taggers, etc. Annotated corpora are specialized for a particular language or NLP task. Hence, a majority of the world’s 6000+ languages lack NLP resources, and therefore remain minority, or under-resourced, languages in modern language technologies. In this paper we present WebBANC, a framework for Building Annotated NLP Corpora from user annotations on the Web. With WebBANC, a casual user can annotate parts of HTML or PDF text on any website and associate the text with semantic concepts speciﬁc to an NLP task. User annotations are combined by WebBANC to produce annotated corpora potentially comparable in diversity to corpora in English, minority languages, and human generated categories, such as those on Yahoo.com, with an average precision and recall of 0.80, which is comparable to automated NER tools on the CoNLL benchmark. * Both authors contributed equally ∓ Corresponding author: samatovan@ornl.gov  
Statistical Machine Translation (SMT) has been successfully employed to support translation of ﬁlm subtitles. We explore the integration of Constraint Grammar corpus annotations into a Swedish–Danish subtitle SMT system in the framework of factored SMT. While the usefulness of the annotations is limited with large amounts of parallel data, we show that linguistic annotations can increase the gains in translation quality when monolingual data in the target language is added to an SMT system based on a small parallel corpus. 
In this paper, we present a new syntactically annotated corpus consisting of daily notes from an intensive care unit in a Finnish hospital. Using the corpus, we perform experiments with both rule-based and statistical parsers. We apply an existing rule-based parser speciﬁcally developed for this clinical language and create a set of conversion rules for transforming the constituency scheme of this parser into the dependency scheme of the corpus. The statistical parser is induced from the corpus using the MaltParser system. We ﬁnd that even with a modestly-sized corpus, the statistical parser achieves results comparable to those previously reported on a number of languages using considerably larger corpora. The accurate constituency-to-dependency conversion improves the applicability of the rule-based parser by inferring grammatical roles, thus deepening its analyses. 
The paper describes the first part of the Nordic Dialect Corpus. This is a tool that combines a number of useful features that together makes it a unique and very advanced resource for researchers of many fields of language search. The corpus is web-based and features full audio-visual representation linked to transcripts. 
Recent work has pointed out the difference between the concepts of semantic similarity and semantic relatedness. Importantly, some NLP applications depend on measures of semantic similarity, while others work better with measures of semantic relatedness. It has also been observed that methods of computing similarity measures from text corpora produce word spaces that are biased towards either semantic similarity or relatedness. Despite these ﬁndings, there has been little work that evaluates the effect of various techniques and parameter settings in the word space construction from corpora. The present paper experimentally investigates how the choice of context, corpus preprocessing and size, and dimension reduction techniques like singular value decomposition and frequency cutoffs inﬂuence the semantic properties of the resulting word spaces. 
Finnish has a very productive compounding and a rich inﬂectional system, which causes ambiguity in the morphological segmentation of compounds made with ﬁnite state transducer methods. In order to disambiguate the compound segmentations, we compare three different strategies, which are all cast in the same probabilistic framework and compared for the ﬁrst time. We present a method for implementing the probabilistic framework as part of the building process of LexC-style morpheme sub-lexicons creating weighted lexical transducers. To implement the structurally disambiguating morphological analyzer, we use the HFST-LEXC tool which is part of the open source Helsinki Finite-State Technology. Using our Finnish test corpus with 53 270 compounds, we demonstrate that it is possible to use non-compound token probabilities to disambiguate the compounding structure. Non-compound token probabilities are easy to obtain from raw data compared with obtaining the probabilities of preﬁxes of segmented and disambiguated compounds. 
Language software applications encounter new words, e.g., acronyms, technical terminology, loan words, names or compounds of such words. To add new words to a lexicon, we need to indicate their inflectional paradigm. In this article, we evaluate a lexicon-based method augmented with data from a corpus or the internet for selecting the inflectional paradigm of new words in Finnish. As an entry generator often produces numerous suggestions, it is important that the best suggestions be among the first few, otherwise it may become more efficient to create the entries by hand. By generating paradigm suggestions with an entry guesser and then further generating key word forms for the suggested paradigms, we were able to find support for the paradigms in a corpus. Our method has 79-83 % precision and 86-88 % recall, i.e. an F-score of 83-86 %, i.e. the first correctly generated entry is on the average found as the first or the second candidate. 
Previous work on part-of-speech (PoS) tagging Icelandic has shown that the morphological complexity of the language poses considerable difﬁculties for PoS taggers. In this paper, we increase the tagging accuracy of Icelandic text by using two methods. First, we present a new tagger, by integrating an HMM tagger into a linguistic rule-based tagger. Our tagger obtains state-of-the-art tagging accuracy of 92.31% using the standard test set derived from the IFD corpus, and 92.51% using a corrected version of the corpus. Second, we design an external tagset, by removing information from the internal tagset which reﬂects distinctions that are not morphologically based. Using the external tagset for evaluation, the tagging accuracy further increases to 93.63%. 
The paper presents experimental results on WSD, with focus on disambiguation of Russian nouns that refer to tangible objects and abstract notions. The body of contexts has been extracted from the Russian National Corpus (RNC). The tool used in our experiments is aimed at statistical processing and classification of noun contexts. The WSD procedure takes into account taxonomy markers of word meanings as well as lexical markers and morphological tagsets in the context. A set of experiments allows us to establish preferential conditions for WSD in Russian texts. 
This paper proposes a method to acquire linguistic features from a corpus of short sentences by extracting analogous sentences like what ’s the next station ? : where ’s the bus station ? :: what is the next stop ? : where is the bus stop ? The procedures used to construct clusters of analogous sentences are presented. Experiments performed on roughly 40,000 short sentences from the tourism domain in English and Japanese are reported, and the clusters produced are analyzed and interpreted in terms of linguistic features. 
This paper shows how large-coverage morphological and syntactic NLP lexicons can be developed by interpreting, converting to a common format and merging existing lexical resources. Applied on Spanish, this allowed us to build a morphological and syntactic lexicon, the Leﬀe. It relies on the Alexina framework, originally developed together with the French lexicon Lefff. We describe how the input resources — two morphological and two syntactic lexicons — were converted into Alexina lexicons and merged. A preliminary evaluation shows that merging diﬀerent sources of lexical information is indeed a good approach to improve the development speed, the coverage and the precision of linguistic resources. 
We investigate the effect of using wordspace models as an approximation of the kind of lexico-semantic and commonsense knowledge needed for coreference resolution of deﬁnite descriptions, that is, deﬁnite NPs with a common noun as head, for Swedish news text. We contrast a system using semantic knowledge from the word-space models with a semantically ignorant system and another system drawing its semantic information from a semantic dictionary called SynLex. We demonstrate an improvement in the results for two different evaluation tasks for the system using word space-derived semantic information over both other systems. 
∗ Most text categorization methods use the vector space model in combination with a representation of documents based on bags of words. As its name indicates, bags of words ignore possible structures in the text and only take into account isolated, unrelated words. Although this limitation is widely acknowledged, most previous attempts to extend the bag-of-words model with more advanced approaches failed to produce conclusive improvements. We propose a novel method that extends the word-level representation to automatically extracted semantic and syntactic features. We investigated three extensions: word-sense information, subject–verb–object triples, and rolesemantic predicate–argument tuples, all ﬁtting within the vector space model. We computed their contribution to the categorization results on the Reuters corpus of newswires (RCV1). We show that these three extensions, either taken individually or in combination, result in statistically signiﬁcant improvements of the microaverage F1 over a baseline using bags of words. We found that our best extended model that uses a combination of syntactic and semantic features reduces the error of the word-level baseline by up to 10 percent for the categories having more than 1,000 documents in the training corpus. ∗Research done while at Lund University.  
Text clustering could be very useful both as an intermediate step in a large natural language processing system and as a tool in its own right. The result of a clustering algorithm is dependent on the text representation that is used. Swedish has a fairly rich morphology and a large number of homographs. This possibly leads to problems in Information Retrieval in general. We investigate the impact on text clustering of adding the part-of-speech-tag to all words in the the common term-bydocument matrix. The experiments are carried out on a few different text sets. None of them give any evidence that part-of-speech tags improve results. However, to represent texts using only nouns and proper names gives a smaller representation without worsen results. In a few experiments this smaller representation gives better results. We also investigate the effect of lemmatization and the use of a stoplist, both of which improves results signiﬁcantly in some cases. 
The first version of the Danish WordNet, DanNet, was released in March 2009 under an open source license similar to the Princeton Licence (cf. www.wordnet.dk). In order to present and discuss the set of encoded semantic information in a focused form and with some empirical data, we dive into a specific ontological type in the WordNet, namely humans. We present and discuss the information types in the lexical semantic resource for this ontological type, and we focus on the information types where DanNet constitutes an extension of the general WordNet framework, namely regarding taxonomical status of a hyponym, qualia structure and connotative information. 
Recent years have seen considerable success in the generation of automatically obtained wide-coverage deep grammars for natural language processing, given reliable and large CFG-like treebanks. For research within Lexical Functional Grammar framework, these deep grammars are typically based on an extended PCFG parsing scheme from which dependencies are extracted. However, increasing success in statistical dependency parsing suggests that such deep grammar approaches to statistical parsing could be streamlined. We explore this novel approach to deep grammar parsing within the framework of LFG in this paper, for French, showing that best results (an f-score of 69.46) for the established integrated architecture may be obtained for French. 
In this article we demonstrate a novel way to resolve conﬂicts in two-level grammars by weighting the rules. The rules are transformed into probabilistic constraints, which are allowed to compete with each other. We demonstrate a method to automatically assign weights to the rules. It acts in a similar way as traditional conﬂict resolution, except that traditionally unresolvable left-arrow rule conﬂicts do not cause lexical forms to be ﬁltered out. The two-level lexicon and probabilistic twolevel grammar are combined using the new transducer operation weighted intersecting composition. The result is a weighted lexical transducer. To the best of our knowledge, this is the ﬁrst time probabilistic rules have been used to solve two-level rule conﬂicts. The possible applications of probabilistic lexical transducers range from debugging ﬂawed two-level grammars to computer-assisted language learning. We test our method using a twolevel lexicon and grammar compiled with the open source tools HFST-LEXC and HFST-TWOLC. 
A linear time extension of deterministic pushdown automata is introduced that recognizes all deterministic context-free languages, but also languages such as {anbncn | n ≥ 0} and the MIX language. It is argued that this new class of automata, called λ-acyclic read-ﬁrst deterministic stack+bag pushdown automata, has applications in natural language processing. 
A polyadic dynamic logic is introduced in which a model-theoretic version of nonlocal multicomponent tree-adjoining grammar can be formulated. It is shown to have a low polynomial time model checking procedure. This means that treebanks for nonlocal MCTAG, incl. all weaker extensions of TAG, can be efﬁciently corrected and queried. Our result is extended to HPSG treebanks (with some qualiﬁcations). The model checking procedures can also be used in heuristics-based parsing. 
In opinion mining, there has been only very little work investigating semi-supervised machine learning on document-level polarity classiﬁcation. We show that semi-supervised learning performs signiﬁcantly better than supervised learning when only few labeled data are available. Semi-supervised polarity classiﬁers rely on a predictive feature set. (Semi-)Manually built polarity lexicons are one option but they are expensive to obtain and do not necessarily work in an unknown domain. We show that extracting frequently occurring adjectives & adverbs of an unlabeled set of in-domain documents is an inexpensive alternative which works equally well throughout different domains. 
This paper presents an eﬃcient compilation algorithm that is several orders of magnitude faster than a standard method for context restriction rules. The new algorithm combines even hundreds of thousands of rules in parallel when the alphabet is large but the resulting automaton is sparse. The method opens new possibilities for representation of contextdependent lexical entries and the related processes. This is demonstrated by encoding complete HunSpell dictionaries as a single context restriction rule whose center placeholder in contexts is replaced with a new operation, called underline operation. The approach gives rise to new superposition-based contextdependent lexicon formalisms and new methods for on-demand compilation and composition of two-level morphology. 
This paper describes and evaluates the automatic annotation of clause-level complements with semantic roles in a Spanish Web corpus, using a rule- and dependency-based approach. In all, 52 different role tags, like agent (§AG), experiencer (§EXP), location (§LOC) etc. are distinguished. The annotator uses a role grammar of 568 hand-written Constraint Grammar rules that take as input the syntactic analysis of the HISPAL parser. A rough evaluation of 5000 running words was performed, where the role annotation achieved an F1 of 81,6% on raw text and 90,0% on syntactically revised input. A Spanish Internet corpus of 11.2 million words has been compiled and automatically annotated with our semantic role grammar, allowing us to provide some linguistic and statistical interpretations about the relationship between semantic roles on the one hand and syntactic functions, part of speech and semantic prototypes on the other. 
We compare the techniques of voting and stacking for system combination in datadriven dependency parsing, using a set of eight different transition-based parsers as component systems. Experimental results show that both methods lead to signiﬁcant improvements over the best component system, and that voting gives the highest overall accuracy. We also investigate different weighting schemes for voting. 
Information retrieval is a ﬁeld of research reaching from computer and information science to lingusitics. As a linguist in the information retrieval ﬁeld, I leave the quest for effective search engines and evaluation models to others, and focus on language aspects. Words, and parts of words such as compound constituents, which are successful in queries, what features do they have in common? Does the domain of search terms have impact in a domain speciﬁc environment? Can search terms with certain features help users of different categories ﬁnd documents suited for them? This paper describes the making of an information retrieval test collection which made it possible to study these questions. The test collection will be used to Evaluate search strategies to retrieve Medical documents, hence the name. To study language aspects of information retrieval a new test collection was called for, a collection which was domain speciﬁc, which regarded user groups, and which had double indexes  for split and unsplit compounds. Since there was no such collection we built MedEval, a Swedish medical test collection, with documents marked for target groups, professionals and laypersons, with a system allowing choice of user group, and with two indexes, treating compounds in different ways. In accordance with the Cranﬁeld Paradigm the MedEval test collection is based on three parts: A set of documents, a set of topics, and a set of known relevant documents with respect to each of the topics (Cleverdon, 1967). 
In data-driven Machine Translation approaches, like Example-Based Machine Translation (EBMT) (Brown, 2000) and Statistical Machine Translation (Vogel et al., 2003), the quality of the translations produced depends on the amount of training data available. While more data is always useful, a large training corpus can slow down a machine translation system. We would like to selectively sample the huge corpus to obtain a sub-corpus of most informative sentence pairs that would lead to good quality translations. Reducing the amount of training data also enables one to easily port an MT system onto small devices that have less memory and storage capacity. In this paper, we propose using Active Learning strategies to sample the most informative sentence pairs. There has not been much progress in the application of active learning theory in machine translation due to the complexity of the translation models. We use a poolbased strategy to selectively sample instances from a parallel corpora which not only outperformed a random selector but also a previously used sampling strategy (Eck et al., 2005) in an EBMT framework (Brown, 2000) by about one BLEU point (Papineni et al., 2002). 
Context-sensitive spelling correction is the task of correcting spelling errors which result in valid words. We present work in progress where we adapt established methods from English to a morphologically rich language and conclude that the rich morphology negatively affects performance. However, our system is still good enough to be useful in regular word processing. 
We introduce PolArt, a robust tool for sentiment analysis. PolArt is a pattern-based approach designed to cope with polarity composition. In order to determine the polarity of larger text units, a cascade of rewrite operations is carried out: word polarities are combined to NP, VP and sentence polarities. Moreover, PolArt is able to cope with the target-speciﬁc polarity of phrases, where two neutral words combine to a non-neutral phrase. Target detection is done with the Wikipedia category system, but also user deﬁned target hierarchies are allowed. PolArt is based on the TreeTagger chunker output, and is customised for English and German. In this paper we evaluate PolArt’s compositional capacity. 
HunPoS, a freely available open source part-of-speech tagger—a reimplementation of one of the best performing taggers, TnT—is applied to Swedish and evaluated when the tagger is trained on various sizes of training data. The tagger’s accuracy is compared to other data-driven taggers for Swedish. The results show that the tagging performance of HunPoS is as accurate as TnT and can be used efﬁciently to tag running text. 
In cases when phrase-based statistical machine translation (SMT) is applied to languages with rather free word order and rich morphology, translated texts often are not fluent due to misused inflectional forms and wrong word order between phrases or even inside the phrase. One of possible solutions how to improve translation quality is to apply factored models. The paper presents work on English-Latvian phrase-based and factored SMT systems and, using evaluation results, demonstrates that although factored models seem more appropriate for highly inflected languages, they have rather small influence on translation results, while using phrase-model with more data better translation quality could be achieved. 
This article presents experiments in the porting of semantic classiﬁcation between two closely related languages, Swedish and Danish. We show that a classiﬁer for the semantic property of animacy, trained on morphosyntactic distributional data for one language may be applied directly to data from another language with little loss in terms of accuracy. 
The point of interest in the present investigation is to find out and to make a pilot statistical presentation of the prominence distinguished by native speakers in read aloud texts taken from the Russian corpus for text-to-speech unit-selection synthesis. The TTS system uses the linguistic information encoded in the input text. Therefore the parameters which are easily extracted from the text (part of speech classes, number of syllables) are admitted as the basis for the classification of the words detected as prominent by listeners. On further steps the TTS system has to assign prosodic structure and its suprasegmental acoustic parameters. The professionally made phonetic segmentation and analysis of syntagmatic structures of the material are compared with the judgments of native speakers in order to find some of these acoustic correlates. 
Segmental and fixed-frame signal representations were compared in different noise conditions in a weakly supervised word recognition task using a non-negative matrix factorization (NMF) framework. The experiments show that fixed-frame windowing results in better recognition rates with clean signals. When noise is introduced to the system, robustness of segmental signal representations becomes useful, decreasing the overall word error rate. It is shown that a combination of fixed-frame and segmental representations yields the best recognition rates in different noise conditions. An entropy based method for dynamically adjusting the weight between representations is also introduced, leading to near-optimal weighting and therefore enhanced recognition rates in varying SNR conditions. 
The Verb Argument Browser is a linguistically relevant corpus query tool, which can be used for investigating argument structure of verbs. The original tool was developed for Hungarian corpora but the methodology is claimed to be language independent because of the dependecy grammar based representation. This paper examines this language independency applying the methodology to a language with different structure, namely: Danish. We will see that the methodology can be applied straightforwardly, and the resulting tool shows the same properties as the original version. The Verb Argument Browser for Danish is available at http://corpus.nytud.hu/vabd (username: nodalida, password: vabd). 
In our demonstration, we will present a new type of lexical resource, built from grammatically analysed corpus data. Co-occurrence strength between mother-daughter dependency pairs is used to automatically produce dictionary entries of typical complementation patterns and collocations, in the fashion of an instant monolingual Advanced Learner's dictionary. Entries are supplied to the user in a graphical interface with various thresholds for lexical frequencies as well as absolute and relative co-occurrence frequencies. DeepDict draws its data from Constraint Grammar-analysed corpora, ranging between tens and hundreds of millions of words, covering the major Germanic and Romance languages. Apart from its obvious lexicographical uses, DeepDict also targets teaching environments and translators. 
We present a simple tool that enables the computer to read subtitles of movies and TV shows aloud. The tool extracts information from subtitle ﬁles, which can be freely downloaded or extracted from a DVD, and reads the text aloud through a speech synthesizer. The target audience is people who have trouble reading subtitles while watching a movie, for example people with visual impairments and people with reading difﬁculties such as dyslexia. The application will be evaluated together with user from these groups to see if this could be an accepted solution to their needs. 
This paper describes an ongoing project where we develop and evaluate setup involving a communication board (for manual sign communication) and a drawing robot, which can communicate with each other via spoken language. The purpose is to help children with severe communication disabilities to learn language, language use and cooperation, in a playful and inspiring way. The communication board speaks and the robot is able to understand and talk back. This encourages the child to use the language and learn to cooperate to reach a common goal, which in this case is to get the robot to draw ﬁgures on a paper. 
This paper presents some principles of terminological ontologies implemented in the prototype that has been developed in the research project CAOS - Computer-Aided Ontology Structuring. Furthermore some issues, that have to be faced to further develop facilities for automatic consistency checking and automatic changes to ontologies, are discussed. The presentation will illustrate central facilities of the current version of the CAOS prototype, which is interactive and presupposes an end-user with a background in terminology rather than in formal ontology. 
We describe the development of a database containing informant judgments on a range of test sentences. The database is intended as a research resource for linguists interested in morphosyntactic variation across Scandinavian dialects. We present the data types contained in the base, and how they are used to create a user-friendly search interface. The database forms part of the efforts undertaken under the ScanDiaSyn project umbrella, currently run at ten universities in Denmark, The Faroe Islands, Finland, Iceland, Norway and Sweden. The database has been developed by the Text Laboratory at the University of Oslo, Norway. 
Case studies for developing and implementing medical portals based on Semantic Technologies and ontological approach in Knowledge Management, Information Extraction and unstructured text processing are presented in the paper. Keywords Semantic Technologies, multi-lingual information extraction, medical content gathering, drug descriptions processing, RDFstorage, semantic Wiki, knowledge based analytics. 1. Introduction Semantic Technologies and the Semantic Web (SW) as the embodiment of know-how for practical usage of these technologies are widely discussed, and it is already clear that semantic content available within knowledge portals shall lead us to a new generation of the Internet and knowledge intensive applications [1]. Medicine should be considered among the top domains for Semantic Web and intelligent applications due to high (and increasing) volumes of health-related information presented in both unstructured and machine readable form [2, 3, 4, 5]. The aim of this paper is to present one particular approach to this task – the Ontos solution for the Semantic Web in the medical domain. Two types of web applications (semantic portals) are examined, as well as the technology which underlies them. 2. Ontos Solution for Semantic Web 2.1 General Remarks One of the main goals of the Semantic Web is “semantizing” the content which already exists within the classic WWW, and of the new content created each day. Significantly, the semantic representation of processed content should be suitable for usage by program agents oriented at solving customers’ tasks. To support customers' activities within the Semantic Web we need common processing platforms with, at least, three key components:  • Knowledge Extractor based on powerful information extraction methods and tools (multilingual, effective and easily scalable). • Knowledge Warehouse based on the RDF, OWL, SPARQL standards (effective and scalable). • Set of customer oriented Semantic Services (Semantic Navigation, Semantic Digesting and Summarization, Knowledge-Based Analytics, etc.). 2.2 Ontos Solution: An Overview 2.2.1 Workflow Overview Semantic Content within the Semantic Web framework can be viewed as a new kind of “raw material”, which serves as input for Semantic Services that process it and present the results to customers [6, 7]. The Ontos Service Oriented Architecture (Ontos SOA) and an appropriate software platform were developed to support these aspects of Semantic Technologies within the Ontos solution. The general workflow within the Ontos Solution is illustrated below. Figure 1. Workflow within the Ontos Solution The crawler component gathers web-pages from a predefined list of resources, and transforms them into plain-  8 Workshop Biomedical Information Extraction 2009 - Borovets, Bulgaria, pages 8–13  text documents. These are then fed as input to the OntosMiner linguistic processors, which are discussed in sections 2.2.2 and 2.2.3. The output of these processors is a semantic graph (in RDF/XML, OWL, Turtle or N3 format) which represents named entities and relations recognized in the input text. This graph is then stored in the knowledge base, where incoming knowledge is integrated with existing data (see section 2.2.4). The data in the knowledge base is accessed by various web-oriented semantic applications, which were designed to provide end users with interesting and powerful services based on semantic metadata (see section 3). 2.2.2 Information Extraction with Systems of the OntosMiner Family Generally speaking, each IE-system of the OntosMiner family takes as input a plain text written in a natural language and returns a set of annotations, which are themselves sets of feature-value correspondences. These output annotations represent the objects and relations which the processor was able to extract from the text. The basic structure of OntosMiner linguistic processors is based on the well-known GATE architecture [8]. Each OntosMiner linguistic processor consists of a set of specialized modules called 'resources' which are organized into 'resource chains'. In this chain the resources are launched one after another and each subsequent resource has access to the output of previously launched resources. The first three basic resources are the Tokenizer, the Morphological Analyzer, and the Gazetteer. The Tokenizer determines word boundaries based on formal features of the text, the Morphological Analyzer generates a set of possible variants of morphological analysis for each word, and the Gazetteer annotates key words and key phrases which are later used for the recognition of named entities and relations. These three modules - prepare the input for the two main modules of the system - the Named Entity Extractor and the Relation Extractor. In the domain of named entity recognition we have adopted the rule-based approach to NLP which means that named entities are identified according to rules defined by developers. Thus, the Named Entity Extractor consists of a set of rules divided into subsets called 'phases' which are applied sequentially to the annotation set. Rules from each subsequent phase have access to the output of rules in previous phases. Each rule consists of a pattern on the annotation set and a sequence of commands which define the action that has to be performed when the pattern is encountered. The pattern is written in the Jape+ language, which is an extended version of the Jape language developed by the Natural Language Processing Group at the University of Sheffield [8]. The action parts of rules are mostly written in Java. The list of possible keys for named entity recognition includes the key words and phrases annotated by the  Gazetteer module, as well as annotations generated by previous phases of the Named Entity Extractor, and even specific features of annotations. For instance, the fact that a word begins with an upper case letter can play a significant role in the recognition of proper names in languages like English and French. Typically, the system of rules for the recognition of a certain type of named entity comprises several dozens of rules which 'build' the target annotations through a number of intermediate steps. One of the main difficulties with the rule based approach that we adopt is the emergence of conflicts between different rules. For instance, one set of rules within the Named Entity Extractor can identify a certain text fragment as part of a person's name, while a different set of rules identifies it as part of a company name. We discovered that when the number of rules involved grows beyond one hundred, it becomes increasingly difficult to try to control for such conflicts within the rule system itself. This is why in OntosMiner processors we allow the rules for named entity extraction to apply freely, but complement the Named Entity Extractor with a special module called Minimizer which defines the policies for conflict resolution. The idea is that different rules have a varying measure of reliability and that the developer can evaluate this measure for each rule, stating it as a feature of the annotation created by this rule. Thus, annotations generated by the Named Entity Extractor come with a feature called 'Weight' which has an integer value ranging from 0 to 100. This feature reflects the probability (as estimated by the developer) that this annotation is correct. The Minimizer resource contains a set of rules which describe different types of conflict and define which annotations should survive and which should be deleted, based on the types of annotations involved in the conflict and their weights. The resulting 'minimized' annotation set is passed on to the Relation Extractor. Semantic relations are certain facts or situations mentioned in the input text which relate one named entity to another, such as information about a person's employment in a company, or, in the medical domain, the fact that a certain condition can be the side-effect of a certain medicine. The module which is responsible for the recognition of semantic relations in OntosMiner processors is the Relation Extractor. Just like the Named Entity Extractor, the Relation Extractor contains a set of rules written in Jape+ and Java, grouped into a sequence of phases. Recognition of semantic relations differs from the recognition of named entities in that named entities are expressed by compact word groups, while the keys for semantic relations can be situated quite far apart from each other within one sentence or within the whole text. This is why in developing rules for relation recognition we exploit a different strategy: we reduce the set of annotations which is fed as input to the rules, so that it includes only key words and phrases needed to identify a particular relation, and conversely, 'stop-words' and 'stop-phrases' which  9  should never interfere between the keys. All other annotations are not included into the input and are not 'visible' to the rules. Another method that we found to be sometimes very effective in relation extraction is to first divide the input texts into meaningful fragments, and then to process each type of fragment with a separate set of rules. This technique proves useful when we are dealing with semi-structured input texts, such as drug descriptions (see below on the MedTrust portal). We can use a different set of rules for each sub-section of the description, which leads to an improvement in both precision and recall. A distinguished type of relation is the relation 'TheSame' (also called the 'identification relation') which is established between two co-referring occurrences of a named entity within a text. The resource which establishes relations of this type is called OntosCoreferencer. This resource builds a matrix of all the relevant annotations and compares them two by two to establish whether the annotations in each pair can count as two co-referring occurrences or not. The final resource in the resource chain of every OntosMiner processor is the Triples Converter. This module takes as input the set of annotations created by previous modules and generates an output in the form of an RDF/XML, OWL, Turtle or N3 document. During its work the Triples Converter accesses the OntosMiner Domains Description database (see below) and replaces all the names of annotations generated by the OntosMiner processor with the names of corresponding concepts and relations of the Domain Ontology, using the mapping rules defined in the Mapping Ontology. All the OntosMiner annotations for which mapping rules have not been defined, are excluded from the output. 2.2.3 Ontological Engineering It is well known that the ontological engineering is one of the core processes in the life cycle of semantic-oriented applications. Today there exists a number of methodologies, technologies and tools supporting this activity [9]. An overwhelming majority of them is oriented at creating and maintaining domain ontologies, and doesn't have anything in common with editing linguistic dictionaries or developing natural language processors. However, on the conceptual level, configuring a linguistic processor or a system of linguistic dictionaries may also be viewed upon as a new domain, which in its turn may be modeled by an ontology or a system of ontologies. The system of ontologies which determines the work of OntosMiner processors is called OntosMiner Domains Description (OMDD). On the physical level OMDD is the data which is uploaded to an RDF based triplestore (OMDD database). Ontological data in the OMDD is stored in a format which is completely compatible with OWL. Generally speaking, OMDD is a system of ontologies which can be divided into 6 classes: • Domain ontologies (concepts and relations which are  relevant for a certain domain). Domain ontologies are interconnected by relations of inheritance. • Internal ontologies (sets of annotation types, features and possible feature values used in specific OntosMiner processors). • Dictionary ontologies (morphological dictionaries and dictionaries of key words and phrases). • Resource ontologies (sequences of text processing resources which are used by OntosMiner processors). • Mapping ontologies (mappings which ensure that concepts from the internal ontology are correctly replaced with concepts from the domain ontology). • Other (auxiliary) ontologies. The current OMDD contains about 120 ontologies (around 2,5 million triples). 2.2.4 Ontos Semantic Knowledge Base The Ontos Semantic Knowledge Base is one of the core components within the Ontos solution. Its main function is to provide effective storage of the RDF-graph which accumulates all the information extracted from large collections of texts by OntoMiner processors. Data in the Knowledge Base can be accessed via queries in the SPARQL query language. At the moment, we have two implementation of the Knowledge Base – one based on RDMS Oracle 11g and another one based on Open Source libraries and platforms for the implementation of RDF-stores. A crucial problem in this regard is the presence of duplicate objects (i.e. objects that represent the same real world entities) within the accumulated RDF graph. The task of merging such instances is performed by algorithms of object identification which take into account the whole set of an object's identifying features, including information about its attributes and relations. 3. Intelligent Applications for the Next Generation Web The presented Ontos solution presumes two modes of access for external users: either the accumulated semantic content can be accessed via our own implemented semantic applications, or semantic content can be provided for use by third-party applications via an API. Our own solutions [10, 11] based on semantic content include packages for Nanomedicine and Pharmacology. 3.1 Semantic Portal for Nanomedicine The main goals of “semantizing” NL-content are related to integrating pieces of information, identifying implicit connections, and providing the possibility to receive an object's profile, to find out trends, etc. All these issues are particularly important for innovative fields such as Nanomedicine.  10  3.1.1 Information Sources and Domain Models In order to make it possible to carry out a full-scale analysis of different aspects of any innovative field, one should integrate information from a variety of sources of different structure and content. The relevant information sources can include (but are not limited to) the following ones: • Patent collections; • Databases with descriptions of international projects and programmes; • Conference materials and scientific papers; • Blogs and forums in the specific domain; • Regulatory documents; • Opinion letters of analytical companies; • Internet portals, news in technology, RSS feeds. It is also worth mentioning that the most interesting data can be extracted from multilingual document collections, allowing users, above all, to form a view of the situation on an international scale. The ontological system used for knowledge extraction in the Ontos Nanomedicine solution is based on a combination of ontologies corresponding to specific domains and information sources. This means that each particular ontology contains concepts and relations relevant for the domain and typical for the specific source (e.g. “Inventors” and “Assignees” for Patent analysis). The system of domain models which underlies the portal is presented below in Table 1.  Table 1. System of domain ontologies for Nanomedicine  № Ontology Description, Concepts, Relations  
The aim of gene mention normalization is to propose an appropriate canonical name, or an identiﬁer from a popular database, for a gene or a gene product mentioned in a given piece of text. The task has attracted a lot of research attention for several organisms under the assumption that both the mention boundaries and the target organism are known. Here we extend the task to also recognizing whether the gene mention is valid and to ﬁnding the organism it is from. We solve this extended task using a joint model for gene and organism name normalization which allows for instances from diﬀerent organisms to share features, thus achieving sizable performance gains with diﬀerent learning methods: Na¨ıve Bayes, Maximum Entropy, Perceptron and mira, as well as averaged versions of the last two. The evaluation results for our joint classiﬁer show F1 score of over 97%, which proves the potential of the approach. Keywords Gene normalization, gene mention tagging, organism recognition, identity resolution. 
The Quantitative Kidney DataBase (QKDB) is a relational database that was created in order to centralize kidney-related experimental results. Each result is characterized by diﬀerent attributes and the scientiﬁc paper from which it was extracted. Currently, this database is populated by hand by experts of the domain. We present a corpus study of some papers that have already been analyzed in order to exhibit the speciﬁcities and diﬃculties of the extraction process; then we propose a ﬁrst solution to extract automatically the desired knowledge from papers. Keywords Information extraction from scientiﬁc papers, database populat- ing, kidney experimental results 
The aim of this paper is to apply and develop methods based on Natural Language Processing for automatically testing the validity, reliability and coverage of various Swedish SNOMED-CT subsets, the Systematized NOmenclature of MEDicine - Clinical Terms a multiaxial, hierarchical classification system which is currently being translated from English to Swedish. Our work has been developed across two dimensions. Initially a Swedish electronic text collection of scientific medical documents has been collected and processed to a uniform format. Secondly, a term processing activity has been taken place. In the first phase of this activity, various SNOMED CT subsets have been mapped to the text collection for evaluating the validity and reliability of the translated terms. In parallel, a large number of term candidates have been extracted from the corpus in order to examine the coverage of SNOMED CT. Term candidates that are currently not included in the Swedish SNOMED CT can be either parts of compounds, parts of potential multiword terms, terms that are not yet been translated or potentially new candidates. In order to achieve these goals a number of automatic term recognition algorithms have been applied to the corpus. The results of the later process is to be reviewed by domain experts (relevant to the subsets extracted) through a relevant interface who can decide whether a new set of terms can be incorporated in the Swedish translation of SNOMED CT or not. Keywords Quality Assessment; Term Validation; Automatic Term Recognition; SNOMED CT; Scientific Medical Corpora. 1. Introduction The purpose of the current paper is to provide an introduction and description of the methodology for the validation and quality assessment of the ongoing Swedish translation of the Systematized NOmenclature of MEDicine – Clinical Terms (SNOMED CT). The translation of SNOMED CT is part of the Swedish strategy for e-health and is expected to facilitate both interoperability between health- and social care systems and communication between health- and social care professionals in clear and unambiguous concepts and terms. SNOMED CT is a very large and systematically organized computer processable collection of health and social care terminology. The main aim of our work is to develop and apply Natural Language  Processing (NLP) techniques for automatically mapping structured SNOMED CT concepts to unrestricted texts in order to evaluate the validity and reliability of the translated terms. Also, algorithms for suggesting new candidate terms are currently being explored and may benefit the translation work. The material used in this work is based on large samples of scientific medical data that cover a broad spectrum of medical subfields. Currently, the medical corpus consists of two main parts. The first part consists of the electronic editions from the latest 14 year publications of the Journal of the Swedish Medical Association, Läkartidningen, (<http://www.lakartidningen.se/>). The second part of the corpus consists of electronic editions of a Swedish diabetes journal, DiabetologNytt, (<http://diabetolognytt.se/>). The corpus is used as a testbed for exploring and measuring the coverage and quality related to the translated concept instances as well as for applying various term extraction techniques, before new services based on SNOMED CT are launched. By applying an empirical approach to the validation of the Swedish translation of various SNOMED CT subsets, we aim to explore issues related to the:  provision of concrete actions for evaluation of the quality of the translated recommended terms;  identification of potential problems or shortcomings related to the choice of recommended terms;  design of activities to overcome such potential deficiencies by e.g. suggesting sets of potential term candidates for future inclusion in the resource;  follow-up monitoring to ensure effectiveness of corrective steps;  (possibility to) measuring the quality of translations and comparing over time; as SNOMED CT and the corpus evolve.  27  Workshop Biomedical Information Extraction 2009 - Borovets, Bulgaria, pages 27–34  This paper will put emphasis on the first three of these issues. The rest of this document provides a short, general overview of SNOMED CT (Section 2) and its characteristics, the textual resources developed for the task (Section 3), as well as methodological issues related to the validation process of subsets of the Swedish translation of SNOMED CT. Moreover, a number of automatic term recognition (or term extraction/mining) techniques have been tested for the purpose of suggesting candidate terms to be included in SNOMED CT after inspection by domain experts.  2. SNOMED CT® SNOMED CT, the Systematized Nomenclature of Medicine Clinical Terms, is a common computerized language, a so called “compositional concept system” which means that concepts can be specialized by combinations with other concepts, e.g. by post-coordination which describes the representation of a clinical meaning using a combination of two or more concept identifiers; [1]. This way a single expression consisting of several concepts related by attributes, such as finding site and severity can be created; e.g. [patient] [currently] has [severe] [fracture] of [left] [shaft of femur]; [2]  During the coming years SNOMED CT will be used by all clinical and information systems in the Swedish healthcare sector in order to facilitate both interoperability between healthcare systems and communications between healthcare professionals in clear and unambiguous terms. Its primary purpose is to be used as the standard reference terminology with Electronic Health Record systems (EHR). According to AHIMA [3], SNOMED CT provides a common language that enables consistency in capturing, storing, retrieving, sharing and aggregating health data across specialties and sites of care.  Table 1. The 19 top-level SNOMED CT-hierarchies.  Body structure Clinical finding Environments geo locations Event Linkage concept Observable entity Organism Pharmaceutical biologic product Physical force  Physical object Procedure Qualifier value Record artifact Situation with explicit content Social context Special concept Specimen Staging and scales Substance  SNOMED CT is a clinically derived terminology, the content of which has been developed by clinical groups, mainly by the College of American Pathologists (CAP, <http://www.cap.org/>). SNOMED CT combines the content and structure of the SNOMED Reference Terminology/RT with the United Kingdom’s National Health Service – NHS  28  – Clinical Terms version 3. SNOMED CT covers most areas of clinical information and according to the international release of July 2008, it includes more than 315,000 active concepts, where each concept is claimed to have a semantic, logic-based definition stated in description logic1. SNOMED CT concepts are organized into 19 toplevel hierarchies (Table 1), each subdivided into several sub-hierarchies. Moreover, SNOMED CT contains over 806,000 English language descriptions (human-readable phrases or names associated with concepts) and more than 945,000 logically-defining relationships. Each concept may have more than one descriptor, and may appear in more than one hierarchy e.g. pneumonia is an infectious disease and a lung disease. SNOMED CT provides a rich set of inter-relationships between concepts. Hierarchical relationships define specific concepts as children of more general concepts. For instance, kidney disease is defined as-a-kind-of disorder of the urinary system. In this way, hierarchical relationships provide links to related information about the concept. This last example shows that kidney disease has a relationship to the concept that represents the part of the body affected by the disorder (i.e., the urinary system). 2.1 IHTSDO and SNOMED CT In April 2007 the International Health Terminology Standards Development Organization (IHTSDO, <http://www.ihtsdo.org>) acquired the intellectual property rights of SNOMED CT and its antecedents from the College of American Pathologists. IHTSDO is a non-profit association under Danish Law and it is established by a group of nine founding nations (Australia, Canada, Denmark, Lithuania, The Netherlands, New Zealand, Sweden, the United States and the United Kingdom). By acquiring the SNOMED CT, the IHTSDO and its member countries, will help to ensure the continued maintenance and evolution of SNOMED CT as well as its availability on an international scale. The IHTSDO assumed responsibility for the ongoing maintenance, development, quality assessment, and distribution of SNOMED CT. In Sweden the Swedish National Board of Health and Welfare (Socialstyrelsen, <http://www.socialstyrelsen.se>) runs the projects that in a few years time will provide a Swedish translation and a release centre with methods, routines, support and organization for national maintenance of SNOMED CT. 3. Materials and Methods 3.1 Corpus This section provides a description of the material developed and used for this work which comprises two 
Hospital Acquired Infections (HAI) has a major impact on public health and on related healthcare cost. HAI experts are fighting against this issue but they are struggling to access data. Information systems in hospitals are complex, highly heterogeneous, and generally not convenient to perform a real time surveillance. Developing a tool able to parse patient records in order to automatically detect signs of a possible issue would be a tremendous help for these experts and could allow them to react more rapidly and as a consequence to reduce the impact of such infections. Recent advances in Computational Intelligence Techniques such as Information Extraction, Risk Patterns Detection in documents and Decision Support Systems now allow to develop such systems. Keywords Natural Language Processing; Anonymization; Terminologies Mapping; Risk Pattern Detection. 1. Introduction Patients’ security is a key issue in hospitals. Specific prevention programs were developed in most of the European countries, including involvement of Infection Control Teams promoting prevention guidelines, control practices and implementing surveillance systems based on national standards. Surveillance systems of adverse events are key elements for prevention as it has been demonstrated by various studies ([1], [2], [3] and [4]). An efficient surveillance system should meet several criteria: it should encompass clear definition of targeted infections, be able to detect and react in a very timely effective manner, be sensitive enough to detect small variations in the occurrence rate and should not require too much effort and time investment from the medical staff which is already overworked. Such a system should also be able to take into account a collection of data such as patient’s  risk factors (morbidity, invasive devices, surgical procedure…). These data have to be gathered from patient records to be recorded on specific standardized forms for further analysis. However the organization of hospital information systems does not help collecting this information. Expertise gained over the last years in Computational Intelligence and more specifically in Risk Patterns detection from the literature allows now to address this problem. The detection of specific combinations of events and underlining relations between symptoms, treatments, drugs, reactions, and biological parameters can allow automatic systems to identify potential adverse events. Alerts could then be sent to risk management teams to help them identifying events that require immediate action and correction measures. The following paper describes a project aiming to detect HAI by using risk patterns identification methods in patient records. The goal is to apply appropriate state of the art technologies included in a global process involving synergies between medical and technical experts to reduce the number of unnoticed cases and time for reaction. To do so Natural Language Processing (NLP) techniques will be applied to identify specific terms and sequences of facts in Patient Discharge Summaries. 2. Hospital Acquired Infections 2.1 Current Status A Hospital Acquired Infection can be defined as: An infection occurring in a patient in a hospital or other health care facility in whom the infection was not present or incubating at the time of admission. This includes infections acquired in the hospital but appearing after discharge, and also occupational infections among staff of the facility. If the exact status of the patient is not clearly known when he first came in a medical unit, a  35 Workshop Biomedical Information Extraction 2009 - Borovets, Bulgaria, pages 35–41  period of 48 hours (or superior to the incubation period if it is known) is considered to separate HAI from other kinds of infections coming from outside. As for infections related to surgery a period of 30 days is considered and extended to 12 months in case of implanted device [5]. 2.2 A Document workflow issue HAI in hospitals is identified as a major issue and many multidimensional efforts have been undertaken to provide solutions to this problem. These solutions cover staffing, organizational and methodological dimensions. Best practice guidelines have been designed and specialists assigned to provide guidance to the medical staff in case of infection surge. However many problems remain. They are related both to the difficulty to isolate HAI signs from other normal symptoms associated with what brings a patient to the hospital, and to the way information workflows are organized and accessible. First of all detecting symptoms related to an HAI is inherently a difficult task as patients coming to a hospital are already sick. Furthermore some time they suffer from several diseases or infections at the same time that generate various different symptoms. Time frame is also an issue because symptoms related to an HAI may take several days to appear. During that period a patient may have moved to different medical units and even may be back home. Information is therefore diluted in various documents covering several days or weeks. The great heterogeneity of information systems adds to the task complexity. Each hospital can use its own tools to process and store data. Therefore it is extremely difficult for HAI experts to track down elements that could lead them to detect a problem, not to say to access documents in real time. This is why most of the time they react only when the issue is obvious and require urgent damage control actions. 2.3 Solutions to overcome this problem Several directions to develop an efficient surveillance system are currently explored [6]. Among them we can identify several main categories: Passive systems that take into account what is declared by the medical staff or by patient themselves Systems based on a retrospective analysis made by HAI experts from patient records Predictive systems based on pre-identified risk factors Automated systems performing a systematic analysis on patient records Most of them are decision support systems where rules have been designed thanks to human expertise or statistical data. The approach here is to compute the risk 36  for a given patient to get a specific HAI according to various parameters such as age, gender, pathologies, medical unit where he is treated etc. But in this case it is only a prediction system ([7], [8], [9]). Other techniques use microbiological analysis results to perform predictions. But here once again we are dealing with a statistical system. However very few attempts have been made in the domain of text-mining to identify HAI risk factors ([10]). Melton et al. have for instance used the MedLEE semantic extraction tool to detect potential problem from patient records. The recall of this system has been evaluated to 28% and the precision to 98%. In this case the priority was to detect only very serious events which stressed the importance of precision. The same tool has also been applied to radiology reports to detect pulmonary infections [11]. In this case the recall was 71% and precision 95%. This evaluation stresses the important of tools customized for very specific targets in order to improve their efficiency. But more generally applying Natural Language processing technologies to detect from medical reports risk fact for HAI is a very promising trends where lot of work remains to be done. 3. Text Mining for Risk Patterns Detection 3.1 The ALADIN project This project is developed in close collaboration between HAI surveillance experts and Linguistic and Knowledge Management experts in order to both characterize HAI risk factors and to design the necessary set of rules to identify such risk factors from patient records. On a first hand only some specific medical units will be targeted, those where most deadly infections occur (Intensive Care Unit and Surgery). 
Clinical named entities convey great deal of knowledge in clinical notes. This paper investigates named entity recognition from clinical notes using machine learning approaches. We present a cascading system that uses a Conditional Random Fields model, a Support Vector Machine and a Maximum Entropy to reclassify the identiﬁed entities in order to reduce misclassiﬁcation. Voting strategy was employed to determine the class of the recognised entities between the three classiﬁers. The experiments were conducted on a corpus of 311 manually annotated admission summaries form an Intensive Care Unit. The recognition of 10 types of clinical named entities using 10 fold cross-validation achieved an overall results of 83.3 F-score. The reclassiﬁer eﬀectively increased the performance over stand-alone CRF models by 3.35 F-score. Keywords Named Entity Recognition, Clinical Information Extraction, Machine Learning, Classiﬁer Ensemble, Two Phase Model 
For an effective search and management of large amounts of medical image and patient data, it is relevant to know the kind of information the clinicians and radiologists seek for. This information is typically represented in their queries when searching for text and medical images about patients. Statistical clinical query pattern derivation described in this paper is an approach to obtain this information semiautomatically. It is based on predicting clinical query patterns given medical ontologies, domain corpora and statistical analysis. The patterns identified in this way are then compared to a corpus of clinical questions to identify possible overlaps between them and the actual questions. Additionally, they are discussed with the clinical experts. We describe our ontology driven clinical query pattern derivation approach, the comparison results with the clinical questions corpus and the evaluation by the radiology experts. Keywords Medical ontology, information extraction, biomedical corpora, information management, medical imaging. 1. Introduction Due to advanced technologies in clinical care, increasingly large amounts of medical imaging and the related textual patient data becomes available. To be able to use this data effectively, it is relevant to know the kind of information the clinicians and radiologists seek for. This information is typically represented in the search queries that demonstrate the information needs of radiologists and clinicians. Our context is the MEDICO use case, which has a focus on semantic, cross-modal image search and information retrieval in the medical domain. Our objective is to identify the kind of queries the clinicians and radiologists use to search for medical images and related textual data. As interviews with clinicians and radiologists are not always possible, alternative solutions become necessary to obtain this information. We aim to discover radiologists’ and clinicians’ information needs by using semi-automatic text analysis methods that are independent of expert interviews. One MEDICO1 scenario concentrates on image search targeting patients that suffer from lymphoma in the neck 
This paper outlines a formal description of grammatical relations between definitions and verbal predications found in Definitional Contexts in Spanish. It can be situated within the framework of Predication Theory, a model derived from Government & Binding Grammar. We use this model to describe: (i) the syntactic patterns that establish the relationship between definitions and predications; (ii) how useful these patterns are for the identification of definitions in technical corpora. Keywords Definition Extraction, Types of Definitions, Predication, Predicative Phrase. 1. Introduction The (semi-)automatic recognition of terms and definitions in a corpus is an important task to research areas such as computational lexicography, terminology, language engineering and others. In the case of term recognition, several works report successful methodologies, computational tools and experiments that aim to identify and extract, in a no-supervised way, term candidates from large specialized corpora (e. g. Cabré, Estopà & Vivaldi 2001). However, the automatic recognition of definitions presents a much higher degree of complexity, since definitions are linguistic structures used to formulate concepts (Sager, 1990). In contrast to terms, which are considered language units whose function is to refer specific entities in a scientific or technical knowledge domain, definitions condense information and establish several conceptual relations, with the purpose to delimitate the essential properties or attributes that characterize an entity in relation to others. There are currently many authors that have proposed different methodologies for identifying candidates to  Gerardo Sierra Instituto de Ingeniería Universidad Nacional Autónoma de México Cubículo 3, Basamento, Torre de Ingeniería C.U., C.P. 04510, México D.F. GsierraM@iingen.unam.mx definitions, considering both linguistic and statistical points of views. Some relevant methodologies are:  Definitional Sentences (fr. énonces définitoires): Auger (1997), Rebeyrolle (2000).  Terms in Contexts: Pearson (1998).  Knowledge-Rich Contexts: Meyer (2001).  Mining Definitions on Texts: Malaisé, Zweigenbaum & Bachimont (2005). In accordance with these methodologies, in this paper we present a methodology to identify different types of definitions in technical corpora, considering that these definitions are configured as grammatical patterns, in particular, as phrase structures. These patterns are linked to verbal predications with syntactic regularities. For the syntactic analysis of these patterns, we use a formal model called Predication Theory (henceforth, PredT). This model is formulated within the framework of Government & Binding Grammar (Rothstein, 1983; Bowers 1993, 2001. So, the PredT allows us to describe, in a formal way, the grammatical relations that definitions establish with verbal predications. Taking this relationship into account, it is possible to identify good candidates to definitions considering their association with verbal predications, specifically when these definitions are introduced in scientific and technical texts. 2. Definitional Contexts We situate this analysis within the framework of Definitional Contexts (or DCs) extraction. According to Sierra et al. (2008), a DC is a discursive structure that contains relevant information to define a term. A DC has at least two constituents: a term and a definition, and usually linguistic or metalinguistic forms, such as verbal phrases, typographical markers and/or pragmatic patterns. An example is: 1. In general, the paraprofessional workers are defined as those persons who are engaged in the provision of social care or social services, but who do not have professional training or qualifications.  
In this paper we present a description and evaluation of a pattern-based approach for definition extraction in Spanish specialised texts. The system is based on the search for definitional verbal patterns related to four different kinds of definitions: analytical, extensional, functional and synonimical. This system could be helpful in the development of ontologies, databases of lexical knowledge, glossaries or specialised dictionaries. Keywords Definition extraction, definitional contexts, definitional verbal patterns, pattern-based approach. 1. Introduction There is a growing interest in the development of systems for the automatic extraction of information that describe the meaning of terms. This information occurs in structures commonly called definitional contexts (DCs), which are structured by a series of lexical and metalinguistic patterns that can be automatically recognised. In this context, in this paper we present a work focused on developing a system for the automatic extraction of definitional contexts on Spanish language specialised texts. This system looks for instances of definitional verbal patterns, filters non-relevant contexts, identifies the main constituent elements on the candidates, i.e., terms and definitions, and performs an automatic ranking of the results. Firstly, we will describe the structure of DCs; secondly, we provide a short review of related works; we then present the methodology followed for the automatic extraction of DCs together with an evaluation of this methodology; and lastly, we propose some future work. 2. Definitional Contexts in Specialised Texts A definitional context is a textual fragment from a specialised text where a definition of a term is given. Its basic structure consists of a term (T) and its definition (D), both elements being connected by typographic or syntactic patterns. Typographic patterns are punctuation marks  (comas, parenthesis), while syntactic patterns include definitional verbs –such as definir (to define) or significar (to signify)– as well as discursive markers –such as es decir (that is, lit. (it) is to say), or o sea (that is, lit. or besubjunctive)–. Apart from these, DCs can include pragmatic patterns (PPR), which provide conditions for the use of the term or clarify its meaning, as in en términos generales (in general terms) or en este sentido (in this sense). For example: “Desde un punto de vista práctico, los opioides se definen como compuestos de acción directa, cuyos efectos se ven antagonizados estereoespecíficamente por la naloxona.” In this case, the term opioides is connected to its definition (compuestos de acción directa […]) by the verbal pattern se definen como (are defined as), while the general sense of the context is modified by the pragmatic pattern desde un punto de vista práctico (from a practical point of view). 3. Advances in Definitional Contexts Extraction Definition extraction from specialised texts has become a relevant task in the field of information extraction. In order to extract definitional information, the most common strategy is to extract certain recurrent patterns, which are commonly found in DCs. The use of this kind of patterns has been applied on different scenarios. One of the first descriptive works can be found in [1], in which the behaviour of the contexts where terms occur is described. This work states that, when authors define a term, they usually employ typographic patterns to visually highlight the presence of terms and/or definitions, as well as lexical and metalinguistic patterns connecting DCs elements by means of syntactic structures. [2] reinforces this idea was reinforced and also explained the fact that definitional patterns can provide keys for the identification of the type of definition occurring in DCs, which facilitates the task of ontology development.  7 Workshop On Deﬁnition Extraction 2009 - Borovets, Bulgaria, pages 7–13,  Regarding applied works, [3] reports a system called Definder for the automatic extraction of definitions from medical texts in English. In the same line of research, other works have been focused on DCs extraction from specialised texts in other languages, for example German [4], Portuguese [5] or Spanish [6]. Definition extraction has also been used as a previous step for the automatic extraction of semantic relations or the automatic development of ontologies [7], [8], as well as for obtaining knowledge for the development of eLearning technologies [9]. Furthermore, the automatic extraction of definitions has been focused on direct Web exploitation. That is the case of the work reported in [10] whose main goal is the extraction of definitions from on on-line sources for question answering systems. [11] reports an application called GlossExtractor, that works on the Web, mainly online glossaries and Web specialised documents, also for the automatic extraction of definitions, but starting from a list of predefined terms. [12] developed a system called DefExplorer for definition extraction of Web documents for the Chinese Language. All of these systems start from the search of specific definitional patterns in each language and they also integrate procedures for filtering non-relevant contexts, i.e., contexts that contain a definitional pattern that does not yield an actual definitional context. Finally, all of these methodologies are based on the exploitation of specialised documents, being the direct Web exploitation a recently incorporated process. 4. ECODE As we have mentioned before, the main purpose of a definitional context extractor is to simplify the search of relevant information about terms, by means of searching for occurrences of definitional patterns. An extractor that only retrieves those occurrences of definitional patterns would be a useful system for terminographical work. However, the manual analysis of the retrieved occurrences would still imply an effort that could be simplified by an extractor that includes the automatic processing of the obtained information. Therefore, we propose a methodology that includes not only the extraction of occurrences of definitional patterns, but also a filtering process of non-relevant contexts (i.e. non definitional contexts), the automatic identification of the possible constitutive elements of a DC: terms and definitions, and a final automatic ranking of the results. This system is called ECODE: extractor de contextos definitorios (definitional contexts extractor). A general overview of the system is shown in figure 1. It can be seen that the system input consists of a corpus tagged with POS categories, since some of them are necessary in the different processes of the system. It can  also be seen that the main three processes are: a) the extraction of DC candidates, b) the analysis of DC candidates, and c) the evaluation of DC candidates. The extraction of DC candidates is a process that uses a grammar of verbal patterns with some specific parameters: the definitional verbs to search for and the nexus that can also be part of the pattern, i.e., the adverb como (as) in the pattern se define como (it is defined as). In this case, the grammar shall also include constraints on the verbal times and grammatical person in which each verb can occur, as well as the different positions for each verb where the term can occur in a DC. Once the DC candidates are extracted, they are analysed in the next process, which is carried out in two steps: the filtering of non-relevant candidates, and the identification of their constituent elements. The filtering process makes use of a set of linguistic and contextual rules to determine those cases where no DCs are found, while the identification of their constituent elements makes use of a decision tree, which also analyses the grammar of verbal patterns in order to identify the term and its definition on each DC candidate. Finally, the system performs an automatic ranking of the candidates proposed as DCs. This process use a set of heuristic rules and aims to identify those candidates that follow a prototypical structure of terms and definitions. Fig. 1. System architecture.  8  4.1 Candidates extraction The ECODE was developed taking the IULA’s Technical Corpus from the Institut Universitari de Lingüística Aplicada (UPF) as starting point. This corpus consists of specialised documents in the fields of Law, Genome, Economy, Environment, Medicine, Informatics and General Language. First, we manually developed a grammar of verbal patterns for Spanish. We identified 29 verbs related to four different types of definitions: analytical, extensional, functional and synonimical. The whole set of verbal patterns is shown in table 1. Table 1. Definitional Verbal patterns Analytical verbal patterns ser + artículo (to be + article) consistir en (to consist in) caracterizar como/por (to characterize as/for) concebir como (to conceibe as) considerar como (to consider as) describir como (to describe as) comprender como (to understand as) definir como (to define as) entender como (to understand as) conocer como (to known as) denominar como/∅ (to denominate as/∅) llamar como/∅ (to call as/∅) nombrar como/∅ (to name as/∅) Extensional verbal patterns comprender (to comprehend) contener (to contain) incluir (to include) integrar (to integrate) constar de (to comprise of) contar de/con (to count of/with) consistir de/en (to consist of/in) formar de/por (to form of/by) componer de/por (to compose of/by) constituir de/por (to constitute of/by) Functional verbal patterns permitir (to allow) encargar de (to undertake of) funcionar como/para (to function as/for) ocupar como/para (to occupy as/for) servir como/en/para (to serve as/in/for) usar como/en/para (to use as/in/for) emplear como/en/para (to employ as/in/for) utilizar como/en/para (to utilise as/in/for) Synonimical verbal patterns conocer también (to known also) denominar (to denominate also) llamar (to call also) nombrar (to name also)  From the table above, we can see different verbs associated to different types of definitions. In some cases, the verbs can occur together with different grammatical particles and can be associated with more than one type of definition, such as the verb denominar (to denominate), which can occur in analytical or synonimical DCs with the nexus como (as) or también (also), respectively. The verbal patterns were searched for taking into account the next constraints: Verbal forms: infinitive, participle and conjugate forms. Verbal tenses: present and past for verbs without nexus, any verbal tense for verbs with nexus. Person: 3rd person singular and plural for verbs without nexus, any for verbs with nexus. Distance: each nexus was searched for within a distance of 15 possible words. With these restrictions, the system obtains a set of DC candidates that are next annotated with contextual tags. These simple tags function as borders in the next automatic processes. For each occurrence, the definitional verbal pattern was annotated with “<dvp></dvp>”; everything after the pattern with “<left></left>”; everything before the pattern with “<right></right>”; and finally, in those cases where the verbal pattern includes a nexus, like the adverb como (as), everything between the verbal pattern and the nexus was annotated with <nexus></nexus>. Here is an example of a DC annotated with contextual tags: <left>El metabolismo</left> <dvp>puede definirse </dvp> <nexus>en términos generales como</nexus> <right>la suma de todos los procesos químicos (y físicos) implicados.</right> 4.2 Candidates analysis Once the DCs were extracted and annotated with definitional verbal patterns they were analysed with the purpose of filtering non-relevant contexts. We applied this step based on the fact that definitional patterns are used not only in definitional sentences but also in a wider range of sentences. In the case of verbal patterns, some verbs tend to have a higher metalinguistic meaning than others. That is the case of definir (to define) or denominar (to denominate), vs. concebir (to conceive) or identificar (to identify), where the last two are used in different contexts. Moreover, verbs having a high metalinguistic meaning are not used only for defining terms. To develop this process, a manual analysis was carried out to determine the type of grammatical particles or syntactic sequences occurring in those cases where a DVP was not used to define a term. These particles and syntactic sequences were found in some specific positions, for example: negation particles such as no (not) or tampoco (either) were found in the first  9  position before or after the DVP; adverbs like tan (so), poco (few) as well as sequences like poco más (not more than) were found between the definitional verb and the nexus como; also, syntactic sequences such as adjective + verb were found in the first position after the definitional verb. Thus, taking this and other frequently combinations into consideration as well as the contextual tags previously annotated, the systems filters contexts as shown in the following examples: Rule: NO <left> <left>En segundo lugar, tras el tratamiento eficaz de los cambios patológicos en un órgano pueden surgir problemas inesperados en tejidos que previamente no </left> <dvp>se identificaron</dvp> <nexus> como </nexus> <right> implicados clínicamente, ya que los pacientes no sobreviven lo suficiente.</right> Rule: <nexus> CONJUGATED VERB <left>Ciertamente esta observación tiene una mayor fuerza cuando el número de categorías </left> <dvp> definidas</dvp> <nexus> es pequeño como</nexus> <der>en nuestro análisis.</der>. Once the non-relevant contexts were filtered, the next process was the identification of terms and definitions in the DC candidates. Depending on each DVP, the terms and definitions may appear in some specific positions in Spanish DCs. For example, in DCs containing the verb definir (to define), the term may occur in left, nexus or right position (T se define como D; se define T como D; se define como T D), while in DCs containing the verb significar (to signify), terms may appear only in left position (T significa D). Therefore, in this phase the automatic process is highly related to deciding the positions in which the constituent elements could appear. We decided to use a decision tree to solve this problem, i.e., to detect by means of logic inferences the probable positions of terms, definitions and pragmatic patterns. We established some simple regular expressions to represent each constituent element1: T = BRD (Det) + N + Adj. {0,2} .* BRD PPR = BRD (sign) (Prep | Adv) .* (sign) BRD D = BRD (Det) + N As in the filtering process, the contextual tags function as borders to demarcate decision tree’s instructions. In addition, each regular expression could function as a border. At the first level, the branches of the tree correspond to the different positions in which constituent elements may occur (left, nexus or right). At the second 
Enriching linguistic resources with domain information has been considered one important target in natural language applications. However, automatic definition extraction of this domain information from specialized resources has revealed certain methodological problems in definition construction. This paper presents some problems encountered in automatic definition extraction that are mainly related to inconsistencies in definitions, different granularity of definitions and embedded definitions. To face these problems some Meaning-Text Theory tools have been used: (a) semantic labels as a solution for inferring knowledge, (b) lexical functions as a way of providing coherence to definitions and (c) the actancial structure as a tool for developing consistent and complete definitions. Our goal is to describe the problems and to show the solutions proposed. Keywords Definition extraction, ontology building, linguistic resource enrichment, Meaning Text Theory. 1. Introduction Reusing and enriching existing resources are nowadays two key issues both in academy and in the business world. In several scientific disciplines such as ontology development, computational linguistics, web semantic, ontologies and computational terminology the interest has been focused on many different aspects ranging from reusing lexicons, thesauri to create ontologies to extracting semantic relations from domain corpora or enriching definitions from specialized texts. One of the current drifts tends to build ontologies extracting definitions from different sources. However, building new resources with linguistic information extracted from different domain sources has revealed a difficult task as quite often the domain sources can be useful for a certain task but may show certain inconsistencies for others. In this paper, we present the  problems encountered when trying to reuse three domain resources for two different purposes: (a) to build an ontology and (b) to populate a general linguistic resource, a database, with specific information from domain documents. With the aim of developing a consistent linguistic resource for further use in natural language applications, we focus on achieving consistent definitions of domain terms. Accordingly, we resort to the MeaningText Theory (MTT) principles [16] to propose some systematic solutions in order to avoid the inconsistency problems when building a terminological resource that can later be used in ontology development. Thus, we have mainly focused on three fundamental aspects: (a) semantic labels as a solution for inferring knowledge, (b) lexical functions as a way of providing coherence to definitions and (c) the actancial structure as a tool for developing consistent and complete definitions. The rest of the paper is organized as follows: In section 2 we provide the scenario in which we have based our research and the tools used. Section 3 focuses on definition extraction and the pitfalls faced in the process. Section 4 presents a short review on definition typology. The MTT tools used and the database, BADELE 3000, are described in section 5. The problems encountered and the solutions proposed are presented in section 6. Finally, some conclusions are outlined in section 7. 2. Background The domain resources used in this project summarized in this section (for more details, see Gómez-Pérez et al [7]) relate to geographic and geospatial information. All geographic information (GI) resources contain data about real entities and how to represent them in a map. So, each entity corresponds to an instance of a geographic phenomenon (feature). Indeed, the most important concept for GI is the feature since the Open GeoSpatial Consortium (OGC) [19] has declared that a geographic feature is the starting point for modelling geospatial information. In other words, a feature, which is the basic unit of GI, is an abstraction of a real world phenomenon associated with a location relative to the Earth, about which data are collected, maintained and disseminated [11]. Features can  14 Workshop On Deﬁnition Extraction 2009 - Borovets, Bulgaria, pages 14–20,  include representations of a wide range of phenomena that can be located in time and space such as buildings, towns and villages or a geometric network, a geo-referenced image, pixel or thematic layer. For modelling this domain we have decided to use an ontology. To achieve this target, we have used three domain resources provided by the National Geographic Institute of Spain (IGN-E): the Concise Gazetteer (NC) scale 1:1,000,000-, the Numerical Cartographic Database (BCN25) -scale 1:25,000-, and the National Topographic Database (BTN25) -scale 1:25,000-. The Concise Gazetteer is a basic corpus of standardized toponyms created by the Spanish Geographical Names Commission. The first version has 3667 toponyms. This gazetteer complies with the United Nations Conference Recommendations on Geographic Names Normalization. The Concise Gazetteer has been created by the Spanish Geographical Names Commission. For further details, refer to Nomenclátor Geográfico Conciso de España [18]. The BCN25 presents an abstraction of reality, represented in one or more sets of geographic data, as a defined classification of phenomena. It defines the feature type, its operations, attributes, and associations represented in geographic data. For more information on this document see Rodriguez [21]. The BTN25 is the latest IGN-E catalogue and intends to be a sort of BCN25 reorganization, following a structure similar to frames. The instance information is the same as in BCN25, but the phenomena classification and its attributes are completely different. These resources have one characteristic in common: each resource has a domain dictionary with phenomena. In the first case, NC phenomena, there is a txt file with 22 definitions. In the second case, BCN25 phenomena, an Excel file contains 366 definitions developed after the catalogue. Finally, there is a PDF document with “Capture rules for GI to be included in BTN25” (a first version), which describes its phenomena with 292 definitions (the document is not complete). In all cases, definitions were formulated by specialists on geography to facilitate the classification of the real entities in order to be included in the instance set of each resource. All definitions are grouped by labels, as illustrated in Table 1 with four examples. These definitions have been used to build the ontology, as explained in section 3.  Granja (farm) Piscifactoría (fish farm) Palomar (pigeon loft)  Hacienda de campo que consta de establos, huerta y casa habitable (Ranch with stables, an orchard and a house) Instalación en la que se crían diversas especies de peces y mariscos con fines comerciales (Installation where fish or seafood are bred for commercial purposes) Edificio donde se recogen y crían palomas (Building where pigeons live and are bred)  3. Definition extraction Definition extraction, as used in this paper, is the process of extracting the definition for a term from different resources. In our case these definitions have not been taken from corpora using machine learning techniques, as in many natural language processing applications [3], but from other domain resources with explicit definitions for these terms, their term variants or other semantically equivalent terms. However, some problems have appeared in this definition extraction process that showed certain inconsistencies and loss of information. The definition extraction process followed to build and enrich a domain ontology is as follows: (1) the application we have developed retrieves the term from “Capture rules for GI to be included in BTN25”; (2) it extracts its definition from the same document; (3) it searches for the term in the auxiliary domain dictionaries; and (4) it extracts the corresponding definitions to add them to the corresponding classes. All these actions are executed automatically. Fig. 1 shows the overall workflow of information.  Table 1. INDUSTRIAL INSTALLATION (source document)  Nouns  Definitions  Corral (corral)  Construcción creada para cobijarse los pastores o para recoger el ganado (Construction created for shepherds or cattle shelter)  Fig. 1. Ontology building with definition extraction As a result of this process, we obtained an ontology (called PhenomenOntology 3.5) which included 108 terms extracted from the documents mentioned and later transformed in 108 classes belonging to three groups: (a) classes without definitions; (b) classes with one definition; (c) classes with more than one definition. However, the  15  retrieval ratio of definitions extracted from the auxiliary dictionaries was very low, although they belonged to the same domain. In fact, only 4 definitions were found in the NC dictionary (although it contains 22 definitions, which means that 18 definitions were lost in the process) and 33 definitions were found in the BCN25 dictionary (it contains 366 definitions, which means that 333 definitions were also lost in the definition retrieval process). The origin of this low ratio mainly lies on the abundance of terminological variants and semantically equivalent terms. For example, when trying to retrieve definitions for ‘río’ (river) in the ontology, the system cannot recognize definitions of term variants such as ‘río 1ª categoría’ (river 1st category) and ‘río 2ª categoría’ (river 2nd category), and consequently it does not retrieve any of these definitions. Moreover, semantically equivalent terms are not retrieved when incorporating definitions in the ontology, as the system cannot recognize the similarity of the definitions of ‘río’ (river) and ‘corriente fluvial’ (flowing current). Therefore, the problem is not only the loss of certain definitions in the extraction process but also the overlapping of some of them with different granularity which led to inconsistencies. For example, ‘río’ (river) was retrieved with two definitions: recorrido de una corriente de agua natural y de caudal más o menos constante, que recoge los aportes de una cuenca fluvial (taken from the original document BTN25: “stream of natural water, with more or less constant flow, which collects water from other water courses”) and curso natural de agua (taken from the NC dictionary: “waterstream”). Although these terminographic resources have been originally compiled by different experts, they show many lexico-semantic divergences that hinder the automatic definition extraction process. Quite often specific domain lexicographic resources are generally built to share information within a project team and attention is not usually paid to terminological principles when defining new terminology. In other words, when building ontologies, automatic extraction of classes implies the annotation of these classes with definitions which are also automatically extracted. The final result of the definition extraction process reveals some problems that we have tried to tackle as explained in the next sections. Nevertheless, ontology building problems are out of the scope of this paper, though they have served as test bed for our research on principles for definition writing. 4. Definition typology According to the traditional aristotelic genus-species definition, a definition should describe the concept and its relations to other concepts in the concept system. This type of definition is traditionally called formal definition, or intensional definition [8, 9]. That is to say, it reflects the  superordinate concept to which the designation belongs and its delimiting characteristics. However, there are also other ways of designating concepts, extensional, ostensive, lexical, precising, and stipulative definitions [8] as well as ontological definitions [4]. For a more exhaustive revision on definitions see [13, 12]. Although these definitions can be useful for certain purposes depending on the user’s needs and the approach adopted, they do not conform to a certain defining formulation and hinder any possibilities of formalizing the knowledge expressed in definitions in order to be used for natural language applications, such as knowledge extraction, ontology enrichment, to mention just a few. For this reason, we claim that some recommendations regarding terminological definitions should be considered when preparing domain resources. As [9, 10] stipulates the selection of an appropriate superordinate is crucial for the intelligibilility of the defining statement. In Pearson’s words [20] “the superordinate or closest generic concept should preferably be one step up in the hierarchy from the term being defined”. Moreover, the same superordinate should be used for all terms that belong to the same class. 5. MTT lexicographic tools and BADELE.3000 In order to get more accurate systematic definitions, we decided to use the MTT tools. We considered two possible ways, (a) applying these tools directly to the ontology; (b) using them to enrich a general purpose lexicographic resource which could be later reused in other applications, for instance, for mapping the PhenomenOntology. At this point, we studied the advantages and disadvantages of the database BADELE.3000 [1, 2] that had been developed according to some MTT lexicographic tools. BADELE.3000 is a database that contains the 3,000 most frequently used Spanish nouns. The information of each noun includes the definition and the combinatorial possibilities, among other linguistic information. A systematic process for the design of the database was followed; consequently the lexical data are well structured and separated from the applications that might use them. This way, the features of the data model and the subsequent database make them useful for different purposes, such as word sense disambiguation, machine translation and text generation. As a result, the database contains a minimum of information useful for any type of ontology (because the general vocabulary includes some basic terms transversal to any specific domain) and more than 20,000 combinations. Besides, this resource allows us to infer knowledge potentially useful in real applications. However, BADELE.3000 is a general-purpose resource with a low utility in commercial exploitations as it does not contain crucial information for real applications. The medium, long-term objective is to enrich this generic  16  linguistic resource by formalizing definitions which can help infer conceptual knowledge Thus, our aim is twofold: To solve the problems of definition extraction and to add domain knowledge to a general purpose linguistic resource. The process followed is presented in Fig. 2.  Fig. 2. Definition extraction and systematic lexicalization during BADELE upgrade  As for the lexicographic tools applied to BADELE.3000, we have resorted to three concepts proposed by the Meaning-Text Theory (MTT). The first one is the lexical function (LF) [17: 39-40]: a LF associates a given lexical expression L (such as sound), which is the argument or keyword of F, with a set of lexical expressions –the value of F (such as loud, strong, heavy, deafening, etc). – expressing a specific meaning associated with F (for instance, ‘intense’ for the examples just mentioned which correspond to the LF known as Magn). The second concept is the semantic label: a semantic label is the equivalent to the genus in traditional definitions by genus and differentia. For instance, whale could be defined as a ‘sea mammal that breathes air through a hole at the top of its head and is hunted for meat and for other purposes, as a source of other materials’. The first part of this definition, ‘sea mammal’, the genus, is known in MTT approach as semantic label; the second part of this definition, the differentia, can be attached to some LFs. The third concept is the actant [14, 15] and its derivate, the actantial structure. Actants correspond to beings or things that participate in the process expressed by a predicate: MTT approach considers that there is a sort of argument structure in all kinds of predicative words, which means that not only do the verbs have actants but also the adjectives, adverbs and the predicative nouns. The actantial structure reflects the syntactic expression of the actants, as shown in the example of fleuve (river) of Dicouèbe, in Table 2:  Table 2. Fleuve (river) Dicouèbe Actantial Structure  Nouns  Actantial Structure  Fleuve  [QUI COMMENCE AU lieu X, PASSE PAR LES lieux Z ET SE TERMINE DANS L’étendue d’eau Y]  River  [WHICH STARTS AT THE X place, FLOWS THROUGH THE Z places AND FINISHES AT THE Y area]  Among these three concepts, LFs have proved to be a specially helpful tool for lexicographic works such as the French dictionary DECFC1, the French database Dicouèbe2(developed in Montreal by Polguère and Mel’cuk) and the Spanish database DiCE3 (developed in La Coruña by Alonso Ramos). Fontenelle [5] has also created (semiautomatically) a database but its originality derives from the fact that he takes as source bilingual dictionaries enriched with lexical-semantic information based on LFs. According to Frawley [6] the methodology followed by these resources is ideally suited to the compilation of specialized dictionaries.  6. Problems and solutions In section 3 two problems have been pointed out when describing the definition extraction process. The low ratio of retrieved definitions can be solved by using linguistic resources (such as domain lexicons, WordNet, etc.) during the label search. So, term variants and semantically equivalent terms could be found and their definitions would be retrieved. The total number of definitions retrieved would increase. However, these definitions would show the same inconsistencies derived from the different granularity and specificity compared to existent ontology definitions. That is, the main problem in the whole process is the linguistic realization of definitions. Thus, we have mainly focused on three subsidiary problems derived from the above mentioned problem and proposed some solutions according to MTT: (a) semantic labels as a solution for inferring knowledge, (b) lexical functions as a way of providing coherence to definitions and (c) the actancial structure as a tool for developing consistent and complete definitions. 6.1 Definitions and semantic labels 6.1.1 Problem: inconsistencies on the first part of definitions The first problem that the technical definitions extracted from the knowledge resources used show is the inconsistencies between the name of the label of a group of terms (such as INDUSTRIAL INSTALLATION) and the first part of the definition, i.e. the superordinate of every single term under this label (such as construction, ranch, installation, building), because it differs from one to 
In this paper we present the implementation of deﬁnition extraction from multilingual corpora of scientiﬁc articles. We establish relations between the deﬁnitions and authors by using indexed references in the text. Our method is based on a linguistic ontology designed for this purpose. We propose two evaluations of the annotations.  2 Methodology We propose a method for the identiﬁcation of deﬁnitions and also for the identiﬁcation of relations between authors. This approach allows us to associate a deﬁnition to an author and to establish a link with other texts that could interest the user. The system allows a fully automated text processing, which comprises several stages.  Keywords Semantic annotation, deﬁnition extraction, indexed references 
Books and other text-based learning material contain implicit information which can aid the learner but which usually can only be accessed through a semantic analysis of the text. Deﬁnitions of new concepts appearing in the text are one such instance. If extracted and presented to the learner in form of a glossary, they can provide an excellent reference for the study of the main text. One way of extracting deﬁnitions is by reading through the text and annotating deﬁnitions manually — a tedious and boring job. In this paper, we explore the use of machine learning to extract deﬁnitions from nontechnical texts, reducing human expert input to a minimum. We report on experiments we have conducted on the use of genetic programming to learn the typical linguistic forms of deﬁnitions and a genetic algorithm to learn the relative importance of these forms. Results are very positive, showing the feasibility of exploring further the use of these techniques in deﬁnition extraction. The genetic program is able to learn similar rules derived by a human linguistic expert, and the genetic algorithm is able to rank candidate deﬁnitions in an order of conﬁdence. Keywords Deﬁnition Extraction, Genetic Algorithms, Genetic Program- ming. 
In this paper we report on the performance of different learning algorithms and diﬀerent sampling technique applied to a deﬁnition extraction task, using data sets in diﬀerent language. We compare our results with those obtained by handcrafted rules to extract deﬁnitions. When Deﬁnition Extraction is handled with machine learning algorithms, two diﬀerent issues arise. On the one hand, in most cases the data set used to extract deﬁnitions is unbalanced, and this means that it is necessary to deal with this characteristic with speciﬁc techniques. On the other hand it is possible to use the same methods to extract deﬁnitions from documents in diﬀerent corpus, making the classiﬁer language independent. Keywords machine learning, imbalanced data set, language independent, deﬁnition extraction 
Example sentences provide an intuitive means of grasping the meaning of a word, and are frequently used to complement conventional word deﬁnitions. When a word has multiple meanings, it is useful to have example sentences for speciﬁc senses (and hence deﬁnitions) of that word rather than indiscriminately lumping all of them together. In this paper, we investigate to what extent such sense-speciﬁc example sentences can be extracted from parallel corpora using lexical knowledge bases for multiple languages as a sense index. We use word sense disambiguation heuristics and a cross-lingual measure of semantic similarity to link example sentences to speciﬁc word senses. From the sentences found for a given sense, an algorithm then selects a smaller subset that can be presented to end users, taking into account both representativeness and diversity. Preliminary results show that a precision of around 80% can be obtained for a reasonable number of word senses, and that the subset selection yields convincing results. Keywords Example Sentence Extraction, Parallel Corpora, Disambiguation, Lexical Databases 
In this paper, a terminological framework, both theoretical and methodological, backed by empirical data, is proposed in order to highlight the particular questions to which attention should be paid when conceiving an evaluation scheme for deﬁnition extraction (DE) in terminology. The premise is that not just any information is relevant to deﬁning a given concept in a given expert domain. Therefore, evaluation guidelines applicable to DE should integrate some understanding of what is relevant for terminographic deﬁnitions and in which cases. This, in turn, requires some understanding of the mechanisms of feature selection. An explanatory hypothesis of feature relevance is then put forward and one of its aspects examined, to see to what extent the example considered may serve as a relevance referential. To conclude, a few methodological proposals for automating the application of relevance tests are discussed. The overall objective is to explore ways of empirically testing broader theoretical hypotheses and principles that should orient the conception of general guidelines to evaluate DE for terminographic purposes. Keywords Terminology, terminographic deﬁnitions, evaluation guidelines, terminological theory, terminographic methodology, concepts, feature relevance  extracted on a given concept in a given specialized corpus is relevant to the deﬁnition of that concept in that expert domain. One could argue that since the corpus from which the information is extracted is a specialized one, all the extracted information on a concept is at least potentially deﬁning. However, as we shall see, this is not always the case. How may it be possible, then, to decide what is (or may be) relevant to the deﬁnition of a concept and what is not? What is addressed here is, therefore, a more fundamental kind of evaluation concerning the relevance of the extracted information for terminographic deﬁnition writing. In that perspective, we shall ﬁrst show that what is extracted is not necessarily a deﬁnition, basing our argument on terminological and terminographic frameworks as well as on an empirical study. This background implies several questions which ought to be considered when designing an evaluation scheme applicable to extracted information and its use for terminographic deﬁnitions. Some hypotheses concerning the elements against which the extracted information may be evaluated are proposed and examined, as are methodological approaches to answering the questions thus raised, therefore providing empirical grounds for an evaluation. The main focus of this paper is therefore highlighting various factors that should be considered in evaluating the relevance of extracted information. 2 Background  
In the knowledge society, researchers on lexicography recognize the need to advance in the structure of dictionary information so that it can be understood by people and computers. In relation with dictionary definitions, the adoption of models or templates would be advantageous for the generation of complete definitions and also for the extraction of semantic information from them. In this study, we manually analyzed specialized dictionary definitions of the ceramic field belonging to the conceptual groups of ceramic processes and ceramic defects in order to identify the relevant conceptual features, such as physical aspect or function, and the linguistic realization of these features in the definitions. Results can help to extract information from definitions and to also generate formalized dictionary definitions. Keywords Terminographic definition, features, linguistic markers, patterns, conceptual information 1. Introduction Definitions are a very valuable source of semantic information for different tasks such as the creation of ontologies, lexicography, terminology and natural language processing. Applied research into computational lexicography and terminology is being carried out to recognize definitions in specialized texts and to analyze the semantic relationships that occur in these definitions [19, 20, 14, 17]. Dictionary definitions are other means of obtaining semantic data and of discovering relationships between the concepts of a domain. We can find numerous studies intended to extract information from the definitions in existing dictionaries. Most of them aim to extract taxonomic relationships [5, 11]. However, more effort is needed to exploit the rest of the information present in the definitions, such as specific features and non- hierarchical conceptual relationships [4, 6, 7, 10]. When attempting to extract this information, the biggest problem that automatic systems face is the lack of homogeneity and systematicity in definitions. Most dictionary definitions present inconsistent and incomplete information as well as terms which should be equally treated and which are defined in a very different way [9]. Many authors highlight the need to create more standardized, precise definitions in a format that is understandable for computers and humans alike so that extraction and reusability are easier.  One of the objectives of the ONTODIC1 project is to develop a system to assist the terminographer in the elaboration of definitions. This system will semiautomatically generate definitions based on a definitional template and a domain ontology. The definitional template will contain the necessary linguistic markers to introduce each feature into the definition. An example of this definitional template for the conceptual group ceramic tiles is proposed by Alcina [1]: “A ceramic tile whose shape is X and size is Y, and is decorated with Z to serve as Q” Variables x, y, z and q will be replaced by the values that each feature in the concept description acquires. In this study, our objective is to observe the type of features which are relevant for the description of two conceptual groups (ceramic processes and ceramic defects) in this domain and to analyze how they are expressed linguistically in the 222 definitions taken from three specialized dictionaries of the ceramic field. We studied the linguistic patterns used in the definitions to introduce features. This set of features and their linguistic realization can, on the one hand, be useful to extract information from dictionaries and, on the other, to generate definitions. 2. Semantic information extraction from texts and dictionary definitions Several studies have been conducted to automatically identify the related concepts in a corpus. However, most of them focus on the English and French languages with few centering on Spanish [23, 19]. The authors agree that a good way to extract information from texts is by searching for recurrent patterns. According to [15], knowledge patterns are “words, word combinations or paralinguistic features which frequently indicate conceptual relations”. The authors distinguish three types of patterns: 
In this paper a combination of linguistic and structural information is used for the extraction of Dutch deﬁnitions. The corpus used is a collection of Dutch texts on computing and elearning containing 603 deﬁnitions. The extraction process consists of two steps. In the ﬁrst step a parser using a grammar deﬁned on the basis of the patterns observed in the deﬁnitions is applied on the complete corpus. Machine learning is thereafter applied to improve the results obtained with the grammar. The experiments show that using a combination of linguistic (n-grams, type of article, type of noun) and structural information (layout, position) is a promising approach to the deﬁnition extraction task. Keywords deﬁnition extraction, machine learning, grammar, linguistic fea- tures, text structure 
As more and more people are expressing their opinions on the web in the form of weblogs (or blogs), research on the blogosphere is gaining popularity. As the outcome of this research, diﬀerent natural language tools such as querybased opinion summarizers have been developed to mine and organize opinions on a particular event or entity in blog entries. However, the variety of blog posts and the informal style and structure of blog entries pose many diﬃculties for these natural language tools. In this paper, we identify and categorize errors which typically occur in opinion summarization from blog entries and compare blog entry summaries with traditional news text summaries based on these error types to quantify the diﬀerences between these two genres of texts for the purpose of summarization. For evaluation, we used summaries from participating systems of the TAC 2008 opinion summarization track and updated summarization track. Our results show that some errors are much more frequent to blog entries (e.g. topic irrelevant information) compared to news texts; while other error types, such as content overlap, seem to be comparable. These ﬁndings can be used to prioritize these error types and give clear indications as to where we should put eﬀort to improve blog summarization. Keywords Opinion summarization, blog summarization, news text summarization. 
Automatic event detection aims to identify novel, interesting topics as they are published online. While existing algorithms for event detection have focused on newswire releases, we examine how event detection can work on less structured corpora of blogs. The proliferation of blogs and other forms of selfpublished media have given rise to an ever-growing corpus of news, commentary and opinion texts. Blogs offer a major advantage for event detection as their content may be rapidly updated. However, blogs texts also pose a signiﬁcant challenge in that the described events may be less easy to detect given the variety of topics, writing styles and possible author biases. We propose a new way of detecting events in this media by looking for changes in word semantics. We ﬁrst outline a new algorithm that makes use of a temporally-annotated semantic space for tracking how words change semantics. Then we demonstrate how identiﬁed changes could be used to detect new events and their associated blog entries. 
In past years, there has been substantial work on the problem of entity coreference resolution whereas much less attention has been paid to event coreference resolution. Starting with some motivating examples, we formally state the problem of event coreference resolution in the ACE 1 program, present an agglomerative clustering algorithm for the task, explore the feature impact in the event coreference model and compare three evaluation metrics that were previously adopted in entity coreference resolution: MUC FMeasure, B-Cubed F-Measure and ECM F-Measure. Keywords Pairwise Event Coreference Model, Event Coreference Resolution, Event Attribute 1. Introduction In this paper, we address the task of event coreference resolution specified in the Automatic Content Extraction (ACE) program: grouping all the mentions of events in a document into equivalent classes so that all the mentions in a given class refer to a unified event. We adopt the following terminologies used in ACE [1]:  Entity: an object or set of objects in the world, such as person, organization, facility.  Event: a specific occurrence involving participants.  Event trigger: the word that most clearly expresses an event’s occurrence.  Event argument: an entity, or a temporal expression or a value that has a certain role (e.g., PLACE) in an event.  Event mention: a sentence or phrase that mentions an event, including a distinguished trigger and involving arguments. An event is a cluster of event mentions.  Event attributes: an event has six event attributes, event type, subtype, polarity, modality, genericity, and tense. We demonstrate some motivating examples in table 1 (event triggers are surrounded by curly brackets and event arguments are underlined). In example 1,event mention EM1 corefers with EM2 because they have the same event type and subtype 
The huge amount of data available on the Web needs to be organized in order to be accessible to users in real time. This paper presents a method for summarizing subjective texts based on the strength of the opinion expressed in them. We used a corpus of blog posts and their corresponding comments (blog threads) in English, structured around five topics and we divided them according to their polarity and subsequently summarized. Despite the difficulties of real Web data, the results obtained are encouraging; an average of 79% of the summaries is considered to be comprehensible. Our work allows the user to obtain a summary of the most relevant opinions contained in the blog. This allows them to save time and be able to look for information easily, allowing more effective searches on the Web. Keywords Opinion Mining, Sentiment Analysis, Blog Posts, Automatic Summarization. 1. Introduction Due to the rapid development of the Social Web, new textual genres expressing subjective content by means of emotions, feelings, sentiments, moods or opinions are growing rapidly. Nowadays, people converse frequently using many non-conventional ways of communication such as blogs, forums or reviews. As a consequence, the number of such emerging text types is growing at an exponential rate, as well as their impact on the everyday lives on millions of people. A research for the Pew Institute [1] shows that 75,000 blogs are created per day by people all over the world, on a great variety of subjects. Thus, blogs are becoming an extremely relevant resource for different kinds of studies focused on many useful applications. This research area have become known as sentiment analysis or opinion mining. However, as there is no overall accepted definition of this task and in order to delimit our research area, the  concepts of emotions, feelings, sentiments, moods and opinions need to be defined with precision. Emotion is “an episode of interrelated, synchronized changes in the states of all or most of the five organismic subsystems) in response to the evaluation of an external or internal stimulus event as relevant to major concerns of the organism [2, 3]. The term “feeling” points to a single component denoting the subjective experience process [4] and is therefore only a small part of an emotion. “Moods” are less specific and intense affective phenomena, product of two dimensions - energy and tension [5]. “Sentiment” is defined in the Webster dictionary1 as: 1 a: an attitude, thought, or judgment prompted by feeling: predilection b: a specific view or notion: opinion; 2 a: emotion b: refined feeling: delicate sensibility especially as expressed in a work of art c: emotional idealism d: a romantic or nostalgic feeling verging on sentimentality; 3 a: an idea colored by emotion b: the emotional significance of a passage or expression as distinguished from its verbal context. Finally, the term “opinion”, according to the Webster Dictionary, is 1 a: a view, judgment, or appraisal formed in the mind about a particular matter b: approval, esteem; 2 a: belief stronger than impression and less strong than positive knowledge b: a generally held view; 3 a: a formal expression of judgment or advice by an expert b: the formal expression (as by a judge, court, or referee) of the legal reasons and principles upon which a legal decision is based. As we can deduce from these definitions, affect-related concepts are similar in nature and in many cases overlap; 
This paper examines a new phenomenon in the emergence of news in Internet. Two key cases have been analyzed. The first one demonstrates the emergence of news in comments under the main news; and the second demonstrates the emergence of news in information-sharing websites and their interpretation in the newspapers. The research is based on two small corpora of texts related to these two key cases. The present study proposes some guidelines for understanding the information in the dynamic context of Internet and analyzes some possible ways to extract information from these new types of texts. Key words: news, comments, blogs, information extraction, intertextuality, paratext, metatext 1. Introduction The common understanding is that the newest information is in the established news media. But nowadays with the development of electronic media the real news could appear in the “non-newspapers texts” like in comments under the news, in blogs or in social networks. After that the news is immediately quoted and expanded by news agencies and online newspapers. Internet communication removes the distinction between readers and editors. Editors become readers and the readers themselves can generate news. The eighties were marked by the philosophy of intertextuality. This pre-Internet theory gives an explanation of what has happened in Internet today: the representation of any kind of news is often made by quotations and interpretations of quotations. The problem with the originality of the text still exists and simply said the big question is where the information resides and how it is possible to catch it. This paper is structured as follows: Section 1 introduces the new types of texts and the quoted news, Section 2 provides the highlights of the philosophical theory which is helpful for understanding the new text types, Section 3 presents the corpora used and gives the context of the real stories with some definitions of the news, Section 4 discusses ways to extract information from corpora and shows some results, Section 5  provides the conclusions and some guidelines for future work. 2. A piece of philosophy: Intertextuality and information To understand and classify all new emerging text types on the Internet, this study uses as a basis the classification made by the French theoretician Gerard Genette in 1982 [2]. Genette’s construct is based on literature analysis, without considering the new phenomenon of Internet but Internet makes the observations of Genette much clearer. Genette defined five levels of “transtextuality” (which is conceived as a complexity of the phemomena related to texts). These five levels are: hypotext (the text basis), hypertext (text's understanding), paratext (title, subtitle, intertitle, prefaces, postscripts, notes, footnotes, final notes), metatext (commentaries, literary critique) and intertexts (relationship between two or more texts, where the most explicit form is the quotation; it also includes plagiarism and allusion). 
TerminoWeb is a web-based platform designed to find and explore specialized domain knowledge on the Web. An important aspect of this exploration is the discovery of domain-specific collocations on the Web and their presentation in a concordancer to provide contextual information. Such information is valuable to a translator or a language learner presented with a source text containing a specific terminology to be understood. The purpose of this article is to show a proof of concept that TerminoWeb, as an integrated platform, allows the user to extract terms from the source text and then automatically build a related specialized corpus from the Web in which collocations will be discovered to help the user understand the unknown specialized terms. Keywords term extraction, collocation extraction, concordancer, Web as corpus, domain-specific corpus 1. Introduction Collocations and concordances found in corpora provide valuable information for both acquiring the sense and usage of a term or word. Corpora resources are usually complementary to dictionaries, and provide a more contextual understanding of a term. Collocations and concordances are rarely viewed as “static” resources, the way dictionary definitions would, but are rather often considered the disposable result of a tool’s process (a concordancer, a collocation extractor) on a particular corpus. The use and value of corpora for vocabulary acquisition and comprehension is quite known. In language learning mostly [2], its use obviously has advantages and disadvantages compared to dictionaries, and its context of usage might influence its value (self-learning or classroom). Early work on vocabulary acquisition [21] argued that the learning of a new word is frequently related to the incidence of reading or repetitive listening. Even earlier [23], one experiment illustrated that the frequency of a word and the richness of the context facilitates the identification of a word by a novice reader. Even so, computer-assisted techniques for the understanding of unknown words [15] in second language learning are still not widely studied. 
This paper outlines an approach to the unsupervised construction from unannotated parallel corpora of a lexical semantic resource akin to WordNet. The paper also describes how this resource can be used to add lexical semantic tags to the text corpus at hand. Finally, we discuss the possibility to add some of the predicates typical for WordNet to its automatically constructed multilingual version, and the ways in which the success of this approach can be measured. Keywords Parallel corpora, WordNet, unsupervised learning 
In recent years, translators have increasingly turned to corpora as a resource in their terminology searches. Consequently university translation courses should include training for future translators in the effective use of corpora, thus enabling them to find the terminology they need for their translations more quickly and efficiently. This paper provides a classification of search techniques in electronic corpora which may serve as a useful guide to the efficient use of electronic corpora both in the training of future translators, and for professional translators. Keywords Electronic corpora for translators, search techniques, corpus queries, translation resources, translation training. 1. Introduction Terminology is a key factor in translators’ work. The development of specialized fields has grown hand in hand with advancements in science and technology. These market demands explain why translators are calling for resources to satisfy their terminological needs quickly and effectively [1]. Dictionary creation cannot keep pace with developments in specialized fields. Many studies show dictionaries to be deficient in the lack of information they include, speed of content update, and the limited ways of accessing contents. For this reason, translators are increasingly turning to other resources, such as the Internet and corpora, to search for the terminology they need. In this paper we analyze the search techniques offered by a range of electronic corpora. Our search technique classification is aimed to provide translation teachers with a reference to help them teach students how to use corpora efficiently. This classification may also be of interest to professional translators who want to further their knowledge of electronic corpora techniques in order to improve their query results. 2. The need for corpora in translation Market demands require translators to work against tight deadlines and with rapidly evolving vocabulary. According to Varantola [22], fifty per cent of the time spent on a translation is taken up with consulting reference resources.  Many studies have revealed that dictionaries do not satisfy all translators’ terminological queries [5, 9, 16]. Gallardo and Irazazábal [10] suggest that the terminology translators need, apart from equivalents in different languages, should also include contexts and information about the concept that allow translators to decide how and where to use a term. In this vein, Zanettin [23] states that the use of corpora in translation training was commonplace even before the development of electronic corpora. Snell-Hornby [21] and Shäffner [18], for instance, argue that by studying similar texts in the source and target languages translators may identify prototypical features that are useful for the target text production. Since the development of electronic corpora, the need for these tools has become more evident, especially as a terminology resource for translators. Several authors state that translators need new terminological resources, such as corpora [3, 4, 11, 15], which complement dictionary and database use [8, 12, 19] and satisfy specific terminological problems quickly and reliably. Some studies have demonstrated that translation quality improves when translators use corpora in their terminology searches. Zanettin [23] conducted an experiment with translation students from the School for Translators and Interpreters at the University of Bologna. He shows that comparable corpora1 help translation students to compare the use of similar discourse units in two languages and facilitate the selection of equivalents adapted to the translation context. Bowker [3] carried out a study with translation students from the School of Applied Language and Intercultural Studies at Dublin City University. She found that corpus-aided translations are of higher quality than translations carried out only with the aid of dictionaries. In a subsequent study, Bowker [4] suggests various ways a target language corpus can be used as a terminological resource for translators. Despite the usefulness of corpora, the need to use a range of resources to access terminology is a daily problem facing translators. According to Alcina [1], if translators 
In this paper we describe word alignment experiments using an approach based on a disjunctive combination of alignment evidence. A wide range of statistical, orthographic and positional clues can be combined in this way. Their weights can easily be learned from small amounts of hand-aligned training data. We can show that this “evidence-based” approach can be used to improve the baseline of statistical alignment and also outperforms a discriminative approach based on a maximum entropy classiﬁer.  alignment models that work on raw parallel (sentence aligned) corpora [2, 16]. However, previous studies have shown that only a small number of training examples (around 100 word-aligned sentence pairs) are suﬃcient to train discriminative models that outperform the traditional generative models. In this paper we present another supervised alignment approach based on association clues trained on small amounts of word-aligned data. This approach diﬀers from previous discriminative ones in the way the evidence for alignment is combined as we will explain in the following section.  
In this paper we propose a discriminative framework for automatic tree alignment. We use a rich feature set and a log-linear model trained on small amounts of hand-aligned training data. We include contextual features and link dependencies to improve the results even further. We achieve an overall F-score of almost 80% which is signiﬁcantly better than other scores reported for this task. 
In this paper we present the on-going grammar engineering project in our group for developing in parallel resource precision grammars for Slavic languages. The project utilizes DELPH-IN software (LKB/[incr tsdb()]) as the grammar development platform, and has strong affinity to the LinGO Grammar Matrix project. It is innovative in that we focus on a closed set of related but extremely diverse languages. The goal is to encode mutually interoperable analyses of a wide variety of linguistic phenomena, taking into account eminent typological commonalities and systematic differences. As one major objective of the project, we aim to develop a core Slavic grammar whose components can be commonly shared among the set of languages, and facilitate new grammar development. As a showcase, we discuss a small HPSG grammar for Russian. The interesting bit of this grammar is that the development is assisted by interfacing with existing corpora and processing tools for the language, which saves significant amount of engineering effort. Keywords Parallel grammar engineering, corpora, Slavic languages 1. Introduction Our long-term goal is to develop grammatical resources for Slavic languages and to make them freely available for the purposes of research, teaching and natural language applications. As one major objective of the project, we aim to develop and implement a core Slavic grammar whose components can be commonly shared among the set of languages, and facilitate new grammar development. A decision on the proper set up along with a commitment to a reliable infrastructure right from the beginning are essential for such an endeavor because the implementation of linguistically-informed grammars for natural languages draws on a combination of engineering skills, sound grammatical theory, and software development tools. 1.1 DELPH-IN initiative Current international collaborative efforts on deep linguistic processing with Head-driven Phrase Structure Grammar [1-3] exploit the notion of shared grammar for  the rapid development of grammars for new languages and for the systematic adaptation of grammars to variants of the same language. This international partnership, which became popular under the name DELPH-IN1, is based on a shared commitment to re-usable, multi-purpose resources and active exchange. Its leading idea is to combine linguistic and statistical processing methods for getting at the meaning of texts and utterances. Based on contributions from several member institutions and joint development over many years, an open-source repository of software and linguistic resources has been created that already enjoys wide usage in education, research, and application building. In accord with the DELPH-IN community we view rulebased precision grammars as linguistically-motivated resources designed to model human languages as accurately as possible. Unlike statistical grammars, these systems are hand-built by grammar engineers, taking into account the engineer's theory and analysis for how to best represent various syntactic and semantic phenomena in the language of interest. A side effect of this, however, is that such grammars tend to be substantially different from each other, with no best practices or common representations.2 As implementations evolved for several languages within the same common formalism, it became clear that homogeneity among existing grammars could be increased and development cost for new grammars greatly reduced by compiling an inventory of cross-linguistically valid (or at least useful) types and constructions. To speed up and simplify the grammar development as well as provide a common framework, making the resulting grammars more 
Chat texts produced in an educational environment are categorized and rated for the purpose of positioning (or placement) of the learner with respect to a learning program (appropriate courses, textbooks, etc). The diﬃculty lies in the fact that the texts are short and informal. A standard LSA/vector-space model is therefore combined with techniques appropriate for short texts. The approach uses phrases rather than words in the term-document matrix, and for determining prototypical documents of each category, a nonparametric permutation test is used. 
This document investigates the possibility of extracting lexical information automatically from the pages of a printed dictionary of Maltese. An experiment was carried out on a small sample of dictionary entries using hand-crafted rules to parse the entries. Although the results obtained were quite promising, a major problem turned out to errors introduced by OCR and the inconsistent style adopted for writing dictionary entries. Keywords lexicon extraction, lexical information, lexicon, semitic 
This paper describes some pioneering work as a joint research project between City University of Hong Kong and Yuan Ze University in Taiwan to adapt language resources and technologies in order to set up a computational framework for the study of the creative language employed in classical Chinese poetry. In particular, it will first of all describe an existing ontology of imageries found in poems written during the Tang and the Song dynasties (7th –14th century AD). It will then propose the augmentation of such imageries into primary, complex, extended and textual imageries. A rationale of such a structured approach is that while poets may use a common dichotomy of primary imageries, creative language use is to be found in the creation of complex and compound imageries. This approach will not only support analysis of inter-poets stylistic similarities and differences but will also effectively reveal intra-poet stylistic characteristics. This article will then describe a syntactic parser designed to produce parse trees that will eventually enable the automatic identification of possible imageries and their subsequent structural analysis and classification. Finally, a case study will be presented that investigated the syntactic properties found in two lyrics written by two stylistically different lyric writers in the Song Dynasty. Keywords Imagery, ontology, parsing, lyrics, poetry, Chinese, stylistic analysis, Su Shi, Liu Yong 1. Introduction The language of poetry is different from that employed in other categories of writing. “Defined from a linguistic perspective, poetry represents a variant form of language, different from speech and common writing, unique in its own way as a linguistic system.” (Yuan 1989:2 [12]). The difference of poetic language from other types of writing typically exists in its intentionally polysemous readings through the creative use of imageries as part of the poet’s  artistic conception. Classical Chinese poetry, because of its formal restrictions in terms of syllables, tonal variations and rhyming patterns, commands a language system that appears to be particularly concise, finely rich, highly rhetorical, and thus linguistically complex, requiring a high degree of creativity in writing it, sophisticated interpretation in reading it and often a significant level of difficulty in understanding it. This article addresses the issue of machine-aided analysis and understanding of classical Chinese poetry in general and attempts to establish a computational framework (cf. Figure 1) within which both inter- and intra-poet stylistic differences and similarities can be usefully investigated with firm grounding in textual analysis from a linguistic perspective. In particular, we believe that the creative language of poetry can be effectively investigated through its manipulation of imageries and through the range of linguistic devices that help to achieve the poetic articulation of the intended artistic ambience.  inter-poet analysis  intra-poet analysis  textual analysis  statistical report  imagery extraction and classification  ontology  syntactic analysis and parsing word-class analysis and tagging word unit segmentation  web-based interface  text collection and management  corpus  Figure 1. A computational framework for the computer-aided analysis and understanding of classical Chinese poems.  27 Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 27–34  human names emperors immortals titles organs senses passions love characters behaviour state modifiers art. practice terminologies  affair allusions laws/rules folk practices disasters good omens administration diseases  Imageries  time seasons durations/ times  space place names locations sceneries fairy worlds underworld secular world spatial exp.  object living beings states action motion daily objects buildings food/drink resources weather astronomy modifiers colours other  Figure 2. An ontology of imageries in classical Chinese poems  miscellany states modifiers onomatope phenomena units numerals  This article thus will first of all describe an ontology of imageries that has been created for the poems written during the Tang and the Song dynasties, ranging from the seventh century AD to the thirteenth century AD. It will then propose a new, structured approach towards the extraction and classification of imageries, according to which poetic imageries can be categorised into primary, complex, extended and textual sub-types. Since the automatic processing of imageries in this fashion requires syntactic analysis, we shall then move on to the description of a syntactic parser that provides a structured description of the syntactic constituents for classical Chinese poetry. We finally present a preliminary syntactic analysis of two contemporary poets from the Song dynasty in support of a syntax-based approach to the processing and understanding of classical Chinese poetry. 2. An Ontology of Imageries Lo (2008 [8]) describes an ontology of imageries designed for the study of classical Chinese poetry. The complete collection of the poems from the Tang dynasty was processed at the lexical level. The collection comprises 51,170 poems by 2,821 poets totaling 4,767,979 characters. Individual characters were segmented into meaningful word units (WUs) before WUs were indexed according to their semantic class. A synset was created for synonymous WUs and each synset was described by a keyword. For example, “ԫ‫( ”ڣ‬one year) is the key word for the following five variant synonymous WUs forming the synset:  ԫ‫ ڣ‬ԫሉΕԫᄣΕԫਞΕԫ‫ڣ‬Εԫટ Six classes are constructed: human, affair, time, space, object, and miscellany. See Figure 2. Each item in Figure 2 is categorised further into subcategories which will eventually include the actual words and expressions found in a poem. Human/characters, for example, is subdivided into positive moral characters and negative moral characters. As another example, object/astronomy is subdivided into sun, moon, star, sky, etc. Thus, in addition to the six pandects, the system notes 54 subclasses with a further 372 subdivisions before reaching the terminal classes comprising the actual WUs indexed from the complete collection of poems. The system is now available from http://cls.hs.yzu.edu.tw/tang/Database/index.html. As an example, a search for imageries involving the use of the concept “‫( ”מ‬winter) in the seasons category would yield 221 records, representing 214 lines from 193 poems by 107 poets. They contain 31 different WUs. Table 1 shows a list of such expressions sorted according to frequency in descending order.  28  Table 1. Poetic expressions found in the Complete Collection of Tang Poems involving the imagery winter.  Frequency ˆ˅ʳʳ ˅˅ʳ ˅˅ʳ ˅˃ʳ ˅˃ʳ ˄ˆʳ ˉʳ ˉʳ ˉʳ ˈʳ ˈʳ ˈʳ ˈʳ ˇʳ ˇʳ ˇʳ ˇʳ ˇʳ ˇʳ ˆʳ ˆʳ ˆʳ ˅ʳ ˅ʳ ˅ʳ 2 ˅ʳ ˄ʳ ˄ʳ ˄ʳ ˄ʳ  Chinese Կ‫מ‬ ‫מ‬ਞ ᣤ‫מ‬ ᒡ‫מ‬ ᆖ‫מ‬ ྤ‫מ‬ ‫מ‬୙ʳ ‫מ‬ຳ 堚‫מ‬ ‫֚מ‬ ‫מ‬ጐ ‫۪מ‬ ‫מװ‬ ԫ‫מ‬ ‫ࠐמ‬ ‫מ‬༃ ‫מ‬ན ର‫מ‬ መ‫מ‬ ‫חמ‬ ‫מ‬ટ ༃‫מ‬ Ե‫מ‬ ‫װמ‬ ‫מ‬෡ ߱‫ހ‬ ߠ‫מ‬ ‫ॣמ‬ ‫מڰ‬ ྲྀ‫מ‬ ນ‫מ‬  English three winters winter spring harsh winter impoverished winter enduring winter no winter winter summer winter snow clear winter winter day winter end winter clothes previous winter one winter winter arriving winter cold winter scene cold winter spend winter winter season winter autumn cold winter entering winter winter departing winter deep early winter showing winter winter start early winter remnant winter meeting winter  3. A Structured Approach to the Analysis of Imageries In this article, we propose a more structured approach to imageries than what was described in the previous section. While the ontology remains more or less sufficient for descriptions of classical Chinese poems, the imageries themselves need to be reprocessed to reveal their inner structures. For instance, the imageries involving “‫”מ‬ (winter) in Table 1 can be analysed into the following according to the role of winter within the phrase structure. Consider Table 2.  Table 2. A structured analysis of winter imageries.  Functionwinter Functioncollocate Collocate F ᣤʿʳᒡʿʳ堚ʿʳ  head  modifier  ‫װ‬ʿʳରʿʳ༃ʿʳ 64  ॣʿʳྲྀʿʳ‫ڰ‬  head  determiner  Կʿʳྤʿʳԫ 42  ᆖʿʳመʿʳߠʿʳ  complement verbal  ෡ʿʳ‫װ‬ʿʳԵʿʳ 33  ນʳ  ຳʿʳ۪ʿʳ֚ʿʳ  modifier  head  ནʿʳ༃ʿʳ‫ח‬ʿʳ 28  ॣʳ  head  coordination ਞʿʳટʳ  25  subject  verbal  ጐʿʳࠐʳ  9  Table 2 has four columns. Functionwinter shows the phrase internal function of winter, the imagery in question. Functioncollocate indicates the phrase internal function of the collocates co-occurring with winter. Collocate lists the actual collocates and F the number of occurrence of Functionwinter. The rows are arranged according to F in descending order. As can be seen, of the 216 poetic expressions in the Complete Collection of Tang Poems involving the imagery winter, there are seven types of structural analysis, of which modifier+headwinter is the most frequent, occurring 64 times. This structure also has nine different types of instantiations of the modifier as collocates with winter, namely, “ᣤ” (harsh), “ᒡ” (impoverished), “堚” (clear), “‫( ”װ‬previous), “ର” (cold), “༃” (cold), “ॣ” (early), “ྲྀ” (remnant), and “‫( ”ڰ‬early). It is apparent that modifier+headwinter is not only the most significant in frequential terms but also in terms of the variety of its collocates. It is thus evident that the poetic expressions involving winter could have a more structured and therefore refined representation than what is currently available. We thus propose to distinguish the following types of imageries: primary, complex, extended, and textual. Primary imageries refer to those head nouns that may have an imagery potential. Winter, for example, is a primary imagery. Complex imageries refer to primary imageries that have either a premodifier or a determiner. By this definition, “ᣤ‫( ”מ‬harsh winter, modifier + head) is a complex imagery and so is “Կ‫( ”מ‬three winters, determiner + head). Extended imageries are defined to include those complex imageries that either serve clausal functions with  29  an overt subject-verb-object structure or with other syntactic constructs that function as predicates.1 Finally, textual imageries are represented in the poem as a system of extended imageries, carefully intended and designed by the poet as part of the artistic conception and articulation. The four types of imageries thus correspond to four levels of linguistic analysis schematised in Figure 3.  Primary Imageries  Word-class Analysis  Complex Imageries  Phrasal Analysis  Extended Imageries  Clausal Analysis  Textual Imageries  Textual Analysis  Figure 3. A schematized correspondence between structured imagery analysis and different levels of linguistic analysis  The neat correspondence between structured imagery analysis and different levels of linguistic analysis shows that linguistic analysis can be deployed as a stepping stone between poems as raw texts and an ontology of structured imageries derived from the nominal groups. Computation can be performed on the expressions of imageries according to the clausal structure to derive extended imageries. If necessary, techniques for textual analysis can be applied to extended imageries to represent the raw text as a system of interrelated extended imageries. The structured approach towards the analysis of imageries described in this section will have two immediate applications. The first has to do with the automatic extraction, analysis and classification of imageries, which practically means that ontology generation can be fully automated. The second application lies in the actual analysis of classical Chinese poems. The idea that there is an intrinsic structure within imageries makes it possible to stratify and hence better analyse imageries from lexical, grammatical, syntactic and textual perspectives (cf Figure 3). In the analysis of two poets for inter-poet stylistic differences and similarities, it might be possible that the two poets both make use of a similar set of primary imageries measured in terms of lexical use and semantic grouping. Their creative use of language, which marks them as two different poets or even two distinctive stylistic schools, comes from the creative manipulation of such primary imageries by way of complex imageries, extended imageries and textual imageries.  4. A Syntactic Parser for Classical Chinese Poems The transfer of nominals to a structured representation of imageries thus requires a syntactic parser of classical Chinese poems. This section describes a parser that represents perhaps the very first effort to analyse poetic lines in a syntactic way. The current version of the parser is driven by a phrase structure grammar (PSG) for the generation of syntactic trees. Written in Java, it takes two input files, one as a collection of PSG rules and the other containing a poetic line where each component character is tagged with a partof-speech symbol.2 It produces all of the possible syntactic analysis for the poetic line permissible by the grammar. Consider the following line from a song lyric (ဲ, Ci) written by Liu Yong (਻‫ )ة‬in the Song Dynasty. ྲֲྀՀˈድԳᏓᄏូ‫װ‬Ǆ (Under the setting sun, fisher men bang on the boat and leave for home.) Each character in the above text is POS tagged to yield the following input text where a POS tag is assigned and associated with the character by an underscore: ྲྀ_adj ֲ_n Հ_marker ˈ_punc ድ_n Գ_n Ꮣ _v ᄏ_n ូ‫_װ‬v Ǆ_punc A PSG grammar is written in the following manner: S -> PP NP VP AJP -> adj NP -> NP n NP -> AJP n NP -> n PP -> NP marker punc VP -> VP VP VP -> v NP VP -> v punc The parser then produces a syntactic tree shown in Figure 4.  
We describe our eﬀorts in adapting ﬁve basic natural language processing components to Bulgarian: sentence splitter, tokenizer, part-of-speech tagger, chunker, and syntactic parser. The components were originally developed for English within OpenNLP, an open source maximum entropy based machine learning toolkit, and were retrained based on manually annotated training data from the BulTreeBank. The evaluation results show an F1 score of 92.54% for the sentence splitter, 98.49% for the tokenizer, 94.43% for the part-of-speech tagger, 84.60% for the chunker, and 77.56% for the syntactic parser, which should be interpreted as baseline for Bulgarian. Keywords Part-of-speech tagging, syntactic parsing, shallow parsing, chunking, tokenization, sentence splitting, maximum entropy. 
A ﬂexible construction kit is presented compiling various forms of ﬁnite state replacement rules. The approach is simpler and more declarative than algorithms in the tradition of Kaplan & Kay. Simple constraints can be combined to achieve complex eﬀects, including eﬀects based on Optimality Theory.  
This extended abstract focuses on the main points we will be touching upon during our talk, the aim of which is to present in a concise manner our group’s work on enhancing robustness of lexicalised grammars for real-life applications and thus also on enabling their adaptation to new domains in its entirety.  linguistic information into the process of automatic extension of the lexicon of such language resources enhances their performance and provides linguistically sound and more informative predictions which bring a bigger beneﬁt for the grammars when employed in practical real-life applications. 2 Main Focus Points  
Currently access to institutional repositories is gained using dedicated web interfaces where users can enter keywords in an attempt to express their needs. In many cases this approach is rather cumbersome for users who are required to learn a syntax speciﬁc to that particular interface. To address this problem, we propose to adapt the QALL-ME framework, a reusable framework for fast development of question answering systems, in order to allow users to access information using natural language questions. This paper describes how the web services part of the QALL-ME framework had to be adapted in order to give access to information gathered from unstructured web pages by the AIR project. Keywords QALL-ME framework, web services, question answering, textual entailment 
We built a system which prevents leaks of personal health information inadvertently disclosed in heterogeneous text data . The system works with free-form texts. We empirically tested the system on ﬁles gathered from peer-to-peer ﬁle exchange networks. This study presents our text analysis apparatus. We discuss adaptation of lexical sources used in medical, scientiﬁc, domain for analysis of personal health information. Keywords information leak prevention, personal health information 
This paper discusses the building of the first Bulgarian– Polish–Lithuanian (for short, BG–PL–LT) experimental corpus. The BG–PL–LT corpus (currently under development only for research) contains more than 3 million words and comprises two corpora: parallel and comparable. The BG–PL– LT parallel corpus contains more than 1 million words. A small part of the parallel corpus comprises original texts in one of the three languages with translations in two others, and texts of official documents of the European Union available through the Internet. The texts (fiction) in other languages translated into Bulgarian, Polish, and Lithuanian form the main part of the parallel corpus. The comparable BG–PL–LT corpus includes: (1) texts in Bulgarian, Polish and Lithuanian with the text sizes being comparable across the three languages, mainly fiction, and (2) excerpts from E-media newspapers, distributed via Internet and with the same thematic content. Some of the texts have been annotated at paragraph level. This allows texts in all three languages and in pairs BG–PL, PL–LT, BG–LT, and vice versa to be aligned at paragraph level in order to produces aligned three- and bilingual corpora. The authors focused their attention on the morphosyntactic annotation of the parallel trilingual corpus, according to the Corpus Encoding Standard (CES). The tagsets for corpora annotation are briefly discussed from the point of view of possible unification in future. Some examples are presented. Keywords Bilingual and multilingual corpora, parallel and comparable corpora, corpus annotation, lexical database, bilingual dictionaries. 1. Introduction Due to the recent development of information and communication technologies and the increased mobility of people around the globe, the number of electronic dictionaries has increased extraordinarily. This concerns, in particular, bilingual dictionaries, in which one of the languages is English. An Internet search shows that no electronic dictionaries exist at all for pairs of languages such as Bulgarian-Polish or Bulgarian-Lithuanian. Traditional printed paper dictionaries are either an antiquarian rarity (the most recent Bulgarian-Polish and Polish-Bulgarian dictionaries were published more than 20 years ago) or have never been published at all (BulgarianLithuanian). It can not be expected however that all people know English to communicate with each other, especially if their native languages (Bulgarian and Polish) belong to the  same language family. For the creation of a bilingual electronic or online dictionary for Bulgarian, Polish and Lithuanian an electronic corpus is necessary which will provide the material for lexical database, supporting the dictionary and its subsequent expansion and update. In the recent decades many multilingual corpora were created in the field of corpus linguistics, such as MULTEXT corpus [6], one of the largest EU projects in the domain of language technologies, the MULTEXT-East corpus (MTE for short, annotated parallel and comparable), an extension of the project MULTEXT for Central and Eastern European (CEE) languages [2], Hong Kong bilingual parallel EnglishChinese corpus of legal and documentary texts [5], etc. 2. From Bilingual to Trilingual corpus The MTE project has developed a multilingual corpus, in which three languages: Bulgarian, Czech and Slovene, belong to the Slavic group. The MTE model is being used in the design of the first Bulgarian-Polish corpus, currently under development in the framework of the joint research project ―Semantics and Contrastive linguistics with a focus on a bilingual electronic dictionary‖ between Institute of Mathematics and Informatics—Bulgarian Academy of Sciences and Institute of Slavic Studies—Polish Academy of Sciences, coordinated by L. Dimitrova and V. Koseska. This bilingual corpus supports the lexical database (LDB) of the first experimental online Bulgarian-Polish dictionary [3]. 2.1 Bulgarian-Polish corpus The Bulgarian–Polish corpus consists of two parts: a parallel and a comparable corpus [4]. All texts in the corpus are texts published in and distributed over the Internet. Some texts in the ongoing version of the corpus are annotated at paragraph level. The Bulgarian–Polish parallel corpus includes two parallel sub-corpora: 1) a pure Bulgarian–Polish corpus consists of original texts in Polish – literary works by Polish writers and their translation in Bulgarian, and original texts in Bulgarian short stories by Bulgarian writers and their translation in Polish. 2) a translated Bulgarian–Polish corpus consists of texts in Bulgarian and in Polish of brochures of the EC, documents of the EU and the EU-Parliament, published in Internet; Bulgarian and Polish translations of literary works in third language (mainly English).  
The main goal of this paper is to investigate the behavior of Romanian syllables related to some classical minimum eﬀort laws: the laws of Chebanow, Menzerath and Fenk. The results are compared with results of similar researches realized for diﬀerent languages. Keywords syllable, minimum eﬀort laws 
One of the LT1-applications that ensures the access to the information, in the user’s mother tongue, is machine translation (MT). Unfortunately less spoken languages - a category in which the Balkan and Slavic languages can be included - have to overcome a major gap in language resources, reference-systems and tools. In its simplest form, statistical machine translation (SMT) is based only on the existence of a big parallel corpus and therefore it seems to be a solution for these languages. In this paper the performance of a Moses-based SMT system, for Romanian and German, is investigated using test data from two diﬀerent domains - legislation (JRCACQUIS) and a manual of an electronic device. The obtained results are compared with the ones given by the Google on-line translation tool. An analysis of the obtained translation results gives an overview of the main challenges and sources of errors in translation, in these experimental settings.  bilingual subsets in JRC-ACQUIS have approximately six times less aligned sentences, for the language-pair considered. In this paper the performance of a simplistic Mosesbased SMT-system, when trained and tested on JRCACQUIS (version 2.2), is investigated. For one of the test-set, data from a small technical corpus is used. The obtained results are compared with the ones given by the Google SMT on-line translation tool. The outcome shows that for less resourced languages - in this case Romanian - the development of further parallel corpora on broader domains and the improvement of the existing resources seem to be unavoidable. In the case of JRC-ACQUIS, for Romanian, such a step has already been done with JRC-ACQUIS Version 33. The paper is organized as follows: in section 2 the used corpora are presented; sections 3 and 4 describe the experiments performed and their results. The last section concludes the presented results.  Keywords SMT, Romanian,German, Moses, Google in-line translation tool 
In this paper we present a versatile language processing tool that can be successfully used for many Balkan languages. For its work, this tool relies on several sophisticated textual and lexical resources that have been developed for most Balkan languages. These resources are based on several de facto standards in natural language processing. Keywords Query expansion, e-dictionaries, wordnets, proper names, aligned texts 1. Introduction The software tool WS4LR (shortened for WorkStation for Language Resources) has been developed by the Language Technology Group organized at the Faculty of Mathematics for several years now. Its first version was introduced in 2004 [8] and it dealt mainly with harmonizing various heterogeneous lexical resources. Subsequently, many new features have been added, particularly those that have helped in the production and exploration of aligned texts on the basis of the lexical resources incorporated [9]. The new tool WS4QE (shortened for Work Station for Query Expansion) was developed on the basis of WS4LR that enables the expansion of queries submitted to the Google search engine [10]. The integrated lexical resources enable modifications of users’ queries for both monolingual and multi lingual searches. When presenting WS4LR and WS4QE, we have always stressed that, although they have been mainly used for Serbian, they are by no means language dependent as long as compatible lexical resources exist for any two languages. Nevertheless, the full potential of these tools was until now used only for Serbian, and in bilingual context, for Serbian and English. In this paper we will show that the tools WS4LR and WS4QE are truly independent both from Serbian, for which they were initially developed, and from English which seems to be in the background of many natural language processing tools. The main presupposition for the usage of these tools for other languages is the existence of textual and lexical resources developed in the same methodological framework. Since this prerequisite has been satisfied for Bulgarian, and, to some extent, for some other Balkan languages (Greek, Romanian, etc…), we will  therefore show that WS4LR and WS4QE can be successfully used for these languages. WS4LR, written in C#, is organized in modules which perform different functions. The core of the system WS4LR_Core comprises four .Net libraries, used by two components: the stand-alone windows application WS4LR.exe and the web service wsQueryExpand.asm. Web application WS4QE.asp manages user query request, than uses web service in order to expand user query, submits the expanded query to Google search engine and finally presents retrieved result. 2. Integrated Language Resources In order to prove the usability of WS4LR and WS4QE for languages other than Serbian and English, we have used various resources, both textual and lexical. In the following sections we will briefly present these resources, what methodological framework was used for their development, and how they were integrated for their successful usage. 2.1 Textual Resources – Aligned Texts The aligned texts as a special form of multilingual corpora were the focus of many projects in the past few decades. A systematic approach to the development of multilingual corpora was initiated within the Multext project, which subsequently included East-European languages through the Multext-East project [5]. In the meantime, many multilingual corpora have been compiled from large corpora, usually fully automatically prepared, which have a range comprised from texts in the limited technical domain [18] to more versatile literary corpora [5] that are often more modest in size but minutely prepared. The main textual resource used to explore WS4LR is Jules Verne’s novel Around the World in Eighty Days. This text was chosen for various reasons. First of all, the text is available in digital form for the majority of European languages, including Balkan languages. Regarding its content, it represents a suitable text for different types of analysis, especially in the domain of named entity recognition (geographical concepts and different measures). Besides this, it has already been used for some interesting research, e.g. multi-word tagging [13] and building models for machine translation [21]. Finally, from a practical point of view, its suitability stems from the fact that it presents a sample text for the French distribution of the Unitex system [15].  19 Multilingual Resources, Technologies and Evaluation for Central and Eastern European Languages 2009, Bulgaria, pages 19–25  Versions of the novel in fifteen different languages have been acquired, but not all of these texts have yet been aligned. Among the already aligned texts are the French original and translations in English and four Balkan languages – Serbian, Bulgarian, Greek, and Romanian. In the preparatory phase each translation was marked in accordance with the TEI-standard in XML, and the title (<head>), paragraph (<p>) and segments (<seg>) were included as units of a text logical layout. At the beginning of the alignment process, all segments coincided with sentences automatically tagged by Unitex. The XAlign system [1] was used for the alignment process. Starting from the French version, the goal of the alignment was to establish 1:1 relations with all other languages on the segmental level. In order to achieve this goal and after manually checking all aligned segments, some of them had to be divided into smaller units, and some were grouped into larger units. Thus, we arrived at the total of 4,409 segments in all texts. This way, the missing segments or the inconsistencies between the source text and its translations were identified in most of the cases. In the following example the English segment is given only for the sake of translation. <tu id=" n2941"> <seg lang="en"> <s id="Verne80days.n2941"> Between Omaha and the Pacific, the railway crosses a territory which is still infested by Indians and wild beasts, and a large tract which the Mormons, after they were driven from Illinois in 1845, began to colonise.</s></seg> <seg lang="fr"> <s id="Verne80days.n2941"> Entre Omaha et le Pacifique, le chemin de fer franchit une contrée encore fréquentée par les Indiens et les fauves, -- vaste étendue de territoire que les Mormons commencèrent à coloniser vers 1845, après qu'ils eurent été chassés de l'Illinois.</s></seg> <seg lang="sr"> <s id="Verne80days. n2941"> Izmeÿu Omahe i Tihog okeana pruga prolazi kroz predeo u kome još ima Indijanaca i divljih zveri - prostranu zemlju koju su poþeli naseljavati mormoni oko 1845. godine, kada su ih prognali iz države Ilinois.</s> </seg> <seg lang="bg"> <s id="Verne80days. n2941"> Ɇɟɠɞɭ Ɉɦɚɯɚ ɢ Ɍɢɯɢɹ ɨɤɟɚɧ ɠɟɥɟɡɨɩɴɬɧɚɬɚ ɥɢɧɢɹ ɩɪɟɤɨɫɹɜɚ ɪɚɣɨɧ, ɜɫɟ ɨɳɟ ɧɚɫɟɥɹɜɚɧ ɨɬ ɢɧɞɢɚɧɰɢ ɢ ɞɢɜɢ ɡɜɟɪɨɜɟ. Ɍɨɜɚ ɟ ɨɛɲɢɪɧɚ ɬɟɪɢɬɨɪɢɹ, ɤɨɹɬɨ ɦɨɪɦɨɧɢɬɟ ɫɚ ɡɚɩɨɱɧɚɥɢ ɞɚ ɤɨɥɨɧɢɡɢɪɚɬ ɨɤɨɥɨ 1845 ɝ., ɫɥɟɞ ɤɚɬɨ ɫɚ ɛɢɥɢ ɩɪɨɝɨɧɟɧɢ ɨɬ ɳɚɬɚ ɂɥɢɧɨɣɫ.</s></seg> <seg lang="gr"> <s id="Verne80days. n2941"> ǹȞȐµİıĮ ıĲȘȞ ȅµȐȤĮ țĮȚ ıĲȠȞ ǼȚȡȘȞȚțȩ, ĲȠ ĲȡȑȞȠ įȚĮıȤȓȗİȚ ʌİȡȚȠȤȑȢ ȩʌȠȣ ıȣȤȞȐȗȠȣȞ ĮțȩµĮ ǿȞįȚȐȞȠȚ țĮȚ ĮȖȡȓµȚĮ ĲİȡȐıĲȚĮ İįĮĳȚțȒ ȑțĲĮıȘ ĲȘȞ ȠʌȠȓĮ ĮȡȤȚıĮȞ ȞĮ ĮʌȠȚțȓȗȠȣȞ ȠȚ µȠȡµȩȞȠȚ µİĲȐ ĲȠ 1845, ȠʌȩĲİ țȣȞȘȖȒșȘțĮȞ Įʌȩ ĲȠ ǿȜȚȞȩȚȢ.</s></seg> <seg lang="ro"> <s id=" Verne80days.n569">  între Omaha úi Pacific drumul de fier trece printr-o regiune populatã încã de indieni úi fiare, - vastã întindere pe care mormonii au început s-o colonizeze pe la 1845 dupã ce au fost izgoniĠi din Illinois.</s> </tu> 2.2 Morphological Dictionaries in LADL Format Morphological dictionaries are a necessary resource in various phases of the automatic analysis of a text. The tool WS4LR expects morphological dictionaries to be in the format known as DELAS/DELAF presented in [2] that was developed in LADL (Laboratoire d'Automatique Documentaire et Linguistique) under the guidance of Maurice Gross. The format of a DELAS-type dictionary basically consists of simple word lemmas accompanied with inflectional class codes which allow for the production of a DELAF-type dictionary, which consists of all inflectional forms with their grammatical information. In the Unitex environment, one finite-state transducer responsible for the generation of all inflectional forms of each DELAS lemma corresponds to each inflectional class code. The Serbian morphological dictionary of simple words contains 121,000 lemmas which yield the production of approximately 1,450,000 different lexical words. Close to 87,000 simple lemmas belong to the general lexica, while the remaining 34,000 lemmas represent various kinds of simple proper names [11]. The Bulgarian Grammar dictionary (DELAS dictionary) consists of 127,000 lemmas distributed as follows: app. 85,000 simple lemmas belong to the general lexis, app. 6,000 lemmas represent domain specific lexis and app. 36,000 lemmas are simple proper names. The corresponding DELAF dictionary consists of app. 1,260,000 entries [7]. 2.3 Semantic Networks - Wordnet Semantic networks, seen as one important node in the hierarchy of ontologies, are used more and more in various phases of the automatic analysis of texts. The tool WS4LR expects them to be in the form of wordnets, that is, nodes representing sets of synonymous words (synsets) which are linked by various semantic relations. The first built wordnet was an English wordnet, the so-called Princeton Wordnet (PWN), having today approximately 140,000 synsets. Due to its remarkable size and successful inclusion in various computer-based applications, it is considered as the de facto standard upon which wordnets for many other languages have been built. One successful application of this concept was achieved by the Balkanet project which was funded by the European Commission from (20012004). In the scope of this project, the development of wordnets for the following Balkan languages was initiated [20]: Bulgarian, Greek, Romanian, Serbian, and Turkish. The important feature of these wordnets is that they are all aligned with PWN via the Interlingual index (ILI) [22]. Namely, ILI consists of concepts, while wordnets represent  20  the lexicalization of concepts in various languages and the way which they are connected. The Serbian wordnet today consists of more than 15,000 synsets built by app. 25,000 literals. All of them are linked to PWN, except for 532 Balkan specific concepts that are connected with other Balkan languages, and 155 Serbian specific concepts that remain unconnected with other languages. The Bulgarian wordnet consists of more then 31,000 synsets built by more than 66,000 literals. The synsets are linked with the PWN as well; again there are 436 Balkan specific concepts shared with other Balkan languages and 182 Bulgarian language specific concepts. Both Serbian and Bulgarian wordnets, as well as wordnets for other Balkan languages, are represented in WS4LR using common XML schema. 2.4 The Prolex Database The Prolex project was initiated in the 1990’s with the study of toponyms in French and had the aim of appropriately processing proper names in natural language applications [16]. This work was followed by the development of a Serbian version, which finally led to the design and construction of a relational multilingual dictionary of Proper Names, the Prolexbase, in the form of a relational database [19]. This model is based on two main concepts: the pivot (that represents the conceptual proper name) at a language independent level and the prolexeme (the projection of the pivot onto a particular language) which is a set of lemmas that includes the name, but also its aliases (variations in orthography, abbreviated forms, acronyms, etc.) and its derivatives. For instance, if a meronymy relation is established between the concepts “New York” and “the United States of America”, then their Serbian Latin equivalents Njujork and Sjedinjene Ameriþke Države, their Serbian Cyrillic equivalents ȵɭʁɨɪɤ and ɋʁɟɞɢʃɟɧɟ Ⱥɦɟɪɢɱɤɟ Ⱦɪɠɚɜɟ, and their Bulgarian equivalents ɇɸ Ƀɨɪɤ and ɋɴɟɞɢɧɟɧɢ ɚɦɟɪɢɤɚɧɫɤɢ ɳɚɬɢ are all connected automatically. 3. Using WS4LR with Aligned Texts The WS4LR module that works with aligned texts expects them to be in the Translation Memory eXchange (TMX) format1. It can transform texts previously aligned by XAlign into this format as well as into several other formats: textual, XML and tabular. This is particularly important since XAlign has been integrated into Unitex software starting from its version 2.1. In addition, the user can also produce various visualizations of aligned texts by applying appropriate XSLT transformations. Thus, the user can freely browse with such visualized texts. One such visualization is represented in Figure 1.  
In this paper, we report a work in progress on transforming syntactic structures from the SynTagRus corpus into tectogrammatical trees in the Prague Dependency Treebank (PDT) style. SynTagRus (Russian) and PDT (Czech) are both dependency treebanks sharing lots of common features and facing similar linguistic challenges due to the close relatedness of the two languages. While in PDT the tectogrammatical representation exists, sentences in SynTagRus are annotated on syntactic level only.  annotation layers: the morphological layer, the analytical layer (describing the surface syntax) and the tectogrammatical layer (describing the deep syntax – transition between syntax and semantics). A highly simpliﬁed example of the annotation layers is in Figure 1. The theoretical background of PDT has its roots in the Prague School of Functional and Structural Linguistics, and especially in the language description framework called Functional Generative Description [9]. The following paragraphs summarize the main features of the three layers.  Keywords Dependency treebank, tectogrammatical trees, dependency relations, parallel corpora  
We propose a novel knowledge-rich approach to measuring the similarity between a pair of words. The algorithm is tailored to Bulgarian and Russian and takes into account the orthographic and the phonetic correspondences between the two Slavic languages: it combines lemmatization, hand-crafted transformation rules, and weighted Levenshtein distance. The experimental results show an 11-pt interpolated average precision of 90.58%, which represents a sizeable improvement over two classic rivaling approaches. Keywords Orthographic similarity, phonetic similarity, cross-lingual transformation. 1. Introduction We propose an algorithm that measures the extent to which a Bulgarian and a Russian word are perceived as similar by a person who is fluent in both languages. Leaving aside the full orthographical identity, we assume that words with different orthography can be also perceived as similar when they have the same or a similar stem and inflections, as in the Bulgarian word афектирахме and the Russian аффектировались (both meaning ‘we were affected’). Bulgarian and Russian are closely related Slavonic languages with rich morphology, which motivates us to study the typical orthographical correspondences between their lexical entries (conditioned phonetically and morphologically), which we use to formulate and apply transformation rules for bringing a Russian word close to Bulgarian reading and vice versa. Our algorithm for measuring the similarity between Bulgarian and Russian words first reduces the Russian word to an intermediate Bulgarian-sounding form and then compares it orthographically to the Bulgarian word. The algorithm starts by transliterating the Russian word with the Bulgarian alphabet, and then transforms some typical Russian morphemes and word parts (e.g., prefixes, suffixes, endings, etc.) to their Bulgarian counter-parts. Since both Bulgarian and Russian are highly-inflectional languages, lemmatization is used to convert the wordforms to their lemmata in order to reduce the differences at the morphological level. Finally, the orthographic similarity is measured using a modified Levenshtein distance with letterspecific substitution weights.  2. Method The normalization of the Bulgarian and the Russian words into corresponding intermediate forms has phonetic and morphological motivation and is performed as a sequence of steps, which will be described below. 2.1. Transliteration from Cyrillic to Cyrillic In a strict linguistic sense, transcription is the process of transition from sounds to letters, i.e., from speech to text; it is carried out generally in a monolingual context. In a bilingual context, the notion of transliteration is used to denotе the transition of sounds and their letter correspondences in one language to letters in another language. The term transliteration is commonly used for the transition of letters when the two languages use different alphabets. In this paper, we deal with transliteration since we work with written texts. The linguistic objective of our investigation is to introduce more formal criteria to the investigation of possible cognates between Russian and Bulgarian. By cognates we mean words with equal or close orthography denoting the same meaning; words with equal/close orthography but different meaning are false cognates/friends. For their further investigation in multilingual research, we need to define the exact expression of that identity/closeness by particular metrics and procedures. For a pair of languages from different families, the source of cognates is borrowing between them or from a third language. Besides borrowing, an essential source of cognates in related languages is their common protolanguage. However, in the historical development of both languages, three factors lead to different grapheme shape for fully identical words: (1) language-specific phonetic laws and resulting changes, (2) settings of the spelling systems regulating the sound-letter transition, and (3) divergence in the grammatical systems and the grammatical formatives. 2.1.1 Full coincidence (equality) of letters Both Russian and Bulgarian use the Cyrillic alphabet in their writing systems, but Russian uses two letters not present in Bulgarian: ы and э. Most other letters generally show a full coincidence with some exceptions to be listed in the following subsections. The list below presents the  32  Multilingual Resources, Technologies and Evaluation for Central and Eastern European Languages 2009, Bulgaria, pages 32–39  full identity of Cyrillic letters in both languages in the cognates: азбука – азбука , буква – буква, воля – воля, гипс – гипс, дух – дух, езда – езда, жена – жена, закон – закон, истина – истина, йод – йод, кипарис – кипарис, лак – лак, монета – монета, нож – нож, опора – опора, пост – пост, река – река, сом – сом, том – том, ум – ум, факт – факт, химия – химия, царь – цар ,чай – чай, шум – шум, щит – щит, юг – юг, яхта – яхта As the above list shows, the full identity of the grapheme shape of cognates is manifested mainly when the transformed letter is in initial position. 2.1.2 Regular letter transitions Replacing Russian letters that are missing in the Bulgarian alphabet. The transitions discussed here stem from historic differences in the phonetic and the spelling systems of the two languages. Bulgarian and Russian differ in their contemporary phonetic system mainly at the level of pronunciation; in the distinction of soft and hard consonants. The Russian-specific letters ы and э serve to denote the variant of a ‘hard consonant+и/е’ while in Bulgarian all consonants preceding и and е are soft. This basic difference of the phonetic systems gives us the regular correspondence ы-и and э-е in all Russian-Bulgarian cognates containing these two letters, e.g., рыба – риба, поэт- поет. Removing a Russian letter. Another regular phonetic difference between the two languages, which is also related to the opposition soft/hard, is the allowed softness of a consonant preceding another consonant (пальто) or in final position (шесть). Such phonetic combinations are not allowed in Bulgarian: see the corresponding палто and шест. This regularity allows us to remove all Russian ь in these positions in the initial stage of the process of cognate comparison. Partial regularity of the letter transitions. In non-initial positions, other not so regular but repeated letter correspondences can be observed, e.g., е-я in хлеб-хляб, е-ъ in серп-сърп, о-ъ in сон-сън, у-ъ in муж-мъж, etc. The iterativity of such transitions is due to the specific development of the spelling systems in the two languages. One such example is the disappearance of some Old Slavic letters and their regular replacement with different letters in Russian and Bulgarian. The above-mentioned change у-ъ is due to the disappearance of an Old Slavic letter called ‘big yus’ and its regular replacement by different vowels in all contemporary Slavic languages. The transition is only partially regular since not all occurrences of the letter have the same etymological origin. 2.1.3 Tranformations of n-grams The sound-letter transition legitimated by the spelling rules of the two languages is specific as well; its specificity is observed at the level of the grapheme composition of the full cognates, i.e., those that are borrowed from third languages or that are identical morphologically.  Transformations originating from spelling. A fundamental difference between Russian and Bulgarian spellings is the treatment of double consonants. Russian allows them in every part of the word structure, while in Bulgarian they are only possible at the morpheme boundary. Thus, all words borrowed from third languages keep their double consonants in Russian, but lose them in Bulgarian, e.g., процесс – процес, аффект – афект, etc. In this way, a regular transition ll-l can be formulated for all double consonants with the following stipulation of grammatical origin. In words of Slavic origin, consonant doubling occurs mainly at the morpheme boundary, but in Russian the phenomenon is more frequent since Russian spelling rules are more “phonetic”. For example, they reflect the change voiced-voiceless for all prefixes ending with з and preceding the initial с of the next morpheme. Bulgarian spelling is more ‘morphological’ and conservative; it keeps the з in writing, although it is voiceless in pronunciation, e.g., рассуждение – разсъждение, бессмертный – безсмъртен, etc. This transformation of hard-soft consonants in the final prefix position is only valid for the couple з-с. Thus, the Bulgarian-Russian transition зс-сс can be formulated as regular for prefixes only and cannot be viewed as a universal for other parts of the word, e.g., кавказский – кавказки. Next, the following general question in treating double consonant correspondences arises: if we want to stay in the domain of uni- and bigram transformations, removing the second consonant in Russian can be ambiguous поддержать – поддържам, but буддист – будист, вводить – въвеждам, раввин – равин. The legal consonant doublings in Bulgarian can be only outlined in a larger context – a window of up to five letters, containing the prefix and the next consonant, as in предд, надд, подд, изз, разз, etc., where the second consonant should be preserved. Note that these exceptions from the rule are only valid for double д, з and в – final letter of prefixes, and for н – first letter of the affix н, e.g., непременно – непременно, but аннотация – анотация. . Transformations of morphological origin. In addition to the divergent development of phonetic and spelling systems, the two languages develop different grammatical systems, both at a systemic and at a morphemic level – different categories with different graphemic expressions. That divergence leads to different grapheme shapes for words that are lexically conceived as cognates, e.g., жены – жената, and the difference is manifested in the ending part of the word, consisting of affixes, and ending and related to grammatical forms. The transformations are made in two directions and for both languages. They can consist of removal of a letter sequence or its transformation. 1. Removing agglutinative morphemes. Each of the two languages has one agglutinative mechanism of word formation (but for different parts of speech) – the reflexive morpheme ся and сь in Russian verb conjugation and the postpositioned article in Bulgari-  33  an in nominal inflections (for nouns and adjectives). The corresponding grammatical meanings are expressed in the twin language by other means (the article is totally missing in Russian and the reflexivity of verbs is expressed by a lexical element in Bulgarian – the particle се). Thus, removing these morphemes is the first step in the process of conversion to an intermediate form, e.g., веселиться – веселить, квадратът – квадрат. Note that the Russian agglutinative morpheme ся/сь at the end of the word are non-ambigous: all 212,000 wordforms with the ending ся in our Russian grammatical dictionary are reflexive verb forms. This is not the case with the Bulgarian article, where only removing the morpheme ът for masculin is non-ambiguous, while removing та, ят and other article morpheme can trim the stem, e.g., жената, but квадрат-а. We intentionally do not derive a transformation rule from the last correspondence. Removing Bulgarian articles depends on the accepted conception about the place of lemmatization in the algorithm – should we set the orthographic similarity for all four members of the language pair – lemmata and wordforms – or should we measure the similarity at the lexical level only – the lemmata. In the latter case, no removal is necessary (see 1.3) 2. Transforming ending strings. There is a big group of adjectives in the two languages derived from other parts of speech and formed with the suffix н and an adjectival ending, e.g., шум – шумный, шум – шумен. When the adjective is derived from a noun ending with н, we get a doubled н in the Russian lemma and in the Bulgarian wordforms, e.g., гарнизон-гарнизонный and гарнизон – гарнизонни. Another regular correspondence is manifested in the word derivation with the suffix ск. All these combinations of н/нн/ск and different adjectival endings give the correspondences shown in Table 1.  Russian Ending -нный -ный -нний -ний -ский -ый -нной -ной -ой  Bulgarian Ending -нен -ен -нен -ен -ски -и -нен -ен -и  Examples военный → военен вечный – вечен ранний → ранен вечерний → вечерен вражеский → вражески стрелковый – стрелкови стенной – стенен родной – роден деловой – делови  Table 1: Transforming Russian adjectives to Bulgarian.  For verbs, there are some regularities in the correspondences of the endings of the Russian infinitive and the Bulgarian verb’s main form in first person singular. Table 2 below shows some examples.  Russian Ending -овать -ить, ять -ать -уть -еть  Bulgarian Ending -ам -я -ам -а -ея  Examples декорировать → декорирам бродить → бродя блеять → блея давать → давам гаснуть – гасна белеть → белея  Table 2 – Transformation of Russian verbs to Bulgarian.  Concerning the transformation of endings, it is important to note that two linguistic problems are interrelated here: (1) the formal revelation of the morpheme boundary, and (2) the correct correspondence with the Bulgarian ending. The existing ambiguity in resolving these two problems requires serious statistical investigations before the rules can be formulated. With ambiguity not taken into account, the proposed transformation rules for Russian word endings could sometimes generate the wrong Bulgarian wordform, e.g., висеть could become висея, while the correct Bulgarian form is вися. In order to limit the negative impact of that, we measure the similarity (1) with and (2) without applying rules for lemmatization; we then return the higher value of the two.  2.2. Lemmatization  Bulgarian and Russian are highly-inflectional languages, i.e., they use variety of endings to express the different forms of the same word. When measuring orthographic similarity, endings could cause major problems since they can make two otherwise very similar words appear somewhat different. For example, the Bulgarian word отправената (‘the directed’, a feminine adjective with a definite article) and the Russian word отправленному (‘the directed’, a masculine adjective in dative case) exhibit only about 50% letter overlap, but, if we ignore the endings, the similarity between them becomes much bigger. Thus, if our algorithm could safely ignore word endings when comparing words, it might perform better. If we could remove the ending, the similarity would be measured using the stem, which is the invariable part of the word. Unfortunately, both the ending as a letter sequence and the location of the morpheme boundary are quite ambiguous in both languages. Thus, we need to lemmatize the text, i.e., convert the word to its main form, the lemma. If every member of the pair of candidate cognates from L1 and L2 is represented by a wordform (WF) and its lemma (L), then we could compare: L1 with L2, WF1 with WF2, L1 with WF2 and WF1 with L2. Considering these four options, we can get a better estimation for the similarity not only between close wordforms like the Bulgarian отправената and the Russian отправленному, which look different orthographically, but have very close lemmata, but also between such  34  very different words like the Bulgarian къпейки (‘bathing’, a gerund) and the Russian копейки (‘copeck’, plural feminine noun). The lemmatization of the Bulgarian and the Russian words can be done using specialized dictionaries. In the present work, we will use two large grammatical dictionaries that contain words, their lemmata, and some grammatical information. 2.3. Transformation Weights Let us now come back to the transliteration rules and to the next steps in our algorithm. There are orthographical correspondences between candidate cognates that are not as undisputable as the general rules, but are still observed in the development of the languages, at least for ones with a proven etymological basis. As was shown above, the regular correspondences between the languages can be due to phonetic and spelling reasons. Besides the unconditional letters transitions described above, not so regular ones occur in several cases, and their existence can be taken into account when constructing the weight scale for measuring similarity. A general principle when building a weight scale is that the correspondences between letters denoting consonants and vowels (hereinafter ‘vowels’ and ‘consonants’ only) should be measured separately. The maximal ortographic distance between different letters is 1 (as for а-ц) and the maximal similarity has weight 0 (as for а-а). All weight values between 0 and 1 are assigned to letter correspondences that exist in a non-regular way in some cognates (the above-mentioned correspondence у-ъ was due to etymological reasons). Another general admission is that consonants and vowels with similar sequences of distinctive phonetic features (differing only in the place of articulation or in the presence/absence of voice, e.g., б-в, б-п) have lower weight distance. The same is valid for the pair of letters denoting a regular phonetic change, e.g., reduction (as in а-ъ, о-у) or softening of the preceding consonant (as in у-ю, а-я). Regular correspondences observed in a limited lexical sector (e.g., borrowed from Latin and Greek) such as г-х also have a lower distance. Table 3 shows the letter transformation weights, which can be used to measure the orthographic similarity after the Bulgarian and Russian words have been transliterated to a subset of the Cyrillic alphabet. The weights w(a, b) are used to transform the letter a into the letter b and vice versa. This weight function w is symmetric by definition, i.e., w(a, b) = w(b, a). All other weights not given in Table 3 are equal to 1. In order to write the Russian words in the modified Bulgarian alphabet used in Table 3, we make the following preliminary transformations for all Russian words: э → е; ы → и; ь → (empty letter); ъ → (empty letter) Table 3 shapes the match between letters and the sounds they denote in Bulgarian and Russian. It further correlates weights for letter transformation that have been phonetically justified.  а w(а, е)=0.7; w(а, и)=0.8; w(а, о)=0.7; w(а, у)=0.6; w(а, ъ)=0.5; w(а, ю)=0.8; w(а, я)=0.5 б w(б, в)=0.8; w(б, п)=0.6 в w(в, ф)=0.6 г w(г, х)=0.5 д w(д, т)=0.6 w(е, и)=0.6; w(е, о)=0.7; w(е, у)=0.8; w(е, ъ)=0.5; е w(е, ю)=0.8; w(е, я)=0.5 ж w(ж, з)=0.8; w(ж, ш)=0.6 з w(з, с)=0.5 w(и, й)=0.6; w(и, о)=0.8; w(и, у)=0.8; w(и, ъ)=0.8; и w(и, ю)=0.7; w(и, я)=0.7 й w(й, ю)=0.7; w(й, я)=0.7 к w(к, т)=0.8; w(к, х)=0.6 м w(м, н)=0.7 о w(о, у)=0.6; w(о, ъ)=0.8; w(о, ю)=0.7; w(о, я)=0.8 п w(п, ф)=0.8; w(п, х)=0.9 с w(с, ц)=0.6; w(с, ш)=0.9 т w(т, ф)=0.8; w(т, х)=0.9; w(т, ц)=0.9 у w(у, ъ)=0.5; w(у, ю)=0.6; w(у, я)=0.8 ф w(ф, ц)=0.8 х w(х, ш)=0.9 ц w(ц, ч)=0.8 ч w(ч, ш)=0.9 ъ w(ъ, ю)=0.8; w(ъ, я)=0.8 ю w(ю, я)=0.8 Table 3– Letter substitution weights. 3. The MMEDR Algorithm The MMEDR algorithm (modified minimum edit distance ratio) measures the orthographic similarity between a pair of Bulgarian and Russian words using some general phonetic and morphologically conditioned correspondences between the letters of the two languages in order to estimate the extent to which the two words would be perceived as similar by people fluent in both languages. It returns a value between 0 and 1, where values close to 1 express very high similarity, while 0 is returned for completely dissimilar words. The algorithm has been tailored for Bulgarian and Russian and thus is not directly applicable to other pairs of languages. However, the general approach can be easily adapted to other languages: all that has to be changed are the rules describing the phonetic and the morphological correspondences.  35  The MMEDR algorithm in steps: 1. Lemmatize the Bulgarian word. 2. Lemmatize the Russian word. 3. Transform the Russian word’s ending. 4. Transliterate the Russian word. 5. Remove some double consonants in the Russian word. 6. Calculate the modified Levenshtein distance using suitable weights for letter substitutions. 7. Normalize and calculate the MMEDR value. The algorithm first tries to rewrite the Russian word following Bulgarian letter constructions. As a result, both words are transformed into a special intermediate form and then are compared orthographically using Levenshtein distance with suitable weights for individual letter substitutions. The above general algorithm is run in eight variants with each of steps 1, 2 and 3 being included or excluded, and the largest of the eight resulting values is returned. A description of each step follows below. 3.1. Lemmatizing Bulgarian and Russian words The Bulgarian word is lemmatized using a grammatical dictionary of Bulgarian as described in Section 1.3. If the dictionary contains no lemmata for the target word, the original word is returned; if it contains more than one lemma, we try using each of them in turn and we choose the one yielding the highest value in the MMEDR algorithm. The Russian word is lemmatized in the same way, using a grammatical dictionary of Russian. 3.2. Transforming the Russian Ending At this step, we transform the endings of the Russian word according to Tables 1 and 2 and we remove the agglutinative suffix ся: нный → нен; ный → ен; нний → нен; ний → ен; ий → и; ый → и; нной → нен; ной → -ен; ой → и; ский - ски; ься → ь; овать → ам; ить → я; ять → я; ать → ам; уть → а; еть → ея The substitutions rules are applied only if the left handside letter sequences are at the end of the word. Rules are applied in the given order; multiple rule applications are allowed. Note that we do not have rules for all possible endings in Russian, but only for the typical ones – object of transformation for adjectives and verbs. Since all words have been already lemmatized in the previous step (if applied), verbs are assumed to be in infinitive and adjectives in singular masculine form. Adjective endings are transformed to their respective Bulgarian counter-parts, and reflexive verbs are turned into non-reflexive. Nouns are not considered since they generally have the same endings in the two languages  (after having been lemmatized) and thus need no additional transformations. Of course, there are many exceptions for the above rules, but our experiments show that using each of them has more positive than negative effect. Initially, we tried using few more additional rules, which were subsequently removed since they were found to be harmful. 3.3. Removing double consonants  According to 1.1.3, the following substitution rules are applied for the Russian word: бб → б; жж → ж; кк → к; лл → л; мм → м; пп → п; рр → р; сс → с; тт → т; фф → ф  3.4. Calculating the Modified Levenshtein Distance with Weights for Letter Substitution  Given two words, the Levenshtein distance [Levenshtein, 1965], also known as the minimum edit distance (MED), is defined as the minimum total number of single-letter substitutions, deletions and/or insertions necessary to convert the first word into the second one. We use a modification, which we call modified minimum edit distance (MMED), where the weights of all insertions and deletions are fixed to 1, and the weights for single-letter substitution are as given in Table 3.  3.5. Calculating MMEDR  At this step, we calculate MMEDR value by normalizing MMED – we divide it by the length of the longer word (the length is calculated after all transformations have been made in the previous steps). We use the following formula:  MMEDR(wbg  ,  wru  )  =  1−  MMED(wbg , wru ) max( wbg , wru )  3.6. Calculating the final result The final result is given by the maximum of the obtained values for all eight variants of the MMEDR algorithm – with/without lemmatization of the Bulgarian word, with/without lemmatization of the Russian word, and with/without transformation of the Russian word ending. Note also, that lemmatization steps might result in calculating additional values for MMEDR – one for each possible lemma of the Russian/Bulgarian word. 3.7. Example As we will see below, the proposed MMEDR algorithm yields significant improvements over classic orthographic similarity measures like LCSR (longest common subsequence ratio, defined as the longest common letter subsequence, normalized by the length of the longer word [Melamed, 1999]) and MEDR (minimum edit distance  36  ratio, defined as the Levenshtein distance with all weights set to 1, normalized by the length of the longer word, also known as normalized edit distance /NED/ [Marzal & Vidal, 1993]). This is due to the above-described steps which turn the Russian word into a Bulgarian-sounding one and the application of letter substitution weights that reflect the closeness of the corresponding phonemes. Let us consider for example the Bulgarian word афектирахме and the Russian word аффектировались. Using the classic Levenshtein distance, we obtain the following: MED(афектирахме, аффектировались) = 7. And after normalization: MEDR=1–(7/15) = 8/15 ≈ 53%. In contrast, with the MMEDR algorithm, we first lemmatize the two words, thus obtaining афектирам and аффектировать respectively. We then replace the double Russian consonant -фф- by -ф- and the Russian ending -овать by the first singular Bulgarian verb ending -ам. We thus obtain the intermediate forms афектирам and афектирам, which are identical, and MMEDR = 100%. Note that some pairs of words like афектирахме and аффектировались could be neither orthographically nor phonetically close but could be perceived as similar due to cross-lingual correspondences that are obvious to people speaking both languages. Let us take another example – with the Bulgarian word избягам and the Russian word отбегать (both meaning ‘to run out’), which sound similarly. Using Levenshtein distance: MED(избягам,отбегать) = 5 and thus MEDR = 1 – (5/8) = 3/8 = 37.5%. In contrast, with the MMEDR algorithm, we first transform отбегать to its intermediate form отбегам and we then calculate MMED(избягам, отбегам) = 0.8 + 1 + 0.5 = 2.3 and MMEDR = 1 – (2.3/7) = 47/70 ≈ 67%, which is a much better reflection of the similarity between the two words. Thus, we can conclude that, at least in the above two examples, the traditional MEDR does not work well for the highly inflectional Bulgarian and Russian. MEDR is based on the classic Levenshtein distance, which uses the same weight for all letter substitution, and thus cannot distinguish small phonetic changes like replacing я with е (two phonetically very close vowels) from more significant differences like replacing я with г (a vowel and a consonant that are quite different). 4. Experiments and Evaluation We performed several experiments in order to assess the accuracy of the proposed MMEDR algorithm for measuring the similarity between Bulgarian and Russian words in a literary text. 4.1. Textual resources We used the Russian novel The Lord of the World (Властелин мира) by Alexander Belyayev [Belayayev, 1940a] and its Bulgarian translation by Assen Trayanov [Belayayev, 1940b] as our test data. We extracted the first 200 different Bulgarian words and the first 200 different Russian words that occur in the novel, and we measured the similarity between them.  Bulga- Rus# rian sian MMEDR Sim Precision Recall word word  
We describe a methodology for improving the generation of multiple-choice test items through the usage of language technologies. We apply common natural language processing techniques, like constituency parsing and automatic term extraction together with additional morphosyntactic rules on raw instructional material in order to determine its key terms. These key terms are then used for the creation of ﬁll-inthe blank test items and the selection of distractors. Our work aims at proving the availability and compatibility of language resources and technologies for Bulgarian, as well as at assessing the readiness for implementation of these techniques in real-world applications. Keywords information extraction, natural language processing application in e-learning 
Recent work on information presentation in dialogue systems combines user modelling (UM) and stepwise reﬁnement through clustering and summarisation (SR) in the UMSR approach. An evaluation in which participants rated dialogue transcripts showed that UMSR presents complex trade-offs understandably, provides users with a good overview of their options, and increases users’ conﬁdence that all relevant options have been presented (Demberg and Moore, 2006). In this paper, we evaluate the effectiveness of the UMSR approach in a more realistic setting, by incorporating this information presentation technique into a full endto-end dialogue system in the city information domain, and comparing it with the traditional approach of presenting information sequentially. Our results suggest that despite complications associated with a real dialogue system setting, the UMSR model retains its advantages. 
We investigate novel approaches to responsive overlap behaviors in dialogue systems, opening possibilities for systems to interrupt, acknowledge or complete a user’s utterance while it is still in progress. Our speciﬁc contributions are a method for determining when a system has reached a point of maximal understanding of an ongoing user utterance, and a prototype implementation that shows how systems can use this ability to strategically initiate system completions of user utterances. More broadly, this framework facilitates the implementation of a range of overlap behaviors that are common in human dialogue, but have been largely absent in dialogue systems. 
In this paper, we describe the development of a meeting assistant agent that helps remote meeting participants by notifying them when they are being addressed. We present experiments that have been conducted to develop machine classiﬁers to decide whether “you are being addressed” where “you” refers to a ﬁxed (remote) participant in a meeting. The experimental results back up the choices made regarding the selection of data, features, and classiﬁcation methods. We discuss variations of the addressee classiﬁcation problem that have been considered in the literature and how suitable they are for addressee detection in a system that plays a role in a live meeting. 
J. B. Bavelas, L. Coates, and T. Johnson. 2000. Listeners as co-narrators. Journal of Personality and Social Psychology, 79:941–952. 
In this paper we do two things: a) we discuss in general terms the task of incremental reference resolution (IRR), in particular resolution of exophoric reference, and specify metrics for measuring the performance of dialogue system components tackling this task, and b) we present a simple Bayesian ﬁltering model of IRR that performs reasonably well just using words directly (no structure information and no hand-coded semantics): it picks the right referent out of 12 for around 50 % of realworld dialogue utterances in our test corpus. It is also able to learn to interpret not only words but also hesitations, just as humans have shown to do in similar situations, namely as markers of references to hard-to-describe entities. 
We describe an approach to dealing with interpretation errors in a tutorial dialogue system. Allowing students to provide explanations and generate contentful talk can be helpful for learning, but the language that can be understood by a computer system is limited by the current technology. Techniques for dealing with understanding problems have been developed primarily for spoken dialogue systems in informationseeking domains, and are not always appropriate for tutorial dialogue. We present a classiﬁcation of interpretation errors and our approach for dealing with them within an implemented tutorial dialogue system. 
This paper describes a probabilistic mechanism for the interpretation of sentence sequences developed for a spoken dialogue system mounted on a robotic agent. The mechanism receives as input a sequence of sentences, and produces an interpretation which integrates the interpretations of individual sentences. For our evaluation, we collected a corpus of hypothetical requests to a robot. Our mechanism exhibits good performance for sentence pairs, but requires further improvements for sentence sequences. 
We propose a framework for analyzing episodic conversational activities in terms of expressed relationships between the participants and utterance content. We test the hypothesis that linguistic features which express such properties, e.g. tense, aspect, and person deixis, are a useful basis for automatic intentional discourse segmentation. We present a novel algorithm and test our hypothesis on a set of intentionally segmented conversational monologues. Our algorithm performs better than a simple baseline and as well as or better than well-known lexical-semantic segmentation methods. 
We present a taxonomy and classification system for distinguishing between different types of paragraphs in movie reviews: formal vs. functional paragraphs and, within the latter, between description and comment. The classification is used for sentiment extraction, achieving improvement over a baseline without paragraph classification. 
Our goal is to make note-taking easier in meetings by automatically detecting noteworthy utterances in verbal exchanges and suggesting them to meeting participants for inclusion in their notes. To show feasibility of such a process we conducted a Wizard of Oz study where the Wizard picked automatically transcribed utterances that he judged as noteworthy, and suggested their contents to the participants as notes. Over 9 meetings, participants accepted 35% of these suggestions. Further, 41.5% of their notes at the end of the meeting contained Wizard-suggested text. Next, in order to perform noteworthiness detection automatically, we annotated a set of 6 meetings with a 3-level noteworthiness annotation scheme, which is a break from the binary “in summary”/ “not in summary” labeling typically used in speech summarization. We report Kappa of 0.44 for the 3way classification, and 0.58 when two of the 3 labels are merged into one. Finally, we trained an SVM classifier on this annotated data; this classifier’s performance lies between that of trivial baselines and inter-annotator agreement. 
A distinguishing feature of dialogue is that more that one person can contribute to the production of an utterance. However, until recently these ‘split’ utterances have received relatively little attention in models of dialogue processing or of dialogue structure. Here we report an experiment that tests the effects of artiﬁcially introduced speaker switches on groups of people engaged in a task-oriented dialogue. The results show that splits have reliable effects on response time and on the number of edits involved in formulating subsequent turns. In particular we show that if the second half of an utterance is ‘misattributed’ people take longer to respond to it. We also show that responses to utterances that are split across speakers involve fewer deletes. We argue that these effects provide evidence that: a) speaker switches affect processing where they interfere with expectations about who will speak next and b) that the pragmatic effect of a split is to suggest to other participants the formation of a coalition or sub-‘party’. 
Gestures are usually looked at in isolation or from an intra-propositional perspective essentially tied to one speaker. The Bielefeld multi-modal Speech-AndGesture-Alignment (SAGA) corpus has many interactive gestures relevant for the structure of dialogue (Rieser 2008, 2009). To describe them, a dialogue theory is needed which can serve as a speechgesture interface. PTT (Poesio and Traum 1997, Poesio and Rieser submitted a) can do this job in principle, how this can be achieved is the main topic of this paper. As a precondition, the empirical research procedure from systematic corpus annotation via gesture typology to a partial ontology for gestures is described. It is then explained how PTT is extended to provide an incremental modelling of speech plus gesture in an assertion-acknowledgement adjacency pair where grounding between dialogue participants is obtained through gesture. 
Demonstrative terms are highly contextdependent elements both in deictic and anaphoric uses. When reference is transferred from a visual, threedimensional context to the textual domain, information-structure factors (i.e. the cognitive status of the antecedent, recency of mention, syntactic structure or the semantic type of the antecedent) have an effect on speaker preferences for selecting demonstrative anaphors over other referring expressions. In certain languages, there seems to be a correlation between demonstratives and tenses in discourse. For example, proximal demonstratives correlate better with present tenses whereas distal demonstratives correlate with past tenses. In this paper, we present a corpus study of Spanish texts that analyzes the ways in which temporal expressions selectively favor the use of specific demonstratives thus confirming the contextual dependency of demonstrative anaphors. 
The authors present a study of prosodic turn-taking indicators. The aim was to investigate whether some of the prosodic cues increase in quality or quantity if the optical feedback channel in the verbal conversation is missing. For the study we built up an experimental setup in which conversational partners held a conversation once with and once without an optical feedback channel. A detailed transcription of the recorded speech material was segmented into turns. In each turn the topic units were identified and the syllables were labelled. We measured and compared prosodic feature characteristics between turn-final and turn-medial topic units. 
This paper presents the first step in designing a speech-enabled robot that is capable of natural management of miscommunication. It describes the methods and results of two WOz studies, in which dyads of naïve participants interacted in a collaborative task. The first WOz study explored human miscommunication management. The second study investigated how shared visual space and monitoring shape the processes of feedback and communication in task-oriented interactions. The results provide insights for the development of human-inspired and robust natural language interfaces in robots. 
We present a new two-tier user simulation model for learning adaptive referring expression generation (REG) policies for spoken dialogue systems using reinforcement learning. Current user simulation models that are used for dialogue policy learning do not simulate users with different levels of domain expertise and are not responsive to referring expressions used by the system. The twotier model displays these features, that are crucial to learning an adaptive REG policy. We also show that the two-tier model simulates real user behaviour more closely than other baseline models, using the dialogue similarity measure based on Kullback-Leibler divergence. 
Our aim is to build listening agents that can attentively listen to the user and satisfy his/her desire to speak and have himself/herself heard. This paper investigates the characteristics of such listening-oriented dialogues so that such a listening process can be achieved by automated dialogue systems. We collected both listening-oriented dialogues and casual conversation, and analyzed them by comparing the frequency of dialogue acts, as well as the dialogue ﬂows using Hidden Markov Models (HMMs). The analysis revealed that listening-oriented dialogues and casual conversation have characteristically different dialogue ﬂows and that it is important for listening agents to selfdisclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners. 
Most studies on speech-based emotion recognition are based on prosodic and acoustic features, only employing artiﬁcial acted corpora where the results cannot be generalized to telephone-based speech applications. In contrast, we present an approach based on utterances from 1,911 calls from a deployed telephone-based speech application, taking advantage of additional dialogue features, NLU features and ASR features that are incorporated into the emotion recognition process. Depending on the task, non-acoustic features add 2.3% in classiﬁcation accuracy compared to using only acoustic features. 
For a spoken dialog system to make good use of a speech recognition N-Best list, it is essential to know how much trust to place in each entry. This paper presents a method for assigning a probability of correctness to each of the items on the N-Best list, and to the hypothesis that the correct answer is not on the list. We ﬁnd that both multinomial logistic regression and support vector machine models yields meaningful, useful probabilities across different tasks and operating conditions. 
There is a long history of using logic to model the interpretation of indirect speech acts. Classical logical inference, however, is unable to deal with the combinations of disparate, conﬂicting, uncertain evidence that shape such speech acts in discourse. We propose to address this by combining logical inference with probabilistic methods. We focus on responses to polar questions with the following property: they are neither yes nor no, but they convey information that can be used to infer such an answer with some degree of conﬁdence, though often not with enough conﬁdence to count as resolving. We present a novel corpus study and associated typology that aims to situate these responses in the broader class of indirect question–answer pairs (IQAPs). We then model the different types of IQAPs using Markov logic networks, which combine ﬁrst-order logic with probabilities, emphasizing the ways in which this approach allows us to model inferential uncertainty about both the context of utterance and intended meanings. 
In this work we examine user adaptation to a dialog system’s choice of realization of task-related concepts. We analyze forms of the time concept in the Let’s Go! spoken dialog system. We ﬁnd that users adapt to the system’s choice of time form. We also ﬁnd that user adaptation is affected by perceived system adaptation. This means that dialog systems can guide users’ word choice and can adapt their own recognition models to gain improved ASR accuracy. 
 2 System Description  In this demonstration we present a mixedinitiative dialog system for address recognition that lets users to specify a complete addresses in a single sentence with address components spoken in their natural sequence. Users can also specify fewer address components in several ways, based on their convenience. The system extracts speciﬁed address components, prompts for missing information, disambiguates items independently or collectively all the while guiding the user so as to obtain the desired valid address. The language modeling and dialog management techniques developed for this purpose are also brieﬂy described. Finally, several use cases with screen shots are presented. The combined system yields very high task completion accuracy on user tests. 
We have developed a complete spoken dialogue framework that includes rule-based and trainable dialogue managers, speech recognition, spoken language understanding and generation modules, and a comprehensive web visualization interface. We present a spoken dialogue system based on Reinforcement Learning that goes beyond standard rule based models and computes on-line decisions of the best dialogue moves. Bridging the gap between handcrafted (e.g. rule-based) and adaptive (e.g. based on Partially Observable Markov Decision Processes - POMDP) dialogue models, this prototype is able to learn high rewarding policies in a number of dialogue situations. 
Nowadays, most commercial and research dialogue applications for call centers are created using sophisticated and fullyfeature development platforms. Surprisingly, most of them lack of some kind of acceleration strategy based on an automatic analysis of the contents or structure of the backend database. This paper describes our efforts to incorporate this kind of information which continues the work done in (D’Haro et al, 2006). Our main proposed strategies are: the generation of automatic state proposals for defining the dialogue flow network, the automatic selection of slots to be requested using mixed-initiative, the semi-automatic generation of SQL statements, and the quick generation of the data model of the application and the connection with the database fields. Subjective and objective evaluations demonstrate the advantages of using the accelerations and their high acceptance, both in our current proposals and in previous work. 
Models for predicting judgments about the quality of Spoken Dialog Systems have been used as overall evaluation metric or as optimization functions in adaptive systems. We describe a new approach to such models, using Hidden Markov Models (HMMs). The user’s opinion is regarded as a continuous process evolving over time. We present the data collection method and results achieved with the HMM model. 
This paper is part of our broader investigation into the utility of discourse structure for performance analysis. In our previous work, we showed that several interaction parameters that use discourse structure predict our performance metric. Here, we take a step forward and show that these correlations are not only a surface relationship. We show that redesigning the system in light of an interpretation of a correlation has a positive impact. 
Motivated by the psycholinguistic ﬁnding that human eye gaze is tightly linked to speech production, previous work has applied naturally occurring eye gaze for automatic vocabulary acquisition. However, unlike in the typical settings for psycholinguistic studies, eye gaze can serve different functions in human-machine conversation. Some gaze streams do not link to the content of the spoken utterances and thus can be potentially detrimental to word acquisition. To address this problem, this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition. Our empirical results indicate that automatic identiﬁcation of closely coupled gaze-speech streams leads to signiﬁcantly better word acquisition performance. 
Our hypothesis is that conversational implicatures are a rich source of clariﬁcation questions. In this paper we do two things. First, we motivate the hypothesis in theoretical, practical and empirical terms. Second, we present a framework for generating the clariﬁcation potential of an instruction by inferring its conversational implicatures with respect to a particular context. General means-ends inference, beyond classical planning, turns out to be crucial. 
Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment. 
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, page 216, Queen Mary University of London, September 2009. c 2009 Association for Computational Linguistics 
This paper presents an experimental study that analyzes how conversational agents activate human communication in thought-evoking multi-party dialogues between multi-users and multi-agents. A thought-evoking dialogue, which is a kind of interaction in which agents act on user willingness to provoke user thinking, has the potential to stimulate multi-party interaction. In this paper, we focus on quiz-style multi-party dialogues between two users and two agents as an example of a thought-evoking multi-party dialogue. The experiment results showed that the presence of a peer agent signiﬁcantly improved user satisfaction and increased the number of user utterances. We also found that agent empathic expressions signiﬁcantly improved user satisfaction, raised user ratings of a peer agent, and increased user utterances. Our ﬁndings will be useful for stimulating multi-party communication in various applications such as educational agents and community facilitators. 
We present computational models that allow spoken dialog systems to handle multiparticipant engagement in open, dynamic environments, where multiple people may enter and leave conversations, and interact with the system and with others in a natural manner. The models for managing the engagement process include components for (1) sensing the engagement state, actions and intentions of multiple agents in the scene, (2) making engagement decisions (i.e. whom to engage with, and when) and (3) rendering these decisions in a set of coordinated low-level behaviors in an embodied conversational agent. We review results from a study of interactions "in the wild" with a system that implements such a model. 
We use directed graphical models (DGMs) to automatically detect decision discussions in multi-party dialogue. Our approach distinguishes between different dialogue act (DA) types based on their role in the formulation of a decision. DGMs enable us to model dependencies, including sequential ones. We summarize decisions by extracting suitable phrases from DAs that concern the issue under discussion and its resolution. Here we use a semantic-similarity metric to improve results on both manual and ASR transcripts. 
We describe a machine learning approach that allows an open-world spoken dialog system to learn to predict engagement intentions in situ, from interaction. The proposed approach does not require any developer supervision, and leverages spatiotemporal and attentional features automatically extracted from a visual analysis of people coming into the proximity of the system to produce models that are attuned to the characteristics of the environment the system is placed in. Experimental results indicate that a system using the proposed approach can learn to recognize engagement intentions at low false positive rates (e.g. 2-4%) up to 3-4 seconds prior to the actual moment of engagement. 
We examine a number of objective, automatically computable TURN-YIELDING CUES — distinct prosodic, acoustic and syntactic events in a speaker’s speech that tend to precede a smooth turn exchange — in the Columbia Games Corpus, a large corpus of task-oriented dialogues. We show that the likelihood of occurrence of a turn-taking attempt from the interlocutor increases linearly with the number of cues conjointly displayed by the speaker. Our results are important for improving the coordination of speaking turns in interactive voice-response systems, so that systems can correctly estimate when the user is willing to yield the conversational ﬂoor, and so that they can produce their own turn-yielding cues appropriately. 
This paper presents a preliminary English corpus study of split utterances (SUs), single utterances split between two or more dialogue turns or speakers. It has been suggested that SUs are a key phenomenon of dialogue, which this study conﬁrms: almost 20% of utterances were found to ﬁt this general deﬁnition, with nearly 3% being the between-speaker case most often studied. Other claims/assumptions in the literature about SUs’ form and distribution are investigated, with preliminary results showing: splits can occur within syntactic constituents, apparently at any point in the string; it is unusual for the separate parts to be complete units in their own right; explicit repair of the antecedent does not occur very often. The theoretical consequences of these results for claims in the literature are pointed out. The practical implications for dialogue systems are mentioned too. 
In real-world applications, modelling dialogue as a POMDP requires the use of a summary space for the dialogue state representation to ensure tractability. Suboptimal estimation of the value function governing the selection of system responses can then be obtained using a gridbased approach on the belief space. In this work, the Monte-Carlo control technique is extended so as to reduce training over-ﬁtting and to improve robustness to semantic noise in the user input. This technique uses a database of belief vector prototypes to choose the optimal system action. A locally weighted k-nearest neighbor scheme is introduced to smooth the decision process by interpolating the value function, resulting in higher user simulation performance. 
In this paper we compare two Machine Learning approaches to the task of pronominal anaphora resolution: a conventional classiﬁcation system based on C5.0 decision trees, and a novel perceptron-based ranker. We use coreference links annotated in the Prague Dependency Treebank 2.0 for training and evaluation purposes. The perceptron system achieves f-score 79.43% on recognizing coreference of personal and possessive pronouns, which clearly outperforms the classiﬁer and which is the best result reported on this data set so far. 
We hypothesize that monitoring the accuracy of the “feeling of another’s knowing” (FOAK) is a useful predictor of tutorial dialogue system performance. We test this hypothesis in the context of a wizarded spoken dialogue tutoring system, where student learning is the primary performance metric. We ﬁrst present our corpus, which has been annotated with respect to student correctness and uncertainty. We then discuss the derivation of FOAK measures from these annotations, for use in building predictive performance models. Our results show that monitoring the accuracy of FOAK is indeed predictive of student learning, both in isolation and in conjunction with other predictors. 
The freely available SPaRKy sentence planner uses hand-written weighted rules for sentence plan construction, and a useror domain-speciﬁc second-stage ranker for sentence plan selection. However, coming up with sentence plan construction rules for a new domain can be difﬁcult. In this paper, we automatically extract sentence plan construction rules from the RST-DT corpus. In our rules, we use only domainindependent features that are available to a sentence planner at runtime. We evaluate these rules, and outline ways in which they can be used for sentence planning. We have integrated them into a revised version of SPaRKy. 
In order to build a dialogue system that can interact with humans in the same way as humans interact with each other, it is important to be able to collect conversational data. This paper introduces a dialogue recording method where an eavesdropping human operator sends instructions to the participants in an ongoing humanhuman task-oriented dialogue. The purpose of the instructions is to control the dialogue progression or to elicit interactional phenomena. The recordings were used to build a Swedish synthesis voice with disfluent diphones. 
We present a toolkit for manipulating and visualising time-aligned linguistic data such as dialogue transcripts or language processing data. The package complements existing editing tools by allowing for conversion between their formats, information extraction from the raw ﬁles, and by adding sophisticated, and easily extended methods for visualising the dynamics of dialogue processing. To illustrate the versatility of the package, we describe its use in three different projects at our site. 
This paper examines the resolution of the second person English pronoun you in multi-party dialogue. Following previous work, we attempt to classify instances as generic or referential, and in the latter case identify the singular or plural addressee. We show that accuracy and robustness can be improved by use of simple lexical features, capturing the intuition that different uses and addressees are associated with different vocabularies; and we show that there is an advantage to treating referentiality and addressee identiﬁcation as separate (but connected) problems. 
This paper presents a simple, yet effective model for managing attention and interaction control in multimodal spoken dialogue systems. The model allows the user to switch attention between the system and other humans, and the system to stop and resume speaking. An evaluation in a tutoring setting shows that the user’s attention can be effectively monitored using head pose tracking, and that this is a more reliable method than using push-to-talk. 
We address an issue of out-of-grammar (OOG) utterances in spoken dialogue systems by generating help messages for novice users. Help generation for OOG utterances is a challenging problem because language understanding (LU) results based on automatic speech recognition (ASR) results for such utterances are always erroneous as important words are often misrecognized or missed from such utterances. We ﬁrst develop grammar veriﬁcation for OOG utterances on the basis of a Weighted Finite-State Transducer (WFST). It robustly identiﬁes a grammar rule that a user intends to utter, even when some important words are missed from the ASR result. We then adopt a ranking algorithm, RankBoost, whose features include the grammar veriﬁcation results and the utterance history representing the user’s experience. 
 2 Data collection  For safety reasons, in-vehicle dialogue systems should be able to take the cognitive load of the driver into consideration. However, it is important to distinguish between two types of cognitive load, namely if the cognitive load is affecting the driving behaviour or not. We will present ﬁndings from a data collection carried out in the DICO project1, where the dialogue behaviour under high cognitive load is analysed, and present a novel theory of how to distinguish between different types of workload. 
In this paper, we test the applicability of a stochastic user simulation technique to generate dialogs which are similar to real human-machine spoken interactions. To do so, we present a comparison between two corpora employing a comprehensive set of evaluation measures. The ﬁrst corpus was acquired from real interactions of users with a spoken dialog system, whereas the second was generated by means of the simulation technique, which decides the next user answer taking into account the previous user turns, the last system answer and the objective of the dialog. 
Segmentation of utterances and annotation as dialogue acts can be helpful for several modules of dialogue systems. In this work, we study a statistical machine learning model to perform these tasks simultaneously using lexical features and incorporating deterministic syntactic restrictions. There is a slight improvement in both segmentation and labelling due to these restrictions. 
In recent years Dialogue Acts have become a popular means of modelling the communicative intentions of human and machine utterances in many modern dialogue systems. Many of these systems rely heavily on the availability of dialogue corpora that have been annotated with Dialogue Act labels. The manual annotation of dialogue corpora is both tedious and expensive. Consequently, there is a growing interest in unsupervised systems that are capable of automating the annotation process. This paper investigates the use of a Dirichlet Process Mixture Model as a means of clustering dialogue utterances in an unsupervised manner. These clusters can then be analysed in terms of the possible Dialogue Acts that they might represent. The results presented here are from the application of the Dirichlet Process Mixture Model to the Dihana corpus. 
We present a set of metrics describing classiﬁcation performance for individual contexts of a spoken dialog system as well as for the entire system. We show how these metrics can be used to train and tune system components and how they are related to Caller Experience, a subjective measure describing how well a caller was treated by the dialog system. 
We present a dialogue annotation scheme for both spoken and written interaction, and use it in a telephone transaction corpus and an email corpus. We train classiﬁers, comparing regular SVM and structured SVM against a heuristic baseline. We provide a novel application of structured SVM to predicting relations between instance pairs. 
We investigate several algorithms related to the parsing problem for weighted automata, under the assumption that the input is a string rather than a tree. This assumption is motivated by several natural language processing applications. We provide algorithms for the computation of parse-forests, best tree probability, inside probability (called partition function), and preﬁx probability. Our algorithms are obtained by extending to weighted tree automata the Bar-Hillel technique, as deﬁned for context-free grammars. 
We show how parsing of trees can be formalized in terms of the intersection of two tree languages. The focus is on weighted regular tree grammars and weighted tree adjoining grammars. Potential applications are discussed, such as parameter estimation across formalisms. 
We describe for dependency parsing an annotation adaptation strategy, which can automatically transfer the knowledge from a source corpus with a different annotation standard to the desired target parser, with the supervision by a target corpus annotated in the desired standard. Furthermore, instead of a hand-annotated one, a projected treebank derived from a bilingual corpus is used as the source corpus. This beneﬁts the resource-scarce languages which haven’t different handannotated treebanks. Experiments show that the target parser gains signiﬁcant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller. 
We present a biparsing algorithm for Stochastic Bracketing Inversion Transduction Grammars that runs in O(bn3) time instead of O(n6). Transduction grammars learned via an EM estimation procedure based on this biparsing algorithm are evaluated directly on the translation task, by building a phrase-based statistical MT system on top of the alignments dictated by Viterbi parses under the induced bigrammars. Translation quality at different levels of pruning are compared, showing improvements over a conventional word aligner even at heavy pruning levels. 
Empirical lower bounds studies in which the frequency of alignment conﬁgurations that cannot be induced by a particular formalism is estimated, have been important for the development of syntax-based machine translation formalisms. The formalism that has received most attention has been inversion transduction grammars (ITGs) (Wu, 1997). All previous work on the coverage of ITGs, however, concerns parse failure rates (PFRs) or sentence level coverage, which is not directly related to any of the evaluation measures used in machine translation. Søgaard and Kuhn (2009) induce lower bounds on translation unit error rates (TUERs) for a number of formalisms, incl. normal form ITGs, but not for the full class of ITGs. Many of the alignment conﬁgurations that cannot be induced by normal form ITGs can be induced by unrestricted ITGs, however. This paper estimates the difference and shows that the average reduction in lower bounds on TUER is 2.48 in absolute difference (16.01 in average parse failure rate). 
Most cellular telephones use numeric keypads, where texting is supported by dictionaries and frequency models. Given a key sequence, the entry system recognizes the matching words and proposes a rankordered list of candidates. The ranking quality is instrumental to an effective entry. This paper describes a new method to enhance entry that combines syntax and language models. We ﬁrst investigate components to improve the ranking step: language models and semantic relatedness. We then introduce a novel syntactic model to capture the word context, optimize ranking, and then reduce the number of keystrokes per character (KSPC) needed to write a text. We ﬁnally combine this model with the other components and we discuss the results. We show that our syntax-based model reaches an error reduction in KSPC of 12.4% on a Swedish corpus over a baseline using word frequencies. We also show that bigrams are superior to all the other models. However, bigrams have a memory footprint that is unﬁt for most devices. Nonetheless, bigrams can be further improved by the addition of syntactic models with an error reduction that reaches 29.4%. 
Program analysis tools used in software maintenance must be robust and ought to be accurate. Many data-driven parsing approaches developed for natural languages are robust and have quite high accuracy when applied to parsing of software. We show this for the programming languages Java, C/C++, and Python. Further studies indicate that post-processing can almost completely remove the remaining errors. Finally, the training data for instantiating the generic data-driven parser can be generated automatically for formal languages, as opposed to the manually development of treebanks for natural languages. Hence, our approach could improve the robustness of software maintenance tools, probably without showing a signiﬁcant negative effect on their accuracy. 
We present an Earley-style parser for simple range concatenation grammar, a formalism strongly equivalent to linear context-free rewriting systems. Furthermore, we present different ﬁlters which reduce the number of items in the parsing chart. An implementation shows that parses can be obtained in a reasonable time. 
We present a parsing algorithm for Interaction Grammars using the deductive parsing framework. This approach brings new perspectives to this problem, departing from previous methods which rely on constraint-solving techniques. 
Several formalisms have been proposed for modeling trees with discontinuous phrases. Some of these formalisms allow for synchronous rewriting. However, it is unclear whether synchronous rewriting is a necessary feature. This is an important question, since synchronous rewriting greatly increases parsing complexity. We present a characterization of recursive synchronous rewriting in constituent treebanks with discontinuous annotation. An empirical investigation reveals that synchronous rewriting is actually a necessary feature. Furthermore, we transfer this property to grammars extracted from treebanks. 
We present an improved training strategy for dependency parsers that use online reordering to handle non-projective trees. The new strategy improves both efﬁciency and accuracy by reducing the number of swap operations performed on non-projective trees by up to 80%. We present state-ofthe-art results for ﬁve languages with the best ever reported results for Czech. 
The paper describes the overall design of a new two stage constraint based hybrid approach to dependency parsing. We define the two stages and show how different grammatical construct are parsed at appropriate stages. This division leads to selective identification and resolution of specific dependency relations at the two stages. Furthermore, we show how the use of hard constraints and soft constraints helps us build an efficient and robust hybrid parser. Finally, we evaluate the implemented parser on Hindi and compare the results with that of two data driven dependency parsers. 
We present an efficient approach for discourse parsing within and across sentences, where the unit of processing is an entire document, and not a single sentence. We apply shift-reduce algorithms for dependency and constituent parsing to determine syntactic dependencies for the sentences in a document, and subsequently a Rhetorical Structure Theory (RST) tree for the entire document. Our results show that our linear-time shiftreduce framework achieves high accuracy and a large improvement in efficiency compared to a state-of-the-art approach based on chart parsing with dynamic programming. 
This paper presents shallow semantic parsing based only on HPSG parses. An HPSG-FrameNet map was constructed from a semantically annotated corpus, and semantic parsing was performed by mapping HPSG dependencies to FrameNet relations. The semantic parsing was evaluated in a Senseval-3 task; the results suggested that there is a high contribution of syntactic information to semantic analysis. 
We apply the idea of weight pushing (Mohri, 1997) to CKY parsing with ﬁxed context-free grammars. Applied after rule binarization, weight pushing takes the weight from the original grammar rule and pushes it down across its binarized pieces, allowing the parser to make better pruning decisions earlier in the parsing process. This process can be viewed as generalizing weight pushing from transducers to hypergraphs. We examine its effect on parsing efﬁciency with various binarization schemes applied to tree substitution grammars from previous work. We ﬁnd that weight pushing produces dramatic improvements in efﬁciency, especially with small amounts of time and with large grammars. 
We present an asymmetric approach to a run-time combination of two parsers where one component serves as a predictor to the other one. Predictions are integrated by means of weighted constraints and therefore are subject to preferential decisions. Previously, the same architecture has been successfully used with predictors providing partial or inferior information about the parsing problem. It has now been applied to a situation where the predictor produces exactly the same type of information at a fully competitive quality level. Results show that the combined system outperforms its individual components, even though their performance in isolation is already fairly high. 
We present a method for dependency and case structure analysis that captures the consistency between intra-clause relations (i.e., case structures or predicate-argument structures) and inter-clause relations. We assess intra-clause relations on the basis of case frames and inter-clause relations on the basis of transition knowledge between case frames. Both knowledge bases are automatically acquired from a massive amount of parses of a Web corpus. The signiﬁcance of this study is that the proposed method selects the best dependency and case structure that are consistent within each clause and between clauses. We conﬁrm that this method contributes to the improvement of dependency parsing of Japanese. 
This paper describes and compares two algorithms that take as input a shared PCFG parse forest and produce shared forests that contain exactly the n most likely trees of the initial forest. Such forests are suitable for subsequent processing, such as (some types of) reranking or LFG fstructure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 
We describe a newly available Hebrew Dependency Treebank, which is extracted from the Hebrew (constituency) Treebank. We establish some baseline unlabeled dependency parsing performance on Hebrew, based on two state-of-the-art parsers, MST-parser and MaltParser. The evaluation is performed both in an artiﬁcial setting, in which the data is assumed to be properly morphologically segmented and POS-tagged, and in a real-world setting, in which the parsing is performed on automatically segmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 
Generative lexicalized parsing models, which are the mainstay for probabilistic parsing of English, do not perform as well when applied to languages with different language-speciﬁc properties such as free(r) word order or rich morphology. For German and other non-English languages, linguistically motivated complex treebank transformations have been shown to improve performance within the framework of PCFG parsing, while generative lexicalized models do not seem to be as easily adaptable to these languages. In this paper, we show a practical way to use grammatical functions as ﬁrst-class citizens in a discriminative model that allows to extend annotated treebank grammars with rich feature sets without having to suffer from sparse data problems. We demonstrate the ﬂexibility of the approach by integrating unsupervised PP attachment and POS-based word clusters into the parser. 
We present a semi-supervised method to improve statistical parsing performance. We focus on the well-known problem of lexical data sparseness and present experiments of word clustering prior to parsing. We use a combination of lexiconaided morphological clustering that preserves tagging ambiguity, and unsupervised word clustering, trained on a large unannotated corpus. We apply these clusterings to the French Treebank, and we train a parser with the PCFG-LA unlexicalized algorithm of (Petrov et al., 2006). We ﬁnd a gain in French parsing performance: from a baseline of F1=86.76% to F1=87.37% using morphological clustering, and up to F1=88.29% using further unsupervised clustering. This is the best known score for French probabilistic parsing. These preliminary results are encouraging for statistically parsing morphologically rich languages, and languages with small amount of annotated data. 
This paper presents a set of experiments performed on parsing the Basque Dependency Treebank. We have applied feature propagation to dependency parsing, experimenting the propagation of several morphosyntactic feature values. In the experiments we have used the output of a parser to enrich the input of a second parser. Both parsers have been generated by Maltparser, a freely data-driven dependency parser generator. The transformations, combined with the pseudoprojective graph transformation, obtain a LAS of 77.12% improving the best reported results for Basque. 
Lexical-Functional Grammar (Kaplan and Bresnan, 1982) f-structures are bilexical labelled dependency representations. We show that the Naive Bayes classiﬁer is able to guess missing grammatical function labels (i.e. bilexical dependency labels) with reasonably high accuracy (82–91%). In the experiments we use f-structure parser output for English and German Europarl data, automatically “broken” by replacing grammatical function labels with a generic UNKNOWN label and asking the classiﬁer to restore the label. 
This paper presents preliminary investigations on the statistical parsing of French by bringing a complete evaluation on French data of the main probabilistic lexicalized and unlexicalized parsers ﬁrst designed on the Penn Treebank. We adapted the parsers on the two existing treebanks of French (Abeillé et al., 2003; Schluter and van Genabith, 2007). To our knowledge, mostly all of the results reported here are state-of-the-art for the constituent parsing of French on every available treebank. Regarding the algorithms, the comparisons show that lexicalized parsing models are outperformed by the unlexicalized Berkeley parser. Regarding the treebanks, we observe that, depending on the parsing model, a tag set with speciﬁc features has direct inﬂuence over evaluation results. We show that the adapted lexicalized parsers do not share the same sensitivity towards the amount of lexical material used for training, thus questioning the relevance of using only one lexicalized model to study the usefulness of lexicalization for the parsing of French. 
Transition-based approaches have shown competitive performance on constituent and dependency parsing of Chinese. Stateof-the-art accuracies have been achieved by a deterministic shift-reduce parsing model on parsing the Chinese Treebank 2 data (Wang et al., 2006). In this paper, we propose a global discriminative model based on the shift-reduce parsing process, combined with a beam-search decoder, obtaining competitive accuracies on CTB2. We also report the performance of the parser on CTB5 data, obtaining the highest scores in the literature for a dependencybased evaluation. 
In this paper, we propose that grammar error detection be disambiguated in generating the connected parse(s) of optimal merit for the full input utterance, in overcoming the cheapest error. The detected error(s) are described as violated grammatical constraints in a framework for ModelTheoretic Syntax (MTS). We present a parsing algorithm for MTS, which only relies on a grammar of well-formedness, in that the process does not require any extragrammatical resources, additional rules for constraint relaxation or error handling, or any recovery process. 
We parse the sentences in three parallel error corpora using a generative, probabilistic parser and compare the parse probabilities of the most likely analyses for each grammatical sentence and its closely related ungrammatical counterpart. 
In this paper, we propose two methods for analyzing errors in parsing. One is to classify errors into categories which grammar developers can easily associate with defects in grammar or a parsing model and thus its improvement. The other is to discover inter-dependencies among errors, and thus grammar developers can focus on errors which are crucial for improving the performance of a parsing model. The ﬁrst method uses patterns of errors to associate them with categories of causes for those errors, such as errors in scope determination of coordination, PPattachment, identiﬁcation of antecedent of relative clauses, etc. On the other hand, the second method, which is based on reparsing with one of observed errors corrected, assesses inter-dependencies among errors by examining which other errors were to be corrected as a result if a speciﬁc error was corrected. Experiments show that these two methods are complementary and by being combined, they can provide useful clues as to how to improve a given grammar. 
We present an approach for deriving syntactic word clusters from parsed text, grouping words according to their unlexicalized syntactic contexts. We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 
Chunking is segmenting a text into chunks, sub-sentential segments, that Abney approximately defined as stress groups. Chunking usually uses monolingual resources, most often exhaustive, sometimes partial : function words and punctuations, which often mark beginnings and ends of chunks. But, to extend this method to other languages, monolingual resources have to be multiplied. We present a new method : endogenous chunking, which uses no other resource than the text to be segmented itself. The idea of this method comes from Zipf : to make the least communication effort, speakers are driven to shorten frequent words. A chunk then can be characterized as the period of the periodic correlated functions length and frequency of words on the syntagmatic axis. This original method takes its advantage to be applied to a great number of languages of alphabetic script, with the same algorithm, without any resource. Introduction Chunking is a frequent segmentation step in many processing types : robust parsers, parsers of linear complexity (Vergne, 2000), computing stress groups and linking them in tts systems, to compute macro-prosody (Vannier et al., 1999), in automatic indexing, the chunk as another indexed grain above the word in the grain hierarchy, and in sub-sentential alignment, the chunk as an aligned grain. The method we propose is based on the properties of the functions length and frequency of words on the syntagmatic axis. These two functions are correlated : integer, periodic, synchronous, in phase opposition, and their period allows to define the chunk. On a period, the length function is non-decreasing, and the frequency function is non-increasing. These concepts con-  tinue in Zipf's direction : minimizing the communication effort drives the speaker to shorten frequent words (Zipf, 1949). The length metrics defined by Zipf is not the number of letters, but the number of syllables or the number of phonemes of the written form (Zipf, 1935); the metrics of our method is also the number of syllables, or more precisely the number of vowel nuclei, computable from the written form; this metrics takes its root into the oral origin of the chunk. The word frequency is measured in the segmented text. This method of segmentation into chunks is based on digital properties, and is valid on languages with alphabetic script. It is endogenous, as it computes on the text to be segmented and does not use any resource external to the parsed text. 
In this short paper, an off-the-shelf maximum entropy-based POS-tagger is used as a partial parser to improve the accuracy of an extremely fast linear time dependency parser that provides state-of-the-art results in multilingual unlabeled POS sequence parsing. 
Supertagging is a widely used speed-up technique for deep parsing. In another aspect, supertagging has been exploited in other NLP tasks than parsing for utilizing the rich syntactic information given by the supertags. However, the performance of supertagger is still a bottleneck for such applications. In this paper, we investigated the relationship between supertagging and parsing, not just to speed up the deep parser; We started from a sequence labeling view of HPSG supertagging, examining how well a supertagger can do when separated from parsing. Comparison of two types of supertagging model, point-wise model and sequential model, showed that the former model works competitively well despite its simplicity, which indicates the true dependency among supertag assignments is far more complex than the crude ﬁrst-order approximation made in the sequential model. We then analyzed the limitation of separated supertagging by using a CFG-ﬁlter. The results showed that big gains could be acquired by resorting to a light-weight parser. 
We present an approach for smoothing treebank-PCFG lexicons by interpolating treebank lexical parameter estimates with estimates obtained from unannotated data via the Inside-outside algorithm. The PCFG has complex lexical categories, making relative-frequency estimates from a treebank very sparse. This kind of smoothing for complex lexical categories results in improved parsing performance, with a particular advantage in identifying obligatory arguments subcategorized by verbs unseen in the treebank. 
This paper discusses the performance difference of wide-coverage parsers on small-domain speech transcripts. Two parsers (C&C CCG and RASP) are tested on the speech transcripts of two different domains (parent-child language, and picture descriptions). The performance difference between the domain-independent parsers and two domain-trained parsers (MSTParser and MEGRASP) is substantial, with a difference of at least 30 percent point in accuracy. Despite this gap, some of the grammatical relations can still be recovered reliably. 
This paper introduces a formal framework that presents a novel Interactive Predictive Parsing schema which can be operated by a user, tightly integrated into the system, to obtain error free trees. This compares to the classical two-step schema of manually post-editing the erroneus constituents produced by the parsing system. We have simulated interaction and calculated evalaution metrics, which established that an IPP system results in a high amount of effort reduction for a manual annotator compared to a two-step system. 
This paper presents a novel approach of incorporating ﬁne-grained treebanking decisions made by human annotators as discriminative features for automatic parse disambiguation. To our best knowledge, this is the ﬁrst work that exploits treebanking decisions for this task. The advantage of this approach is that use of human judgements is made. The paper presents comparative analyses of the performance of discriminative models built using treebanking decisions and state-of-the-art features. We also highlight how differently these features scale when these models are tested on out-of-domain data. We show that, features extracted using treebanking decisions are more efﬁcient, informative and robust compared to traditional features. 
We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect. 
We present a model which integrates dependency parsing with reinforcement learning based on Markov decision process. At each time step, a transition is picked up to construct the dependency tree in terms of the long-run reward. The optimal policy for choosing transitions can be found with the SARSA algorithm. In SARSA, an approximation of the stateaction function can be obtained by calculating the negative free energies for the Restricted Boltzmann Machine. The experimental results on CoNLL-X multilingual data show that the proposed model achieves comparable results with the current state-of-the-art methods. 
We propose a framework for dependency parsing based on a combination of discriminative and generative models. We use a discriminative model to obtain a kbest list of candidate parses, and subsequently rerank those candidates using a generative model. We show how this approach allows us to evaluate a variety of generative models, without needing different parser implementations. Moreover, we present empirical results that show a small improvement over state-of-the-art dependency parsing of English sentences. 
We propose a generic method to perform lexical disambiguation in lexicalized grammatical formalisms. It relies on dependency constraints between words. The soundness of the method is due to invariant properties of the parsing in a given grammar that can be computed statically from the grammar. 
Range Concatenation Grammars (RCGs) are a syntactic formalism which possesses many attractive properties. It is more powerful than Linear Context-Free Rewriting Systems, though this power is not reached to the detriment of efﬁciency since its sentences can always be parsed in polynomial time. If the input, instead of a string, is a Directed Acyclic Graph (DAG), only simple RCGs can still be parsed in polynomial time. For non-linear RCGs, this polynomial parsing time cannot be guaranteed anymore. In this paper, we show how the standard parsing algorithm can be adapted for parsing DAGs with RCGs, both in the linear (simple) and in the non-linear case. 
A common computational goal is to encapsulate the modeling of a target phenomenon within a uniﬁed and comprehensive ”engine”, which addresses a broad range of the required processing tasks. This goal is followed in common modeling of the morphological and syntactic levels of natural language, where most processing tasks are encapsulated within morphological analyzers and syntactic parsers. In this talk I suggest that computational modeling of the semantic level should also focus on encapsulating the various processing tasks within a uniﬁed module (engine). The input/output speciﬁcation of such engine (API) can be based on the textual entailment paradigm, which will be described in brief and suggested as an attractive framework for applied semantic inference. The talk will illustrate an initial proposal for the engine’s API, designed to be embedded within the prominent language processing applications. Finally, I will sketch the entailment formalism and eﬃcient inference algorithm developed at Bar-Ilan University, which illustrates a principled transformational (rather than interpretational) approach towards developing a comprehensive semantic engine. 
Supervised word sense disambiguation requires training corpora that have been tagged with word senses, and these word senses typically come from a pre-existing sense inventory. Space limitations imposed by dictionary publishers have biased the ﬁeld towards lists of discrete senses for an individual lexeme. This approach does not capture information about relatedness of individual senses. How important is this information to knowing which sense distinctions are critical for particular types of NLP applications? How much does sense relatedness aﬀect automatic word sense disambiguation performance? Recent psycholinguistic evidence seems to indicate that closely related word senses may be represented in the mental lexicon much like a single sense, whereas distantly related senses may be represented more like discrete entities. These results suggest that, for the purposes of WSD, closely related word senses can be clustered together into a more general sense with little meaning loss. This talk will describe the relatedness of verb senses and its impact on NLP applications and WSD components as well as recent psycholinguistic research results. 
The lack of large-scale corpora annotated with semantic information has been a serious bottleneck for computational semantics, slowing down not only the development of more advanced statistical methods, but also our empirical understanding of the phenomena. The creation of the Ontonotes corpus will ﬁnally bring computational semantics to the point where computational syntax was in 1993 - but in the meantime, we have come to appreciate the limitations of that methodology both theoretically and as a way of gathering judgments. In this talk, I will discuss an ongoing effort to use the ’Games with a Purpose’ methodology to create a large-scale anaphorically annotated corpus in which multiple judgments are maintained about the interpretation of each anaphoric expression - and in particular, the Phrase Detectives game: 
Comparative constructions are common in dialogue, especially in negotiative dialogue where a choice must be made between diﬀerent options, and options must be evaluated using multiple metrics. Comparatives explicitly assert a relationship between two elements along a scale, but they may also implicate positions on the scale especially if constraints on the possible values are present. Dialogue systems must often understand more from a comparative than the explicit assertion in order to understand why the comparative was uttered. In this paper we examine the pragmatic meaning of comparative constructions from a computational perspective. 
In this paper, we explore the application of inference rules for recognizing textual entailment (RTE). We start with an automatically acquired collection and then propose methods to reﬁne it and obtain more rules using a hand-crafted lexical resource. Following this, we derive a dependency-based representation from texts, which aims to provide a proper base for the inference rule application. The evaluation of our approach on the RTE data shows promising results on precision and the error analysis suggests future improvements. 
Semantic space models represent the meaning of a word as a vector in high-dimensional space. They oﬀer a framework in which the meaning representation of a word can be computed from its context, but the question remains how they support inferences. While there has been some work on paraphrase-based inferences in semantic space, it is not clear how semantic space models would support inferences involving hyponymy, like horse ran → animal moved. In this paper, we ﬁrst discuss what a point in semantic space stands for, contrasting semantic space with G¨ardenforsian conceptual space. Building on this, we propose an extension of the semantic space representation from a point to a region. We present a model for learning a region representation for word meaning in semantic space, based on the fact that points at close distance tend to represent similar meanings. We show that this model can be used to predict, with high precision, when a hyponymybased inference rule is applicable. Moving beyond paraphrase-based and hyponymy-based inference rules, we last discuss in what way semantic space models can support inferences. 
Abstract We propose a model of natural language inference which identiﬁes valid inferences by their lexical and syntactic features, without full semantic interpretation. We extend past work in natural logic, which has focused on semantic containment and monotonicity, by incorporating both semantic exclusion and implicativity. Our model decomposes an inference problem into a sequence of atomic edits linking premise to hypothesis; predicts a lexical semantic relation for each edit; propagates these relations upward through a semantic composition tree according to properties of intermediate nodes; and joins the resulting semantic relations across the edit sequence. A computational implementation of the model achieves 70% accuracy and 89% precision on the FraCaS test suite. Moreover, including this model as a component in an existing system yields signiﬁcant performance gains on the Recognizing Textual Entailment challenge. 
Abstract The literature contains a wealth of theoretical and empirical analyses of discourse marker functions in human communication. Some of these studies address the phenomenon that discourse markers are often multifunctional in a given context, but do not study this in systematic and formal ways. In this paper we show that the use of multiple dimensions in distinguishing and annotating semantic units supports a more accurate analysis of the meaning of discourse markers. We present an empirically-based analysis of the semantic functions of discourse markers in dialogue. We demonstrate that the multiple functions, which a discourse marker may have, are automatically recognizable from utterance surface-features using machine-learning techniques. 
The work reported here arises from an attempt to provide a body of simple information about diet and its eﬀect on various common medical conditions. Expressing this knowledge in natural language has a number of advantages. It also raises a number of diﬃcult issues. We will consider solutions, and partial solutions, to these issues below. 
Using the RST annotated corpus [Carlson et , al. 2003], we use simple statistics on the distribution of discourse markers or cue phrases as evidence of the three-way distinction of Contrast relations, Contrast, Antithesis and Concession, recognized in standard Rhetorical Structure Theory (RST, Mann and Thompson 1987). We also show that , however an intuitive marker of Contrast, is not actually used statistically signicantly more often in Contrast relations than in Cause-Eect relations. These results highlight the need for empirically based discourse marker identication rather than the intuitive method that is the current norm. 
Abstract We present an approach to disambiguating verb senses which diﬀer w.r.t. the inferences they allow. It combines standard ontological tools and formalisms with a formal semantic analysis and is hence more formalised and more detailed than existing lexical semantic resources like WordNet and FrameNet [Fellbaum, 1998, Baker et al., 1998]. The resource presented here implements formal semantic descriptions of verbs in the Web Ontology Language (OWL) and exploits its reasoning potential based on Description Logics (DL) for the disambiguation of verbs in context, since before the correct sense of a verb can be reliably determined, its syntactic arguments have to be disambiguated ﬁrst. We present details on this process, which is based on a mapping from the French EuroWordNet [Vossen, 1998] to SUMO [Niles and Pease, 2003]. Moreover, we focus on the selectional restrictions of verbs w.r.t. the ontological type of their arguments, as well as their representation as necessary and suﬃcient conditions in the TBox. After a DL reasoner has identiﬁed the verb sense on the basis of these conditions, we make use of the more expressive Semantic Web Rule Language to calculate the inferences that are permitted on the selected interpretation. 
Sandor.Daranyi@hb.se Chew Lim Tan Department of Computer Science National University of Singapore tancl@comp.nus.edu.sg Abstract Term selection methods typically employ a statistical measure to ﬁlter or weight terms. Term expansion for IR may also depend on statistics, or use some other, non-metric method based on a lexical resource. At the same time, a wide range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation. This paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure. This semantic order can be exploited by term weighting and term expansion methods. 
Abstract Traditionally, domain ontologies are created manually, based on human experts’ views on the classes and relations of the domain at hand. We present ongoing work on two approaches to the automatic construction of ontologies from a ﬂat database of records, and compare them to a manually constructed ontology. The latter CIDOC-CRM ontology focusses on the organisation of classes and relations. In contrast, the ﬁrst automatic method, based on machine learning, focuses on the mutual predictiveness between classes, while the second automatic method, created with the aid of Wikipedia, stresses meaningful relations between classes. The three ontologies show little overlap; their diﬀerences illustrate that a diﬀerent focus during ontology construction can lead to radically diﬀerent ontologies. We discuss the implications of these diﬀerences, and argue that the two alternative ontologies may be useful in higher-level information systems such as search engines. 
Abstract On a pilot corpus of Dutch medical encyclopedia texts, we focus on the mechanism of taxonomic inference that involves the extraction of co-hyponym terms, and the taxonomic or domain-speciﬁc lexicosemantic relation in the form of a textual hypothesis, which coordinates these. An initial set of inference elements can be acquired by syntactic and semantic alignment, additionally driven by document structure. From the terms and the related hypothesis we can harvest lexical patterns, which are linked to annotated domain concepts. The aim of the process is to learn inference patterns and apply the system to short as well as unstructured documents where fewer or no discourse-level cues are available, in order to acquire new co-hyponyms linked to their coordinating term via a speciﬁed relation. 
In this paper, we describe the state of our work on the possible derivation of ontological structures from textual analysis. We propose an approach to semi-automatic generation of domain ontologies from scratch, on the basis of heuristic rules applied to the result of a multi-layered processing of textual documents. 
We present a normalisation framework for linguistic representations and illustrate its use by normalising the Stanford Dependency graphs (SDs) produced by the Stanford parser into Labelled Stanford Dependency graphs (LSDs). The normalised representations are evaluated both on a testsuite of constructed examples and on free text. The resulting representations improve on standard Predicate/Argument structures produced by SRL by combining role labelling with the semantically oriented features of SDs. Furthermore, the proposed normalisation framework opens the way to stronger normalisation processes which should be useful in reducing the burden on inference. 
We present a maximum entropy classiﬁer that signiﬁcantly improves the accuracy of Argumentative Zoning in scientiﬁc literature. We examine the features used to achieve this result and experiment with Argumentative Zoning as a sequence tagging task, decoded with Viterbi using up to four previous classiﬁcation decisions. The result is a 23% F-score increase on the Computational Linguistics conference papers marked up by Teufel (1999). Finally, we demonstrate the performance of our system in different scientiﬁc domains by applying it to a corpus of Astronomy journal articles annotated using a modiﬁed Argumentative Zoning scheme. 
Classifying research papers into patent classification systems enables an exhaustive and effective invalidity search, prior art search, and technical trend analysis. However, it is very costly to classify research papers manually. Therefore, we have studied automatic classification of research papers into a patent classification system. To classify research papers into patent classification systems, the differences in terms used in research papers and patents should be taken into account. This is because the terms used in patents are often more abstract or creative than those used in research papers in order to widen the scope of the claims. It is also necessary to do exhaustive searches and analyses that focus on classification of research papers written in various languages. To solve these problems, we propose some classification methods using two machine translation models. When translating English research papers into Japanese, the performance of a translation model for patents is inferior to that for research papers due to the differences in terms used in research papers and patents. However, the model for patents is thought to be useful for our task because translation results by patent translation models tend to contain more patent terms than those for research papers. To confirm the effectiveness of our methods, we conducted some experiments using the data of the Patent Mining Task in the NTCIR-7 Workshop. From the experimental results, we found that our method using translation models for both research papers and patents was more effective than using a single translation model.  
The evaluation of scientific performance is gaining importance in all research disciplines. The basic process of the evaluation is peer reviewing, which is a time-consuming activity. In order to facilitate and speed up peer reviewing processes we have developed an exploratory NLP system in the field of educational sciences. The system highlights key sentences, which are supposed to reflect the most important threads of the article The highlighted sentences offer guidance on the content-level while structural elements – the title, abstract, keywords, section headings – give an orientation about the design of the argumentation in the article. The system is implemented using a discourse analysis module called concept matching applied on top of the Xerox Incremental Parser, a rule-based dependency parser. The first results are promising and indicate the directions for the future development of the system. 
Practitioners and researchers need to stay up-to-date with the latest advances in their ﬁelds, but the constant growth in the amount of literature available makes this task increasingly difﬁcult. We investigated the literature browsing task via a user requirements analysis, and identiﬁed the information needs that biomedical researchers commonly encounter in this application scenario. Our analysis reveals that a number of literature-based research tasks are preformed which can be served by both generic and contextually tailored preview summaries. Based on this study, we describe the design of an implemented literature browsing support tool which helps readers of scientiﬁc literature decide whether or not to pursue and read a cited document. We present ﬁndings from a preliminary user evaluation, suggesting that our prototype helps users make relevance judgements about cited documents. 
We introduce the ACL Anthology Network (AAN), a manually curated networked database of citations, collaborations, and summaries in the field of Computational Linguistics. We also present a number of statistics about the network including the most cited authors, the most central collaborators, as well as network statistics about the paper citation, author citation, and author collaboration networks. 
Hierarchical faceted metadata is a proven and popular approach to organizing information for navigation of information collections. More recently, digital libraries have begun to adopt faceted navigation for collections of scholarly holdings. A key impediment to further adoption is the need for the creation of subject-oriented faceted metadata. The Castanet algorithm was developed for the purpose of (semi) automated creation of such structures. This paper describes the application of Castanet to journal title content, and presents an evaluation suggesting its efﬁcacy. This is followed by a discussion of areas for future work. 
 We present FireCite, a Mozilla Firefox browser extension that helps scholars assess and manage scholarly references on the web by automatically detecting and parsing such reference strings in real-time. FireCite has two main components: 1) a reference string recognizer that has a high recall of 96%, and 2) a reference string parser that can process HTML web pages with an overall F1 of .878 and plaintext reference strings with an overall F1 of .97. In our preliminary evaluation, we presented our FireCite prototype to four academics in separate unstructured interviews. Their positive feedback gives evidence to the desirability of FireCite’s citation management capabilities. 
Scholars of Classics cite ancient texts by using abridged citations called canonical references. In the scholarly digital library, canonical references create a complex textile of links between ancient and modern sources reﬂecting the deep hypertextual nature of texts in this ﬁeld. This paper aims to demonstrate the suitability of Conditional Random Fields (CRF) for extracting this particular kind of reference from unstructured texts in order to enhance the capabilities of navigating and aggregating scholarly electronic resources. In particular, we developed a parser which recognizes word level n-grams of a text as being canonical references by using a CRF model trained with both positive and negative examples. 
This paper proposes a new method based on coreference-chains for extracting citations from research papers. To evaluate our method we created a corpus of citations comprised of citing papers for 4 cited papers. We analyze some phenomena of citations that are present in our corpus, and then evaluate our method against a cue-phrase-based technique. Our method demonstrates higher precision by 7–10%. 
Automata theory, transliteration, and machine translation (MT) have an interesting and intertwined history. 
We present DIRECTL: an online discriminative sequence prediction model that employs a many-to-many alignment between target and source. Our system incorporates input segmentation, target character prediction, and sequence modeling in a uniﬁed dynamic programming framework. Experimental results suggest that DIRECTL is able to independently discover many of the language-speciﬁc regularities in the training data. 
In this paper we present a statistical transliteration technique that is language independent. This technique uses statistical alignment models and Conditional Random Fields (CRF). Statistical alignment models maximizes the probability of the observed (source, target) word pairs using the expectation maximization algorithm and then the character level alignments are set to maximum posterior predictions of the model. CRF has efﬁcient training and decoding processes which is conditioned on both source and target languages and produces globally optimal solution. 
We use a Phrase-Based Statistical Machine Translation approach to Transliteration where the words are replaced by characters and sentences by words. We employ the standard SMT tools like GIZA++ for learning alignments and Moses for learning the phrase tables and decoding. Besides tuning the standard SMT parameters, we focus on tuning the Character Sequence Model (CSM) related parameters like order of the CSM, weight assigned to CSM during decoding and corpus used for CSM estimation. Our results show that paying sufﬁcient attention to CSM pays off in terms of increased transliteration accuracies. 
We address the issues of transliteration between Indian languages and English, especially for named entities. We use an EM algorithm to learn the alignment between the languages. We ﬁnd that there are lot of ambiguities in the rules mapping the characters in the source language to the corresponding characters in the target language. Some of these ambiguities can be handled by capturing context by learning multi-character based alignments and use of character n-gram models. We observed that a word in the source script may have actually originated from different languages. Instead of learning one model for the language pair, we propose that one may use multiple models and a classiﬁer to decide which model to use. A contribution of this work is that the models and classiﬁers are learned in a completely unsupervised manner. Using our system we were able to get quite accurate transliteration models. 
Although most of previous transliteration methods are based on a generative model, this paper presents a discriminative transliteration model using conditional random ﬁelds. We regard character(s) as a kind of label, which enables us to consider a transliteration process as a sequential labeling process. This approach has two advantages: (1) fast decoding and (2) easy implementation. Experimental results yielded competitive performance, demonstrating the feasibility of the proposed approach. 
We propose a framework for transliteration which uses (i) a word-origin detection engine (pre-processing) (ii) a CRF based transliteration engine and (iii) a re-ranking model based on lexiconlookup (post-processing). The results obtained for English-Hindi and EnglishKannada transliteration show that the preprocessing and post-processing modules improve the top-1 accuracy by 7.1%. 
 4. Substring segmentation and transliteration of source language input.  Motivated by phrase-based translation research, we present a transliteration system where characters are grouped into substrings to be mapped atomically into the target language. We show how this substring representation can be incorporated into a Conditional Random Field model that uses local context and phonemic information. 
This paper presents a transliteration system based on pair Hidden Markov Model (pair HMM) training and Weighted Finite State Transducer (WFST) techniques. Parameters used by WFSTs for transliteration generation are learned from a pair HMM. Parameters from pair-HMM training on English-Russian data sets are found to give better transliteration quality than parameters trained for WFSTs for corresponding structures. Training a pair HMM on English vowel bigrams and standard bigrams for Cyrillic Romanization, and using a few transformation rules on generated Russian transliterations to test for context improves the system’s transliteration quality. 
This paper presents a hybrid approach to English-Korean name transliteration. The base system is built on MOSES with enabled factored translation features. We expand the base system by combining with various transliteration methods including a Web-based n-best re-ranking, a dictionary-based method, and a rule-based method. Our standard run and best nonstandard run achieve 45.1 and 78.5, respectively, in top-1 accuracy. Experimental results show that expanding training data size signiﬁcantly contributes to the performance. Also we discover that the Web-based re-ranking method can be successfully applied to the English-Korean transliteration. 
We present a transliteration system that introduces minimum description length training for transliteration and combines it with discriminative modeling. We apply the proposed approach to transliteration from English to 8 non-Latin scripts, with promising results. 
We describe in detail a method for transliterating an English string to a foreign language string evaluated on ﬁve different languages, including Tamil, Hindi, Russian, Chinese, and Kannada. Our method involves deriving substring alignments from the training data and learning a weighted ﬁnite state transducer from these alignments. We deﬁne an ǫ-extension Hidden Markov Model to derive alignments between training pairs and a heuristic to extract the substring alignments. Our method involves only two tunable parameters that can be optimized on held-out data. 
In this paper we use the popular phrasebased SMT techniques for the task of machine transliteration, for English-Hindi language pair. Minimum error rate training has been used to learn the model weights. We have achieved an accuracy of 46.3% on the test set. Our results show these techniques can be successfully used for the task of machine transliteration. 
We propose an English-Chinese name transliteration system using a maximum N-gram Hidden Markov Model. To handle special challenges with alphabet-based and characterbased language pair, we apply a two-phase transliteration model by building two HMM models, one between English and Chinese Pinyin and another between Chinese Pinyin and Chinese characters. Our model improves traditional HMM by assigning the longest prior translation sequence of syllables the largest weight. In our non-standard runs, we use a Web-mining module to boost the performance by adding online popularity information of candidate translations. The entire model does not rely on any dictionaries and the probability tables are derived merely from training corpus. In participation of NEWS 2009 experiment, our model achieved 0.462 Top-1 accuracy and 0.764 Mean F-score. 
This paper explores a very basic linguistic phenomenon in multilingualism: the lexicalizations of entities are very often identical within different languages while concepts are usually lexicalized differently. Since entities are commonly referred to by proper names in natural language, we measured their distribution in the lexical overlap of the terminologies extracted from comparable corpora. Results show that the lexical overlap is mostly composed by unambiguous words, which can be regarded as anchors to bridge languages: most of terms having the same spelling refer exactly to the same entities. Thanks to this important feature of Named Entities, we developed a multilingual super sense tagging system capable to distinguish between concepts and individuals. Individuals adopted for training have been extracted both by YAGO and by a heuristic procedure. The general F1 of the English tagger is over 76%, which is in line with the state of the art on super sense tagging while augmenting the number of classes. Performances for Italian are slightly lower, while ensuring a reasonable accuracy level which is capable to show effective results for knowledge acquisition. 
This paper presents an approach to translating Chinese organization names into English based on correlative expansion. Firstly, some candidate translations are generated by using statistical translation method. And several correlative named entities for the input are retrieved from a correlative named entity list. Secondly, three kinds of expansion methods are used to generate some expanded queries. Finally, these queries are submitted to a search engine, and the refined translation results are mined and re-ranked by using the returned web pages. Experimental results show that this approach outperforms the compared system in overall translation accuracy. 
There are generally many ways to transliterate a name from one language script into another. The resulting ambiguity can make it very difficult to “untransliterate” a name by reverse engineering the process. In this paper, we present a highly successful cross-script name matching system that we developed by combining the creativity of human intuition with the power of machine learning. Our system determines whether a name in Roman script and a name in Chinese script match each other with an F-score of 96%. In addition, for name pairs that satisfy a computational test, the F-score is 98%. 
This paper focuses on the change of named entities over time and its inﬂuence on the performance of the named entity tagger. First, we analyze Japanese named entities which appear in Mainichi Newspaper articles published in 1995, 1996, 1997, 1998 and 2005. This analysis reveals that the number of named entity types and the number of named entity tokens are almost steady over time and that 70 ∼ 80% of named entity types in a certain year occur in the articles published either in its succeeding year or in its preceding year. These facts lead that 20 ∼ 30% of named entity types are replaced with new ones every year. The experiment against these texts shows that our proposing semi-supervised method which combines a small annotated corpus and a large unannotated corpus for training works robustly although the traditional supervised method is fragile against the change of name entity distribution. 
We present two techniques to reduce machine learning cost, i.e., cost of manually annotating unlabeled data, for adapting existing CRF-based named entity recognition (NER) systems to new texts or domains. We introduce the tag posterior probability as the tag confidence measure of an individual NE tag determined by the base model. Dubious tags are automatically detected as recognition errors, and regarded as targets of manual correction. Compared to entire sentence posterior probability, tag posterior probability has the advantage of minimizing system cost by focusing on those parts of the sentence that require manual correction. Using the tag confidence measure, the first technique, known as active learning, asks the editor to assign correct NE tags only to those parts that the base model could not assign tags confidently. Active learning reduces the learning cost by 66%, compared to the conventional method. As the second technique, we propose bootstrapping NER, which semiautomatically corrects dubious tags and updates its model. 
We report in this paper a novel hybrid approach for Urdu to Hindi transliteration that combines finite-state machine (FSM) based techniques with statistical word language model based approach. The output from the FSM is filtered with the word language model to produce the correct Hindi output. The main problem handled is the case of omission of diacritical marks from the input Urdu text. Our system produces the correct Hindi output even when the crucial information in the form of diacritic marks is absent. The approach improves the accuracy of the transducer-only approach from 50.7% to 79.1%. The results reported show that performance can be improved using a word language model to disambiguate the output produced by the transducer-only approach, especially when diacritic marks are not present in the Urdu input.  scripts), etc., where words are transliterated from one script to the other, irrespective of their type (noun, verb, etc., and not only proper nouns and unknown words). In this study, we will focus on Hindi/Urdu example. Hindi and Urdu are written in two mutually incomprehensible scripts, Devanagari and Urdu script – a derivative of Persio-Arabic script respectively. Hindi and Urdu are the official languages of India and the later is also the National language of Pakistan (Rahman, 2004). Table 1 gives an idea about the number of speakers of Hindi and Urdu.  Native 2nd Lang. Speaker Speaker  Total  Hindi 366  487  853  Urdu 60.29  104  164.29  Total 426.29  591  1,017.29  Source: (Grimes, 2000) all numbers are in millions  
Although direct orthographic mapping has been shown to outperform phoneme-based methods in English-to-Chinese (E2C) transliteration, it is observed that phonological context plays an important role in resolving graphemic ambiguity. In this paper, we investigate the use of surface graphemic features to approximate local phonological context for E2C. In the absence of an explicit phonemic representation of the English source names, experiments show that the previous and next character of a given English segment could effectively capture the local context affecting its expected pronunciation, and thus its rendition in Chinese. 
This paper deals with recognition of named entities in Czech texts. We present a recently released corpus of Czech sentences with manually annotated named entities, in which a rich two-level classiﬁcation scheme was used. There are around 6000 sentences in the corpus with roughly 33000 marked named entity instances. We use the data for training and evaluating a named entity recognizer based on Support Vector Machine classiﬁcation technique. The presented recognizer outperforms the results previously reported for NE recognition in Czech. 
This paper reports a voted Named Entity Recognition (NER) system with the use of appropriate unlabeled data. The proposed method is based on the classifiers such as Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM) and has been tested for Bengali. The system makes use of the language independent features in the form of different contextual and orthographic word level features along with the language dependent features extracted from the Part of Speech (POS) tagger and gazetteers. Context patterns generated from the unlabeled data using an active learning method have been used as the features in each of the classifiers. A semi-supervised method has been used to describe the measures to automatically select effective documents and sentences from unlabeled data. Finally, the models have been combined together into a final system by weighted voting technique. Experimental results show the effectiveness of the proposed approach with the overall Recall, Precision, and F-Score values of 93.81%, 92.18% and 92.98%, respectively. We have shown how the language dependent features can improve the system performance. 
The Japanese WordNet currently has 51,000 synsets with Japanese entries. In this paper, we discuss three methods of extending it: increasing the cover, linking it to examples in corpora and linking it to other resources (SUMO and GoiTaikei). In addition, we outline our plans to make it more useful by adding Japanese deﬁnition sentences to each synset. Finally, we discuss how releasing the corpus under an open license has led to the construction of interfaces in a variety of programming languages. 
This paper presents an empirical work for Vietnamese NP chunking task. We show how to build an annotation corpus of NP chunking and how discriminative sequence models are trained using the corpus. Experiment results using 5 fold cross validation test show that discriminative sequence learning are well suitable for Vietnamese chunking. In addition, by empirical experiments we show that the part of speech information contribute signiﬁcantly to the performance of there learning models. 
Lexicon is in important resource in any kind of language processing application. Corpus-based lexica have several advantages over other traditional approaches. The lexicon developed for Sinhala was based on the text obtained from a corpus of 10 million words drawn from diverse genres. The words extracted from the corpus have been labeled with parts of speech categories defined according to a novel classification proposed for Sinhala. The lexicon reports 80% coverage over unrestricted text obtained from online sources. The lexicon has been implemented in Lexical Mark up Framework. 
In this paper, two corpora of Urdu (with 110K and 120K words) tagged with different POS tagsets are used to train TnT and Tree taggers. Error analysis of both taggers is done to identify frequent confusions in tagging. Based on the analysis of tagging, and syntactic structure of Urdu, a more refined tagset is derived. The existing tagged corpora are tagged with the new tagset to develop a single corpus of 230K words and the TnT tagger is retrained. The results show improvement in tagging accuracy for individual corpora to 94.2% and also for the merged corpus to 91%. Implications of these results are discussed. 
This paper introduces a new corpus of consulting dialogues, which is designed for training a dialogue manager that can handle consulting dialogues through spontaneous interactions from the tagged dialogue corpus. We have collected 130 h of consulting dialogues in the tourist guidance domain. This paper outlines our taxonomy of dialogue act annotation that can describe two aspects of an utterances: the communicative function (speech act), and the semantic content of the utterance. We provide an overview of the Kyoto tour guide dialogue corpus and a preliminary analysis using the dialogue act tags. 
Both Inflectional and derivational morphology lead to multiple surface forms of a word. Stemming reduces these forms back to its stem or root, and is a very useful tool for many applications. There has not been any work reported on Urdu stemming. The current work develops an Urdu stemmer or Assas-Band and improves the performance using more precise affix based exception lists, instead of the conventional lexical lookup employed for developing stemmers in other languages. Testing shows an accuracy of 91.2%. Further enhancements are also suggested. 1. Introduction A stemmer extracts stem from various forms of words, for example words actor, acted, and acting all will reduce to stem act. Stemmers are very useful for a variety of applications which need to acquire root form instead of inflected or derived forms of words. This is especially true for Information Retrieval tasks, which search for the base forms, instead of inflected forms. The need of stemmers becomes even more pronounced for languages which are morphologically rich, and have a variety of inflected and derived forms. Urdu is spoken by more than a 100 million people (accessed from http://www.ethnologue.com/show_ language.asp ?code =urd). It is the national language of Pakistan and a state language of India. It is an Indo-Aryan language, and is morphologically rich. Currently there is no stemmer for Urdu, however recent work has shown that it may have much utility for a variety of applications, much wider than some other languages. Due to the morphological richness of Urdu, its application to information retrieval tasks is quite apparent. However, there are also a few other areas of application, including automatic diacritization for text to speech systems, chunking, word sense disambiguation and statistical machine translation. In most of these cases, stemming addresses the sparseness of data caused by multiple surface forms which are caused mostly by inflections, though also applicable to some derivations.  Due to urgent need for some applications, an Urdu stemmer called Assas-Band1, has been developed. The current work explains the details of Assas-Band and its enhancements using exceptions lists instead of lexical lookup methods, to improve its accuracy. Finally results are reported and discussed. 2. Literature Review Urdu is rich in both inflectional and derivational morphology. Urdu verbs inflect to show agreement for number, gender, respect and case. In addition to these factors, verbs in Urdu also have different inflections for infinitive, past, non-past, habitual and imperative forms. All these forms (twenty in total) for a regular verb are duplicated for transitive and causative (di-transitive) forms, thus giving a total of more than sixty inflected variations. Urdu nouns also show agreement for number, gender and case. In addition, they show diminutive and vocative affixation. Moreover, the nouns show derivational changes into adjectives and nouns. Adjectives show similar agreement changes for number, gender and case. A comprehensive computational analysis of Urdu morphology is given by Hussain (2004). Stemmers may be developed by using either rulebased or statistical approaches. Rule-based stemmers require prior morphological knowledge of the language, while statistical stemmers use corpus to calculate the occurrences of stems and affixes. Both rule-based and statistical stemmers have been developed for a variety of languages. A rule-based stemmer is developed for English by Krovetz (1993) using machine-readable dictionaries. Along with a dictionary, rules for inflectional and derivational morphology are defined. Due to high dependency on dictionary the systems lacks consistency (Croft and Xu 1995). In Porter Stemmer (Porter 1980) the algorithm enforces some terminating conditions of a stem. Until any of the conditions is achieved, it keeps on removing endings of the word iteratively. Thabet has proposed a stemmer that performs stemming of classical Arabic 
Machine transliteration has a number of applications in a variety of natural language processing related tasks such as machine translation, information retrieval and question-answering. For automated learning of machine transliteration, a large parallel corpus of names in two scripts is required. In this paper we present a simple yet powerful method for automatic mining of HindiEnglish names from a parallel corpus. An average 93% precision and 85% recall is achieved in mining of proper names. The method works even with a small corpus. We compare our results with Giza++ word alignment tool that yields 30% precision and 63% recall on the same corpora. We also demonstrate that this very method of name mining works for other Indian languages as well. 
The REFLEX-LCTL (Research on English and Foreign Language ExploitationLess Commonly Taught Languages) program, sponsored by the United States government, was an eﬀort in simultaneous creation of basic language resources and technologies for under-resourced languages, with the aim to enrich sparse areas in language technology resources and encourage new research. We were tasked to produce basic language resources for 8 Asian languages: Bengali, Pashto, Punjabi, Tamil, Tagalog, Thai, Urdu and Uzbek, and 5 languages from Europe and Africa, and distribute them to research and development also funded by the program. This paper will discuss the streamlined approach to language resource development we designed to support simultaneous creation of multiple resources for multiple languages. 
We present for the first time a computational model for the reduplication of the Vietnamese language. Reduplication is a popular phenomenon of Vietnamese in which reduplicative words are created by the combination of multiple syllables whose phonics are similar. We first give a systematical study of Vietnamese reduplicative words, bringing into focus clear principles for the formation of a large class of bi-syllabic reduplicative words. We then make use of optimal finite-state devices, in particular minimal sequential string-to string transducers to build a computational model for very efficient recognition and production of those words. Finally, several nice applications of this computational model are discussed. 
The performance of a corpus-based language and speech processing system depends heavily on the quantity and quality of the training corpora. Although several famous Chinese corpora have been developed, most of them are mainly written text. Even for some existing corpora that contain spoken data, the quantity is insufficient and the domain is limited. In this paper, we describe the development of Chinese conversational annotated textual corpora currently being used in the NICT/ATR speech-to-speech translation system. A total of 510K manually checked utterances provide 3.5M words of Chinese corpora. As far as we know, this is the largest conversational textual corpora in the domain of travel. A set of three parallel corpora is obtained with the corresponding pairs of Japanese and English words from which the Chinese words are translated. Evaluation experiments on these corpora were conducted by comparing the parameters of the language models, perplexities of test sets, and speech recognition performance with Japanese and English. The characteristics of the Chinese corpora, their limitations, and solutions to these limitations are analyzed and discussed. 1. Introduction In corpus-based machine translation and speech recognition, the performance of the language model depends heavily on the size and quality of the corpora. Therefore, the corpora are indispensable for these studies and applications. In recent decades, corpus development has seen rapid growth for many languages such as English, Japanese, and Chinese. For Chinese, since there are no plain delimiters among the words, the creation of a segmented and part-of-speech (POS)-tagged corpus is the initial step for most statistical language processes. Several such Chinese corpora have been developed since the 1990s. The two most typical are People’s Daily corpus (referred to as PKU), jointly developed by the Institute of Computational Linguistics of Peking University and the Fujitsu Research & Development Center [1], and  the Sinica Corpus (referred to as Sinica) developed by the Institute of Information Science and the CKIP Group in Academia Sinica of Taiwan [2]. The former is based on the People’s Daily newspaper in 1998. It! uses standard articles of news reports. The latter is a balanced corpus collected from different areas and classified according to five criteria: genre, style, mode, topic, and source. Although conversational text is also contained in this corpus, it has only 75K of utterances and the domains are limited to a few fields, such as academia and economics, and the style is mostly in address and seldom in conversation. Since the features of conversation differ from written text, especially in news articles, the development of a segmented and POS-tagged corpus of conversational language is promising work for spontaneous speech recognition and speech-to-speech translation. In the Spoken Communication Group of NICT, in order to study corpus-based speech translation technologies for the real world, a set of corpora on travel conversation has been built for Japanese, English, and Chinese [3]. These corpora are elaborately designed and constructed on the basis of the concept of variety in samples, situations, and expressions. Now these corpora have been used in the NICT speech-tospeech translation (S2ST) system [8] and other services. In this paper, we introduce our work on this Chinese corpora development and applications in S2ST speech recognition using these corpora. In Section 2, we provide a brief description of the contents of the NICT corpora, then describe how the Chinese data were obtained. In Section 3, we illustrate the specifications for the segmentation and POS tagging designed for these corpora. Here, we explain the guidelines of segmentation and POS tagging, placing particular emphasis on the features of conversation and speech recognition application. In Section 4, we outline the development procedures and explain our methods of how to get the segmented and POS-tagged data. Some statistical characteristics of the corpora will be shown here. In Section 5, evaluation experiments of speech recognition utilizing these corpora are reported by comparing the results using the same data sets of  70 Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 70–75, Suntec, Singapore, 6-7 August 2009. c 2009 ACL and AFNLP  Japanese and English. Finally, in Section 6, we discuss the performance of the corpora, the problems that remain in the corpora, and give our ideas concerning future work.  2. Current NICT Chinese Corpora on Travel Dialog Domain  At NICT, in order to deal with various conversational cases of S2ST research, several kinds of corpora were elaborately designed and constructed [3]. Table 1 gives a brief description of the data sets related to the development of the Chinese corpora. Each corpus shown in this table was collected using different methods, for different application purposes, and was categorized into different domains.  Table 1. NICT Corpora Used for Chinese Processing  Name Collecting Method  Bilingual conversation SLDB evolved by interpreters.  Bilingual conversation MAD evolved by a machine translation system.  BTEC  Text in guidebooks for overseas travelers  Uttr. 16K 19K 475K  Domain Dialogues with the front desk clerk at a hotel General dialogues on travel General dialogues on travel  The SLDB (Spoken Language Database) is a collection of transcriptions of spoken language between two people speaking different languages and mediated by a professional interpreter. In comparison, the MAD (Machine Translation Aid Dialogue) is a similar collection, but it uses our S2ST system instead of an interpreter. The BTEC (Basic Travel Expression Corpus) is a collection of Japanese sentences and their English translations written by bilingual travel experts. This corpus covers such topics related to travel as shopping, hotel or restaurant reservations, airports, lost and found, and so on. The original data of the above corpora were developed in the form of English-to-Japanese translation pairs. The Chinese versions are mainly translated from the Japanese, but a small portion of BTEC (namely, BTEC4, about 70K of utterances) was translated from English. Every sentence in these corpora has an equivalent in the other two languages, and they share a common header (ID), except for the language mark. All the data in these three languages  constitute a set of parallel corpora. The following shows examples of sentences in the three languages: Chn.: BTEC1\jpn067\03870\zh\\\\我想喝浓咖啡。 Eng.: BTEC1\jpn067\03870\en\\\\I'd like to have some strong coffee. Jap.:BTEC1\jpn067\03870\ja\\\\濃いコーヒーが飲みたい。 3. Specifications of Segmentation and Partof-Speech Tagging By mainly referring to the PKU and taking into account the characteristics of conversational data, we made our definitions for segmentation units and POS tags. Here, we explain the outlines of these definitions, then illustrate the segmentation and POS-tagging items relating to those considerations on conversations. 3.1. Guidelines of the Definitions (1) Compatibility with the PKU and Taking into account the Demand of Speech Recognition of S2ST Since the specification of segmentations and POStagging proposed by the PKU [4] has its palpability and maneuverability and is close to China’s national standard [5] on segmentation and close to the specification on POS tags recommended by the National Program of China, namely, the 973-project [6], we mainly followed PKU’s specification. We adopted the concept of “segmentation unit,” i.e., words with disyllable, trisyllable, some compound words, and some phrases were regarded as segmentation units. The morpheme character (word) was also regarded as an independent unit. However, we made some adjustments to these specifications. In the speech recognition phase of S2ST to deal with data sparseness, the word for “training” needed to be shortened. So a numeral was divided into syllabic units, while both the PKU and the Sinica took the whole number as a unit. For the same reason, the directional verbs (趋向动词), such as “上, 下，来， 去 ， 进 去 ， and 出 来 ,” which generally follow another verb and express action directions, were divided from the preceding verb. The modal auxiliary verbs (能愿动词), such as “能，想，and 要,” which often precede another verb were separated and tagged with an individual tag set. Because the numeral can be easily reunited as an integrated unit, such a processing method for numerals does not harm the translation phase of S2ST. Moreover, if the directional verb and the modal auxiliary verb can be identified, they will help the syntactic analysis and improve the translation phrase. These two kinds of verbs, together with “是 (be)” and “ 有 (have)” are more frequently used in  71  colloquial conversations than in written text, so we took them as an individual segmentation unit and assigned a POS tag to each. The special processes for these kinds of words aim at reflecting the features of spoken language and improve the performance of the S2ST system. (2) Ability for Future Expansion Although the corpora were developed for speech recognition in S2ST system, it is desirable that they can be used in other fields when necessary. This reflects in both segmentation and POS-tagging. In segmentation, the compound words with definitive suffix or prefix are divided, so they can be combined easily when necessary. In POS-tagging, the nouns and verbs are mainly further categorized into several subtags. We selected about 40 POS tags for our corpora, as shown in Table 1 in the Appendix. With such scale of tag sets, it is regarded to be suitable for language model of ASR. When necessary, it is also easy to choice an adequate tag set from it to meet the needs of other tasks. (3) Relation with the Corpora of Other Languages in NICT The original data of the corpora are in Japanese or English. It is meaningful to build connections at the morphological level among these trilingual parallel corpora at least for “named entities.” For example, we adopted the same segmentation units as in Japanese, and we subcategorized these words into personal names, organization names, location names, and drink and food names and assigned them each an individual tag. Personal names were further divided into family names and first names for Chinese, Japanese, and Western names. These subclasses are useful in language modeling, especially in the travel domain. 3.2. Some Explanations on Segmentation and POS-tagging (1) About Segmentation In our definition of a segmentation unit, words longer than 4 Hanzis (Chinese characters) were generally divided into their syntactic units. Idioms and some greeting phrases were also regarded as segmentation units. For example: “你好/，欢迎光临/，再见/，好 的 /.” Semantic information was also used to judge segmentation unit. For example: • 我/ 想/ 去/ 最/ 好/ 的/ 餐馆/ 。/ (Tell me the best restaurant around here.) • 最好/ 是/ 价钱/ 不太/ 贵/ 的/ 宾馆/ 。/ (I'd like a hotel that is not too expensive.)  For segmenting compound words with different structures, we constituted detailed items to deal with them. These structures include “coordinated (并列)， modifying ( 偏 正 ), verb-object ( 动 宾 ), subjectpredicate (主谓), and verb-complement (述补).” The main consideration for these was to divide them without changing the original meaning. For those words that have a strong ability to combine with others, we generally separated them from the others. This was due to the consideration that if it were done in another way, it would result in too many words. For example, in the verb-object ( 动 宾 ) structure, “ 买 (buy)” can combine with many nouns to get meaningful words or phrases, such as “ 买 书 (buy book), 买肉 (buy meat)，买票 (buy ticket)，and 买衣 服 (buy clothes).” We prescribed separating such active characters or words, no matter how frequently they are used in the real world, to ensure that the meaning did not change and ambiguity did not arise. So the above phrases should be separated in following forms: “买/ 书/ (buy book), 买/ 肉/ (buy meat)，买/ 票/ (buy ticket)，and 买/ 衣服 /buy clothes).” For the directional verbs, we generally separated them from their preceding verbs. For example: 我/ 可以/ 换/ 到/ 别的/ 座位/ 吗/ ？/ (Is it all right to move to another seat?) 请/ 把/ 这/ 个/ 行李箱/ 保管/ 到/ 一点钟/ 。/ (Please keep this suitcase until one o'clock.) Prefix and appendix were commonly separated from the root words. For example: 学生/ 们/ 都/ 去/ 京都/ 吗/ ？/ (Are all students going to Kyoto?) 我/ 是/ 自由/ 职业/ 者/. (I do free-lance work.) (2) About POS-Tagging The POS tag sets are shown in Table 1 in the Appendix. The POS tagging was conducted by the grammar function based on how the segmentation unit behaves in a sentence. 4. Procedure of Developing the Chinese Corpora The segmented and POS-tagged data were obtained in two steps. The first step was to get the raw segmented and POS-tagged data automatically by computer. The second was to check the raw segmented and POStagged data manually. (1) Getting Raw Segmented and POS-Tagged Data The text data were segmented and POS tagged by using the language model shown in formula (1). P(L) =αP(wi | wi-1wi-2) +(1−α)P(wi | ci )P(ci | ci-1ci-2) (1)  72  Here wi denotes the word at the ith position of a sentence, and ci stands for the class to which the word wi belongs. The class we used here is a POS-tag set, and α is set 0.9. The initial data for training the model were from the Sinica due to their balanced characteristics. The annotated data were added to the training data when producing new data. When the annotated data reached a given quantity (here, the BTEC1 was finished, and the total words in the corpora exceeded 1M), the Sinica data were not used for training. We have conducted an experiment with this model for an open test text of 510 utterances from BTEC, and the segmentation and POStagging accuracy was more than 95%. Furthermore, proper noun information was extracted from Japanese corpora and marked in the corresponding lines of the Chinese segmented and POS-tagged data. (2) Manual Annotation The manual annotations were divided into two phases. The first was a line-by-line check of the raw segmented and POS-tagged data. The second was to check the consistency. The consistency check was conducted in the following manner: • Find the candidates having differences between the manually checked data and the automatically segmented and POS-tagged data. • Pick up the candidates having a high frequency of updating in the above step, and build an inconsistency table. The candidates in this table are the main objects of the later checks. • Check the same sentences with different segmentations and POS tags. • List all words having multiple POS tags and their frequencies. Determine the infrequent ones as distrustful candidates and add them into the inconsistency tables. The released annotated data were appended with a header ID for each token (pair of word entry and POS tag) in an utterance including a start marker and end marker, shown as follows: BTEC1\jpn067\03870\zh\\\00010\||||UTT-START|||| BTEC1\jpn067\03870\zh\\\00020\我|我||我|r|||| BTEC1\jpn067\03870\zh\\\00030\想|想||想|vw|||| BTEC1\jpn067\03870\zh\\\00040\喝|喝||喝|v|||| BTEC1\jpn067\03870\zh\\\00050\浓|浓||浓|a|||| BTEC1\jpn067\03870\zh\\\00060\咖啡|咖啡||咖啡|n|||| BTEC1\jpn067\03870\zh\\\00070\。||||UTT-END|||| Table 2 shows some of the statistics for the 510K utterances in Table 1 for different languages.  Table 2. Some Statistics of Each Corpora in NICT  Chinese Japanese English  Utter. 510K 510K 510K  Ave. words /Uttr. 6.95 8.60 7.74  Words 3.50M 4.30M 3.80M  Vocab. 47.3K 45.5K 32.9K  Figure 1 shows the distributions of utterance length (words in an utterance) for 3 languages among the 510K annotated data. From Figure 1, we know that the Chinese has the fewest words in an utterance, followed by English, with the Japanese having the most.  Percentage  18% 16% 14% 12% 10% 8% 6% 4% 2% 0% 2  Distribution of Utterance Length Japanese English Chinese  4  6  8  10  12  14 >15  length(words) Figure 1. Distribution of utterance length  5. Evaluation Experiments  To verify the effectiveness of the developed Chinese textual corpora, we built a language model for speech recognition using these corpora. For comparisons with other languages, including Japanese and English, we also built language models for these two languages using the same training sets. Meanwhile, the same test set of each language was selected for speech recognition.  5.1. Data Sets for Language Models and Speech Recognitions  For simplicity, we adopted word 2-gram and word 3gram for evaluating perplexities and speech recognition performance. The training data were selected from the 510K utterances in Table 1, while the test sets were also extracted from them, but they are guaranteed not to exist in the training sets. In evaluations of perplexity, 1524 utterances (a total of three sets) were chosen as the test set. In evaluation of recognition, 510 utterances were chosen as test set. For Japanese and English, the same data sets were also chosen for comparisons.  73  70  Ratio of 2-gram Item [%]  60  Chinese  50  Japanese  40  30  English  20  10  0  
Acquisition of verb subcategorization frames is important as verbs generally take different types of relevant arguments associated with each phrase in a sentence in comparison to other parts of speech categories. This paper presents the acquisition of different subcategorization frames for a Bengali verb Kara (do). It generates compound verbs in Bengali when combined with various noun phrases. The main hypothesis here is that the subcategorization frames for a Bengali verb are same with the subcategorization frames for its equivalent English verb with an identical sense tag. Syntax plays the main role in the acquisition of Bengali verb subcategorization frames. The output frames of the Bengali verbs have been compared with the frames of the equivalent English verbs identified using a Bengali-English bilingual lexicon. The flexible ordering of different phrases, additional attachment of optional phrases in Bengali sentences make this frames acquisition task challenging. This system has demonstrated precision and recall values of 77.11% and 88.23% respectively on a test set of 100 sentences. 
We analyze a collection of 3208 reported errors of Chinese words. Among these errors, 7.2% involved rarely used character, and 98.4% were assigned common classifications of their causes by human subjects. In particular, 80% of the errors observed in the writings of middle school students were related to the pronunciations and 30% were related to the logographs of the words. We conducted experiments that shed light on using the Web-based statistics to correct the errors, and we designed a software environment for preparing test items whose authors intentionally replace correct characters with wrong ones. Experimental results show that using Web-based statistics can help us correct only about 75% of these errors. In contrast, Web-based statistics are useful for recommending incorrect characters for composing test items for “incorrect character identification” tests about 93% of the time. 
Parallel text is one of the most valuable resources for development of statistical machine translation systems and other NLP applications. However, manual translations are very costly, and the number of known parallel text is limited. Hence, our research started with creating and collecting a large amount of parallel text resources for Indonesian-English. We describe in this paper the creation of parallel corpora: ANTARA News, BPPTPANL and BTEC-ATR. In order to be useful, these resources must be available in reasonable quantities and qualities to be useful for statistical approaches to language processing. We describe problem and solution as well robust tools and annotation schema to build and process these corpora. 
This paper presents Thai syntactic resource: Thai CG treebank, a categorial approach of language resources. Since there are very few Thai syntactic resources, we designed to create treebank based on CG formalism. Thai corpus was parsed with existing CG syntactic dictionary and LALR parser. The correct parsed trees were collected as preliminary CG treebank. It consists of 50,346 trees from 27,239 utterances. Trees can be split into three grammatical types. There are 12,876 sentential trees, 13,728 noun phrasal trees, and 18,342 verb phrasal trees. There are 17,847 utterances that obtain one tree, and an average tree per an utterance is 1.85. 
This paper introduces the current result of a research work which aims to build a 5 million tagged word corpus for Mongolian. Currently, around 1 million words have been automatically tagged by developing a POS tagset and a bigram POS tagger. 
In this paper we propose a modelization of the construction of Persian noun and adjectival phrases in a phrase structure grammar. This modelization uses the Interaction Grammar (IG) formalism by taking advantage of the polarities on features and tree descriptions for the various constructions that we studied. The proposed grammar was implemented with a Metagrammar compiler named XMG. A small test suite was built and tested with a parser based on IG, called LEOPAR. The experimental results show that we could parse the phrases successfully, even the most complex ones which have various constructions in them.  adjectival phrases as the first step to build a grammar for this language. The current work covers only noun and adjectival phrases; it is only a first step toward a full coverage of Persian grammar. The grammar presented here could have been expressed in Tree Adjoining Grammar (TAG) or even in Context Free Grammar with features, but we strongly believe that the modelization of the verbal construction of Persian, which is much more complex, can benefit from advanced specificities of IG, like polarities, underspecifications and trees. 2 Previous Studies 2.1 IG for French and English  
TimeML, TimeBank, and TTK (TARSQI Project) have been playing an important role in enhancement of IE, QA, and other NLP applications. TimeML is a specification language for events and temporal expressions in text. This paper presents the problems and solutions for porting TimeML to Korean as a part of the Korean TARSQI Project. We also introduce the KTTK which is an automatic markup tool of temporal and event-denoting expressions in Korean text. 
Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons. LMF facilitates data exchange among computational linguistic resources, and also promises a convenient uniformity for future application. This study describes the design and implementation of the WordNet-LMF used to represent lexical semantics in Chinese WordNet. The compiled CWN-LMF will be released to the community for linguistic researches. 
We present the diverse research activities on Philippine languages from all over the country, with focus on the Center for Language Technologies of the College of Computer Studies, De La Salle University, Manila, where majority of the work are conducted. These projects include the formal representation of Philippine languages and the processes involving these languages. Language representation entails the manual and automatic development of language resources such as lexicons and corpora for various human languages including Philippine languages, across various forms such as text, speech and video files. Tools and applications on languages that we have worked on include morphological processes, part of speech tagging, language grammars, machine translation, sign language processing and speech systems. Future directions are also presented. 
This paper describes semi-automatic construction of Thai WordNet and the applied method for Asian wordNet. Based on the Princeton WordNet, we develop a method in generating a WordNet by using an existing bi-lingual dictionary. We align the PWN synset to a bilingual dictionary through the English equivalent and its part-of-speech (POS), automatically. Manual translation is also employed after the alignment. We also develop a webbased collaborative workbench, called KUI (Knowledge Unifying Initiator), for revising the result of synset assignment and provide a framework to create Asian WordNet via the linkage through PWN synset. 
This paper reports prototype multilingual query expansion system relying on LMF compliant lexical resources. The system is one of the deliverables of a three-year project aiming at establishing an international standard for language resources which is applicable to Asian languages. Our important contributions to ISO 24613, standard Lexical Markup Framework (LMF) include its robustness to deal with Asian languages, and its applicability to cross-lingual query tasks, as illustrated by the prototype introduced in this paper. 
This paper presents problems and solutions in developing Thai National Corpus (TNC). TNC is designed to be a comparable corpus of British National Corpus. The project aims to collect eighty million words. Since 2006, the project can now collect only fourteen million words. The data is accessible from the TNC Web. Delay in creating the TNC is mainly caused from obtaining authorization of copyright texts. Methods used for collecting data and the results are discussed. Errors during the process of encoding data and how to handle these errors will be described. 
The aim of this short paper is to present the FLaReNet Thematic Network for Language Resources and Language Technologies to the Asian Language Resources Community. Creation of a wide and committed community and of a shared policy in the field of Language Resources is essential in order to foster a substantial advancement of the field. This paper presents the background, overall objectives and methodology of work of the project, as well as a set of preliminary results. 
The paper gives an overview of some of the major primary resources and applications developed in the ﬁeld of Natural Language Processing(NLP) for the Nepali language and their prospective for building advanced NLP applications. The paper also sheds light on the approaches followed by the current applications and their coverage as well as limitations. 
As the web content becomes more accessible to the Vietnamese community across the globe, there is a need to process Vietnamese query texts properly to find relevant information. The recent deployment of a Vietnamese translation tool on a well-known search engine justifies its importance in gaining popularity with the World Wide Web. There are still problems in the translation and retrieval of Vietnamese language as its word recognition is not fully addressed. In this paper we introduce a semi-supervised approach in building a general scalable web corpus for Vietnamese using search engine to facilitate the word segmentation process. Moreover, we also propose a segmentation algorithm which recognizes effectively Out-OfVocabulary (OOV) words. The result indicates that our solution is scalable and can be applied for real time translation program and other linguistic applications. This work is here is a continuation of the work of Nguyen D. (2008). 
Word segmentation is a process to divide a sentence into meaningful units called “word unit” [ISO/DIS 24614-1]. What is a word unit is judged by principles for its internal integrity and external use constraints. A word unit’s internal structure is bound by principles of lexical integrity, unpredictability and so on in order to represent one syntactically meaningful unit. Principles for external use include language economy and frequency such that word units could be registered in a lexicon or any other storage for practical reduction of processing complexity for the further syntactic processing after word segmentation. Such principles for word segmentation are applied for Chinese, Japanese and Korean, and impacts of the standard are discussed. 
Gazetteers or entity dictionaries are important knowledge resources for solving a wide range of NLP problems, such as entity extraction. We introduce a novel method to automatically generate gazetteers from seed lists using an external knowledge resource, the Wikipedia. Unlike previous methods, our method exploits the rich content and various structural elements of Wikipedia, and does not rely on language- or domainspecific knowledge. Furthermore, applying the extended gazetteers to an entity extraction task in a scientific domain, we empirically observed a significant improvement in system accuracy when compared with those using seed gazetteers. 
Named entity recognition (NER) is used in many domains beyond the newswire text that comprises current gold-standard corpora. Recent work has used Wikipedia’s link structure to automatically generate near gold-standard annotations. Until now, these resources have only been evaluated on newswire corpora or themselves. We present the ﬁrst NER evaluation on a Wikipedia gold standard (WG) corpus. Our analysis of cross-corpus performance on WG shows that Wikipedia text may be a harder NER domain than newswire. We ﬁnd that an automatic annotation of Wikipedia has high agreement with WG and, when used as training data, outperforms newswire models by up to 7.7%. 
Wiktionary, a satellite of the Wikipedia initiative, can be seen as a potential resource for Natural Language Processing. It requires however to be processed before being used efﬁciently as an NLP resource. After describing the relevant aspects of Wiktionary for our purposes, we focus on its structural properties. Then, we describe how we extracted synonymy networks from this resource. We provide an in-depth study of these synonymy networks and compare them to those extracted from traditional resources. Finally, we describe two methods for semiautomatically improving this network by adding missing relations: (i) using a kind of semantic proximity measure; (ii) using translation relations of Wiktionary itself. Note: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 
This paper presents our work on using the graph structure of Wiktionary for synonym detection. We implement semantic relatedness metrics using both a direct measure of information ﬂow on the graph and a comparison of the list of vertices found to be “close” to a given vertex. Our algorithms, evaluated on ESL 50, TOEFL 80 and RDWP 300 data sets, perform better than or comparable to existing semantic relatedness measures. 
Wikipedia’s article contents and its category hierarchy are widely used to produce semantic resources which improve performance on tasks like text classiﬁcation and keyword extraction. The reverse – using text classiﬁcation methods for predicting the categories of Wikipedia articles – has attracted less attention so far. We propose to “return the favor” and use text classiﬁers to improve Wikipedia. This could support the emergence of a virtuous circle between the wisdom of the crowds and machine learning/NLP methods. We deﬁne the categorization of Wikipedia articles as a multi-label classiﬁcation task, describe two solutions to the task, and perform experiments that show that our approach is feasible despite the high number of labels. 
The vast majority of parser evaluation is conducted on the 1984 Wall Street Journal (WSJ). In-domain evaluation of this kind is important for system development, but gives little indication about how the parser will perform on many practical problems. Wikipedia is an interesting domain for parsing that has so far been underexplored. We present statistical parsing results that for the ﬁrst time provide information about what sort of performance a user parsing Wikipedia text can expect. We ﬁnd that the C&C parser’s standard model is 4.3% less accurate on Wikipedia text, but that a simple self-training exercise reduces the gap to 3.8%. The self-training also speeds up the parser on newswire text by 20%. 
One of the difﬁculties in using Folksonomies in computational systems is tag ambiguity: tags with multiple meanings. This paper presents a novel method for building Folksonomy tag ontologies in which the nodes are disambiguated. Our method utilizes a clustering algorithm called DSCBC, which was originally developed in Natural Language Processing (NLP), to derive committees of tags, each of which corresponds to one meaning or domain. In this work, we use Wikipedia as the external knowledge source for the domains of the tags. Using the committees, an ambiguous tag is identiﬁed as one which belongs to more than one committee. Then we apply a hierarchical agglomerative clustering algorithm to build an ontology of tags. The nodes in the derived ontology are disambiguated in that an ambiguous tag appears in several nodes in the ontology, each of which corresponds to one meaning of the tag. We evaluate the derived ontology for its ontological density (how close similar tags are placed), and its usefulness in applications, in particular for a personalized tag retrieval task. The results showed marked improvements over other approaches. 
Being expensive and time consuming, human knowledge acquisition has consistently been a major bottleneck for solving real problems. In this paper, we present a practical framework for acquiring high quality non-expert knowledge from on-demand workforce using Amazon Mechanical Turk (MTurk). We show how to apply this framework to collect large-scale human knowledge on AOL query classification in a fast and efficient fashion. Based on extensive experiments and analysis, we demonstrate how to detect low-quality labels from massive data sets and their impact on collecting high-quality knowledge. Our experimental findings also provide insight into the best practices on balancing cost and data quality for using MTurk. 
This paper reports on the ongoing work of Phrase Detectives, an attempt to create a very large anaphorically annotated text corpus. Annotated corpora of the size needed for modern computational linguistics research cannot be created by small groups of hand-annotators however the ESP game and similar games with a purpose have demonstrated how it might be possible to do this through Web collaboration. We show that this approach could be used to create large, high-quality natural language resources. 
We present ongoing work in a scalable, distributed implementation of over 200 million individual language models, each capturing a single user’s dialect in a given language (multilingual users have several models). These have a variety of practical applications, ranging from spam detection to speech recognition, and dialectometrical methods on the social graph. Users should be able to view any content in their language (even if it is spoken by a small population), and to browse our site with appropriately translated interface (automatically generated, for locales with little crowd-sourced community effort). 
Archaeological excavations in the sites of the Indus Valley civilization (2500-1900 BCE) in Pakistan and northwestern India have unearthed a large number of artifacts with inscriptions made up of hundreds of distinct signs. To date, there is no generally accepted decipherment of these sign sequences, and there have been suggestions that the signs could be non-linguistic. Here we apply complex network analysis techniques on the database of available Indus inscriptions, with the aim of detecting patterns indicative of syntactic structure in this sign system. Our results show the presence of regularities, e.g., in the segmentation trees of the sequences, that suggest the existence of a grammar underlying the construction of the sequences. 
In this study we used bipartite spectral graph partitioning to simultaneously cluster varieties and sound correspondences in Dutch dialect data. While clustering geographical varieties with respect to their pronunciation is not new, the simultaneous identiﬁcation of the sound correspondences giving rise to the geographical clustering presents a novel opportunity in dialectometry. Earlier methods aggregated sound differences and clustered on the basis of aggregate differences. The determination of the signiﬁcant sound correspondences which co-varied with cluster membership was carried out on a post hoc basis. Bipartite spectral graph clustering simultaneously seeks groups of individual sound correspondences which are associated, even while seeking groups of sites which share sound correspondences. We show that the application of this method results in clear and sensible geographical groupings and discuss the concomitant sound correspondences. 
Many tasks in NLP stand to beneﬁt from robust measures of semantic similarity for units above the level of individual words. Rich semantic resources such as WordNet provide local semantic information at the lexical level. However, effectively combining this information to compute scores for phrases or sentences is an open problem. Our algorithm aggregates local relatedness information via a random walk over a graph constructed from an underlying lexical resource. The stationary distribution of the graph walk forms a “semantic signature” that can be compared to another such distribution to get a relatedness score for texts. On a paraphrase recognition task, the algorithm achieves an 18.5% relative reduction in error rate over a vector-space baseline. We also show that the graph walk similarity between texts has complementary value as a feature for recognizing textual entailment, improving on a competitive baseline system. 
This paper presents a method for classifying Japanese polysemous verbs using an algorithm to identify overlapping nodes with more than one cluster. The algorithm is a graph-based unsupervised clustering algorithm, which combines a generalized modularity function, spectral mapping, and fuzzy clustering technique. The modularity function for measuring cluster structure is calculated based on the frequency distributions over verb frames with selectional preferences. Evaluations are made on two sets of verbs including polysemies. 
Computing semantic relatedness of natural language texts is a key component of tasks such as information retrieval and summarization, and often depends on knowledge of a broad range of real-world concepts and relationships. We address this knowledge integration issue by computing semantic relatedness using personalized PageRank (random walks) on a graph derived from Wikipedia. This paper evaluates methods for building the graph, including link selection strategies, and two methods for representing input texts as distributions over the graph nodes: one based on a dictionary lookup, the other based on Explicit Semantic Analysis. We evaluate our techniques on standard word relatedness and text similarity datasets, ﬁnding that they capture similarity information complementary to existing Wikipedia-based relatedness measures, resulting in small improvements on a stateof-the-art measure. 
Both vector space models and graph random walk models can be used to determine similarity between concepts. Noting that vectors can be regarded as local views of a graph, we directly compare vector space models and graph random walk models on standard tasks of predicting human similarity ratings, concept categorization, and semantic priming, varying the size of the dataset from which vector space and graph are extracted. 
In this paper, we address the problem of event coreference resolution as specified in the Automatic Content Extraction (ACE 1) program. In contrast to entity coreference resolution, event coreference resolution has not received great attention from researchers. In this paper, we first demonstrate the diverse scenarios of event coreference by an example. We then model event coreference resolution as a spectral graph clustering problem and evaluate the clustering algorithm on ground truth event mentions using ECM F-Measure. We obtain the ECM-F scores of 0.8363 and 0.8312 respectively by using two methods for computing coreference matrices. 
Label Propagation, a standard algorithm for semi-supervised classiﬁcation, suffers from scalability issues involving memory and computation when used with largescale graphs from real-world datasets. In this paper we approach Label Propagation as solution to a system of linear equations which can be implemented as a scalable parallel algorithm using the map-reduce framework. In addition to semi-supervised classiﬁcation, this approach to Label Propagation allows us to adapt the algorithm to make it usable for ranking on graphs and derive the theoretical connection between Label Propagation and PageRank. We provide empirical evidence to that effect using two natural language tasks – lexical relatedness and polarity induction. The version of the Label Propagation algorithm presented here scales linearly in the size of the data with a constant main memory requirement, in contrast to the quadratic cost of both in traditional approaches. 
This work shows how to construct discourse-level opinion graphs to perform a joint interpretation of opinions and discourse relations. Speciﬁcally, our opinion graphs enable us to factor in discourse information for polarity classiﬁcation, and polarity information for discourse-link classiﬁcation. This inter-dependent framework can be used to augment and improve the performance of local polarity and discourse-link classiﬁers. 
We present a graph-based model for representing the lexical cohesion of a discourse. In the graph structure, vertices correspond to the content words of a text and edges connecting pairs of words encode how closely the words are related semantically. We show that such a structure can be used to distinguish literal and non-literal usages of multi-word expressions. 
The ﬁrst task of statistical computational linguistics, or any other type of datadriven processing of language, is the extraction of counts and distributions of phenomena. This is much more difﬁcult for the type of complex structured data found in treebanks and in corpora with sophisticated annotation than for tokenized texts. Recent developments in data mining, particularly in the extraction of frequent subtrees from treebanks, offer some solutions. We have applied a modiﬁed version of the TreeMiner algorithm to a small treebank and present some promising results. 
There is also an opportunity to take advantage of repetition in comparable corpora. Repetition is very common. Standard bag-of-word models in Information Retrieval do not attempt to model discourse structure such as given/new. The ﬁrst mention in a news article (e.g., “Manuel Noriega, for-  mer President of Panama”) is different from subsequent mentions (e.g., “Noriega”). Adaptive language models were introduced in Speech Recognition to capture the fact that probabilities change or adapt. After we see the ﬁrst mention, we should expect a subsequent mention. If the ﬁrst mention has probability p, then under standard (bagof-words) independence assumptions, two mentions ought to have probability p2, but we ﬁnd the probability is actually closer to p/2. Adaptation matters more for meaningful units of text. In Japanese, words (meaningful sequences of characters) are more likely to be repeated than fragments (meaningless sequences of characters from words that happen to be adjacent). In newswire, we ﬁnd more adaptation for content words (proper nouns, technical terminology and good keywords for information retrieval), and less adaptation for function words, clichés and ordinary ﬁrst names. There is more to meaning than frequency. Content words are not only low frequency, but likely to be repeated.  
Whereas multilingual comparable corpora have been used to identify translations of words or terms, monolingual corpora can help identify paraphrases. The present work addresses paraphrases found between two different discourse types: specialized and lay texts. We therefore built comparable corpora of specialized and lay texts in order to detect equivalent lay and specialized expressions. We identiﬁed two devices used in such paraphrases: nominalizations and neo-classical compounds. The results showed that the paraphrases had a good precision and that nominalizations were indeed relevant in the context of studying the differences between specialized and lay language. Neo-classical compounds were less conclusive. This study also demonstrates that simple paraphrase acquisition methods can also work on texts with a rather small degree of similarity, once similar text segments are detected. 
Automatic assessment of the readability level (i.e., the relative linguistic complexity) of documents in a large number of languages is an important problem that can be applied to many real-world applications, such as retrieving age-appropriate search engine results for kids, constructing automatic tutoring systems, and so on. Unfortunately, existing readability labeling techniques have only been applied to a very small number of languages. In this paper, we present an extensible crosslinguistic readability framework based on the use of parallel corpora to quickly create readability software for thousands of languages, including languages for which no linguists are available to deﬁne readability rules or for which documents with readability labels are lacking to train readability models. To demonstrate our idea, we developed a system based on the proposed framework. This paper discusses the theoretical and practical issues involved in designing such a system and presents the results of an experiment conducted with the system. 
In this short paper we show how Comparable corpora can be constructed in order to analyze the notion of ’calque’. We then investigate the way comparable corpora contribute to a better linguistic analysis of the calque effect and how it can help improve error correction for non-native language productions. 
We propose using active learning for tagging extractive reference summary of lecture speech. The training process of feature-based summarization model usually requires a large amount of training data with high-quality reference summaries. Human production of such summaries is tedious, and since inter-labeler agreement is low, very unreliable. Active learning helps assuage this problem by automatically selecting a small amount of unlabeled documents for humans to hand correct. Our method chooses the unlabeled documents according to the similarity score between the document and the comparable resource—PowerPoint slides. After manual correction, the selected documents are returned to the training pool. Summarization results show an increasing learning curve of ROUGE-L F-measure, from 0.44 to 0.514, consistently higher than that of using randomly chosen training samples. Index Terms: active learning, summarization 
Statistical machine translation relies heavily on available parallel corpora, but SMT may not have the ability or intelligence to make full use of the training set. Instead of collecting more and more parallel training corpora, this paper aims to improve SMT performance by exploiting the full potential of existing parallel corpora. We first identify literally translated sentence pairs via lexical and grammatical compatibility, and then use these data to train SMT models. One experiment indicates that larger training corpora do not always lead to higher decoding performance when the added data are not literal translations. And another experiment shows that properly enlarging the contribution of literal translation can improve SMT performance significantly. 
This paper describes a new task to extract and align information networks from comparable corpora. As a case study we demonstrate the effectiveness of this task on automatically mining name translation pairs. Starting from a small set of seeds, we design a novel approach to acquire name translation pairs in a bootstrapping framework. The experimental results show this approach can generate highly accurate name translation pairs for persons, geopolitical and organization entities. 
This paper, which builds on previous studies on sentence alignment, introduces a sentence alignment method in which some sentences are used as “anchors” and a two step procedure is applied. In the first step, some lexical information such as proper names, technical terms, numbers and punctuation marks, location information and length information are used to generate anchor sentences that satisfy some conditions. In the second step, texts are divided into several segments by using the anchor sentences as boundaries, and then the sentences in each segment are aligned by using a length-based approach. By applying this segmentation technique, the method avoids complex computation and error spreading. Experimental results show that the precision of the method is 94.6% on the average for Chinese-Uyghur sentence alignment for multi-domain texts. 
In this paper we present an extension of a successful simple and effective method for extracting parallel sentences from comparable corpora and we apply it to an Arabic/English NIST system. We experiment with a new TERp ﬁlter, along with WER and TER ﬁlters. We also report a comparison of our approach with that of (Munteanu and Marcu, 2005) using exactly the same corpora and show performance gain by using much lesser data. Our approach employs an SMT system built from small amounts of parallel texts to translate the source side of the nonparallel corpus. The target side texts are used, along with other corpora, in the language model of this SMT system. We then use information retrieval techniques and simple ﬁlters to create parallel data from a comparable news corpora. We evaluate the quality of the extracted data by showing that it signiﬁcantly improves the performance of an SMT systems. 
We present in this paper the development of a specialized comparable corpora compilation tool, for which quality would be close to a manually compiled corpus. The comparability is based on three levels: domain, topic and type of discourse. Domain and topic can be ﬁltered with the keywords used through web search. But the detection of the type of discourse needs a wide linguistic analysis. The ﬁrst step of our work is to automate the detection of the type of discourse that can be found in a scientiﬁc domain (science and popular science) in French and Japanese languages. First, a contrastive stylistic analysis of the two types of discourse is done on both languages. This analysis leads to the creation of a reusable, generic and robust typology. Machine learning algorithms are then applied to the typology, using shallow parsing. We obtain good results, with an average precision of 80% and an average recall of 70% that demonstrate the efﬁciency of this typology. This classiﬁcation tool is then inserted in a corpus compilation tool which is a text collection treatment chain realized through IBM UIMA system. Starting from two specialized web documents collection in French and Japanese, this tool creates the corresponding corpus. 
This paper addresses the notion of parallel, noisy parallel and comparable corpora in the sign language research field. As it is quite a new field, the categorization of sign language corpora is not well established, and does not rely on a straightforward basis. Nevertheless, several kinds of corpora are now available and could raise interesting issues, provided that adapted tools and techniques are developed. 
Emotion computing is very important for expressive information extraction. In this paper, we provide a robust and versatile emotion annotation scheme based on cognitive emotion theories, which not only can annotate both explicit and implicit emotion expressions, but also can encode different levels of emotion information for the given emotion content. In addition, motivated by a cognitive framework, an automatic emotion annotation system is developed, and large and comparatively high-quality emotion corpora are created for emotion computing, one in Chinese and the other in English. Such an annotation system can be easily adapted for different kinds of emotion applications and be extended to other languages. 
Alternative paths to linguistic annotation, such as those utilizing games or exploiting the web users, are becoming popular in recent times owing to their very high benefit-to-cost ratios. In this paper, however, we report a case study on POS annotation for Bangla and Hindi, where we observe that reliable linguistic annotation requires not only expert annotators, but also a great deal of supervision. For our hierarchical POS annotation scheme, we find that close supervision and training is necessary at every level of the hierarchy, or equivalently, complexity of the tagset. Nevertheless, an intelligent annotation tool can significantly accelerate the annotation process and increase the inter-annotator agreement for both expert and non-expert annotators. These findings lead us to believe that reliable annotation requiring deep linguistic knowledge (e.g., POS, chunking, Treebank, semantic role labeling) requires expertise and supervision. The focus, therefore, should be on design and development of appropriate annotation tools equipped with machine learning based predictive modules that can significantly boost the productivity of the annotators.1 
In this paper, we present the results of an experiment in which we assess the usefulness of partial semi-automatic annotation for frame labeling. While we found no conclusive evidence that it can speed up human annotation, automatic pre-annotation does increase its overall quality. 
This paper explores interoperability for data represented using the Graph Annotation Framework (GrAF) (Ide and Suderman, 2007) and the data formats utilized by two general-purpose annotation systems: the General Architecture for Text Engineering (GATE) (Cunningham, 2002) and the Unstructured Information Management Architecture (UIMA). GrAF is intended to serve as a “pivot” to enable interoperability among different formats, and both GATE and UIMA are at least implicitly designed with an eye toward interoperability with other formats and tools. We describe the steps required to perform a round-trip rendering from GrAF to GATE and GrAF to UIMA CAS and back again, and outline the commonalities as well as the differences and gaps that came to light in the process. 
Given the contemporary trend to modular NLP architectures and multiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everyday’s NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences for the handling of queries on annotated corpora. 
In this paper we explain how we annotated subordinators in the Turkish Discourse Bank (TDB), an effort that started in 2007 and is still continuing. We introduce the project and describe some of the issues that were important in annotating three subordinators, namely kars¸ın, rag˘men and halde, all of which encode the coherence relation Contrast-Concession. We also describe the annotation tool. 
We present two modules for the recognition and annotation of temporal expressions and events in French texts according to the TimeML speciﬁcation language. The Temporal Expression Tagger we have developed is based on a large coverage cascade of ﬁnite state transducers and our Event Tagger on a set of simple heuristics applied over local context in a chunked text. We present results of a preliminary evaluation and compare them with those obtained by a similar system. 
PlayCoref is a concept of an on-line language game designed to acquire a substantial amount of text data with the coreference annotation. We describe in detail various aspects of the game design and discuss features that affect the quality of the annotation. 
In this paper, we report our work on automatic image annotation by combining several textual features drawn from the text surrounding the image. Evaluation of our system is performed on a dataset of images and texts collected from the web. We report our ﬁndings through comparative evaluation with two gold standard collections of manual annotations on the same dataset. 
Evaluating systems that correct errors in non-native writing is difﬁcult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that inﬂuence agreement levels in judging the usage of articles and noun number. 
We present the annotation architecture of the National Corpus of Polish and discuss problems identified in the TEI stand-off annotation system, which, in its current version, is still very much unfinished and untested, due to both technical reasons (lack of tools implementing the TEIdefined XPointer schemes) and certain problems concerning data representation. We concentrate on two features that a stand-off system should possess and that are conspicuously missing in the current TEI Guidelines. 
We present a preliminary pilot study of belief annotation and automatic tagging. Our objective is to explore semantic meaning beyond surface propositions. We aim to model people’s cognitive states, namely their beliefs as expressed through linguistic means. We model the strength of their beliefs and their (the human) degree of commitment to their utterance. We explore only the perspective of the author of a text. We classify predicates into one of three possibilities: committed belief, non committed belief, or not applicable. We proceed to manually annotate data to that end, then we build a supervised framework to test the feasibility of automatically predicting these belief states. Even though the data is relatively small, we show that automatic prediction of a belief class is a feasible task. Using syntactic features, we are able to obtain signiﬁcant improvements over a simple baseline of 23% F-measure absolute points. The best performing automatic tagging condition is where we use POS tag, word type feature AlphaNumeric, and shallow syntactic chunk information CHUNK. Our best overall performance is 53.97% F-measure. 
The goal of the presented project is to assign a structure of clauses to Czech sentences from the Prague Dependency Treebank (PDT) as a new layer of syntactic annotation, a layer of clause structure. The annotation is based on the concept of segments, linguistically motivated and easily automatically detectable units. The task of the annotators is to identify relations among segments, especially relations of super/subordination, coordination, apposition and parenthesis. Then they identify individual clauses forming complex sentences. In the pilot phase of the annotation, 2,699 sentences from PDT were annotated with respect to their sentence structure. 
In this paper we show how to exploit typographical and textual features of raw text for creating a ﬁne–grain XML Schema Markup with special focus on capturing linguistic variation in dictionaries. We use declarative programming techniques and context–free grammars implemented in PROLOG. 
Corpus annotation plays an important role in linguistic analysis and computational processing of both written and spoken language. Syntactic annotation of spoken texts becomes clearly a topic of considerable interest nowadays, driven by the desire to improve automatic speech recognition systems by incorporating syntax in the language models, or to build language understanding applications. Syntactic annotation of both written and spoken texts in the Czech Academic Corpus was created thirty years ago when no other (even annotated) corpus of spoken texts has existed. We will discuss how much relevant and inspiring this annotation is to the current frameworks of spoken text annotation. 
NLP systems that deal with large collections of text require signiﬁcant computational resources, both in terms of space and processing time. Moreover, these systems typically add new layers of linguistic information with references to another layer. The spreading of these layered annotations across different ﬁles makes them more difﬁcult to process and access the data. As the amount of input increases, so does the difﬁculty to process it. One approach is to use distributed parallel computing for solving these larger problems and save time. We propose a framework that simpliﬁes the integration of independently existing NLP tools to build language-independent NLP systems capable of creating layered annotations. Moreover, it allows the development of scalable NLP systems, that executes NLP tools in parallel, while offering an easy-to-use programming environment and a transparent handling of distributed computing problems. With this framework the execution time was decreased to 40 times less than the original one on a cluster with 80 cores. 
The present paper outlines an ongoing project of annotation of the extended nominal coreference and the bridging anaphora in the Prague Dependency Treebank. We describe the annotation scheme with respect to the linguistic classification of coreferential and bridging relations and focus also on details of the annotation process from the technical point of view. We present methods of helping the annotators – by a pre-annotation and by several useful features implemented in the annotation tool. Our method of the inter-annotator agreement is focused on the improvement of the annotation guidelines; we present results of three subsequent measurements of the agreement. 
We report on the re-annotation of selected types of named entities from the MUC7 corpus where our focus lies on recording the time it takes to annotate these entities given two basic annotation units – sentences vs. complex noun phrases. Such information may be helpful to lay the empirical foundations for the development of cost measures for annotation processes based on the investment in time for decision-making per entity mention. 
GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 
This short paper describes the use of the linguistic annotation available in parallel PropBanks (Chinese and English) for the enhancement of automatically derived word alignments. Speciﬁcally, we suggest ways to reﬁne and expand word alignments for verb-predicates by using predicate-argument structures. Evaluations demonstrate improved alignment accuracies that vary by corpus type. 
WordNet and FrameNet are widely used lexical resources, but they are very different from each other and are often used in completely different ways in NLP. In a case study in which a short passage is annotated in both frameworks, we show how the synsets and deﬁnitions of WordNet and the syntagmatic information from FrameNet can complement each other, forming a more complete representation of the lexical semantic of a text than either could alone. Close comparisons between them also suggest ways in which they can be brought into alignment. 
In this short paper, we present annotations for tagging grammatical and stylistic errors, together with attributes about the nature of the correction which are then interpreted as arguments. A decision model is introduced in order for the author to be able to decide on the best correction to make. This introduces an operational semantics for tags and related attributes. 
 In this paper we present a collaborative work between computer and social scientists resulting in the development of annotation software for conducting research and analysis in social semiotics in both multimodal and linguistic aspects. The paper describes the proposed software and discusses how this tool can contribute for development of social semiotic theory and practice.  
We present a new method for automated discovery of inconsistencies in a complex manually annotated corpora. The proposed technique is based on Apriori algorithm for mining association rules from datasets. By setting appropriate parameters to the algorithm, we were able to automatically infer highly reliable rules of annotation and subsequently we searched for records for which the inferred rules were violated. We show that the violations found by this simple technique are often caused by an annotation error. We present an evaluation of this technique on a hand-annotated corpus PDT 2.0, present the error analysis and show that in the ﬁrst 100 detected nodes 20 of them contained an annotation error. 
Today, the named entity recognition task is considered as fundamental, but it involves some speciﬁc difﬁculties in terms of annotation. Those issues led us to ask the fundamental question of what the annotators should annotate and, even more important, for which purpose. We thus identify the applications using named entity recognition and, according to the real needs of those applications, we propose to semantically deﬁne the elements to annotate. Finally, we put forward a number of methodological recommendations to ensure a coherent and reliable annotation scheme. 
A user-friendly interface to search bilingual resources is of great help to NLP developers as well as pure-linguists. Using bilingual resources is difficult for linguists who are unfamiliar with computation, which hampers capabilities of bilingual resources. NLP developers sometimes need a kind of workbench to check their resources. The online interface this paper introduces can satisfy these needs. In order to implement the interface, this research deals with how to align Korean and Japanese phrases and interpolates them into the original bilingual corpus in an automatic way. 
As part of the STATEMENT MAP project, we are constructing a Japanese corpus annotated with the semantic relations bridging facts and opinions that are necessary for online information credibility evaluation. In this paper, we identify the semantic relations essential to this task and discuss how to efﬁciently collect valid examples from Web documents by splitting complex sentences into fundamental units of meaning called “statements” and annotating relations at the statement level. We present a statement annotation scheme and examine its reliability by annotating around 1,500 pairs of statements. We are preparing the corpus for release this winter. 
The DADA system is being developed to support collaborative access to and annotation of language resources over the web. DADA implements an abstract model of annotation suitable for storing many kinds of data from a wide range of language resources. This paper describes the process of ingesting data from a corpus of Australian Sign Language (Auslan) into the DADA system. We describe the format of the RDF data used by DADA and the issues raised in converting the ELAN annotations from the corpus. 
We describe the Hindi Discourse Relation Bank project, aimed at developing a large corpus annotated with discourse relations. We adopt the lexically grounded approach of the Penn Discourse Treebank, and describe our classification of Hindi discourse connectives, our modifications to the sense classification of discourse relations, and some crosslinguistic comparisons based on some initial annotations carried out so far. 
This paper is an attempt to show that an intermediary level of analysis is an effective way for carrying out various NLP tasks for linguistically similar languages. We describe a process for developing a simple parser for doing such tasks. This parser uses a grammar driven approach to annotate dependency relations (both inter and intra chunk) at an intermediary level. Ease in identifying a particular dependency relation dictates the degree of analysis reached by the parser. To establish efficiency of the simple parser we show the improvement in its results over previous grammar driven dependency parsing approaches for Indian languages like Hindi. We also propose the possibility of usefulness of the simple parser for Indian languages that are similar in nature. 
In this paper, we present preliminary work on corpus-based anaphora resolution of discourse deixis in German. Our annotation guidelines provide linguistic tests for locating the antecedent, and for determining the semantic types of both the antecedent and the anaphor. The corpus consists of selected speaker turns from the Europarl corpus. 
This paper presents an on-going effort which aims to annotate the Wall Street Journal sections of the Penn Treebank with the help of a hand-written large-scale and wide-coverage grammar of English. In doing so, we are not only focusing on the various stages of the semi-automated annotation process we have adopted, but we are also showing that rich linguistic annotations, which can apart from syntax also incorporate semantics, ensure that the treebank is guaranteed to be a truly sharable, re-usable and multi-functional linguistic resource†. 
We present in this paper a formal and computational scheme in the perspective of broad-coverage multimodal annotation. We propose in particular to introduce the notion of annotation hypergraphs in which primary and secondary data are represented by means of the same structure. This paper addresses the question of resources and corpora for natural human-human interaction, in other words broad-coverage annotation of natural data. In this kind of study, most of domains have do be taken into consideration: prosody, pragmatics, syntax, gestures, etc. All these different domains interact in order to build an understandable message. We need then large multimodal annotated corpora of real data, precisely annotated for all domains. Building this kind of resource is a relatively new, but very active research domain, illustrated by the number of workshops (cf. (Martin, 2008)), international initiatives, such as MUMIN (Allwood, 2005), annotation tools such as NITE NXT (Carletta, 2003), Anvil (Kipp, 2001), etc. 
Two major projects in the U.S. and Europe have joined in a collaboration to work toward achieving interoperability among language resources. In the U.S., a project entitled ”Sustainable Interoperability for Language Technology” (SILT) has been funded by the National Science Foundation under the INTEROP program, and in Europe, FLaReNet Fostering Language Resources Network has been funded by the European Commission under the eContentPlus framework. This international collaborative effort involves members of the language processing community and others working in related areas to build consensus regarding the sharing of data and technologies for language resources and applications, to work towards interoperability of existing data, and, where possible, to promote standards for annotation and resource building. In addition to broad-based US and European participation, we are seeking the participation of colleagues in Asia. This presentation describing the projects and their goals will, we hope, serve to involve members of the community who may not have been aware of the effort before, in particular colleagues in Asia. 
Treebank is an important resource for both research and application of natural language processing. For Vietnamese, we still lack such kind of corpora. This paper presents up-to-date results of a project for Vietnamese treebank construction. Since Vietnamese is an isolating language and has no word delimiter, there are many ambiguities in sentence analysis. We systematically applied a lot of linguistic techniques to handle such ambiguities. Annotators are supported by automaticlabeling tools and a tree-editor tool. Raw texts are extracted from Tuoi Tre (Youth), an online Vietnamese daily newspaper. The current annotation agreement is around 90 percent. 
 (crossing arcs), or we can use a trace and coindexation.  This paper describes the simultaneous development of dependency structure and phrase structure treebanks for Hindi and Urdu, as well as a PropBank. The dependency structure and the PropBank are manually annotated, and then the phrase structure treebank is produced automatically. To ensure successful conversion the development of the guidelines for all three representations are carefully coordinated. 
Multiword Expressions (MWEs) are one of the stumbling blocks for more precise Natural Language Processing (NLP) systems. Particularly, the lack of coverage of MWEs in resources can impact negatively on the performance of tasks and applications, and can lead to loss of information or communication errors. This is especially problematic in technical domains, where a signiﬁcant portion of the vocabulary is composed of MWEs. This paper investigates the use of a statisticallydriven alignment-based approach to the identiﬁcation of MWEs in technical corpora. We look at the use of several sources of data, including parallel corpora, using English and Portuguese data from a corpus of Pediatrics, and examining how a second language can provide relevant cues for this tasks. We report results obtained by a combination of statistical measures and linguistic information, and compare these to the reported in the literature. Such an approach to the (semi-)automatic identiﬁcation of MWEs can considerably speed up lexicographic work, providing a more targeted list of MWE candidates. 
We tackle two major issues in automatic keyphrase extraction using scientiﬁc articles: candidate selection and feature engineering. To develop an efﬁcient candidate selection method, we analyze the nature and variation of keyphrases and then select candidates using regular expressions. Secondly, we re-examine the existing features broadly used for the supervised approach, exploring different ways to enhance their performance. While most other approaches are supervised, we also study the optimal features for unsupervised keyphrase extraction. Our research has shown that effective candidate selection leads to better performance as evaluation accounts for candidate coverage. Our work also attests that many of existing features are also usable in unsupervised extraction. 
We address the problem of classifying multiword expression tokens in running text. We focus our study on Verb-Noun Constructions (VNC) that vary in their idiomaticity depending on context. VNC tokens are classiﬁed as either idiomatic or literal. We present a supervised learning approach to the problem. We experiment with different features. Our approach yields the best results to date on MWE classiﬁcation combining different linguistically motivated features, the overall performance yields an F-measure of 84.58% corresponding to an Fmeasure of 89.96% for idiomaticity identiﬁcation and classiﬁcation and 62.03% for literal identiﬁcation and classiﬁcation. 
Based on a study of verb translations in the Europarl corpus, we argue that a wide range of MWE patterns can be identiﬁed in translations that exhibit a correspondence between a single lexical item in the source language and a group of lexical items in the target language. We show that these correspondences can be reliably detected on dependency-parsed, word-aligned sentences. We propose an extraction method that combines word alignment with syntactic ﬁlters and is independent of the structural pattern of the translation. 
We review lexical Association Measures (AMs) that have been employed by past work in extracting multiword expressions. Our work contributes to the understanding of these AMs by categorizing them into two groups and suggesting the use of rank equivalence to group AMs with the same ranking performance. We also examine how existing AMs can be adapted to better rank English verb particle constructions and light verb constructions. Specifically, we suggest normalizing (Pointwise) Mutual Information and using marginal frequencies to construct penalization terms. We empirically validate the effectiveness of these modified AMs in detection tasks in English, performed on the Penn Treebank, which shows significant improvement over the original AMs. 
Complex predicate is a noun, a verb, an adjective or an adverb followed by a light verb that behaves as a single unit of verb. Complex predicates (CPs) are abundantly used in Hindi and other languages of Indo Aryan family. Detecting and interpreting CPs constitute an important and somewhat a difficult task. The linguistic and statistical methods have yielded limited success in mining this data. In this paper, we present a simple method for detecting CPs of all kinds using a Hindi-English parallel corpus. A CP is hypothesized by detecting absence of the conventional meaning of the light verb in the aligned English sentence. This simple strategy exploits the fact that CP is a multiword expression with a meaning that is distinct from the meaning of the light verb. Although there are several shortcomings in the methodology, this empirical method surprisingly yields mining of CPs with an average precision of 89% and a recall of 90%. 
Multiword expressions (MWEs) have been proved useful for many natural language processing tasks. However, how to use them to improve performance of statistical machine translation (SMT) is not well studied. This paper presents a simple yet effective strategy to extract domain bilingual multiword expressions. In addition, we implement three methods to integrate bilingual MWEs to Moses, the state-ofthe-art phrase-based machine translation system. Experiments show that bilingual MWEs could improve translation performance signiﬁcantly. 
This paper proposes Japanese bottom-up named entity recognition using a twostage machine learning method. Most work has formalized Named Entity Recognition as a sequential labeling problem, in which only local information is utilized for the label estimation, and thus a long named entity consisting of several morphemes tends to be wrongly recognized. Our proposed method regards a compound noun (chunk) as a labeling unit, and ﬁrst estimates the labels of all the chunks in a phrasal unit (bunsetsu) using a machine learning method. Then, the best label assignment in the bunsetsu is determined from bottom up as the CKY parsing algorithm using a machine learning method. We conducted an experimental on CRL NE data, and achieved an F measure of 89.79, which is higher than previous work. 
The past five years have seen the emergence of robust, scalable natural language processing systems that can summarize and answer questions about online material. One key to the success of such systems is that they re-use text that appeared in the documents rather than generating new sentences from scratch. Re-using text is absolutely essential for the development of robust systems; full semantic interpretation of unrestricted text is beyond the state of the art. Better summaries and answers can be produced, however, if systems can generate new sentences from the input text, fusing relevant phrases and discarding irrelevant ones. When the underlying sources for summarization come from multiple languages, the need for text-totext generation is even more pronounced. In this invited talk I present research on query-focused summarization over a variety of sources, including news, broadcast news, talks shows and blogs. Our research combines approaches from summarization and information extraction to answer open-ended questions. Because our sources include informal genres as well as formal genres and draw from English, Arabic and Chinese, text-to-text generation is critical for improving the intelligibility of responses. In our systems, we exploit information available at question answering time to edit sentences, removing redundant and irrelevant information and correcting errors in translated sentences. 3 
We introduce a content selection method for opinion summarization based on a well-studied, formal mathematical model, the p-median clustering problem from facility location theory. Our method replaces a series of local, myopic steps to content selection with a global solution, and is designed to allow content and realization decisions to be naturally integrated. We evaluate and compare our method against an existing heuristic-based method on content selection, using human selections as a gold standard. We ﬁnd that the algorithms perform similarly, suggesting that our content selection method is robust enough to support integration with other aspects of summarization. 
In this paper, we propose a new unsupervised approach to sentence compression based on shallow linguistic processing. For that purpose, paraphrase extraction and alignment is performed over web news stories extracted automatically from the web on a daily basis to provide structured data examples to the learning process. Compression rules are then learned through the application of Inductive Logic Programming techniques. Qualitative and quantitative evaluations suggests that this is a worth following approach, which might be even improved in the future. 
In evaluation of automatic summaries, it is necessary to employ multiple topics and human-produced models in order for the assessment to be stable and reliable. However, providing multiple topics and models is costly and time-consuming. This paper examines the relation between the number of available models and topics and the correlations with human judgment obtained by automatic metrics ROUGE and BE, as well as the manual Pyramid method. Testing all these methods on the same data set, taken from the TAC 2008 Summarization track, allows us to compare and contrast the methods under different conditions. 
We investigate the problem of generating the structure of short domain independent abstracts. We apply a supervised machine learning approach trained over a set of abstracts collected from abstracting services and automatically annotated with a text analysis tool. We design a set of features for learning inspired from past research in content selection, information ordering, and rhetorical analysis for training an algorithm which then predicts the discourse structure of unseen abstracts. The proposed approach to the problem which combines local and contextual features is able to predict the local structure of the abstracts in just over 60% of the cases. 
We propose a method of revising lead sentences in a news broadcast. Unlike many other methods proposed so far, this method does not use the coreference relation of noun phrases (NPs) but rather, insertion and substitution of the phrases modifying the same head chunk in lead and other sentences. The method borrows an idea from the sentence fusion methods and is more general than those using NP coreferencing as ours includes them. We show in experiments the method was able to find semantically appropriate revisions thus demonstrating its basic feasibility. We also show that that parsing errors mainly degraded the sentential completeness such as grammaticality and redundancy. 
In this paper, we propose an event-based approach for Chinese sentence compression without using any training corpus. We enhance the linguistically-motivated heuristics by exploiting event word significance and event information density. This is shown to improve the preservation of important information and the tolerance of POS and parsing errors, which are more common in Chinese than English. The heuristics are only required to determine possibly removable constituents instead of selecting specific constituents for removal, and thus are easier to develop and port to other languages and domains. The experimental results show that around 72% of our automatic compressions are grammatically and semantically correct, preserving around 69% of the most important information on average. 
Users of Natural Language Generation systems are required to have sophisticated linguistic and sometimes even programming knowledge, which has hindered the adoption of this technology by individuals outside the computational linguistics research community. We have designed and implemented a visual environment for creating and modifying NLG templates which requires no programming ability and minimum linguistic knowledge. It allows specifying templates with any number of variables and dependencies between them. Internally, it uses SimpleNLG to provide the linguistic background knowledge. We tested the performance of our system in the context of an interactive simulation game. We describe the templates used for testing and show examples of sentences that our system generates from these templates. 
We present an automatic multi-document summarization system for Dutch based on the MEAD system. We focus on redundancy detection, an essential ingredient of multi-document summarization. We introduce a semantic overlap detection tool, which goes beyond simple string matching. Our results so far do not conﬁrm our expectation that this tool would outperform the other tested methods. 
We describe a learning-based system that creates draft reports based on observation of people preparing such reports in a target domain (conference replanning). The reports (or brieﬁngs) are based on a mix of text and event data. The latter consist of task creation and completion actions, collected from a wide variety of sources within the target environment. The report drafting system is part of a larger learningbased cognitive assistant system that improves the quality of its assistance based on an opportunity to learn from observation. The system can learn to accurately predict the brieﬁng assembly behavior and shows signiﬁcant performance improvements relative to a non-learning system, demonstrating that it’s possible to create meaningful verbal descriptions of activity from event streams. 
This work describes ﬁrst steps towards building a system that synchronously generates multimodal (textual and visual) route directions for pedestrians. We pursue a corpus-based approach for building a generation model that produces natural instructions in multiple languages. We conducted an empirical study to collect verbal route directions, and annotated the acquired texts on different levels. Here we describe the experimental setting and an analysis of the collected data. 
The GREC-MSR Task at Generation Challenges 2009 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia. Three teams submitted one system each, and we additionally created four baseline systems. Systems were tested automatically using existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in an intrinsic evaluation involving human judges. This report describes the GREC-MSR Task and the evaluation methods applied, gives brief descriptions of the participating systems, and presents the evaluation results. 
The GREC-NEG Task at Generation Challenges 2009 required participating systems to select coreference chains for all people entities mentioned in short encyclopaedic texts about people collected from Wikipedia. Three teams submitted six systems in total, and we additionally created four baseline systems. Systems were tested automatically using a range of existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in an intrinsic evaluation involving human judges. This report describes the GREC-NEG Task and the evaluation methods applied, gives brief descriptions of the participating systems, and presents the evaluation results. 
In this paper, we describe our contribution to the Generation Challenge 2009 for the tasks of generating Referring Expressions to the Main Subject References (MSR) and Named Entities Generation (NEG). To generate the referring expressions, we employ the Conditional Random Fields (CRF) learning technique due to the fact that the selection of an expression depends on the selection of the previous references. CRFs ﬁt very well to this task since they are designed for the labeling of sequences. For the MSR task, our system has a String Accuracy of 0.68 and a REG08Type Accuracy of 0.76 and for the NEG task a String Accuracy of 0.79 and REG08-Type Accuracy of 0.83. 
We present an approach to generating referring expressions in context utilizing feature selection informed by psycholinguistic research. Features suggested by studies on pronoun interpretation were used to train a classiﬁer system which determined the most appropriate selection from a list of possible references. This application demonstrates one way to help bridge the gap between computational and empirical means of reference generation. 
The GREC-MSR task is to generate appropriate references to an entity in the context of a piece of discourse longer than a sentence. In MSR ’09 run of this task, the main aim is to select the actual main subject reference (MSR) from a list of given referential expressions that is appropriate in context. We used a machine learning approach augmented with some rules to select the most appropriate referential expression. Our approach uses the training set for learning and then combines some of the rules found by observation to improve the system. 
We report on an attempt to extend a reference generation system, originally designed only for main subjects, to generate references for multiple entities in a single document. This endeavor yielded three separate systems: one utilizing the original classiﬁer, another with a retrained classiﬁer, and a third taking advantage of new data to improve the identiﬁcation of interfering antecedents. Each subsequent system improved upon the results of the previous iteration. 
This article presents the machine learning approach used by the University of Wolverhampton in the GREC-NEG’09 task. A classiﬁer based on J48 decision tree and a meta-classiﬁer were used to produce two runs. Evaluation on the development set shows that the metaclassiﬁer achieves a better performance. 
Restricted domains such as medicine set a context where question-answering is more likely expected to be associated with knowledge and reasoning (Mollá and Vicedo, 2007; Ferret and Zweigenbaum, 2007). On the one hand, knowledge and reasoning may be more necessary than in open-domain question-answering because of more speciﬁc or more difﬁcult questions. On the other hand, it may also be more manageable, since by definition restricted-domain QA should not have to face the same breadth of questions as open-domain QA. It is therefore interesting to study the role of knowledge and reasoning in restricted-domain question-answering systems. We shall do so in the case of the (bio-)medical domain, which has a long tradition of investigating knowledge representation and reasoning and, more generally, artiﬁcial intelligence methods (Shortliffe et al., 1975), and which has seen a growing interest in question-answering systems (Zweigenbaum, 2003; Yu et al., 2005; DemnerFushman and Lin, 2007; Zweigenbaum et al., 2007). 
In this paper, we propose the development of the Question-Answering Services System for the Farmer, through SMS, by focusing on query analysis and annotation based on a similar technique previously applied to language generation, thematic roles, and primitive systems of the Lexical Conceptual Structure (LCS). The annotation places emphasis on the semantics model of “What” and “How” queries, lexical inference identification, and semantic role, for the answer. Finally, we show how these annotations and inference rules contribute to the generalization of the matching system over semantic categories in order to have a large scale question-answering system. 
We propose an open-domain question answering system using Thai Wikipedia as the knowledge base. Two types of information are used for answering a question: (1) structured information extracted and stored in the form of Resource Description Framework (RDF), and (2) unstructured texts stored as a search index. For the structured information, SPARQL transformed query is applied to retrieve a short answer from the RDF base. For the unstructured information, keyword-based query is used to retrieve the shortest text span containing the questions’s key terms. From the experimental results, the system which integrates both approaches could achieve an average MRR of 0.47 based on 215 test questions. 
Comparative and evaluative question answering (QA) requires a detailed semantic analysis of comparative expressions and complex processing. Semantics of predicates from questions have to be translated to quantiﬁable criteria before extraction of information can be done. This paper presents some challenges faced in answering comparative and evaluative questions. An application on the domain of business intelligence is discussed. 
In this document, we illustrate how complex questions such as procedural (how-to) ones can be addressed in an interactive format by means of a spoken dialogue system. The advantages of interactivity and in particular of spoken dialogue with respect to standard Question Answering settings are numerous. First, addressing user needs that do not necessarily arise in front of a computer; moreover, a spoken or multimodal answer format can often be better suited to the user’s need. Finally, the procedural nature of the information itself makes iterative question formulation and answer production particularly appealing. 
LTAG-spinal is a novel variant of traditional Lexicalized Tree Adjoining Grammar (LTAG) introduced by (Shen, 2006). The LTAG-spinal Treebank (Shen et al., 2008) combines elementary trees extracted from the Penn Treebank with Propbank annotation. In this paper, we present a semantic role labeling (SRL) system based on this new resource and provide an experimental comparison with CCGBank and a state-of-the-art SRL system based on Treebank phrase-structure trees. Deep linguistic information such as predicateargument relationships that are either implicit or absent from the original Penn Treebank are made explicit and accessible in the LTAG-spinal Treebank, which we show to be a useful resource for semantic role labeling. 
This paper reports on the development of a core semantics for German which was implemented on the basis of an English semantics that converts LFG f-structures to ﬂat meaning representations in a Neo-Davidsonian style. Thanks to the parallel design of the broad-coverage LFG grammars written in the context of the ParGram project (Butt et al., 2002) and the general surface independence of LFG f-structure analyses, the development process was substantially facilitated. We also discuss the overall architecture of the semantic conversion system from a crosslinguistic, theoretical perspective. 
The availability of large parsed corpora and improved computing resources now make it possible to extract vast amounts of lexical data. We describe the process of extracting structured data and several methods of deriving argument structure mappings for deverbal nouns that signiﬁcantly improves upon non-lexicalized rule-based methods. For a typical model, the F-measure of performance improves from a baseline of about 0.72 to 0.81. 
In this paper I shall present a treatment of lexical and grammatical tone and vowel length in Hausa, as implemented in an emerging bidirectional HPSG of the language based on the Lingo Grammar Matrix (Bender et al., 2002). I shall argue in particular that a systematic treatment of suprasegmental phonology is indispensible in an implemented grammar of the language, both for theoretical and practical reasons. I shall propose an LKB representation that is strongly inspired by linguistic and computational work on Autosegmental Phonology. Finally, I shall show that the speciﬁc implementation presented here is ﬂexible enough to accommodate different levels of suprasegmental information in the input. 
Grammar extraction in deep formalisms has received remarkable attention in recent years. We recognise its value, but try to create a more precision-oriented grammar, by hand-crafting a core grammar, and learning lexical types and lexical items from a treebank. The study we performed focused on German, and we used the Tiger treebank as our resource. A completely hand-written grammar in the framework of HPSG forms the inspiration for our core grammar, and is also our frame of reference for evaluation. 1 
This paper presents an argument against modularizing linguistic information in natural language generation systems. We argue that complex linguistic constructions require grammatical information to be located in the same module, in order to avoid over-complicating the system architecture. We demonstrate this point by showing how parenthetical constructions — which have only been generated in previous systems using an aggregation or revision module — can be generated by a surface realizer when using an integrated grammar. 
This paper reports on guiding parser development by extracting information from output of a large-scale parser applied to Wikipedia documents. Data-driven parser improvement is especially important for applications where the corpus may differ from that originally used to develop the core grammar and where efﬁciency concerns affect whether a new construction should be added, or existing analyses modiﬁed. The large size of the corpus in question also brings scalability concerns to the foreground. 
Error mining is a useful technique for identifying forms that cause incomplete parses of sentences. We extend the iterative method of Sagot and de la Clergerie (2006) to treat n-grams of an arbitrary length. An inherent problem of incorporating longer n-grams is data sparseness. Our new method takes sparseness into account, producing n-grams that are as long as necessary to identify problematic forms, but not longer. Not every cause for parsing errors can be captured effectively by looking at word n-grams. We report on an algorithm for building more general patterns for mining, consisting of words and part of speech tags. It is not easy to evaluate the various error mining techniques. We propose a new evaluation metric which will enable us to compare different error miners. 
Multi-word expressions (MWE) have seen much attention from the NLP community. In this paper, we investigate their impact on the recognition of textual entailment (RTE). Using the manual Microsoft Research annotations, we ﬁrst manually count and classify MWEs in RTE data. We ﬁnd few, most of which are arguably unlikely to cause processing problems. We then consider the impact of MWEs on a current RTE system. We are unable to conﬁrm that entailment recognition suffers from wrongly aligned MWEs. In addition, MWE alignment is difﬁcult to improve, since MWEs are poorly represented in state-of-the-art paraphrase resources, the only available sources for multi-word similarities. We conclude that RTE should concentrate on other phenomena impacting entailment, and that paraphrase knowledge is best understood as capturing general lexico-syntactic variation. 
We outline problems with the interpretation of accuracy in the presence of bias, arguing that the issue is a particularly pressing concern for RTE evaluation. Furthermore, we argue that average precision scores are unsuitable for RTE, and should not be reported. We advocate mutual information as a new evaluation measure that should be reported in addition to accuracy and conﬁdence-weighted score. 
The ability to generate or to recognize paraphrases is key to the vast majority of NLP applications. As correctly exploiting context during translation has been shown to be successful, using context information for paraphrasing could also lead to improved performance. In this article, we adopt the pivot approach based on parallel multilingual corpora proposed by (Bannard and Callison-Burch, 2005), which ﬁnds short paraphrases by ﬁnding appropriate pivot phrases in one or several auxiliary languages and back-translating these pivot phrases into the original language. We show how context can be exploited both when attempting to ﬁnd pivot phrases, and when looking for the most appropriate paraphrase in the original subsentential “envelope”. This framework allows the use of paraphrasing units ranging from words to large sub-sentential fragments for which context information from the sentence can be successfully exploited. We report experiments on a text revision task, and show that in these experiments our contextual sub-sentential paraphrasing system outperforms a strong baseline system. 
WordNet is a useful resource for lexical inference in applications. Inference over predicates, however, often requires a change in argument positions, which is not speciﬁed in WordNet. We propose a novel framework for augmenting WordNet-based inferences over predicates with corresponding argument mappings. We further present a concrete implementation of this framework, which yields substantial improvement to WordNet-based inference. 
This paper introduces a new method to improve tree edit distance approach to textual entailment recognition, using particle swarm optimization. Currently, one of the main constraints of recognizing textual entailment using tree edit distance is to tune the cost of edit operations, which is a difﬁcult and challenging task in dealing with the entailment problem and datasets. We tried to estimate the cost of edit operations in tree edit distance algorithm automatically, in order to improve the results for textual entailment. Automatically estimating the optimal values of the cost operations over all RTE development datasets, we proved a signiﬁcant enhancement in accuracy obtained on the test sets. 
This paper presents an approach for building a corpus for the domain of motion and spatial inference using a speciﬁc class of verbs. The approach creates a distribution of inference features that maximize the discriminatory power of a system trained on the corpus. The paper addresses the issue of using an existing textual inference system for generating the examples. This enables the corpus annotation method to assert whether more data is necessary. 
Within the task of Recognizing Textual Entailment, various existing work has proposed the idea that tackling speciﬁc subtypes of entailment could be more productive than taking a generic approach to entailment. In this paper we look at one such subtype, where the entailment involves hypernymy relations, often found in Question Answering tasks. We investigate current work on hypernymy acquisition, and show that adapting one such approach leads to a marked improvement in entailment classiﬁcation accuracy. 
c-rater is Educational Testing Service’s technology for the content scoring of short student responses. A major step in the scoring process is Model Building where variants of model answers are generated that correspond to the rubric for each item or test question. Until recently, Model Building was knowledge-engineered (KE) and hence labor and time intensive. In this paper, we describe our approach to automating Model Building in c-rater. We show that c-rater achieves comparable accuracy on automatically built and KE models. 
Previous work has presented an accurate natural logic model for natural language inference. Other work has demonstrated the effectiveness of computing presuppositions for solving natural language inference problems. We extend this work to create a system for correctly computing lexical presuppositions and their interactions within the natural logic framework. The combination allows our system to properly handle presupposition projection from the lexical to the sentential level while taking advantage of the accuracy and coverage of the natural logic system. To solve an inference problem, our system computes a sequence of edits from premise to hypothesis. For each edit the system computes an entailment relation and a presupposition entailment relation. The relations are then separately composed according to a syntactic tree and the semantic properties of its nodes. Presuppositions are projected based on the properties of their syntactic and semantic environment. The edits are then composed and the resulting entailment relations are combined with the presupposition relation to yield an answer to the inference problem. 
We present a pilot study of word-sense annotation using multiple annotators, relatively polysemous words, and a heterogenous corpus. Annotators selected senses for words in context, using an annotation interface that presented WordNet senses. Interannotator agreement (IA) results show that annotators agree well or not, depending primarily on the individual words and their general usage properties. Our focus is on identifying systematic differences across words and annotators that can account for IA variation. We identify three lexical use factors: semantic speciﬁcity of the context, sense concreteness, and similarity of senses. We discuss systematic differences in sense selection across annotators, and present the use of association rules to mine the data for systematic differences across annotators. 
We reﬁne the most frequent sense baseline for word sense disambiguation using a number of novel word sense disambiguation techniques. Evaluating on the S-3 English all words task, our combined system focuses on improving every stage of word sense disambiguation: starting with the lemmatization and part of speech tags used, through the accuracy of the most frequent sense baseline, to highly targeted individual systems. Our supervised systems include a ranking algorithm and a Wikipedia similarity measure. 
We revisit the one sense per discourse hypothesis of Gale et al. in the context of machine translation. Since a given sense can be lexicalized differently in translation, do we observe one translation per discourse? Analysis of manual translations reveals that the hypothesis still holds when using translations in parallel text as sense annotation, thus conﬁrming that translational differences represent useful sense distinctions. Analysis of Statistical Machine Translation (SMT) output showed that despite ignoring document structure, the one translation per discourse hypothesis is strongly supported in part because of the low variability in SMT lexical choice. More interestingly, cases where the hypothesis does not hold can reveal lexical choice errors. A preliminary study showed that enforcing the one translation per discourse constraint in SMT can potentially improve translation quality, and that SMT systems might beneﬁt from translating sentences within their entire document context. 
This research examines a word sense disambiguation method using selectors acquired from the Web. Selectors describe words which may take the place of another given word within its local context. Work in using Web selectors for noun sense disambiguation is generalized into the disambiguation of verbs, adverbs, and adjectives as well. Additionally, this work incorporates previously ignored adverb context selectors and explores the effectiveness of each type of context selector according to its part of speech. Overall results for verb, adjective, and adverb disambiguation are well above a random baseline and slightly below the most frequent sense baseline, a point which noun sense disambiguation overcomes. Our experiments ﬁnd that, for noun and verb sense disambiguation tasks, each type of context selector may assist target selectors in disambiguation. Finally, these experiments also help to draw insights about the future direction of similar research. 
We introduce a large-scale semantic-network annotation effort based on the MutliNet formalism. Annotation is achieved via a process which incorporates several independent tools including a MultiNet graph editing tool, a semantic concept lexicon, a user-editable knowledge-base for semantic concepts, and a MultiNet parser. We present an evaluation metric for these semantic networks, allowing us to determine the quality of annotations in terms of inter-annotator agreement. We use this metric to report the agreement rates for a pilot annotation effort involving three annotators. 
 2 Background  Despite early intuitions, semantic similarity has not proven to be robust for splitting multiparty interactions into separate conversations. We discuss some initial successes with using thesaural headwords to abstract the semantics of an utterance. This simple proﬁling technique showed improvements over baseline conversation threading models. 
Temporal expressions are one of the important structures in natural language. In order to understand text, temporal expressions have to be identified and normalized by providing ISObased values. In this paper we present a shallow approach for automatic recognition of temporal expressions based on a supervised machine learning approach trained on an annotated corpus for temporal information, namely TimeBank. Our experiments demonstrate a performance level comparable to a rule-based implementation and achieve the scores of 0.872, 0.836 and 0.852 for precision, recall and F1-measure for the detection task respectively, and 0.866, 0.796, 0.828 when an exact match is required. 
This paper investigates methods for using lexical patterns in a corpus to deduce the semantic relation that holds between two nouns in a noun-noun compound phrase such as “flu virus” or “morning exercise”. Much of the previous work in this area has used automated queries to commercial web search engines. In our experiments we use the Google Web 1T corpus. This corpus contains every 2,3, 4 and 5 gram occurring more than 40 times in Google's index of the web, but has the advantage of being available to researchers directly rather than through a web interface. This paper evaluates the performance of the Web 1T corpus on the task compared to similar systems in the literature, and also investigates what kind of lexical patterns are most informative when trying to identify a semantic relation between two nouns. 
In this paper we describe the ﬁrst evaluation contest (track) for Portuguese whose goal was to detect and classify relations between named entities in running text, called ReRelEM. Given a collection annotated with named entities belonging to ten different semantic categories, we marked all relationships between them within each document. We used the following fourfold relationship classiﬁcation: identity, included-in, located-in, and other (which was later on explicitly detailed into twenty different relations). We provide a quantitative description of this evaluation resource, as well as describe the evaluation architecture and summarize the results of the participating systems in the track. 
The task to classify a temporal relation between temporal entities has proven to be difﬁcult with unsatisfactory results of previous research. In TempEval07 that was a ﬁrst attempt to standardize the task, six teams competed with each other for three simple relationidentiﬁcation tasks and their results were comparably poor. In this paper we provide an analysis of the TempEval07 competition results, identifying aspects of the tasks which presented the systems with particular challenges and those that were accomplished with relative ease. 
We present GLARF, a framework for representing three linguistic levels and systems for generating this representation. We focus on a logical level, like LFG’s F-structure, but compatible with Penn Treebanks. While less ﬁnegrained than typical semantic role labeling approaches, our logical structure has several advantages: (1) it includes all words in all sentences, regardless of part of speech or semantic domain; and (2) it is easier to produce accurately. Our systems achieve 90% for English/Japanese News and 74.5% for Chinese News – these F-scores are nearly the same as those achieved for treebank-based parsing. 
A key concern in building syntax-based machine translation systems is how to improve coverage by incorporating more traditional phrase-based SMT phrase pairs that do not correspond to syntactic constituents. At the same time, it is desirable to include as much syntactic information in the system as possible in order to carry out linguistically motivated reordering, for example. We apply an extended and modiﬁed version of the approach of Tinsley et al. (2007), extracting syntax-based phrase pairs from a large parallel parsed corpus, combining them with PBSMT phrases, and performing joint decoding in a syntax-based MT framework without loss of translation quality. This effectively addresses the low coverage of purely syntactic MT without discarding syntactic information. Further, we show the potential for improved translation results with the inclusion of a syntactic grammar. We also introduce a new syntaxprioritized technique for combining syntactic and non-syntactic phrases that reduces overall phrase table size and decoding time by 61%, with only a minimal drop in automatic translation metric scores. 
When aligning very different language pairs, the most important needs are the use of structural information and the capability of generating one-to-many or many-to-many correspondences. In this paper, we propose a novel phrase alignment method which models word or phrase dependency relations in dependency tree structures of source and target languages. The dependency relation model is a kind of tree-based reordering model, and can handle non-local reorderings which sequential word-based models often cannot handle properly. The model is also capable of estimating phrase correspondences automatically without any heuristic rules. Experimental results of alignment show that our model could achieve F-measure 1.7 points higher than the conventional word alignment model with symmetrization algorithms. 
The empirical adequacy of synchronous context-free grammars of rank two (2-SCFGs) (Satta and Peserico, 2005), used in syntaxbased machine translation systems such as Wu (1997), Zhang et al. (2006) and Chiang (2007), in terms of what alignments they induce, has been discussed in Wu (1997) and Wellington et al. (2006), but with a one-sided focus on so-called “inside-out alignments”. Other alignment conﬁgurations that cannot be induced by 2-SCFGs are identiﬁed in this paper, and their frequencies across a wide collection of hand-aligned parallel corpora are examined. Empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in Och and Ney (2000) and one where only complete translation units are considered, are derived for 2-SCFGs and related formalisms. 
We argue that learning word alignments through a compositionally-structured, joint process yields higher phrase-based translation accuracy than the conventional heuristic of intersecting conditional models. Flawed word alignments can lead to flawed phrase translations that damage translation accuracy. Yet the IBM word alignments usually used today are known to be flawed, in large part because IBM models (1) model reordering by allowing unrestricted movement of words, rather than constrained movement of compositional units, and therefore must (2) attempt to compensate via directed, asymmetric distortion and fertility models. The conventional heuristics for attempting to recover from the resulting alignment errors involve estimating two directed models in opposite directions and then intersecting their alignments – to make up for the fact that, in reality, word alignment is an inherently joint relation. A natural alternative is provided by Inversion Transduction Grammars, which estimate the joint word alignment relation directly, eliminating the need for any of the conventional heuristics. We show that this alignment ultimately produces superior translation accuracy on BLEU, NIST, and METEOR metrics over three distinct language pairs. 
Because of the variations of the languages, the coverage of the references is very important to the reference based automatic evaluation of machine translation systems. We propose a method to extend the reference set of the automatic evaluation only based on multiple manual references and their syntactic structures. In our approach, the syntactic equivalents in the reference sentences are identified and hybridized to generate new references. The new method need no external knowledge and can obtain the equivalents of long subsegments of reference sentences. The experimental results show that using the extended reference set the popular automatic evaluation metrics achieve better correlations with the human assessments. 
Recently, numerous statistical machine translation models which can utilize various kinds of translation rules are proposed. In these models, not only the conventional syntactic rules but also the non-syntactic rules can be applied. Even the pure phrase rules are includes in some of these models. Although the better performances are reported over the conventional phrase model and syntax model, the mixture of diversiﬁed rules still leaves much room for study. In this paper, we present a reﬁned rule classiﬁcation system. Based on this classiﬁcation system, the rules are classiﬁed according to different standards, such as lexicalization level and generalization. Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules. This novel classiﬁcation system may supports the SMT research community with some helpful references. 
The prevalence in Chinese of grammatical structures that translate into English in different word orders is an important cause of translation difﬁculty. While previous work has used phrase-structure parses to deal with such ordering problems, we introduce a richer set of Chinese grammatical relations that describes more semantically abstract relations between words. Using these Chinese grammatical relations, we improve a phrase orientation classiﬁer (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. We then apply the log probability of the phrase orientation classiﬁer as an extra feature in a phrase-based MT system, and get signiﬁcant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77). Our Chinese grammatical relations are also likely to be useful for other NLP tasks. 
The alignment problem for synchronous grammars in its unrestricted form, i.e. whether for a grammar and a string pair the grammar induces an alignment of the two strings, reduces to the universal recognition problem, but restrictions may be imposed on the alignment sought, e.g. alignments may be 1 : 1, island-free or sure-possible sorted. The complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntax-based machine translation, inversion transduction grammars (ITGs) (Wu, 1997) and a restricted form of range concatenation grammars ((2,2)-BRCGs) (Søgaard, 2008), are investigated. The universal recognition problems, and therefore also the unrestricted alignment problems, of both formalisms can be solved in time O(n6|G|). The complexities of the restricted alignment problems differ signiﬁcantly, however. 
This paper presents a reordering model using syntactic information of a source tree for phrase-based statistical machine translation. The proposed model is an extension of ISTITG (imposing source tree on inversion transduction grammar) constraints. In the proposed method, the target-side word order is obtained by rotating nodes of the source-side parse-tree. We modeled the node rotation, monotone or swap, using word alignments based on a training parallel corpus and sourceside parse-trees. The model efﬁciently suppresses erroneous target word orderings, especially global orderings. Furthermore, the proposed method conducts a probabilistic evaluation of target word reorderings. In Englishto-Japanese and English-to-Chinese translation experiments, the proposed method resulted in a 0.49-point improvement (29.31 to 29.80) and a 0.33-point improvement (18.60 to 18.93) in word BLEU-4 compared with IST-ITG constraints, respectively. This indicates the validity of the proposed reordering model. 
In this paper, we start with the existing idea of taking reordering rules automatically derived from syntactic representations, and applying them in a preprocessing step before translation to make the source sentence structurally more like the target; and we propose a new approach to hierarchically extracting these rules. We evaluate this, combined with a lattice-based decoding, and show improvements over stateof-the-art distortion models. 
 We consider semi-supervised learning of  information extraction methods, especially  for extracting instances of noun categories  (e.g., ‘athlete,’ ‘team’) and relations (e.g.,  ‘playsForTeam(athlete,team)’).  Semi-  supervised approaches using a small number  of labeled examples together with many un-  labeled examples are often unreliable as they  frequently produce an internally consistent,  but nevertheless incorrect set of extractions.  We propose that this problem can be over-  come by simultaneously learning classiﬁers  for many different categories and relations  in the presence of an ontology deﬁning  constraints that couple the training of these  classiﬁers. Experimental results show that  simultaneously learning a coupled collection  of classiﬁers for 30 categories and relations  results in much more accurate extractions  than training classiﬁers individually.  
We consider the task of learning a classiﬁer from the feature space X to the set of classes Y = {0, 1}, when the features can be partitioned into class-conditionally independent feature sets X1 and X2. We show that the class-conditional independence can be used to represent the original learning task in terms of 1) learning a classiﬁer from X2 to X1 (in the sense of estimating the probability P (x1|x2))and 2) learning the classconditional distribution of the feature set X1. This fact can be exploited for semi-supervised learning because the former task can be accomplished purely from unlabeled samples. We present experimental evaluation of the idea in two real world applications. 
We address two critical issues involved in applying semi-supervised learning (SSL) to a real-world task: parameter tuning and choosing which (if any) SSL algorithm is best suited for the task at hand. To gain a better understanding of these issues, we carry out a medium-scale empirical study comparing supervised learning (SL) to two popular SSL algorithms on eight natural language processing tasks under three performance metrics. We simulate how a practitioner would go about tackling a new problem, including parameter tuning using cross validation (CV). We show that, under such realistic conditions, each of the SSL algorithms can be worse than SL on some datasets. However, we also show that CV can select SL/SSL to achieve “agnostic SSL,” whose performance is almost always no worse than SL. While CV is often dismissed as unreliable for SSL due to the small amount of labeled data, we show that it is in fact effective for accuracy even when the labeled dataset size is as small as 10. 
Support Vector Machines present an interesting and effective approach to solve automated classiﬁcation tasks. Although it only handles binary and supervised problems by nature, it has been transformed into multiclass and semi-supervised approaches in several works. A previous study on supervised and semi-supervised SVM classiﬁcation over binary taxonomies showed how the latter clearly outperforms the former, proving the suitability of unlabeled data for the learning phase in this kind of tasks. However, the suitability of unlabeled data for multiclass tasks using SVM has never been tested before. In this work, we present a study on whether unlabeled data could improve results for multiclass web page classiﬁcation tasks using Support Vector Machines. As a conclusion, we encourage to rely only on labeled data, both for improving (or at least equaling) performance and for reducing the computational cost. 
This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains. The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Self-training (Abney, 2007; McClosky et al., 2006). A preliminary evaluation favors the use of SCL over the simpler self-training techniques. 
Latent Dirichlet Allocation is an unsupervised graphical model which can discover latent topics in unlabeled data. We propose a mechanism for adding partial supervision, called topic-in-set knowledge, to latent topic modeling. This type of supervision can be used to encourage the recovery of topics which are more relevant to user modeling goals than the topics which would be recovered otherwise. Preliminary experiments on text datasets are presented to demonstrate the potential effectiveness of this method. 
We present a semi-supervised (bootstrapping) approach to the extraction of time expression mentions in large unlabelled corpora. Because the only supervision is in the form of seed examples, it becomes necessary to resort to heuristics to rank and ﬁlter out spurious patterns and candidate time expressions. The application of bootstrapping to time expression recognition is, to the best of our knowledge, novel. In this paper, we describe one such architecture for bootstrapping Information Extraction (IE) patterns —suited to the extraction of entities, as opposed to events or relations— and summarize our experimental ﬁndings. These point out to the fact that a pattern set with a good increase in recall with respect to the seeds is achievable within our framework while, on the other side, the decrease in precision in successive iterations is succesfully controlled through the use of ranking and selection heuristics. Experiments are still underway to achieve the best use of these heuristics and other parameters of the bootstrapping algorithm. 
We present a simple semi-supervised learning algorithm for named entity recognition (NER) using conditional random ﬁelds (CRFs). The algorithm is based on exploiting evidence that is independent from the features used for a classiﬁer, which provides high-precision labels to unlabeled data. Such independent evidence is used to automatically extract highaccuracy and non-redundant data, leading to a much improved classiﬁer at the next iteration. We show that our algorithm achieves an average improvement of 12 in recall and 4 in precision compared to the supervised algorithm. We also show that our algorithm achieves high accuracy when the training and test sets are from different domains. 
This paper proposes a new bootstrapping framework using cross-lingual information projection. We demonstrate that this framework is particularly effective for a challenging NLP task which is situated at the end of a pipeline and thus suffers from the errors propagated from upstream processing and has low-performance baseline. Using Chinese event extraction as a case study and bitexts as a new source of information, we present three bootstrapping techniques. We first conclude that the standard mono-lingual bootstrapping approach is not so effective. Then we exploit a second approach that potentially benefits from the extra information captured by an English event extraction system and projected into Chinese. Such a crosslingual scheme produces significant performance gain. Finally we show that the combination of mono-lingual and cross-lingual information in bootstrapping can further enhance the performance. Ultimately this new framework obtained 10.1% relative improvement in trigger labeling (F-measure) and 9.5% relative improvement in argument-labeling. 
This paper investigates semi-supervised learning of Gaussian mixture models using an uniﬁed objective function taking both labeled and unlabeled data into account. Two methods are compared in this work – the hybrid discriminative/generative method and the purely generative method. They differ in the criterion type on labeled data; the hybrid method uses the class posterior probabilities and the purely generative method uses the data likelihood. We conducted experiments on the TIMIT database and a standard synthetic data set from UCI Machine Learning repository. The results show that the two methods behave similarly in various conditions. For both methods, unlabeled data improve training on models of higher complexity in which the supervised method performs poorly. In addition, there is a trend that more unlabeled data results in more improvement in classiﬁcation accuracy over the supervised model. We also provided experimental observations on the relative weights of labeled and unlabeled parts of the training objective and suggested a critical value which could be useful for selecting a good weighing factor. 
Proficiency testing is an important ingredient in successful language teaching. However, repeated testing for course placement, over the course of instruction or for certification can be time-consuming and costly. We present the design and validation of the Versant Arabic Test, a fully automated test of spoken Modern Standard Arabic, that evaluates test-takers' facility in listening and speaking. Experimental data shows the test to be highly reliable (testretest r=0.97) and to strongly predict performance on the ILR OPI (r=0.87), a standard interview test that assesses oral proficiency. 
Assessment of reading proficiency is typically done by asking subjects to read a text passage silently and then answer questions related to the text. An alternate approach, measuring reading-aloud proficiency, has been shown to correlate well with the aforementioned common method and is used as a paradigm in this paper. We describe a system that is able to automatically score two types of children’s read speech samples (text passages and word lists), using automatic speech recognition and the target criterion “correctly read words per minute”. Its performance is dependent on the data type (passages vs. word lists) as well as on the relative difficulty of passages or words for individual readers. Pearson correlations with human assigned scores are around 0.86 for passages and around 0.80 for word lists. 
The field of intelligent tutoring systems has seen many successes in recent years. A significant remaining challenge is the automatic creation of corpus-based tutorial dialogue management models. This paper reports on early work toward this goal. We identify tutorial dialogue modes in an unsupervised fashion using hidden Markov models (HMMs) trained on input sequences of manually-labeled dialogue acts and adjacency pairs. The two best-fit HMMs are presented and compared with respect to the dialogue structure they suggest; we also discuss potential uses of the methodology for future work. 
The goal of this research is to increase the value of each individual student's vocabulary by finding words that the student doesn’t know, needs to, and is ready to learn. To help identify such words, a better model of how well any given word is expected to be known was created. This is accomplished by using a semantic language model, LSA, to track how every word changes with the addition of more and more text from an appropriate corpus. We define the “maturity” of a word as the degree to which it has become similar to that after training on the entire corpus. An individual student’s average vocabulary level can then be placed on the wordmaturity scale by an adaptive test. Finally, the words that the student did or did not know on the test can be used to predict what other words the same student knows by using multiple maturity models trained on random samples of typical educational readings. This detailed information can be used to generate highly customized vocabulary teaching and testing exercises, such as Cloze tests. 
In this paper we investigate the task of text simplification for Brazilian Portuguese. Our purpose is three-fold: to introduce a simplification tool for such language and its underlying development methodology, to present an on-line authoring system of simplified text based on the previous tool, and finally to discuss the potentialities of such technology for education. The resources and tools we present are new for Portuguese and innovative in many aspects with respect to previous initiatives for other languages. 
 We present an application of Latent Semantic Analysis to word sense discrimination within a tutor for English vocabulary learning. We attempt to match the meaning of a word in a document with the meaning of the same word in a ﬁll-in-the-blank question. We compare the performance of the Lesk algorithm to Latent Semantic Analysis. We also compare the performance of Latent Semantic Analysis on a set of words with several unrelated meanings and on a set of words having both related and unrelated meanings. 
One of the most common and persistent error types in second language writing is collocation errors, such as learn knowledge instead of gain or acquire knowledge, or make damage rather than cause damage. In this work-inprogress report, we propose a probabilistic model for suggesting corrections to lexical collocation errors. The probabilistic model incorporates three features: word association strength (MI), semantic similarity (via WordNet) and the notion of shared collocations (or intercollocability). The results suggest that the combination of all three features outperforms any single feature or any combination of two features. 
We describe and motivate an unsupervised lexical error detection and correction algorithm and its application in a tool called Lexbar appearing as a query box on the Web browser toolbar or as a search engine interface. Lexbar accepts as user input candidate strings of English to be checked for acceptability and, where errors are detected, offers corrections. We introduce the notion of hybrid n-gram and extract these from BNC as the knowledgebase against which to compare user input. An extended notion of edit distance is used to identify most likely candidates for correcting detected errors. Results are illustrated with four types of errors. 
We present an innovative application of discourse processing concepts to educational technology. In our corpus analysis of peer learning dialogues, we found that initiative and initiative shifts are indicative of learning, and of learning-conducive episodes. We are incorporating this ﬁnding in KSC-PaL, the peer learning agent we have been developing. KSC-PaL will promote learning by encouraging shifts in task initiative. 
In this paper, we investigate a novel approach to correcting grammatical and lexical errors in texts written by second language authors. Contrary to previous approaches which tend to use unilingual models of the user's second language (L2), this new approach uses a simple roundtrip Machine Translation method which leverages information about both the author’s first (L1) and second languages. We compare the repair rate of this roundtrip translation approach to that of an existing approach based on a unilingual L2 model with shallow syntactic pruning, on a series of preposition choice errors. We find no statistically significant difference between the two approaches, but find that a hybrid combination of both does perform significantly better than either one in isolation. Finally, we illustrate how the translation approach has the potential of repairing very complex errors which would be hard to treat without leveraging knowledge of the author's L1. 
ESL Assistant is a prototype web-based writing-assistance tool that is being developed for English Language Learners. The system focuses on types of errors that are typically made by non-native writers of American English. A freely-available prototype was deployed in June 2008. User data from this system are manually evaluated to identify writing domain and measure system accuracy. Combining the user log data with the evaluated rewrite suggestions enables us to determine how effectively English language learners are using the system, across rule types and across writing domains. We find that repeat users typically make informed choices and can distinguish correct suggestions from incorrect. 
This paper explores the issue of automatically generated ungrammatical data and its use in error detection, with a focus on the task of classifying a sentence as grammatical or ungrammatical. We present an error generation tool called GenERRate and show how GenERRate can be used to improve the performance of a classiﬁer on learner data. We describe initial attempts to replicate Cambridge Learner Corpus errors using GenERRate. 
Using metaphor-annotated material that is sufﬁciently representative of the topical composition of a similar-length document in a large background corpus, we show that words expressing a discourse-wide topic of discussion are less likely to be metaphorical than other words in a document. Our results suggest that to harvest metaphors more effectively, one is advised to consider words that do not represent a discourse topic. Traditionally, metaphor detectors use the observation that a metaphorically used item creates a local incongruity because there is a violation of a selectional restriction, such as providing a non-vehicle object to the verb derail in Protesters derailed the conference. Current state of art in metaphor detection therefore tends to be “localistic” – the distributional proﬁle of the target word in its immediate grammatical or collocational context in a background corpus or a database like WordNet is used to determine metaphoricity (Mason, 2004; Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). However, some theories of metaphor postulate certain features of metaphors that connect it to the surrounding text beyond the small grammatical or proximal locality. For example, for Kittay (1987) metaphor is a discourse phenomenon; although the minimal metaphoric unit is a clause, often much larger chunks of text constitute a metaphor. Consider, for example, the TRAIN metaphor in the  following excerpt from a Sunday Times article on 20 September 1992: Thatcher warned EC leaders to stop their endless round of summits and take notice of their own people. “There is a fear that the European train will thunder forward, laden with its customary cargo of gravy, towards a destination neither wished for nor understood by electorates. But the train can be stopped,” she said. In the example above, the quotation is not in itself a metaphor, as there is no indication that something other than the actual train is being discussed (and so no local incongruities exist). Only when situated in the context prepared by the ﬁrst sentence (and indeed the rest of the article), the train imagery becomes a metaphor. According to Kittay, a metaphor occurs when a semantic ﬁeld is used to discuss a different content domain. The theory therefore predicts that a metaphorically used semantic domain would be offtopic in the given document. Although a single document can have singular, idiosyncratic topics, it is likelier to discuss a mix of topics that are typical of the discourse of which it is part. We therefore derive the following hypothesis: Words in a given document that represent a common topic of discussion in a corpus of relevant documents would be predominantly non-metaphorical. That is, a smaller share of metaphorically used words in a document would fall in such topical words than the share of topical words in the document. We test this hypothesis in the current article.  
Psycholinguistic studies of metaphor processing must control their stimuli not just for word frequency but also for the frequency with which a term is used metaphorically. Thus, we consider the task of metaphor frequency estimation, which predicts how often target words will be used metaphorically. We develop metaphor classiﬁers which represent metaphorical domains through Latent Dirichlet Allocation, and apply these classiﬁers to the target words, aggregating their decisions to estimate the metaphorical frequencies. Training on only 400 sentences, our models are able to achieve 61.3% accuracy on metaphor classiﬁcation and 77.8% accuracy on HIGH vs. LOW metaphorical frequency estimation. 
An eggcorn is a type of linguistic error where a word is substituted with one that is semantically plausible – that is, the substitution is a semantic reanalysis of what may be a rare, archaic, or otherwise opaque term. We build a system that, given the original word and its eggcorn form, ﬁnds a semantic path between the two. Based on these paths, we derive a typology that reﬂects the different classes of semantic reinterpretation underlying eggcorns. 
Computational models can be built to capture the syntactic structures and semantic patterns of human punning riddles. This model is then used as rules by a computer to generate its own puns. This paper presents T-PEG, a system that utilizes phonetic and semantic linguistic resources to automatically extract word relationships in puns and store the knowledge in template form. Given a set of training examples, it is able to extract 69.2% usable templates, resulting in computer-generated puns that received an average score of 2.13 as compared to 2.70 for human-generated puns from user feedback. 
This paper presents our on­going work to automatically generate lyrics for a given melody, for phonetic languages such as Tamil. We approach the task of identifying the required syllable pattern for the lyric as a sequence labeling problem and hence use the popular CRF++ toolkit for learning. A corpus comprising of 10 melodies was used to train the system to understand the syllable patterns. The trained model is then used to guess the syllabic pattern for a new melody to produce an optimal sequence of syllables. This sequence is presented to the Sentence Generation module which uses the Dijkstra's shortest path algorithm to come up with a meaningful phrase matching the syllabic pattern. 
This paper is concerned with the possibility of quantifying and comparing the productivity of similar yet distinct syntactic constructions, predicting the likelihood of encountering unseen lexemes in their unfilled slots. Two examples are explored: variants of comparative correlative constructions (CCs, e.g. the faster the better), which are potentially very productive but in practice lexically restricted; and ambiguously attached prepositional phrases with the preposition with, which can host both large and restricted inventories of arguments under different conditions. It will be shown that different slots in different constructions are not equally likely to be occupied productively by unseen lexemes, and suggested that in some cases this can help disambiguate the underlying syntactic and semantic structure. 
Interactive fiction (often called “IF”) is a venerable thread of creative computing that includes Adventure, Zork, and the computer game The Hitchhiker’s Guide to the Galaxy as well as innovative recent work. These programs are usually known as “games,” appropriately, but they can also be rich forms of text-based computer simulation, dialog systems, and examples of computational literary art. Theorists of narrative have long distinguished between the level of underlying content or story (which can usefully be seen as corresponding to the simulated world in interactive fiction) and that of expression or discourse (corresponding to the textual exchange between computer and user). While IF development systems have offered a great deal of power and flexibility to author/programmers by providing a computational model of the fictional world, previous systems have not systematically distinguished between the telling and what is told. Developers were not able to control the content and expression levels independently so that they could, for instance, have a program relate events out of chronological order or have it relate events from the perspective of different characters. Curveship is an interactive fiction system which draws on narrative theory and computational linguistics to allow the transformation of the narrating in these ways. This talk will briefly describe interactive fiction, narrative variation, and how Curveship provides new capabilities for interactive fiction authors.  
Cell phone text messaging users express themselves brieﬂy and colloquially using a variety of creative forms. We analyze a sample of creative, non-standard text message word forms to determine frequent word formation processes in texting language. Drawing on these observations, we construct an unsupervised noisy-channel model for text message normalization. On a test set of 303 text message forms that differ from their standard form, our model achieves 59% accuracy, which is on par with the best supervised results reported on this dataset. 
This paper investigates a little-studied class of adjectives that we refer to as ‘complex adjectives’, i.e., operationally, adjectives constituted of at least two word tokens separated by a hyphen. We study the properties of these adjectives using two very large text collections: a portion of Wikipedia and a Web corpus. We consider three corpus-based measures of morphological productivity, and we investigate how productivity rankings based on them correlate with each other under different conditions, thus providing different angles both on the morphological productivity of complex adjectives, and on the productivity measures themselves. 
We demonstrate that subjective creativity in sentence-writing can in part be predicted using computable quantities studied in Computer Science and Cognitive Psychology. We introduce a task in which a writer is asked to compose a sentence given a keyword. The sentence is then assigned a subjective creativity score by human judges. We build a linear regression model which, given the keyword and the sentence, predicts the creativity score. The model employs features on statistical language models from a large corpus, psychological word norms, and WordNet. 
We are interested in the ways that language is used to achieve a variety of goals, where the same utterance may have vastly different consequences in different situations. This is closely related to the topic of creativity in language. The fact that the same utterance can be used to achieve a variety of goals opens up the possibility of using it to achieve new goals. The current paper concentrates largely on an implemented system for exploring how the effects of an utterance depend on the situation in which it is produced, but we will end with some speculations about how how utterances can come to have new kinds of uses. 
In this paper we present our experiments with active learning to improve the performance of our probabilistic anaphora resolution system. We have adopted entropy-based uncertainty measures to select new instances to be added to our training data. The actively selected instances, however, were not more successful in improving the performance of the system than the same amount of randomly selected instances. The uncertainty measures we used behave differently from each other when selecting new instances, but none of them achieved remarkable performance. Further studies on active sample selection for anaphora resolution are necessary. 
Active learning is an effective method for creating training sets cheaply, but it is a biased sampling process and fails to explore large regions of the instance space in many applications. This can result in a missed cluster effect, which signﬁcantly lowers recall and slows down learning for infrequent classes. We show that missed clusters can be avoided in sequence classiﬁcation tasks by using sentences as natural multi-instance units for labeling. Co-selection of other tokens within sentences provides an implicit exploratory component since we found for the task of named entity recognition on two corpora that entity classes co-occur with sufﬁcient frequency within sentences. 
We present an empirical investigation of the annotation cost estimation task for active learning in a multi-annotator environment. We present our analysis from two perspectives: selecting examples to be presented to the user for annotation; and evaluating selective sampling strategies when actual annotation cost is not available. We present our results on a movie review classiﬁcation task with rationale annotations. We demonstrate that a combination of instance, annotator and annotation task characteristics are important for developing an accurate estimator, and argue that both correlation coefﬁcient and root mean square error should be used for evaluating annotation cost estimators. 
Annotation acquisition is an essential step in training supervised classiﬁers. However, manual annotation is often time-consuming and expensive. The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates. In this paper, we consider the difﬁcult problem of classifying sentiment in political blog snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identiﬁed to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis conﬁrm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classiﬁcation models, and evaluate the utility of annotation selection on classiﬁcation accuracy and efﬁciency. 
This paper presents pilot work integrating machine labeling and active learning with human annotation of data for the language documentation task of creating interlinearized gloss text (IGT) for the Mayan language Uspanteko. The practical goal is to produce a totally annotated corpus that is as accurate as possible given limited time for manual annotation. We describe ongoing pilot studies which examine the inﬂuence of three main factors on reducing the time spent to annotate IGT: suggestions from a machine labeler, sample selection methods, and annotator expertise. 
As supervised machine learning methods for addressing tasks in natural language processing (NLP) prove increasingly viable, the focus of attention is naturally shifted towards the creation of training data. The manual annotation of corpora is a tedious and time consuming process. To obtain high-quality annotated data constitutes a bottleneck in machine learning for NLP today. Active learning is one way of easing the burden of annotation. This paper presents a ﬁrst probe into the NLP research community concerning the nature of the annotation projects undertaken in general, and the use of active learning as annotation support in particular. 
When faced with the task of building machine learning or NLP models, it is often worthwhile to turn to active learning to obtain human annotations at minimal costs. Traditional active learning schemes query a human for labels of intelligently chosen examples. However, human effort can also be expended in collecting alternative forms of annotations. For example, one may attempt to learn a text classiﬁer by labeling class-indicating words, instead of, or in addition to, documents. Learning from two different kinds of supervision brings a new, unexplored dimension to the problem of active learning. In this paper, we demonstrate the value of such active dual supervision in the context of sentiment analysis. We show how interleaving queries for both documents and words signiﬁcantly reduces human effort – more than what is possible through traditional one-dimensional active learning, or by passive combinations of supervisory inputs. 
Building machine translation (MT) for many minority languages in the world is a serious challenge. For many minor languages there is little machine readable text, few knowledgeable linguists, and little money available for MT development. For these reasons, it becomes very important for an MT system to make best use of its resources, both labeled and unlabeled, in building a quality system. In this paper we argue that traditional active learning setup may not be the right ﬁt for seeking annotations required for building a Syntax Based MT system for minority languages. We posit that a relatively new variant of active learning, Proactive Learning, is more suitable for this task. 
Text summarization is one of the oldest problems in natural language processing. Popular approaches rely on extracting relevant sentences from the original documents. As a side effect, sentences that are too long but partly relevant are doomed to either not appear in the ﬁnal summary, or prevent inclusion of other relevant sentences. Sentence compression is a recent framework that aims to select the shortest subsequence of words that yields an informative and grammatical sentence. This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 
We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the subsentence or “concept”-level, to a sentencelevel model, previously solved with an ILP. Our model scales more efﬁciently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences. We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics. 
We evaluate several heuristic solvers for correlation clustering, the NP-hard problem of partitioning a dataset given pairwise afﬁnities between all points. We experiment on two practical tasks, document clustering and chat disentanglement, to which ILP does not scale. On these datasets, we show that the clustering objective often, but not always, correlates with external metrics, and that local search always improves over greedy solutions. We use semi-deﬁnite programming (SDP) to provide a tighter bound, showing that simple algorithms are already close to optimality. 
We develop a new objective function for word alignment that measures the size of the bilingual dictionary induced by an alignment. A word alignment that results in a small dictionary is preferred over one that results in a large dictionary. In order to search for the alignment that minimizes this objective, we cast the problem as an integer linear program. We then extend our objective function to align corpora at the sub-word level, which we demonstrate on a small Turkish-English corpus. 
 She put the batter in the refrigerator. (1)  In this paper, a framework for acquiring common sense knowledge from the Web is pre-  He ate the apple in the refrigerator. (2) In (1), we are dealing with lexical ambiguity. There  sented. Common sense knowledge includes  is little doubt for us to determine just what the “bat-  information about the world that humans use in their everyday lives. To acquire this knowledge, relationships between nouns are retrieved by using search phrases with automatically ﬁlled constituents. Through empirical analysis of the acquired nouns over WordNet, probabilities are produced for relationships between a concept and a word rather  ter” is (food/substance used in baking). However, a computer must determine that it is not someone who swings a bat in baseball that is being put into a refrigerator, although it is entirely possible to do (depending on the size of the refrigerator). This demonstrates how CSK can be useful in solving word sense disambiguation. We know it is common for food to  than between two words. A speciﬁc goal of our acquisition method is to acquire knowledge that can be successfully applied to NLP problems. We test the validity of the acquired knowledge by means of an application to the problem of word sense disambiguation. Results show that the knowledge can be used to improve the accuracy of a state of the art un-  be found in a refrigerator and so we easily resolve batter as a food/substance rather than a person. CSK can also help to solve syntactic ambiguity. The problem of prepositional phrase attachment occurs in sentences similar to (2). In this case, it is difﬁcult for a computer to determine if “he” is in the refrigerator eating an apple or if the “apple” which  supervised disambiguation system.  he ate was in the refrigerator. Like the previous ex-  ample, the knowledge that food is commonly found  
 from non-parallel but comparable corpora. Our  This paper demonstrates one efﬁcient technique in extracting bilingual word pairs from non-parallel but comparable corpora. Instead of using the common approach of taking high frequency words to build up the initial bilingual lexicon, we show contextually relevant terms that co-occur with cognate pairs can be efﬁciently utilized to build a bilingual dictionary. The result shows that our models using this technique have signiﬁcant improvement over baseline models especially when highestranked translation candidate per word is considered.  model using this technique with spelling similarity approach obtains 85.4 percent precision at 50.0 percent recall. Precision of 79.0 percent at 50.0 percent recall is recorded when using this technique with context similarity approach. Furthermore, by using a string edit-distance vs. precision curve, we also reveal that the latter model is able to capture words efﬁciently compared to a baseline model. Section 2 is dedicated to mention some of the related works. In Section 3, the technique that we used is explained. Section 4 describes our experimental setup followed by the evaluation results in Section 5. Discussion and conclusion are in Section 6 and 7  
 ing such dictionaries have been developed under the  Various techniques have been developed to automatically induce semantic dictionaries from text corpora and from the Web. Our research combines corpus-based semantic lexicon induction with statistics acquired from the Web to improve the accuracy of automatically acquired domain-speciﬁc dictionaries. We use a weakly supervised bootstrapping algorithm to induce a semantic lexicon from a text corpus, and then issue Web queries to generate co-occurrence statistics between each lexicon entry and semantically related terms. The Web  rubrics of lexical acquisition, hyponym learning, semantic class induction, and Web-based information extraction. These techniques can be used to rapidly create semantic lexicons for new domains and languages, and to automatically increase the coverage of existing resources. Techniques for semantic lexicon induction can be subdivided into two groups: corpus-based methods and Web-based methods. Although the Web can be viewed as a (gigantic) corpus, these two approaches tend to have different goals. Corpus-based methods  statistics provide a source of independent evidence to conﬁrm, or disconﬁrm, that a word belongs to the intended semantic category. We evaluate this approach on 7 semantic categories representing two domains. Our results show that the Web statistics dramatically improve the ranking of lexicon entries, and can also be used to ﬁlter incorrect entries.  are typically designed to induce domain-speciﬁc semantic lexicons from a collection of domain-speciﬁc texts. In contrast, Web-based methods are typically designed to induce broad-coverage resources, similar to WordNet. Ideally, one would hope that broadcoverage resources would be sufﬁcient for any domain, but this is often not the case. Many domains use specialized vocabularies and jargon that are not  
In this paper we present two approaches to automatically extract cross-lingual predicate clusters, based on bilingual parallel corpora and cross-lingual information extraction. We demonstrate how these clusters can be used to improve the NIST Automatic Content Extraction (ACE) event extraction task1. We propose a new inductive learning framework to automatically augment background data for lowconfidence events and then conduct global inference. Without using any additional data or accessing the baseline algorithms this approach obtained significant improvement over a state-of-the-art bilingual (English and Chinese) event extraction system. 
 lexical acquisition (Lin, 1998b; Dagan et al., 1999;  The task of automatically acquiring semantically related words have led people to study distributional similarity. The distributional hypothesis states that words that are similar share similar contexts. In this paper we present a technique that aims at improving  Curran and Moens, 2002; Alfonseca and Manandhar, 2002). Distributional methods for automatic acquisition of semantically related words suffer from data sparseness. They generally perform less well on low-frequency words (Weeds and Weir, 2005;  the performance of a syntax-based distributional method by augmenting the original input of the system (syntactic co-occurrences) with the output of the system (nearest neighbours). This technique is based on the idea of the transitivity of similarity.  van der Plas, 2008). This is a pity because the available resources for semantically related words usually cover the frequent words rather well. It is for the low-frequency words that automatic methods would be most welcome. This paper tries to ﬁnd a way to improve the per-  
Most cross-lingual speech retrieval assumes intensive knowledge about all involved languages. However, such resource may not exist for some less popular languages. Some applications call for speech retrieval in unknown languages. In this work, we leverage on a quasi-language-independent subword recognizer trained on multiple languages, to obtain an abstracted representation of speech data in an unknown language. Languageindependent query expansion is achieved either by allowing a wide lattice output for an audio query, or by taking advantage of distinctive features in speech articulation to propose subwords most similar to the given subwords in a query. We propose using a retrieval model based on ﬁnite state machines for fuzzy matching of speech sound patterns, and further for speech retrieval. A pilot study of speech retrieval in unknown languages is presented, using English, Spanish and Russian as training languages, and Croatian as the unknown target language. 
Most of the Internet data for Indian languages exist in various encodings, causing difﬁculties in searching for the information through search engines. In the Indian scenario, majority web pages are not searchable or the intended information is not efﬁciently retrieved by the search engines due to the following: (1) Multiple text-encodings are used while authoring websites. (2) Inspite of Indian languages sharing common phonetic nature, common words like loan words (borrowed from other languages like Sanskrit, Urdu or English), transliterated terms, pronouns etc., can not be searched across languages. (3) Finally the query input mechanism is another major problem. Most of the users hardly know how to type in their native language and prefer to access the information through English based transliteration. This paper addresses all these problems and presents a transliteration based search engine (inSearch) which is capable of searching 10 multi-script and multiencoded Indian languages content on the web. 
For many languages, the size of Wikipedia is an order of magnitude smaller than the English Wikipedia. We present a method for cross-lingual alignment of template and infobox attributes in Wikipedia. The alignment is used to add and complete templates and infoboxes in one language with information derived from Wikipedia in another language. We show that alignment between English and Dutch Wikipedia is accurate and that the result can be used to expand the number of template attribute-value pairs in Dutch Wikipedia by 50%. Furthermore, the alignment provides valuable information for normalization of template and attribute names and can be used to detect potential inconsistencies. 
Multilingual Wikipedia has been used extensively for a variety Natural Language Processing (NLP) tasks. Many Wikipedia entries (people, locations, events, etc.) have descriptions in several languages. These descriptions, however, are not identical. On the contrary, descriptions in different languages created for the same Wikipedia entry can vary greatly in terms of description length and information choice. Keeping these peculiarities in mind is necessary while using multilingual Wikipedia as a corpus for training and testing NLP applications. In this paper we present preliminary results on quantifying Wikipedia multilinguality. Our results support the observation about the substantial variation in descriptions of Wikipedia entries created in different languages. However, we believe that asymmetries in multilingual Wikipedia do not make Wikipedia an undesirable corpus for NLP applications training. On the contrary, we outline research directions that can utilize multilingual Wikipedia asymmetries to bridge the communication gaps in multilingual societies. 
In this paper we present a new statistical approach to opinion detection and its’ evaluation on the English, Chinese and Japanese corpora. Besides, the proposed method is compared with three baselines, namely Naïve Bayes classifier, a language model and an approach based on significant collocations. These models being language independent are improved with the use of language-dependent technique on the example of the English corpus. We show that our method almost always gives better performance compared to the considered baselines. 
In this paper, we describe a sentence position based summarizer that is built based on a sentence position policy, created from the evaluation testbed of recent summarization tasks at Document Understanding Conferences (DUC). We show that the summarizer thus built is able to outperform most systems participating in task focused summarization evaluations at Text Analysis Conferences (TAC) 2008. Our experiments also show that such a method would perform better at producing short summaries (upto 100 words) than longer summaries. Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task. 
We propose an efficient text summarization technique that involves two basic operations. The first operation involves finding coherent chunks in the document and the second operation involves ranking the text in the individual coherent chunks and picking the sentences that rank above a given threshold. The coherent chunks are formed by exploiting the lexical relationship between adjacent sentences in the document. Occurrence of words through repetition or relatedness by sense relation plays a major role in forming a cohesive tie. The proposed text ranking approach is based on a graph theoretic ranking model applied to text summarization task. 
Part of Speech (POS) tagging and Named Entity (NE) tagging have become important components of effective text analysis. In this paper, we propose a bootstrapped model that involves four levels of text processing for Urdu. We show that increasing the training data for POS learning by applying bootstrapping techniques improves NE tagging results. Our model overcomes the limitation imposed by the availability of limited ground truth data required for training a learning model. Both our POS tagging and NE tagging models are based on the Conditional Random Field (CRF) learning approach. To further enhance the performance, grammar rules and lexicon lookups are applied on the final output to correct any spurious tag assignments. We also propose a model for word boundary segmentation where a bigram HMM model is trained for character transitions among all positions in each word. The generated words are further processed using a probabilistic language model. All models use a hybrid approach that combines statistical models with hand crafted grammar rules. 
We summarize our experiences building a comprehensive suite of tests for a statistical natural language processing toolkit, ClearTK. We describe some of the challenges we encountered, introduce a software project that emerged from these efforts, summarize our resulting test suite, and discuss some of the lessons learned. 
Regression testing of natural language systems is problematic for two main reasons: component input and output is complex, and system behaviour is context-dependent. We have developed a generic approach which solves both of these issues. We describe our regression tool, CONTEST, which supports context-dependent testing of dialogue system components, and discuss the regression test sets we developed, designed to effectively isolate components from changes and problems earlier in the pipeline. We believe that the same approach can be used in regression testing for other dialogue systems, as well as in testing any complex NLP system containing multiple components. 
Rule-based spoken dialogue systems require a good regression testing framework if they are to be maintainable. We argue that there is a tension between two extreme positions when constructing the database of test examples. On the one hand, if the examples consist of input/output tuples representing many levels of internal processing, they are ﬁnegrained enough to catch most processing errors, but unstable under most system modiﬁcations. If the examples are pairs of user input and ﬁnal system output, they are much more stable, but too coarse-grained to catch many errors. In either case, there are fairly severe difﬁculties in judging examples correctly. We claim that a good compromise can be reached by implementing a paraphrasing mechanism which maps internal semantic representations into surface forms, and carrying out regression testing using paraphrases of semantic forms rather than the semantic forms themselves. We describe an implementation of the idea using the Open Source Regulus toolkit, where paraphrases are produced using Regulus grammars compiled in generation mode. Paraphrases can also be used at runtime to produce conﬁrmations. By compiling the paraphrase grammar a second time, as a recogniser, it is possible in a simple and natural way to guarantee that conﬁrmations are always within system coverage.  
To understand the key characteristics of NLP tools, evaluation and comparison against different tools is important. And as NLP applications tend to consist of multiple semiindependent sub-components, it is not always enough to just evaluate complete systems, a fine grained evaluation of underlying components is also often worthwhile. Standardization of NLP components and resources is not only significant for reusability, but also in that it allows the comparison of individual components in terms of reliability and robustness in a wider range of target domains. But as many evaluation metrics exist in even a single domain, any system seeking to aid inter-domain evaluation needs not just predefined metrics, but must also support pluggable user-defined metrics. Such a system would of course need to be based on an open standard to allow a large number of components to be compared, and would ideally include visualization of the differences between components. We have developed a pluggable evaluation system based on the UIMA framework, which provides visualization useful in error analysis. It is a single integrated system which includes a large ready-to-use, fully interoperable library of NLP tools.  ing different systems. Due to this, the reusability of tools designed for such subtasks is a common design consideration; fine grained interoperability between sub components, not just between complete systems. In addition to the benefits of reusability, interoperability is also important in evaluation of components. Evaluations are normally done by comparing two sets of data, a gold standard data and test data showing the components performance. Naturally this comparison requires the two data sets to be in the same data format with the same semantics. Comparing of "Apples to Apples" provides another reason why standardization of NLP tools is beneficial. Another advantage of standardization is that the number of gold standard data sets that can be compared against is also increased, allowing tools to be tested in a wider range of domains. The ideal is that all components are standardized to conform to an open, widely used interoperability framework. One possible such framework is UIMA; Unstructured Information Management Architecture (Ferrucci et al., 2004), which is an open project of OASIS and Apache. We have been developing U-Compare (Kano et al., 2009)1, an integrated testing an evaluation platform based on this framework.  
We present Tightly Packed Tries (TPTs), a compact implementation of read-only, compressed trie structures with fast on-demand paging and short load times. We demonstrate the beneﬁts of TPTs for storing n-gram back-off language models and phrase tables for statistical machine translation. Encoded as TPTs, these databases require less space than ﬂat text ﬁle representations of the same data compressed with the gzip utility. At the same time, they can be mapped into memory quickly and be searched directly in time linear in the length of the key, without the need to decompress the entire ﬁle. The overhead for local decompression during search is marginal. 
In this paper we will present work carried out to scale up the system for text understanding called GETARUNS, and port it to be used in dialogue understanding. We will present the adjustments we made in order to cope with transcribed spoken dialogues like those produced in the ICSI Berkely project. In a final section we present preliminary evaluation of the system on non-referential pronominals individuation. 
c-rater® is the Educational Testing Service technology for automatic content scoring for short free-text responses. In this paper, we contend that an Agile and test-driven development environment optimizes the development of an NLP-based technology.  Example Item (Full Credit 2) Concepts or main/key points:  Figures are given Prompt: The figures show three polygons. Is the polygon in Figure 1 an octagon, hexagon, or parallelogram? Explain your answer.  C1: The polygon/it is a quadrilateral with two sets of parallel sides OR the opposite sides are of equal length OR opposite angles are equal C2: The polygon/it has four/4 sides  
We describe the design and implementation of a system for data exploration over dependency parses and derived semantic representations in a large-scale NLP-based search system at powerset.com. Because of the distributed nature of the document repository and the processing infrastructure, and also the complex representations of the corpus data, standard text analysis tools such as grep or awk or language modeling toolkits are not applicable. This paper explores the challenges of extracting statistical information and of building language models in such a distributed NLP environment, and introduces a corpus analysis system, Oceanography, that simpliﬁes the writing of analysis code and transparently takes advantage of existing distributed processing infrastructure. 
Natural Language Processing systems are large-scale softwares, whose development involves many man-years of work, in terms of both coding and resource development. Given a dictionary of 110k lemmas, a few hundred syntactic analysis rules, 20k ngrams matrices and other resources, what will be the impact on a syntactic analyzer of adding a new possible category to a given verb? What will be the consequences of a new syntactic rules addition? Any modiﬁcation may imply, besides what was expected, unforeseeable side-effects and the complexity of the system makes it difﬁcult to guess the overall impact of even small changes. We present here a framework designed to effectively and iteratively improve the accuracy of our linguistic analyzer LIMA by iterative reﬁnements of its linguistic resources. These improvements are continuously assessed by evaluating the analyzer performance against a reference corpus. Our ﬁrst results show that this framework is really helpful towards this goal. 
Integrating rules and statistical systems is a challenge often faced by natural language processing system builders. A common subclass is integrating high precision rules with a Markov statistical sequence classiﬁer. In this paper we suggest that using such rules to constrain the sequence classiﬁer decoder results in superior accuracy and efﬁciency. In a case study of a named entity tagging system, we provide evidence that this method of combination does prove efﬁcient than other methods. The accuracy was the same. 
We describe our system for the BioNLP 2009 event detection task. It is designed to be as domain-independent and unsupervised as possible. Nevertheless, the precisions achieved for single theme event classes range from 75% to 92%, while maintaining reasonable recall. The overall F-scores achieved were 36.44% and 30.80% on the development and the test sets respectively. 
We approached the problems of event detection, argument identiﬁcation, and negation and speculation detection as one of concept recognition and analysis. Our methodology involved using the OpenDMAP semantic parser with manually-written rules. We achieved state-of-the-art precision for two of the three tasks, scoring the highest of 24 teams at precision of 71.81 on Task 1 and the highest of 6 teams at precision of 70.97 on Task 2. The OpenDMAP system and the rule set are available at bionlp.sourceforge.net. *These two authors contributed equally to the paper. 
In this paper, we propose a system for biomedical event extraction using multi-phase approach. It consists of event trigger detector, event type classiﬁer, and relation recognizer and event compositor. The system ﬁrstly identiﬁes triggers in a given sentence. Then, it classiﬁes the triggers into one of nine predeﬁned classes. Lastly, the system examines each trigger whether it has a relation with participant candidates, and composites events with the extracted relations. The ofﬁcial score of the proposed system recorded 61.65 precision, 9.40 recall and 16.31 f-score in approximate span matching. However, we found that the threshold tuning for the third phase had negative effect. Without the threshold tuning, the system showed 55.32 precision, 16.18 recall and 25.04 f-score. 
Determining whether a condition is historical or recent is important for accurate results in biomedicine. In this paper, we investigate four types of information found in clinical text that might be used to make this distinction. We conducted a descriptive, exploratory study using annotation on clinical reports to determine whether this temporal information is useful for classifying conditions as historical or recent. Our initial results suggest that few of these feature values can be used to predict temporal classification. 
This paper introduces ONYX, a sentencelevel text analyzer that implements a number of innovative ideas in syntactic and semantic analysis. ONYX is being developed as part of a project that seeks to translate spoken dental examinations directly into chartable findings. ONYX integrates syntax and semantics to a high degree. It interprets sentences using a combination of probabilistic classifiers, graphical unification, and semantically annotated grammar rules. In this preliminary evaluation, ONYX shows inter-annotator agreement scores with humans of 86% for assigning semantic types to relevant words, 80% for inferring relevant concepts from words, and 76% for identifying relations between concepts. 
Identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information. In this paper we present a machine learning system that ﬁnds the scope of hedge cues in biomedical texts. The system is based on a similar system that ﬁnds the scope of negation cues. We show that the same scope ﬁnding approach can be applied to both negation and hedging. To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus that represent different text types. 
We explore a rule system and a machine learning (ML) approach to automatically harvest information on gene regulation events (GREs) from biological documents in two different evaluation scenarios – one uses self-supplied corpora in a clean lab setting, while the other incorporates a standard reference database of curated GREs from REGULONDB, real-life data generated independently from our work. In the lab condition, we test how feasible the automatic extraction of GREs really is and achieve F-scores, under different, not directly comparable test conditions though, for the rule and the ML systems which amount to 34% and 44%, respectively. In the REGULONDB condition, we investigate how robust both methodologies are by comparing them with this routinely used database. Here, the best F-scores for the rule and the ML systems amount to 34% and 19%, respectively. 
Text mining for biomedicine requires a signiﬁcant amount of domain knowledge. Much of this information is contained in biomedical ontologies. Developers of text mining applications often look for appropriate ontologies that can be integrated into their systems, rather than develop new ontologies from scratch. However, there is often a lack of documentation of the qualities of the ontologies. A number of methodologies for evaluating ontologies have been developed, but it is difﬁcult for users by using these methods to select an ontology. In this paper, we propose a framework for selecting the most appropriate ontology for a particular text mining application. The framework comprises three components, each of which considers different aspects of requirements of text mining applications on ontologies. We also present an experiment based on the framework choosing an ontology for a gene normalization system. 
Dictionaries of biomedical concepts (e.g. diseases, medical treatments) are critical source of background knowledge for systems doing biomedical information retrieval, extraction, and automated discovery. However, the rapid pace of biomedical research and the lack of constraints on usage ensure that such dictionaries are incomplete. Focusing on medical treatment concepts (e.g. drugs, medical procedures and medical devices), we have developed an unsupervised, iterative pattern learning approach for constructing a comprehensive dictionary of medical treatment terms from randomized clinical trial (RCT) abstracts. We have investigated different methods of seeding, either with a seed pattern or seed instances (terms), and have compared different ranking methods for ranking extracted context patterns and instances. When used to identify treatment concepts from 100 randomly chosen, manually annotated RCT abstracts, our medical treatment dictionary shows better performance (precision:0.40, recall: 0.92 and F-measure: 0.54) over the most widely used manually created medical treatment terminology (precision: 0.41, recall: 0.52 and F-measure: 0.42). 
Abbreviations are common in biomedical documents and many are ambiguous in the sense that they have several potential expansions. Identifying the correct expansion is necessary for language understanding and important for applications such as document retrieval. Identifying the correct expansion can be viewed as a Word Sense Disambiguation (WSD) problem. A WSD system that uses a variety of knowledge sources, including two types of information speciﬁc to the biomedical domain, is also described. This system was tested on a corpus of ambiguous abbreviations, created by automatically identifying the correct expansion in Medline abstracts, and found to identify the correct expansion with up to 99% accuracy. 
In biomedical information extraction (IE), a central problem is the disambiguation of ambiguous names for domain speciﬁc entities, such as proteins, genes, etc. One important dimension of ambiguity is the organism to which the entities belong: in order to disambiguate an ambiguous entity name (e.g. a protein), it is often necessary to identify the speciﬁc organism to which it refers. In this paper we present an approach to the detection and disambiguation of the focus organism(s), i.e. the organism(s) which are the subject of the research described in scientiﬁc papers, which can then be used for the disambiguation of other entities. The results are evaluated against a gold standard derived from IntAct annotations. The evaluation suggests that the results may already be useful within a curation environment and are certainly a baseline for more complex approaches. 
Computing the semantic similarity between terms relies on existence and usage of semantic resources. However, these resources, often composed of equivalent units, or synonyms, must be ﬁrst analyzed and weighted in order to deﬁne within them the reliability zones where the semantic cohesiveness is stronger. We propose an original method for acquisition of elementary synonyms based on exploitation of structured terminologies, analysis of syntactic structure of complex (multi-unit) terms and their compositionality. The acquired synonyms are then proﬁled thanks to endogenous lexical and linguistic indicators (other types of relations, lexical inclusions, productivity), which are automatically inferred within the same terminologies. Additionally, synonymy relations are observed within graph, and its structure is analyzed. Particularly, we explore the usefulness of the graph theory notions such as connected component, clique, density, bridge, articulation vertex, and centrality of vertices. 
 In this paper we present an extractive system that au-  tomatically generates gene summaries from the biomed-  ical literature. The proposed text summarization system  selects and ranks sentences from multiple MEDLINE  abstracts by exploiting gene-specific information and  similarity relationships between sentences. We evaluate  our system on a large dataset of 7,294 human genes and  187,628 MEDLINE abstracts using Recall-Oriented  Understudy for Gisting Evaluation (ROUGE), a widely  used automatic evaluation metric in the text summariza-  tion community. Two baseline methods are used for  comparison. Experimental results show that our system  significantly outperforms the other two methods with  regard to all ROUGE metrics. A demo website of our  system  is  freely  accessible  at  http://60.195.250.72/onbires/summary.jsp.  
One of the most neglected areas of biomedical Text Mining (TM) is the development of systems based on carefully assessed user needs. We investigate the needs of an important task yet to be tackled by TM — Cancer Risk Assessment (CRA) — and take the ﬁrst step towards the development of TM for the task: identifying and organizing the scientiﬁc evidence required for CRA in a taxonomy. The taxonomy is based on expert annotation of 1297 MEDLINE abstracts. We report promising results with inter-annotator agreement tests and automatic classiﬁcation experiments, and a user test which demonstrates that the resources we have built are well-deﬁned, accurate, and applicable to a real-world CRA scenario. We discuss extending and reﬁning the taxonomy further via manual and machine learning approaches, and the subsequent steps required to develop TM for the needs of CRA. 
We introduce a controlled natural language for biomedical queries, called BIOQUERYCNL, and present an algorithm to convert a biomedical query in this language into a program in answer set programming (ASP)—a formal framework to automate reasoning about knowledge. BIOQUERYCNL allows users to express complex queries (possibly containing nested relative clauses and cardinality constraints) over biomedical ontologies; and such a transformation of BIOQUERYCNL queries into ASP programs is useful for automating reasoning about biomedical ontologies by means of ASP solvers. We precisely describe the grammar of BIOQUERYCNL, implement our transformation algorithm, and illustrate its applicability to biomedical queries by some examples. 
Medical concepts in clinical reports can be found with a high degree of variability of expression. Normalizing medical concepts to standardized vocabularies is a common way of accounting for this variability. One of the challenges in medical concept normalization is the difficulty in comparing two concepts which are orthographically different in representation but are identical in meaning. In this work we describe a method to compare medical phrases by utilizing the information found in syntactic dependencies. We collected a large corpus of radiology reports from our university medical center. A shallow semantic parser was used to identify anatomical phrases. We performed a series of transformations to convert the anatomical phrase into a normalized syntactic dependency representation. The new representation provides an easy intuitive way of comparing the phrases for the purpose of concept normalization. 
An important task in information retrieval is to identify sentences that contain important relationships between key concepts. In this work, we propose a novel approach to automatically extract sentence patterns that contain interactions involving concepts of molecular biology. A pattern is deﬁned in this work as a sequence of specialized Part-of-Speech (POS) tags that capture the structure of key sentences in the scientiﬁc literature. Each candidate sentence for the classiﬁcation task is encoded as a POS array and then aligned to a collection of pre-extracted patterns. The quality of the alignment is expressed as a pairwise alignment score. The most innovative component of this work is the use of a Genetic Algorithm (GA) to maximize the classiﬁcation performance of the alignment scoring scheme. The system achieves an F-score of 0.834 in identifying sentences which describe interactions between biological entities. This performance is mostly affected by the quality of the preprocessing steps such as term identiﬁcation and POS tagging. 
In the framework of contextual information retrieval in the biomedical domain, this paper reports on the automatic detection of disease concepts in two genres of biomedical text: sentences from the literature and PubMed user queries. A statistical model and a Natural Language Processing algorithm for disease recognition were applied on both corpora. While both methods show good performance (F=77% vs. F=76%) on the sentence corpus, results on the query corpus indicate that the statistical model is more robust (F=74% vs. F=70%). 
This paper introduces the task of automatically answering clinical comparison questions using MEDLINE® abstracts. In the beginning, clinical comparison questions and the main challenges in recognising and extracting their components are described. Then, different strategies for retrieving MEDLINE® abstracts are shown. Finally, the results of an initial experiment judging the relevance of MEDLINE® abstracts retrieved by searching for the components of twelve comparison questions will be shown and discussed. 
This paper compares domain-oriented and linguistically-oriented semantics, based on the GENIA event corpus and FrameNet. While the domain-oriented semantic structures are direct targets of Text Mining (TM), their extraction from text is not straghtforward due to the diversity of linguistic expressions. The extraction of linguistically-oriented semactics is more straghtforward, and has been studied independentely of speciﬁc domains. In order to ﬁnd a use of the domain-independent research achievements for TM, we aim at linking classes of the two types of semantics. The classes were connected by analyzing linguistically-oriented semantics of the expressions that mention one biological class. With the obtained relationship between the classes, we discuss a link between TM and linguistically-oriented semantics. 
Question answering is different from information retrieval in that it attempts to answer questions by providing summaries from numerous retrieved documents rather than by simply providing a list of documents that requires users to do additional work. However, the quality of answers that question answering provides has not been investigated extensively, and the practical approach to presenting question answers still needs more study. In addition to factoid answering using phrases or entities, most question answering systems use a sentence-based approach for generating answers. However, many sentences are often only meaningful or understandable in their context, and a passage-based presentation can often provide richer, more coherent context. However, passage-based presentations may introduce additional noise that places greater burden on users. In this study, we performed a quantitative evaluation on the two kinds of presentation produced by our online clinical question answering system, AskHERMES (http://www.AskHERMES.org). The overall finding is that, although irrelevant context can hurt the quality of an answer, the passage-based approach is generally more effective in that it provides richer context and matching across sentences. 
Historically, suicide risk assessment has relied on question-and-answer type tools. These tools, built on psychometric advances, are widely used because of availability. Yet there is no known tool based on biologic and cognitive evidence. This absence often cause a vexing clinical problem for clinicians who question the value of the result as time passes. The purpose of this paper is to describe one experiment in a series of experiments to develop a tool that combines Biological Markers (Bm) with Thought Markers (Tm), and use machine learning to compute a real-time index for assessing the likelihood repeated suicide attempt in the next six-months. For this study we focus using unsupervised machine learning to distinguish between actual suicide notes and newsgroups. This is important because it gives us insight into how well these methods discriminate between real notes and general conversation. 
With the rapidly growing use of electronic health records, the possibility of large-scale clinical information extraction has drawn much attention. It is not, however, easy to extract information because these reports are written in natural language. To address this problem, this paper presents a system that converts a medical text into a table structure. This system’s core technologies are (1) medical event recognition modules and (2) a negative event identification module that judges whether an event actually occurred or not. Regarding the latter module, this paper also proposes an SVM-based classifier using syntactic information. Experimental results demonstrate empirically that syntactic information can contribute to the method’s accuracy. 
In this paper we introduce a web application (SAPIENT) for sentence based annotation of full papers with semantic information. SAPIENT enables experts to annotate scientiﬁc papers sentence by sentence and also to link related sentences together, thus forming spans of interesting regions, which can facilitate text mining applications. As part of the system, we developed an XML-aware sentence splitter (SSSplit) which preserves XML markup and identiﬁes sentences through the addition of in-line markup. SAPIENT has been used in a systematic study for the annotation of scientiﬁc papers with concepts representing the Core Information about Scientiﬁc Papers (CISP) to create a corpus of 225 annotated papers. 
I describe a fast multilingual parser for semantic dependencies. The parser is implemented as a pipeline of linear classiﬁers trained with support vector machines. I use only ﬁrst order features, and no pair-wise feature combinations in order to reduce training and prediction times. Hyper-parameters are carefully tuned for each language and sub-problem. The system is evaluated on seven different languages: Catalan, Chinese, Czech, English, German, Japanese and Spanish. An analysis of learning rates and of the reliance on syntactic parsing quality shows that only modest improvements could be expected for most languages given more training data; Better syntactic parsing quality, on the other hand, could greatly improve the results. Individual tuning of hyper-parameters is crucial for obtaining good semantic parsing quality. 
I contend that the chief reason for this failure is that errors cascade and accumulate through a pipeline of naively chained components. For example, if we naively use the single most likely output of a part-of-speech tagger as the input to a syntactic parser, and those parse trees as the input to a coreference system, and so on, errors in each step will propagate to later ones: each components 90% accuracy multiplied through six components becomes only 53%. Consider, for instance, the sentence “I know you like your mother.” If a part-of-speech tagger deterministically labels “like” as a verb, then certain later syntactic and semantic analysis will be blocked from alternative interpretations, such as “I know you like your mother (does).” The part-of-speech tagger needs more syntactic and semantic information to make this choice. Consider also the classic example “The boy saw the man with the telescope.” No single correct syntactic parse of this sentence is possible in isolation. Correct interpretation requires the integration of these syntactic decisions with semantics and context. Humans manage and resolve ambiguity by uniﬁed, simultaneous consideration of morphology, syntax, semantics, pragmatics and other contextual information. In statistical modeling such uniﬁed  consideration is known as joint inference. The need for joint inference appears not only in natural language processing, but also in information integration, computer vision, robotics and elsewhere. All of these applications require integrating evidence from multiple sources, at multiple levels of abstraction. I believe that joint inference is one of the most fundamentally central issues in all of artiﬁcial intelligence. In this talk I will describe work in probabilistic models that perform joint inference across multiple components of an information processing pipeline in order to avoid the brittle accumulation of errors. I will survey work in exact inference, variational inference and Markov-chain Monte Carlo methods. We will discuss various approaches that have been applied to natural language processing, and hypothesize about why joint inference has helped in some cases, and not in others. I will then focus on our recent work at University of Massachusetts in large-scale conditional random ﬁelds with complex relational structure. In a single factor graph we seamlessly integrate multiple subproblems, using our new probabilistic programming language to compactly express complex, mutable variable-factor structure both in ﬁrst-order logic as well as in more expressive Turing-complete imperative procedures. We avoid unrolling this graphical model by using Markov-chain Monte Carlo for inference, and make inference more efﬁcient with learned proposal distributions. Parameter estimation is performed by SampleRank, which avoids complete inference as a subroutine by learning simply to correctly rank successive states of the Markovchain. Joint work with Aron Culotta, Michael Wick, Rob Hall, Khashayar Rohanimanesh, Karl Schultz, Sameer Singh, Charles Sutton and David Smith.  
How do children learn their ﬁrst words? I describe a model that makes joint inferences about what speakers are trying to talk about and the meanings of the words they use. This model provides a principled framework for integrating a wide variety of non-linguistic information sources into the process of word learning. Talk Pre´cis How do children learn their ﬁrst words? Much work in this ﬁeld has focused on the social aspects of word learning: that children make use of speakers’ intentions—as signaled by a wide range of non-linguistic cues such as their eye-gaze, what they are pointing at, or even what referents are new to them—to infer the meanings of words (Bloom, 2002). However, recent evidence has suggested that adults and children are able to learn words simply from the consistent co-occurrence of words and their referents, even across otherwise ambiguous situations and without explicit social cues as to which referent is being talked about (Yu & Smith 2007; Smith & Yu, 2008). In this talk I describe work aiming to combine these two sets of evidence within a single probablistic framework (Frank, Goodman, & Tenenbaum, 2009). We propose a model in which learners attempt to infer speakers’ moment-to-moment communicative intentions jointly with the meanings of the words they have used to express these intentions. This process of joint inference allows our model to  explain away two major sources of noise in simpler statistical word learning proposals: the fact that speakers do not talk about every referent and that not all words that speakers utter are referential. We ﬁnd that our model outperforms associative models in learning words accurately from natural corpus data and is able to ﬁt children’s behavior in a number of experimental results from developmental psychology. In addition, we have used this basic framework to begin investigating how learners use the rich variety of non-linguistic information signaling speakers’ intentions in service of word learning. As an example of this work, I will describe an extension of the model to use discourse continuity as a cue for speakers’ intentions. Acknowledgments This work supported by a Jacob Javits Graduate Fellowship and NSF Doctoral Dissertation Research Improvement Grant #0746251. References 
Creating large amounts of manually annotated training data for statistical parsers imposes heavy cognitive load on the human annotator and is thus costly and error prone. It is hence of high importance to decrease the human efforts involved in creating training data without harming parser performance. For constituency parsers, these efforts are traditionally evaluated using the total number of constituents (TC) measure, assuming uniform cost for each annotated item. In this paper, we introduce novel measures that quantify aspects of the cognitive efforts of the human annotator that are not reﬂected by the TC measure, and show that they are well established in the psycholinguistic literature. We present a novel parameter based sample selection approach for creating good samples in terms of these measures. We describe methods for global optimisation of lexical parameters of the sample based on a novel optimisation problem, the constrained multiset multicover problem, and for cluster-based sampling according to syntactic parameters. Our methods outperform previously suggested methods in terms of the new measures, while maintaining similar TC performance. 
Finding negation signals and their scope in text is an important subtask in information extraction. In this paper we present a machine learning system that ﬁnds the scope of negation in biomedical texts. The system combines several classiﬁers and works in two phases. To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus representing different text types. It achieves the best results to date for this task, with an error reduction of 32.07% compared to current state of the art results. 
The combination of Support Vector Machines with very high dimensional kernels, such as string or tree kernels, suffers from two major drawbacks: ﬁrst, the implicit representation of feature spaces does not allow us to understand which features actually triggered the generalization; second, the resulting computational burden may in some cases render unfeasible to use large data sets for training. We propose an approach based on feature space reverse engineering to tackle both problems. Our experiments with Tree Kernels on a Semantic Role Labeling data set show that the proposed approach can drastically reduce the computational footprint while yielding almost unaffected accuracy. 
 Active Learning Curve (F Measure vs Number of Annotations) 90  A survey of existing methods for stopping active learning (AL) reveals the needs for methods that are: more widely applicable; more aggressive in saving annotations; and more stable across changing datasets. A new method for stopping AL based on stabilizing predictions is presented that addresses these needs. Furthermore, stopping methods are required to handle a broad range of different annotation/performance tradeoff valuations. Despite this, the existing body of work is dominated by conservative methods with little (if any) attention paid to providing users with control over the behavior of stopping methods. The proposed method is shown to ﬁll a gap in the level of aggressiveness available for stopping AL and supports providing users with control over stopping behavior. 
Sets of lexical items sharing a signiﬁcant aspect of their meaning (concepts) are fundamental for linguistics and NLP. Unsupervised concept acquisition algorithms have been shown to produce good results, and are preferable over manual preparation of concept resources, which is labor intensive, error prone and somewhat arbitrary. Some existing concept mining methods utilize supervised language-speciﬁc modules such as POS taggers and computationally intensive parsers. In this paper we present an efﬁcient fully unsupervised concept acquisition algorithm that uses syntactic information obtained from a fully unsupervised parser. Our algorithm incorporates the bracketings induced by the parser into the meta-patterns used by a symmetric patterns and graph-based concept discovery algorithm. We evaluate our algorithm on very large corpora in English and Russian, using both human judgments and WordNetbased evaluation. Using similar settings as the leading fully unsupervised previous work, we show a signiﬁcant improvement in concept quality and in the extraction of multiword expressions. Our method is the ﬁrst to use fully unsupervised parsing for unsupervised concept discovery, and requires no languagespeciﬁc tools or pattern/word seeds. 
Vector space models of word meaning typically represent the meaning of a word as a vector computed by summing over all its corpus occurrences. Words close to this point in space can be assumed to be similar to it in meaning. But how far around this point does the region of similar meaning extend? In this paper we discuss two models that represent word meaning as regions in vector space. Both representations can be computed from traditional point representations in vector space. We ﬁnd that both models perform at over 95% F-score on a token classiﬁcation task. 
Specifying an appropriate feature space is an important aspect of achieving good performance when designing systems based upon learned classiﬁers. Effectively incorporating information regarding semantically related words into the feature space is known to produce robust, accurate classiﬁers and is one apparent motivation for efforts to automatically generate such resources. However, naive incorporation of this semantic information may result in poor performance due to increased ambiguity. To overcome this limitation, we introduce the interactive feature space construction protocol, where the learner identiﬁes inadequate regions of the feature space and in coordination with a domain expert adds descriptiveness through existing semantic resources. We demonstrate effectiveness on an entity and relation extraction system including both performance improvements and robustness to reductions in annotated data. 
In this paper we address the problem of identifying reciprocal relationships in English. In particular we introduce an algorithm that semi-automatically discovers patterns encoding reciprocity based on a set of simple but effective pronoun templates. Using a set of most frequently occurring patterns, we extract pairs of reciprocal pattern instances by searching the web. Then we apply two unsupervised clustering procedures to form meaningful clusters of such reciprocal instances. The pattern discovery procedure yields an accuracy of 97%, while the clustering procedures indicate accuracies of 91% and 82%. Moreover, the resulting set of 10,882 reciprocal instances represent a broad-coverage resource. 
Theories of human language acquisition assume that learning to understand sentences is a partially-supervised task (at best). Instead of using ‘gold-standard’ feedback, we train a simpliﬁed “Baby” Semantic Role Labeling system by combining world knowledge and simple grammatical constraints to form a potentially noisy training signal. This combination of knowledge sources is vital for learning; a training signal derived from a single component leads the learner astray. When this largely unsupervised training approach is applied to a corpus of child directed speech, the BabySRL learns shallow structural cues that allow it to mimic striking behaviors found in experiments with children and begin to correctly identify agents in a sentence. 
We propose a novel machine learning task that consists in learning to predict which words in a text are ﬁxated by a reader. In a ﬁrst pilot experiment, we show that it is possible to outperform a majority baseline using a transitionbased model with a logistic regression classiﬁer and a very limited set of features. We also show that the model is capable of capturing frequency effects on eye movements observed in human readers. 
Recent advances in statistical machine translation have used beam search for approximate NP-complete inference within probabilistic translation models. We present an alternative approach of sampling from the posterior distribution deﬁned by a translation model. We deﬁne a novel Gibbs sampler for sampling translations given a source sentence and show that it effectively explores this posterior distribution. In doing so we overcome the limitations of heuristic beam search and obtain theoretically sound solutions to inference problems such as ﬁnding the maximum probability translation and minimum expected risk training and decoding. 
In this paper we investigate the task of automatic generation of slide presentations from academic papers, focusing initially on slide to paper alignment. We compare and evaluate four different alignment systems which utilize various combinations of methods used widely in other alignment and question answering approaches, such as TF-IDF term weighting and query expansion. Our best aligner achieves an accuracy of 75% and our ﬁndings show that for this application, average TF-IDF scoring performs more poorly than a simpler method based on the number of matched terms, and query expansion degrades aligner performance. 
English pronouns like he and they reliably reﬂect the gender and number of the entities to which they refer. Pronoun resolution systems can use this fact to ﬁlter noun candidates that do not agree with the pronoun gender. Indeed, broad-coverage models of noun gender have proved to be the most important source of world knowledge in automatic pronoun resolution systems. Previous approaches predict gender by counting the co-occurrence of nouns with pronouns of each gender class. While this provides useful statistics for frequent nouns, many infrequent nouns cannot be classiﬁed using this method. Rather than using co-occurrence information directly, we use it to automatically annotate training examples for a large-scale discriminative gender model. Our model collectively classiﬁes all occurrences of a noun in a document using a wide variety of contextual, morphological, and categorical gender features. By leveraging large volumes of unlabeled data, our full semi-supervised system reduces error by 50% over the existing stateof-the-art in gender classiﬁcation. 
This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses. We introduce a dependency-based context model that incorporates long-range dependencies, variable context sizes, and reordering. It provides a 16% relative improvement over the baseline approach that uses a ﬁxed context window of adjacent words. Its Top 10 accuracy for noun translation is higher than that of a statistical translation model trained on a Spanish-English parallel corpus containing 100,000 sentence pairs. We generalize the evaluation to other word-types, and show that the performance can be increased to 18% relative by preserving part-of-speech equivalencies during translation. 
As supervised machine learning methods are increasingly used in language technology, the need for high-quality annotated language data becomes imminent. Active learning (AL) is a means to alleviate the burden of annotation. This paper addresses the problem of knowing when to stop the AL process without having the human annotator make an explicit decision on the matter. We propose and evaluate an intrinsic criterion for committee-based AL of named entity recognizers. 
The average results obtained by unsupervised statistical parsers have greatly improved in the last few years, but on many speciﬁc sentences they are of rather low quality. The output of such parsers is becoming valuable for various applications, and it is radically less expensive to create than manually annotated training data. Hence, automatic selection of high quality parses created by unsupervised parsers is an important problem. In this paper we present PUPA, a POS-based Unsupervised Parse Assessment algorithm. The algorithm assesses the quality of a parse tree using POS sequence statistics collected from a batch of parsed sentences. We evaluate the algorithm by using an unsupervised POS tagger and an unsupervised parser, selecting high quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 
Clustering is crucial for many NLP tasks and applications. However, evaluating the results of a clustering algorithm is hard. In this paper we focus on the evaluation setting in which a gold standard solution is available. We discuss two existing information theory based measures, V and VI, and show that they are both hard to use when comparing the performance of different algorithms and different datasets. The V measure favors solutions having a large number of clusters, while the range of scores given by VI depends on the size of the dataset. We present a new measure, NVI, which normalizes VI to address the latter problem. We demonstrate the superiority of NVI in a large experiment involving an important NLP application, grammar induction, using real corpus data in English, German and Chinese. 
We compare two different types of extraction patterns for automatically deriving semantic information from text: lexical patterns, built from words and word class information, and dependency patterns with syntactic information obtained from a full parser. We are particularly interested in whether the richer linguistic information provided by a parser allows for a better performance of subsequent information extraction work. We evaluate automatic extraction of hypernym information from text and conclude that the application of dependency patterns does not lead to substantially higher precision and recall scores than using lexical patterns. 
 the viability of database searching by visible light  Experimenting with different mathematical objects for text representation is an important step of building text classiﬁcation models. In order to be efﬁcient, such objects of a formal model, like vectors, have to reasonably reproduce language-related phenomena such as word meaning inherent in index terms. We in-  using a quantum algorithm, albeit on meaningless items. The question was, what kind of document representation would be necessary to extend their in-principle results to include semantics, one that has been leading us to test both periodic and nonperiodic functions for this purpose. Since representation and retrieval by colors was implied in their  troduce an algorithm for sense-based semantic ordering of index terms which approximates Cruse’s description of a sense spectrum. Following semantic ordering, text classiﬁcation by support vector machines can beneﬁt from semantic smoothing kernels that regard semantic relations among index terms while computing document similarity. Adding expansion terms to the vector representation can also improve effectiveness. This paper proposes a new kernel which discounts less important expansion terms based on lexical relatedness.  method, we speculated that the following components could be useful in a rephrased model: (a) a metaphorically presented spectral expression of lexical semantic phenomena, (b) a ranked onedimensional condensate of multidimensional sense structure, and (c) representation of documents and queries by functions in L2 space with a similarity measure. Our anticipation was that by matching these components, a new model could demonstrate new capacities in general, and contribute to computing meaning by waves in particular. Semantic ordering (component b) is an approxi-  
This paper is concerned with statistical methods for treating long-distance dependencies. We focus in particular on a case of substantial recent interest: that of long-distance dependency effects in entity extraction. We introduce a new approach to capturing these effects through a simple feature copying preprocess, and demonstrate substantial performance gains on several entity extraction tasks. 
We present a kernel-based approach for ﬁnegrained classiﬁcation of named entities. The only training data for our algorithm is a few manually annotated entities for each class. We deﬁned kernel functions that implicitly map entities, represented by aggregating all contexts in which they occur, into a latent semantic space derived from Wikipedia. Our method achieves a signiﬁcant improvement over the state of the art for the task of populating an ontology of people, although requiring considerably less training instances than previous approaches. 
This paper presents a method for automatic topic identiﬁcation using an encyclopedic graph derived from Wikipedia. The system is found to exceed the performance of previously proposed machine learning algorithms for topic identiﬁcation, with an annotation consistency comparable to human annotations. 
Many applications in the context of natural language processing or information retrieval may be largely improved if they were able to fully exploit the rich semantic information annotated in high-quality, publicly available resources such as the FrameNet and the WordNet databases. Nevertheless, the practical use of similar resources is often biased by the limited coverage of semantic phenomena that they provide. A natural solution to this problem would be to automatically establish anchors between these resources that would allow us 1) to jointly use the encoded information, thus possibly overcoming limitations of the individual corpora, and 2) to extend each resource coverage by exploiting the information encoded in the others. In this paper, we present a supervised learning framework for the mapping of FrameNet lexical units onto WordNet synsets based on a reduced set of novel and semantically rich features. The automatically learnt mapping, which we call MapNet, can be used 1) to extend frame sets in the English FrameNet, 2) to populate frame sets in the Italian FrameNet via MultiWordNet and 3) to add frame labels to the MultiSemCor corpus. Our evaluation on these tasks shows that the proposed approach is viable and can result in accurate automatic annotations. 
We discuss a cue-based grammar induction approach based on a parallel theory of grammar. Our model is based on the hypotheses of interdependency between linguistic levels (of representation) and inductability of speciﬁc structural properties at a particular level, with consequences for the induction of structural properties at other linguistic levels. We present the results of three different cue-learning experiments and settings, covering the induction of phonological, morphological, and syntactic properties, and discuss potential consequences for our general grammar induction model.1 
This paper presents a model-based approach to dialogue management that is guided by data-driven dialogue act prediction. The statistical prediction is based on stochastic context-free grammars that have been obtained by means of grammatical inference. The prediction performance of the method compares favourably to that of a heuristic baseline and to that of n-gram language models. The act prediction is explored both for dialogue acts without realised semantic content (consisting only of communicative functions) and for dialogue acts with realised semantic content. 
The phenomenon of meaning-preserving corrections given by an adult to a child involves several aspects: (1) the child produces an incorrect utterance, which the adult nevertheless understands, (2) the adult produces a correct utterance with the same meaning and (3) the child recognizes the adult utterance as having the same meaning as its previous utterance, and takes that as a signal that its previous utterance is not correct according to the adult grammar. An adequate model of this phenomenon must incorporate utterances and meanings, account for how the child and adult can understand each other’s meanings, and model how meaning-preserving corrections interact with the child’s increasing mastery of language production. In this paper we are concerned with how a learner who has learned to comprehend utterances might go about learning to produce them. We consider a model of language comprehension and production based on ﬁnite sequential and subsequential transducers. Utterances are modeled as ﬁnite sequences of words and meanings as ﬁnite sequences of predicates. Comprehension is interpreted as a mapping of utterances to meanings and production as a mapping of meanings to utterances. Previous work (Castellanos et al., 1993; Pieraccini et al., 1993) has applied subsequential transducers and the OSTIA algorithm to the problem of learning to comprehend language; here we apply them to the problem of learning to produce language. For ten natural languages and a limited domain of geometric shapes and their properties and rela-  tions we deﬁne sequential transducers to produce pairs consisting of an utterance in that language and its meaning. Using this data we empirically explore the properties of the OSTIA and DD-OSTIA algorithms for the tasks of learning comprehension and production in this domain, to assess whether they may provide a basis for a model of meaning-preserving corrections. 
 GREAT is a ﬁnite-state toolkit which is devoted to Machine Translation and that learns structured models from bilingual data. The training procedure is based on grammatical inference techniques to obtain stochastic transducers that model both the structure of the languages and the relationship between them. The inference of grammars from natural language causes the models to become larger when a less restrictive task is involved; even more if a bilingual modelling is being considered. GREAT has been successful to implement the GIATI learning methodology, using different scalability issues to be able to deal with corpora of high volume of data. This is reported with experiments on the EuroParl corpus, which is a state-of-theart task in Statistical Machine Translation.  
Contextual Binary Feature Grammars were recently proposed by (Clark et al., 2008) as a learnable representation for richly structured context-free and context sensitive languages. In this paper we examine the representational power of the formalism, its relationship to other standard formalisms and language classes, and its appropriateness for modelling natural language. 
The problem of identifying and correcting confusibles, i.e. context-sensitive spelling errors, in text is typically tackled using speciﬁcally trained machine learning classiﬁers. For each different set of confusibles, a speciﬁc classiﬁer is trained and tuned. In this research, we investigate a more generic approach to context-sensitive confusible correction. Instead of using speciﬁc classiﬁers, we use one generic classiﬁer based on a language model. This measures the likelihood of sentences with different possible solutions of a confusible in place. The advantage of this approach is that all confusible sets are handled by a single model. Preliminary results show that the performance of the generic classiﬁer approach is only slightly worse that that of the speciﬁc classiﬁer approach. 
Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference. However, these properties do not state the maximal performance they can achieve when they are evaluated against a gold treebank that is not produced by an UNTS grammar. In this paper we investigate such an upper bound. We develop a method to ﬁnd an upper bound for the unlabeled F 1 performance that any UNTS grammar can achieve over a given treebank. Our strategy is to characterize all possible versions of the gold treebank that UNTS grammars can produce and to ﬁnd the one that optimizes a metric we deﬁne. We show a way to translate this score into an upper bound for the F 1. In particular, we show that the F 1 parsing score of any UNTS grammar can not be beyond 82.2% when the gold treebank is the WSJ10 corpus. 
In this paper, I show that a problem of learning a morphological paradigm is similar to a problem of learning a partition of the space of Boolean functions. I describe several learners that solve this problem in different ways, and compare their basic properties. 
 Mother: Not a birdie, a seal.  This paper introduces a formal view of the semantics and pragmatics of corrective feedback in dialogues between adults and children. The goal of this research is to give a formal account of language coordination in dialogue, and semantic coordination in particular. Accounting for semantic coordination requires (1) a semantics, i.e. an architecture allowing for dynamic meanings and meaning updates as results of dialogue moves, and (2) a pragmatics, describing the dialogue moves involved in semantic coordination. We illustrate the general approach by applying it to some examples from the literature on corrective feedback, and provide a fairly detailed discussion of one example using TTR (Type Theory with Records) to formalize concepts. TTR provides an analysis of linguistic content which is structured in order to allow modiﬁcation and similarity metrics, and a framework for describing dialogue moves and resulting updates to linguistic resources. 
A large number of computational language learners have been proposed for modelling the process of child language acquisition. Com­ paring them, however, can be difficult due to the different assumptions that they make, the diverse test results presented, and the different linguistic behaviours investigated. This paper introduces a toolkit that allows different lan­ guage learners to be trained, tested and ana­ lysed under standardised conditions. The res­ ults can be easily compared with one another and with typical child language development to highlight the relative advantages and disad­ vantages of learners. 
In this paper we present the ﬁrst step in a larger series of experiments for the induction of predicate/argument structures. The structures that we are inducing are very similar to the conceptual structures that are used in Frame Semantics (such as FrameNet). Those structures are called messages and they were previously used in the context of a multi-document summarization system of evolving events. The series of experiments that we are proposing are essentially composed from two stages. In the ﬁrst stage we are trying to extract a representative vocabulary of words. This vocabulary is later used in the second stage, during which we apply to it various clustering approaches in order to identify the clusters of predicates and arguments—or frames and semantic roles, to use the jargon of Frame Semantics. This paper presents in detail and evaluates the ﬁrst stage. 
Indirect negative evidence is clearly an important way for learners to constrain overgeneralisation, and yet a good learning theoretic analysis has yet to be provided for this, whether in a PAC or a probabilistic identiﬁcation in the limit framework. In this paper we suggest a theoretical analysis of indirect negative evidence that allows the presence of ungrammatical strings in the input and also accounts for the relationship between grammaticality/acceptability and probability. Given independently justiﬁed assumptions about lower bounds on the probabilities of grammatical strings, we establish that a limited number of membership queries of some strings can be probabilistically simulated. 
Building on the use of local contexts, or frames, for human category acquisition, we explore the treatment of contexts as categories. This allows us to examine and evaluate the categorical properties that local unsupervised methods can distinguish and their relationship to corpus POS tags. From there, we use lexical information to combine contexts in a way which preserves the intended category, providing a platform for grammatical category induction. 
In this paper, we attempt to explain the emergence of the linguistic diversity that exists across the consonant inventories of some of the major language families of the world through a complex network based growth model. There is only a single parameter for this model that is meant to introduce a small amount of randomness in the otherwise preferential attachment based growth process. The experiments with this model parameter indicates that the choice of consonants among the languages within a family are far more preferential than it is across the families. Furthermore, our observations indicate that this parameter might bear a correlation with the period of existence of the language families under investigation. These ﬁndings lead us to argue that preferential attachement seems to be an appropriate high level abstraction for language acquisition and change. 
This paper describes a model that has been developed in the Turgama Project at Leiden University to meet the challenges encountered in the computational analysis of ancient Syriac Biblical manuscripts. The small size of the corpus, the absence of native speakers, and the variation attested in the multitude of textual witnesses require a model of encoding—rather than tagging—that moves from the formal distributional registration of linguistic elements to functional deductions. The model is illuminated by an example from verb inflection. It shows how a corpus-based analysis can improve upon the inflectional paradigms given in traditional grammars and how the various orthographic representations can be accounted for by an encoding system that registers both the paradigmatic forms and their attested realizations. 
Karamel is a system for ﬁnite-state morphology which is multi-tape and uses a typed Cartesian product to relate tapes in a structured way. It implements statically compiled feature structures. Its language allows the use of regular expressions and Generalized Restriction rules to deﬁne multi-tape transducers. Both simultaneous and successive application of local constraints are possible. This system is interesting for describing rich and structured morphologies such as the morphology of Semitic languages. 
Various methods have been devised to produce morphological analyzers and generators for Semitic languages, ranging from methods based on widely used ﬁnitestate technologies to very speciﬁc solutions designed for a speciﬁc language or problem. Since the earliest proposals of how to adopt the elsewhere successful ﬁnite-state methods to root-andpattern morphologies, the solution of encoding Semitic grammars using multi-tape automata has resurfaced on a regular basis. Multi-tape automata, however, require speciﬁc algorithms and reimplementation of ﬁnite-state operators across the board, and hence such technology has not been readily available to linguists. This paper, using an actual Arabic grammar as a case study, describes an approach to encoding multi-tape automata on a single tape that can be implemented using any standard ﬁnite-automaton toolkit. 
Modern standard Arabic is usually written without diacritics. This makes it difficult for performing Arabic text processing. Diacritization helps clarify the meaning of words and disambiguate any vague spellings or pronunciations, as some Arabic words are spelled the same but differ in meaning. In this paper, we address the issue of adding diacritics to undiacritized Arabic text using a hybrid approach. The approach requires an Arabic lexicon and large corpus of fully diacritized text for training purposes in order to detect diacritics. CaseEnding is treated as a separate post processing task using syntactic information. The hybrid approach relies on lexicon retrieval, bigram, and SVM-statistical prioritized techniques. We present results of an evaluation of the proposed diacritization approach and discuss various modifications for improving the performance of this approach. 
Fully unsupervised pattern-based methods for discovery of word categories have been proven to be useful in several languages. The majority of these methods rely on the existence of function words as separate text units. However, in morphology-rich languages, in particular Semitic languages such as Hebrew and Arabic, the equivalents of such function words are usually written as morphemes attached as preﬁxes to other words. As a result, they are missed by word-based pattern discovery methods, causing many useful patterns to be undetected and a drastic deterioration in performance. To enable high quality lexical category acquisition, we propose a simple unsupervised word segmentation algorithm that separates these morphemes. We study the performance of the algorithm for Hebrew and Arabic, and show that it indeed improves a state-of-art unsupervised concept acquisition algorithm in Hebrew. 
A number of papers have reported on methods for the automatic acquisition of large-scale, probabilistic LFG-based grammatical resources from treebanks for English (Cahill and al., 2002), (Cahill and al., 2004), German (Cahill and al., 2003), Chinese (Burke, 2004), (Guo and al., 2007), Spanish (O’Donovan, 2004), (Chrupala and van Genabith, 2006) and French (Schluter and van Genabith, 2008). Here, we extend the LFG grammar acquisition approach to Arabic and the Penn Arabic Treebank (ATB) (Maamouri and Bies, 2004), adapting and extending the methodology of (Cahill and al., 2004) originally developed for English. Arabic is challenging because of its morphological richness and syntactic complexity. Currently 98% of ATB trees (without FRAG and X) produce a covering and connected f-structure. We conduct a qualitative evaluation of our annotation against a gold standard and achieve an f-score of 95%. 
The Arabic language is a collection of multiple variants, among which Modern Standard Arabic (MSA) has a special status as the formal written standard language of the media, culture and education across the Arab world. The other variants are informal spoken dialects that are the media of communication for daily life. Arabic dialects differ substantially from MSA and each other in terms of phonology, morphology, lexical choice and syntax. In this paper, we describe a system that automatically identiﬁes the Arabic dialect (Gulf, Iraqi, Levantine, Egyptian and MSA) of a speaker given a sample of his/her speech. The phonotactic approach we use proves to be effective in identifying these dialects with considerable overall accuracy — 81.60% using 30s test utterances. 
We investigate syntactic reordering within an English to Arabic translation task. We extend a pre-translation syntactic reordering approach developed on a close language pair (English-Danish) to the distant language pair, English-Arabic. We achieve signiﬁcant improvements in translation quality over related approaches, measured by manual as well as automatic evaluations. These results prove the viability of this approach for distant languages. 
We describe the Lwazi corpus for automatic speech recognition (ASR), a new telephone speech corpus which includes data from nine Southern Bantu languages. Because of practical constraints, the amount of speech per language is relatively small compared to major corpora in world languages, and we report on our investigation of the stability of the ASR models derived from the corpus. We also report on phoneme distance measures across languages, and describe initial phone recognisers that were developed using this data. 
Research in data-driven methods for Machine Translation has greatly beneﬁted from the increasing availability of parallel corpora. Processing the same text in two different languages yields useful information on how words and phrases are translated from a source language into a target language. To investigate this, a parallel corpus is typically aligned by linking linguistic tokens in the source language to the corresponding units in the target language. An aligned parallel corpus therefore facilitates the automatic development of a machine translation system and can also bootstrap annotation through projection. In this paper, we describe data collection and annotation efforts and preliminary experimental results with a parallel corpus English - Swahili. 
In this paper, we describe tools and resources for the study of African languages developed at the Collaborative Research Centre “Information Structure”. These include deeply annotated data collections of 25 subsaharan languages that are described together with their annotation scheme, and further, the corpus tool ANNIS that provides a unified access to a broad variety of annotations created with a range of different tools. With the application of ANNIS to several African data collections, we illustrate its suitability for the purpose of language documentation, distributed access and the creation of data archives. 
We demonstrate the use of default default inheritance hierarchies to represent the morphology of Yoru`ba´ verbs in the KATR formalism, treating inﬂectional exponences as markings associated with the application of rules by which complex word forms are deduced from simpler roots or stems. In particular, we suggest a scheme of slots that together make up a verb and show how each slot represents a subset of the morphosyntactic properties associated with the verb. We also show how we can account for the tonal aspects of Yoru`ba´, in particular, the tone associated with the emphatic ending. Our approach allows linguists to gain an appreciation for the structure of verbs, gives teachers a foundation for organizing lessons in morphology, and provides students a technique for generating forms of any verb. 
South African languages (and indigenous African languages in general) lag behind other languages in terms of the availability of linguistic resources. Efforts to improve or fasttrack the development of linguistic resources are required to bridge this ever-increasing gap. In this paper we emphasize the advantages of technology transfer between two languages to advance an existing linguistic technology/resource. The advantages of technology transfer are illustrated by showing how an existing lemmatiser for Setswana can be improved by applying a methodology that was first used in the development of a lemmatiser for Afrikaans.  information about lemmatisation. Section 3 provides specific information about lemmatisation and the concept of a lemma in Setswana. Section 4 describes previous work on lemmatisation in Afrikaans. Section 5 gives an overview of memory based learning (the machine learning techniques used in this study) and the generic architecture developed for machine learning based lemmatisation. Data requirements and the data preparation process are discussed in Section 6. The implementation of a machine learning based lemmatiser for Setswana is explained in Section 7, while some concluding remarks and future directions are provided in Section 8. 2 Lemmatisation  
A major obstacle to part-of-speech (=POS) tagging of Northern Sotho (Bantu, S 32) are ambiguous function words. Many are highly polysemous and very frequent in texts, and their local context is not always distinctive. With certain taggers, this issue leads to comparatively poor results (between 88 and 92 % accuracy), especially when sizeable tagsets (over 100 tags) are used. We use the RF-tagger (Schmid and Laws, 2008), which is particularly designed for the annotation of ﬁne-grained tagsets (e.g. including agreement information), and we restructure the 141 tags of the tagset proposed by Taljard et al. (2008) in a way to ﬁt the RF tagger. This leads to over 94 % accuracy. Error analysis in addition shows which types of phenomena cause trouble in the POS-tagging of Northern Sotho. 
This paper presents a speech synthesis system for Amharic language and describes and how the important prosodic features of the language were modeled in the system. The developed Amharic Text-to-Speech system (AmhTTS) is parametric and rule-based that employs a cepstral method. The system uses a source filter model for speech production and a Log Magnitude Approximation (LMA) filter as the vocal tract filter. The intelligibility and naturalness of the system was evaluated by word and sentence listening tests respectively and we achieved 98% correct-rates for words and an average Mean Opinion Score (MOS) of 3.2 (which is categorized as good) for sentences listening tests. The synthesized speech has high intelligibility and moderate naturalness. Comparing with previous similar study, our system produced considerably similar quality speech with a fairly good prosody. In particular our system is mainly suitable for building new languages with little modification. 
The development of Human Language Technology (HLT) is one of the important by-products of the information revolution. However, the level of knowledge and skills in HLT for African languages remain unfortunately low as most scholars continue to work within the frameworks of knowledge production for an industrial society while the information age dawns. This paper reports the work of African Languages Technology Initiative (Alt-i) over a five-year period, and thereby presents a proposal for the acceleration of the development of knowledge and skills in HLT for African languages. 
This paper describes sociological fieldwork conducted in the autumn of 2008 in eleven rural communities of South Africa. The goal of the fieldwork was to evaluate the potential role of automated telephony services in improving access to important government information and services. Our interviews, focus group discussions and surveys revealed that Lwazi, a telephone-based spoken dialog system, could greatly support current South African government efforts to effectively connect citizens to available services, provided such services be toll free, in local languages, and with content relevant to each community. 
Setswana, a Bantu language in the Sotho group, is one of the eleven official languages of South Africa. The language is characterised by a disjunctive orthography, mainly affecting the important word category of verbs. In particular, verbal prefixal morphemes are usually written disjunctively, while suffixal morphemes follow a conjunctive writing style. Therefore, Setswana tokenisation cannot be based solely on whitespace, as is the case in many alphabetic, segmented languages, including the conjunctively written Nguni group of South African Bantu languages. This paper shows how a combination of two tokeniser transducers and a finite-state (rule-based) morphological analyser may be combined to effectively solve the Setswana tokenisation problem. The approach has the important advantage of bringing the processing of Setswana beyond the morphological analysis level in line with what is appropriate for the Nguni languages. This means that the challenge of the disjunctive orthography is met at the tokenisation/morphological analysis level and does not in principle propagate to subsequent levels of analysis such as POS tagging and shallow parsing, etc. Indeed, the approach ensures that an aspect such as orthography does not obfuscate sound linguistics and, ultimately, proper semantic analysis, which remains the ultimate aim of linguistic analysis and therefore also computational linguistic analysis.  
In a manuscript William Labov (1987) states that although linguistics is a ﬁeld with a long historical tradition and with a high degree of consensus on basic categories, it experiences a fundamental devision concerning the role that quantitative methods should play as part of the research progress. Linguists differ in the role they assign to the use of natural language examples in linguistic research and in the publication of its results. In this paper we suggest that the general availability of richly annotated, multi-lingual data directly suited for scientiﬁc publications could have a positive impact on the way we think about language, and how we approach linguistics.We encourage the systematic generation of linguistic data beyond what emerges from ﬁeldwork and other descriptive studies and introduce an online glossing tool for textual data annotation. We argue that the availability of such an online tool will facilitate the generation of in-depth annotated linguistic examples as part of linguistic research. This in turn will allow the build-up of linguistic resources which can be used independent of the research focus and of the theoretical framework applied. The tool we would like to present is a non-expert-user system designed in particular for the work with lesser documented languages. It has been used for the documentation of several African languages, and has served for two projects involving universities in Africa. 
We present the Tamajaq language and the dictionary we used as main linguistic resource in the two first parts. The third part details the complex morphology of this language. In the part 4 we describe the conversion of the dictionary into electronic form, the inflectional rules we wrote and their implementation in the Nooj software. Finally we present a plan for our future work. 1. The Tamajaq language 1.1 Socio-linguistic situation In Niger, the official language is French and there are eleven national languages. Five are taught in a experimental schools: Fulfulde, Hausa, Kanuri, Tamajaq and Soŋay-Zarma. According to the last census in 1998, the Tamajaq language is spoken by 8,4% of the 13.5 million people who live in Niger. This language is also spoken in Mali, Burkina-Faso, Algeria and Libya. It is estimated there are around 5 millions Tamajaq-speakers around the world. The Tamacheq language belongs to the group of Berber languages. 1.2 Tamajaq alphabet The Tamajaq alphabet used in Niger (Republic of Niger, 1999) uses 41 characters, 14 with diacritical marks that all figure in the Unicode standard (See appendix A). There are 12 vowels: a, â, ă, ə, e, ê, i, î, o, ô, u, û.  1.3 Articulary phonetics  Consonants  Voiceless Voiced  Bilabial Plosive  b  Nasal  m  Trill  r  Semivowel  w  Labiodental Fricative f  Dental  Plosive  t  d  Fricative s  z  Nasal  n  Lateral  l  Pharyngeal Plosive  ṭ  ḍ  Fricative  ṣ  ẓ  Lateral  ḷ  Palatal  Plosive  c  ǰ  Fricative š  j  Semivowel  y  Velar  Plosive  k  g, ğ  Fricative  ɣ  x  Nasal  ŋ  Glottal  Plosive  q  Fricative h Table 1a: Articulary phonetics of Tamajaq consonants  Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages – AfLaT 2009, pages 81–88, Athens, Greece, 31 March 2009. c 2009 Association for Computational Linguistics 81  Vowels Close Close-mid Open-mid Open  Palatal i  e  Central  ə  a  a  Labial u  o  Table 1b: Articulary phonetics of Tamajaq vowels  1.4 Tools on computers There are no specific TALN tools for the Tamajaq language. However characters can be easily typed on French keyboards thanks to the AFRO keyboard layout (Enguehard and al. 2008). 2 Lexicographic resources We use the school editorial dictionary "dictionnaire Tamajaq-français destiné à l'enseignement du cycle de base 1". It was written by the SOUTEBA1 project of the DED2 organisation in 2006. Because it targets children, this dictionary consists only of 5,390 entries. Words have been chosen by compiling school books. 2.1 Structure of an entry Each entry generally details : - lemma, - lexical category, - translation in French, - an example, - gender (for nouns), - plural form (for nouns). Examples: « ăbada1: sn. bas ventre. Daw tǝdist. Bărar wa yǝllûẓăn ad t-yǝltǝɣ ăbada-net. tǝmust.: yy. igǝt: ibadan. » « ăbada2: sn. flanc. Tasăga meɣ daw ădăg ǝyyăn. Imǝwwǝẓla ǝklăn dăɣ ăbada n ǝkašwar. Anammelu.: azador. tǝmust.: yy. Ǝsǝfsǝs.: ă. Igǝt: ibadan. » Homonyms are described in different entries and followed by a number, as in the above example.  2.2 Lexical categories The linguistic terms used in the dictionary are written in the Tamajaq language using the abbre- 1Soutien à l'éducation de base. 2DED: Deutscher Entwicklungsdienst.  viations presented in table 2. In addition, this table gives information about the number of entries of each lexical category.  Lexical category  Tamajaq  English  Abbrevi- Number ation of entries  əḍəkuḍ  number  ḍkḍ.  3  ənalkam  deteminant nlkm.  
We report on a project which we believe to have the potential to become home to, among others, bilingual dictionaries for African languages. Kept in a well-structured XML format with several possible degrees of conformance, the dictionaries will be able to get usable even in their early versions, which will be then subject to supervised improvement as user feedback accumulates. The project is FreeDict, part of SourceForge, a well-known Internet repository of open source content. We demonstrate a possible process of dictionary development on the example of one of FreeDict dictionaries, a Swahili-English dictionary that we maintain and have been developing through subsequent stages of increasing complexity and machineprocessability. The aim of the paper is to show that even a small bilingual lexical resource can be submitted to this project and gradually developed into a machineprocessable form that can then interact with other FreeDict resources. We also present the immediate benefits of locating bilingual African dictionaries in this project. We have found FreeDict to be a very promising project with a lot of potential, and the present paper is meant to spread the news about it, in the hope to create an active community of linguists and lexicographers of various backgrounds, where common research subprojects can be fruitfully carried out. 
This paper investigates the possibilities that cross-linguistic similarities and dissimilarities between related languages offer in terms of bootstrapping a morphological analyser. In this case an existing Zulu morphological analyser prototype (ZulMorph) serves as basis for a Xhosa analyser. The investigation is structured around the morphotactics and the morphophonological alternations of the languages involved. Special attention is given to the so-called “open” class, which represents the word root lexicons for specifically nouns and verbs. The acquisition and coverage of these lexicons prove to be crucial for the success of the analysers under development. The bootstrapped morphological analyser is applied to parallel test corpora and the results are discussed. A variety of cross-linguistic effects is illustrated with examples from the corpora. It is found that bootstrapping morphological analysers for languages that exhibit significant structural and lexical similarities may be fruitfully exploited for developing analysers for lesser-resourced languages. 
The paper describes a set of experiments involving the application of three state-ofthe-art part-of-speech taggers to Ethiopian Amharic, using three different tagsets. The taggers showed worse performance than previously reported results for English, in particular having problems with unknown words. The best results were obtained using a Maximum Entropy approach, while HMM-based and SVMbased taggers got comparable results. 
This paper presents the Ontology for Accessing Transcription Systems (OATS), a knowledge base that supports interoperation over disparate transcription systems and practical orthographies. The knowledge base includes an ontological description of writing systems and relations for mapping transcription system segments to an interlingua pivot, the IPA. It includes orthographic and phonemic inventories from 203 African languages. OATS is motivated by the desire to query data in the knowledge base via IPA or native orthography, and for error checking of digitized data and conversion between transcription systems. The model in this paper implements these goals. 
Augmentative and Alternative Communication (AAC) systems are communication aids for people who cannot speak because of motor or cognitive impairments. We are developing AAC systems where users select information they wish to communicate, and this is expressed using an NLG system. We believe this model will work well in contexts where AAC users wish to go beyond simply making requests or answering questions, and have more complex communicative goals such as story-telling and social interaction. 
Widespread use of Semantic Web technologies requires interfaces through which knowledge can be viewed and edited without deep understanding of Description Logic and formalisms like OWL and RDF. Several groups are pursuing approaches based on Controlled Natural Languages (CNLs), so that editing can be performed by typing in sentences which are automatically interpreted as statements in OWL. We suggest here a variant of this approach which relies entirely on Natural Language Generation (NLG), and propose requirements for a system that can reliably generate transparent realisations of statements in Description Logic. 
Data-to-text generation systems tend to be knowledge-based and manually built, which limits their reusability and makes them time and cost-intensive to create and maintain. Methods for automating (part of) the system building process exist, but do such methods risk a loss in output quality? In this paper, we investigate the cost/quality trade-off in generation system building. We compare four new data-to-text systems which were created by predominantly automatic techniques against six existing systems for the same domain which were created by predominantly manual techniques. We evaluate the ten systems using intrinsic automatic metrics and human quality ratings. We ﬁnd that increasing the degree to which system building is automated does not necessarily result in a reduction in output quality. We ﬁnd furthermore that standard automatic evaluation metrics underestimate the quality of handcrafted systems and over-estimate the quality of automatically created systems. 
Data-driven approaches to sentence compression deﬁne the task as dropping any subset of words from the input sentence while retaining important information and grammaticality. We show that only 16% of the observed compressed sentences in the domain of subtitling can be accounted for in this way. We argue that part of this is due to evaluation issues and estimate that a deletion model is in fact compatible with approximately 55% of the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work. 
Many studies in natural language processing are concerned with how to generate deﬁnite descriptions that evoke a discourse entity already introduced in the context. A solution to this problem has been initially proposed by Dale (1989) in terms of distinguishing descriptions and distinguishable entities. In this paper, we give a formal deﬁnition of the terms “distinguishable entity” in non trivial cases and we show that its properties lead us to the deﬁnition of a distance between entities. Then, we give a polynomial algorithm to compute distinguishing descriptions. 
Georeferenced data sets are often large and complex. Natural Language Generation (NLG) systems are beginning to emerge that generate texts from such data. One of the challenges these systems face is the generation of geographic descriptions referring to the location of events or patterns in the data. Based on our studies in the domain of meteorology we present a two staged approach to generating geographic descriptions. The ﬁrst stage involves using domain knowledge based on the task context to select a frame of reference, and the second involves using constraints imposed by the end user to select values within a frame of reference. Because geographic concepts are inherently vague our approach does not guarantee a distinguishing description. Our evaluation studies show that NLG systems, because they can analyse input data exhaustively, can produce more ﬁne-grained geographic descriptions that are more useful to end users than those generated by human experts. 
This paper introduces a class-based approach to ordering prenominal modiﬁers. Modiﬁers are grouped into broad classes based on where they tend to occur prenominally, and a framework is developed to order sets of modiﬁers based on their classes. This system is developed to generate several orderings for modiﬁers with more ﬂexible positional constraints, and lends itself to bootstrapping for the classiﬁcation of previously unseen modiﬁers. 
In this paper, we explore a corpus of human-produced referring expressions to see to what extent we can learn the referential behaviour the corpus represents. Despite a wide variation in the way subjects refer across a set of ten stimuli, we demonstrate that component elements of the referring expression generation process appear to generalise across participants to a signiﬁcant degree. This leads us to propose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic. 
This paper shows a model of automatic instruction giving for guiding human users in virtual 3D environments. A multilevel model for choosing what instruction to give in every state is presented, and so are the different modules that compose the whole generation system. How 3D information in the virtual world is used is explained, and the ﬁnal order generation is detailed. This model has been implemented as a solution for the GIVE Challenge, an instruction generation challenge. 
We address the problem that different users have different lexical knowledge about problem domains, so that automated dialogue systems need to adapt their generation choices online to the users’ domain knowledge as it encounters them. We approach this problem using policy learning in Markov Decision Processes (MDP). In contrast to related work we propose a new statistical user model which incorporates the lexical knowledge of different users. We evaluate this user model by showing that it allows us to learn dialogue policies that automatically adapt their choice of referring expressions online to different users, and that these policies are signiﬁcantly better than adaptive hand-coded policies for this problem. The learned policies are consistently between 2 and 8 turns shorter than a range of different hand-coded but adaptive baseline lexical alignment policies. 
Alignment of interlocutors is a well known psycholinguistic phenomenon of great relevance for dialogue systems in general and natural language generation in particular. In this paper, we present the alignmentcapable microplanner SPUD prime. Using a priming-based model of interactive alignment, it is ﬂexible enough to model the alignment behaviour of human speakers to a high degree. This will allow for further investigation of which parameters are important to model alignment and how the human–computer interaction changes when the computer aligns to its users. 
This paper describes SimpleNLG, a realisation engine for English which aims to provide simple and robust interfaces to generate syntactic structures and linearise them. The library is also ﬂexible in allowing the use of mixed (canned and noncanned) representations. 
We present a Wizard-of-Oz environment for data collection on Referring Expression Generation (REG) in a real situated spoken dialogue task. The collected data will be used to build user simulation models for reinforcement learning of referring expression generation strategies. 
This paper discusses the evaluation of a Generation of Referring Expressions algorithm that takes structural ambiguity into account. We describe an ongoing study with human readers. 
This paper argues for a game-theoretic approach to content determination that uses text-type speciﬁc strategies in order to determine the optimal content for various user types. By means of content determination for the description of numerical data the beneﬁts of a game-theoretic treatment of content determination are outlined. 
This paper gives an overview of ongoing work on a system for the generation of NL descriptions of classes deﬁned in OWL ontologies. We present a general structuring approach for such descriptions. Since OWL ontologies do not by default contain the information necessary for lexicalization, lexical information has to be added to the data via annotations. A rulebased mechanism for automatically deriving these annotations is presented. 
In order to pursue research on generating referring expressions in a situated collaboration task, we set up a data-collection experiment based on the Tangram puzzle. For a pair of participants we recorded every utterance in synchronisation with the current state of the puzzle as well as all operations by the participants. Referring expressions were annotated with their referents in order to build a referring expression corpus in Japanese. We provide preliminary results on the analysis of the corpus from various standpoints, focussing on action-mentioning expressions. 
In this paper we examine the effect of linguistic devices on recall and comprehension in information presentation using both recall and eye-tracking data. In addition, the results were validated via an experiment using Amazon’s Mechanical Turk micro-task environment. 
In a corpus study we found that authors vary both mathematical form and precision1 when expressing numerical quantities. Indeed, within the same document, a quantity is often described vaguely in some places and more accurately in others. Vague descriptions tend to occur early in a document and to be expressed in simpler mathematical forms (e.g., fractions or ratios), whereas more accurate descriptions of the same proportions tend to occur later, often expressed in more complex forms (e.g., decimal percentages). Our results can be used in Natural Language Generation (1) to generate repeat descriptions within the same document, and (2) to generate descriptions of numerical quantities for different audiences according to mathematical ability. 
For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. 
The background for this paper is the aim to build robotic assistants that can “naturally” interact with humans. One prerequisite for this is that the robot can correctly identify objects or places a user refers to, and produce comprehensible references itself. As robots typically act in environments that are larger than what is immediately perceivable, the problem arises how to identify the appropriate context, against which to resolve or produce a referring expression (RE). Existing algorithms for generating REs generally bypass this problem by assuming a given context. In this paper, we explicitly address this problem, proposing a method for context determination in large-scale space. We show how it can be applied both for resolving and producing REs. 
The content selection component of a natural language generation system decides which information should be communicated in its output. We use information from reports on the game of cricket. We ﬁrst describe a simple factoid-to-text alignment algorithm then treat content selection as a collective classiﬁcation problem and demonstrate that simple ‘grouping’ of statistics at various levels of granularity yields substantially improved results over a probabilistic baseline. We additionally show that holding back of speciﬁc types of input data, and linking database structures with commonality further increase performance. 
Present-day sentence generators are often incapable of producing a wide variety of wellformed elliptical versions of coordinated clauses, in particular, of combined elliptical phenomena (Gapping, Forward and Backward Conjunction Reduction, etc.). The applicability of the various types of clausal coordinate ellipsis (CCE) presupposes detailed comparisons of the syntactic properties of the coordinated clauses. These nonlocal comparisons argue against approaches based on local rules that treat CCE structures as special cases of clausal coordination. We advocate an alternative approach where CCE rules take the form of postediting rules applicable to nonelliptical structures. The advantage is not only a higher level of modularity but also applicability to languages belonging to different language families. We describe a language-neutral module (called Elleipo; implemented in JAVA) that generates as output all major CCE versions of coordinated clauses. Elleipo takes as input linearly ordered nonelliptical coordinated clauses annotated with lexical identity and coreferentiality relationships between words and word groups in the conjuncts. We demonstrate the feasibility of a single set of postediting rules that attains multilingual coverage. 
One major aim of research in affective natural language generation is to be able to use language intelligently to induce effects on the emotions of the reader/ hearer. Although varying the content of generated language (“strategic” choices) might be expected to change the effect on emotions, it is not obvious that varying the form of the language (“tactical” choices) can do this. Indeed, previous experiments have been unable to show emotional effects of tactical variations. Building on what has been discovered in previous experiments, we present a new experiment which does demonstrate such effects. This represents an important step towards the empirical evaluation of affective NLG systems. 
This informal position paper brings together some recent developments in formal semantics and pragmatics to argue that the discipline of Game Theory is well placed to become the theoretical backbone of Natural Language Generation. To demonstrate some of the strengths and weaknesses of the Game-Theoretical approach, we focus on the utility of vague expressions. More speciﬁcally, we ask what light Game Theory can shed on the question when an NLG system should generate vague language. 
We describe a new realiser developed for the TUNA 2009 Challenge, and present its evaluation scores on the development set, showing a clear increase in performance compared to last year’s simple realiser. 
 attribute g such that g, if selected, would rule out all remaining distractors in the context.  We present a follow-up of our previous frequency-based greedy attribute selection strategy. The current version takes into account also the instructions given to the participants of TUNA trials regarding the use of location information, showing an overall improvement on string-edit distance values driven by the results on the Furniture domain. 
This paper presents a probabilistic model both for generation and understanding of referring expressions. This model introduces the concept of parts of objects, modelling the necessity to deal with the characteristics of separate parts of an object in the referring process. This was ignored or implicit in previous literature. Integrating this concept into a probabilistic formulation, the model captures human characteristics of visual perception and some type of pragmatic implicature in referring expressions. Developing this kind of model is critical to deal with more complex domains in the future. As a ﬁrst step in our research, we validate the model with the TUNA corpus to show that it includes conventional domain modeling as a subset. 
 This article is concerned with Extreme Case Formulations (ECFs) (Edwards, 2000; Pomerantz, 1986) in spontaneous Cypriot Greek conversations.1 This study confirms the occurrence of ECFs in complaints as identified by Edwards (2000) Pomerantz (1986), but goes one step further to analyse the sequential and interaction work accomplished with ECFs in reporting “opposition-type stories” (Schegloff, 1984) and in complaining about a non-present party’s misbehaviour. Opposition-type stories report the oppositional conversation of the teller with a third non-present party (id.). Interestingly, in the conversational extracts examined in this study, the conversation reported is culminated with the opponent’s reported extreme claim (ECF) occupying the last turn. The occurrence of an ECF at that marked place, that is, at the punchline of the telling, is associated with issues of affiliation and stance since it is placed exactly before the recipient’s slot upon story completion, which is a regular place for the occurrence of evaluation (Schegloff, 1984).  This article reports some of the findings of a study of extreme case formulations (ECFs) (Edwards, 2000; Pomerantz, 1986) in spontaneous conversations exclusively conducted in Cypriot Greek. In a seminal article, Pomerantz (1986) drew attention to the conversational uses of extreme case formulations (ECFs). Edwards (2000: 3478) explains that ECFs are “descriptions or assessments that deploy extreme expressions such as every, all, none, best, least, as good as it gets, always, perfectly, brand new, and absolutely”. Pomerantz (1986: 219-220) summarizes the three main uses of ECFs, mainly used in complaints, in the following way: (1) to assert the strongest case in anticipation of non-sympathetic hearings, (2) to propose the cause of a phenomenon, (3) to speak for the rightness (wrongness) of a practice.  
This paper presents a theoretical approach to the characterization of requests boundaries and structure in general spoken dialogue. Emphasis is laid on the fracture between the illocutionary act of requesting (for which the term ‘request’ is kept) and the locutionary elements that carry it out (its ‘instantiation’). This approach leads to a representation of requests based on the inclusion of a semantic level under a pragmatic level via a structural level. These distinctions are meant to beneﬁt to the semantic-pragmatic segmentation of dialogue and the study of request strategies. 
In this paper, we describe the use of lexical and semantic features for topic classiﬁcation in dictated medical reports. First, we employ SVM classiﬁcation to assign whole reports to coarse work-type categories. Afterwards, text segments and their topic are identiﬁed in the output of automatic speech recognition. This is done by assigning work-type-speciﬁc topic labels to each word based on features extracted from a sliding context window, again using SVM classiﬁcation utilizing semantic features. Classiﬁer stacking is then used for a posteriori error correction, yielding a further improvement in classiﬁcation accuracy. 
Being confronted with spontaneous speech, our current annotation scheme requires alterations that would reflect the abundant use of non-sentential fragments with clausal meaning tightly connected to their context, which do not systematically occur in written texts. The purpose of this paper is to list the common patterns of non-sentential fragments and their contexts and to find a smooth resolution of their semantic annotation. 
We are interested in extracting semantic structures from spoken utterances generated within conversational systems. Current Spoken Language Understanding systems rely either on hand-written semantic grammars or on ﬂat attribute-value sequence labeling. While the former approach is known to be limited in coverage and robustness, the latter lacks detailed relations amongst attribute-value pairs. In this paper, we describe and analyze the human annotation process of rich semantic structures in order to train semantic statistical parsers. We have annotated spoken conversations from both a human-machine and a human-human spoken dialog corpus. Given a sentence of the transcribed corpora, domain concepts and other linguistic features are annotated, ranging from e.g. part-of-speech tagging and constituent chunking, to more advanced annotations, such as syntactic, dialog act and predicate argument structure. In particular, the two latter annotation layers appear to be promising for the design of complex dialog systems. Statistics and mutual information estimates amongst such features are reported and compared across corpora. 
Most dialog systems explicitly conﬁrm user-provided task-relevant concepts. User responses to these system conﬁrmations (e.g. corrections, topic changes) may be misrecognized because they contain unrequested task-related concepts. In this paper, we propose a concept-speciﬁc language model adaptation strategy where the language model (LM) is adapted to the concept type(s) actually present in the user’s post-conﬁrmation utterance. We evaluate concept type classiﬁcation and LM adaptation for post-conﬁrmation utterances in the Let’s Go! dialog system. We achieve 93% accuracy on concept type classiﬁcation using acoustic, lexical and dialog history features. We also show that the use of concept type classiﬁcation for LM adaptation can lead to improvements in speech recognition performance. 
LOGUS is a French-speaking spoken language understanding (SLU) system which carries out a deeper analysis than those achieved by standard concept spotters. It is designed for multi-domain conversational systems or for systems that are working on complex application domains. Based on a logical approach, the system adapts the ideas of incremental robust parsing to the issue of SLU. The paper provides a detailed description of the system as well as results from two evaluation campaigns that concerned all of current French-speaking SLU systems. The observed error rates suggest that our logical approach can stand comparison with concept spotters on restricted application domains, but also that its behaviour is promising for larger domains. The question of the generality of the approach is precisely addressed by our current investigations on a new task: SLU for an emotional robot companion for young hospital patents. 
Spoken dialogue is notoriously hard to process with standard NLP technologies. Natural spoken dialogue is replete with disﬂuent, partial, elided or ungrammatical utterances, all of which are very hard to accommodate in a dialogue system. Furthermore, speech recognition is known to be a highly error-prone task, especially for complex, open-ended discourse domains. The combination of these two problems – ill-formed and/or misrecognised speech inputs – raises a major challenge to the development of robust dialogue systems. We present an integrated approach for addressing these two issues, based on a incremental parser for Combinatory Categorial Grammar. The parser takes word lattices as input and is able to handle illformed and misrecognised utterances by selectively relaxing its set of grammatical rules. The choice of the most relevant interpretation is then realised via a discriminative model augmented with contextual information. The approach is fully implemented in a dialogue system for autonomous robots. Evaluation results on a Wizard of Oz test suite demonstrate very signiﬁcant improvements in accuracy and robustness compared to the baseline. 
We present RUBISC, a new incremental chunker that can perform incremental slot ﬁlling and revising as it receives a stream of words. Slot values can inﬂuence each other via a uniﬁcation mechanism. Chunks correspond to sense units, and end-of-sentence detection is done incrementally based on a notion of semantic/pragmatic completeness. One of RUBISC’s main ﬁelds of application is in dialogue systems where it can contribute to responsiveness and hence naturalness, because it can provide a partial or complete semantics of an utterance while the speaker is still speaking. The chunker is evaluated on a German transcribed speech corpus and achieves a concept error rate of 43.3% and an F-Score of 81.5. 
Taking so-called split utterances as our point of departure, we argue that a new perspective on the major challenge of disambiguation becomes available, given a framework in which both parsing and generation incrementally involve the same mechanisms for constructing trees reﬂecting interpretation (Dynamic Syntax: (Cann et al., 2005; Kempson et al., 2001)). With all dependencies, syntactic, semantic and pragmatic, deﬁned in terms of incremental progressive tree growth, the phenomenon of speaker/hearer role-switch emerges as an immediate consequence, with the potential for clariﬁcation, acknowledgement, correction, all available incrementally at any sub-sentential point in the interpretation process. Accordingly, at all intermediate points where interpretation of an utterance subpart is not fully determined for the hearer in context, uncertainty can be resolved immediately by suitable clariﬁcation/correction/repair/extension as an exchange between interlocutors. The result is a major check on the combinatorial explosion of alternative structures and interpretations at each choice point, and the basis for a model of how interpretation in context can be established without either party having to make assumptions about what information they and their interlocutor share in resolving ambiguities. 
This paper describes a simple evaluation metric for MT which attempts to overcome the well-known deﬁcits of the standard BLEU metric from a slightly different angle. It employes Levenshtein’s edit distance for establishing alignment between the MT output and the reference translation in order to reﬂect the morphological properties of highly inﬂected languages. It also incorporates a very simple measure expressing the differences in the word order. The paper also includes evaluation on the data from the previous SMT workshop for several language pairs. 
We present two regression models for the prediction of pairwise preference judgments among MT hypotheses. Both models are based on feature sets that are motivated by textual entailment and incorporate lexical similarity as well as local syntactic features and speciﬁc semantic phenomena. One model predicts absolute scores; the other one direct pairwise judgments. We ﬁnd that both models are competitive with regression models built over the scores of established MT evaluation metrics. Further data analysis clariﬁes the complementary behavior of the two feature sets. 
We present a simple method for generating translations with the Moses toolkit (Koehn et al., 2007) from existing hypotheses produced by other translation engines. As the structures underlying these translation engines are not known, an evaluationbased strategy is applied to select systems for combination. The experiments show promising improvements in terms of BLEU. 
 RWTH participated in the System Combination task of the Fourth Workshop on Statistical Machine Translation (WMT 2009). Hypotheses from 9 German→English MT systems were combined into a consensus translation. This consensus translation scored 2.1% better in BLEU and 2.3% better in TER (abs.) than the best single system. In addition, cross-lingual output from 10 French, German, and Spanish→English systems was combined into a consensus translation, which gave an improvement of 2.0% in BLEU/3.5% in TER (abs.) over the best single system. 
We describe a synthetic method for combining machine translations produced by different systems given the same input. One-best outputs are explicitly aligned to remove duplicate words. Hypotheses follow system outputs in sentence order, switching between systems mid-sentence to produce a combined output. Experiments with the WMT 2009 tuning data showed improvement of 2 BLEU and 1 METEOR point over the best HungarianEnglish system. Constrained to data provided by the contest, our system was submitted to the WMT 2009 shared system combination task. 
This paper describes the incremental hypothesis alignment algorithm used in the BBN submissions to the WMT09 system combination task. The alignment algorithm used a sentence speciﬁc alignment order, ﬂexible matching, and new shift heuristics. These reﬁnements yield more compact confusion networks compared to using the pair-wise or incremental TER alignment algorithms. This should reduce the number of spurious insertions in the system combination output and the system combination weight tuning converges faster. System combination experiments on the WMT09 test sets from ﬁve source languages to English are presented. The best BLEU scores were achieved by combing the English outputs of three systems from all ﬁve source languages. 
RWTH participated in the shared translation task of the Fourth Workshop of Statistical Machine Translation (WMT 2009) with the German-English, French-English and Spanish-English pair in each translation direction. The submissions were generated using a phrase-based and a hierarchical statistical machine translation systems with appropriate morpho-syntactic enhancements. POS-based reorderings of the source language for the phrase-based systems and splitting of German compounds for both systems were applied. For some tasks, a system combination was used to generate a ﬁnal hypothesis. An additional English hypothesis was produced by combining all three ﬁnal systems for translation into English. 
We present a word substitution approach to combine the output of different machine translation systems. Using part of speech information, candidate words are determined among possible translation options, which in turn are estimated through a precomputed word alignment. Automatic substitution is guided by several decision factors, including part of speech, local context, and language model probabilities. The combination of these factors is deﬁned after careful manual analysis of their respective impact. The approach is tested for the language pair GermanEnglish, however the general technique itself is language independent. 
In this paper we describe the statistical machine translation system of the Universita¨t Karlsruhe developed for the translation task of the Fourth Workshop on Statistical Machine Translation. The state-ofthe-art phrase-based SMT system is augmented with alternative word reordering and alignment mechanisms as well as optional phrase table modiﬁcations. We participate in the constrained condition of German-English and English-German as well as in the constrained condition of French-English and English-French. 
This study presents the TALP-UPC submission to the EACL Fourth Worskhop on Statistical Machine Translation 2009 evaluation campaign. It outlines the architecture and conﬁguration of the 2009 phrase-based statistical machine translation (SMT) system, putting emphasis on the major novelty of this year: combination of SMT systems implementing different word reordering algorithms. Traditionally, we have concentrated on the Spanish-to-English and English-toSpanish News Commentary translation tasks. 
This paper describes the MulTra project, aiming at the development of an efﬁcient multilingual translation technology based on an abstract and generic linguistic model as well as on object-oriented software design. In particular, we will address the issue of the rapid growth both of the transfer modules and of the bilingual databases. For the latter, we will show that a signiﬁcant part of bilingual lexical databases can be derived automatically through transitivity, with corpus validation. 
In this paper, we describe the machine translation system in the evaluation campaign of the Fourth Workshop on Statistical Machine Translation at EACL 2009. We describe the modular design of our multi-engine MT system with particular focus on the components used in this participation. We participated in the translation task for the following translation directions: French–English and English–French, in which we employed our multi-engine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and Confusion Network decoder. We report results on the provided development and test sets. 
 We describe here the two Systran/University of Edinburgh submissions for WMT2009. They involve a statistical post-editing model with a particular handling of named entities (English to French and German to English) and the extraction of phrasal rules (English to French). 
 We describe two systems for English-toCzech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach.  
We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements sufﬁx-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. 
This paper describes the techniques we explored to improve the translation of news text in the German-English and Hungarian-English tracks of the WMT09 shared translation task. Beginning with a convention hierarchical phrase-based system, we found beneﬁts for using word segmentation lattices as input, explicit generation of beginning and end of sentence markers, minimum Bayes risk decoding, and incorporation of a feature scoring the alignment of function words in the hypothesized translation. We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements. 
We describe the system used in our submission to the WMT-2009 French-English translation task. We use the Moses phrasebased Statistical Machine Translation system with two simple modications of the decoding input and word-alignment strategy based on morphology, and analyze their impact on translation quality. 
In this article, we describe the machine translation systems we used to create MorphoLogic’s submissions to the WMT09 shared Hungarian to English and English to Hungarian shared translation tasks. We used our rule based MetaMorpho system to generate our primary submission. In addition, we created a hybrid system where the Moses decoder is used to rank translations or assemble partial translations created by MetaMorpho. Our third system was a purely statistical morpheme based system for the Hungarian to English task. 
This paper presents our first attempt at constructing a Vietnamese-French statistical machine translation system. Since Vietnamese is an under-resourced language, we concentrate on building a large VietnameseFrench parallel corpus. A document alignment method based on publication date, special words and sentence alignment result is proposed. The paper also presents an application of the obtained parallel corpus to the construction of a Vietnamese-French statistical machine translation system, where the use of different units for Vietnamese (syllables, words, or their combinations) is discussed. 
We present a comparison of two approaches for Arabic-Chinese machine translation using English as a pivot language: sentence pivoting and phrase-table pivoting. Our results show that using English as a pivot in either approach outperforms direct translation from Arabic to Chinese. Our best result is the phrase-pivot system which scores higher than direct translation by 1.1 BLEU points. An error analysis of our best system shows that we successfully handle many complex Arabic-Chinese syntactic variations. 
Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions. The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system. Previous work showed small performance gains by adapting from limited in-domain bilingual data. Here, we aim instead at signiﬁcant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language. We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language. Investigations were conducted on a stateof-the-art phrase-based system trained on the Spanish–English part of the UN corpus, and adapted on the corresponding Europarl data. Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline. By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set. 
Chinese and Korean belong to different language families in terms of word-order and morphological typology. Chinese is an SVO and morphologically poor language while Korean is an SOV and morphologically rich one. In Chinese-to-Korean SMT systems, systematic differences between the verbal systems of the two languages make the generation of Korean verbal phrases difficult. To resolve the difficulties, we address two issues in this paper. The first issue is that the verb position is different from the viewpoint of word-order typology. The second is the difficulty of complex morphology generation of Korean verbs from the viewpoint of morphological typology. We propose a Chinese syntactic reordering that is better at generating Korean verbal phrases in Chinese-to-Korean SMT. Specifically, we consider reordering rules targeting Chinese verb phrases (VPs), preposition phrases (PPs), and modality-bearing words that are closely related to Korean verbal phrases. We verify our system with two corpora of different domains. Our proposed approach significantly improves the performance of our system over a baseline phrased-based SMT system. The relative improvements in the two corpora are +9.32% and +5.43%, respectively. 
Reordering is a serious challenge in statistical machine translation. We propose a method for analysing syntactic reordering in parallel corpora and apply it to understanding the differences in the performance of SMT systems. Results at recent large-scale evaluation campaigns show that synchronous grammar-based statistical machine translation models produce superior results for language pairs such as Chinese to English. However, for language pairs such as Arabic to English, phrasebased approaches continue to be competitive. Until now, our understanding of these results has been limited to differences in BLEU scores. Our analysis shows that current state-of-the-art systems fail to capture the majority of reorderings found in real data. 
In this paper we describe a new approach to model long-range word reorderings in statistical machine translation (SMT). Until now, most SMT approaches are only able to model local reorderings. But even the word order of related languages like German and English can be very different. In recent years approaches that reorder the source sentence in a preprocessing step to better match target sentences according to POS(Part-of-Speech)-based rules have been applied successfully. We enhance this approach to model long-range reorderings by introducing discontinuous rules. We tested this new approach on a GermanEnglish translation task and could signiﬁcantly improve the translation quality, by up to 0.8 BLEU points, compared to a system which already uses continuous POSbased rules to model short-range reorderings. 
Linking constructions involving (DE) are ubiquitous in Chinese, and can be translated into English in many different ways. This is a major source of machine translation error, even when syntaxsensitive translation models are used. This paper explores how getting more information about the syntactic, semantic, and discourse context of uses of (DE) can facilitate producing an appropriate English translation strategy. We describe a ﬁnergrained classiﬁcation of (DE) constructions in Chinese NPs, construct a corpus of annotated examples, and then train a log-linear classiﬁer, which contains linguistically inspired features. We use the DE classiﬁer to preprocess MT data by explicitly labeling (DE) constructions, as well as reordering phrases, and show that our approach provides signiﬁcant BLEU point gains on MT02 (+1.24), MT03 (+0.88) and MT05 (+1.49) on a phrasedbased system. The improvement persists when a hierarchical reordering model is applied. 
Translation systems are complex, and most metrics do little to pinpoint causes of error or isolate system differences. We use a simple technique to discover induction errors, which occur when good translations are absent from model search spaces. Our results show that a common pruning heuristic drastically increases induction error, and also strongly suggest that the search spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences. 
In this paper we present a novel transliteration technique which is based on deep belief networks. Common approaches use ﬁnite state machines or other methods similar to conventional machine translation. Instead of using conventional NLP techniques, the approach presented here builds on deep belief networks, a technique which was shown to work well for other machine learning problems. We show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an Arabic-English transliteration task. 
The most commonly used method for training feature weights in statistical machine translation (SMT) systems is Och’s minimum error rate training (MERT) procedure. A well-known problem with Och’s procedure is that it tends to be sensitive to small changes in the system, particularly when the number of features is large. In this paper, we quantify the stability of Och’s procedure by supplying different random seeds to a core component of the procedure (Powell’s algorithm). We show that for systems with many features, there is extensive variation in outcomes, both on the development data and on the test data. We analyze the causes of this variation and propose modiﬁcations to the MERT procedure that improve stability while helping performance on test data. 
Linguistic metrics based on syntactic and semantic information have proven very effective for Automatic MT Evaluation. However, no results have been presented so far on their performance when applied to heavily ill-formed low quality translations. In order to glean some light into this issue, in this work we present an empirical study on the behavior of a heterogeneous set of metrics based on linguistic analysis in the paradigmatic case of speech translation between non-related languages. Corroborating previous ﬁndings, we have veriﬁed that metrics based on deep linguistic analysis exhibit a very robust and stable behavior at the system level. However, these metrics suffer a signiﬁcant decrease at the sentence level. This is in many cases attributable to a loss of recall, due to parsing errors or to a lack of parsing at all, which may be partially ameliorated by backing off to lexical similarity. 
Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields signiﬁcantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating signiﬁcant differences between the types of human judgments. 
In many cases, museum documentation consists of semi-structured data records with free text ﬁelds, which usually refer to contents of other ﬁelds, in the same data record, as well as in others. Most of these references comprise of person and place names, as well as time speciﬁcations. It is, therefore, important to recognize those in the ﬁrst place. We report on techniques and results of partial parsing in an ongoing project, using a large database on German goldsmith art. The texts are encoded according to the TEI guidelines and expanded by structured descriptions of named entities and time speciﬁcations. These are building blocks for event descriptions, at which the next step is aiming. The identiﬁcation of named entities allows the data to be linked with various resources within the domain of cultural heritage and beyond. For the latter case, we refer to a biological database and present a solution in a transdisciplinary perspective by means of the CIDOC Conceptual Reference Model (CRM). 
In this paper we describe an authoring environment for the creation of culturaldomain ontologies and the associated linguistic and proﬁle annotations, for dynamically generating adaptable naturallanguage descriptions of the cultural objects in the ontology. Adaptation is achieved at the expense of considerable authoring effort, since it relies on providing numerical parameters for each ontological entity. To assist the authoring process, we provide an intelligent authoring back-end that completes manually authored models by inferring missing values. This intelligent authoring support facility, combined with immediate previews, can considerably reduce the effort required to create a fully functional model as the author can iterate through cycles of providing information, previewing the generated text, and only elaborating the model where the text is unsatisfactory. 
In this study we apply and evaluate an iterative pairwise alignment program for producing multiple sequence alignments, ALPHAMALIG (Alonso et al., 2004), using as material the phonetic transcriptions of words used in Bulgarian dialectological research. To evaluate the quality of the multiple alignment, we propose two new methods based on comparing each column in the obtained alignments with the corresponding column in a set of gold standard alignments. Our results show that the alignments produced by ALPHAMALIG correspond well with the gold standard alignments, making this algorithm suitable for the automatic generation of multiple string alignments. Multiple string alignment is particularly interesting for historical reconstruction based on sound correspondences. 
Pairwise string alignment (PSA) is an important general technique for obtaining a measure of similarity between two strings, used e.g., in dialectology, historical linguistics, transliteration, and in evaluating name distinctiveness. The current study focuses on evaluating different PSA methods at the alignment level instead of via the distances it induces. About 3.5 million pairwise alignments of Bulgarian phonetic dialect data are used to compare four algorithms with a manually corrected gold standard. The algorithms evaluated include three variants of the Levenshtein algorithm as well as the Pair Hidden Markov Model. Our results show that while all algorithms perform very well and align around 95% of all alignments correctly, there are speciﬁc qualitative differences in the (mis)alignments of the different algorithms. 
This paper reports on completed work carried out in the framework of an EU-funded project aimed at (a) developing a bilingual collection of cultural texts in Greek and Bulgarian, (b) creating a number of accompanying resources that will facilitate study of the primary texts across languages, and (c) integrating a system which aims to provide web-enabled and speech-enhanced access to digitized bilingual Cultural Heritage resources. This simple user interface, which incorporates advanced search mechanisms, also offers innovative accessibility for visually impaired Greek and Bulgarian users. The rationale behind the work (and the relative resource) was to promote the comparative study of the cultural heritage of the two countries.  tion 5 presents the Language Technologies (LT) deployed in the project elaborating on the Greek and the Bulgarian text processing tools, and discusses the LT methods that have been (a) exploited in the course of the project to facilitate the web-interface construction and (b) integrated in the search and retrieval mechanisms to improve the system performance. Finally, Section 6 describes the main components of the web interface and the way various features are exploited to facilitate users’ access to the data. In the last section, we present conclusions and future work. 2 Project description The project aims at highlighting cultural resources that, as of yet, remain non-exploited to their greatest extent, and at creating the necessary infrastructure with the support of LT with a  
We present a valency lexicon for Latin verbs extracted from the Index Thomisticus Treebank, a syntactically annotated corpus of Medieval Latin texts by Thomas Aquinas. In our corpus-based approach, the lexicon reflects the empirical evidence of the source data. Verbal arguments are induced directly from annotated data. The lexicon contains 432 Latin verbs with 270 valency frames. The lexicon is useful for NLP applications and is able to support annotation. 
The ﬁeld of linguistics has always been reliant on language data, since that is its principal object of study. One of the major obstacles that linguists encounter is ﬁnding data relevant to their research. In this paper, we propose a three-stage approach to help linguists ﬁnd relevant data. First, language data embedded in existing linguistic scholarly discourse is collected and stored in a database. Second, the language data is automatically analyzed and enriched, and language proﬁles are created from the enriched data. Third, a search facility is provided to allow linguists to search the original data, the enriched data, and the language proﬁles in a variety of ways. This work demonstrates the beneﬁts of using natural language processing technology to create resources and tools for linguistic research, allowing linguists to have easy access not only to language data embedded in existing linguistic papers, but also to automatically generated language proﬁles for hundreds of languages. 
An approach is presented to the automatic discovery of labels of relations between pairs of ontological classes. Using a hyperlinked encyclopaedic resource, we gather evidence for likely predicative labels by searching for sentences that describe relations between terms. The terms are instances of the pair of ontological classes under consideration, drawn from a populated knowledge base. Verbs or verb phrases are automatically extracted, yielding a ranked list of candidate relations. Human judges rate the extracted relations. The extracted relations provide a basis for automatic ontology discovery from a non-relational database. The approach is demonstrated on a database from the natural history domain. 
Digital preservation is an integral part of the management of information and the institutions in the cultural heritage sector are seeking for ways to incorporate it into their everyday practice. While there are generic approaches to long-term preservation, further research and development work is needed to address any specifics of the digital objects in the cultural heritage domain. In this paper, we will take two case studies of recent projects and analyse to what extent the metadata accompanying digital objects contribute to guarantee longevity. We summarize this analysis in two scenarios for sustainability of resources produced by small projects because compared to big institutions their digital assets are facing a higher risk not to be preserved properly. We also identify processes where natural language technologies could be of help to make the preservation more efficient. 
We propose an approach to corpus-based semantics, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus. Task-speciﬁc semantic spaces are then built on demand from the repository. A straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks. 
In the recognition of words that are typical of a speciﬁc language variety, the classic keyword approach performs rather poorly. We show how this keyword analysis can be complemented with a word space model constructed on the basis of two corpora: one representative of the language variety under investigation, and a reference corpus. This combined approach is able to recognize the markers of a language variety as words that not only have a signiﬁcantly higher frequency as compared to the reference corpus, but also a different distribution. The application of word space models moreover makes it possible to automatically discover the lexical alternative to a speciﬁc marker in the reference corpus. 
We present the results of clustering experiments with a number of different evaluation sets using dependency based word spaces. Contrary to previous results we found a clear advantage using a parsed corpus over word spaces constructed with the help of simple patterns. We achieve considerable gains in performance over these spaces ranging between 9 and 13% in absolute terms of cluster purity. 
This paper discusses a new convolution tree kernel by introducing local alignments. The main idea of the new kernel is to allow some syntactic alternations during each match between subtrees. In this paper, we give an algorithm to calculate the composite kernel. The experiment results show promising improvements on two tasks: semantic role labeling and question classiﬁcation. 
We introduce a way to represent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently. The resulting features are of sufﬁcient generality to allow us, with the help of a standard supervised machine learning algorithm, to tackle a variety of unrelated semantic tasks with good results and almost no task-speciﬁc tailoring. 
With increasing opportunities to learn online, the problem of positioning learners in an educational network of content offers new possibilities for the utilisation of geometry-based natural language processing techniques. In this article, the adoption of latent semantic analysis (LSA) for guiding learners in their conceptual development is investigated. We propose ﬁve new algorithmic derivations of LSA and test their validity for positioning in an experiment in order to draw back conclusions on the suitability of machine learning from previously accredited evidence. Special attention is thereby directed towards the role of distractors and the calculation of thresholds when using similarities as a proxy for assessing conceptual closeness. Results indicate that learning improves positioning. Distractors are of low value and seem to be replaceable by generic noise to improve threshold calculation. Furthermore, new ways to ﬂexibly calculate thresholds could be identiﬁed. 
Mitkov and Ha (2003) and Mitkov et al. (2006) offered an alternative to the lengthy and demanding activity of developing multiple-choice test items by proposing an NLP-based methodology for construction of test items from instructive texts such as textbook chapters and encyclopaedia entries. One of the interesting research questions which emerged during these projects was how better quality distractors could automatically be chosen. This paper reports the results of a study seeking to establish which similarity measures generate better quality distractors of multiple-choice tests. Similarity measures employed in the procedure of selection of distractors are collocation patterns, four different methods of WordNetbased semantic similarity (extended gloss overlap measure, Leacock and Chodorow’s, Jiang and Conrath’s as well as Lin’s measures), distributional similarity, phonetic similarity as well as a mixed strategy combining the aforementioned measures. The evaluation results show that the methods based on Lin’s measure and on the mixed strategy outperform the rest, albeit not in a statistically significant fashion. 
The appropriateness of paraphrases for words depends often on context: “grab” can replace “catch” in “catch a ball”, but not in “catch a cold”. Structured Vector Space (SVS) (Erk and Padó, 2008) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases. This paper investigates “best-practice” parameter settings for SVS, and it presents a method to obtain large datasets for paraphrase assessment from corpora with WSD annotation. 
 probabilistic taxonomy learning models. Given  In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models. We leverage on the computation of logistic regression to exploit unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances. 
In this work, we apply Dirichlet Process Mixture Models (DPMMs) to a learning task in natural language processing (NLP): lexical-semantic verb clustering. We thoroughly evaluate a method of guiding DPMMs towards a particular clustering solution using pairwise constraints. The quantitative and qualitative evaluation performed highlights the beneﬁts of both standard and constrained DPMMs compared to previously used approaches. In addition, it sheds light on the use of evaluation measures and their practical application. 
Distributional similarity methods have proven to be a valuable tool for the induction of semantic similarity. Up till now, most algorithms use two-way cooccurrence data to compute the meaning of words. Co-occurrence frequencies, however, need not be pairwise. One can easily imagine situations where it is desirable to investigate co-occurrence frequencies of three modes and beyond. This paper will investigate a tensor factorization method called non-negative tensor factorization to build a model of three-way cooccurrences. The approach is applied to the problem of selectional preference induction, and automatically evaluated in a pseudo-disambiguation task. The results show that non-negative tensor factorization is a promising tool for NLP. 
This paper presents a graph-theoretic approach to the identiﬁcation of yetunknown word translations. The proposed algorithm is based on the recursive SimRank algorithm and relies on the intuition that two words are similar if they establish similar grammatical relationships with similar other words. We also present a formulation of SimRank in matrix form and extensions for edge weights, edge labels and multiple graphs. 
We address the problem of classifying multiword expression tokens in running text. We focus our study on Verb-Noun Constructions (VNC) that vary in their idiomaticity depending on context. VNC tokens are classiﬁed as either idiomatic or literal. Our approach hinges upon the assumption that a literal VNC will have more in common with its component words than an idiomatic one. Commonality is measured by contextual overlap. To this end, we set out to explore different contextual variations and different similarity measures handling the sparsity in the possible contexts via four different parameter variations. Our approach yields state of the art performance with an overall accuracy of 75.54% on a TEST data set. 
This paper presents a new statistical method for detecting and tracking changes in word meaning, based on Latent Semantic Analysis. By comparing the density of semantic vector clusters this method allows researchers to make statistical inferences on questions such as whether the meaning of a word changed across time or if a phonetic cluster is associated with a specific meaning. Possible applications of this method are then illustrated in tracing the semantic change of „dog‟, „do‟, and „deer‟ in early English and examining and comparing phonaesthemes. 
We present the context-theoretic framework, which provides a set of rules for the nature of composition of meaning based on the philosophy of meaning as context. Principally, in the framework the composition of the meaning of words can be represented as multiplication of their representative vectors, where multiplication is distributive with respect to the vector space. We discuss the applicability of the framework to a range of techniques in natural language processing, including subsequence matching, the lexical entailment model of Dagan et al. (2005), vector-based representations of taxonomies, statistical parsing and the representation of uncertainty in logical semantics. 
This paper attempts to explore the interrelation between philosophical accounts of language and respective technological developments in the field of human language technologies. In doing so, it focuses on the interaction between analytical philosophy and machine translation development, trying to draw the emerging methodological analogies. 
Without lengthy, iterative refinement of guidelines, and equally lengthy and iterative training of annotators, the level of inter-subjective agreement on simple tasks of phonetic, phonological, syntactic, semantic, and pragmatic annotation is shockingly low. This is a significant practical problem in speech and language technology, but it poses questions of interest to psychologists, philosophers of language, and theoretical linguists as well. 
This paper discusses some of the ways that the “statistical revolution” has changed and continues to change the relationship between linguistics and computational linguistics. I claim that it is more useful in parsing to make an open world assumption about possible linguistic structures, rather than the closed world assumption usually made in grammar-based approaches to parsing, and I sketch two different ways in which grammar-based approaches might be modiﬁed to achieve this. I also describe some of the ways in which probabilistic models are starting to have a signiﬁcant impact on psycholinguistics and language acquisition. In language acquisition Bayesian techniques may let us empirically evaluate the role of putative universals in universal grammar. 
It is remarkable if any relationship at all persists between computational linguists (CL) and that part of general linguistics comprising the mainstream of MIT transformational-generative (TG) theoretical syntax. If the lines are still open, it represents something of a tribute to CL practitioners’ tolerance — a triumph of hope and goodwill over the experience of abuse — because the TG community has shown considerable hostility toward CL and everything it stands for over the past ﬁfty years. I offer some brief historical notes, and hint at prospects for a better basis for collaboration in the future. 
As my title suggests, this position paper focuses on the relevance of linguistics in NLP instead of asking the inverse question. Although the question about the role of computational linguistics in the study of language may theoretically be much more interesting than the selected topic, I feel that my choice is more appropriate for the purpose and context of this workshop. This position paper starts with some retrospective observations clarifying my view on the ambivalent and multi-facetted relationship between linguistics and computational linguistics as it has evolved from both applied and theoretical research on language processing. In four brief points I will then strongly advocate a strengthened relationship from which both sides benefit. First, I will observe that recent developments in both deep linguistic processing and statistical NLP suggest a certain plausible division of labor between the two paradigms. Second, I want to propose a systematic approach to research on hybrid systems which determines optimal combinations of the paradigms and continuously monitors the division of labor as both paradigm progress. Concrete examples illustrating the proposal are taken from our own research. Third, I will argue that a central vision of computational linguistics is still alive, the dream of a formalized reusable linguistic knowledge source embodying the core competence of a language that can be utilized for wide range of applications. 
In this position paper, I argue that in order to create truly language-independent NLP systems, we need to incorporate linguistic knowledge. The linguistic knowledge in question is not intricate rule systems, but generalizations from linguistic typology about the range of variation in linguistic structures across languages. 
Knowledge-based parsers are now accurate, fast and robust enough to be used to obtain syntactic annotations for very large corpora fully automatically. We argue that such parsed corpora are an interesting new resource for linguists. The argument is illustrated by means of a number of recent results which were established with the help of parsed corpora. 
Generic relation identiﬁcation (GRI) aims to build models of relation-forming entity pairs that can be transferred across domains without modiﬁcation of model parameters. GRI has high utility in terms of cheap components for applications like summarisation, automated data exploration and initialisation of bootstrapping of relation extraction. A detailed evaluation of GRI is presented for the ﬁrst time, including explicit tests of portability between newswire and biomedical domains. Experimental results show that a novel approach incorporating dependency parsing is better in terms of recall. And, accuracy is shown to be comparable across domains.  [place American] saxophonist [person David Murray] recruited [person Amidu Berry] and DJ [person Awadi] from [organisation PBS]. Figure 1: Example input to GRI task (from ACE 2004). Square brackets indicate the extent of entity mentions with type as italicised superscript.  Entity 1 American David Murray David Murray Amidu Berry Awadi  Entity 2 David Murray Amidu Berry Awadi PBS PBS  Relation Type CITIZEN/RESIDENT BUSINESS BUSINESS MEMBER-OF-GROUP MEMBER-OF-GROUP  Table 1: Example output from GRI task. Relation types are not part of the relation identiﬁcation task but are given here for purposes of illustration.  
Information is fundamental to Finance, and understanding how it ﬂows from ofﬁcial sources to news agencies is a central problem. Readers need to digest information rapidly from high volume news feeds, which often contain duplicate and irrelevant stories, to gain a competitive advantage. We propose a text categorisation task over pairs of ofﬁcial announcements and news stories to identify whether the story repeats announcement information and/or adds value. Using features based on the intersection of the texts and relative timing, our system identiﬁes information ﬂow at 89.5% F-score and three types of journalistic contribution at 73.4% to 85.7% Fscore. Evaluation against majority annotator decision performs 13% better than a bag-of-words baseline. 
Named Entity (NE) information is critical for Information Extraction (IE) tasks. However, the cost of manually annotating sufﬁcient data for training purposes, especially for multiple languages, is prohibitive, meaning automated methods for developing resources are crucial. We investigate the automatic generation of NE annotated data in German from Wikipedia. By incorporating structural features of Wikipedia, we can develop a German corpus which accurately classiﬁes Wikipedia articles into NE categories to within 1% F -score of the state-of-the-art process in English. 
In the last eighteen months, a consensus has emerged from researchers in various disciplines that a vital piece of research infrastructure is lacking in Australia, namely, a substantial collection of computerised language data. A result of this consensus is an initiative aimed at the establishment of an Australian National Corpus. The progress of this initiative is presented in this paper, along with discussion of some important design issues and a consideration of how the initiative relates to the field of language technology in Australia. 
This paper investigates reduplication in Indonesian. In particular, we focus on verb reduplication that has the agentive voice afﬁx meN, exhibiting a homorganic nasal. We outline the recent changes we have made to the implementation of our Indonesian grammar, and the motivation for such changes. There are two main issues that we deal with in our implementation: how we account for the morphophonemic facts relating to sound changes in the morpheme; and how we construct word formation (i.e. sublexical) rules in creating these derived words exhibiting reduplication. 
Attempts to proﬁle authors based on their characteristics, including native language, have drawn attention in recent years, via several approaches using machine learning with simple features. In this paper we investigate the potential usefulness to this task of contrastive analysis from second language acquistion research, which postulates that the (syntactic) errors in a text are inﬂuenced by an author’s native language. We explore this, ﬁrst, by conducting an analysis of three syntactic error types, through hypothesis testing and machine learning; and second, through adding in these errors as features to the replication of a previous machine learning approach. This preliminary study provides some support for the use of this kind of syntactic errors as a clue to identifying the native language of an author. 
Parsers are often the bottleneck for data acquisition, processing text too slowly to be widely applied. One way to improve the efﬁciency of parsers is to construct more conﬁdent statistical models. More training data would enable the use of more sophisticated features and also provide more evidence for current features, but gold standard annotated data is limited and expensive to produce. We demonstrate faster methods for training a supertagger using hundreds of millions of automatically annotated words, constructing statistical models that further constrain the number of derivations the parser must consider. By introducing new features and using an automatically annotated corpus we are able to double parsing speed on Wikipedia and the Wall Street Journal, and gain accuracy slightly when parsing Section 00 of the Wall Street Journal. 
There is an inherent redundancy in natural languages whereby certain common phrases (or n-grams) appear frequently in general sentences, each time with the same syntactic analysis. We explore the idea of exploiting this redundancy by pre-constructing the parse structures for these frequent n-grams. When parsing sentences in the future, the parser does not have to re-derive the parse structure for these n-grams when they occur. Instead, their preconstructed analysis can be reused. By generating these pre-constructed databases over WSJ sections 02 to 21 and evaluating on section 00, a preliminary result of no signiﬁcant change in F-score nor parse time was observed. 
This paper presents an update on PENG Light, a lightweight and portable controlled natural language processor that can be used to translate a well-deﬁned subset of English unambiguously into a formal target language. We illustrate by example of a Firefox extension that provides a simple interface to the controlled natural language processor how web pages can be annotated with textual information written in controlled natural language and how these annotations can be translated incrementally into ﬁrst-order logic. We focus in particular on technical aspects of the controlled language processor and show in detail how look-ahead information that can be used to guide the writing process of the author is generated during the parsing process. Additionally, we discuss what kind of user interaction is required for processing unknown content words. 
Financial investors trade on the basis of information, and in particular, on the likelihood that a piece of information will impact the market. The ability to predict this within milliseconds of the information being released would be useful in applications such as algorithmic trading. We present a solution for classifying investor sentiment on internet stock message boards. Our solution develops upon prior work and examines several approaches for selecting features in a messy and sparse data set. Using a variation of the Bayes classiﬁer with feature selection methods allows us to produce a system with better accuracy, execution performance and precision than using conventional Na¨ıve Bayes and SVM classiﬁers. Evaluation against author-selected sentiment labels results in an accuracy of 78.72% compared to a human annotation and conventional Na¨ıve Bayes accuracy of 57% and 65.63% respectively. 
Domain-speciﬁc terms provide vital semantic information for many natural language processing (NLP) tasks and applications, but remain a largely untapped resource in the ﬁeld. In this paper, we propose an unsupervised method to extract domain-speciﬁc terms from the Reuters document collection using term frequency and inverse document frequency. 
Information Extraction, from the electronic clinical record is a comparatively new topic for computational linguists. In order to utilize the records to improve the efficiency and quality of health care, the knowledge content should be automatically encoded; however this poses a number of challenges for Natural Language Processing (NLP). In this paper, we present a cascade approach to discover the medicationrelated information (MEDICATION, DOSAGE, MODE, FREQUENCY, DURATION, REASON, and CONTEXT) from narrative patient records. The prototype of this system was used to participate the i2b2 2009 medication extraction challenge. The results show better than 90% accuracy on 5 out of 7 entities used in the study. 
The accuracy of named entity recognition systems relies heavily upon the volume and quality of available training data. Improving the process of automatically producing such training data is an important task, as manual acquisition is both time consuming and expensive. We explore the use of a variety of machine learning algorithms for categorising Wikipedia articles, an initial step in producing the named entity training data. We were able to achieve a categorisation accuracy of 95% F -score over six coarse categories, an improvement of up to 5% F -score over previous methods. 
Despite their prevalence in the English language, multiword expressions like verb-particle constructions (VPCs) are often poorly handled by NLP systems. This problem is partly due to inadequacies in existing corpora; the primary corpus for CCG-oriented work, CCGbank, does not account for VPCs at all, and is inconsistent in its handling of them. In this paper, we apply some corrective transformations to CCGbank, and then use it to retrain an augmented version of the Clark and Curran CCG parser. Using our technique, we observe no signiﬁcant change in F-score, while the resulting parse is semantically more sound. 
In this paper, we introduce our Recognizing Textual Entailment (RTE) system developed on the basis of Lexical Entailment between two text excerpts, namely the hypothesis and the text. To extract atomic parts of hypotheses and texts, we carry out syntactic parsing on the sentences. We then utilize WordNet and FrameNet lexical resources for estimating lexical coverage of the text on the hypothesis. We report the results of our RTE runs on the Text Analysis Conference RTE datasets. Using a failure analysis process, we also show that the main difﬁculty of our RTE system relates to the underlying difﬁculty of syntactic analysis of sentences. 
In many languages general syntactic cues are insufficient to disambiguate crucial relations in the task of Parsing. In such cases semantics is necessary. In this paper we show the effect of minimal semantics on parsing. We did experiments on Hindi, a morphologically rich free word order language to show this effect. We conducted experiments with the two data-driven parsers MSTPaser and MaltParser. We did all the experiments on a part of Hyderabad Dependency Treebank. With the introduction of minimal semantics we achieved an increase of 1.65% and 2.01% in labeled attachment score and labeled accuracy respectively over state-of-the-art data driven dependency parser. Keywords Minimal semantics, dependency parsing, free word order language, Malt parser, MST parser. 1. Introduction Parsing morphologically rich free word order language like Hindi1 is a challenging task. For such languages dependency based framework suits better than the constituency based one [10][19][15][2]. Data driven dependency parsers has achieved considerable success due to the availability of annotated corpora in recent years. In spite of availability of annotated treebanks, state-of-the-art parsers for these languages have not reached the performance obtained for English [16]. Small size of the treebanks, non-projectivity, complex linguistic phenomenon, long distance dependencies and lack of explicit cues are most frequently stated reasons for low performance [16][17][3]. Previously, [5] used semantic features and showed an improvement in error reduction in the LAS2 up to 5.8% 
While there are several data-driven dependency parsers, there is still a gap with regards to incrementality. However, as shown in Brick and Scheutz [3], incremental processing is necessary in human-robot interaction. As is shown in Nivre et al. [12], dependency parsing is well-suited for mostly incremental processing. However, there is as of yet no dependency parser that combines syntax and semantics by including traditional dependency parsing, CCG tagging, and lambdalogical structures in one fast, accurate application suitable for embodied natural language processing. This paper addresses that gap by introducing Mink, an incremental data-driven dependency parser with integrated conversion to semantics. We show that Mink is comparable to similar but non-incremental parsers, and that it succeeds at performing some semantic analysis. Keywords incremental parsing, dependency parsing, CCG, semantics con- version 
This paper presents a system that uses machine learning algorithms and a combination of data sets for the task of recognizing textual entailment. The chosen features quantify lexical, syntactic and semantic level by matching between texts and hypothesis sentences. Additionally, we created a filter which uses a set of heuristics based on Named Entities to detect cases where no entailment was found. We analyzed how the different sizes of data sets and classifiers could impact on the final overall performance of the systems. We show that the system performs better than the baseline and the average of the systems from the RTE on both two and three way tasks. We concluded that evaluating using the RTE3 test set, the model learned using MLP from the RTE3 alone outperforms other models that employed different ML algorithms and additional training data from the RTE1 and RTE 2. Keywords Textual entailment, machine learning, rte data sets. 1. Approach The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T). Recently the RTE4 Challenge has changed to a 3-way task that consists in distinguish among entailment, contradiction and unknown when there is no information to accept or reject the hypothesis. However the traditional two-way distinction between entailment and non-entailment is still allowed. In the past, RTEs Challenges machine learning algorithms were widely used for the task of recognizing textual entailment (Marneffe et al., Zanzotto et al.). Thus in this paper we tested the most common classifiers that have been used by other researchers in order to provide a common framework of evaluation of ML algorithms (fixing the features) and showing how the development data set could impact over them.  We generated a feature vector with the following components for both Text and Hypothesis: - Levenshtein distance, - Lexical level: a lexical distance based on Levenshtein, - Semantic level: a semantic similarity measure Wordnet based, - LCS (longest common substring) metric. We chose only four features in order to learn the development sets. Larger feature sets do not necessarily lead to improving classification performance because it could increase the risk of overfitting the training data. In section 3 we provide a correlation analysis of these features. The motivation of the input features: Levenshtein distance is motivated by the good results obtained as a measure of similarity between two strings. Additionally, we proposed a lexical distance which is based on Levenshtein distance but working to sentence level. We created a metric based on Wordnet to try to capture the semantic similarity between T and H to sentence level. Longest common substring is selected because is easy to implement and provides a good measure for word overlap. Furthermore, the system uses a NER filter that detects cases where no entailment relation is found. This filter applies heuristic rules over Named Entities found in the text and hypothesis. The system produces feature vectors for all possible combinations of the available development data RTE1, RTE2 and RTE3. Weka (Witten and Frank, 2000) is used to train classifiers on these feature vectors. We experimented with the following five machine learning algorithms: - Support Vector Machine (SVM), - AdaBoost (AB), - BayesNet (BN), - Multilayer Perceptron (MLP), - Decision Trees (DT).  12 Student Research Workshop, RANLP 2009 - Borovets, Bulgaria, pages 12–17  The Decision Trees are interesting because we can see what features were selected from the top levels of the trees. SVM, Bayes Net and AdaBoost were selected because they are known for achieving high performances. MLP was used because has achieved high performance in others NLP tasks. We experimented with various parameters (settings) for the machine learning algorithm, such like increasing the confidence factor in DT for more pruning of the trees, different configuration(layers and neurons) for the neural network, and different kernels for SVM. Thus, we tested classifiers used by other researchers in order to provide a common framework of evaluation. For two-way classification task, we used the RTE1, RTE2, RTE3 development sets from Pascal RTE Challenge, and BPI1 test suite. For three-way task we used the RTE1, RTE2 and RTE3 development sets from Stanford group2. Additionally, we generated the following development sets: RTE1+RTE2, RTE2+RTE3, RTE1+RTE3, and RTE1+RTE2+RTE3 in order to train with different corpus and different sizes. In all the cases, RTE4 TAC 2008 gold standard data set was used as test-set. The remainder of the paper is organized as follows: Section 2 describes the architecture of our system, whereas Section 3 shows the results of experimental evaluation and discussion of them. Finally, Section 4 summarizes the conclusions and lines for future work.  pair, (2) lexical distance based on Levenshtein, (3) a semantic distance based on WordNet and (4) their Longest Common Substring. The second way, is using the “NERpreprocessing module” to determinate whether nonentailment is found between text-hypothesis, therefore differing only on the treatment of Named Entities. The Levenshtein distance [5] is computed between the characters in the stemmed Text and Hypothesis strings. The others three features are detailed below. Text-hypothesis pairs are stemmed with Porter’s stemmer [3] and PoS tagged with the tagger in the OpenNLP3 framework.  TestSet RTE4 Trainning Sets: RTE 1, RTE2, RTE3,RTE1+RTE2, RTE2+RTE3 , and RTE1+RTE2+RTE3. BPI- test suite  Preprocessing NER Levenshtein Wordnet Longest Common Substring  2. System description This section provides an overview of our system that was evaluated in Fourth Pascal RTE Challenge. The system is based on a machine learning approach for recognizing textual entailment. In Figure 1 we present a brief overview of the system. Using a machine learning approach we tested with different classifiers in order to classify RTE-4 test pairs in three classes: entailment, contradiction or unknown. To deal with RTE4 in a two-way task, we needed to convert this corpus only into two classes: yes and no. For this purpose both contradiction and unknown were taken as class no. There are two variants to deal with every particular texthypothesis pair or instance. The first way is directly using four features: (1) the Levenshtein distance between each 
Partial parsing is an established NLP technique used to perform syntactic analysis without generating a full constituent parse tree. This paper presents LOGICON, an endto-end system using partial parsing, which assigns novel semantic structures to natural language text. Evaluating against a test set of 500 previously unseen sentences, the system has an accuracy of 62.4% as measured by exact matching against the expected semantic output. Since partial parsing is used, the system is robust and will assign partial semantic structure to sentences it may not fully understand. As stochastic methods are not used, the system is deterministic and fast. A syntactic tagging scheme is proposed which is closely aligned to the corresponding semantics. The system was developed as part of a PhD research project, and was written to evaluate partial parsing as the first step to creating a full natural language question-answering system. Keywords natural language processing, partial parsing, annotated corpora, constituent structure, thematic relations, semantic role labeling, syntax parse trees, part-of-speech tagging 1. Introduction The LOGICON system was developed as part of an ongoing PhD research project, with the aim of using partial parsing to extract semantic structures from natural language. LOGICON contrasts with PARASITE, a system which produces formal semantics for unrestricted text [9], since partial parsing [1][2] is used instead of deep parsing, and semantic roles are used for representing meaning as opposed to logic statements with variables and quantifiers. For example, given a simple sentence focusing around an event, LOGICON attempts to identify roles for the actor (who did the event), the action (what the event was) and the target (what entity the actor performed the action on). The system employs simple partial parsing techniques as described by Abney [1], [2]. A syntax-driven approach is then used to derive semantic roles through recursion. 2. Semantic Structures Thematic relations are an intuitive approach to assigning meaning to the constituents of a sentence. A typical  problem is what role to assign to a noun phrase. Traditionally thematic relations include Agent, Patient, Theme, Location, etc. However, there is no definitive list of roles, and in some cases which role to use is not immediately clear: in "the key opened the door" is the key the agent or the instrument? In order to produce a practical system, the research focused on working with a small set of well-defined roles:  Table 1. Semantic relations describing an event  Relation ACTION  Description the action, or main verb  ACTOR  the doer of the action  TARGET  what the action was performed on  LOCATION  where the action was performed  TIME  when the action was performed  The ACTOR role is typically assigned to the syntactic subject of the sentence, and the TARGET is typically assigned to the object. Together, the roles are grouped into a semantic structure called an EVENT. For the simple sentence "Jack helped John", the corresponding semantic structure produced by LOGICON would be: EVENT: ACTOR: Jack ACTION: help TIME: PAST TARGET: John A semantic structure called a LINK is used to represent sentences that use a copula to link a subject (the SOURCE) to its predicate (the TARGET). For example, "The apple is red" would have the following semantic structure: LINK: SOURCE: apple TARGET: red Table 2 below gives a brief summary of the important keywords used in the semantic structures found in the annotated corpus:  18 Student Research Workshop, RANLP 2009 - Borovets, Bulgaria, pages 18–22  Table 2. Keywords in LOGICON semantic structures  Keyword  Description  SPEAKER  represents the first person  LISTENER  represents the second person  OTHER  represents the third person  OF  used in a possessive construction  CONFIRM  "Is the sky blue?"  EXPLAIN  "Why is the sky blue?"  QUANTIFY  "How much" / "How many"  PARAMETER second object (ditransitive verbs)  LOCATION  "Jack put the book on the table"  SPECIFIC  represents the definite article  GENERAL  represents an indefinite article  CONCEPT  an isolated noun phrase  POSSIBLE  "Jack might eat"  EXPECTED  "Jack will eat"  RECOMMENDED  "Jack should eat"  MODIFIER  "Jack ate quickly" (adverb)  NOT  used to negate a structure  AND  "Jack and Mary are clever"  OR  "Eat the apple or the orange"  Special handling is given to pronouns and to possessive constructions, using the first 4 keywords listed in table 2. As an example, LOGICON translates the sentence "You broke my car" into the corresponding semantic structure: EVENT: ACTOR: LISTENER ACTION: break TIME: PAST TARGET: car OF SPEAKER The aim of the system is to produce enough semantic detail to enable effective question-answering. Since partial parsing, and not deep parsing is used, some structures are not dissected. For example the internals of noun phrases are not handled directly. In this respect, the semantic structures can be considered a form of semantic role labeling. The structures are generalpurpose so that it would be more accurate to say that they are an intermediate form between the frame semantics found in FrameNet [3] and the verb-argument annotations found in the PropBank corpus [8].  2.1 Semantic recursion Since natural language is inherently recursive, it is not unreasonable to expect that any corresponding semantic structures should show similar recursion. The semantic structures generated by LOGICON are syntactically-driven, that is to say they are derived directly from parse trees constructed via partial parsing. Since these parse trees are recursive structures, so are the corresponding semantics. A simple example would be the sentence "Who said time is money?". In the corresponding semantic structure, this is analyzed as a LINK (a subject/predicate construction) embedded within an EVENT (an action in time or space): EVENT: ACTOR: UNKNOWN ACTION: say TIME: PAST TARGET: LINK: SOURCE: time TARGET: money The actor (the doer of the action) is UNKNOWN ("who?"). The UNKNOWN keyword is used as a placeholder for thematic roles in sentences which use interrogative pronouns. In the event structure above, the target of the action (what was said) is itself another semantic structure, a link between a subject and its predicate: "time is money". 3. Partial Parsing 3.1 Abney’s partial parsing scheme The partial parsing scheme introduced by Abney [1] and implemented in the Cass partial parser [2], successively builds a parse tree bottom-up by using a cascade of finite state transducers. Customizable patterns are used to define the regular expressions used to parse at each level. These patterns are specified in a human readable format (similar to Backus-Naur form) and are then complied into a unified finite state transducer automatically. A distinguishing feature of the original scheme is that there is no definite top-level node representing the entire sentence. The system is more like a chunking analyzer as opposed to a full syntactic parser. The main advantages of the Cass partial parsing scheme is that it is robust (it will not fail to produce a partial analysis given input it may not fully understand), it is fast (orders of magnitude faster than stochastic parsers) and relatively easy to implement. 3.2 The annotated corpus An annotated corpus was constructed at the start of the project. By adopting a corpus-driven methodology, the effectiveness of potential parser rules was decided by available corpus evidence. The annotations were produced as follows:  19  1. A set of 2000 sentences was collected. 2. For each sentence, the semantic structure expected to be produced by the system was manually annotated. 3. From the expected semantic structure, a syntactic parse tree was also annotated that would provide the suitable semantic skeleton from which to derive the semantics. After annotation the corpus was divided into two sets: a training set of 1500 sentences, and an evaluation set of 500 sentences. The training set would be used as a reference when building the system, in order to test the effectiveness of the parser during its construction, and to try out various partial parsing rules. An example annotated sentence is shown below: "Who wrote ‘The Moon is a Harsh Mistress?’" (EV (C Who) (V wrote) (LN (C (Q The) (C Moon)) (AUX is) (C (Q a) (C Harsh Mistress)))) EVENT: ACTOR: UNKNOWN ACTION: write TIME: PAST TARGET: LINK: SOURCE: SPECIFIC Moon TARGET: GENERAL Harsh Mistress The following resources were used to construct corpus: 1. Example sentences from the Link Grammar Parser [10]. 2. Sentences based on patterns in A.L.I.C.E. [11]. 3. Questions from the TREC-10 QA track [6]. 4. Sentences from novels in Project Gutenberg. 5. News headlines from news.google.com. Different sources were used so that a wide-coverage parser could be constructed. Focusing on a particular genre – such as newspaper text – might have resulted in a more limited parser. A large proportion of the data was derived from the question templates found in the A.L.I.C.E. chat program, which is relevant to question-answering because this data was formed after studying the most frequent inputs given to a popular chat system. 3.3 Partial parsing in LOGICON Three possible parsing schemes were considered at the outset of the project: using a stochastic parser such as Bikel’s parser [4], using a dependency parser such the Link Grammar Parser [10], or using a partial parser. An existing stochastic parser was not suitable for use in LOGICON, because either these are pre-trained on a different tagset or need to be trained using a large corpus. It was felt that converting the output from the Link Grammar Parser would  be too time consuming, so it was decided to construct a partial parser which matched the syntax in the annotated corpus. A Brill tagger using transformation-based machine learning was first applied to the training set [5]. With an effective part-of-speech tagger in place, Abney’s original partial parsing scheme was then adapted. Initially, this yielded encouraging results. Out of the 1500 sentences in the training corpus, 7 simple rules resulted in partial syntax trees which had an accuracy of 90.84% as measured by the number of nodes parsed and connected to the correct constituent nodes. A total of 35 rules were finally used. 3.4 The partial parsing algorithm The partial parsing algorithm used by the LOGICON system is described as follows1. The parser constructs a parse tree bottom-up. At each stage of its operation, there is a set of top-level nodes, which are grouped together to form new top-level nodes at the next iteration. The result of the algorithm is a partial parse tree, defined as a set of one or more final top-level nodes, each of which is a complete parse tree: 1. Construct a node for each word, using part-of-speech tags from the tagger. 2. For each parser rule R, apply R to the top-level nodes. 3. If at least one rule did apply, repeat step 2 until no rules apply, and no new top-level nodes can be created. Figure 1. Rule in partial parser specifying a new node  Q  C  V  Q  C  The man opened the door  Figure 1 above shows a syntactic structure constructed by the parser during its analysis. The current top-level nodes are shown in black. At this stage of operation, the three toplevel nodes represent a noun-phrase, followed by a verb, followed by a second noun phrase. These would be recognized by a parser rule as a subject-verb-object construction, and these 3 nodes would be collected into a new top-level node shown in gray.  
Object-oriented analysis and design has now become a major approach in the design of software system. This paper presents a method to automate natural language requirements analysis for object identiﬁcation and generation based on the Parsed Use Case Descriptions (PUCDs) for capturing the output of the parsing stage. We employ Use-Case Descriptions (UCDs) as input into the whole framework of identiﬁcation of classes and relationship. PUCD is used to extract nouns, verbs, adjectives and adverbs from use case descriptions as part of an identiﬁcation process to identify objects/classes and relationships. We reﬁne classes by human expert to produce a class model as output. Keywords Requirements speciﬁcations, object-oriented, parsing, analysis, Natural Language Processing.  called Parsed Use Case Descriptions (PUCD) for capturing the output of the parsing stage, which is then used in subsequent steps. A PUCD is a set of original sentences, parsed sentences, nouns, verbs, adjectives and adverbs, which we use to extract nouns, verbs, adjectives and adverbs from use case descriptions. The next step is the identiﬁcation process to identify objects/classes, attributes, operations, associations, aggregations and inheritance so as to produce a class model. We reﬁne classes by human expert. In addition to the literature review, the work achieved to date includes an outline of the proposed method for object identiﬁcation, which is based on existing work on how to map English sentences into conceptual models. The next step identify objects/classes, attributes, operations, and the association, aggregation and inheritance abstractions to produce a class model by applying a set of rules. 2 Motivation  
A combination of established theories [1].[2].[3].[4] are applied in an attempt to improve the output from a system [5],[6] which automatically generates MCQ (Multiple Choice Question) test items from source documents. The literature observes that NLG (Natural Language Generation) system evaluation is non-trivial [7] and so the method is evaluated using a process suited to the featured domain [8]. The experiment intersperses 38 MCQ test items whose question stems have been generated using Controlled Rhetorical Structure Theory (CRST) with 62 manually created MCQ test items to form an item bank. A usability score is assigned to each item by a domain expert and these scores are used in the evaluation of the effectiveness of the method. The results provide some evidence to support the incorporation of CRST into future versions of the software. . Keywords Controlled Language, Natural Language Generation (NLG), Rhetorical Structure Theory (RST), Multiple Choice Question (MCQ) test item generation, Controlled Rhetorical Structure Theory (CRST) 1. Introduction Multiple Choice Question (MCQ) test items have been used by the UK Company featured in this study to regularly confirm staff knowledge of documents from the company’s Policy Library. The MCQ test items are delivered in the form of pre and post tests associated with training courses and field audits. The stored responses from these tests allow the company to demonstrate that training has been received by staff in accordance with requirements stated in UK Legislation [8]. However an internal study proved that creating and updating the item bank manually is an expensive process. In response to these results we are investigating various ways to automatically generate MCQ test items, the most promising one being the application of a MCQ test item generator [5], [6]. The creators of this system were the only researchers in the field who expressed an interest in collaborating with us in order to improve their system.  The MCQ test item generator [5], [6] uses the following steps to generate MCQ test items: 
The word is the basic unit in natural language processing (NLP), as it is at the lexical level upon which further processing rests. The lack of word delimiters such as spaces in Chinese texts makes Chinese word segmentation (CWS) an interesting while challenging issue. This paper describes the in-depth research following our participation in the fourth International Chinese Language Processing Bakeoff 1 . Originally, we incorporate unsupervised segmentation into Conditional Random Fields (CRFs) in the purpose of dealing with unknown words. Normalization is delicately involved in order to cater to problem of small data size. Experiments on CWS corpora from Bakeoff-4 present comparable results with state-of-the-art performance. Keywords Unsupervised Segmentation, Conditional Random Fields, Normalized Accessor Variety. 1. Introduction Words are the basic linguistic units of natural language. However, Chinese texts are character based, not word based. Thus, the identification of lexical words or the delimitation of words in running texts is a prerequisite of NLP. Chinese word segmentation can be cast as simple and effective formulation of character sequence labeling. A prevailing technique for this kind of labeling task would be Conditional Random Fields1 (CRFs) [1], following the current trend of applying machine learning as a core technology in the field of natural language processing. Based on conditional dependency assumption, CRFs could exert predominant performance on the known words  
This paper describes the preliminary work on the project of extending the BulTreeBank with temporal information that will serve as a golden standard for Bulgarian language. We outline a flexible markup scheme that is based on a language-specific verb taxonomy and test its capabilities by implementing algorithms for temporal entities recognition in the CLaRK System tool. Keywords temporal expressions, temporal relations annotation, verb categories, boundedness 1. Introduction Recently, an extensive work is being done on the automatic recognition and normalization of temporal expressions in natural languages (e.g. the MUC 6 and MUC 7 Named Entity Recognition Task, the Temporal Expression Recognition and Normalization Task). We propose a TimeML-based annotation scheme for temporal expressions in Bulgarian. The original scheme [9] was modified so that the annotation could benefit from the language-specific means for conveying temporal information: lexical aspectual type, Slavic Aspect (the so called vid category), tense and evidentiality. In Bulgarian, a language with rich verbal morphology, they play a crucial role in temporal order decoding. Our final aim is to facilitate the creation of a gold standard by annotating automatically some of the temporal information. On structure level we focus on the interaction between verb phrases and temporal function words (conjunctions and prepositions). The technical part is carried out using the BulTreeBank, an HPSG syntactically annotated corpus of Bulgarian [11]. A rule-based algorithm for temporal relations detection is implemented in the XML-based CLaRK System [12]. Its performance proves that morphologically encoded aspectual data is important when analyzing temporal relations for Bulgarian. 2. Exploiting Bulgarian verb categories Although when analyzing temporal relations (TRs) we would like to take into account world-knowledge  information, especially causation and knowledge of language usage, at this stage of annotation we do not have the resources to complete such a task in a short time. We decided to calculate automatically temporal relations, which depend solely on sentential syntax, word order, morphological and limited lexical information. In order to achieve this goal we have systematized the information that can be found in the existing descriptive literature [2]. Our next step on this preliminary stage was to develop a taxonomy of lexical aspectual types, which proved to be relevant for encoding temporal ordering. 2.1 Aspectual verb classification Verbal aspect category vid has two subcategories – namely, imperfective (IPF) and perfective (PF). Verbs are overtly marked for their vid, except for a relatively small group of biaspectual verbs in third declension. We accept that for Bulgarian language vid category encodes information about the boundedness of the eventuality denoted by the verb. This, of course, does not imply that the aspectual type of the verb is fixed, but we argue that this feature imposes some rigid limitations concerning the scope on the structure of the event, and hence some restrictions on the set of possible aspectual properties of the verb [6]. That is why we have decided to build our verb classification with respect to which nucleus element(s) verbs are related to. The wellknown nucleus components (Figure 1) are described in the works of Moens and Steedman [8]. Figure 1. Nucleus structure. Further subcategorization based on Vendlerian lexical aspectual classification is made with respect to affixation. For Slavic languages like Polish, Bulgarian, Russian and so on it has long been known that the aspectual type is marked by word-formational features and changed through derivational processes (just to mention a few recent studies: [3], [12], [6]). Verb classes whose differences proved to be relevant for TRs recognition are listed below.  40 Student Research Workshop, RANLP 2009 - Borovets, Bulgaria, pages 40–44  2.1.1 Imperfective stem verbs These are atelic verbs that focus on the unboundedness of the eventuality – states and activities, which are not related to any nucleus as its preparatory process. 2.1.2 Perfective verbs Here we distinguish three groups. Telic stem verbs are typically achievements or accomplishments (culminated processes in Moens and Steedman terminology). The former focuses only on the culmination of the event structure and the latter both on the preparatory process and the culmination. The same holds for telic verbs derived by prefixes from imperfective base verbs. Delimitatives derived by po- and nad- prefixation and expressing bounded but atelic eventualities are accomplishment verbs. In contrast, utterances with the socalled majorative-resultatives, which express activity that ends “beyond the proper limit” [5], e.g. prejam – “to have eaten too much”, could equally receive the accomplishment as well as the achievement profile. Both classes focus on a process, but in the first case this process does not belong to a nucleus structure, and in the second it is identified as a preparatory process. Verbs derived by -n1- suffixation express punctual events with no internal structure. Only few of them denote points that are not incorporated in a nucleus structure. Most of these verbs could receive ingressive reading, focusing on the point which serves as the initial bound for the process. Either way, we treat all -n1- perfectives as achievement verbs. This inconsistency is corrected on the level of TR annotation. When the perfective verb has a semelfactive reading, it is marked as MOMENT, but for an ingressive reading it receives INITIATION markup (see Table 1). 2.1.3 Secondary imperfective verbs Verbs derived from perfectives by the -a- suffix or -v- suffix and its variants focus on the preparation process in the nucleus structure. For this reason, in many contexts the realization of the culminated process is implied, especially in a present historical tense, and on a number of occasions the nucleus component referred to by the utterance is not the process, but the culmination itself. 2.1.4 Ingressive and terminative verbs Bulgarian perfective ingressive verbs, prefixed with proand za-, and terminative verbs, prefixed with do-, are derived from their imperfective counterparts: zapeja (PF) → zapjavam (IPF), “to start singing”, dopeja (PF) → dopjavam (IPF), “to finish singing”. Since perfectives focus on the process starting point, respectively culmination, they are assigned aspectual class achievement (that can be shifted to accomplishment). Again, for ingressive verbs this is obviously not the most adequate interpretation, but it suits us for the moment. On the other hand, imperfectives are assigned aspectual class activity (that can be shifted to achievement), because they focus on the beginning phase of  a process, not necessarily culminated or otherwise limited, respectively the finishing phase of a culminated process that is implied to be interrupted. 2.1.5 Encoding aspectual class Since on a token level verb forms in the BulTreeBank corpus are annotated with morphosyntactic tags providing information about vid category [10], we decided to use yet another attribute, AspCat (Aspectual Category). In accordance with the above classification, this tag receives one of the following five values: state, act, ach, acc-ach, acc-act (corresponding to Vendlerian types state, activity, achievement, accomplishment or achievement, accomplishment or activity). The ambiguity of the values is intended. The introduced attribute is not part of the tag set for temporal information mark-up. For the moment, the annotation is done manually but is computer-assisted1. Verbs that have only iterative readings are regarded as processes and their AspCat attribute receives act value, but on the level of TRs annotation they are further subcategorized as SERIES. 2.1.6 Encoding phase Bulgarian verbs encode not only information about the type of eventuality expressed, but also about its phase. TimeML temporal annotation scheme provides a special mark-up for aspectual verbs and their complements, but we have to employ another attribute for ingressives and terminatives, namely, @phase (Table 1). 3. TimeML adopted for Bulgarian TimeML emerged as a markup language for time, events and temporal links after the TERQAS workshop held in 2002 [9]. Temporal information should be represented via several tag types: EVENT – for event tokens, where event is any kind of situation that happens or occurs, MAKEINSTANCE for event instances (in contrast to event tokens), SIGNAL for textual elements that explicitly mark temporal or modal relations and quantification over events, TIMEX3 for temporal expressions, and LINK for relationships. The LINK tag is always one of the following types: TLINK (Temporal Link) for relations between two events or an event and a time, SLINK (Subordination Link) for relations between two events or an event and a signal, and ALINK (Aspectual Link) for relations between an aspectual event and its argument event. The corpus annotated according to TimeML, TimeBank, comprises English newspaper articles marked for temporal information only, but our corpus is HPSG syntactically annotated on HPSG-based grounds, which, besides language specificity, calls for altering some of the TimeML tags. 
In this paper we propose an hybrid system of Arabic words disambiguation. To achieve this goal we use the methods employed in the domain of information retrieval: Latent semantic analysis, Harman, Croft, Okapi, combined to the lesk algorithm. These methods are used to estimate the most relevant sense of the ambiguous word. This estimation is based on the calculation of the proximity between the current context (Context of the ambiguous word), and the different contexts of use of each meaning of the word. The Lesk algorithm is used to assign the correct sense of those proposed by the LSA, Harman, Croft and Okapi. The results found by the proposed system are satisfactory, we obtained a rate of disambiguation equal to 73%. Keywords Arabic ambiguous words, LSA, Harman, Okapi, Croft, Lesk algorithm, signatures and syntactic tagger. 1. Introduction This work is part of the understanding of the Arabic speech [15]. In this paper we are interested in determining the meaning of Arabic ambiguous words that we can encounter in the messages transcribed by the module of speech recognition. The word sense disambiguation (WSD) involves the association of a given word in a text or discourse with a definition or meaning (sense) which is distinguishable from other meanings potentially attributable to that word [12]. To assign the correct meaning, our method starts with the application of several pre-processing (tf × idf [14], normalization and syntactic tagging [2]) on words belonging to the context of the ambiguous word, subsequently we have applied the measures of similarities (Latent Semantic Analysis [5], Harman [8], Croft[3] and Okapi [13]) which will allow the system to choose the context of using the most closer to the current context of the ambiguous word, and we have applied Lesk algorithm [10] to distinguish the exact sense of the different senses given by this measures of similarity. This paper is structured as follows, in section 2 we present the ambiguity of the Arabic language, after that in section 3 we describe the proposed method for disambiguation of ambiguous Arabic words later in section 4, we present the results of tests of our model.  2. Disambiguation of Arabic The Arabic language is considered a difficult language to be automatically processed [10]. Among the characteristics that make this language processing ambiguous, we quote: • The non vocalization of the Arabic language: a non vocalized Arabic word has several possible meanings. However, in modern editions, the texts in Arabic languages are not vocalized. We recall that vocalization in Arabic language is the addition of signs to the consonant to precise the pronunciation. Here is an example of a non-vocalized word: ‫( ﻛﺘﺐ‬Kataba), this word might mean by way of his vocalization: َ‫( ﻛَﺘَﺐ‬he wrote), ُ‫( ﻛُﺘُﺐ‬books), َ‫( ﻛُﺘِﺐ‬it was written). This phenomenal makes the problem of disambiguation more difficult; • The structure of an Arabic word has a big problem for the automatic disambiguation. Indeed, an Arabic word can mean any expression in English or french. Here are some examples: the word ‫( وﺗﺘﺬﻛﺮوﻧﻨﺎ‬watatathakarounana) expresses the sentence in french “ and you remember us ", the same word (‫( )وﺑﻘﻮﻟﮫ‬wabikawlihi) which means in English" and by his word”. Thus the automatic understanding of such words requires a prior segmentation, a task that is not obvious; • Another source of problems is the lack of language resources such as dictionaries, previously tagged corpus, and so on. This lack of resources with the characteristics of this language makes automatic processing more difficult; In what follows, we describe the proposed method for disambiguation of the meaning of ambiguous Arabic words. 3. Proposed System 3.1 Method Because of the lack of linguistic resources necessary for the automatic processing of the Arabic language, we preferred to use and test a non-supervised method. We note that Unsupervised methodology identifies patterns in a large sample of data, without the benefit of any manually labeled examples or external knowledge sources, on the other hand the supervised methodology Create a sample of training data where a given target word is manually annotated with a sense from a predetermined set of possibilities. The Principe of our method is as follows: First, we started by collecting, from the web, various Arabic texts to  45 Student Research Workshop, RANLP 2009 - Borovets, Bulgaria, pages 45–52 
This paper describes the methodology adopted in the construction of an annotated corpus for the study of zero anaphora in Portuguese, the ZAC corpus. To our knowledge, no such corpus exists at this time for the Portuguese language. The purpose of this linguistic resource is to promote the use of automatic discovery of linguistic parameters for anaphora resolution systems. Because of the complexity of the linguistic phenomena involved, a detailed description of the different situations is provided. This paper will only focus on the annotation of subject zero anaphors. The main issues regarding zero anaphora in Portuguese are: indefinite subjects, either without verbal agreement marks or with first person plural or third person plural verbal agreement; position of the anaphor relative to its antecedent, i.e. anaphoric and cataphoric relations; coreference chains inside the same sentence and spanning several sentences; and determining the head of the antecedent noun phrase for a given anaphor. Finally, preliminary observations taken from the ZAC corpus are presented. Keywords Anaphora resolution, zero anaphora, corpus linguistics, corpus annotation, syntax, Brazilian Portuguese. 1. Introduction In many linguistic situations, redundant NPs, usually already present in a previous utterance or in a previous constituent of the same utterance may be reduced to pronoun or to zero (NP deletion) in order to avoid redundancy [1]. (1.1) *John went to school and then John went to the mall (1.2) John went to school and then [he went] to the mall Portuguese has a very rich verbal inflection, and the subject can easily be recovered through verbal inflection. The grammatical rules governing NP deletion may vary among languages, even among different varieties of the ‘same’ language, as in the case of Brazilian (bp) vs. European Portuguese (ep). For example, the Portuguese equivalent for the examples (1.1)-(1.2) should be: (1.3) *O Joãoi foi à escola e depois o Joãoi foi ao epcentro comercial/ep,bpshopping (1.4) O Joãoi foi à escola e depois (ε + *ep,bpelei) foi ao epcentro comercial/ep,bpshopping  (1.5) O Joãoi foi à escola e depois ao epcentro comercial/ep,bpshopping In the previous examples, the reduction of the verb imposes the subject NP deletion; otherwise it can be reduced, in Brazilian Portuguese, both to pronoun and to zero, while in European Portuguese only zero-reduction is allowed. In order to correctly resolve zero anaphora [2], NLP systems require (a) the correct identification of the zero anaphor and (b) the correct identification of the antecedent of the zero anaphor1. Several strategies can be used to achieve this goal. For machine learning techniques, an annotated corpus is required. This paper describes the methodology adopted in the construction of an annotated corpus for the study of zero anaphora in Portuguese, the ZAC corpus. To our knowledge, no such corpus exists at this time for the Portuguese language. The purpose of this linguistic resource is to promote the use of automatic discovery of linguistic parameters for anaphora resolution systems2. Our ultimate goal is to implement a module for zero anaphora resolution in the Portuguese grammar [3] developed under Xerox Incremental Parser (XIP) [4]. Because of the complexity of the linguistic phenomena involved, a detailed description of the different situations is provided. This paper will only focus on the annotation of subject zero anaphors. The main issues regarding zero anaphora in Portuguese are: indefinite subjects, either without verbal agreement marks or with first person plural or third person plural verbal agreement; position of the 
This paper presents a new rule-based method to identify Spanish zero pronouns. The paper describes the comparative evaluation of a baseline method for the identiﬁcation of zero pronouns with an approach that supplements the baseline by adding a set of restrictions treating impersonal sentences and other zero subject expressions. The identiﬁcation rules have been tested on a new corpus in which zero pronouns have been manually annotated (the Z-Corpus). The comparative evaluation shows that this rulebased method outperforms the baseline. Keywords zero pronoun identiﬁcation; pronominal zero anaphora; subject ellipsis 
Identifying translations in comparable corpora has inspired many studies in bilingual terminology extraction [4, 5]. Projection-based approaches, which are among the most popular ones, rely on a seed bilingual lexicon. Surprisingly, there is no careful analysis of the impact of the size the initial context and coverage of the lexicon. This is precisely the focus of this study. We observe that source context size and lexicon coverage inﬂuence robustness in projection-based term translation. In particular, we show that increasing the number of seed words by a factor of three leads to a 20% relative improvement in accuracy. 
This article reports an exploratory evaluation of the output quality of two prevalent English-Persian Machine Translation programs. The purpose of the research is to find out which program produces relatively better output, and what major linguistic bottlenecks MT programs will encounter in their processing of texts. Criteria were established in light of structural theories to solve the MT output from the perspective of Accuracy and Intelligibility. For each program, the mean score it obtained for its output and the rate of correctness of its translation of the testing points were calculated. An analysis of the mean score and the rate of correctness of each program generated the following findings about the output quality of these two programs: 1) Padideh Translator produces the best output. 2) The major linguistic bottlenecks in English-Persian MT programs occur in the areas of morphology, complex sentences, syntactic ambiguity and semantic analysis, generation of Persian, and long sentences. KEYWORDS Machine Translation, Accuracy, Intelligibility 1. INTRODUCTION MT technology is very important in the future of business. More and more business is being done on the Internet. People from every country are starting to consider the World Wide Web a mall from which they can buy anything they need [3]. Although English is spoken widely across world; it is the 4th most spoken language, and is by far the most extensively used language to communicate science, propagate technology and do business, not all potential users have access to this language This leaves many potential customers who do not understand the English-only websites on the internet. MT helps the business adapt to the customers.  The economic necessity of finding a cheaper solution to international exchange has resulted in continuing technological progress in terms of translation tools designed to automate and computerize the translation of natural language texts or to use computers as an aid to translation [2]. Although MT has some disadvantages, we will be able to use MT for cheaper and faster translation in near future. The present, relatively poor quality of translation yield by the computer: where total grammatical, semantic/associative meanings and pragmatic adequacy are concerned, it can lead the native speaker to reject the text on the grounds that it is strange, awkward, and even nonsensical [1] when reading. But there are also good reasons why we use machines to translate our texts. The primary reasons for using machine translation are speed, cost savings, and availability. John Hutchins [8] summarizes the reasons for using computers in translation as follows and insists any one of these may justify MT or computer aids: • Too much translation for humans • Technical materials too boring for humans • Greater consistency required • Need results more quickly • Not everything needs to be top quality • Reduce costs MT prompts researchers to ask whether it is possible that we have MT systems that can produce translation that is as good as human translation but faster and cheaper. What programs produce relatively better translation? And what difficulties are most MT programs confronted with? This article intends to probe into these questions and report an exploratory evaluation of the output quality of two prevalent English-Persian MT programs, namely Pars Translator and Padideh Translator.  71 Student Research Workshop, RANLP 2009 - Borovets, Bulgaria, pages 71–75  1.1. Machine Translation Evaluation (MTE)1 The general agreement about the basic features of MT evaluation are not, at the outset, subject to much dissention, but there are no collectively acknowledged and reliable methods and measures, and evaluation methodology has been the subject of much discussion in recent years. “As in other areas of NLP [5], three types of evaluation are recognized: Adequacy Evaluation to determine the fitness of MT systems within a specified operational context; Diagnostic Evaluation to identify limitations, errors and deficiencies; and Performance Evaluation to assess stages of system development or different technical implementations. Adequacy evaluation is typically performed by potential users and/or purchasers of system; diagnostic evaluation is the concern mainly of researchers and developers; and performance evaluation may be undertaken by researchers, developers, or potential users. MT evaluations typically include features not present in evaluations of other NLP systems. The quality of the raw translations, e.g., intelligibility, accuracy, appropriateness of style/register; the usability of facilities for creating and updating dictionaries, for post-editing texts, for controlling input language, for customization of documents, etc.; the extendibility to new language pairs and/or new subject domains; and cost-benefit comparisons with human translation performance.” According to Hutchins and Somers [4] the most obvious tests of the quality of a translation are: A. Accuracy, that is the extent to which the translation accurately renders the meaning of the source text, without intensifying or weakening any part of the meaning [7]; and B. Transparency, which is the extent to which the translation appears to a native speaker of the TL to have originally been written in that language, and conforms to the language’s grammatical, syntactic and idiomatic conventions [7]. The evaluation made in this research focused on the quality of the output, i.e., the translation of two prevalent English-Persian MT programs. 
Bootstrapping has been empirically proved to be a powerful method in learning lexico-syntactic patterns for extracting specific relations such as book-author and organizationheadquarters. However, it is not clear how to adapt this method to extract more general relations such as the employment-organization (EMP-ORG) relation. Relations like EMP-ORG are actually a set of relations which involves many nominals such as executive, secretary, officer, editor and soldier. To address this challenge, we propose a two-stage bootstrapping algorithm in this paper. The first stage is a commonly used bootstrapping framework, starting with a small set of seeds (entity pairs) and a large corpus to learn relation patterns which are further used to extract more seeds. We combined it with a second stage bootstrapping which takes as input the relation patterns learned in the first stage and aims to learn relation nominals and their contexts. After the two-stage bootstrapping learning, we incorporate features extracted from learned nominals and their contexts into a state-of-the-art SVM based relation extractor and we observe a 2% gain in F-measure. Keywords Information Extraction; Relation Extraction; Two-stage Bootstrapping 1. Introduction Relation Extraction is a challenging Information Extraction (IE) task which needs to find instances of predefined relations between pairs of entities. For example, there is an employment-organization (EMP-ORG) relation between entities CEO and Microsoft in the phrase the CEO of Microsoft. One way to combat this challenge is by applying machine learning techniques to a corpus with relation annotations. Supervised learning systems such as (Kambhatla 2004), (Zhou et al., 2005) and (Zhao and Grishman 2005) extract diverse lexical and syntactic features from an annotated corpus to train their system. While a supervised relation extraction system could achieve promising results, its portability to new domains is limited by the availability of annotated corpora. Porting such systems to new domains would involve substantial expert manual labor. Another direction in addressing this challenging problem is using semi-supervised methods such as bootstrapping techniques. A bootstrapping-based system only needs a small set of seed examples and an unannotated corpus. These seeds are used to generate relation patterns, which in  turn result in new examples being extracted from the corpus. For example, Brin (1998) uses bootstrapping for extracting pairs of book titles and authors from HTML documents. Agichtein and Gravano (2000) uses bootstrapping for extracting organization and location pairs which participate in the organization-headquarters relation from a large collection of plain texts. This paper characterizes these systems as single-stage bootstrapping since they carry out a loop from seeds to patterns and from patterns to seeds. Previous research in using single-stage bootstrapping for relation extraction has been focusing on relations which are specific and do not seem to contain subtypes of relations. However, there are many other relations which are really a set of relations. Take EMP-ORG for example; it contains at least 3 different types of relations, executive-organization, staff-organization and other-organization (where the contexts are not sufficient enough to determine whether a person holds a managerial or general staff position in the organization). One can imagine that, compared to the organization-headquarters relation, there are more diverse ways of stating employment than headquarters of organizations. In particular, relation patterns for EMPORG involve more relation nominals including executive, head, manager, programmer, editor and many others. Suppose we start with the seed Bill Gates and Microsoft; a simple question for single-stage bootstrapping is how could we learn nominal patterns with economist or editor, involving words other than synonyms of the position of Bill Gates such as CEO, chairman or head? To address this problem, we propose here a novel bootstrapping algorithm which we call two-stage bootstrapping1. The first stage is a commonly used singlestage bootstrapping learning framework, i.e. it starts with seeds to learn patterns and uses learned patterns to extract more seeds. The second stage bootstrapping takes as input the relation patterns learned from the first stage. It first picks out informative nominal patterns which are then used to generate queries for learning new nominals. For 
This paper presents a data-centric approach to XML information retrieval which benefits from XML document structure and adapts traditional text-centric information retrieval techniques to deal with text content inside XML. We implement our ideas in a configurable, general purpose XML retrieval library which can be tuned to operate on multilingual XML resources with different structure and can be used to extract relevant document fragments with different granularity according to user preferences. We present a rich query format and an algorithm for indexing and query processing. Keywords XML Retrieval, IR, XML-IR, XPath, document fragment, indexing schema, full-text indexing 1. Introduction The popularity of the eXtensible Markup Language (XML) has led large quantities of structured information to be stored in this format. Due to this ubiquity, there has lately been interest in information retrieval (IR) from XML. XML-IR presents different challenges than retrieval in text documents due to the semi-structured nature of the data. The goal is to take advantage of the structure of explicitly marked up documents to provide more focused retrieval results. For example, the correct result for a search query might not be a whole document, but a document fragment. Alternatively, the user could directly specify conditions to limit the scope of search to specific XML nodes. Previous work [2, 4] addresses several challenges specific to retrieval from XML documents: (1) Granularity of indexing units (Which parts of an XML document should we index?) (2) Granularity of the retrieved results(Which XML nodes are most relevant?) (3) Ranking of XML sub-trees (How should the ranking depend on the type of enclosing XML element and term frequency/inverse document frequency (tf-idf)?) The aim of this work is to define an approach for XML retrieval that can be used for indexing and search independently of the document structure. We call our approach context driven XML retrieval because indexing and search operate on parts of XML documents called contexts. These contexts represent searchable and retrievable parts of an XML document, and for us the IR problem can be viewed as the extraction of contexts that  match some search criteria. Traditional IR is a special case of XML-IR where the context has to be a whole document. Narrower contexts could be separate XML elements or their combinations. Our setting assumes knowledge of the XML document structure and the retrieval requirements. Thus an administrator creates indexing and retrieval rules for different XML document corpora. Each corpus requires different indexing rules to define contexts and relations between them – document fragments referable at search time. Using this context driven approach we address challenges (1) and (2). Concerning ranking (3), we employ a strategy which combines the unstructured and structured IR scoring techniques. In the paper we present a scalable index structure, indexing, search algorithms, indexing rules and query language format. We implement our ideas in a general purpose XML retrieval library that can be integrated in different kinds of applications: web applications, standalone systems, web services. The rest of the paper is organized as follows: Section 2 describes related work; Section 3 describes motivation; Section 4 presents implementation details. Section 5 concludes the paper and describes future work. 2. Related work XML retrieval systems vary according to the query language, index structure, document preprocessing, indexing and scoring algorithms they employ. A great variety of XML query languages already exist. Standard ones proposed by W3C are XPath and XQuery. Unfortunately, they do not reflect IR properties such as weighing, relevance-oriented search, data types and vague predicates, structural relativism [1, 14]. Amer-Yahia et al. [4] classifies XML query languages into three classes: keyword query languages (KQL) [5, 6, 12, 13]; tag & KQL [6]; path & KQL [7, 8, 12]; XQuery & KQL [10]. The query language we introduce is a path and KQL in XML format, and most related to XPath 2.0, XIRQL, XXL, NEXI CAS queries. Different term and structure statistics are implemented in separate XML-IR systems. We share the idea of Mass and Mandelbrod [11] that an XML index consists of a set of separate full-text indices. For full-text search we use the Apache search API Lucene [9].  83 Student Research Workshop, RANLP 2009 - Borovets, Bulgaria, pages 83–88  The context driven approach we present can be classified as Content-And-Structure (CAS) retrieval under the system developed by the Initiative for the Evaluation of XML Retrieval (INEX) [12]. 3. Motivation In addition to the growing interest in XML retrieval, we had a practical need for an IR system for XML documents. In order to aid annotation efforts, we needed a platform independent search engine that could be tuned for specific applications. Since the document structure was known and important, we wanted to create indexing and retrieval rules to improve retrieval. The system we present allows exactly such application-specific indexing and search. 4. Context Driven XML Retrieval An XML document is a tree-like data structure which consists of three node types: elements, attributes, character data/text. XML document tree nodes are instances of elements called tags/markups, which can be either empty or have nested elements or text nodes. Attributes are name-value pairs attached to tags. Figure 1 is an example of a textile multimedia XML document created by our group for the purposes of the AsIsKnown project [15]. The example document contains text and images annotated with concepts from a textile knowledge base. Text is delimited in sentences which are organized in paragraphs.  the problem by using an extension of XPath with variables. The expression below extracts the sentences containing the text of the title: {x:=/title/descendant::text()}/s[contains(descendant::text(), $x)] We set the variable x to be equal to the document title text. The XPath engine extracts the sentences whose text contains the value of the x variable. 4.1 System architecture We implement our system in an XML retrieval library. Each document corpus has a separate XML index in a central XML Index Repository. Each index has its own model, defined in an indexing schema. The indexing schemas specify how to extract indexing units (XML subtrees) called contexts from XML documents with a common structure and defines how the separate contexts are related. Our basic assumption is that a context is a document fragment whose content is indexed in several text fields. A field is a name-value pair whose value is character data. Contexts and fields can be referred to in search queries. An indexing schema is added to an empty index on its creation. Existent XML indices are populated with documents according to the definitions in their corresponding indexing schema. For each context defined in the indexing schema we create and populate a full-text index called context index. The rest of the subsection gives an overview on the system architecture (Figure 2)  Figure 1. Multimedia XML Document There exist W3C standard languages for navigation through XML documents (XPath) and querying (XQuery). We employ XPath in the implementation of our framework. After the evaluation of an Xpath expression, a set of XML nodes are retrieved. For example, if we want to extract the sentences which contain the word pattern from our example XML document in Figure 1, we can use the XPath expression: //s[contains (descendant::text(), “pattern”)] . If we need sentences containing the document’s title, we need variables to store temporary results and be used in the search expression. We deal with  Figure 2. System Architecture The search engine lifecycle has two sequential phases: system initialization and user interactions. When the system is started a processing module reads system configurations from a configuration file. This file specifies an indexing/search analyzer, document storage, and result extractor implementation classes. The opportunity to configure different implementations makes our framework highly adaptable to various search scenarios. We can use this to tune text analysis, document access policy, result formatting and extraction. Instances of the configured implementations are created by reflection and are made  84  available at runtime. The indexing and retrieval tasks are performed by the indexer and searcher operational modules which manipulate the XML indices, system files and interact with the already instantiated objects. 4.2 Indexing schema and index structure Each XML index consists of an indexing schema and one or more full-text indices. The indexing schema is an XML document which defines indexing and extraction rules. Central concepts are context and field, as defined in the previous subsection. Their usage is clarified with the following example. Assume that we have a corpus with documents with the same structure as the one in Figure 1 and we want to retrieve particular paragraphs/ images. For the purpose we define 2 independent contexts: paragraph and image. Our aim is to search for a combination of text matches and concepts. We need to create two fields for the paragraph context (text and concept) and one for the image context (concept). We add one more requirement we want to extract paragraphs whose adjacent paragraphs and images comply with supplementary search criteria. In this case we need to define some kind of relation between a paragraph and its adjacent paragraphs and images. We call this type of relation coordination between contexts. In the indexing schema the contexts are defined as context elements, fields are elements nested in context elements and coordination between contexts is expressed by nesting context elements.  Figure 5. Lucene and XML Index Structure The Lucene index (left) is an inverted index consisting of a set of Lucene documents. Each Lucene document carries a unique identifier and contains fields. The XML index (on the right) contains an indexing schema document and a set of Lucene context indices. They are populated with Lucene documents with identifiers which encode the system identifier of the XML document, path to an XML context node and paths to its related XML nodes. The Lucene documents are populated with fields as defined in the indexing schema document. An illustrative example is to index the document in Figure 1. We want to search by concept to extract paragraphs that match one concept pattern with adjacent paragraphs or images matching another pattern. Such a relation between structural document units allows us to create complex search queries. An indexing schema satisfying our requirements is given in Figure 6.  Figure 4. Indexing Schema Logical Tree View. Each context and field element has an identifier and is associated with an XPath expression. Indexing schema elements are relative to their parent elements, i.e. their identifiers are unique within the scope of their parent element and their XPath expressions are applied relative to the nodes extracted by the XPath expressions of their parent element. The schema root element denotes the default context, i.e. the whole XML document. Field elements have no nested elements and can be boosted. For context elements with child field elements we create separate full-text indices in the XML index repository. Figure 5 illustrates the Lucene index (full-text index) and the XML index structures.  Figure 6. Example indexing schema The index created according to the definitions of the indexing schema on Figure 6 contains five full-text indexes (one for each context with a field). If the document in Figure 1 has a system identifier f1, the content of the separate full-text indexes after the indexing would be:  Lucene context index  Lucene document IDs Field content  paragraph  f1_p#1  paragraph previous_paragraph; (no data added)*  The concept URIs in the first paragraph.  paragraph following_paragraph (no data added)*  paragraph  f1_p#1@@@f1_img#1  previous_images  paragraph following_images (no data added)*  The concept URIs in the first image.  85  Table 1. Content of Lucene indices for the example XML Index1 Indexing is incremental. When a new XML documents is added to an XML index, its Lucene context indices are updated by creating and populating Lucene documents. 4.3 Indexing algorithm Below (Figure 7) is listed the indexing algorithm which is recursive in nature. The recursive structure is inherited from the nesting of contexts.  Queries have a recursive structure similar to the indexing schema. The context and field elements in queries refer to corresponding elements in the indexing schema by ID. The parent-child element relationship for both contexts and fields should follow the order in the indexing schema. The element content of query context elements consists of: - field element(s) – their element content is transformed to a Lucene query. The supported full-text queries for a field include term, phrase, fuzzy, Boolean, span, wildcard, range queries. Search results are sorted by tf-idf. - AND; OR | and;or elements – recursive multiargument operations denoting intersection and union of search results returned for sets of context arguments (AND; OR) or field arguments (and; or). In either case, results are sorted by tf-idf. The search algorithm is presented below (Figure 9).  Figure 7. Search procedure 4.4 Query syntax and search algorithm Search is performed within a single index in order to retrieve relevant content. The search criteria are specified in a query XML document whose format is presented below. Contexts and fields2 are referred to in search queries. Figure 8 (below) illustrates the query structure.  Figure 8. Search query structure 
Attentional State Theory and Rhetorical Structure Theory are two predominant theories of discourse parsing. Combining these two approaches, in this paper, we describe a novel approach for discourse parsing. The resulting discourse tree structure retains following properties: structure of purpose from Attentional State Theory and relations between sentences from Rhetorical Structure Theory. We demonstrate the utility of our model by constructing a summarization system. Keywords Discourse Parsing, Attentional state theory, Rhetorical structure theory, Coreference, Cohesion, Similarity metrics, Sentence similarity 
In this paper, we investigate an unsupervised approach to Relation Extraction to be applied in the context of automatic generation of multiple-choice questions (MCQs). The approach aims to identify the most important semantic relations in a document without assigning explicit labels to them in order to ensure broad coverage, unrestricted to predefined types of relations. The paper examines three different surface pattern types, each implementing different assumptions about linguistic expression of semantic relations between named entities. Our main findings indicate that the approach is capable of achieving high precision rates and its enhancement with linguistic knowledge helps to produce significantly better patterns. The intended application for the method is an e-learning system for automatic assessment of students’ comprehension of training texts; however it can also be applied to other NLP scenarios, where it is necessary to recognise important semantic relations without any prior knowledge as to their types. Keywords Information Extraction, Relation Extraction, Biomedical domain, MCQ generation. 1. Introduction Information Extraction (IE) is an important problem in many information access applications. The goal is to identify instances of specific semantic relations between named entities of interest in the text. As is known from the literature, Relation Extraction in the biomedical domain is quite difficult compared to other domains, such as news domain, due to the inherently complex nature of its texts: biomedical Named Entities (NEs) are expressed in various linguistic forms such as abbreviations, plurals, compounds, coordination, cascades, acronyms and apposition. Sentences in such texts are syntactically complex as the subsequent Relation Extraction phase depends upon the correct identification of the named entities and correct analysis of linguistic constructions expressing relations between them (e.g., [3, 21]). The main advantage of the approach presented in this paper is that it can cover a potentially unrestricted range of semantic relations while most supervised and semisupervised approaches can learn to extract only those relations that have been exemplified in annotated text,  Viktor Pekar Oxford University Press Great Clarendon St. Oxford, OX2 6DP, UK viktor.pekar@oup.com seed patterns or seed named entities. Moreover, our approach is suitable in situations where a lot of unannotated text is available as it does not require manually annotated text or seeds. These properties of the method can be useful, specifically, in such applications as Multiple-Choice Question generation [12] or a preemptive approach in which viable IE patterns are created in advance without human intervention [20,15]. In the future, we plan to employ the Relation Extraction method for automatic MCQ generation, where it will be used to find relations and named entities in educational texts that are important for testing students’ familiarity with key facts contained in the texts. In order to achieve this, we need an IE method that has a high precision and at the same time works with unrestricted semantic types of relations (i.e. without reliance on seeds), while recall is of secondary importance to precision. 2. Related Work There is a large body of research dedicated to the problem of extracting relations from general-domain texts, and from biomedical texts in particular. Most previous work focused on supervised methods and tried to both extract relations and assign labels describing their semantic types [16 and 5, among many others]. As a rule, these approaches required a manually annotated corpus, which is very laborious and time-consuming to produce. Semi-supervised and unsupervised approaches relied on seeds patterns and/or examples of specific types of relations [1, 17, 20, and 15]. They often employ bootstrapping techniques which use a small set of seeds in order to start the learning process. An unsupervised approach based on clustering of candidate patterns for the discovery of the most important relation types among NEs from a newspaper domain was presented by [6]. In the biomedical domain, most approaches were supervised and relied on regular expressions to learn patterns [4], while semi-supervised approaches exploited pre-defined seed patterns and cue words [2, 7, 11]. Supervised approaches or those based on manuallywritten extraction rules that have been previously used for Relation Extraction in the biomedical domain are  
This paper presents a novel approach to automatic captioning of toponym-referenced images. The automatic captioning procedure works by summarizing multiple web-documents that contain information related to an image’s location. Our summarizer can generate both query-based and language model-biased multidocument summaries. The models are created from large numbers of existing articles pertaining to places of the same “object type”. Evaluation relative to human written captions shows that when language models are used to bias the summarizer the summaries score more highly than the non-biased ones. Keywords Multi-Document Summarization, Image Captioning, Lan- guage Models, Statistical Methods, NLP 
In this paper we extend a shallow parser [6] with prepositional phrase attachment. Although the PP attachment task is a well-studied task in a discriminative learning context, it is mostly addressed in the context of artiﬁcial situations like the quadruple classiﬁcation task [18] in which only two possible attachment sites, each time a noun or a verb, are possible. In this paper we provide a method to evaluate the task in a more natural situation, making it possible to compare the approach to full statistical parsing approaches. First, we show how to extract anchor-pp pairs from parse trees in the GENIA and WSJ treebanks. Next, we discuss the extension of the shallow parser with a PP-attacher. We compare the PP attachment module with a statistical full parsing approach [4] and analyze the results. More speciﬁcally, we investigate the domain adaptation properties of both approaches (in this case domain shifts between journalistic and medical language). Keywords prepositional phrase attachment, shallow parsing, machine learning of language 
The development of the Web 2.0 led to the birth of new textual genres such as blogs, reviews or forum entries. The increasing number of such texts and the highly diverse topics they discuss make blogs a rich source for analysis. This paper presents a comparative study on open domain and opinion QA systems. A collection of opinion and mixed fact-opinion questions in English is defined and two Question Answering systems are employed to retrieve the answers to these queries. The first one is generic, while the second is specific for emotions. We comparatively evaluate and analyze the systems’ results, concluding that opinion Question Answering requires the use of specific resources and methods. Keywords Question Answering, Multi-perspective Question Answering, Opinion Annotation, Opinion Mining, NonTraditional Textual Genres. 1. Introduction Recent years’ statistics show that the number of blogs has been increasing at an exponential rate. A research of the Pew Institute [1] shows that 2-7% of Internet users created a blog and that 11% usually read them. Moreover, researches in different fields proved that this new textual genre is a valuable resource for large community behavior analysis, since blogs address a great variety of topics from a high diversity of social spheres. A common belief is that they are written in a colloquial style, but [2] shows that the language of these texts is not restricted to the more informal levels of expression and a large number of different genres are involved. As a consequence, free expressions, literary prose and newspaper writing coexist without a clear predominance. When using this textual genre, people tend to express themselves freely, using colloquial expressions employed only in day-by-day conversations. Moreover, they can introduce quotes from newspaper articles, news or other sources of information to support their arguments, make references to previous posts or the opinion expressed by others in the discussion thread. Users intervening in debates over one specific topic are from different geographical regions and belong to diverse cultures. All the abovementioned features make blogs a valuable source of  information that can be exploited for different purposes. However, due to their language being heterogeneous, it is complex to understand and formalize in order to create effective Natural Language Processing (NLP) tools. At the same time, due to the high volume of data contained in blogs, automatic NLP systems are needed to manage the language understanding and generation. Analyzing emotions and/ or opinions expressed in blog posts could also be useful to predict people’s opinion or preferences about a product or an event. One of the other possible applications is an effective Question Answering (QA) system, able to recognize different queries and give the correct answer to both factoid and opinion questions. 2. Related work QA is the task in which, given a set of questions and a collection of documents where the answers can be found, an automatic NLP system is employed to retrieve the answer to these queries in Natural Language. The main difference between QA and Information Retrieval (IR) is that in the first one, the system is supposed to output the exact answer snippet, whereas in the second task whole paragraphs or even documents are retrieved. Research in building factoid QA systems has a long tradition; however, it is only recently that studies have started to focus on the creation and development of opinion QA systems. Recent years have seen the growth of interest in this field, both by the research and publishing of studies on the requirements and peculiarities of opinion QA systems [4] as well as the organization of international conferences that promote the creation of effective QA systems both for general and subjective texts, such as the Text Analysis Conference (TAC)1. Last year’s TAC 2008 Opinion QA track proposed a mixed setting of factoid and opinion questions (so called “rigid list” and “squishy list”), to which the traditional systems had to be adapted. Participating systems employed different resources, techniques and methods to overcome the newly introduced difficulties related to opinion mining and polarity classification. The Alyssa system [5], which performed better in the “squishy list” questions than in the 
Feature norms can be regarded as repositories of common sense knowledge for basic level concepts. We acquire from very large corpora feature-norm-like concept descriptions using a combination of a weakly supervised method and an unsupervised method. The success in identifying the speciﬁc properties listed in the feature norms as well as the success in acquiring the classes of properties present in the norms are reported. Keywords basic level categories, common-sense knowledge, feature norms 
A novel method for unsupervised acquisition of knowledge for taxonomies of concepts from raw Wikipedia text is presented. We assume that the concepts classiﬁed under the same node in a taxonomy are described in a comparable way in Wikipedia. The concepts in 6 taxonomies extracted from WordNet are mapped onto Wikipedia pages and the lexico-syntactic patterns describing semantic structures expressing relevant knowledge for the concepts are automatically learnt. Keywords wikipedia, unsupervised knowledge acquisition, taxonomy 
This paper presents a set of experiments performed on parsing the Basque Dependency Treebank. We have concentrated on treebank transformations, maintaining the same basic parsing algorithm across the experiments. The experiments can be classified in two groups: 1) feature optimization, which is important mainly due to the fact that Basque is an agglutinative language, with a rich set of morphosyntactic features attached to each word, 2) graph transformations, ranging from language independent methods, such as projectivization, to language specific approaches, as coordination and subordinated sentences, where syntactic properties of Basque have been used to reshape the dependency trees used for training the system. The transformations have been tested independently and also in combination, showing that their order of application is relevant. The experiments were performed using a freely available state of the art data-driven dependency parser [11]. Keywords Dependency parsing, treebank parsing, agglutinative language. 
Discourse theories claim that text gets meaning in context. Most summarization systems do not take advantage of this. They assess the relevance of each passage individually rather than modeling the way context aﬀects the relevance of passages. This paper presents a framework for graph-based summarization in order to model relations in text, so that the passages can be viewed in a broader context. The result is a summarization system which is more in line with discourse theory but still fully automatic. I evaluated the content selection performance of an implementation of the framework in diﬀerent conﬁgurations. The system signiﬁcantly outperforms a competitive baseline (and participant systems) on the DUC 2005 evaluation set. Keywords Query-based summarization, content selection, semantic networks, discourse structure, graph theory. 
In this paper, we present a novel approach for automatic summarization. CBSEAS, the system implementing this approach, integrates a method to detect redundancy at its very core, in order to produce more expressive summaries than previous approaches. The evaluation of our system during TAC 2008 —the Text Analysis Conference— revealed that, even if our system performed well on blogs, it had some failings on news stories. A post-mortem analysis of the weaknesses of our original system showed the importance of text structure for automatic summarization, even in the case of short texts like news stories. We describe some ongoing work dealing with these issues and show that ﬁrst experiments provide a signiﬁcant improvement of the results. Keywords Multi-document Summarization; Text structure; Evaluation; Text Analysis Conference. 
We explore the adaptation of English resources and techniques for text sentiment analysis to a new language, Spanish. Our main focus is the modification of an existing English semantic orientation calculator and the building of dictionaries; however we also compare alternate approaches, including machine translation and Support Vector Machine classification. The results indicate that, although languageindependent methods provide a decent baseline performance, there is also a significant cost to automation, and thus the best path to long-term improvement is through the inclusion of language-specific knowledge and resources. 1. Introduction Sentiment analysis refers to the automatic determination of subjectivity (whether a text is objective or subjective), polarity (positive or negative) and strength (strongly or weakly positive/negative). It is a growing field of research, especially given the gains to be obtained from mining opinions available online. Approaches to sentiment analysis have tackled the problem from two different angles: a word-based or semantic approach, or a machine learning (ML) approach. The word-based approach uses dictionaries of words tagged with their semantic orientation (SO), and calculates sentiment by aggregating the values of those present in a text or sentence [17]. The ML approach uses collections of texts that are known to express a favorable or unfavorable opinion as training data, and learns to recognize sentiment based on those examples [13]. Our approach is semantic, and makes use of a series of dictionaries, additionally taking into account the role of negation, intensification and irrealis expressions. We believe that a semantic approach offers the advantage of taking many different aspects of a text into account. One of the disadvantages of a semantic approach is that the resources necessary for a new domain or a new language need to be built from scratch, whereas a machine-learning approach only needs enough data to train. In this paper we show that porting to a new language, Spanish, requires only a small initial investment, while providing the opportunities for further improvement available only to semantic methods. For comparison, we have taken three approaches to performing sentiment analysis in a new language. Our main approach involves deploying Spanish-specific 50  resources, which we build both manually and automatically. The second approach, used in Bautin et al. [4] and Wan [18], consists of translating the texts into English, and using an existing English calculator. Finally, the third approach builds unigram Support Vector Machine classifiers from our Spanish corpora. Our evaluation on multi-domain corpora indicates that, although translation and machine learning classification both perform reasonably well, there is a significant cost to automated translation. A languagespecific SO Calculator with dictionaries built using words that actually appear in relevant texts gives the best performance, with significant potential for improvement. 2. The English SO Calculator Our semantic orientation calculator (SO-CAL) uses five main dictionaries: four lexical dictionaries with 2,257 adjectives, 1,142 nouns, 903 verbs, and 745 adverbs, and a fifth dictionary containing 177 intensifying words and expressions. Although the vast majority of the entries are single words, our calculator also allows for multiword entries written in regular expression-like language. The SO-carrying words in these dictionaries were taken from a variety of sources, the three largest a corpus of 400 mixed reviews from Epinions.com, a 100 text subset of the 2,000 movie reviews in the Polarity Dataset [12], and positive and negative words from the General Inquirer dictionary [15]. Each of the open-class words were given a hand-ranked SO value between 5 and -5 by a native English speaker. The numerical values were chosen to reflect both the prior polarity and strength of the word, averaged across likely interpretations. For example, the word phenomenal is a 5, nicely a 2, disgust a -3, and monstrosity a -5. The dictionary was later reviewed by a committee of three other researchers in order to minimize the subjectivity of ranking SO by hand. SO-CAL also implements a modified version of contextual valence shifting as originally proposed by Polanyi and Zaenen [14], including negation and intensification. We have also added irrealis blocking. Our approach to negation differs from Polanyi and Zaenen’s in that negation involves a polarity shift instead of a switch: A negated adjective is shifted by a fixed amount (4) toward the origin. This means that the negation of a strongly negative word (like terrible) will be neutral or weakly negative (not terrible -5 + 4 = -1 instead of 5), while the negation of a weakly positive word like nice is equally negative (not nice 2 – 4 = -2).  International Conference RANLP 2009 - Borovets, Bulgaria, pages 50–54  The calculation of intensification is somewhat more sophisticated than simple addition and subtraction. Each expression in our intensifier dictionary is associated with a multiplier value. For instance, very has a value of .25, which means the SO value of any adjective modified by very is increased by 25%. We also included three other kinds of intensification that are common within our genre: the use of all capital letters, the use of exclamation points, and the use of discourse but to indicate more salient information (e.g., …but the movie was GREAT!). Some markers indicate that the words appearing in a sentence might not be reliable for the purposes of sentiment analysis. We refer to these using the linguistic term irrealis. Irrealis markers in English include modals (would, could), some verbs (expect, doubt), and certain kinds of punctuation (questions, quotations). When SOcarrying words appear within the scope of these markers, our calculator ignores them. Lexicon-based sentiment classifiers generally show a positive bias [10], likely the result of a human tendency to favor positive language [6]. In order to overcome this bias, we increase the final SO of any negative expression (after other modifiers have applied) by a fixed amount (currently 50%). For initial testing, we use the 400 text Epinions corpus (50 texts in each of eight different product types), the other 1,900 texts in the Polarity Dataset (Movie), and a 2,400 text corpus of camera, printer, and stroller reviews (Camera) taken from a larger set of Epinions reviews also used by Bloom et al. [5], for a total of 4,700 texts split equally between positive and negative. Table 1 shows the performance of the English calculator with all features, and disabling the three types of valence shifters (negation, intensification and irrealis) and the extra weight on negative words. An asterisk (*) indicates that a chi-square test yielded significance at the p<0.05 level, as compared to the result with all features enabled. Whereas not all the differences are statistically significant, it does seem that the set of features that we have chosen has a positive effect on performance.  Table 1. Effects of disabling various features  Features All No Neg No Int No Irreal No Neg W  Percent Correct by Corpus  Epinions Movie Camera Total  80.3  76.4 80.3 78.7*  75.8* 74.6 76.1* 75.4*  79.0* 74.7 77.5* 76.5*  78.8* 74.8 79.6 77.6*  71.8* 75.6 71.5* 73.2*  3. The Spanish SO Calculator Compared to English, Spanish is a highly inflected language, with gender and plural markers on nouns, as well as a rich system of verbal inflection (45 possible verb forms). In the English version of SO-CAL, the only external software we made use of was the Brill tagger [7]; lemmatization of noun and verbs was simple enough to be carried out during the calculation. For Spanish, we used a high-accuracy statistical tagger, the SVMTool [9],  51  and we adapted a 500,000+ word lemma dictionary included in the FreeLing software package1, which we used to both lemmatize the words and to add more detail to the basic verb tags assigned by SVMTool (each verb is lemmatized, but tagged with information about its tense and mood). We found that some sentiment-relevant words were not being lemmatized properly, so we also implemented a second layer of lemmatization within the calculator. Most of the Python code written for the English version of SO-CAL could be reused. With regards to detecting negation, intensification, and modifier blocking, it was necessary to take into account the fact that in Spanish adjectives appear both before and (more commonly) after the noun. The most interesting difference was the fact that verb forms in Spanish provide irrealis information. In particular, the conditional tense and the imperative and subjunctive moods often serve to indicate that the situation being referred to is not in fact the case. Thus, in Spanish we used a mixture of word and inflection-based irrealis blocking, using the same words as the English version whenever possible. We built new Spanish dictionaries, including dictionaries for adjectives, nouns, verbs, adverbs and intensifiers. For intensifiers, given the fact that they are closed-class and highly idiosyncratic, we simply created a new list of 157 expressions, based on the English list. For the open-class dictionaries, we tested three different methods of dictionary-building; we compare their performance on the Spanish corpus in Section 5. The first set of dictionaries started with the English dictionaries for each part of speech, which we translated automatically into Spanish, preserving the semantic orientation value for each word. For the automatic translation we used, in turn, two different methods. The first was an online bilingual dictionary, from the site www.spanishdict.com. We extracted the first definition under the appropriate syntactic category, ignoring any cases where either the English or the Spanish were multiword expressions. The second automatic translation method involved simply plugging our English dictionaries into the Google translator and parsing the results. For the second method of dictionary creation, we took the lists from Spanishdict.com and manually fixed entries that were obviously wrong. This involved mostly removing words in the wrong dictionary for their part of speech, but also changing some of the values (less than 10% for each dictionary). This hand-correction took a native speaker of Spanish about two hours to complete. Finally, the third method consisted in creating all dictionaries from scratch. Our source corpora created for this project consists of reviews extracted from the Ciao.es review website. Following the basic format of the Epinions corpus, we collected 400 reviews from the domains of hotels, movies, music, phones, washing machines, books, cars, and computers. Each category 
This paper explores the inﬂuence of text preprocessing techniques on plagiarism detection. We examine stop-word removal, lemmatization, number replacement, synonymy recognition, and word generalization. We also look into the inﬂuence of punctuation and word-order within N-grams. All these techniques are evaluated according to their impact on F1-measure and speed of execution. Our experiments were performed on a Czech corpus of plagiarized documents about politics. At the end of this paper, we propose what we consider to be the best combination of text pre-processing techniques.  2 Pre-processing Techniques Plagiarism detection can employ various preprocessing techniques in order to improve the accuracy or decrease the number of features that need to be processed. Figure 1 shows the text pre-processing step-by-step. The most essential block is Tokenization, which extracts single words from the structured text. Punctuation marks can be extracted if they are required by other processes. The other blocks represent optional techniques that can be applied if the user wishes.  Corpus  Tokenization  Keywords  Stop-word removal (STR)  Stop-word dictionary  Plagiarism, Copy Detection, Natural Language Processing, Stop-words, Lemmatization, Synonymy, WordNet, Thesaurus.  Lemmatization (LM)  Lemma dictionary  
Many NLP systems make use of various lexicons and dictionaries. However, unknown words are a major problem for such resources when applied to real-life data. We propose a method that combines nite state techniques and web queries to deliver possible analyses for a given unknown word and to generate its paradigm. We ensure the general applicability of our approach by applying it to a test set of Dutch words. 
Full-Parsing systems able to analyze sentences robustly and completely at an appropriate accuracy can be useful in many computer applications like information retrieval and machine translation systems. Increasing the domain of locality by using tree-adjoining-grammars (TAG) caused some researchers to use it as a modeling formalism in their language application. But parsing with a rich grammar like TAG faces two main obstacles: low parsing speed and a lot of ambiguous syntactical parses. In order to decrease the parse time and these ambiguities, we use an idea of combining statistical chunker based on TAG formalism, with a heuristically rule-based search method to achieve the full parses. The partial parses induced from statistical chunker are basically resulted from a system named supertagger, and are followed by two different phases: error detection and error correction, which in each phase, different completion heuristics apply on the partial parses. The experiments on Penn Treebank show that by using a trained probability model considerable improvement in full-parsing rate is achieved. Keywords Full Parsing, Partial Parsing, Tree Adjoining Grammar, SuperTagging 1. Introduction In many applications like information retrieval and Rulebased machine translation systems, accurate deep parse structure of a sentence is required; hence a lot of research is being done on introducing methods to produce deep hierarchical syntactical structure of a given natural language sentence [6]. Over the last decade, there has been a great increase in the performance of parsers. Current parsers achieve to a score of about 90% when measuring just the accuracy of choosing these dependencies [4, 5 and 7]. The choice of formalism does not change the parsers’ accuracy significantly, because in all approaches wordword dependencies are used as the only underlying information. But because of the inherent ambiguity in the natural languages, achieving to a full parses of a sentence is a big challenges. Tree-adjoining-grammars (TAG) have some specific features, which are interested by researchers to be used as modeling formalisms in their language application. The parsing methods based on this formalism involve different problems such as a lot of ambiguities and low parsing  speed. One of the main parsing algorithms based on TAG formalism is presented by Van Noord [10] which runs in O(n6) time complexity. This complexity in a real-size grammar (like XTAG [9]) is not acceptable, especially for a more complicated system like information retrieval and machine translation systems. Also, because of the ambiguities in the resulted parses, the output of this algorithm must be disambiguated by another approach. To overcome the mentioned problems, we use an alternative approach which is based on statistical partial parsers. One of the partial parser systems which alleviate the TAG formalism problems in time complexity and ambiguity is named supertagging, proposed by (Bangalore and Joshi [2]). The idea behind supertagging is to extend the notion of “tag” from a part of speech to a tag that represents rich syntactic information. Each supertag can be thought as an element in TAG formalism. They also introduced “lightweight” parsing which follows the supertagging. If words in a string can be tagged with this rich syntactic information, then Bangalore and Joshi claim, the remaining step of determining the actual syntactic structure is trivial [2]. They propose a “lightweight dependency parser” (LDA) which is a heuristically-driven, very simple program that creates a dependency structure from the supertags of the words. While the supertagging only requires a notion of syntactically relevant features, the stage of determining a syntactic structure requires a grammar that uses these syntactically relevant features. Given the correct supertags, LDA performs with an unlabeled accuracy of about 95%. Although supertagging is a worthwhile notion pursuing the full-parsing, but approaching to a full-parse by the proposed lightweight parser has a major obstacle. Bangalore announced the accuracy of supertagging to be about 92% based on the experiments done on Penn Treebank [1]. This accuracy is not satisfiable to generate a complete deep structure of the sentence by using lightweight dependency analyzer. In a sentence with 15words length, LDA parser determines the correct full-parse of the sentence with the probability about 95% * (0.92)15 = 27.5%. For longer sentences, lower accuracy has been achieved. Nasr and Rambow try to improve the accuracy by changing the heuristic dependency linker with a nonlexical chart parser [8]. Like the original supertagger, their  71 International Conference RANLP 2009 - Borovets, Bulgaria, pages 71–75  method still has no access to lexical information and only information about the supertags is combined with a chart parser. They cut the error rate of the heuristic LDA by more than half. In this paper, we present a full-parsing method by combining different heuristics with lightweight shallow parser. Our approach is still in the spirit of Bangalore’s work in the sense that lexical information is only used during supertagging. The idea of this paper is based on finding the erroneous supertags which are most probable to be wrongly assigned, and then replacing them with proper candidates. 2. Full Parsing Although full parsing based on fully correct supertags is very time-efficient [8], but acquiring the fully correct supertagging itself is the main obstacle. The probability of assigning correct supertag set S={s1,s2,…,sn} to all words of a sentence W={w1,w2,…,wn} is equal to product of the probability of correct assigning a single supertag si to i-th word wi (i.e. p(si | wi) ). Based on the experiments done by Bangalore, the probability p(si | wi) is equal to 92.2%. So full parsing probability by linking all supertags resulted from supertagging process for a sentence with 15 words length is equal almost be 29.5% and with 25 words near to 13.1%. To overcome this problem, n-best supertagging that assign n-best supertags to each word was proposed by [1]. Based on this approach, by setting n = 3, supertagging correctness increased to 97.1% and accordingly the rate of fully-parsing for whole words in a sentence improved efficiently. (e.g. 74.5% for sentences with 15 words length and 64.3% for sentences with 25 words length). But using n-best supertags followed by lightweight analyzer is equal to find a combination of these supertags which satisfies all available syntactical constraints on TAG. For 3-best supertagger in 15 words length sentence, there are 315 = 14,348,907 combinations which should be checked in order to choose the correct combination. In [8] a dynamic programming method to resolve this complexity is used. This problem can be seen as a search problem in the state space of all supertags assigned to the words of the sentence. The initial state is a combination of those tags which are assigned by supertagger and the goal states are those which LDA succeed to make a fully dependency linkage between the supertags and hence in those states full-syntactic structure of the sentence is generated. Hillclimbing approach is chosen for search method and the accuracy of LDA is calculated as a heuristic performance measure of problem. 3. Search in the Supertag State Space Same as other local search problems, the search can be divided into two distinct phases: error detection and  correction. In fact, instead of associating n-best supertag to every word of the sentence, the most probable erroneous supertags resulted from n-best supertagging are detected and substituted with proper alternatives which are proposed by an error correction algorithm.  In each non-goal state (i.e. partial parse), error nodes are supertags that are wrongly assigned and therefore they are the cause of preventing LDA to produce exactly one dependency diagram as the correct full parse tree of the sentence. The result of LDA is a dependency diagram which links all supertags based on its syntactical behavior [1]. Four our experiments, we gathered 341 sentences, which are failed to be parsed by LDA, and analyzed the failing reasons. In the case of failing LDA to generate the full connected structure, one of the three cases may happen. These cases are shown in Table 1. As it’s shown in the table, different heuristics for detecting the faulty nodes are demonstrated too. These heuristics show the supertags which are most probable to be wrong and should to be replaced with proper candidates.  Table 1. The cases in which supertagger fails to generate the full syntactic structure  Case 1  The LDA output diagram is not fully connected graph and it contains multiple partial graphs. In this case, substitution slots of some supertags are not filled by other tags. From the total 341 faulty test sentences, this case appears in 172 sentences, that is about 50% of all corpus fails to be parsed because of this problem.  Proposed faulty nodes in case 1  The partial trees’ root is mostly an erroneous node, which its supertag should be substituted to better one (i.e. should to replace with another supertag which contains more substitution slots in order to make a link with other partial trees). Changing this node with proper one could correct 150 sentences from the total 172 faulty sentences of this case.  Case 2  Supertags of some words do not participate in the dependency diagram and so some child nodes are not included in its parent diagram. Either footnode or substitution slots are required to make a link between the orphan child and parent node. This case appears in more than 30% of test sentences.  Proposed faulty nodes in case 2  The root node of trees that some of their children are missed has a large potential to be wrongly assigned supertag. These missed children can be seen as slots that are not filled. In our experiments, the total faulty sentences of this case have been corrected by changing this node.  72  Case 3  In the 15% of mentioned faulty test sentences, the LDA output diagram has cycles in its dependencies and therefore is not a valid dependency structure diagram.  Proposed faulty nodes in case 3  In the case of existence any loop in the diagram, the verb nodes are usually ambiguous and have a large potential to be erroneous. By using this heuristic, the full parse structure of 70% of all unparsed sentences of case 3 is correctly acquired.  In each of the mentioned cases, the noisy nodes are detected and then replaced with some other supertags which will be proposed by other heuristics. So, the whole search for finding the full-parse can be summarized as the follows: 1- Use supertagger to achieve partial parse 2- Detect the full linkage by using LDA 3- In the case of using full linkage, stop 4- In the case of failure the full parses, check if one of the three mentioned cases happened 5- In the case of happening one of the mentioned cases, replace the faulty node proposed by error detection heuristic with a better candidate 6- Go to step 2 4. Error Correction Heuristics After detecting the erroneous nodes, a list of proper candidates required to be substituted with the erroneous supertags. Three heuristics are presented here to propose the candidates to be replaced with the erroneous nodes, where each of which improves the full parsing rate and speed. These heuristics are as follows: 4.1 N-Best Heuristic In this heuristic, the outputs of n-best supertagger are used as successor candidates. The n-best supertagger is a modified version of simple supertagger which proposes n supertags for each word of the sentence. Suppose that m is the number of faulty nodes which are detected by the previously mentioned heuristics and n is the number of nbest candidates which are predicted by supertagger, so finding the best combination in this space involves O(mn) cases. Breath first search (BFS) strategy is used to find the best match in this state space. That is for each node; all its successor nodes are generated first and then are evaluated by LDA as an evaluation function. The search terminated when the full parse structure of the input sentence is acquired. 4.2 XTAG-Based Heuristic In this heuristic, a human-crafted grammar based on treeadjoining formalism, named XTAG, is used. XTAG is an  on-going project to develop a wide-coverage grammar for English using TAG formalism [9]. XTAG uses Lexicalized TAG, where each lexical item is associated to many elementary trees which can satisfy its structural constraints. In this heuristic, these associations between each lexical item and elementary trees are used as candidates to be replaced with the detected faulty nodes. When an error node is detected, other TAGs, which are associated to those nodes’ lexical in the XTAG grammar bank, are chosen as a substitution list. XTAG grammar contains 1226 elementary trees which are categorized into 26 different family trees, and each lexical item especially verb, associated to more than 10 elementary trees. Thus, the candidate list to be substituted with erroneous nodes in this method is much larger than previous one. Therefore, both the time and performance are much higher than n-best correction heuristic. 4.3 Trained Probability Model Heuristic Although n-best is faster than XTAG heuristic, but the performance of full-parsing is much lower. In the first method the candidate list for correcting the error nodes is so shorter than the later one, and thus it needs less time to search among the combinations. Here a method using a trained probability model is proposed. In fact, for any supertags si, sj, the probability of changing a faulty supertag sj to supertag si (i.e. P(si | sj)) which concludes a full-parse tree is calculated. These probabilities are estimated by using maximum likelihood estimation method with counting the number of successful changes of faulty supertag (sj) to correct supertag (si). By using from an annotated corpus of 40,000 sentences and their syntactic parses, these changes are computed in an iterative fashion. At each iteration, the sentences are tagged by the supertagger and the correctness of LDA algorithms is checked by the previously mentioned error detection heuristics and the erroneous nodes are detected. The faulty nodes then substituted with other supertags proposed by a combination of XTAG-based and 10-best heuristics. Each time an error supertag sj is replaced with supertag si, the resulting parse structure is evaluated by PARSEVAL metrics [3]. If the result is a satisfiable full deep structure, the frequency of successful changing si to sj increases one unit. The whole process of calculating the probability model P(si | sj) is shown in figure 1. The training algorithm is terminated when the changes of the probabilities after running the experiment on the whole 40,000 sentences become ignorable. That is the total number of changes in whole probabilities becomes less that a predefined threshold. In our method, we set this threshold to be less than 0.05% of all entry values. At the end of process, all frequencies of changes in any faulty node should be normalized by using equation (1) in order to get  73  the probability P(sj | si). Having these probabilities, an ordered list of candidate nodes for any error supertag si is achieved, which can be used in the error correction method:  P(si | sj ) = count(si, sj) / Σk count(sk, sj)  (1)  Training Corpus  Supertagger  Shallow Parsing LDA  Error = root node Error = partial tree root node Error = verb node  Partial Parses  Case 1  Case 2 Case 3  Error Case  Error Detection  Replace the erroneous supertag with a member of list  Generate a list of candidates based on XTAG elementary trees and 10-best heuristic  No Shallow Parsing  Full-parse achieved?  No The whole changes ignorable?  Yes Increase the successful changes one unit  Yes  Calculate P(si | sj) by  using equation (1)  End  Figure 1. The whole process of calculating the probability model P(si | sj)  5. Evaluation In order to evaluate the proposed methods, 3000 sentences with their syntactic structure from Penn Treebank are  selected as test corpus. These sentences are completely different from those that are used in the process of calculating the changing probabilities. We divided the test corpus into three different categories based on the sentence length: the sentences shorter than 16 words, sentences with length between 16 and 25 words and sentences longer than 25 words1. The experiments include the evaluation of mentioned heuristics such as 1-best, 10-best, 25-best, XTAG based and trained probability model heuristics. In each experiment, the percentage of full-parsed sentences and parsing time are computed. Also, in order to evaluate the resulting full-parse quality, PARSEVAL metrics, introduced by [3], are calculated. We measure PARSEVAL metric only for those sentences which have been fullyparsed successfully. Figure 2, 3 and 4 show the results of these experiments on each of the mentioned category. The evaluations show that considerable improvements both in time and percentage of full-parsed sentences are achieved by using the trained probability model heuristic. This method increases the fullparse rate from the native supertagger (1-best heuristic) by a factor of 3 in the first category, by a factor of 11 in the second category and by the factor of 21 in the third category. That is, the effects of the trained probability model in long sentences are more than short sentences. Comparing the mentioned figures, shows that by increasing the sentence length, the percentage of full-parsing rate and parsing speed decreases dramatically. Also, in the trained probability model heuristic, the parsing speed increases about twice than XTAG-based heuristic, while the fullparsing rate also increases about 20%.  100 90 80 70 60 50 40 30 20 10 0 1-Best  10 – Best  25 – Best  XTAG-based  Trained P robabilit y Model  Full Parsed (%) Parse time (s) Precision (%) Recall (%)  Figure 2: Experimental results on sentences shorter than 16 words  
We present a method for grouping the synonyms of a lemma according to its dictionary senses. The senses are deﬁned by a large machine readable dictionary for French, the TLFi (Tr´esor de la langue franc¸aise informatis´e) and the synonyms are given by 5 synonym dictionaries (also for French). To evaluate the proposed method, we manually constructed a gold standard where for each (word, deﬁnition) pair and given the set of synonyms deﬁned for that word by the 5 synonym dictionaries, 4 lexicographers speciﬁed the set of synonyms they judge adequate. While inter-annotator agreement ranges on that task from 67% to at best 88% depending on the annotator pair and on the synonym dictionary being considered, the automatic procedure we propose scores a precision of 67% and a recall of 71%. The proposed method is compared with related work namely, word sense disambiguation, synonym lexicon acquisition and WordNet construction. Keywords Similarity measures, Synonyms, Lexical Acquisition 
In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models. We leverage on the computation of logistic regression to exploit unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively aﬀects performances. 
Topic segmentation was addressed by a large amount of work from which it is not easy to draw conclusions, especially about the need for knowledge. In this article, we propose to combine in the same framework two methods for improving the results of a topic segmenter based on lexical reiteration. The ﬁrst one is endogenous and exploits the distributional similarity of words in a document for discovering its topics. These topics are then used to facilitate the detection of topical similarity between discourse units. The second approach achieves the same goal by relying on external resources. Two resources are tested: a network of lexical co-occurrences built from a large corpus and a set of word senses induced from this network. An evaluation of the two approaches and their combination is performed in a reference framework and shows the interest of this combination both for French and English.  one component of [1]. These statistical topic models enable segmenters to improve their precision but they also restrict their scope. Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: [12] combined word recurrence, co-occurrences and a thesaurus; [1] relied on both lexical modeling and discourse cues; [10] made use of word reiteration through lexical chains and discourse cues. The work we report in this article takes place in the last category we have presented. More precisely, it ﬁrst conﬁrms the interest of combining lexical recurrence with an external resource about lexical cohesion of texts. Second, it shows that the improvement brought by the use of a resource about lexical cohesion and the improvement brought by an endogenous method such as the one presented in [9] are complementary and can be fruitfully combined.  
The Edlin toolkit provides a machine learning framework for linear models, designed to be easy to read and understand. The main goal is to provide easy to edit working examples of implementations for popular learning algorithms. The toolkit consists of 27 Java classes with a total of about 1400 lines of code, of which about 25% are I/O and driver classes for examples. A version of Edlin has been integrated as a processing resource for the GATE architecture, and has been used for gene tagging, gene name normalization, named entity recognition in Bulgarian and biomedical relation extraction. Keywords Information Extraction, Classiﬁcation, Software Tools 
Document Retrieval assumes that a document is independent of its relevance, and non-relevance. Previous works showed that the same assumption is being considered for passage retrieval in the context of Question Answering. In this paper, we relax this assumption and describe a method for estimating the prior of a passage being relevant, and non-relevant to a question. These prior probabilities are used in the process of ranking passages. We also describe a trivial method for identifying relevant and nonrelevant text to a question using the Web and AQUAINT corpus as information sources. An empirical evaluation on TREC 2006 Question Answering test set showed that in the context of Question Answering prior probabilities are necessary in ranking the passages.  
Retrieving answer containing passages is a challenging task in Question Answering. In this paper we describe a novel query expansion method which aims to rank the answer containing passages better. It uses content and structured information (link structure and category information) of Wikipedia to generate a set of terms semantically related to the question. As Boolean model allows a ﬁne-grained control over query expansion, these semantically related terms are added to the original query to form an expanded Boolean query. We conducted experiments on TREC 2006 QA data. The experimental results show signiﬁcant improvements of about 24.6%, 11.1% and 12.4% in precision at 1, MRR at 20 and TDRR scores respectively using our query expansion method. 
 2 Background  An important question in the evaluation of Natural Language Generation systems concerns the relationship between textual characteristics and task performance. If the results of task-based evaluation can be correlated to properties of the text, there are better prospects for improving the system. The present paper investigates this relationship by focusing on the outcomes of a task-based evaluation of a system that generates summaries of patient data, attempting to correlate these with the results of an analysis of the system’s texts, compared to a set of gold standard human-authored summaries. Keywords Natural Language Generation, evaluation, decision support, domain ontology 
The paper presents a feature-rich approach to the automatic recognition and categorization of named entities (persons, organizations, locations, and miscellaneous) in news text for Bulgarian. We combine well-established features used for other languages with language-speciﬁc lexical, syntactic and morphological information. In particular, we make use of the rich tagset annotation of the BulTreeBank (680 morpho-syntactic tags), from which we derive suitable task-speciﬁc tagsets (local and nonlocal). We further add domain-speciﬁc gazetteers and additional unlabeled data, achieving F1=89.4%, which is comparable to the state-of-the-art results for English. Keywords Named entity recognition, information extraction, conditional random ﬁelds, linear models, machine learning, morphology. 
In this paper, we explore the task of automatically identifying educational materials, by classifying documents with respect to their educative value. Through experiments carried out on a data set of manually annotated documents, we show that the generally accepted notion of a learning object’s “educativeness” is indeed a property that can be reliably assigned through automatic classiﬁcation. Keywords learning objects, educational applications, text classiﬁcation 
Even leaving aside concerns of cognitive plausibility, incremental parsing is appealing for applications such as speech recognition and machine translation because it could allow the incorporation of syntactic features into the decoding process without blowing up the search space. Nevertheless, incremental parsing is often associated with greedy parsing decisions and intolerable loss of accuracy. Would the use of lexicalized grammars provide a new perspective on incremental parsing? In this paper we explore incremental left-to-right dependency parsing using a lexicalized grammatical formalism that works with lexical categories (supertags) and a small set of combinatory operators. A strictly incremental parser would conduct only a single pass over the input, use no lookahead and make only local decisions at every word. We show that such a parser suffers heavy loss of accuracy. Instead, we explore the utility of a two-pass approach that incrementally builds a dependency structure by ﬁrst assigning a supertag to every input word and then selecting an incremental operator that allows assembling every supertag with the dependency structure built thus far to its left. We instantiate this idea in different models that allow a trade-off between aspects of full incrementality and performance, and explore the differences between these models empirically. Our exploration shows that a semi-incremental (two-pass), linear-time parser that employs ﬁxed and limited look-ahead exhibits an appealing balance between the efﬁciency advantages of incrementality and the achieved accuracy. Surprisingly, taking local or global decisions matters very little for the accuracy of this linear-time parser. Such a parser ﬁts seamlessly with the currently dominant ﬁnite-state decoders for machine translation. 
When creating dictionaries for use in for example crosslanguage search engines, one often uses a word alignment system that takes parallel or comparable text pairs as input and produces a word list. Multilingual web sites may contain parallel texts but these can be difficult to detect. In this article we describe an experiment on automatic identification of parallel text pairs. We utilize the frequency distribution of word initial letters in order to map a text in one language to a corresponding text in another in the JRC-Acquis corpus (European Council legal texts). Using English and Swedish as language pair, and running a ten-fold random pairing, the algorithm made 87 percent correct matches (baseline-random 50 percent). Attempting to map the correct text among nine randomly chosen false matches and one true yielded a success rate of 68 percent (baseline-random 10 percent). Keywords Cross Language Information Retrieval, Identification of Parallel Text, Prefix Frequency Distribution, A-priori Probability. 1. Introduction Dictionaries are an important part of natural language processing tasks and linguistic work. Domain-specific dictionaries can for example be used in cross-language web and intranet search engines. Word alignment tools are often used for the creation of bilingual word lists. These tools need parallel corpora to work properly. One source is Internet and the multilingual web sites there. Unfortunately these web sites are often only parallel with regard to web pages. In [6] and in [2] are described different heuristics to download and identify parallel text. However, these methods are not enough since the downloaded parallel text still can be very noisy. For example [13] found only 45 percent parallel text pairs on the multilingual parallel web site Hallå Norden (Hello Scandinavia) that was intended to be completely parallel and the parallel pages contained 5 percent nonparallel elements. Therefore, we found a need to develop and evaluate a new method for identifying parallel and non-parallel texts in corpora covering different language pairs.  2. Related Work The distinction between a parallel and a comparable corpus is very important and has been discussed in for example [10] and also in [3]. Freely available multilingual resources are often noisy and non-parallel sections need to be removed. Many methods for identifying such sections automatically have been proposed. Maximum entropy (ME) classification is used in [7] in order to improve machine translation performance. From large Chinese, Arabic and English nonparallel newspaper corpora, parallel data was extracted. For this method, a bilingual dictionary and a small amount of parallel data for the ME classifier is needed. By selecting pairs of similar documents from two monolingual corpora, all possible sentence pairs are passed through a wordoverlap based filter and then sent to the ME classifier. The authors reported significant improvements over the baseline for Arabic-English and for Chinese-English In [3] a method for extracting parallel sentences through bootstrapping and Expectation Maximization (EM) learning methods is presented. An iterative bootstrapping framework is presented, based on the idea that documents, even those with a low similarity score, containing one pair of parallel sentences must contain others. In particular, the proposed method works well for corpora with very disparate contents. The approach achieves 65.7 percent accuracy and a 50 percent relative improvement over their baseline. Latent Semantic Indexing (LSI) has been experimented with in [5] in order to identify parallel sequences in corpora. In this work, the hypothesis that LSI reveals similarities between parallel texts not apparent in nonparallel texts is presented and evaluated. Corpora from digital libraries were used with the language combinations English-French, English-Russian, French-Russian and English-Russian-Italian. Applying correlation coefficient analysis, a threshold of 0.75 was reported to successfully hold as a lower bound for identifying parallel text pairs. Non-parallel text pairs did not, in these experiments, exceed a correlation coefficient value of 0.70. Unfortunately, most work has been performed on different types of corpora and on different language pairs. Moreover, they have been evaluated differently depending  135 International Conference RANLP 2009 - Borovets, Bulgaria, pages 135–138  on available resources and the nature of the experiments, which makes them difficult to compare. However, the different approaches show the need for these types of methods. 3. Identifying Parallel Texts in Bilingual Corpora using Fingerprints When comparing documents for content similarity it is common practice to produce some form of document signatures, or “fingerprints”. These fingerprints represent the content in some way, often as a vector of features, which are used as the basis for such comparison. One common method when comparing the likeness of two documents is to utilize the so-called Vector Space model [9]. In this model the documents’ fingerprints are represented as feature vectors consisting of the words that occur within the documents, with weights attached to each word denoting its importance for the document. We can, for example, for each feature (in this example, a word) record the number of times it occurs within each document. This gives us what is commonly called a document-by-term matrix where the rows represent the documents in the document collection and the columns each represent a specific term existing in any of the documents (a weight can thus be zero). We can now, somewhat simplified, compare the documents’ fingerprints by looking at how many times each feature occurs in each document, taking the cosine angle between the vectors, and pair the two most similar together. One obvious drawback of the basic use of this model is that when comparing texts written in different languages we do not necessarily know which feature in one language corresponds to which feature in another. Another drawback when building a word vector space representing more than one language is that the vocabulary, i.e. the number of features in the feature vectors, grows alarmingly (this is in many cases already a problem representing just one language [8]). Ways of limiting the vocabulary include using stop-word lists to remove “information poor” features, frequency thresholding and conflation into feature classes (for example lemmatization). In word vector spaces the latter is often accomplished by bringing semantically related words to a common lemma or stem. In the experiments described below conflation was attempted by moving from term frequency classes towards prefix frequency classes, i.e. the leading characters of each token. This way a document’s fingerprint effectively is represented by a feature vector containing the frequency of each prefix of a set length n occurring in the corpus. Fingerprinting using prefix frequencies has for example been used in information retrieval for filtering of similar documents written in the same language [11]. We here attempt to utilize this notion in cross-language text alignment.  4. Data sets and experimental setup In this set of experiments we have used the JRC-Acquis corpus [12]. This corpus consists of European Union law texts, which are domain specific and also very specific in their structure. Many texts are listings of regulations with numerical references to other law texts1 and named entities (such as countries). We have investigated the language pair Swedish-English, i.e. we used Swedish as a source language attempting to find the corresponding parallel text in English. We have also used only those documents that have a counterpart in both languages, resulting in a total of 20.145 document pairs. In order to delimit the search space for the practicality of this experiment we have not compared each Swedish source text with each and every English text. Instead we, in one experiment, compare the similarity between a true positive (the corresponding, parallel, English text) and one true negative (a randomly chosen non-parallel English text), letting the algorithm choose the closest match (as defined by the cosine angle between the feature vectors for each text). In another experiment we repeated the setup, but instead of only using one true negative we used nine. This setup gave us a random chance of picking the true positive of 50 percent in the case of one true positive and one true negative, and 10 percent in the case of one true positive and nine true negatives. In order to rule out any random fluke in the choice of true negative(s) for each true positive both experiments were carried out 10 times, making new random pairings each time. An average was then taken, calculated over these ten runs. As in [11] we have extracted a-priori probabilities of prefix classes from reference corpora. Since we are dealing with the language pair Swedish-English we have used a Swedish reference corpus, the Swedish Parole corpus [4], and an English ditto, the British National Corpus [1]. The Swedish reference corpus is comprised of roughly 20 million words. In order to have a comparable English reference corpus we have only used the first 20 million words of BNC. These two corpora can be seen as the expected distribution of the prefix classes for each language, while each text’s feature vector then is the deviation to the expected distribution. We would like to find if a deviation from the expected frequency distribution pattern in one language in the pair could possibly reflect a similar deviation in the other. In this set of experiments the feature vector for each text was preprocessed in two ways: 
This paper introduces Stochastic Deﬁnite Clause Grammars, a stochastic variant of the wellknown Deﬁnite Clause Grammars. The grammar formalism supports parameter learning from annotated or unannotated corpora and provides a mechanism for parse selection by means of statistical inference. Unlike probabilistic contextfree grammars, it is a context-sensitive grammar formalism and it has the ability to model cross-serial dependencies in natural language. SDCG also provides some syntax extensions which makes it possible to write more compact grammars and makes it straight-forward to add lexicalization schemes to a grammar. 
We consider the problem of query-focused multidocument summarization, where a summary containing the information most relevant to a user’s information need is produced from a set of topic-related documents. We propose a new method based on probabilistic latent semantic analysis, which allows us to represent sentences and queries as probability distributions over latent topics. Our approach combines queryfocused and thematic features computed in the latent topic space to estimate the summaryrelevance of sentences. In addition, we evaluate several diﬀerent similarity measures for computing sentence-level feature scores. Experimental results show that our approach outperforms the best reported results on DUC 2006 data, and also compares well on DUC 2007 data. Keywords text summarization, probabilistic latent semantic analysis, plsa 
A model of episodic memory is derived to propose algorithms of text categorization with semantic space models. Performances of two algorithms named Target vector and Sub-target vector are contrasted using textual material of the text-mining context ‘DEFT09’. The experience reported here have been realized on the english corpus which is composed of articles of the economic newspaper “The Financial Times”. The aim of the task was to categorize texts in function of the factuality or subjectivity they expressed. Results confirm (i) that the episodic memory metaphor provides a convenient framework to propose efficient algorithm for text categorization, and (ii) that Sub-target vector algorithm outperforms the Target vector algorithm. Keywords Random Indexing, episodic memory, text-mining, categorization. 1. Introduction Since its early introduction, the model that is now named Latent Semantic Analysis [14] has been proposed as a method of matrix reduction and vectorial representation of information for indexing textual documents. The model was known as Latent Semantic Indexing [3] at that time. Originally only concerned by indexing tasks, LSA has been extended to the area of human memory simulation. Researchers in cognitive psychology got interested in it and then proposed it as a plausible model of human behavior in different tasks such as synonymy test [14] and problem solving [17]. The most famous application in cognitive psychology is the coupled CI-LSA model of text comprehension [12], which combines the previous “Construction-Integration” model of reading [11] with LSA as model of semantic memory. Whereas research involving LSA has been split in two main fields with the text-mining on the one hand and cognitive psychology on the other hand, our paper deals with both of those fields. Discussions of MINERVA 2 model of human episodic memory [6][7] allow proposing an operative algorithm for texts categorization. LSA has been known to perform in synonymy test and other equivalent thematic classification tasks [14]. The model has been recently successfully applied on opinion judgment task [1]. There are very important differences  between thematic classification, and opinion judgment classification. Firstly, thematic classification is directly connected to the distributional hypothesis, which states that “words that appear in similar contexts have similar meanings”. Here is the reason why LSA is able to find words that share the same thematic, ie “appear in equivalent contexts”. Secondly, in opinion judgment classification, different thematics could possibly belong to the same category of opinion. For example, I have a good opinion of different movies, which do not deal with the same topic. If I write texts in which I give my opinion of each movie, those texts will be influenced by the topic of the movie for a part, as well as by my motivation to exhibit how and why I loved them for another part. In consequence, the basic application of the distributional hypothesis cannot account for judgment opinion task. In this paper, we will explore two lines of investigation. In the first line, we will propose the paradigmatic breakthrough that has been realized to find a solution to the limitation of the basic application of the distributional hypothesis. This breakthrough consists in switching from the semantic memory research field to the episodic memory metaphor to drive the similarity comparison stage. The episodic memory metaphor has been tested with LSA [8]. The second line that will be developed in this paper will consist in testing the episodic memory metaphor with an alternative method of Words Vectors construction, named Random Indexing. 2. Abstractive versus non-abstractive models of memory In the debate within cognitive psychology about the distinction between “abstractive” versus “non-abstractive” models of memory [18][21], LSA has been proposed as belonging to the abstractive family [2]. This proposition is congruent with the affirmation by Landauer, Foltz and Laham that “the representations of passages that LSA forms can be interpreted as abstractions of “episodes”, sometimes of episodes of purely verbal content such as philosophical arguments, and sometimes episodes from real or imagined life coded into verbal descriptions” [15: 15]. Tiberghien considers that “it would be more precise and theoretically more adequate, to consider that all the models are ‘abstractive’ but, for some of them this abstractive process  150 International Conference RANLP 2009 - Borovets, Bulgaria, pages 150–154  happens during encoding and for some others it happens during retrieval” [21: 145]. Because the abstractive process occurs during encoding, LSA and other Word Vector models are categorized as belonging to the abstractive model family. A model like MINERVA 2 or other Multiple-Trace models are considered as “non-abstractive” because the abstractive process occurs during retrieval. According to MINERVA 2, memory consists of events or episodes that are represented and stored as vectors. The activation value of each coordinate stores features of episodes. Each vector corresponds to an episode in the system’s life. Retrieval consists of a two stage calculation. First, a similarity calculation is carried out between the probe-vector and all the episode-vectors in memory (see Eq 1). Episodes that are most similar will be affected by a higher level of activation than episodes that are least similar. Second, a calculation is made to compare the level of activation of each feature and this corresponds to the “echo” phenomena of memory. The “echo” calculation produces a new vector that inherits the features of the most activated vectors, even those parts that did not actually exist in the probes. The “echo” has two components: intensity which is denoted I (see Eq 2), and content which corresponds to the sum of the content of all traces in memory, weighted by their activation level (see Eq 3). “Echo” constitutes the process of abstraction that Rousset (2000) qualified as “re-creation. Eq 1 Similarity of a trace i, where Pj is the value of feature j in the probe, and Ti,j the value of feature j in trace I Eq 2 Intensity of the « echo » Eq 3 The content of the « echo » 3. The episodic memory metaphor in opinion judgment classification task LSA has been successfully applied in tasks of text classification with texts expressing subjective opinion in the DEFT07 contest [1]. The Multiple-Trace approach has been proposed to account for semantic space performance when modifying factors like generality/specificity of episodes that compose the space [8]. Two predictions of  MINERVA 2 model has been tested and confirmed. First, two methods of semantic space construction are compared. In one method, different categories of episodes are blended in the same global semantic space. In the other method, each semantic space is built from a single category of episodes. These spaces are named specific. For each method of semantic space construction (global vs specific), two experimental conditions are compared. In the first condition, the number of episodes corresponding to each category is equalized. In the other condition, the number of episodes corresponding to each category is not controlled. For the global space condition, correlation analysis showed that the relationship between relative amount of episodes and F-score was more important than the relationship between absolute amount of episode and Fscore (r = .96, α > .001 versus r = .74, α > .05). For the specific space condition, the relationship between F-score and relative amount of data was almost the same as the relationship between F-score and absolute amount of data (r = .84, α > .01 versus r = .87, α > .01). As predicted by MINERVA 2, modifying the relative amount of episodes or the absolute amount of episodes has an almost equivalent effect on performance for specialized spaces, whereas modifying relative amount of episodes has a more important effect on performance than modifying absolute amount of episode for general spaces. 4. The episodic memory metaphor for similarity judgment algorithm The algorithm used in Deft07 to identify opinion judgment expressed by unknown texts, consisted in creating a target vector for each type of opinion that should be identified. These target vectors are created by the sum of vectors of all documents that belong to a given category of opinion1. For example, the target vector that was used to identify “good critics of movies” was a summed vector of all documents known to be a “good critic of movie”. In-comings “text-tobe-indexed” were compared to the target vectors of each category of opinion. Then, the text was categorized with the opinion of the target vector to which it was the more similar. The comparison of similarity used the calculation of the cosine of the angle between the vector of the “textto-be-indexed” and the target vector. The use of cosine calculation makes it possible to compare the very large target-vectors (hundreds of documents) to the very small text-to-be-indexed vector (one document). The intuition that was underlying the construction of these very large target-vectors was that the classical distributional hypothesis approach has to be derived to perform in opinion judgment task. The idea was to sum vectors of all documents corresponding to a given opinion 1In data-mining contests, a classified corpus is given in what is called a learning stage to make it possible to implement algorithms that will be used to categorize un-classified documents in the test stage.  151  category to take advantage of the great number of documents to draw a vector that (i) would not correspond to any topic in particular, and in contrast, (ii) would hold information that would correspond to the linguistic way a given opinion is statistically expressed in numbers of texts. Applying the Multiple-Trace approach specifically to the stage of similarity comparison makes it possible to consider a target vector as an episodic memory that should behave like MINERVA 2 model predicts. Indeed, in considering each document as a specific episode, target-vectors become episodic memories, which are constituted of different episodes of the same category of opinion. As described above, the calculus of “echo” of MINERVA 2 predicts that the more a probe is similar to great numbers of episodes, the more the memory system would react by a strong value of “echo”. It is neither mathematically nor psychologically wrong to consider that the value of “echo” in MINERVA 2 and the value of the cosine in LSA behave and can be interpreted in the same way. In consequence, MINERVA 2 gives a theoretical basement to our first intuitive method of vector target construction. The large size target vector method functioned pretty well and contributed to rank second in the Deft07. 5. Target-vectors as homogeneous episodic memory The use of the episodic memory metaphor accounts for the limitation of the basic application of the distributional hypothesis for opinion judgment task. In creating these large target vectors, we are creating episodic memories, which behaviors became understandable with the MINERVA 2 model. Predictions concerning “echo” involve that the episodic memories will be more sensitive to probe episodes that are well represented in the memory and less sensible to probe episodes that are less represented. In other words, target vectors will be more sensible to typical documents and less sensible to non-typical documents. Theories of categorization showed that some items are typical of the category they belong, others are not. The typicality of an item is generally defined as (i) a high similarity with items of a given category and (ii) a low similarity with items of other categories. Target vectors have been produced with the aim of creating episodic memories, which would hold the statistical linguistic signature of a given category of opinion. “Echo” predicts that target vectors will not identify non-typical documents as well as typical documents. We assume that a homogeneous episodic memory, which holds non-typical documents of a given category will be more sensitive to non-typical documents than a heterogeneous episodic memory, which holds typical and non-typical documents, all blended. Our hypothesis has been implemented for the DEFT09. The aim of the task 1 was to classify texts that express facts or opinions, respectively corresponding  “objective” versus “subjective” categories. First, we created a target vector in summing all vectors of all documents for each category. These target vectors had the same properties than those of the Deft07. Second, to be able to identify and regroup typical versus non-typical documents, a calculation of similarity is realized between (i) each document that composes the target vector, and (ii) the target vector. Documents that compose the target vector are ordered in function of their similarity with the target vector. Third, documents are regrouped in n sub-target vectors in a way that (i) each sub-target vector has the same amount of documents, and (ii) documents of the same degree of similarity with the target vector are regrouped in the same sub-target vector. The number of sub-target vectors for each category is a parameter of the algorithm we developed. Whereas the target vector algorithm has been tested with LSA, we propose to compare the target-vector algorithm with the sub-target vector algorithm using an alternative method of Word vector named Random Indexing. 6. Random Indexing Word vectors correspond to a family of models in which LSA is the most known. Several principles are common to all of these models (see [18]): • They are based on the distributional hypothesis • They involve a method of counting words in a given unit of context • They have a statistical method, which abstracts the meaning of concepts from large distributions of words in context • They use a vectorial representation of word meaning. As we will see, Random Indexing is not a typical item of its category. In the other models, the list of principles enounced above is also the stages of a semantic space construction. Particularities of the Random Indexing (RI) model are that (i) it does not create co-occurrence matrix (but it is possible if needed) and (ii) it does not need heavy statistical treatments like SVD for LSA. Contrary to the other Word Vector models, RI is not based on statistics but on random projections. The construction of a semantic space with RI is as follows: • Create a matrix A (d x N), containing Index vectors, where d is the number of documents or contexts and N, the number of dimensions (N > 1000) decided by the experimenter. Index vectors are sparse and randomly generated. They consist in small numbers +1 and -1 and thousands of 0. • Create a matrix B (t x N), containing term vectors, where t is the number of different terms in the  152  corpus. Set all vectors with null values to start the semantic space construction. • Scan each document of the corpus. Each time a term t appears in a document d, accumulate the randomly generated d-index vector to the t-term vector. At the end of the process, term vectors that appeared in similar contexts have accumulated similar index vectors. There is a training cycle option in the model. When the scan has been computed for all documents, the matrix B is charged for all term vectors. Then a matrix A’ (d’ x N), with d’ = d can be computed with the output of term vectors. The number of training cycle is a parameter in the model. The training process output is consistent with what has been described for neural network learning. The RI model has performed in TOEFL synonymy test [9][10] as well as in text categorization [18].  7. Experiment 7.1 Method and material The experiment reported here has been realized the task 1 of the DEFT09 using the english corpus. The purpose of the task 1 was the detection of the subjectivity or objectivity character of a text. As described by the committee, “the reference is established by projecting each section on both the subjective and the objective dimension. For instance, the Letter from the editor, which usually states an opinion, has the type subjective, while the News, describing actual facts, have the type objective”2. The english corpus was composed of articles of the economic newspaper “The Financial Time”. In the learning stage, 60% of the total corpus is given to each team engaged to allow them to implement algorithms that will then be applied on the 40% of uncategorized documents during the test stage. We realized our learning session using the “ten cross-folder” method. Table 1 give a description of the corpus.  Table 1. Description of the corpus of learning and test  Objective Subjective  Learning Number of Size documents (Kb) 3440 15840 4426 26016  Test Number of Size documents (Kb) 5245 27996  7.2 Results Precision and recall performances are reported for the Target vector algorithm and the Sub-target vector algorithm. Taking account that the value of 1 for recall means that all documents have been categorized in the 2 DEFT09 website: http://deft09.limsi.fr/index.php?id=1&lang=en  same category. Hence those scores should be considered as aberrant. This is the case for two reported results: the Target vector algorithm, which have 1 target and the Sub-target vector algorithm using 11 targets (indicated by the double slash in Table 2) both have 0.432 for Precision and 1 in Recall. Those results demonstrate that the Target vector algorithm was not able to perform in the considered task. Concerning the Sub-target vector algorithm, the systems performs better using 9 sub-targets (Precision of 0.740 and Recall of 0.708 using 1000 dimensions and respectively 0.746 and 0.718 using 2000 dimensions). This result involves that there is an optimum threshold for the number of sub-target vectors. Considering Multiple-Trace approach, this threshold corresponds to the moment where episodic memories or sub-targets are the most homogeneous. Runs realized changing the specific parameters of Random Indexing as the number of dimensions and the number of training cycles show that the optimum partition realized with the Sub-target vector algorithm using 9 subtargets doest not change significantly (between 0.740 and 0.746 for the Precision and between 0.708 and 0.718 for Recall). Those results show that performance of the Subtarget vector algorithm is more dependent of the number of sub-target used and less dependent of the parameters of Random Indexing.  Table 2. Parameters and scores  Parameters  Score  Dimensions Cycles Sub-target Precision Recall  Target-vector algorithm  1500  10  
We present a study of the impact of morphological and syntactic ambiguity in the process of grammatical error detection. We will present three diﬀerent systems that have been devised with the objective of detecting grammatical errors in Basque and will examine the inﬂuence of ambiguity in their results. We infer that the ambiguity rate in the input to an error detection tool can have a considerable impact on the quality of the system. Keywords Grammatical error detection, morphosyntactic ambiguity 
This paper proposes two techniques for fast sequential labeling such as part-of-speech (POS) tagging and text chunking. The ﬁrst technique is a boosting-based algorithm that learns rules represented by combination of features. To avoid time-consuming evaluation of combination, we divide features into not used ones and used ones for learning combination. The other is a rule representation. Usual POS taggers and text chunkers decide the tag of each word by using the features generated from the word and its surrounding words. Thus similar rules, for example, that consist of the same set of words but only differ in locations from current words, are generated. We use a rule representation that enables us to merge such rules. We evaluate our methods with POS tagging and text chunking. The experimental results show that our methods show faster processing speed than taggers and chunkers without our methods while maintaining accuracy. 
This paper proposes a new task of cross-document event extraction and tracking and its evaluation metrics. We identify important person entities which are frequently involved in events as ‘centroid entities’. Then we link the events involving the same centroid entity along a time line. We also present a system performing this task and our current approaches to address the main research challenges. We demonstrate that global inference from background knowledge and cross-document event aggregation are crucial to enhance the performance. This new task defines several extensions to the traditional single-document Information Extraction paradigm beyond ‘slot filling’. Keywords Information Extraction, Cross-document Extraction, Event Extraction 1. Introduction Consider a user monitoring or browsing a multi-source news feed, with assistance from an Information Extraction (IE) system. Various events are evolving, updated, repeated and corrected in different documents; later information may override earlier more tentative or incomplete facts. In this environment, traditional singledocument IE would be of little value; a user would be confronted by thousands of unconnected events with tens of thousands of arguments. Add to this the fact that the extracted results contain unranked, redundant and erroneous facts and some crucial facts are missing, and it’s not clear whether these IE results are really beneficial. How can we take proper advantage of the power of extraction to aid news analysis? In this paper we define a new cross-document IE task beyond ‘slot filling’ to generate more coherent, salient, complete and concise facts. A high-coherence text has fewer conceptual gaps and thus requires fewer inferences and less prior knowledge, rendering the text easier to understand [1]. In our task, coherence is the extent to which the relationships between the events in the documents can be made explicit. We aim to provide a more coherent presentation by linking events based on shared arguments. In the news from a certain period some entities are more central than others; we propose to identify these centroid entities, and then link the events involving the same centroid entity on a time 166  line. In this way we provide coherent event chains so that users can more efficiently review and analyze events, such as tracking a person’s movement activities and trends. This will offer a richer set of views than is possible with document clustering for summarization or with topic tracking. To sum up, the specific goals of this paper are to: • Formulate a tractable but challenging task of cross- document IE, producing a product useful for browsing, analysis, and search; • Propose a set of metrics for this task; • Present a first cut at a system for performing this task; • Lay out the potential research challenges and suggest some directions for improving this system's performance. 2. Traditional Single-document IE and Its Limitations We shall start by illustrating, through the ACE 1 event extraction task, the limitations of traditional singledocument IE. 2.1 Terminology and Task ACE defines the following terminology: entity: an object or a set of objects in one of the semantic categories of interest, e.g. persons, locations, organizations. mention: a reference to an entity (typically, a noun phrase) relation: one of a specified set of relationships between a pair of entities. event: a specific occurrence involving participants, including 8 types of events, with 33 subtypes; for the purpose of this paper, we will treat these simply as 33 distinct event types. event mention: a phrase or sentence within which an event is described. event trigger: the main word which most clearly expresses an event occurrence. event argument: an entity involved in an event with some specific role. event time: an exact date normalized from time expressions and a role to indicate that an event occurs before/after/within the date. 
We present an asymmetric approach to a runtime combination of two parsers where one component serves as a predictor to the other one. Predictions are integrated by means of weighted constraints and therefore are subject to preferential decisions. Previously, the same architecture has been successfully used with predictors providing partial or inferior information about the parsing problem. It has now been applied to a situation where the predictor produces exactly the same type of information at a fully competitive quality level. Results show that the combined system outperforms its individual components, even though their performance in isolation is already fairly high. Keywords Dependency Parsing, Hybrid Parsing 
We describe a pattern-based system for polarity classiﬁcation from texts. Our system is currently restricted to the positive, negative or neutral polarity of phrases and sentences. It analyses the input texts with the aid of a polarity lexicon that speciﬁes the prior polarity of words. A chunker is used to determine phrases that are the basis for a compositional treatment of phrase-level polarity assignment. In our current experiments we focus on sentences that are targeted towards persons, be it the writer (I, my, me, ..), the social group including the writer (we, our, ..) or the reader (you, your, ..). We evaluate our system on a manually annotated set of sentences taken from texts from a panel group called ’I battle depression’. We present the results of comparing our system’s performance over this gold standard against a baseline system. Keywords sentiment analysis, polarity composition 
In the application of Conditional Random Fields (CRF), a huge number of features is typically taken into account. These models can deal with inter-dependent and correlated data with an enormous complexity. The application of feature subset selection is important to improve performance, speed and explainability. We present and compare ﬁltering methods using information gain or χ2 as well as an iterative approach for pruning features with low weights. The evaluation shows that with only 3 % of the original number of features a 60 % inference speed-up is possible. The F1 measure decreases only slightly. 
Conditional Random Fields are commonly trained to maximize likelihood. The corresponding Fβ measure, the weighted harmonic mean of precision and recall, which is established for evaluation in information retrieval and text mining, is not necessarily the optimal result for the user’s choice of β. Some approaches have been published to optimize multivariate measures like Fβ to overcome this inconsistency. The limitation is that constraints like the value of β have to be known at training time. This publication proposes a method of multiobjective optimization of both precision and recall based on a preceding likelihood training. The output is an estimation of pareto-optimal solutions from which the user can select the best for the actual application. Evaluated on two publicly available data sets in the ﬁeld of named entity recognition, nearly all Fβ values are superior to those resulting from log-likelihood training. Keywords Named Entity Recognition, Conditional Random Fields, Multi- Objective Optimization, NSGA-II, Fβ measure, Recall, Precision 
In this paper, we discuss the importance of the quality against the quantity of automatically extracted examples for word sense disambiguation (WSD). We ﬁrst show that we can build a competitive WSD system with a memory-based classiﬁer and a feature set reduced to easily and eﬃciently computable features. We then show that adding automatically annotated examples improves the performance of this system when the examples are carefully selected based on their quality. Keywords word sense disambiguation, memory-based learning, semi-supervised learning  on distinguishing reliably from unreliably annotated examples. Our hypothesis is that the automatically annotated examples that are added to the training material are not of high enough quality to improve the results. In order to investigate the problem, we start with the SensEval-3 English lexical sample task data set3 and then add examples extracted from a selection of lexicons and corpora. The results show that only reliably annotated examples should be considered and that they can be included in approaches diﬀerent from the ones previously proposed (e.g. by Mihalcea [12]). We will show that a careful selection of automatically annotated examples gives a modest improvement over the supervised results, as compared to a signiﬁcant drop in accuracy when all examples are added.  
The paper presents Treelex, a valence lexicon of French adjectives automatically extracted from a treebank. The corpus contains morphological and syntactic annotations but no subcategorisation information is present for adjectives. Due to rich corpus annotations, our extraction method is guided by linguistic knowledge. The obtained lexicon (about 2000 adjectives and 40 frames) has been evaluated against hand-crafted adjectival tables described in [13] and achieved 0.46 F-measure. Keywords adjectives, valence, treebank, syntactic lexicon, lexicon- grammar tables, French 
This paper presents a novel automatic approach to partially integrate FrameNet and WordNet. In that way we expect to extend FrameNet coverage, to enrich WordNet with frame semantic information and possibly to extend FrameNet to languages other than English. The method uses a knowledge-based Word Sense Disambiguation algorithm for linking FrameNet lexical units to WordNet synsets. Speciﬁcally, we exploit a graph-based Word Sense Disambiguation algorithm that uses a large-scale knowledge-base derived from WordNet. We have developed and tested four additional versions of this algorithm showing a substantial improvement over previous results. 
We present a sub-sentential alignment method that extracts high quality multi-word alignments from sentence-aligned multilingual parallel corpora. Unlike other methods, it exploits low frequency terms, which makes it highly scalable. As it relies on alingual concepts, it can process any number of languages at once. Experiments have shown that it is competitive with state-of-the-art methods.  We propose a diﬀerent approach to sub-sentential alignment that solely relies on low frequency terms. While often neglected, they actually provide an elegant solution to the above-mentioned issues. This paper is organized as follows. Section 2 gives an overview of the concepts of the proposed multilingual alignment technique. Section 3 describes the technique in more details. Section 4 addresses the issue of multilingual alignment scoring. Section 5 compares the method with state-of-the-art tools.  Keywords Sub-sentential alignment, low frequency term, hapax, sampling. 
Nowadays, the temporal aspects of natural language are receiving a great research interest. TimeML has been adopted as a standard for temporal information annotation by a large number of researchers. Available TimeML resources are very limited in size and in diversity of languages. This paper analyzes a combination of semantic roles and semantic networks information for improving this situation. An automatic approach using semantic networks to convert temporal semantic roles into TimeML TIMEX3 elements is presented. This approach has been quantitatively evaluated for English and Spanish. The results point out that the presented approach can help in a semi-automatic creation of TimeML resources for the evaluated languages and could be also valid for other European languages. Keywords TimeML, TimeBank, TE identiﬁcation, Semantic Roles 
We present a pilot experiment to measure the effects of redundancy in the resolution of definite descriptions as performed by a small number of human readers. Although originally intended to provide evidence of how much redundancy should ideally be included in generated anaphoric descriptions, preliminary findings reveal a number of little explored issues that are relevant to both referring expressions generation and interpretation. Keywords Referring expressions generation, Anaphora resolution. 1. Introduction Human speakers routinely make use of redundant information when referring to world or discourse objects via definite descriptions, and they often do so even when the sole purpose of referring is the identification of the target object. By contrast, Natural Language Generation (NLG) systems usually implement referring expressions generation (REG) algorithms that are far less prepared to include redundant information in their output. One possible reason for this difference between real language use and NLG output is the fact that generating redundancy without a proper reason comes with a price, namely, false logic implicatures in the sense defined by H. P. Grice [1]. For instance, in a context containing only one object of type ‘door’, a redundant (from the point of view of identification) reference to the colour attribute of the referent as in “please open the red door” may cause the hearer to consider whether there is any special reason for mentioning such ‘unnecessary’ information at all. To avoid this sort of mishap, most REG algorithms to date (including one of the most influential works in the field, the Incremental algorithm in [2]) attempt to avoid the inclusion of any information not strictly necessary for the identification of the intended referent. Referring expressions produced in this way are suitably brief, but they may look unnaturally so. In extreme cases, certain instances of short descriptions may actually make the identification of the intended referent a daunting task. One such example is the case of deictic references in structurally-complex (e.g., spatial) domains. Deictic referents may be unidentifiable unless a certain amount of redundant information is added [3]. For example, a distinguishing description such as “the girl in white shoes” is not of much help if, say, the referred person is part of a large crowd. Redundancy in this case (e.g., “the girl in white shoes, next to the elevator”) may facilitate  Ivandré Paraboni University of São Paulo (USP/EACH) Av. Arlindo Bettio, 1000 São Paulo, Brazil ivandre@usp.br the resolution of these expression (here understood as the task of interpreting the referring expression and identifying the intend referent.) The implication of this for REG algorithms is that redundancy should be somehow taken into account at least when generating instances of space deixis, and this is precisely the kind of insight needed to design NLG systems that describe objects in physical contexts. For other kinds of application, however, this may be only a minor issue. This is the case, for example, of systems that generate textual reports or documents making intensive use of anaphoric referring expressions. In these cases, it is far less clear whether the same principle of ‘redundancy as a means to help resolution’ is applicable, and if not, what role redundancy should play at all. In this work we investigate the effects of redundant information in anaphora resolution to gather evidence on how to generate more human-like anaphoric descriptions. More specifically, we designed a small pilot experiment to measure reader’s search behaviour under a number of controlled situations of anaphora resolution. Preliminary findings suggest that some of the existing evidence on deixis may not hold for anaphora, and reveal a number of little explored issues that are relevant not only to REG, but to research on language interpretation as well. 2. Background Probably the best-known REG algorithm to date is the Incremental algorithm in [2]. The input to the algorithm is a context set C containing a number of objects – a target object and its distractors – with their corresponding semantic properties (represented as attribute-value pairs as in ‘colour-blue’), and the intended referent r to be described by means of a definite description. The goal of the algorithm is to compute a list of properties L such that L denotes the intended referent r and no other distractor in C. Redundancy in this case is merely a by-product of a more general strategy to cope with the computational complexity of the task: once an attribute is selected for inclusion in L, it can never be removed (and, crucially, not even if a subsequent addition makes this attribute redundant), which gives the name ‘incremental to the approach. In previous work [3] we describe an experiment to measure the effort involved in the resolution of deictic descriptions in spatial domains, whose results suggest that under certain circumstances the inclusion of logical redundancy may be necessary if the hearer is to identify  225  International Conference RANLP 2009 - Borovets, Bulgaria, pages 225–229  the intending referents at all. The findings in [3] however do not cover anaphora, and it is unclear whether they are still applicable to these cases. For a start, unlike space deictic expressions, anaphors do not normally convey location information to help find the antecedent term1, e.g., in a context with only one object of the type ‘cup’, the redundancy in “the cup on the table” may facilitate search for the intended referent in a deictic situation, but less clearly so in an anaphoric context. Secondly, anaphora resolution involves not only searching for the antecedent term in the text (as when searching for domain objects in space deixis) but also interpreting multiple candidate descriptions (including those of the competing discourse objects, or distractors, and the real antecedent term.) Descriptions of distractor objects may vary greatly in the number of attributes that they share with the referring expression, which may somehow have an impact on the overall resolution effort. For instance, given the antecedent term “the large white cat” and the anaphor “the white cat”, the reader may come across distractors such as “the small cat”, “the large black cat” and so on, each of them representing a particular obstacle to resolution. Finally, the work in [3] does not distinguish between discriminatory and non-discriminatory redundancy, that is, it is not clear how redundant information impacts resolution when it may help ruling out distractors (e.g., the use of a redundant attribute ‘white’ in a context in which all distractors are black) or not (e.g., the same, in a context in which some of the distractors are also white.) 3. Experiment Design We are interested in collecting evidence of how redundancy may affect anaphora resolution (i.e., the task of interpreting the referring expression and then identifying the antecedent term in the previous text), so that in the future this information could be taken into account in the development of more human-like REG algorithms. To this end, we designed an experiment in which subjects were instructed to identify anaphoric antecedents of descriptions conveying various degrees of redundancy in a number of documents in electronic format, while their navigation steps and resolution times were recorded with millisecond precision. Subjects. 38 native speakers of Brazilian Portuguese. Procedure. All subjects were shown 13 documents in electronic format in random order. Each document conveyed a short text in a randomly selected domain (e.g., cars, pets, books etc.) Each text was shown one paragraph at a time. Subjects were told to read each paragraph and scroll down to reveal the next one. Upon reaching the end of the text, an instruction of the kind ‘Click on the expression that refers to a X in the text’ was displayed, in which X was an unambiguous 
In this paper, we present a novel, cognitively motivated framework for modelling the cross-modal inﬂuence of visual scene context upon language processing. We illustrate how semantic relations in a knowledge representation of visual scene context can effect syntactic attachment modulations in a weightedconstraint dependency parser. In line with a central tenet of conceptual semantics, visual scene context and linguistic processing are hypothesised to interact via an intermediate, cross-modally integrated level of semantic representation. Cross-modal interaction in our model is restricted by conceptual compatibility between the concepts activated linguistically and contextually. We apply our framework to syntactically ambiguous sentences of German and parse them in the presence of biasing visual scene contexts. The observed modulations in syntactic attachment support our two modelling hypotheses: 1) The inﬂuence of visual context upon syntactic processing is mediated by semantics. 2) The compatibility of concepts from different modalities is a suitable criterion to restrict the scope of cross-modal interaction.  vision-language interface, we begin with the identiﬁcation of elementary requirements for the design of a cognitively motivated framework for the cross-modal integration between vision and language. Adopting a weightedconstraint model of language processing, we outline how in our framework the interpretation of visual context constitutes an additional constraint on the cross-modally integrated semantic representation which is built up based on linguistic and contextual input. In the following section, we provide a brief overview over selected key ﬁndings from milestone experiments at the vision-language interface to motivate the requirements for our framework. In Section 3, we outline how the components of our framework interact with each other and which procedures we employ to achieve cross-modal interaction. In Section 4, we provide experimental results from the integration of visual context into parsing for a particular class of syntactically ambiguous sentences. In Section 5, we summarise our central points and draw conclusions. 2 Milestone Investigations into the Vision-Language Interface  Keywords Cross-Modal Interaction, Parsing, Syntax-Semantics Interface, Context Integration, Information Fusion. 
This paper presents a new study on automatic terminology extraction in the context of bimodal corpora that were generated from lectures and meetings. More speciﬁcally, the study aims to observe to which extent written text (discussed documents) and spoken text (dialogue transcript) share keywords. Using a hybrid terminology extraction approach, experiments have been performed on a collection of bimodal English corpora, including one scientiﬁc conference presentations corpus and two decision-making meetings corpora respectively. The evaluation results highlight a diﬀerence between keywords extracted from written text and from spoken text. Moreover, the obtained results emphasise the importance of considering text obtained from diﬀerent modalities in order to generate rich and consistent keyword lists for bimodal corpora. Keywords bimodal corpus, keyword extraction, keyword similarity, spoken document, written document. 
Good hypertext writing style mandates that link texts clearly indicate the nature of the link target. While this guideline is routinely ignored in HTML, the lightweight markup languages used by wikis encourage or even force hypertext authors to use semantically appropriate link texts. This property of wiki hypertext makes it an ideal candidate for processing with latent semantic analysis, a factor analysis technique for ﬁnding latent transitive relations among naturallanguage documents. In this study, we design, implement, and test an LSA-based information retrieval system for wikis. Instead of a full-text index, our system indexes only link texts and document titles. Nevertheless, its precision exceeds that of a popular full-text search engine, and is comparable to that of PageRank-based systems such as Google. Keywords latent semantic analysis, LSA, latent semantic indexing, LSI, information retrieval, search engines, Wikipedia, wikis, hypertext, hyperlinks, large corpora 
The paper presents the results of a project completed by the authors for realizing a continuous speech recognition system for Bulgarian. The state-of-the-art speech recognition technology used in the system is discussed. Special attention is given to the problems with some specics of the Bulgarian language namely the large vocabulary (450000 wordforms). Some implementation details of the language module are given. At the end the paper provides evaluation of the accuracy of recognition. Keywords Speech recognition, language model 
For Arabic, diacritizing written text is important for many NLP tasks. In the work presented here, we investigate the quality of a diacritization approach, with a high success rate for treebank data but with a more limited success on realworld data. One of the problems we encountered is the non-standard use of the hamza diacritic, which leads to a decrease in diacritization accuracy. If an automatic hamza restoration module precedes diacritization, the results improve from a word error rate of 9.20% to 7.38% in treebank data, and from 7.96% to 5.93% on selected real-world texts. This shows clearly that hamza restoration is a necessary step for improving diacritization quality for Arabic real-world texts. Keywords Arabic diacritization, memory-based learning 
We present a compositional framework for modelling entity-level sentiment (sub)contexts, and demonstrate how holistic multi-entity polarity scoring emerges as a by-product of compositional sentiment parsing. A data set of ﬁve annotators’ multi-entity judgements is presented, and a human ceiling is established for the challenging new task. The accuracy of an initial implementation, which includes both supervised learning and heuristic distance-based scoring methods, is 5.6∼6.8 points below the human ceiling amongst sentences and 8.1∼8.7 points amongst phrases. Keywords Entity-level sentiment analysis, sentiment scoring, sentiment parsing, sentiment annotation, compositional semantics  We take a diﬀerent view on the problem and investigate the possibility of a holistic multi -entity analysis in that we make no categorical distinctions between individual entity mentions, topics, or sentiment roles of any kind. We instead refer to all base nouns simply as entity markers which may (or may not) serve the above metafunctions, and aim at classifying all such markers in sentences using a single, uniﬁed approach. For the sentence in Ex. 1, we envisage a classiﬁer that classiﬁes all of the bracketed entities as positive(+), neutral(N), or negative(-) (NB. / = ‘or’):  (1) “Here’s the  [thing](N)/(+):  Other  [studies](N)/(+) have found that [clergy](+),  and not [psychologists](-)/(+) or other men-  tal [health](+) [experts](+)/(-), are the most  common [source](+)/(N) of [help](+) sought in  [times](N)/(-) of psychological [distress](-).”  
In this paper, we introduce the Le´xico de Formas Flexionadas del Espan˜ol (Leffe), a wide-coverage morphological and syntactic Spanish lexicon based on the Alexina lexical framework. We explain how the Leffe has been created by merging together several heterogeneous lexicons and how the Alexina lexical framework has been applied to Spanish. We also introduce a semi-automatic technique based on a tagger to detect the lexicon’s deﬁciencies. A preliminary evaluation shows the potential of the Leffe and the relevance of both creation and extension processes. 
In this paper an exploratory map of what intelligent natural language processing systems can achieve will be drawn up, given the advances that have been made in recent years as revealed in the latest developments in practical applications of natural language technology in areas as diverse as natural language generation, natural language understanding, machine translation, dialog system etc. Here a mathematical exploration of the issue in question will lay out the constraints on what they can achieve in their goal of automatizing language processing that humans do. It will be shown that these constraints together constitute a fundamental limit which these systems seem to fail to cross. Keywords Intelligent natural language processing systems; constraints; mathematical exploration; language processing. 1. Introduction In recent years we have encountered a massive change in our conception of what natural language processing systems have achieved [1]. We have also gained a broader understanding of conceptual and empirical challenges that we face today in designing and implementing better systems that can be robust without incurring heavy computational costs [2]. With all this we are perhaps moving more towards the goal of automatization of human language processing in machines. But in spite of what has been gained in terms of theoretical and conceptual understanding of the problems, challenges, prospects in building natural language processing systems, there still seems to be an enormous gap between the level of performance these systems have come up to and how human language processing occurs [3] [4]. Is there any fundamental reason why the gap cannot be bridged? If there is any reason, why cannot we overcome it? And what is it about us that makes us do effortlessly all that these systems are designed to do, but are still far behind fully being capable of doing? It would be argued that a fundamental answer to all these questions perhaps exists. And the fundamental answer underlies a fundamental nature of human language processing mechanism. 2. What has been achieved In recent years we have seen a spurt in the growth of computational models and practical systems in natural language processing. In the domain of natural language generation systems we have seen massive developments in 
We present a comparison between two systems for establishing syntactic and semantic dependencies: one that performs dependency parsing and semantic role labeling as a single task, and another that performs the two tasks in isolation. The systems are based on local memorybased classiﬁers predicting syntactic and semantic dependency relations between pairs of words. In a second global phase, the systems perform a deterministic ranking procedure in which the output of the local classiﬁers is combined per sentence into a dependency graph and semantic role labeling assignments for all predicates. The comparison shows that in the learning phase a joint approach produces better-scoring classiﬁers, while after the ranking phase the isolated approach produces the most accurate syntactic dependencies, while the joint approach yields the most accurate semantic role assignments. Keywords Joint learning, dependency parsing, semantic role labeling 
We propose a new method which enables the training of a kernelized structured output model. The structured output learning can ﬂexibly represent a problem, and thus is gaining popularity in natural language processing. Meanwhile the polynomial kernel method is effective in many natural language processing tasks, since it takes into account the combination of features. However, it is computationally difﬁcult to simultaneously use both the structured output learning and the kernel method. Our method avoids this difﬁculty by transforming the kernel function, and enables the kernelized structured output learning. We theoretically discuss the computational complexity of the proposed method and also empirically show its high efﬁciency and effectiveness through experiments in the task of identifying agreement and disagreement relations between utterances in meetings. Identifying agreement and disagreement relations consists of two mutuallycorrelated problems: identiﬁcation of the utterance which each utterance is intended for, and classiﬁcation of each utterance into approval, disapproval or others. We simultaneously use both of the structured output learning and the kernel method in order to take into account this correlation of the two problems. Keywords Structured Output Learning, Machine Learning, Passive-Aggressive Algorithm, Kernel, Meeting Records, Dialog Act, Adjacency-Pairs. 
Word Sense Disambiguation is the task dedicated to the problem of ﬁnding out the sense of a word in context, from all of its many possible senses. Solving this problem requires to know the set of possible senses for a given word, which can be acquired from human knowledge, or from automatic discovery, called Word Sense Induction. In this article, we adapt two existing meta-methods of Word Sense Induction for the automatic construction of a disambiguation lexicon. Our adaptation is based on multiple semantic spaces (also called Word Space Models) produced from a syntactic analysis of a very large number of web pages. These adaptations and the results presented in this article diﬀer from the original methods in that they use a combination of several high dimensional spaces instead of one single representation. Each of these competing semantic spaces takes part in a clustering phase in which they vote on sense induction. Keywords semantic space, word space model, dimensionality reduction, locality sensitive hashing, word sense induction, words clustering, multi-represented data 
False friends are pairs of words in two languages that are perceived as similar, but have diﬀerent meanings, e.g., Gift in German means poison in English. In this paper, we present several unsupervised algorithms for acquiring such pairs from a sentence-aligned bi-text. First, we try different ways of exploiting simple statistics about monolingual word occurrences and cross-lingual word co-occurrences in the bi-text. Second, using methods from statistical machine translation, we induce word alignments in an unsupervised way, from which we estimate lexical translation probabilities, which we use to measure cross-lingual semantic similarity. Third, we experiment with a semantic similarity measure that uses the Web as a corpus to extract local contexts from text snippets returned by a search engine, and a bilingual glossary of known word translation pairs, used as “bridges”. Finally, all measures are combined and applied to the task of identifying likely false friends. The evaluation for Russian and Bulgarian shows a signiﬁcant improvement over previously-proposed algorithms. Keywords Cognates, false friends, cross-lingual semantic similarity, Web as a corpus, statistical machine translation. 
In contrast with other NLP tasks, only few and limited evaluation challenges have been carried out for terminology acquisition. It is nevertheless important to assess the progress made, the quality and limitations of terminological tools. This paper argues that it is possible to deﬁne evaluation protocols for tasks as complex as computational terminology. We focus on the core task of term extraction for which we propose evaluation metrics. We take into account the speciﬁcity of computational terminology, the complexity of its outputs, the application, the user’s role and the absence of well-established gold standard.  Sections 4 and 5 present our proposal: a protocol for evaluating term extractors and the speciﬁc metrics on which it relies. Section 6 describes experiments for meta-evaluating the proposed metrics. 2 State of the art Various experiments have been made to evaluate terminological tools. Some were technologically oriented and took the form of evaluation challenges while others put focus on the application context. 2.1 Evaluation challenges  Keywords Term extraction, evaluation, terminological distance. 
This paper addresses question analysis in the framework of Question Answering over structured data. The problem is set as a relation extraction task, where all the relations of interest in a given domain have to be extracted from natural language questions. The proposed approach applies the notion of Textual Entailment to compare the input questions with a repository of relational textual patterns. The underlying assumption is that a question expresses a certain relation if a pattern for that relation is entailed by the question. We report on a number of experiments, testing diﬀerent simple distancebased entailment algorithms over a dataset of 1487 English questions covering the domain of cultural events in a town, and 75 relations that are relevant in this domain. The positive results obtained demonstrate the feasibility of the overall approach, and its eﬀectiveness in the proposed QA scenario. Keywords Restricted-Domain Question Answering, Textual Entailment, Relation Extraction. 
This paper presents a semi-supervised model for generating a table-of-contents as an indicative summarization. We mainly focus on using word cluster-based information derived from a large amount of unannotated data by an unsupervised algorithm. We integrate word cluster-based features into a discriminative structured learning model, and show that our approach not only increases the quality of the resulting table-of-contents, but also reduces the number of iterations in the training process. In the experiments, our model shows better results than the baseline model in generating a table-of-contents, about 6.5% improvement in terms of averaged ROUGE-L score. Keywords text generation, text summarization, semi-supervised learning 
In order to produce efﬁcient Natural Language Processing (NLP) tools, reliable linguistic resources are a preliminary requirement. When available for a given language, the resources are generally far below the expectations in terms of quality, coverage or usability. This paper presents a project whose ambition is to enhance the production capacities of linguistic resources through the creation and intensive use of interconnected acquisition and correction tools, inter-lingual transfer processes and a collaborative online development framework. 
Document Planning - the task of deciding which content messages should be realised in a target document based on raw data provided by an underlying application, and how these messages should be structured - is arguably one of the most crucial tasks in Natural Language Generation (NLG). In this work we present a machine learning approach to Document Planning that is entirely trainable from annotated corpora, and which paves the way to our long-term goal of developing a text generator system based on a series of classifiers for a simple NLG application in the education domain. Keywords Document Planning; Content Selection. 1. Introduction Natural Language Generation (NLG) systems are used whenever simple, 'canned' text is not sufficient, and greater (i.e., closer to human performance) linguistic variation is required. The traditional NLG architecture is often depicted in simplified form as a 3-stages pipelined process (Document Planning, Sentence Planning and Surface Realisation, cf. [1]), a division that is at least partially motivated by the sheer complexity of the task. Starting from a high-level communicative goal of describing a given domain concept, the system builds up a plan to represent the input data up to the point in which fully-specified text in natural language is produced. The Document Planning module is responsible for deciding what information to communicate (this being the task of Content Determination) and then how this information should be structured for presentation (this being the task of Document Structuring.) For a more comprehensive discussion of the role of Document Planning and its subtasks in the NLG architecture we report to [3]. Document Planning is arguably one of the most crucial components of an NLG system [1]: if a generated document presents the required information in a reasonably coherent structure, then the system may be considered successful even if the text shows surface flaws or limited linguistic variation. On the other hand, if the required information is missing from the text, or if the text is poorly structured, then the overall results are most likely unsatisfactory regardless of how well the individual sentences were realised. When speaking of Data-to-Text generation1, Document Planning is often preceded by a Data 
State-of-the-art Machine Translation (MT) systems are still far from being perfect. An alternative is the so-called Interactive Machine Translation (IMT) framework. In this framework, the knowledge of a human translator is combined with a MT system. We present a new technique for IMT which is based on the generation of partial alignments at phrase-level. The proposed technique partially aligns the source sentence with the user preﬁx and then translates the unaligned portion of the source sentence. The generation of such partial alignments is driven by statistical phrase-based models. Our technique relies on the application of smoothing techniques over the phrase models to appropriately assign probabilities to unseen events. We report experiments investigating the impact of the different smoothing techniques in the accuracy of our system. In addition, we compare the results obtained by our system with those obtained by other well-known IMT systems.  Following these TT ideas, [1] proposed a new approach to IMT. In this approach, fully-ﬂedged statistical MT (SMT) systems are used to produce full target sentence hypotheses, or portions thereof, which can be partially or completely accepted and amended by a human translator. Each partial correct text segment is then used by the SMT system as additional information to achieve further, hopefully improved suggestions. Figure 1 illustrates a typical IMT session. In this paper, we also focus on the IMT approach to CAT. Speciﬁcally, we propose a new IMT engine based on the generation of partial alignments at phrase-level. The proposed technique partially aligns the source sentence with the user preﬁx and then translates the unaligned portion of the source sentence. The partial alignments are generated using the statistical knowledge provided by a phrase-based model. As it will be shown, the techniques proposed here require the application of smoothing techniques over the phrasebased models to correctly assign probabilities to unseen events.  Keywords Statistical machine translation, interactive machine translation, phrase-based translation, phrase-based alignments, smoothing. 
This paper addresses the problem of scientiﬁc research analysis. We use the topic model Latent Dirichlet Allocation [2] and a novel classiﬁer to classify research papers based on topic and language. Moreover, we show various insightful statistics and correlations within and across three research ﬁelds: Linguistics, Computational Linguistics, and Education. In particular, we show how topics change over time within each ﬁeld, what relations and inﬂuences exist between topics within and across ﬁelds, as well as what trends can be established for some of the world’s natural languages. Finally, we talk about trend prediction and topic suggestion as future extensions of this research.  correlations, and graphics which show the dynamics of topics and local and global trends based on the proceedings of the major conferences and journals in the three ﬁelds over many years. In particular, we show how topics change over time within each ﬁeld, what relations exist between topics, what temporal correlations and topic inﬂuences can be determined across ﬁelds, as well as what trends can be established for some of the world’s natural languages. Finally, we mention some future extensions of this research including suggestions for novel topics by combining research ideas across ﬁelds as well as predicting future trends from this combination.  Keywords topic models; scientiﬁc research analysis; statistical approaches 
We present a fairly complete grammar of interrogative and relative clauses in French, written in the formalism of Interaction Grammars. Interaction Grammars combine two key ideas: a grammar is viewed as a constraint system which is expressed through the notion of tree description, and the resource sensitivity of natural languages is used as a syntactic composition principle by means of a system of polarities. Keywords Syntax, grammatical formalism, tree description, polarity, interrogative clause , relative clause, interaction grammar 
The goal of this paper is to compare a set of distance/similarity measures, some motivated statistically, others motivated stylistically, regarding their ability to reﬂect stylistic similarity between texts. To assess the ability of these distance/similarity functions to capture stylistic similarity between texts, we have tested them in the two most frequently employed multivariate statistical analysis settings: cluster analysis and (kernel) principal components analysis. Keywords Stylistic Multivariate Analysis, Statistical Similarity Measures, Cluster Analysis, Kernel Principal Components Analysis 
In this paper, we use corpus-based measures for constructing phylogenetic trees and try to address some questions about the validity of doing this and applicability to linguistic areas as against language families. We experiment with four corpus based distance measures for constructing phylogenetic trees. Three of these measures were earlier tried for estimating language distances. We use a fourth measure based on phonetic and orthographic feature n-grams. We compare the trees obtained using these measures and present our observations. Keywords Language distances, similarity measures, phylogenetic trees  to these questions are aﬃrmative. Overall, the contributions of the paper are the following a) use a new measure for estimating language distance b) present results of the experiments on constructing phylogenetic trees from corpus based word lists rather than handcrafted ones c) validate the hypothesis that India is a linguistic area [10]. The paper is organized as follows. Related work is discussed in Section 2. A brief discussion of various inter-language measures is given in Section 3. The experimental setup and the analysis of the results have been given in Section 4 and Section 5, respectively. We present a summary of our experiments, analysis of the results and future directions of the work in Section 6. 2 Related Work  
We describe a novel language-independent approach to the task of determining the polarity, positive or negative, of the author’s opinion on a speciﬁc topic in natural language text. In particular, weights are assigned to attributes, individual words or word bi-grams, based on their position and on their likelihood of being subjective. The subjectivity of each attribute is estimated in a two-step process, where ﬁrst the probability of being subjective is calculated for each sentence containing the attribute, and then these probabilities are used to alter the attribute’s weights for polarity classiﬁcation. The evaluation results on a standard dataset of movie reviews shows 89.85% classiﬁcation accuracy, which rivals the best previously published results for this dataset for systems that use no additional linguistic information nor external resources. Keywords Sentiment analysis, subjectivity identiﬁcation, polarity classiﬁcation, text categorization. 
In the task of semantic category labeling, given a text, every word in it has to be assigned a semantic category. Our language of interest is Hindi. We use the ontological categories deﬁned in Hindi Wordnet as semantic category inventories. In this paper we present two unsupervised approaches namely Flat Semantic Category Labeler (FSCL) and Hierarchical Semantic Category Labeler (HSCL ). The former method treats semantic categories as a ﬂat list, whereas the latter one exploits the hierarchy among the semantic categories in a top down manner. Further our methods use simple probabilistic models, using which the category labeling becomes a simple table look up with little extra computation and thus opening the possibility of it’s use in real-time interactive systems. Keywords  Fig. 1: Hindi wordnet entry of the word billA. The word has two senses meaning male cat and badge. Ontological category mappings of the two senses are shown on the left side of the ﬁgure. On the right, the semantic category tree(SCT) of the word is shown.  word sense disambiguation, Semantic category labeling, unsupervised 
Evaluation of word space models is usually local in the sense that it only considers words that are deemed very similar by the model. We propose a global evaluation scheme based on clustering of the words. A clustering of high quality in an external evaluation against a semantic resource, such as a dictionary of synonyms, indicates a word space model of high quality. We use Random Indexing to create several different models and compare them by clustering evaluation against the People’s Dictionary of Synonyms, a list of Swedish synonyms that are graded by the public. Most notably we get better results for models based on syntagmatic information (words that appear together) than for models based on paradigmatic information (words that appear in similar contexts). This is quite contrary to previous results that have been presented for local evaluation. Clusterings to ten clusters result in a recall of 83% for a syntagmatic model, compared to 34% for a comparable paradigmatic model, and 10% for a random partition. Keywords Random Indexing, Word Space Model, Word Clustering, Evaluation, Dictionary of Synonyms 
This paper addresses the problem of semantic relation identiﬁcation for a set of relations difﬁcult to differentiate: near-misses and overlaps. Based on empirical observations on a fairly large dataset of such examples we provide an analysis and a taxonomy of such cases. Using this taxonomy we create various contingency sets of relations. These semantic categories are automatically identiﬁed by training and testing three state-of-the-art semantic classiﬁers employing various feature sets. The results show that in order to identify such near-misses and overlaps accurately, a semantic relation identiﬁcation system needs to go beyond the ontological information of the two nouns and rely heavily on contextual and pragmatic knowledge. Keywords lexical semantics; semantic relations; machine learning 
We introduce a formal framework that allows the calculation of new purely statistical conﬁdence measures for parsing, which are estimated from posterior probability of constituents. These measures allow us to mark each constituent of a parse tree as correct or incorrect. Experimental assessment using the Penn Treebank shows favorable results for the classical conﬁdence evaluation metrics: the CER and the ROC curve. We also present preliminar experiments on application of conﬁdence measures to improve parse trees by automatic constituent relabeling. 
We explore the performance of the Vector Space Model (VSM) in ﬁnding verb synonyms in Portuguese by analyzing the impact of three operating parameters: (i) the weighting function, (ii) the context window used for automatically extracting features, and (iii) the minimum number of vector features. We rely on distributional statistics taken from a large n-gram database to build feature vectors, using minimal linguistic pre-processing. Automatic evaluation of synonym candidates using gold-standard information from the OpenOﬃce and Wiktionary thesaurus shows that low frequency features carry most information regarding verb similarity, and that a [0, +2] window carries more information than a [-2, 0] window. We show that satisfactory precision levels require vectors with 50 or more non-nil components. Manual evaluation over a set of declarative verbs and psychological verbs show that VSM-based approaches achieve good precision in ﬁnding verb synonyms for Portuguese, even when using minimal linguistic knowledge. This lead us to proposing a performance baseline for this task. Keywords Vector Space Model, Semantics, Relation Extraction, Statistical Methods, Language Resources, Evaluation 
We present a method for extracting verbcentered constructions (VCCs) from corpora. In our framework, simple and multiword verbs, with or without valence are all VCCs. They are treated uniformly, from e.g. to breathe till e.g. to take something into consideration. In order to extract VCCs we represent the corpus as a sequence of clauses that contain a verb together with all its NP dependents. The method is a generalization of a former subcategorization frame extraction method. It is based on cumulative counting of frequent subframes: small frequency counts are inherited to one of the longest available subframes using random selection. The method nds out automatically the number of elements in VCCs; and it detects automatically whether a content word is integral part of the VCC (forming a multiword verb), or just the verb-dependent relation is important (forming a valence slot of the verb). Signicance of our method lies in its capability to deal with multiword verbs and (their) valence simultaneously. The paper includes evaluation for Hungarian, we obtain precision values above 80% using nbest lists evaluation. The representation and the method is in essence language independent, it could be applied to other languages as well. 
Stemming refers to the grouping of morphologically related words into so-called stem classes for the purpose of improving information retrieval performance. Traditional approaches to stemming are language-speciﬁc and require a substantial amount of linguistic knowledge. A viable alternative is string distance-based stemming, in which stem classes are obtained by clustering word-forms from a corpus. In this paper, we apply string distance-based stemming to the highly inﬂected Croatian language using a number of string distance measures proposed in the literature. We focus on evaluating the stemming performance at both inﬂectional and derivational level, and investigate how this performance relates to the choice of the distance threshold value. Although our focus is on the Croatian language, we believe our results transfer well to languages of similar morphological complexity. Keywords Stemming, morphology, string distance,Croatian language 
This paper presents a machine learning study of aﬀective words in Russian and Romanian languages. We tag the word aﬀective meaning by one of the WordNet Aﬀect six labels anger, disgust, fear, joy, sadness, surprise and group into “positive” (joy, surprise) and “negative” (anger, disgust, fear, sadness) classes. We use the word spelling, a word form, to represent words in machine learning experiments to solve the multiclass classiﬁcation and binary classiﬁcation problems. The results show that the word form can be a reliable source of learning the aﬀect. Keywords: phonosemantics, sentiment analysis, machine learning 
We propose domain-independent language patterns that purposefully omit the aﬀective words for the classiﬁcation of opinions. The information extracted with those patterns is then used to analyze opinions expressed in the texts. Empirical evidence shows that opinions can be discovered without the use of aﬀective words. We ran experiments on four sets of reviews of consumer goods: books, DVD, electronics, kitchen, and house ware. Our results support the practical use of our approach and its competitiveness in comparison with other data-driven methods. This method can also be applied to analyze texts which do not explicitly disclose aﬀects such as medical and legal documents. 
This paper presents Amharic part of speech taggers developed for factored language modeling. Hidden Markov Model (HMM) and Support Vector Machine (SVM) based taggers have been trained using the TnT and SVMTool. The overall accuracy of the best performing TnT- and SVM-based taggers is 82.99% and 85.50%, respectively. Generally, with respect to accuracy SVM-based taggers perform better than TnTbased taggers although TnT-based taggers are more eﬃcient with regard to speed and memory requirement. We have developed factored language models (with two and four parents) for which the estimation of the probability for each word depends on the previous one or two words and their POS. These language models have been used in an Amharic speech recognition task in a lattice rescoring framework and a signiﬁcant improvement in word recognition accuracy has been observed. Keywords POS tagging, Amharic, factored language model 
An important part of a dialogue system is the correct labelling of turns with dialogue-related meaning. This meaning is usually represented by dialogue acts, which give the system semantic information about user intentions. Each dialogue act gives the semantic of a segment of a turn, which can be formed by several segments. Probabilistic models that perform dialogue act labelling can be used on segmented or unsegmented turns. The last option is the more realistic one, but provides poorer results. An hypothesis on the number of segments can be provided in this case to improve the results. We propose some methods to estimate the probability of the number of segments based on the transcription of the turn. The new labelling model includes the estimation of the probability of the number of segments in the turn. The results show that this inclusion signiﬁcantly improves the labelling accuracy. Keywords dialogue systems, dialogue act, statistical labelling 
In this paper we address the task of transferring FrameNet annotations from an English corpus to an aligned Italian corpus. Experiments were carried out on an English-Italian bitext extracted from the Europarl corpus and on a set of selected sentences from the English FrameNet corpus that have been manually translated into Italian. Our research activity is aimed at answering the following three questions: (1) What is the best annotation transfer algorithm for the English-Italian couple? (2) What kind of parallel corpus is best suitable to the annotation transfer task? (3) How should the annotation transfer be evaluated, given the ﬁnal aim of the transfer? Keywords Frame semantics, cross-language annotation transfer, automatic development of lexical resources. 
This paper studies the application of text similarity methods to disambiguate ambiguous links between WordNet nouns and Wikipedia categories. The methods range from word overlap between glosses, random projections, WordNetbased similarity, and a full-ﬂedged textual entailment system. Both unsupervised and supervised combinations have been tried. The goldstandard with disambiguated links is publicly available. The results range from 64.7% for the ﬁrst sense heuristic, 68% for an unsupervised combination, and up to 77.74% for a supervised combination. 
In this paper, we show that by integrating existing NLP techniques and Semantic Web tools in a novel way, we can provide a valuable contribution to the solution of the knowledge acquisition bottleneck problem. NLP techniques to create a domain ontology on the basis of an open domain corpus have been combined with Semantic Web tools. More specifically, Watson and Prompt have been employed to enhance the kick-oﬀ ontology while Cornetto, a lexical database for Dutch, has been adopted to establish a link between the concepts and their Dutch lexicalization. The lexicalized ontology constitutes the basis for the cross-language retrieval of learning objects within the LT4eL eLearning project. Keywords Ontology learning, eLearning, NLP, Semantic Web, cross-lingual retrieval 
The article presents two automatic methods that reduce the complexity of the ambiguous space introduced by the omission of the part of speech tagger from the architecture of a shallow machine translation system. The methods were implemented in a fully functional translation system for related languages. The language pair chosen for the experiments was Slovenian-Serbian as these languages are highly inﬂectional with morphologically ambiguous forms. The empirical evaluations show an improvement over the original system.  selects the best translation among all available translation candidates. The rest of the article is organized as follows: the section 2 presents the current accomplishments in the ﬁeld of the machine translation for related languages, section 3 presents the basic motives that led the authors to this set of experiments. The section 4 shows the motivation that led to the experiment presented in the paper. the section 5 presents the main method and the section presents the evaluation methodology with the results. The article concludes with a discussion. 2 State of the art  Keywords MT, RBMT, SMT, related languages, ranking, Apertium. 
This paper explores methods for increasing performance of CRF models, with a particular concern for transfer learning. We consider in particular the transfer case from political news to hard-to-tag business news, and show the effectiveness of several methods, including a novel semi-supervised approach. Keywords Entity extraction, machine learning, business intelligence. 1. Introduction: name tagging Named entity recognition is one of the most widely studied problems in computational language processing. It was one of the first tasks to be treated with the corpus-based method, and has remained a touchstone for benchmarking corpus-based algorithms and learning regimens. Part of the enduring interest is that name tagging continues to provide technical challenges that help drive research. In particular, while the fundamentals of training a name tagger are well understood, such barriers to practical application as robust coverage and transfer training remain active research areas. Indeed, name-tagging systems tend to perform best when both training and test data are drawn from the same distribution of sources and sample times. However, even seemingly small divergences between training and test can lead to steep drop-offs in performance. Overcoming this lack of carry-over from training to test is thus a key precondition for practical entity recognition applications. This paper addresses this issue in the context of training conditional random fields (CRFs) to tag named entities in business texts. We explore several orthogonal strategies for bringing a name tagger to bear on a new domain, with the aim of providing high test-time performance, robustness to out-of-training phenomena, and minimal transfer training costs. We apply these strategies to a business news corpus, and achieve substantial performance gains while only requiring modest investments in transfer training. 2. Tagging business news The potential divergence between a name tagger’s training and test performance was documented as far back as the MUC7 evaluation [15]. At issue was a shift in topic between training and test conditions: from air incidents to  satellite launches. While the training and test data were otherwise comparable (same sources, same broad topic of aerospace), several system developers implicated this as a cause for poor test-time performance. In a recent study [18], we sought to quantify this divergence in the case of business texts. Our study found a substantial training-to-test performance gap for several mature recent systems trained (or hand-configured) to process current events news. While many of the systems did well with current events, their F scores dropped by 15 to 25 points for business news and financial reports. The present paper takes these observations as a challenge to train a business entity tagger. The business genre is primarily of interest here as a case study, though all the more interesting because it appears so challenging. The framework we have chosen towards this end is that of conditional random fields. In the few years since their introduction [10], CRF models have enjoyed a groundswell of interest, especially as a method for discriminative sequence labeling. They have been applied to conventional sequence labeling tasks like part-of-speech tagging [20] or chunking [14], and unconventional ones like anonymization [21]. For our purposes, conditional random fields provide a number of distinct advantages. A key factor is that discriminative CRF training is not confounded by conditionally dependent features. This makes it safe to include useful features that may be conditionally dependent, e.g. lexical and part-of-speech n-grams. This also allows for features that encode non-local dependencies and external knowledge sources: these typically capture generalizations that co-vary in useful ways with the data, and are thus not independent of other features. CRF training also scales well, even with large numbers of n-gram features. Finally, a CRF allows for post-hoc adjustment of the prior probability of a label. By artificially decreasing the prior, one causes the CRF decoder to generate fewer instances of the label, hence increasing precision at the expense of recall [12]; this proved very useful in this work. We used the Carafe open source CRF package.1 
We investigate the problem of extracting synonyms from dictionary deﬁnitions. Our premise for using definition texts in dictionaries is that, in contrast to freetexts, their composition usually exhibits more regularities in terms of syntax and style and thus, will provide a better controlled environment for synonym extraction. We propose three extraction methods: two rule-based ones and one using the maximum entropy model; each method is evaluated on three experiments — by solving TOEFL synonym questions, by comparing extraction results with existing thesauri, and by labeling synonyms in deﬁnition texts. Results show that simple rule-based extraction methods perform surprisingly well on solving TOEFL synonym questions; they actually out-perform the best reported lexicon-based method by a large margin, although they do not correlate as well with existing thesauri. Keywords Lexical semantics, synonym extraction, dictionary deﬁnition mining, maximum entropy classiﬁcation 
Instance sampling is a method to balance extremely skewed training sets as they occur, for example, in machine learning settings for anaphora resolution. Here, the number of negative samples (i.e. non-anaphoric pairs) is usually substantially larger than the number of positive samples. This causes classiﬁers to be biased towards negative classiﬁcation, leading to suboptimal performance. In this paper, we explore how diﬀerent techniques of instance sampling inﬂuence the performance of an anaphora resolution system for German given diﬀerent classiﬁers. All sampling methods prove to increase the F-score for all classiﬁers, but the most successful method is random sampling. In the best setting, the F-score improves from 0.541 to 0.608 for memory-based learning, from 0.561 to 0.611 for decision tree learning and from 0.511 to 0.584 for maximum entropy learning. 
We propose a new evaluation strategy for keyphrase extraction based on approximate keyphrase matching. It corresponds well with human judgments and is better suited to assess the performance of keyphrase extraction approaches. Additionally, we propose a generalized framework for comprehensive analysis of keyphrase extraction that subsumes most existing approaches, which allows for fair testing conditions. For the ﬁrst time, we compare the results of state-of-the-art unsupervised and supervised keyphrase extraction approaches on three evaluation datasets and show that the relative performance of the approaches heavily depends on the evaluation metric as well as on the properties of the evaluation dataset. Keywords keyphrase extraction; approximate matching 
Automatic Term Recognition systems extract domain-speciﬁc terms from text corpora. Unfortunately current systems fail to capture the whole of the domain covered by a corpus. To address this problem, we present a novel term re-ranking method that generates term lists containing terms that are not only individually salient, but also contribute to a globally diverse list that is truly representative of the corpus. We show that, even without any prior knowledge about the domain, our proposed method improves the diversity of the results produced by two popular automatic term recognition algorithms. Keywords Automatic Term Recognition, Diversity in Ranking, Random Walk, Semantic Similarity 
Qin Lu Department of Computing Hong Kong Polytechnic University csluqin@comp.polyu.edu.hk  
Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk  
We demonstrate an information credibility analysis system called WISDOM. The purpose of WISDOM is to evaluate the credibility of information available on the Web from multiple viewpoints. WISDOM considers the following to be the source of information credibility: information contents, information senders, and information appearances. We aim at analyzing and organizing these measures on the basis of semantics-oriented natural language processing (NLP) techniques. 1. Introduction As computers and computer networks become increasingly sophisticated, a vast amount of information and knowledge has been accumulated and circulated on the Web. They provide people with options regarding their daily lives and are starting to have a strong influence on governmental policies and business management. However, a crucial problem is that the information available on the Web is not necessarily credible. It is actually very difficult for human beings to judge the credibility of the information and even more difficult for computers. However, computers can be used to develop a system that collects, organizes, and relativises information and helps human beings view information from several viewpoints and judge the credibility of the information. Information organization is a promising endeavor in the area of next-generation Web search. The search engine Clusty provides a search result clustering1, and Cuil classifies a search result on the basis of query-related terms2. The persuasive technology research project at Stanford University discussed how websites can be designed to influence people’s perceptions (B. J. Fogg, 2003). However, as per our knowledge, no research has been carried out for supporting the human judgment on information credibility and information organization systems for this purpose. In order to support the judgment of information credibility, it is necessary to extract the background, facts, and various opinions and their 
This is a paper supporting the demonstration of the LX-Center at ACL-IJCNLP-09. LX-Center is a web center of online linguistic services aimed at both demonstrating a range of language technology tools and at fostering the education, research and development in natural language science and technology. 
We have developed a novel, publicly available annotation tool for the semantic encoding of texts, especially those in the narrative domain. Users can create formal propositions to represent spans of text, as well as temporal relations and other aspects of narrative. A built-in naturallanguage generation component regenerates text from the formal structures, which eases the annotation process. We have run collection experiments with the tool and shown that non-experts can easily create semantic encodings of short fables. We present this tool as a stand-alone, reusable resource for research in semantics in which formal encoding of text, especially in a narrative form, is required. 
ProLiV - Animated Process-modeler of Complex (Computational) Linguistic Methods and Theories - is a fully modular, ﬂexible, XML-based stand-alone Java application, used for computer-assisted learning in Natural Language Processing (NLP) or Computational Linguistics (CL). Having a ﬂexible and extendible architecture, the system presents the students, by means of text, of visual elements (such as pictures and animations) and of interactive parameter set-up, the following topics: Latent Semantics Analysis (LSA), (computational) lexicons, question modeling, Hidden-Markov-Models (HMM), and Topic-Focus. These topics are addressed to ﬁrst-year students in computer science and/or linguistics. 
We developed caitra, a novel tool that aids human translators by (a) making suggestions for sentence completion in an interactive machine translation setting, (b) providing alternative word and phrase translations, and (c) allowing them to postedit machine translation output. The tool uses the Moses decoder, is implemented in Ruby on Rails and C++ and delivered over the web. 
 2 Joshua Toolkit  We describe Joshua (Li et al., 2009a)1, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for translation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements sufﬁx-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the ﬁeld or power users interested in extending the toolkit. 
In this demo, we present a wiki-style platform – WikiBABEL – that enables easy collaborative creation of multilingual content in many nonEnglish Wikipedias, by leveraging the relatively larger and more stable content in the English Wikipedia. The platform provides an intuitive user interface that maintains the user focus on the multilingual Wikipedia content creation, by engaging search tools for easy discoverability of related English source material, and a set of linguistic and collaborative tools to make the content translation simple. We present two different usage scenarios and discuss our experience in testing them with real users. Such integrated content creation platform in Wikipedia may yield as a by-product, parallel corpora that are critical for research in statistical machine translation systems in many languages of the world. 
This paper presents a system for querying treebanks. The system consists of a powerful query language with natural support for cross-layer queries, a client interface with a graphical query builder and visualizer of the results, a command-line client interface, and two substitutable query engines: a very efﬁcient engine using a relational database (suitable for large static data), and a slower, but paralel-computing enabled, engine operating on treebank ﬁles (suitable for “live” data). 
This work describes an online application that uses Natural Language Generation (NLG) methods to generate walking directions in combination with dynamic 2D visualisation. We make use of third party resources, which provide for a given query (geographic) routes and landmarks along the way. We present a statistical model that can be used for generating natural language directions. This model is trained on a corpus of walking directions annotated with POS, grammatical information, frame-semantics and markup for temporal structure. 
Over several years, we have developed an approach to spoken dialogue systems that includes rule-based and trainable dialogue managers, spoken language understanding and generation modules, and a comprehensive dialogue system architecture. We present a Reinforcement Learning-based dialogue system that goes beyond standard rule-based models and computes on-line decisions of the best dialogue moves. The key concept of this work is that we bridge the gap between manually written dialog models (e.g. rule-based) and adaptive computational models such as Partially Observable Markov Decision Processes (POMDP) based dialogue managers. 
The use of ﬁgurative language is ubiquitous in natural language texts and it is a serious bottleneck in automatic text understanding. We address the problem of interpretation of logical metonymy, using a statistical method. Our approach originates from that of Lapata and Lascarides (2003), which generates a list of nondisambiguated interpretations with their likelihood derived from a corpus. We propose a novel sense-based representation of the interpretation of logical metonymy and a more thorough evaluation method than that of Lapata and Lascarides (2003). By carrying out a human experiment we prove that such a representation is intuitive to human subjects. We derive a ranking scheme for verb senses using an unannotated corpus, WordNet sense numbering and glosses. We also provide an account of the requirements that different aspectual verbs impose onto the interpretation of logical metonymy. We tested our system on verb-object metonymic phrases. It identiﬁes and ranks metonymic interpretations with the mean average precision of 0.83 as compared to the gold standard. 
Large scale efforts are underway to create dependency treebanks and parsers for Hindi and other Indian languages. Hindi, being a morphologically rich, ﬂexible word order language, brings challenges such as handling non-projectivity in parsing. In this work, we look at non-projectivity in Hyderabad Dependency Treebank (HyDT) for Hindi. Non-projectivity has been analysed from two perspectives: graph properties that restrict non-projectivity and linguistic phenomenon behind non-projectivity in HyDT. Since Hindi has ample instances of non-projectivity (14% of all structures in HyDT are non-projective), it presents a case for an in depth study of this phenomenon for a better insight, from both of these perspectives. We have looked at graph constriants like planarity, gap degree, edge degree and well-nestedness on structures in HyDT. We also analyse non-projectivity in Hindi in terms of various linguistic parameters such as the causes of non-projectivity, its rigidity (possibility of reordering) and whether the reordered construction is the natural one. 
This paper presents ongoing research in clinical information extraction. This work introduces a new genre of text which are not well-written, noise prone, ungrammatical and with much cryptic content. A corpus of clinical progress notes drawn form an Intensive Care Service has been manually annotated with more than 15000 clinical named entities in 11 entity types. This paper reports on the challenges involved in creating the annotation schema, and recognising and annotating clinical named entities. The information extraction task has initially used two approaches: a rule based system and a machine learning system using Conditional Random Fields (CRF). Different features are investigated to assess the interaction of feature sets and the supervised learning approaches to establish the combination best suited to this data set. The rule based and CRF systems achieved an F-score of 64.12% and 81.48% respectively. 
This paper presents three methods that can be used to recognize paraphrases. They all employ string similarity measures applied to shallow abstractions of the input sentences, and a Maximum Entropy classiﬁer to learn how to combine the resulting features. Two of the methods also exploit WordNet to detect synonyms and one of them also exploits a dependency parser. We experiment on two datasets, the MSR paraphrasing corpus and a dataset that we automatically created from the MTC corpus. Our system achieves state of the art or better results. 
Mapping and classiﬁcation of chemical compound names are important aspects of the tasks of BioNLP. This paper introduces the architecture of a system for the syntactic and semantic analysis of such names. Our system aims at yielding both the denoted chemical structure and a classiﬁcation of a given name. We employ a novel approach to the task which promises an elegant and efﬁcient way of solving the problem. The proposed system differs signiﬁcantly from existing systems, in that it is also able to deal with underspecifying names and class names. 
Dependency parsers show syntactic relations between words using a directed graph, but comparing dependency parsers is difﬁcult because of differences in theoretical models. We describe a system to convert dependency models to a structural grammar used in grammar education. Doing so highlights features that are potentially overlooked in the dependency graph, as well as exposing potential weaknesses and limitations in parsing models. Our system performs automated analysis of dependency relations and uses them to populate a data structure we designed to emulate sentence diagrams. This is done by mapping dependency relations between words to the relative positions of those words in a sentence diagram. Using an original metric for judging the accuracy of sentence diagrams, we achieve precision of 85%. Multiple causes for errors are presented as potential areas for improvement in dependency parsers. 
Data-driven function tag assignment has been studied for English using Penn Treebank data. In this paper, we address the question of whether such method can be applied to other languages and Treebank resources. In addition to simply extend previous method from English to Chinese, we also proposed an effective way to recognize function tags directly from lexical information, which is easily scalable for languages that lack sufﬁcient parsing resources or have inherent linguistic challenges for parsing. We investigated a supervised sequence learning method to automatically recognize function tags, which achieves an F-score of 0.938 on gold-standard POS (Part-ofSpeech) tagged Chinese text – a statistically signiﬁcant improvement over existing Chinese function label assignment systems. Results show that a small number of linguistically motivated lexical features are sufﬁcient to achieve comparable performance to systems using sophisticated parse trees. 
Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query. In this paper, we present a novel approach that employs the generalized Expectation Maximization (EM) algorithm to improve language models by representing their parameters as observation probabilities of Hidden Markov Models (HMM). In the experiments, we demonstrate that our method outperforms standard SLM-based and tf.idfbased methods on TREC 2005 HARD Track data. 
Parallel corpora are made by human beings. However, as an MT system is an aggregation of state-of-the-art NLP technologies without any intervention of human beings, it is unavoidable that quite a few sentence pairs are beyond its analysis and that will therefore not contribute to the system. Furthermore, they in turn may act against our objectives to make the overall performance worse. Possible unfavorable items are n : m mapping objects, such as paraphrases, non-literal translations, and multiword expressions. This paper presents a pre-processing method which detects such unfavorable items before supplying them to the word aligner under the assumption that their frequency is low, such as below 5 percent. We show an improvement of Bleu score from 28.0 to 31.4 in English-Spanish and from 16.9 to 22.1 in German-English. 
In this paper we describe the Rovereto Emotive Corpus (REC) which we collected to investigate the relationship between emotion and cooperation in dialogue tasks. It is an area where still many unsolved questions are present. One of the main open issues is the annotation of the socalled “blended” emotions and their recognition. Usually, there is a low agreement among raters in annotating emotions and, surprisingly, emotion recognition is higher in a condition of modality deprivation (i. e. only acoustic or only visual modality vs. bimodal display of emotion). Because of these previous results, we collected a corpus in which “emotive” tokens are pointed out during the recordings by psychophysiological indexes (ElectroCardioGram, and Galvanic Skin Conductance). From the output values of these indexes a general recognition of each emotion arousal is allowed. After this selection we will annotate emotive interactions with our multimodal annotation scheme, performing a kappa statistic on annotation results to validate our coding scheme. In the near future, a logistic regression on annotated data will be performed to find out correlations between cooperation and negative emotions. A final step will be an fMRI experiment on emotion recognition of blended emotions from face displays. 
Focusing on multi-document personal name disambiguation, this paper develops an agglomerative clustering approach to resolving this problem. We start from an analysis of pointwise mutual information between feature and the ambiguous name, which brings about a novel weight computing method for feature in clustering. Then a trade-off measure between within-cluster compactness and among-cluster separation is proposed for stopping clustering. After that, we apply a labeling method to find representative feature for each cluster. Finally, experiments are conducted on word-based clustering in Chinese dataset and the result shows a good effect. 
Sentence Clustering is often used as a ﬁrst step in Multi-Document Summarization (MDS) to ﬁnd redundant information. All the same there is no gold standard available. This paper describes the creation of a gold standard for sentence clustering from DUC document sets. The procedure of building the gold standard and the guidelines which were given to six human judges are described. The most widely used and promising evaluation measures are presented and discussed. 
Variational EM has become a popular technique in probabilistic NLP with hidden variables. Commonly, for computational tractability, we make strong independence assumptions, such as the meanﬁeld assumption, in approximating posterior distributions over hidden variables. We show how a looser restriction on the approximate posterior, requiring it to be a mixture, can help inject prior knowledge to exploit soft constraints during the variational E-step. 
Past work on English coordination has focused on coordination scope disambiguation. In Japanese, detecting whether coordination exists in a sentence is also a problem, and the state-of-the-art alignmentbased method specialized for scope disambiguation does not perform well on Japanese sentences. To take the detection of coordination into account, this paper introduces a ‘bypass’ to the alignment graph used by this method, so as to explicitly represent the non-existence of coordinate structures in a sentence. We also present an effective feature decomposition scheme based on the distance between words in conjuncts. 
We present a CYK and an Earley-style algorithm for parsing Range Concatenation Grammar (RCG), using the deductive parsing framework. The characteristic property of the Earley parser is that we use a technique of range boundary constraint propagation to compute the yields of non-terminals as late as possible. Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart. 
Discourse connectives are words or phrases such as once, since, and on the contrary that explicitly signal the presence of a discourse relation. There are two types of ambiguity that need to be resolved during discourse processing. First, a word can be ambiguous between discourse or non-discourse usage. For example, once can be either a temporal discourse connective or a simply a word meaning “formerly”. Secondly, some connectives are ambiguous in terms of the relation they mark. For example since can serve as either a temporal or causal connective. We demonstrate that syntactic features improve performance in both disambiguation tasks. We report state-ofthe-art results for identifying discourse vs. non-discourse usage and human-level performance on sense disambiguation. 
This paper proposes a novel user intention simulation method which is a data-driven approach but able to integrate diverse user discourse knowledge together to simulate various type of users. In Markov logic framework, logistic regression based data-driven user intention modeling is introduced, and human dialog knowledge are designed into two layers such as domain and discourse knowledge, then it is integrated with the data-driven model in generation time. Cooperative, corrective and selfdirecting discourse knowledge are designed and integrated to mimic such type of users. Experiments were carried out to investigate the patterns of simulated users, and it turned out that our approach was successful to generate user intention patterns which are not only unseen in the training corpus and but also personalized in the designed direction. 
The abundance of homophones in Chinese significantly increases the number of similarly acceptable candidates in English-to-Chinese transliteration (E2C). The dialectal factor also leads to different transliteration practice. We compare E2C between Mandarin Chinese and Cantonese, and report work in progress for dealing with homophones and tonal patterns despite potential skewed distributions of individual Chinese characters in the training data. 
A collection of 3208 reported errors of Chinese words were analyzed. Among which, 7.2% involved rarely used character, and 98.4% were assigned common classifications of their causes by human subjects. In particular, 80% of the errors observed in writings of middle school students were related to the pronunciations and 30% were related to the compositions of words. Experimental results show that using intuitive Web-based statistics helped us capture only about 75% of these errors. In a related task, the Web-based statistics are useful for recommending incorrect characters for composing test items for "incorrect character identification" tests about 93% of the time. 
Most NLP applications work under the assumption that a user input is error-free; thus, word segmentation (WS) for written languages that use word boundary markers (WBMs), such as spaces, has been regarded as a trivial issue. However, noisy real-world texts, such as blogs, e-mails, and SMS, may contain spacing errors that require correction before further processing may take place. For the Korean language, many researchers have adopted a traditional WS approach, which eliminates all spaces in the user input and re-inserts proper word boundaries. Unfortunately, such an approach often exacerbates the word spacing quality for user input, which has few or no spacing errors; such is the case, because a perfect WS model does not exist. In this paper, we propose a novel WS method that takes into consideration the initial word spacing information of the user input. Our method generates a better output than the original user input, even if the user input has few spacing errors. Moreover, the proposed method signiﬁcantly outperforms a state-of-the-art Korean WS model when the user input initially contains less than 10% spacing errors, and performs comparably for cases containing more spacing errors. We believe that the proposed method will be a very practical pre-processing module. 
 Assamese  is  a morphologically rich, agglutinative and  relatively free word order Indic language.  Although spoken by nearly 30 million  people, very little computational linguistic  work has been done for this language. In  this paper, we present our work on part  of speech (POS) tagging for Assamese  using the well-known Hidden Markov  Model. Since no well-deﬁned suitable  tagset was available, we develop a tagset  of 172 tags in consultation with experts  in linguistics. For successful tagging,  we examine relevant linguistic issues in  Assamese. For unknown words, we  perform simple morphological analysis  to determine probable tags. Using a  manually tagged corpus of about 10000  words for training, we obtain a tagging  accuracy of nearly 87% for test inputs.  
This paper presents experiments which combine a grammar-driven and a datadriven parser. We show how the conversion of LFG output to dependency representation allows for a technique of parser stacking, whereby the output of the grammar-driven parser supplies features for a data-driven dependency parser. We evaluate on English and German and show signiﬁcant improvements stemming from the proposed dependency structure as well as various other, deep linguistic features derived from the respective grammars. 
This paper describes an incremental parser based on an adjoining operation. By using the operation, we can avoid the problem of inﬁnite local ambiguity in incremental parsing. This paper further proposes a restricted version of the adjoining operation, which preserves lexical dependencies of partial parse trees. Our experimental results showed that the restriction enhances the accuracy of the incremental parsing. 
Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn. Past approaches have resorted to heuristics. In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size. The learned grammars perform signiﬁcantly better than heuristically extracted ones on parsing accuracy. 
We describe an algorithm for Japanese analysis that does both base phrase chunking and dependency parsing simultaneously in linear-time with a single scan of a sentence. In this paper, we show a pseudo code of the algorithm and evaluate its performance empirically on the Kyoto University Corpus. Experimental results show that the proposed algorithm with the voted perceptron yields reasonably good accuracy. 
We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art Penn Treebank (PTB) parser. An accuracy comparison is performed by converting the CCG derivations into PTB trees. We show that the conversion is extremely difﬁcult to perform, but are able to fairly compare the parsers on a representative subset of the PTB test section, obtaining results for the CCG parser that are statistically no different to those for the Berkeley parser. 
We deﬁne the problem of recognizing entailed relations – given an open set of relations, ﬁnd all occurrences of the relations of interest in a given document set – and pose it as a challenge to scalable information extraction and retrieval. Existing approaches to relation recognition do not address well problems with an open set of relations and a need for high recall: supervised methods are not easily scaled, while unsupervised and semi-supervised methods address a limited aspect of the problem, as they are restricted to frequent, explicit, highly localized patterns. We argue that textual entailment (TE) is necessary to solve such problems, propose a scalable TE architecture, and provide preliminary results on an Entailed Relation Recognition task. 
This paper proposes to solve the bottleneck of finding training data for word sense disambiguation (WSD) in the domain of web queries, where a complete set of ambiguous word senses are unknown. In this paper, we present a combination of active learning and semi-supervised learning method to treat the case when positive examples, which have an expected word sense in web search result, are only given. The novelty of our approach is to use “pseudo negative examples” with reliable confidence score estimated by a classifier trained with positive and unlabeled examples. We show experimentally that our proposed method achieves close enough WSD accuracy to the method with the manually prepared negative examples in several Japanese Web search data. 
Identifying whether a multi-word expression (MWE) is compositional or not is important for numerous NLP applications. Sense induction can partition the context of MWEs into semantic uses and therefore aid in deciding compositionality. We propose an unsupervised system to explore this hypothesis on compound nominals, proper names and adjective-noun constructions, and evaluate the contribution of sense induction. The evaluation set is derived from WordNet in a semisupervised way. Graph connectivity measures are employed for unsupervised parameter tuning. 
Distributional word similarity is most commonly perceived as a symmetric relation. Yet, one of its major applications is lexical expansion, which is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical beneﬁt of directional measures for expansion. 
This paper explores methods to alleviate the effect of lexical sparseness in the classiﬁcation of verbal arguments. We show how automatically generated selectional preferences are able to generalize and perform better than lexical features in a large dataset for semantic role classiﬁcation. The best results are obtained with a novel second-order distributional similarity measure, and the positive effect is specially relevant for out-of-domain data. Our ﬁndings suggest that selectional preferences have potential for improving a full system for Semantic Role Labeling. 
We present a syntactic and lexically based discourse segmenter (SLSeg) that is designed to avoid the common problem of over-segmenting text. Segmentation is the ﬁrst step in a discourse parser, a system that constructs discourse trees from elementary discourse units. We compare SLSeg to a probabilistic segmenter, showing that a conservative approach increases precision at the expense of recall, while retaining a high F-score across both formal and informal texts. 
The development of Dialog-Based ComputerAssisted Language Learning (DB-CALL) systems requires research on the simulation of language learners. This paper presents a new method for generation of grammar errors, an important part of the language learner simulator. Realistic errors are generated via Markov Logic, which provides an effective way to merge a statistical approach with expert knowledge about the grammar error characteristics of language learners. Results suggest that the distribution of simulated grammar errors generated by the proposed model is similar to that of real learners. Human judges also gave consistently close judgments on the quality of the real and simulated grammar errors. 
This paper presents a predicate-argument structure analysis that simultaneously conducts zero-anaphora resolution. By adding noun phrases as candidate arguments that are not only in the sentence of the target predicate but also outside of the sentence, our analyzer identiﬁes arguments regardless of whether they appear in the sentence or not. Because we adopt discriminative models based on maximum entropy for argument identiﬁcation, we can easily add new features. We add language model scores as well as contextual features. We also use contextual information to restrict candidate arguments. 
Modeling of individual users is a promising way of improving the performance of spoken dialogue systems deployed for the general public and utilized repeatedly. We deﬁne “implicitly-supervised” ASR accuracy per user on the basis of responses following the system’s explicit conﬁrmations. We combine the estimated ASR accuracy with the user’s barge-in rate, which represents how well the user is accustomed to using the system, to predict interpretation errors in barge-in utterances. Experimental results showed that the estimated ASR accuracy improved prediction performance. Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling. 
One of the basic problems of efﬁciently generating information-seeking dialogue in interactive question answering is to ﬁnd the topic of an information-seeking question with respect to the answer documents. In this paper we propose an approach to solving this problem using concept clusters. Our empirical results on TREC collections and our ambiguous question collection shows that this approach can be successfully employed to handle ambiguous and list questions. 
We examine correlations between native speaker judgements on automatically generated German text against automatic evaluation metrics. We look at a number of metrics from the MT and Summarisation communities and ﬁnd that for a relative ranking task, most automatic metrics perform equally well and have fairly strong correlations to the human judgements. In contrast, on a naturalness judgement task, the General Text Matcher (GTM) tool correlates best overall, although in general, correlation between the human judgements and the automatic metrics was quite weak. 
Prior approaches to sentence compression have taken low level syntactic constraints into account in order to maintain grammaticality. We propose and successfully evaluate a more comprehensive, generalizable feature set that takes syntactic and structural relationships into account in order to sustain variable compression rates while making compressed sentences more coherent, grammatical and readable. 
In the context of the Document Understanding Conferences, the task of Query-Focused Multi-Document Summarization is intended to improve agreement in content among humangenerated model summaries. Query-focus also aids the automated summarizers in directing the summary at speciﬁc topics, which may result in better agreement with these model summaries. However, while query focus correlates with performance, we show that highperforming automatic systems produce summaries with disproportionally higher query term density than human summarizers do. Experimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance. 
We demonstrate that the bidirectionality of deep grammars, allowing them to generate as well as parse sentences, can be used to automatically and effectively identify errors in the grammars. The system is tested on two implemented HPSG grammars: Jacy for Japanese, and the ERG for English. Using this system, we were able to increase generation coverage in Jacy by 18% (45% to 63%) with only four weeks of grammar development. 
This paper introduces a novel hierarchical summarization approach for automatic multidocument summarization. By creating a hierarchical representation of the words in the input document set, the proposed approach is able to incorporate various objectives of multidocument summarization through an integrated framework. The evaluation is conducted on the DUC 2007 data set. 
In this paper, we propose a novel ranking framework – Co-Feedback Ranking (CoFRank), which allows two base rankers to supervise each other during the ranking process by providing their own ranking results as feedback to the other parties so as to boost the ranking performance. The mutual ranking refinement process continues until the two base rankers cannot learn from each other any more. The overall performance is improved by the enhancement of the base rankers through the mutual learning mechanism. We apply this framework to the sentence ranking problem in query-focused summarization and evaluate its effectiveness on the DUC 2005 data set. The results are promising. 
This paper presents an effective approach to discard most entries of the rule table for statistical machine translation. The rule table is ﬁltered by monolingual key phrases, which are extracted from source text using a technique based on term extraction. Experiments show that 78% of the rule table is reduced without worsening translation performance. In most cases, our approach results in measurable improvements in BLEU score. 
Recently, various synchronous grammars are proposed for syntax-based machine translation, e.g. synchronous context-free grammar and synchronous tree (sequence) substitution grammar, either purely formal or linguistically motivated. Aiming at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves signiﬁcant improvement over three baseline systems. 
 In Cross-Language Information Retrieval (CLIR), Out-of-Vocabulary (OOV) detection and translation pair relevance evaluation still remain as key problems. In this paper, an English-Chinese Bi-Directional OOV translation model is presented, which utilizes Web mining as the corpus source to collect translation pairs and combines supervised learning to evaluate their association degree. The experimental results show that the proposed model can successfully filter the most possible translation candidate with the lower computational cost, and improve the OOV translation ranking effect, especially for popular new words. 
Automatic tools for machine translation (MT) evaluation such as BLEU are well established, but have the drawbacks that they do not perform well at the sentence level and that they presuppose manually translated reference texts. Assuming that the MT system to be evaluated can deal with both directions of a language pair, in this research we suggest to conduct automatic MT evaluation by determining the orthographic similarity between a back-translation and the original source text. This way we eliminate the need for human translated reference texts. By correlating BLEU and back-translation scores with human judgments, it could be shown that the backtranslation score gives an improved performance at the sentence level. 
Tree-based statistical machine translation models have made significant progress in recent years, especially when replacing 1-best trees with packed forests. However, as the parsing accuracy usually goes down dramatically with the increase of sentence length, translating long sentences often takes long time and only produces degenerate translations. We propose a new method named subsentence division that reduces the decoding time and improves the translation quality for tree-based translation. Our approach divides long sentences into several sub-sentences by exploiting tree structures. Large-scale experiments on the NIST 2008 Chinese-toEnglish test set show that our approach achieves an absolute improvement of 1.1 BLEU points over the baseline system in 50% less time. 
Binarization of n-ary rules is critical for the efﬁciency of syntactic machine translation decoding. Because the target side of a rule will generally reorder the source side, it is complex (and sometimes impossible) to ﬁnd synchronous rule binarizations. However, we show that synchronous binarizations are not necessary in a two-stage decoder. Instead, the grammar can be binarized one way for the parsing stage, then rebinarized in a different way for the reranking stage. Each individual binarization considers only one monolingual projection of the grammar, entirely avoiding the constraints of synchronous binarization and allowing binarizations that are separately optimized for each stage. Compared to n-ary forest reranking, even simple target-side binarization schemes improve overall decoding accuracy. 
We would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the ﬁeld of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modiﬁed Viterbi algorithm for tasks interpretable as labeling nodes of dependency trees. In particular, we show that the transfer phase in a Machine Translation system based on tectogrammatical dependency trees can be seen as a task suitable for HMTM. When using the HMTM approach for the English-Czech translation, we reach a moderate improvement over the baseline. 
In this paper, emotion analysis on blog texts has been carried out for a less privileged language like Bengali. Ekman’s six basic emotion types have been selected for reliable and semi automatic word level annotation. An automatic classifier has been applied for recognizing six basic emotion types for different words in a sentence. Application of different scoring strategies to identify sentence level emotion tag based on the acquired word level emotion constituents have produced satisfactory performance. 
This paper proposes how to automatically identify Korean comparative sentences from text documents. This paper first investigates many comparative sentences referring to previous studies and then defines a set of comparative keywords from them. A sentence which contains one or more elements of the keyword set is called a comparative-sentence candidate. Finally, we use machine learning techniques to eliminate non-comparative sentences from the candidates. As a result, we achieved significant performance, an F1-score of 88.54%, in our experiments using various web documents. 
The importance of the new textual genres such as blogs or forum entries is growing in parallel with the evolution of the Social Web. This paper presents two corpora of blog posts in English and in Spanish, annotated according to the EmotiBlog annotation scheme. Furthermore, we created 20 factual and opinionated questions for each language and also the Gold Standard for their answers in the corpus. The purpose of our work is to study the challenges involved in a mixed fact and opinion question answering setting by comparing the performance of two Question Answering (QA) systems as far as mixed opinion and factual setting is concerned. The first one is open domain, while the second one is opinionoriented. We evaluate separately the two systems in both languages and propose possible solutions to improve QA systems that have to process mixed questions. Introduction and motivation In the last few years, the number of blogs has grown exponentially. Thus, the Web contains more and more subjective texts. A research from the Pew Institute shows that 75.000 blogs are created daily (Pang and Lee, 2008). They approach a great variety of topics (computer science, sociology, political science or economics) and are written by different types of people, thus are a relevant resource for large community behavior analysis. Due to the high volume of data contained in blogs, new Natural Language Proc-  essing (NLP) resources, tools and methods are needed in order to manage their language understanding. Our fist contribution consists in carrying out a multilingual research, for English and Spanish. Secondly, many sources are present in blogs, as people introduce quotes from newspaper articles or other information to support their arguments and make references to previous posts in the discussion thread. Thus, when performing a task such as Question Answering (QA), many new aspects have to be taken into consideration. Previous studies in the field (Stoyanov, Cardie and Wiebe, 2005) showed that certain types of queries, which are factual in nature, require the use of Opinion Mining (OM) resources and techniques to retrieve the correct answers. A further contribution this paper brings is the analysis and definition of the criteria for the discrimination among types of factual versus opinionated questions. Previous researchers mainly concentrated on newspaper collections. We formulated and annotated of a set of questions and answers over a multilingual blog collection. A further contribution is the evaluation and comparison of two different approaches to QA a fact-oriented one and another designed for opinion QA scenarios. Related work Research in building factoid QA systems has a long history. However, it is only recently that studies have started to focus also on the creation and development of QA systems for opinions. Recent years have seen the growth of interest in this field, both by the research performed and the publishing of various studies on the requirements  157 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 157–160, Suntec, Singapore, 4 August 2009. c 2009 ACL and AFNLP  and peculiarities of opinion QA systems (Stoyanov, Cardie and Wiebe, 2005), (Pustejovsky and Wiebe, 2006), as well as the organization of international conferences that promote the creation of effective QA systems both for general and subjective texts, as, for example, the Text Analysis Conference (TAC)1. Last year’s TAC 2008 Opinion QA track proposed a mixed setting of factoid (“rigid list”) and opinion questions (“squishy list”), to which the traditional systems had to be adapted. The Alyssa system (Shen et al., 2007), classified the polarity of the question and of the extracted answer snippet, using a Support Vector Machines classifier trained on the MPQA corpus (Wiebe, Wilson and Cardie, 2005), English NTCIR2 data and rules based on the subjectivity lexicon (Wilson, Wiebe and Hoffman, 2005). The PolyU (Wenjie et al., 2008) system determines the sentiment orientation with two estimated language models for the positive versus negative categories. The QUANTA (Li, 2008) system detects the opinion holder, the object and the polarity of the opinion using a semantic labeler based on PropBank3 and some manually defined patterns.  Evaluation  In order to carry out our evaluation, we employed a corpus of blog posts presented in (Boldrini et al., 2009). It is a collection of blog entries in English, Spanish and Italian. However, for this research we used the first two languages. We annotated it using EmotiBlog (Balahur et al., 2009) and we also created a list of 20 questions for each language. Finally, we produced the Gold Standard, by labeling the corpus with the correct answers corresponding to the questions.  1.1 Questions  No TYPE  QUESTION  What international organization do people criticize for  
We introduce the novel task of determining whether a newswire article is “true” or satirical. We experiment with SVMs, feature scaling, and a number of lexical and semantic feature types, and achieve promising results over the task. 
Text categorization is a crucial and wellproven method for organizing the collection of large scale documents. In this paper, we propose a hierarchical multi-class text categorization method with global margin maximization. We not only maximize the margins among leaf categories, but also maximize the margins among their ancestors. Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classiﬁcation algorithms. 
We propose categories of finer-grained polarity for a more effective aspect-based sentiment summary, and describe linguistic and ontological clues that may affect such fine-grained polarity. We argue that relevance for satisfaction, contrastive weight clues, and certain adverbials work to affect the polarity, as evidenced by the statistical analysis. 
We investigate the automatic detection of sentences containing linguistic hedges using corpus statistics and syntactic patterns. We take Wikipedia as an already annotated corpus using its tagged weasel words which mark sentences and phrases as non-factual. We evaluate the quality of Wikipedia as training data for hedge detection, as well as shallow linguistic features. 
This paper proposes a method to extract product features from user reviews and generate a review summary. This method only relies on product speciﬁcations, which usually are easy to obtain. Other resources like segmenter, POS tagger or parser are not required. At feature extraction stage, multiple speciﬁcations are clustered to extend the vocabulary of product features. Hierarchy structure information and unit of measurement information are mined from the speciﬁcation to improve the accuracy of feature extraction. At summary generation stage, hierarchy information in speciﬁcations is used to provide a natural conceptual view of product features. 
Automatic key phrase extraction is fundamental to the success of many recent digital library applications and semantic information retrieval techniques and a difficult and essential problem in Vietnamese natural language processing (NLP). In this work, we propose a novel method for key phrase extracting of Vietnamese text that exploits the Vietnamese Wikipedia as an ontology and exploits specific characteristics of the Vietnamese language for the key phrase selection stage. We also explore NLP techniques that we propose for the analysis of Vietnamese texts, focusing on the advanced candidate phrases recognition phase as well as part-of-speech (POS) tagging. Finally, we review the results of several experiments that have examined the impacts of strategies chosen for Vietnamese key phrase extracting. 
Query segmentation is essential to query processing. It aims to tokenize query words into several semantic segments and help the search engine to improve the precision of retrieval. In this paper, we present a novel unsupervised learning approach to query segmentation based on principal eigenspace similarity of queryword-frequency matrix derived from web statistics. Experimental results show that our approach could achieve superior performance of 35.8% and 17.7% in Fmeasure over the two baselines respectively, i.e. MI (Mutual Information) approach and EM optimization approach. 
As the web grows larger, knowledge acquisition from the web has gained increasing attention. In this paper, we propose using web search clickthrough logs to learn semantic categories. Experimental results show that the proposed method greatly outperforms previous work using only web search query logs.  through logs to improve semantic category acquisition in both precision and recall. We cast semantic category acquisition from search logs as the task of learning labeled instances from few labeled seeds. To our knowledge this is the ﬁrst study that exploits search clickthrough logs for semantic category learning.2 2 Related Work  
We query Web Image search engines with words (e.g., spring) but need images that correspond to particular senses of the word (e.g., ﬂexible coil). Querying with polysemous words often yields unsatisfactory results from engines such as Google Images. We build an image search engine, IDIOM, which improves the quality of returned images by focusing search on the desired sense. Our algorithm, instead of searching for the original query, searches for multiple, automatically chosen translations of the sense in several languages. Experimental results show that IDIOM outperforms Google Images and other competing algorithms returning 22% more relevant images. 
In this paper, we study the problem of extracting technical paraphrases from a parallel software corpus, namely, a collection of duplicate bug reports. Paraphrase acquisition is a fundamental task in the emerging area of text mining for software engineering. Existing paraphrase extraction methods are not entirely suitable here due to the noisy nature of bug reports. We propose a number of techniques to address the noisy data problem. The empirical evaluation shows that our method signiﬁcantly improves an existing method by up to 58%. 
Negative life events, such as death of a family member, argument with a spouse and loss of a job, play an important role in triggering depressive episodes. Therefore, it is worth to develop psychiatric services that can automatically identify such events. In this paper, we propose the use of association language patterns, i.e., meaningful combinations of words (e.g., <loss, job>), as features to classify sentences with negative life events into predefined categories (e.g., Family, Love, Work). The language patterns are discovered using a data mining algorithm, called association pattern mining, by incrementally associating frequently co-occurred words in the sentences annotated with negative life events. The discovered patterns are then combined with single words to train classifiers. Experimental results show that association language patterns are significant features, thus yielding better performance than the baseline system using single words alone. 
In this paper, we propose a method for compiling travel information automatically. For the compilation, we focus on travel blogs, which are defined as travel journals written by bloggers in diary form. We consider that travel blogs are a useful information source for obtaining travel information, because many bloggers' travel experiences are written in this form. Therefore, we identified travel blogs in a blog database and extracted travel information from them. We have confirmed the effectiveness of our method by experiment. For the identification of travel blogs, we obtained scores of 38.1% for Recall and 86.7% for Precision. In the extraction of travel information from travel blogs, we obtained 74.0% for Precision at the top 100 extracted local products, thereby confirming that travel blogs are a useful source of travel information. 
We propose the PlayCoref game, whose purpose is to obtain substantial amount of text data with the coreference annotation. We provide a description of the game design that covers the strategy, the instructions for the players, the input texts selection and preparation, and the score evaluation. 
This paper presents a new term extraction approach using relevance between term candidates calculated by a link analysis based method. Different types of relevance are used separately or jointly for term verification. The proposed approach requires no prior domain knowledge and no adaptation for new domains. Consequently, the method can be used in any domain corpus and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach. 
In this paper we introduce a bilingual dictionary generating tool that does not use any large bilingual corpora. With this tool we implement our novel pivot based bilingual dictionary generation method that uses mainly the WordNet of the pivot language to build a new bilingual dictionary. We propose the usage of WordNet for good accuracy, introducing also a double directional selection method with local thresholds to maximize recall. 
The Columbia Arabic Treebank (CATiB) is a database of syntactic analyses of Arabic sentences. CATiB contrasts with previous approaches to Arabic treebanking in its emphasis on speed with some constraints on linguistic richness. Two basic ideas inspire the CATiB approach: no annotation of redundant information and using representations and terminology inspired by traditional Arabic syntax. We describe CATiB’s representation and annotation procedure, and report on interannotator agreement and speed. 
This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). For a given source sentence S, a maximum entropy (ME) classiﬁer is applied to a large set of candidate target translations . A beam-search algorithm is used to abandon target sentences as non-parallel early on during classiﬁcation if they fall outside the beam. This way, our novel algorithm avoids any document-level preﬁltering step. The algorithm increases the number of extracted parallel sentence pairs signiﬁcantly, which leads to a BLEU improvement of about 1 % on our SpanishEnglish data. 
Combining word alignments trained in two translation directions has mostly relied on heuristics that are not directly motivated by intended applications. We propose a novel method that performs combination as an optimization process. Our algorithm explicitly maximizes the effectiveness function with greedy search for phrase table training or synchronized grammar extraction. Experimental results show that the proposed method leads to signiﬁcantly better translation quality than existing methods. Analysis suggests that this simple approach is able to maintain accuracy while maximizing coverage. 
Often, Statistical Machine Translation (SMT) between English and Korean suffers from null alignment. Previous studies have attempted to resolve this problem by removing unnecessary function words, or by reordering source sentences. However, the removal of function words can cause a serious loss in information. In this paper, we present a possible method of bridging the morpho-syntactic gap for EnglishKorean SMT. In particular, the proposed method tries to transform a source sentence by inserting pseudo words, and by reordering the sentence in such a way that both sentences have a similar length and word order. The proposed method achieves 2.4 increase in BLEU score over baseline phrase-based system. 
We investigate the use of Fisher’s exact signiﬁcance test for pruning the translation table of a hierarchical phrase-based statistical machine translation system. In addition to the signiﬁcance values computed by Fisher’s exact test, we introduce compositional properties to classify phrase pairs of same signiﬁcance values. We also examine the impact of using signiﬁcance values as a feature in translation models. Experimental results show that 1% to 2% BLEU improvements can be achieved along with substantial model size reduction in an Iraqi/English two-way translation task. 
We propose a distance phrase reordering model (DPR) for statistical machine translation (SMT), where the aim is to capture phrase reorderings using a structure learning framework. On both the reordering classiﬁcation and a Chinese-to-English translation task, we show improved performance over a baseline SMT system. 
We study the global topology of the syntactic and semantic distributional similarity networks for English through the technique of spectral analysis. We observe that while the syntactic network has a hierarchical structure with strong communities and their mixtures, the semantic network has several tightly knit communities along with a large core without any such welldeﬁned community structure. 
We propose a new speciﬁcally designed method for paraphrase generation based on Monte-Carlo sampling and show how this algorithm is suitable for its task. Moreover, the basic algorithm presented here leaves a lot of opportunities for future improvement. In particular, our algorithm does not constraint the scoring function in opposite to Viterbi based decoders. It is now possible to use some global features in paraphrase scoring functions. This algorithm opens new outlooks for paraphrase generation and other natural language processing applications like statistical machine translation. 
In Semantic Role Labeling (SRL), it is reasonable to globally assign semantic roles due to strong dependencies among arguments. Some relations between arguments signiﬁcantly characterize the structural information of argument structure. In this paper, we concentrate on thematic hierarchy that is a rank relation restricting syntactic realization of arguments. A loglinear model is proposed to accurately identify thematic rank between two arguments. To import structural information, we employ re-ranking technique to incorporate thematic rank relations into local semantic role classiﬁcation results. Experimental results show that automatic prediction of thematic hierarchy can help semantic role classiﬁcation. 
We propose a novel approach for improving Feature Selection for Word Sense Disambiguation by incorporating a feature relevance prior for each word indicating which features are more likely to be selected. We use transfer of knowledge from similar words to learn this prior over the features, which permits us to learn higher accuracy models, particularly for the rarer word senses. Results on the ONTONOTES verb data show signiﬁcant improvement over the baseline feature selection algorithm and results that are comparable to or better than other state-of-the-art methods. 
Most previous studies on meeting summarization have focused on extractive summarization. In this paper, we investigate if we can apply sentence compression to extractive summaries to generate abstractive summaries. We use different compression algorithms, including integer linear programming with an additional step of ﬁller phrase detection, a noisychannel approach using Markovization formulation of grammar rules, as well as human compressed sentences. Our experiments on the ICSI meeting corpus show that when compared to the abstractive summaries, using sentence compression on the extractive summaries improves their ROUGE scores; however, the best performance is still quite low, suggesting the need of language generation for abstractive summarization. 
This paper presents a Bayesian decision framework that performs automatic story segmentation based on statistical modeling of one or more lexical chain features. Automatic story segmentation aims to locate the instances in time where a story ends and another begins. A lexical chain is formed by linking coherent lexical items chronologically. A story boundary is often associated with a significant number of lexical chains ending before it, starting after it, as well as a low count of chains continuing through it. We devise a Bayesian framework to capture such behavior, using the lexical chain features of start, continuation and end. In the scoring criteria, lexical chain starts/ends are modeled statistically with the Weibull and uniform distributions at story boundaries and non-boundaries respectively. The normal distribution is used for lexical chain continuations. Full combination of all lexical chain features gave the best performance (F1=0.6356). We found that modeling chain continuations contributes significantly towards segmentation performance. 
Acquisition of prosody, in addition to vocabulary and grammar, is essential for language learners. However, it has received less attention in instruction. To enable automatic identiﬁcation and feedback on learners’ prosodic errors, we investigate automatic pitch accent labeling for nonnative speech. We demonstrate that an acoustic-based context model can achieve accuracies over 79% on binary pitch accent recognition when trained on withingroup data. Furthermore, we demonstrate that good accuracies are achieved in crossgroup training, where native and nearnative training data result in no signiﬁcant loss of accuracy on non-native test speech. These ﬁndings illustrate the potential for automatic feedback in computer-assisted prosody learning. 
This paper presents the ﬁrst stochastic ﬁnite-state morphological parser for Turkish. The non-probabilistic parser is a standard ﬁnite-state transducer implementation of two-level morphology formalism. A disambiguated text corpus of 200 million words is used to stochastize the morphotactics transducer, then it is composed with the morphophonemics transducer to get a stochastic morphological parser. We present two applications to evaluate the effectiveness of the stochastic parser; spelling correction and morphology-based language modeling for speech recognition. 
This paper describes a parsing model for speech with repairs that makes a clear separation between linguistically meaningful symbols in the grammar and operations speciﬁc to speech repair in the operation of the parser. This system builds a model of how unﬁnished constituents in speech repairs are likely to ﬁnish, and ﬁnishes them probabilistically with placeholder structure. These modiﬁed repair constituents and the restarted replacement constituent are then recognized together in the same way that two coordinated phrases of the same type are recognized. 
This paper presents an efﬁcient inference algorithm of conditional random ﬁelds (CRFs) for large-scale data. Our key idea is to decompose the output label state into an active set and an inactive set in which most unsupported transitions become a constant. Our method uniﬁes two previous methods for efﬁcient inference of CRFs, and also derives a simple but robust special case that performs faster than exact inference when the active sets are sufﬁciently small. We demonstrate that our method achieves dramatic speedup on six standard natural language processing problems. 
 Maximum entropy (Maxent) is useful in many areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difﬁcult to understand them and see the differences. In this paper, we create a general and uniﬁed framework for IS methods. This framework also connects IS and coordinate descent (CD) methods. Besides, we develop a CD method for Maxent. Results show that it is faster than existing iterative scaling methods1. 
Recently, there is a growing interest in working with tree-structured data in different applications and domains such as computational biology and natural language processing. Moreover, many applications in computational linguistics require the computation of similarities over pair of syntactic or semantic trees. In this context, Tree Edit Distance (TED) has been widely used for many years. However, one of the main constraints of this method is to tune the cost of edit operations, which makes it difﬁcult or sometimes very challenging in dealing with complex problems. In this paper, we propose an original method to estimate and optimize the operation costs in TED, applying the Particle Swarm Optimization algorithm. Our experiments on Recognizing Textual Entailment show the success of this method in automatic estimation, rather than manual assignment of edit costs. 
Most approaches to topic modeling assume an independence between documents that is frequently violated. We present an topic model that makes use of one or more user-speciﬁed graphs describing relationships between documents. These graph are encoded in the form of a Markov random ﬁeld over topics and serve to encourage related documents to have similar topic structures. Experiments on show upwards of a 10% improvement in modeling performance. 
Most of the existing multi-document summarization methods decompose the documents into sentences and work directly in the sentence space using a term-sentence matrix. However, the knowledge on the document side, i.e. the topics embedded in the documents, can help the context understanding and guide the sentence selection in the summarization procedure. In this paper, we propose a new Bayesian sentence-based topic model for summarization by making use of both the term-document and term-sentence associations. An efﬁcient variational Bayesian algorithm is derived for model parameter estimation. Experimental results on benchmark data sets show the effectiveness of the proposed model for the multi-document summarization task. 
We present a discourse-level Tree Adjoining Grammar which tightly integrates syntax and discourse levels, including a representation for discourse entities. We show that this technique makes it possible to extend an optimisation algorithm used in natural language generation (polarity ﬁltering) to the discourse level. We implemented the grammar in a surface realizer and show that this technique can be used to reduce the search space by ﬁltering out referentially incoherent solutions. 
In this paper, we present initial experiments in the recognition of deceptive language. We introduce three data sets of true and lying texts collected for this purpose, and we show that automatic classiﬁcation is a viable technique to distinguish between truth and falsehood as expressed in language. We also introduce a method for class-based feature analysis, which sheds some light on the features that are characteristic for deceptive text. You should not trust the devil, even if he tells the truth. – Thomas of Aquin (medieval philosopher) 
We explore how features based on syntactic dependency relations can be utilized to improve performance on opinion mining. Using a transformation of dependency relation triples, we convert them into “composite back-off features” that generalize better than the regular lexicalized dependency relation features. Experiments comparing our approach with several other approaches that generalize dependency features or ngrams demonstrate the utility of composite back-off features. 
With the aim to deal with sentiment-transfer problem, we proposed a novel approach, which integrates the sentiment orientations of documents into the graph-ranking algorithm. We apply the graph-ranking algorithm using the accurate labels of old-domain documents as well as the “pseudo” labels of new-domain documents. Experimental results show that proposed algorithm could improve the performance of baseline methods dramatically for sentiment transfer.  graph-ranking algorithm for sentiment transfer by integrating the sentiment orientations of the documents, which could be considered as a sentiment-transfer version of the graph-ranking algorithm. In this algorithm, we assign a score for every unlabelled document to denote its extent to “negative” or “positive”, then we iteratively calculate the score by making use of the accurate labels of old-domain data as well as the “pseudo” labels of new-domain data, and the final score for sentiment classification is achieved when the algorithm converges, so we can label the newdomain data based on these scores.  
Content-based approaches to detecting mobile spam to date have focused mainly on analyzing the topical aspect of a SMS message (what it is about) but not on the stylistic aspect (how it is written). In this paper, as a preliminary step, we investigate the utility of commonly used stylistic features based on shallow linguistic analysis for learning mobile spam ﬁlters. Experimental results show that the use of stylistic information is potentially effective for enhancing the performance of the mobile spam ﬁlters. 
This paper introduces the concepts of asking point and expected answer type as variations of the question focus. They are of particular importance for QA over semistructured data, as represented by Topic Maps, OWL or custom XML formats. We describe an approach to the identiﬁcation of the question focus from questions asked to a Question Answering system over Topic Maps by extracting the asking point and falling back to the expected answer type when necessary. We use known machine learning techniques for expected answer type extraction and we implement a novel approach to the asking point extraction. We also provide a mathematical model to predict the performance of the system. 
In this paper, we analyze the impact of different automatic annotation methods on the performance of supervised approaches to the complex question answering problem (deﬁned in the DUC-2007 main task). Huge amount of annotated or labeled data is a prerequisite for supervised training. The task of labeling can be accomplished either by humans or by computer programs. When humans are employed, the whole process becomes time consuming and expensive. So, in order to produce a large set of labeled data we prefer the automatic annotation strategy. We apply ﬁve different automatic annotation techniques to produce labeled data using ROUGE similarity measure, Basic Element (BE) overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel (ESSK). The representative supervised methods we use are Support Vector Machines (SVM), Conditional Random Fields (CRF), Hidden Markov Models (HMM), and Maximum Entropy (MaxEnt). Evaluation results are presented to show the impact. 
When a multi-lingual question-answering (QA) system provides an answer that has been incorrectly translated, it is very likely to be regarded as irrelevant. In this paper, we propose a novel method for correcting a deletion error that affects overall understanding of the sentence. Our post-editing technique uses information available at query time: examples drawn from related documents determined to be relevant to the query. Our results show that 4%-7% of MT sentences are missing the main verb and on average, 79% of the modified sentences are judged to be more comprehensible. The QA performance also benefits from the improved MT: 7% of irrelevant response sentences become relevant. 1. Introduction We are developing a multi-lingual questionanswering (QA) system that must provide relevant English answers for a given query, drawing pieces of the answer from translated foreign source. Relevance and translation quality are usually inseparable: an incorrectly translated sentence in the answer is very likely to be regarded as irrelevant even when the corresponding source language sentence is actually relevant. We use a phrase-based statistical machine translation system for the MT component and thus, for us, MT serves as a black box that produces the translated documents in our corpus; we cannot change the MT system itself. As MT is used in more and more multi-lingual applications, this situation will become quite common. We propose a novel method which uses redundant information available at questionanswering time to correct errors. We present a  
The implementation of collapsed Gibbs samplers for non-parametric Bayesian models is non-trivial, requiring considerable book-keeping. Goldwater et al. (2006a) presented an approximation which signiﬁcantly reduces the storage and computation overhead, but we show here that their formulation was incorrect and, even after correction, is grossly inaccurate. We present an alternative formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efﬁcient data structure which has a better space complexity than the naive approach. 
Efﬁcient processing of tera-scale text data is an important research topic. This paper proposes lossless compression of N gram language models based on LOUDS, a succinct data structure. LOUDS succinctly represents a trie with M nodes as a 2M + 1 bit string. We compress it further for the N -gram language model structure. We also use ‘variable length coding’ and ‘block-wise compression’ to compress values associated with nodes. Experimental results for three large-scale N -gram compression tasks achieved a signiﬁcant compression rate without any loss. 
We experiment with splitting words into their stem and sufﬁx components for modeling morphologically rich languages. We show that using a morphological analyzer and disambiguator results in a signiﬁcant perplexity reduction in Turkish. We present ﬂexible n-gram models, FlexGrams, which assume that the n−1 tokens that determine the probability of a given token can be chosen anywhere in the sentence rather than the preceding n − 1 positions. Our ﬁnal model achieves 27% perplexity reduction compared to the standard n-gram model. 
 Kneser-Ney (1995) smoothing and its variants are generally recognized as having the best perplexity of any known method for estimating N-gram language models. Kneser-Ney smoothing, however, requires nonstandard N-gram counts for the lowerorder models used to smooth the highestorder model. For some applications, this makes Kneser-Ney smoothing inappropriate or inconvenient. In this paper, we introduce a new smoothing method based on ordinary counts that outperforms all of the previous ordinary-count methods we have tested, with the new method eliminating most of the gap between Kneser-Ney and those methods.  
For many NLP tasks, including named entity tagging, semi-supervised learning has been proposed as a reasonable alternative to methods that require annotating large amounts of training data. In this paper, we address the problem of analyzing new data given a semi-supervised NE tagger trained on data from an earlier time period. We will show that updating the unlabeled data is sufﬁcient to maintain quality over time, and outperforms updating the labeled data. Furthermore, we will also show that augmenting the unlabeled data with older data in most cases does not result in better performance than simply using a smaller amount of current unlabeled data. 
We describe a set of techniques for Arabic cross-document coreference resolution. We compare a baseline system of exact mention string-matching to ones that include local mention context information as well as information from an existing machine translation system. It turns out that the machine translation-based technique outperforms the baseline, but local entity context similarity does not. This helps to point the way for future crossdocument coreference work in languages with few existing resources for the task. 
Searching for a person name in a Web Search Engine usually leads to a number of web pages that refer to several people sharing the same name. In this paper we study whether it is reasonable to assume that pages about the desired person can be ﬁltered by the user by adding query terms. Our results indicate that, although in most occasions there is a query reﬁnement that gives all and only those pages related to an individual, it is unlikely that the user is able to ﬁnd this expression a priori. 
The automatic extraction of relations between entities expressed in natural language text is an important problem for IR and text understanding. In this paper we show how different kernels for parse trees can be combined to improve the relation extraction quality. On a public benchmark dataset the combination of a kernel for phrase grammar parse trees and for dependency parse trees outperforms all known tree kernel approaches alone suggesting that both types of trees contain complementary information for relation extraction. 
Many events in news articles don’t include time arguments. This paper describes two methods, one based on rules and the other based on statistical learning, to predict the unknown time argument for an event by the propagation from its related events. The results are promising – the rule based approach was able to correctly predict 74% of the unknown event time arguments with 70% precision. 
In this paper, we present a new learning scenario, heterogeneous transfer learning, which improves learning performance when the data can be in different feature spaces and where no correspondence between data instances in these spaces is provided. In the past, we have classiﬁed Chinese text documents using English training data under the heterogeneous transfer learning framework. In this paper, we present image clustering as an example to illustrate how unsupervised learning can be improved by transferring knowledge from auxiliary heterogeneous data obtained from the social Web. Image clustering is useful for image sense disambiguation in query-based image search, but its quality is often low due to imagedata sparsity problem. We extend PLSA to help transfer the knowledge from social Web data, which have mixed feature representations. Experiments on image-object clustering and scene clustering tasks show that our approach in heterogeneous transfer learning based on the auxiliary data is indeed effective and promising. 
The vast majority of work on word senses has relied on predeﬁned sense inventories and an annotation schema where each word instance is tagged with the best ﬁtting sense. This paper examines the case for a graded notion of word meaning in two experiments, one which uses WordNet senses in a graded fashion, contrasted with the “winner takes all” annotation, and one which asks annotators to judge the similarity of two usages. We ﬁnd that the graded responses correlate with annotations from previous datasets, but sense assignments are used in a way that weakens the case for clear cut sense boundaries. The responses from both experiments correlate with the overlap of paraphrases from the English lexical substitution task which bodes well for the use of substitutes as a proxy for word sense. This paper also provides two novel datasets which can be used for evaluating computational systems. 
A number of studies have presented machine-learning approaches to semantic role labeling with availability of corpora such as FrameNet and PropBank. These corpora deﬁne the semantic roles of predicates for each frame independently. Thus, it is crucial for the machine-learning approach to generalize semantic roles across different frames, and to increase the size of training instances. This paper explores several criteria for generalizing semantic roles in FrameNet: role hierarchy, human-understandable descriptors of roles, semantic types of ﬁller phrases, and mappings from FrameNet roles to thematic roles of VerbNet. We also propose feature functions that naturally combine and weight these criteria, based on the training data. The experimental result of the role classiﬁcation shows 19.16% and 7.42% improvements in error reduction rate and macro-averaged F1 score, respectively. We also provide in-depth analyses of the proposed criteria. 
The task of Semantic Role Labeling (SRL) is often divided into two sub-tasks: verb argument identiﬁcation, and argument classiﬁcation. Current SRL algorithms show lower results on the identiﬁcation sub-task. Moreover, most SRL algorithms are supervised, relying on large amounts of manually created data. In this paper we present an unsupervised algorithm for identifying verb arguments, where the only type of annotation required is POS tagging. The algorithm makes use of a fully unsupervised syntactic parser, using its output in order to detect clauses and gather candidate argument collocation statistics. We evaluate our algorithm on PropBank10, achieving a precision of 56%, as opposed to 47% of a strong baseline. We also obtain an 8% increase in precision for a Spanish corpus. This is the ﬁrst paper that tackles unsupervised verb argument identiﬁcation without using manually encoded rules or extensive lexical or syntactic resources. 
We describe a semantic role labeling system that makes primary use of CCG-based features. Most previously developed systems are CFG-based and make extensive use of a treepath feature, which suffers from data sparsity due to its use of explicit tree conﬁgurations. CCG affords ways to augment treepathbased features to overcome these data sparsity issues. By adding features over CCG wordword dependencies and lexicalized verbal subcategorization frames (“supertags”), we can obtain an F-score that is substantially better than a previous CCG-based SRL system and competitive with the current state of the art. A manual error analysis reveals that parser errors account for many of the errors of our system. This analysis also suggests that simultaneous incremental parsing and semantic role labeling may lead to performance gains in both tasks. 
We address the issue of using heterogeneous treebanks for parsing by breaking it down into two sub-problems, converting grammar formalisms of the treebanks to the same one, and parsing on these homogeneous treebanks. First we propose to employ an iteratively trained target grammar parser to perform grammar formalism conversion, eliminating predeﬁned heuristic rules as required in previous methods. Then we provide two strategies to reﬁne conversion results, and adopt a corpus weighting technique for parsing on homogeneous treebanks. Results on the Penn Treebank show that our conversion method achieves 42% error reduction over the previous best result. Evaluation on the Penn Chinese Treebank indicates that a converted dependency treebank helps constituency parsing and the use of unlabeled data by self-training further increases parsing f-score to 85.2%, resulting in 6% error reduction over the previous best result. 
This paper proposes an approach to enhance dependency parsing in a language by using a translated treebank from another language. A simple statistical machine translation method, word-by-word decoding, where not a parallel corpus but a bilingual lexicon is necessary, is adopted for the treebank translation. Using an ensemble method, the key information extracted from word pairs with dependency relations in the translated text is effectively integrated into the parser for the target language. The proposed method is evaluated in English and Chinese treebanks. It is shown that a translated English treebank helps a Chinese parser obtain a state-ofthe-art result. 
Freer-word-order languages such as German exhibit linguistic phenomena that present unique challenges to traditional CFG parsing. Such phenomena produce discontinuous constituents, which are not naturally modelled by projective phrase structure trees. In this paper, we examine topological ﬁeld parsing, a shallow form of parsing which identiﬁes the major sections of a sentence in relation to the clausal main verb and the subordinating heads. We report the results of topological ﬁeld parsing of German using the unlexicalized, latent variable-based Berkeley parser (Petrov et al., 2006) Without any language- or model-dependent adaptation, we achieve state-of-the-art results on the Tu¨Ba-D/Z corpus, and a modiﬁed NEGRA corpus that has been automatically annotated with topological ﬁelds (Becker and Frank, 2002). We also perform a qualitative error analysis of the parser output, and discuss strategies to further improve the parsing results. 
We investigate the task of unsupervised constituency parsing from bilingual parallel corpora. Our goal is to use bilingual cues to learn improved parsing models for each language and to evaluate these models on held-out monolingual test data. We formulate a generative Bayesian model which seeks to explain the observed parallel data through a combination of bilingual and monolingual parameters. To this end, we adapt a formalism known as unordered tree alignment to our probabilistic setting. Using this formalism, our model loosely binds parallel trees while allowing language-speciﬁc syntactic structure. We perform inference under this model using Markov Chain Monte Carlo and dynamic programming. Applying this model to three parallel corpora (Korean-English, Urdu-English, and Chinese-English) we ﬁnd substantial performance gains over the CCM model, a strong monolingual baseline. On average, across a variety of testing scenarios, our model achieves an 8.8 absolute gain in F-measure. 1 
In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that deﬁnes the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.1 
A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difﬁculty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. 
In this paper, we propose a new Bayesian model for fully unsupervised word segmentation and an efﬁcient blocked Gibbs sampler combined with dynamic programming for inference. Our model is a nested hierarchical Pitman-Yor language model, where Pitman-Yor spelling model is embedded in the word model. We conﬁrmed that it signiﬁcantly outperforms previous reported results in both phonetic transcripts and standard datasets for Chinese and Japanese word segmentation. Our model is also considered as a way to construct an accurate word n-gram language model directly from characters of arbitrary language, without any “word” indications. 
Empirical studies on corpora involve making measurements of several quantities for the purpose of comparing corpora, creating language models or to make generalizations about speciﬁc linguistic phenomena in a language. Quantities such as average word length are stable across sample sizes and hence can be reliably estimated from large enough samples. However, quantities such as vocabulary size change with sample size. Thus measurements based on a given sample will need to be extrapolated to obtain their estimates over larger unseen samples. In this work, we propose a novel nonparametric estimator of vocabulary size. Our main result is to show the statistical consistency of the estimator – the ﬁrst of its kind in the literature. Finally, we compare our proposal with the state of the art estimators (both parametric and nonparametric) on large standard corpora; apart from showing the favorable performance of our estimator, we also see that the classical Good-Turing estimator consistently underestimates the vocabulary size. 
Correct stress placement is important in text-to-speech systems, in terms of both the overall accuracy and the naturalness of pronunciation. In this paper, we formulate stress assignment as a sequence prediction problem. We represent words as sequences of substrings, and use the substrings as features in a Support Vector Machine (SVM) ranker, which is trained to rank possible stress patterns. The ranking approach facilitates inclusion of arbitrary features over both the input sequence and output stress pattern. Our system advances the current state-of-the-art, predicting primary stress in English, German, and Dutch with up to 98% word accuracy on phonemes, and 96% on letters. The system is also highly accurate in predicting secondary stress. Finally, when applied in tandem with an L2P system, it substantially reduces the word error rate when predicting both phonemes and stress. 
Letter-to-phoneme (L2P) conversion is the process of producing a correct phoneme sequence for a word, given its letters. It is often desirable to reduce the quantity of training data — and hence human annotation — that is needed to train an L2P classiﬁer for a new language. In this paper, we confront the challenge of building an accurate L2P classiﬁer with a minimal amount of training data by combining several diverse techniques: context ordering, letter clustering, active learning, and phonetic L2P alignment. Experiments on six languages show up to 75% reduction in annotation effort. 
This paper studies transliteration alignment, its evaluation metrics and applications. We propose a new evaluation metric, alignment entropy, grounded on the information theory, to evaluate the alignment quality without the need for the gold standard reference and compare the metric with F -score. We study the use of phonological features and afﬁnity statistics for transliteration alignment at phoneme and grapheme levels. The experiments show that better alignment consistently leads to more accurate transliteration. In transliteration modeling application, we achieve a mean reciprocal rate (MRR) of 0.773 on Xinhua personal name corpus, a signiﬁcant improvement over other reported results on the same corpus. In transliteration validation application, we achieve 4.48% equal error rate on a large LDC corpus. 
We propose a method to automatically train lemmatization rules that handle prefix, infix and suffix changes to generate the lemma from the full form of a word. We explain how the lemmatization rules are created and how the lemmatizer works. We trained this lemmatizer on Danish, Dutch, English, German, Greek, Icelandic, Norwegian, Polish, Slovene and Swedish full form-lemma pairs respectively. We obtained significant improvements of 24 percent for Polish, 2.3 percent for Dutch, 1.5 percent for English, 1.2 percent for German and 1.0 percent for Swedish compared to plain suffix lemmatization using a suffix-only lemmatizer. Icelandic deteriorated with 1.9 percent. We also made an observation regarding the number of produced lemmatization rules as a function of the number of training pairs. 
This paper revisits the pivot language approach for machine translation. First, we investigate three different methods for pivot translation. Then we employ a hybrid method combining RBMT and SMT systems to ﬁll up the data gap for pivot translation, where the sourcepivot and pivot-target corpora are independent. Experimental results on spoken language translation show that this hybrid method signiﬁcantly improves the translation quality, which outperforms the method using a source-target corpus of the same size. In addition, we propose a system combination approach to select better translations from those produced by various pivot translation methods. This method regards system combination as a translation evaluation problem and formalizes it with a regression learning model. Experimental results indicate that our method achieves consistent and signiﬁcant improvement over individual translation outputs. 
Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems. The algorithms were originally developed to work with N -best lists of translations, and recently extended to lattices that encode many more hypotheses than typical N -best lists. We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars. These algorithms are more efﬁcient than the lattice-based versions presented earlier. We show how MERT can be employed to optimize parameters for MBR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs. 
This paper proposes a forest-based tree sequence to string translation model for syntaxbased statistical machine translation, which automatically learns tree sequence to string translation rules from word-aligned sourceside-parsed bilingual texts. The proposed model leverages on the strengths of both tree sequence-based and forest-based translation models. Therefore, it can not only utilize forest structure that compactly encodes exponential number of parse trees but also capture nonsyntactic translation equivalences with linguistically structured information through tree sequence. This makes our model potentially more robust to parse errors and structure divergence. Experimental results on the NIST MT-2003 Chinese-English translation task show that our method statistically significantly outperforms the four baseline systems. 
Statistical machine translation (SMT) models require bilingual corpora for training, and these corpora are often multilingual with parallel text in multiple languages simultaneously. We introduce an active learning task of adding a new language to an existing multilingual set of parallel text and constructing high quality MT systems, from each language in the collection into this new target language. We show that adding a new language using active learning to the EuroParl corpus provides a signiﬁcant improvement compared to a random sentence selection baseline. We also provide new highly effective sentence selection methods that improve AL for phrase-based SMT in the multilingual and single language pair setting. 
This paper presents DEPEVAL(summ), a dependency-based metric for automatic evaluation of summaries. Using a reranking parser and a Lexical-Functional Grammar (LFG) annotation, we produce a set of dependency triples for each summary. The dependency set for each candidate summary is then automatically compared against dependencies generated from model summaries. We examine a number of variations of the method, including the addition of WordNet, partial matching, or removing relation labels from the dependencies. In a test on TAC 2008 and DUC 2007 data, DEPEVAL(summ) achieves comparable or higher correlations with human judgments than the popular evaluation metrics ROUGE and Basic Elements (BE). 
Wikipedia provides a wealth of knowledge, where the ﬁrst sentence, infobox (and relevant sentences), and even the entire document of a wiki article could be considered as diverse versions of summaries (deﬁnitions) of the target topic. We explore how to generate a series of summaries with various lengths based on them. To obtain more reliable associations between sentences, we introduce wiki concepts according to the internal links in Wikipedia. In addition, we develop an extended document concept lattice model to combine wiki concepts and non-textual features such as the outline and infobox. The model can concatenate representative sentences from non-overlapping salient local topics for summary generation. We test our model based on our annotated wiki articles which topics come from TREC-QA 2004-2006 evaluations. The results show that the model is effective in summarization and deﬁnition QA. 
In this paper, we investigate an approach for creating a comprehensive textual overview of a subject composed of information drawn from the Internet. We use the high-level structure of human-authored texts to automatically induce a domainspeciﬁc template for the topic structure of a new overview. The algorithmic innovation of our work is a method to learn topicspeciﬁc extractors for content selection jointly for the entire template. We augment the standard perceptron algorithm with a global integer linear programming formulation to optimize both local ﬁt of information into each topic and global coherence across the entire overview. The results of our evaluation conﬁrm the beneﬁts of incorporating structural information into the content selection process. 
Computational story telling has sparked great interest in artiﬁcial intelligence, partly because of its relevance to educational and gaming applications. Traditionally, story generators rely on a large repository of background knowledge containing information about the story plot and its characters. This information is detailed and usually hand crafted. In this paper we propose a data-driven approach for generating short children’s stories that does not require extensive manual involvement. We create an end-to-end system that realizes the various components of the generation pipeline stochastically. Our system follows a generate-and-and-rank approach where the space of multiple candidate stories is pruned by considering whether they are plausible, interesting, and coherent. 
This paper presents an unsupervised opinion analysis method for debate-side classiﬁcation, i.e., recognizing which stance a person is taking in an online debate. In order to handle the complexities of this genre, we mine the web to learn associations that are indicative of opinion stances in debates. We combine this knowledge with discourse information, and formulate the debate side classiﬁcation task as an Integer Linear Programming problem. Our results show that our method is substantially better than challenging baseline methods. 
The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem. We propose a cotraining approach to making use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers. 
Sentiment classiﬁcation refers to the task of automatically identifying whether a given piece of text expresses positive or negative opinion towards a subject at hand. The proliferation of user-generated web content such as blogs, discussion forums and online review sites has made it possible to perform large-scale mining of public opinion. Sentiment modeling is thus becoming a critical component of market intelligence and social media technologies that aim to tap into the collective wisdom of crowds. In this paper, we consider the problem of learning high-quality sentiment models with minimal manual supervision. We propose a novel approach to learn from lexical prior knowledge in the form of domain-independent sentimentladen terms, in conjunction with domaindependent unlabeled data and a few labeled documents. Our model is based on a constrained non-negative tri-factorization of the term-document matrix which can be implemented using simple update rules. Extensive experimental studies demonstrate the effectiveness of our approach on a variety of real-world sentiment prediction tasks. 
This paper describes an approach to utilizing term weights for sentiment analysis tasks and shows how various term weighting schemes improve the performance of sentiment analysis systems. Previously, sentiment analysis was mostly studied under data-driven and lexicon-based frameworks. Such work generally exploits textual features for fact-based analysis tasks or lexical indicators from a sentiment lexicon. We propose to model term weighting into a sentiment analysis system utilizing collection statistics, contextual and topicrelated characteristics as well as opinionrelated properties. Experiments carried out on various datasets show that our approach effectively improves previous methods. 
 Can we automatically compose a large set of Wiktionaries and translation dictionaries to yield a massive, multilingual dictionary whose coverage is substantially greater than that of any of its constituent dictionaries? The composition of multiple translation dictionaries leads to a transitive inference problem: if word A translates to word B which in turn translates to word C, what is the probability that C is a translation of A? The paper introduces a novel algorithm that solves this problem for 10,000,000 words in more than 1,000 languages. The algorithm yields PANDICTIONARY, a novel multilingual dictionary. PANDICTIONARY contains more than four times as many translations than in the largest Wiktionary at precision 0.90 and over 200,000,000 pairwise translations in over 200,000 language pairs at precision 0.8. 
This paper presents a novel metric-based framework for the task of automatic taxonomy induction. The framework incrementally clusters terms based on ontology metric, a score indicating semantic distance; and transforms the task into a multi-criteria optimization based on minimization of taxonomy structures and modeling of term abstractness. It combines the strengths of both lexico-syntactic patterns and clustering through incorporating heterogeneous features. The flexible design of the framework allows a further study on which features are the best for the task under various conditions. The experiments not only show that our system achieves higher F1-measure than other state-of-the-art systems, but also reveal the interaction between features and various types of relations, as well as the interaction between features and term abstractness. 
It is usually assumed that the kind of noise existing in annotated data is random classiﬁcation noise. Yet there is evidence that differences between annotators are not always random attention slips but could result from different biases towards the classiﬁcation categories, at least for the harder-to-decide cases. Under an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations. We show that these are relatively unproblematic for an algorithm operating under the 0-1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time. 
Semantic role labels are the representation of the grammatically relevant aspects of a sentence meaning. Capturing the nature and the number of semantic roles in a sentence is therefore fundamental to correctly describing the interface between grammar and meaning. In this paper, we compare two annotation schemes, PropBank and VerbNet, in a task-independent, general way, analysing how well they fare in capturing the linguistic generalisations that are known to hold for semantic role labels, and consequently how well they grammaticalise aspects of meaning. We show that VerbNet is more verb-speciﬁc and better able to generalise to new semantic role instances, while PropBank better captures some of the structural constraints among roles. We conclude that these two resources should be used together, as they are complementary. 
Existing evaluation metrics for machine translation lack crucial robustness: their correlations with human quality judgments vary considerably across languages and genres. We believe that the main reason is their inability to properly capture meaning: A good translation candidate means the same thing as the reference translation, regardless of formulation. We propose a metric that evaluates MT output based on a rich set of features motivated by textual entailment, such as lexical-semantic (in-)compatibility and argument structure overlap. We compare this metric against a combination metric of four state-of-theart scores (BLEU, NIST, TER, and METEOR) in two different settings. The combination metric outperforms the individual scores, but is bested by the entailment-based metric. Combining the entailment and traditional features yields further improvements. 
A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested. However, n-gram based metrics are still today the dominant approach. The main reason is that the advantages of employing deeper linguistic information have not been clariﬁed yet. In this work, we propose a novel approach for meta-evaluation of MT evaluation metrics, since correlation cofﬁcient against human judges do not reveal details about the advantages and disadvantages of particular metrics. We then use this approach to investigate the beneﬁts of introducing linguistic features into evaluation metrics. Overall, our experiments show that (i) both lexical and linguistic metrics present complementary advantages and (ii) combining both kinds of metrics yields the most robust metaevaluation performance. 
Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it dif cult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotationdriven syntactic modeling, we address this problem by observing the in uential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields signi cant gains in performance. 
 An efﬁcient decoding algorithm is a crucial element of any statistical machine translation system. Some researchers have noted certain similarities between SMT decoding and the famous Traveling Salesman Problem; in particular (Knight, 1999) has shown that any TSP instance can be mapped to a sub-case of a word-based SMT model, demonstrating NP-hardness of the decoding task. In this paper, we focus on the reverse mapping, showing that any phrase-based SMT decoding problem can be directly reformulated as a TSP. The transformation is very natural, deepens our understanding of the decoding problem, and allows direct use of any of the powerful existing TSP solvers for SMT decoding. We test our approach on three datasets, and compare a TSP-based decoder to the popular beam-search algorithm. In all cases, our method provides competitive or better performance.  
We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efﬁcient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. 
We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input. Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from ﬁve languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. 
In this paper, we propose a novel method for semi-supervised learning of nonprojective log-linear dependency parsers using directly expressed linguistic prior knowledge (e.g. a noun’s parent is often a verb). Model parameters are estimated using a generalized expectation (GE) objective function that penalizes the mismatch between model predictions and linguistic expectation constraints. In a comparison with two prominent “unsupervised” learning methods that require indirect biasing toward the correct syntactic structure, we show that GE can attain better accuracy with as few as 20 intuitive constraints. We also present positive experimental results on longer sentences in multiple languages. 
Pure statistical parsing systems achieves high in-domain accuracy but performs poorly out-domain. In this paper, we propose two different approaches to produce syntactic dependency structures using a large-scale hand-crafted HPSG grammar. The dependency backbone of an HPSG analysis is used to provide general linguistic insights which, when combined with state-of-the-art statistical dependency parsing models, achieves performance improvements on out-domain tests.† 
In this paper, we propose a novel system for translating organization names from Chinese to English with the assistance of web resources. Firstly, we adopt a chunkingbased segmentation method to improve the segmentation of Chinese organization names which is plagued by the OOV problem. Then a heuristic query construction method is employed to construct an efficient query which can be used to search the bilingual Web pages containing translation equivalents. Finally, we align the Chinese organization name with English sentences using the asymmetric alignment method to find the best English fragment as the translation equivalent. The experimental results show that the proposed method outperforms the baseline statistical machine translation system by 30.42%.  
Iterative bootstrapping algorithms are typically compared using a single set of handpicked seeds. However, we demonstrate that performance varies greatly depending on these seeds, and favourable seeds for one algorithm can perform very poorly with others, making comparisons unreliable. We exploit this wide variation with bagging, sampling from automatically extracted seeds to reduce semantic drift. However, semantic drift still occurs in later iterations. We propose an integrated distributional similarity ﬁlter to identify and censor potential semantic drifts, ensuring over 10% higher precision when extracting large semantic lexicons. 
Recent work on temporal relation identiﬁcation has focused on three types of relations between events: temporal relations between an event and a time expression, between a pair of events and between an event and the document creation time. These types of relations have mostly been identiﬁed in isolation by event pairwise comparison. However, this approach neglects logical constraints between temporal relations of different types that we believe to be helpful. We therefore propose a Markov Logic model that jointly identiﬁes relations of all three relation types simultaneously. By evaluating our model on the TempEval data we show that this approach leads to about 2% higher accuracy for all three types of relations —and to the best results for the task when compared to those of other machine learning based systems. 
Cross-lingual tasks are especially difficult due to the compounding effect of errors in language processing and errors in machine translation (MT). In this paper, we present an error analysis of a new cross-lingual task: the 5W task, a sentence-level understanding task which seeks to return the English 5W's (Who, What, When, Where and Why) corresponding to a Chinese sentence. We analyze systems that we developed, identifying specific problems in language processing and MT that cause errors. The best cross-lingual 5W system was still 19% worse than the best monolingual 5W system, which shows that MT significantly degrades sentence-level understanding. Neither source-language nor targetlanguage analysis was able to circumvent problems in MT, although each approach had advantages relative to the other. A detailed error analysis across multiple systems suggests directions for future research on the problem. 
This paper proposes a novel framework called bilingual co-training for a largescale, accurate acquisition method for monolingual semantic knowledge. In this framework, we combine the independent processes of monolingual semanticknowledge acquisition for two languages using bilingual resources to boost performance. We apply this framework to largescale hyponymy-relation acquisition from Wikipedia. Experimental results show that our approach improved the F-measure by 3.6–10.3%. We also show that bilingual co-training enables us to build classiﬁers for two languages in tandem with the same combined amount of data as required for training a single classiﬁer in isolation while achieving superior performance. 
An important and well-studied problem is the production of semantic lexicons from a large corpus. In this paper, we present a system named ASIA (Automatic Set Instance Acquirer), which takes in the name of a semantic class as input (e.g., “car makers”) and automatically outputs its instances (e.g., “ford”, “nissan”, “toyota”). ASIA is based on recent advances in webbased set expansion - the problem of ﬁnding all instances of a set given a small number of “seed” instances. This approach effectively exploits web resources and can be easily adapted to different languages. In brief, we use languagedependent hyponym patterns to ﬁnd a noisy set of initial seeds, and then use a state-of-the-art language-independent set expansion system to expand these seeds. The proposed approach matches or outperforms prior systems on several Englishlanguage benchmarks. It also shows excellent performance on three dozen additional benchmark problems from English, Chinese and Japanese, thus demonstrating language-independence. 
This paper describes the extraction from Wikipedia of lexical reference rules, identifying references to term meanings triggered by other terms. We present extraction methods geared to cover the broad range of the lexical reference relation and analyze them extensively. Most extraction methods yield high precision levels, and our rule-base is shown to perform better than other automatically constructed baselines in a couple of lexical expansion and matching tasks. Our rule-base yields comparable performance to WordNet while providing largely complementary information. 
We present a novel approach to deciding whether two sentences hold a paraphrase relationship. We employ a generative model that generates a paraphrase of a given sentence, and we use probabilistic inference to reason about whether two sentences share the paraphrase relationship. The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006). Furthermore, using a product of experts (Hinton, 2002), we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 
Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion. This learning framework is attractive because it often requires much less training time in practice than batch training algorithms. However, L1-regularization, which is becoming popular in natural language processing because of its ability to produce compact models, cannot be efﬁciently applied in SGD training, due to the large dimensions of feature vectors and the ﬂuctuations of approximate gradients. We present a simple method to solve these problems by penalizing the weights according to cumulative values for L1 penalty. We evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging. Experimental results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 
We present a global joint model for lemmatization and part-of-speech prediction. Using only morphological lexicons and unlabeled data, we learn a partiallysupervised part-of-speech tagger and a lemmatizer which are combined using features on a dynamically linked dependency structure of words. We evaluate our model on English, Bulgarian, Czech, and Slovene, and demonstrate substantial improvements over both a direct transduction approach to lemmatization and a pipelined approach, which predicts part-of-speech tags before lemmatization. 
Supervised sequence-labeling systems in natural language processing often suffer from data sparsity because they use word types as features in their prediction tasks. Consequently, they have difﬁculty estimating parameters for types which appear in the test set, but seldom (or never) appear in the training set. We demonstrate that distributional representations of word types, trained on unannotated text, can be used to improve performance on rare words. We incorporate aspects of these representations into the feature space of our sequence-labeling systems. In an experiment on a standard chunking dataset, our best technique improves a chunker from 0.76 F1 to 0.86 F1 on chunks beginning with rare words. On the same dataset, it improves our part-of-speech tagger from 74% to 80% accuracy on rare words. Furthermore, our system improves signiﬁcantly over a baseline system when applied to text from a different domain, and it reduces the sample complexity of sequence labeling. 
We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings. 
In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus. We describe an efﬁcient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature. 
Manually annotated corpora are valuable but scarce resources, yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corpora with different and incompatible annotation guidelines or standards. This seems to be a great waste of human efforts, and it would be nice to automatically adapt one annotation standard to another. We present a simple yet effective strategy that transfers knowledge from a differently annotated corpus to the corpus with desired annotation. We test the efﬁcacy of this method in the context of Chinese word segmentation and part-of-speech tagging, where no segmentation and POS tagging standards are widely accepted due to the lack of morphology in Chinese. Experiments show that adaptation from the much larger People’s Daily corpus to the smaller but more popular Penn Chinese Treebank results in signiﬁcant improvements in both segmentation and tagging accuracies (with error reductions of 30.2% and 14%, respectively), which in turn helps improve Chinese parsing accuracy. 
To support the real-time understanding of spoken monologue such as lectures and commentaries, the development of a captioning system is required. In monologues, since a sentence tends to be long, each sentence is often displayed in multi lines on one screen, it is necessary to insert linefeeds into a text so that the text becomes easy to read. This paper proposes a technique for inserting linefeeds into a Japanese spoken monologue text as an elemental technique to generate the readable captions. Our method appropriately inserts linefeeds into a sentence by machine learning, based on the information such as dependencies, clause boundaries, pauses and line length. An experiment using Japanese speech data has shown the effectiveness of our technique. 
Most of previous approaches to automatic prosodic event detection are based on supervised learning, relying on the availability of a corpus that is annotated with the prosodic labels of interest in order to train the classiﬁcation models. However, creating such resources is an expensive and time-consuming task. In this paper, we exploit semi-supervised learning with the co-training algorithm for automatic detection of coarse level representation of prosodic events such as pitch accents, intonational phrase boundaries, and break indices. We propose a conﬁdence-based method to assign labels to unlabeled data and demonstrate improved results using this method compared to the widely used agreement-based method. In addition, we examine various informative sample selection methods. In our experiments on the Boston University radio news corpus, using only a small amount of the labeled data as the initial training set, our proposed labeling method combined with most conﬁdence sample selection can effectively use unlabeled data to improve performance and ﬁnally reach performance closer to that of the supervised method using all the training data. 
This paper presents a model for summarizing multiple untranscribed spoken documents. Without assuming the availability of transcripts, the model modiﬁes a recently proposed unsupervised algorithm to detect re-occurring acoustic patterns in speech and uses them to estimate similarities between utterances, which are in turn used to identify salient utterances and remove redundancies. This model is of interest due to its independence from spoken language transcription, an error-prone and resource-intensive process, its ability to integrate multiple sources of information on the same topic, and its novel use of acoustic patterns that extends previous work on low-level prosodic feature detection. We compare the performance of this model with that achieved using manual and automatic transcripts, and ﬁnd that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 33–37% range without actually having to do the ASR, plus it better handles utterances with out-ofvocabulary words. 
Current tree-to-tree models suffer from parsing errors as they usually use only 1best parses for rule extraction and decoding. We instead propose a forest-based tree-to-tree model that uses packed forests. The model is based on a probabilistic synchronous tree substitution grammar (STSG), which can be learned from aligned forest pairs automatically. The decoder ﬁnds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 
The minimum Bayes risk (MBR) decoding objective improves BLEU scores for machine translation output relative to the standard Viterbi objective of maximizing model score. However, MBR targeting BLEU is prohibitively slow to optimize over k-best lists for large k. In this paper, we introduce and analyze an alternative to MBR that is equally effective at improving performance, yet is asymptotically faster — running 80 times faster than MBR in experiments with 1000-best lists. Furthermore, our fast decoding procedure can select output sentences based on distributions over entire forests of translations, in addition to k-best lists. We evaluate our procedure on translation forests from two large-scale, state-of-the-art hierarchical machine translation systems. Our forest-based decoding objective consistently outperforms k-best list MBR, giving improvements of up to 1.0 BLEU. 
Current SMT systems usually decode with single translation models and cannot beneﬁt from the strengths of other models in decoding phase. We instead propose joint decoding, a method that combines multiple translation models in one decoder. Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually. Therefore, one model can share translations and even derivations with other models. Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding. 
This paper presents collaborative decoding (co-decoding), a new method to improve machine translation accuracy by leveraging translation consensus between multiple machine translation decoders. Different from system combination and MBR decoding, which postprocess the n-best lists or word lattice of machine translation decoders, in our method multiple machine translation decoders collaborate by exchanging partial translation results. Using an iterative decoding approach, n-gram agreement statistics between translations of multiple decoders are employed to re-rank both full and partial hypothesis explored in decoding. Experimental results on data sets for NIST Chinese-to-English machine translation task show that the co-decoding method can bring significant improvements to all baseline decoders, and the outputs from co-decoding can be used to further improve the result of system combination. 
Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations). In principle, the goodness of a string is measured by the total probability of its many derivations. However, ﬁnding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. Our particular variational distributions are parameterized as n-gram models. We also analytically show that interpolating these n-gram models for different n is similar to minimumrisk decoding for BLEU (Tromble et al., 2008). Experiments show that our approach improves the state of the art. 
We describe an unsupervised system for learning narrative schemas, coherent sequences or sets of events (arrested(POLICE,SUSPECT), convicted( JUDGE, SUSPECT)) whose arguments are ﬁlled with participant semantic roles deﬁned over words (JUDGE = {judge, jury, court}, POLICE = {police, agent, authorities}). Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predeﬁned classes of events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-speciﬁc semantic roles. 
We present a new approach to learning a semantic parser (a system that maps natural language sentences into logical form). Unlike previous methods, it exploits an existing syntactic parser to produce disambiguated parse trees that drive the compositional semantic interpretation. The resulting system produces improved results on standard corpora on natural language interfaces for database querying and simulated robot control. 
This paper presents a set of Bayesian methods for automatically extending the WORDNET ontology with new concepts and annotating existing concepts with generic property ﬁelds, or attributes. We base our approach on Latent Dirichlet Allocation and evaluate along two dimensions: (1) the precision of the ranked lists of attributes, and (2) the quality of the attribute assignments to WORDNET concepts. In all cases we ﬁnd that the principled LDA-based approaches outperform previously proposed heuristic methods, greatly improving the speciﬁcity of attributes at each concept. 
This paper describes our system for generating Chinese aspect expressions. In the system, the semantics of different aspects is characterized by specific temporal and conceptual features. The semantic applicability conditions of each individual aspect are theoretically represented by an aspect selection function (ASF). The generation is realized by evaluating implemented inquiries which formally define the ASFs, traversing the grammatical network, and making aspect selections. 
Recent advances in functional Magnetic Resonance Imaging (fMRI) offer a significant new approach to studying semantic representations in humans by making it possible to directly observe brain activity while people comprehend words and sentences. In this study, we investigate how humans comprehend adjective-noun phrases (e.g. strong dog) while their neural activity is recorded. Classification analysis shows that the distributed pattern of neural activity contains sufficient signal to decode differences among phrases. Furthermore, vector-based semantic models can explain a significant portion of systematic variance in the observed neural activity. Multiplicative composition models of the two-word phrase outperform additive models, consistent with the assumption that people use adjectives to modify the meaning of the noun, rather than conjoining the meaning of the adjective and noun. 
 A typical machine learning-based approach  This paper explores how to apply the notion of caching introduced by Walker (1996) to the task of zero-anaphora resolution. We propose a machine learning-based implementation of a cache model to reduce the computational cost of identifying an antecedent. Our empirical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it. 
We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task deﬁnitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora. First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection. We measure the impact of each subproblem on coreference resolution and conﬁrm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task. Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets. 
This paper introduces a new algorithm to parse discourse within the framework of Rhetorical Structure Theory (RST). Our method is based on recent advances in the ﬁeld of statistical machine learning (multivariate capabilities of Support Vector Machines) and a rich feature space. RST offers a formal framework for hierarchical text organization with strong applications in discourse analysis and text generation. We demonstrate automated annotation of a text with RST hierarchically organised relations, with results comparable to those achieved by specially trained human annotators. Using a rich set of shallow lexical, syntactic and structural features from the input text, our parser achieves, in linear time, 73.9% of professional annotators’ human agreement F-score. The parser is 5% to 12% more accurate than current state-of-the-art parsers. 
Articles in the Penn TreeBank were identiﬁed as being reviews, summaries, letters to the editor, news reportage, corrections, wit and short verse, or quarterly proﬁt reports. All but the latter three were then characterised in terms of features manually annotated in the Penn Discourse TreeBank — discourse connectives and their senses. Summaries turned out to display very different discourse features than the other three genres. Letters also appeared to have some different features. The two main ﬁndings involve (1) differences between genres in the senses associated with intra-sentential discourse connectives, inter-sentential discourse connectives and inter-sentential discourse relations that are not lexically marked; and (2) differences within all four genres between the senses of discourse relations not lexically marked and those that are marked. The ﬁrst ﬁnding means that genre should be made a factor in automated sense labelling of non-lexically marked discourse relations. The second means that lexically marked relations provide a poor model for automated sense labelling of relations that are not lexically marked. 
We present a series of experiments on automatically identifying the sense of implicit discourse relations, i.e. relations that are not marked with a discourse connective such as “but” or “because”. We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses. We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modiﬁcations. Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency. 
Supervised polarity classiﬁcation systems are typically domain-speciﬁc. Building these systems involves the expensive process of annotating a large amount of data for each domain. A potential solution to this corpus annotation bottleneck is to build unsupervised polarity classiﬁcation systems. However, unsupervised learning of polarity is difﬁcult, owing in part to the prevalence of sentimentally ambiguous reviews, where reviewers discuss both the positive and negative aspects of a product. To address this problem, we propose a semi-supervised approach to sentiment classiﬁcation where we ﬁrst mine the unambiguous reviews using spectral techniques and then exploit them to classify the ambiguous reviews via a novel combination of active learning, transductive learning, and ensemble learning. 
This paper presents and evaluates several original techniques for the latent classiﬁcation of biographic attributes such as gender, age and native language, in diverse genres (conversation transcripts, email) and languages (Arabic, English). First, we present a novel partner-sensitive model for extracting biographic attributes in conversations, given the differences in lexical usage and discourse style such as observed between same-gender and mixedgender conversations. Then, we explore a rich variety of novel sociolinguistic and discourse-based features, including mean utterance length, passive/active usage, percentage domination of the conversation, speaking rate and ﬁller word usage. Cumulatively up to 20% error reduction is achieved relative to the standard Boulis and Ostendorf (2005) algorithm for classifying individual conversations on Switchboard, and accuracy for gender detection on the Switchboard corpus (aggregate) and Gulf Arabic corpus exceeds 95%. 
We present a graph-based semi-supervised learning for the question-answering (QA) task for ranking candidate sentences. Using textual entailment analysis, we obtain entailment scores between a natural language question posed by the user and the candidate sentences returned from search engine. The textual entailment between two sentences is assessed via features representing high-level attributes of the entailment problem such as sentence structure matching, question-type named-entity matching based on a question-classiﬁer, etc. We implement a semi-supervised learning (SSL) approach to demonstrate that utilization of more unlabeled data points can improve the answer-ranking task of QA. We create a graph for labeled and unlabeled data using match-scores of textual entailment features as similarity weights between data points. We apply a summarization method on the graph to make the computations feasible on large datasets. With a new representation of graph-based SSL on QA datasets using only a handful of features, and under limited amounts of labeled data, we show improvement in generalization performance over state-of-the-art QA models. 
Monolingual translation probabilities have recently been introduced in retrieval models to solve the lexical gap problem. They can be obtained by training statistical translation models on parallel monolingual corpora, such as question-answer pairs, where answers act as the “source” language and questions as the “target” language. In this paper, we propose to use as a parallel training dataset the deﬁnitions and glosses provided for the same term by different lexical semantic resources. We compare monolingual translation models built from lexical semantic resources with two other kinds of datasets: manually-tagged question reformulations and question-answer pairs. We also show that the monolingual translation probabilities obtained (i) are comparable to traditional semantic relatedness measures and (ii) signiﬁcantly improve the results over the query likelihood and the vector-space model for answer ﬁnding. 
Opinion Question Answering (Opinion QA), which aims to ﬁnd the authors’ sentimental opinions on a speciﬁc target, is more challenging than traditional factbased question answering problems. To extract the opinion oriented answers, we need to consider both topic relevance and opinion sentiment issues. Current solutions to this problem are mostly ad-hoc combinations of question topic information and opinion information. In this paper, we propose an Opinion PageRank model and an Opinion HITS model to fully explore the information from different relations among questions and answers, answers and answers, and topics and opinions. By fully exploiting these relations, the experiment results show that our proposed algorithms outperform several state of the art baselines on benchmark data set. A gain of over 10% in F scores is achieved as compared to many other systems. 
Spontaneously produced speech text often includes disﬂuencies which make it difﬁcult to analyze underlying structure. Successful reconstruction of this text would transform these errorful utterances into ﬂuent strings and offer an alternate mechanism for analysis. Our investigation of naturally-occurring spontaneous speaker errors aligned to corrected text with manual semanticosyntactic analysis yields new insight into the syntactic and structural semantic differences between spoken and reconstructed language. 
While OOV is always a problem for most languages in ASR, in the Chinese case the problem can be avoided by utilizing character n-grams and moderate performances can be obtained. However, character ngram has its own limitation and proper addition of new words can increase the ASR performance. Here we propose a discriminative lexicon adaptation approach for improved character accuracy, which not only adds new words but also deletes some words from the current lexicon. Different from other lexicon adaptation approaches, we consider the acoustic features and make our lexicon adaptation criterion consistent with that in the decoding process. The proposed approach not only improves the ASR character accuracy but also signiﬁcantly enhances the performance of a characterbased spoken document retrieval system. 
We demonstrate that transformation-based learning can be used to correct noisy speech recognition transcripts in the lecture domain with an average word error rate reduction of 12.9%. Our method is distinguished from earlier related work by its robustness to small amounts of training data, and its resulting efﬁciency, in spite of its use of true word error rate computations as a rule scoring function. 
Efﬁciency is a prime concern in syntactic MT decoding, yet signiﬁcant developments in statistical parsing with respect to asymptotic efﬁciency haven’t yet been explored in MT. Recently, McDonald et al. (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence. They show that MST parsing is almost as accurate as cubic-time dependency parsing in the case of English, and that it is more accurate with free word order languages. This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores. Our results show that augmenting a state-ofthe-art phrase-based system with this dependency language model leads to signiﬁcant improvements in TER (0.92%) and BLEU (0.45%) scores on ﬁve NIST Chinese-English evaluation test sets. 
We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efﬁcient, allowing the model to be applied to larger translation corpora than previous approaches. 
This paper addresses the task of handling unknown terms in SMT. We propose using source-language monolingual models and resources to paraphrase the source text prior to translation. We further present a conceptual extension to prior work by allowing translations of entailed texts rather than paraphrases only. A method for performing this process efﬁciently is presented and applied to some 2500 sentences with unknown terms. Our experiments show that the proposed approach substantially increases the number of properly translated texts. 
We report in this paper our work on accurately generating case markers and sufﬁxes in English-to-Hindi SMT. Hindi is a relatively free word-order language, and makes use of a comparatively richer set of case markers and morphological sufﬁxes for correct meaning representation. From our experience of large-scale English-Hindi MT, we are convinced that ﬂuency and ﬁdelity in the Hindi output get an order of magnitude facelift if accurate case markers and sufﬁxes are produced. Now, the moot question is: what entity on the English side encodes the information contained in case markers and sufﬁxes on the Hindi side? Our studies of correspondences in the two languages show that case markers and sufﬁxes in Hindi are predominantly determined by the combination of sufﬁxes and semantic relations on the English side. We, therefore, augment the aligned corpus of the two languages, with the correspondence of English sufﬁxes and semantic relations with Hindi sufﬁxes and case markers. Our results on 400 test sentences, translated using an SMT system trained on around 13000 parallel sentences, show that sufﬁx + semantic relation → case marker/sufﬁx is a very useful translation factor, in the sense of making a signiﬁcant difference to output quality as indicated by subjective evaluation as well as BLEU scores. 
This paper describes log-linear models for a general-purpose sentence realizer based on dependency structures. Unlike traditional realizers using grammar rules, our method realizes sentences by linearizing dependency relations directly in two steps. First, the relative order between head and each dependent is determined by their dependency relation. Then the best linearizations compatible with the relative order are selected by log-linear models. The log-linear models incorporate three types of feature functions, including dependency relations, surface words and headwords. Our approach to sentence realization provides simplicity, efficiency and competitive accuracy. Trained on 8,975 dependency structures of a Chinese Dependency Treebank, the realizer achieves a BLEU score of 0.8874. 
We investigate the inﬂuence of information status (IS) on constituent order in German, and integrate our ﬁndings into a loglinear surface realisation ranking model. We show that the distribution of pairs of IS categories is strongly asymmetric. Moreover, each category is correlated with morphosyntactic features, which can be automatically detected. We build a loglinear model that incorporates these asymmetries for ranking German string realisations from input LFG F-structures. We show that it achieves a statistically significantly higher BLEU score than the baseline system without these features. 
Conventional sentence compression methods employ a syntactic parser to compress a sentence without changing its meaning. However, the reference compressions made by humans do not always retain the syntactic structures of the original sentences. Moreover, for the goal of ondemand sentence compression, the time spent in the parsing stage is not negligible. As an alternative to syntactic parsing, we propose a novel term weighting technique based on the positional information within the original sentence and a novel language model that combines statistics from the original sentence and a general corpus. Experiments that involve both human subjective evaluations and automatic evaluations show that our method outperforms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 
Paraphrase generation (PG) is important in plenty of NLP applications. However, the research of PG is far from enough. In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance. In our experiments, we use the proposed method to generate paraphrases for three different applications. The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases. 
We introduce cause identiﬁcation, a new problem involving classiﬁcation of incident reports in the aviation domain. Speciﬁcally, given a set of pre-deﬁned causes, a cause identiﬁcation system seeks to identify all and only those causes that can explain why the aviation incident described in a given report occurred. The difﬁculty of cause identiﬁcation stems in part from the fact that it is a multi-class, multilabel categorization task, and in part from the skewness of the class distributions and the scarcity of annotated reports. To improve the performance of a cause identiﬁcation system for the minority classes, we present a bootstrapping algorithm that automatically augments a training set by learning from a small amount of labeled data and a large amount of unlabeled data. Experimental results show that our algorithm yields a relative error reduction of 6.3% in F-measure for the minority classes in comparison to a baseline that learns solely from the labeled data. 
Short Messaging Service (SMS) is popularly used to provide information access to people on the move. This has resulted in the growth of SMS based Question Answering (QA) services. However automatically handling SMS questions poses signiﬁcant challenges due to the inherent noise in SMS questions. In this work we present an automatic FAQ-based question answering system for SMS users. We handle the noise in a SMS query by formulating the query similarity over FAQ questions as a combinatorial search problem. The search space consists of combinations of all possible dictionary variations of tokens in the noisy query. We present an efﬁcient search algorithm that does not require any training data or SMS normalization and can handle semantic variations in question formulation. We demonstrate the effectiveness of our approach on two reallife datasets. 
 We present a novel approach to parse web search queries for the purpose of automatic tagging of the queries. We will define a set of probabilistic context-free rules, which generates bags (i.e. multi-sets) of words. Using this new type of rule in combination with the traditional probabilistic phrase structure rules, we define a hybrid grammar, which treats each search query as a bag of chunks (i.e. phrases). A hybrid probabilistic parser is used to parse the queries. In order to take contextual information into account, a discriminative model is used on top of the parser to re-rank the n-best parse trees generated by the parser. Experiments show that our approach outperforms a basic model, which is based on Conditional Random Fields.  
Mining bilingual data (including bilingual sentences and terms1) from the Web can benefit many NLP applications, such as machine translation and cross language information retrieval. In this paper, based on the observation that bilingual data in many web pages appear collectively following similar patterns, an adaptive pattern-based bilingual data mining method is proposed. Specifically, given a web page, the method contains four steps: 1) preprocessing: parse the web page into a DOM tree and segment the inner text of each node into snippets; 2) seed mining: identify potential translation pairs (seeds) using a word based alignment model which takes both translation and transliteration into consideration; 3) pattern learning: learn generalized patterns with the identified seeds; 4) pattern based mining: extract all bilingual data in the page using the learned patterns. Our experiments on Chinese web pages produced more than 7.5 million pairs of bilingual sentences and more than 5 million pairs of bilingual terms, both with over 80% accuracy. 
We present a human-robot dialogue system that enables a robot to work together with a human user to build wooden construction toys. We then describe a study in which na¨ıve subjects interacted with this system under a range of conditions and then completed a user-satisfaction questionnaire. The results of this study provide a wide range of subjective and objective measures of the quality of the interactions. To assess which aspects of the interaction had the greatest impact on the users’ opinions of the system, we used a method based on the PARADISE evaluation framework (Walker et al., 1997) to derive a performance function from our data. The major contributors to user satisfaction were the number of repetition requests (which had a negative effect on satisfaction), the dialogue length, and the users’ recall of the system instructions (both of which contributed positively). 
User simulations are shown to be useful in spoken dialog system development. Since most current user simulations deploy probability models to mimic human user behaviors, how to set up user action probabilities in these models is a key problem to solve. One generally used approach is to estimate these probabilities from human user data. However, when building a new dialog system, usually no data or only a small amount of data is available. In this study, we compare estimating user probabilities from a small user data set versus handcrafting the probabilities. We discuss the pros and cons of both solutions for different dialog system development tasks. 
This paper shows the results of an experiment in dialogue segmentation. In this experiment, segmentation was done on a level of analysis similar to adjacency pairs. The method of annotation was somewhat novel: volunteers were invited to participate over the Web, and their responses were aggregated using a simple voting method. Though volunteers received a minimum of training, the aggregated responses of the group showed very high agreement with expert opinion. The group, as a unit, performed at the top of the list of annotators, and in many cases performed as well as or better than the best annotator. 
The present paper describes a robust approach for abbreviating terms. First, in order to incorporate non-local information into abbreviation generation tasks, we present both implicit and explicit solutions: the latent variable model, or alternatively, the label encoding approach with global information. Although the two approaches compete with one another, we demonstrate that these approaches are also complementary. By combining these two approaches, experiments revealed that the proposed abbreviation generator achieved the best results for both the Chinese and English languages. Moreover, we directly apply our generator to perform a very different task from tradition, the abbreviation recognition. Experiments revealed that the proposed model worked robustly, and outperformed ﬁve out of six state-of-the-art abbreviation recognizers. 
This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efﬁciency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which signiﬁcantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 
In this paper we present a conﬁdence measure for word alignment based on the posterior probability of alignment links. We introduce sentence alignment conﬁdence measure and alignment link conﬁdence measure. Based on these measures, we improve the alignment quality by selecting high conﬁdence sentence alignments and alignment links from multiple word alignments of the same sentence pair. Additionally, we remove low conﬁdence alignment links from the word alignment of a bilingual training corpus, which increases the alignment F-score, improves Chinese-English and Arabic-English translation quality and signiﬁcantly reduces the phrase translation table size. 
Recently confusion network decoding shows the best performance in combining outputs from multiple machine translation (MT) systems. However, overcoming different word orders presented in multiple MT systems during hypothesis alignment still remains the biggest challenge to confusion network-based MT system combination. In this paper, we compare four commonly used word alignment methods, namely GIZA++, TER, CLA and IHMM, for hypothesis alignment. Then we propose a method to build the confusion network from intersection word alignment, which utilizes both direct and inverse word alignment between the backbone and hypothesis to improve the reliability of hypothesis alignment. Experimental results demonstrate that the intersection word alignment yields consistent performance improvement for all four word alignment methods on both Chinese-to-English spoken and written language tasks. 
Inspired by the incremental TER alignment, we re-designed the Indirect HMM (IHMM) alignment, which is one of the best hypothesis alignment methods for conventional MT system combination, in an incremental manner. One crucial problem of incremental alignment is to align a hypothesis to a confusion network (CN). Our incremental IHMM alignment is implemented in three different ways: 1) treat CN spans as HMM states and deﬁne state transition as distortion over covered ngrams between two spans; 2) treat CN spans as HMM states and deﬁne state transition as distortion over words in component translations in the CN; and 3) use a consensus decoding algorithm over one hypothesis and multiple IHMMs, each of which corresponds to a component translation in the CN. All these three approaches of incremental alignment based on IHMM are shown to be superior to both incremental TER alignment and conventional IHMM alignment in the setting of the Chinese-to-English track of the 2008 NIST Open MT evaluation. 
A∗ parsing makes 1-best search efﬁcient by suppressing unlikely 1-best items. Existing kbest extraction methods can efﬁciently search for top derivations, but only after an exhaustive 1-best pass. We present a uniﬁed algorithm for k-best A∗ parsing which preserves the efﬁciency of k-best extraction while giving the speed-ups of A∗ methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A∗ parser. Empirically, optimal k-best lists can be extracted signiﬁcantly faster than with other approaches, over a range of grammar types. 
 We propose a hybrid approach to coordinate structure analysis that combines a simple grammar to ensure consistent global structure of coordinations in a sentence, and features based on sequence alignment to capture local symmetry of conjuncts. The weight of the alignmentbased features, which in turn determines the score of coordinate structures, is optimized by perceptron training on a given corpus. A bottom-up chart parsing algorithm efﬁciently ﬁnds the best scoring structure, taking both nested or nonoverlapping ﬂat coordinations into account. We demonstrate that our approach outperforms existing parsers in coordination scope detection on the Genia corpus. 
We consider the problem of learning context-dependent mappings from sentences to logical form. The training examples are sequences of sentences annotated with lambda-calculus meaning representations. We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7% accuracy. 
Linear context-free rewriting systems (LCFRSs) are grammar formalisms with the capability of modeling discontinuous constituents. Many applications use LCFRSs where the fan-out (a measure of the discontinuity of phrases) is not allowed to be greater than 2. We present an efﬁcient algorithm for transforming LCFRS with fan-out at most 2 into a binary form, whenever this is possible. This results in asymptotical run-time improvement for known parsing algorithms for this class. 
This paper investigates the class of TreeTuple MCTAG with Shared Nodes, TTMCTAG for short, an extension of Tree Adjoining Grammars that has been proposed for natural language processing, in particular for dealing with discontinuities and word order variation in languages such as German. It has been shown that the universal recognition problem for this formalism is NP-hard, but so far it was not known whether the class of languages generated by TT-MCTAG is included in PTIME. We provide a positive answer to this question, using a new characterization of TTMCTAG. 
Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we ﬁnd all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classiﬁer. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classiﬁer) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression. 
Creating labeled training data for relation extraction is expensive. In this paper, we study relation extraction in a special weakly-supervised setting when we have only a few seed instances of the target relation type we want to extract but we also have a large amount of labeled instances of other relation types. Observing that different relation types can share certain common structures, we propose to use a multi-task learning method coupled with human guidance to address this weakly-supervised relation extraction problem. The proposed framework models the commonality among different relation types through a shared weight vector, enables knowledge learned from the auxiliary relation types to be transferred to the target relation type, and allows easy control of the tradeoff between precision and recall. Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods. 
This paper presents an unsupervised relation extraction method for discovering and enhancing relations in which a speciﬁed concept in Wikipedia participates. Using respective characteristics of Wikipedia articles and Web corpus, we develop a clustering approach based on combinations of patterns: dependency patterns from dependency analysis of texts in Wikipedia, and surface patterns generated from highly redundant information related to the Web. Evaluations of the proposed approach on two different domains demonstrate the superiority of the pattern combination over existing approaches. Fundamentally, our method demonstrates how deep linguistic patterns contribute complementarily with Web surface patterns to the generation of various relations. 
We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers. To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification. Our results show that phrase clusters offer significant improvements over word clusters. Our NER system achieves the best current result on the widely used CoNLL benchmark. Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts. 
While Active Learning (AL) has already been shown to markedly reduce the annotation efforts for many sequence labeling tasks compared to random selection, AL remains unconcerned about the internal structure of the selected sequences (typically, sentences). We propose a semisupervised AL approach for sequence labeling where only highly uncertain subsequences are presented to human annotators, while all others in the selected sequences are automatically labeled. For the task of entity recognition, our experiments reveal that this approach reduces annotation efforts in terms of manually labeled tokens by up to 60 % compared to the standard, fully supervised AL scheme. 
The use of phrases in retrieval models has been proven to be helpful in the literature, but no particular research addresses the problem of discriminating phrases that are likely to degrade the retrieval performance from the ones that do not. In this paper, we present a retrieval framework that utilizes both words and phrases ﬂexibly, followed by a general learning-to-rank method for learning the potential contribution of a phrase in retrieval. We also present useful features that reﬂect the compositionality and discriminative power of a phrase and its constituent words for optimizing the weights of phrase use in phrase-based retrieval models. Experimental results on the TREC collections show that our proposed method is effective. 
User generated content is characterized by short, noisy documents, with many spelling errors and unexpected language usage. To bridge the vocabulary gap between the user’s information need and documents in a speciﬁc user generated content environment, the blogosphere, we apply a form of query expansion, i.e., adding and reweighing query terms. Since the blogosphere is noisy, query expansion on the collection itself is rarely effective but external, edited collections are more suitable. We propose a generative model for expanding queries using external collections in which dependencies between queries, documents, and expansion documents are explicitly modeled. Different instantiations of our model are discussed and make different (in)dependence assumptions. Results using two external collections (news and Wikipedia) show that external expansion for retrieval of user generated content is effective; besides, conditioning the external collection on the query is very beneﬁcial, and making candidate expansion terms dependent on just the document seems sufﬁcient. 
 We consider the language identiﬁcation problem for search engine queries. First, we propose a method to automatically generate a data set, which uses clickthrough logs of the Yahoo! Search Engine to derive the language of a query indirectly from the language of the documents clicked by the users. Next, we use this data set to train two decision tree classiﬁers; one that only uses linguistic features and is aimed for textual language identiﬁcation, and one that additionally uses a non-linguistic feature, and is geared towards the identiﬁcation of the language intended by the users of the search engine. Our results show that our method produces a highly reliable data set very efﬁciently, and our decision tree classiﬁer outperforms some of the best methods that have been proposed for the task of written language identiﬁcation on the domain of search engine queries. 
Web search quality can vary widely across languages, even for the same information need. We propose to exploit this variation in quality by learning a ranking function on bilingual queries: queries that appear in query logs for two languages but represent equivalent search interests. For a given bilingual query, along with corresponding monolingual query log and monolingual ranking, we generate a ranking on pairs of documents, one from each language. Then we learn a linear ranking function which exploits bilingual features on pairs of documents, as well as standard monolingual features. Finally, we show how to reconstruct monolingual ranking from a learned bilingual ranking. Using publicly available Chinese and English query logs, we demonstrate for both languages that our ranking technique exploiting bilingual data leads to signiﬁcant improvements over a state-of-the-art monolingual ranking algorithm. 
This study examines the influence of lexical tone on voice onset time (VOT) in Mandarin and Hakka spoken in Taiwan. The examination of VOT values for Mandarin and Hakka word-initial stops /p, t, k, ph, th, kh/ followed by three vowels /i, u, a/ in different lexical tones revealed that lexical tone has significant influence on the VOT values for stops. The results are important as they suggest that future studies should take the influence of lexical tone into account when studying VOT values and when designing wordlists for stops in tonal languages. In Mandarin, stops’ VOT values, from the longest to the shortest, are in MR, FR, HL, and HF tones. This sequence is the same as in Liu, Ng, Wan, Wang, and Zhang (2008). Later, however, it was found that it is very likely that the sequence results from the existence of non-words. In order to produce non-words correctly, participants tended to pronounce them at a slower speed, especially those in MR tone. Therefore, we further examined the data without non-words, in which no clear sequence was found. For Hakka, post-hoc tests (Scheffe) show that aspirated stops in entering tones, which are syllables ending with a stop, have significantly shorter VOT values than they have in other tones. Although the tonal effects on VOT values are not consistently found in different sets of data, probably due to a methodology problem, the possibility of tonal effect on VOT values could not be excluded. Tonal effect, thus, should be taken into consideration in designing word lists for VOT studies. Moreover, further studies should include both real words and non-words in separate sets of word lists to verify the current study results. Keywords: Voice Onset Time, Hakka Stops, Mandarin Stops, Tonal Effect  ∗ Department of Foreign Languages and Literature, National Cheng Kung University, 1 University Rd., Tainan, Taiwan. Telephone: (06)2757575 ext. 52231 E-mail: leemay@mail.ncku.edu.tw The author for correspondence is Li-mei Chen.  342  Jui-Feng Peng et al.  1. Introduction The aim of this paper is to explore whether lexical tones influence the VOT values for word-initial stops. This issue is important because VOT is considered one of the reliable acoustic features for differentiating consonant stops (Cho & Ladefoged, 1999; Gósy, 2001; Lisker & Abramson, 1964; Riney, Takagi, Ota, & Uchida, 2007; Rochet & Fei, 1991; Zheng & Li, 2005) and it has been applied recently to the study of the language production of patients with language deficits or disorders (Auzou, Ozsancak, Morris, Jan, Eustache, & Hannequin, 2000; Jäncke, 1994). Among the languages being investigated, some are tonal languages, i.e. Mandarin, Cantonese, and Taiwanese. In a tonal language, the duration of each lexical tone (which can change the meaning of a word) differs slightly. Consequently, it is possible that lexical tone will affect stops’ VOT to some extent; nevertheless, few studies have taken this factor into consideration when studying tonal languages. Therefore, the current study examines two tonal languages, Mandarin and Hakka spoken in Taiwan, to verify the effects of lexical tone. It is hoped that the results of the current study can establish the groundwork for future studies related to VOTs in tonal languages. If lexical tone does influence VOT, it should be considered when creating speech materials in future studies for tonal languages. 2. Literature Review 2.1 Voice Onset Time (VOT) Lisker and Abramson (1964) defined VOT as the temporal interval from the release of an initial stop to the onset of glottal pulsing for a following vowel. VOT has been considered a reliable phonetic cue for categorizing stop consonants (i.e., voiced versus voiceless or unaspirated versus aspirated) in various languages (Cho & Ladefoged, 1999; Gósy, 2001; Keating, Linker, & Huffman, 1983; Lisker & Abramson, 1964; Riney et al., 2007; Rochet & Fei, 1991; Zheng & Li, 2005). In addition, by comparing VOT values for stops produced by native and non-native speakers for specific languages, researchers have put forth specific suggestions for language learning and teaching (Liao, 2005; Riney & Takagi, 1999; Zheng & Li, 2005). Moreover, recently, researchers have studied production deficits of aphasia, apraxia, and stuttering patients by observing their VOT values for stops (Jäncke, 1994; Auzou et al., 2000; Tsen, 1994). 2.2 Factors Affecting Voice Onset Time When investigating stops, researchers found that the VOT values for stops varied in relation to the place of articulation. Lisker and Abramson (1964) demonstrated that, for both unaspirated and aspirated stops, velar stops have longer mean VOT values than alveolar and bilabial stops.  Tonal Effects on Voice Onset Time  343  In the languages examined, except for Tamil, Cantonese, and Eastern Armenian, alveolar stops tend to have longer mean VOT values than bilabial stops. Cho and Ladefoged’s (1999) study further revealed that velar stops have the longest mean VOT values, alveolar stops have intermediate mean VOTs, and bilabial stops have the shortest mean VOTs, with the exception of Navajo and Dahalo. The fact that VOT values get longer when the place of articulation moves from an anterior to a posterior position is confirmed in most languages (Cho & Ladefoged, 1999; Lisker & Abramson, 1964; Rosner, López- Bascuas, García-Albea, & Fahey, 2000; Zheng & Li, 2005); nevertheless, some exceptions exist, including Hungarian, Japanese, and Mandarin. As for the influence of vowel context, Lisker and Abramson (1967) reported that the vowels following the consonants do not have a significant effect on stops’ VOTs. Recently, however, other researchers have made opposing claims. Morris, McCrea, & Herring (2008), who studied English word-initial stops, claimed that stops preceding the high vowels /i/ and /u/ had longer VOTs than stops preceding the low vowel /a/. Similar results were revealed in Rochet and Fei (1991), Chao, Khattab, and Chen (2006), and Chen, Chao, and Peng (2007) studies of Mandarin and Gósy’s (2001) study of Hungarian. Furthermore, Gósy (2001) indicated that the higher the tongue position, the longer the VOTs for the preceding voiceless stops. Fant (1973), however, found the opposite to be true in a study of Swedish: the VOTs for aspirated stops preceding /a/ were longer than the VOTs for stops preceding /i/ and /u/. Fant’s results are extraordinary, as most studies report that stops preceding high vowels tend to have longer VOTs than stops preceding low vowels. Moreover, speaking rate might have influences on stops’ VOTs. Kessinger and Blumstein (1997), who investigated English, French, and Thai, claimed that the speaking rate affected VOT values for long lag stops in Thai and English and for pre-voiced stops in Thai and French, but did not influence VOTs in the short lag category. Magloire and Green (1999) suggested that the speaking rate affected English monolinguals’ VOT production and Spanish monolinguals’ production of pre-voicing of the voiced stops. By examining English, Kessinger and Blumstein (1998) also reported that both VOT and vowel duration increased as the speaking rate slowed down. Gósy’s (2001) study results further proved this. Gósy found that Hungarian bilabial and velar stops had significantly shorter mean VOTs in natural fluent speech than in carefully produced speech. Therefore, it is reasonable to expect that, in careful speech, the speaking rate will decrease and the accompanying VOT will get longer. The VOT values for word-initial stops in various languages have been extensively investigated. Although some of the languages studied are tonal languages (e.g., Mandarin, Taiwanese, and Cantonese), few studies have considered the effects of lexical tone when designing speech materials (Chao et al., 2006; Chen et al., 2007; Liao, 2005; Lisker & Abramson, 1964; Rochet & Fei, 1991). Gu (2005) claimed that tone is affected primarily by  344  Jui-Feng Peng et al.  pitch. Different tones have different pitch levels, which are determined by the vibrating frequency of the vocal cord. When the vocal cord tenses, the frequency of vibration increases, resulting in a higher pitch level. Conversely, the pitch level is low when the vocal cord is loose. Liu, Ng, Wan, Wang, and Zhang (2008) speculated that VOT durations may be affected by tone, as different tones have different fundamental frequencies and pitch levels, which are determined primarily by the tension of the vibrating structure. In order to achieve different levels of tension, different amounts of time might be needed. Consequently, VOT values may vary when they occur in different lexical tones. Gu (2005) further indicated that duration affects lexical tone to some extent; for example, in Hakka, the entering tone is short and rapid, meaning less time is needed to produce it. In a tonal language, the durations for each lexical tone are slightly different; therefore, it is reasonable that lexical tone might have some effects on stops’ VOTs. Liu et al. (2008), who studied the effect of tonal changes on VOTs between normal laryngeal and superior esophageal speakers of Mandarin Chinese, reported an important finding. Normal laryngeal speakers produce significant differences in VOT values as a result of lexical tones. According to their results (Figure 1), stops in the High-falling tone have significantly shorter mean VOT values than stops in the Mid-rising tone and Falling-rising tone. Nevertheless, it should be noted that, in Liu et al.’s study, some of the speech materials were non-words. The researchers did not determine whether participants produced real words and non-words differently; therefore, more studies examining the influences of tone are needed. By carrying out a systematic study with respect to the influence of lexical tone on a stop’s VOT using two tonal languages (i.e., Mandarin and Hakka spoken in Taiwan), the current study aims to create a foundation for future linguistic studies focused on tonal languages.  Figure 1. VOTs for Mandarin stops in individual tones produced by normal laryngeal speakers (Taken from Liu et al., 2008).  Tonal Effects on Voice Onset Time  345  2.3 Tones in Mandarin and Hakka Spoken in Taiwan Mandarin and Hakka only have voiceless stops; therefore, the current study investigates the unaspirated stops /p, t, k/ and aspirated stops /ph, th, kh/. In addition, Mandarin and Hakka are tonal languages, in which a word’s meaning can be changed by the tone in which it is pronounced. Chao (1967) suggested a numerical notation for lexical tones, dividing a speaker’s pitch range into four equal intervals by five points: 1 (low), 2 (half-low), 3 (middle), 4 (half-high), and 5 (high). The numerical notation indicates how the pitches of a lexical tone change. For example, the numerical notation for a Mid-rising tone in Mandarin is 35, which indicates that the pitch will go from middle to high. Mandarin has four contrasting lexical tones: High-level (HL) (55), Mid-rising (MR) (35), Falling-rising (FR) (214), and High-falling (HF) (51). Sixian Hakka has six contrasting lexical tones: low-rising (LR) (24), mid-falling (MF) (31), high-level (HL) (55), low-entering (LE) (32), low-level (LL) (11), and high-entering (HE) (55). Among them, LE and HE tones are short and rapid, and the words in these two tones end in a stop, like /p/, /t/, /k/. Mandarin Chinese and Hakka have specific tone sandhi rules. In Mandarin, FR tone, which has the longest duration among the four lexical tones, becomes MR tone when followed by another FR tone (Cheng, 1973). In Sixian Hakka, LR tone becomes LL tone when preceding a LR tone, HL tone, or HE tone. Therefore, tone sandhi rules are taken into consideration when developing speech materials in order to avoid the combinations that might cause tonal change.  Mandarin (Chao, 1967) FR Tone → MR Tone /  {FR Tone}  Sixian Hakka (Chung, 2004)  LR Tone → LL Tone /  {LR Tone, HL Tone, HE Tone}  3. Methodology This study examined word-initial unaspirated stops /p, t, k/, and aspirated stops /ph, th, kh/, in combination with three corner vowels /i, u, a/ in Mandarin and Hakka spoken in Taiwan. Except for participants and speech materials, the methodology employed for both languages was the same.  346  Jui-Feng Peng et al.  3.1 Participants In this study, the Mandarin and Hakka participants were different. The Mandarin participants included 15 male and 15 female Mandarin speakers from Tainan City with an age range from 23 to 33 years (mean = 27.2 years). All participants had grown up in Taiwan and had no hearing or speech defects. For Hakka, Sixian Hakka was chosen because it is the most extensively used Hakka dialect in Taiwan. The average age of the 21 participants - 11 men and 10 women - was 51 years, with the oldest being 80 and the youngest being 36. All of the participants for Hakka were also fluent Mandarin speakers as Mandarin is the official language in Taiwan. In the current study, the age range of Mandarin participants is controlled to within 10 years to avoid the effect of age difference. As for Hakka participants, the age-range was quite wide because it is not easy to find fluent Hakka speakers. 3.2 Data Collection The speech materials in both languages were combinations of six stops /p, t, k, ph, th, kh/ and three vowels /i, u, a/, resulting in 18 combinations. Mandarin’s 4 contrasting lexical tones meant that a total of 72 monosyllabic words were created; among them, 18 combinations do not have corresponding Chinese characters in Mandarin. The 6 contrasting lexical tones in Sixian Hakka resulted in 108 monosyllabic words, 12 of which do not have corresponding Chinese characters. Chen et al. (2007) claimed that disyllabic words can create a more natural-like context for participants. Therefore, in order to make speakers produce the words more naturally, all of the words were followed by another word in order to create meaningful disyllables, including non-words. For example, the Mandarin word /pi/ was followed by another word /phuo/ to become the existing disyllable /pi phuo/ “force.” Even non-words were arranged in disyllabic forms to give them a more natural-like quality. Since the neutral tone in Mandarin never occurs in phrase-initial position, it was not evaluated in this study. The structure of non-words was the same as real words, which is a CV syllable with one consonant (stop) and one vowel (corner vowel). For example, there is no /kha/ in MR tone in Mandarin or /pu/ in MF tone in Hakka, so non-words were created for these combinations. The way we measure VOTs of non-words is the same as with the real words. For example, /kha/ is measured from target consonant /kh/ to /a/. The corpus was arranged randomly. Participants were asked to read the words out loud in a normal voice and at a comfortable rate. After finishing, the participants were asked to read the words a second time. Therefore, two groups of data were gathered for each participant. All speech was recorded using a 24 bit WAVE/MPS recorder, connected to AKG C520 Head-Worn Condenser Microphone positioned approximately 10 to 15 centimeters from the participant’s mouth in a quiet room.  Tonal Effects on Voice Onset Time  347  3.3 Data Measurement and Analysis After recording, data were edited into individual files and analyzed using the Praat software. VOT, measured in milliseconds (ms), was obtained by measuring the temporal interval between the beginning of the release burst and the onset of the following vowel, as shown in Figure 2. The values of both the waveform and spectrogram were recorded, but the VOTs were determined primarily through waveform analysis, with the values in the spectrogram being provided as references. If the values in waveform differed from the values in the spectrogram by more than five milliseconds, the data were re-measured to verify accuracy.  Figure 2. Spectrogram and waveform for Mandarin word /pu iau/ ‘don’t want’. The values in the circle are the starting and endpoints of VOT. The VOT values were measured by one investigator. Furthermore, 10% of each recording (selected randomly) was re-measured by another investigator to verify the reliability of the results. Ultimately, 7 Mandarin words and 11 Hakka words for each recording were re-measured. Pearson’s product-moment correlations (Gravetter & Wallnau, 2008) indicated high inter-rater agreement for both Mandarin and Hakka data (Mandarin: r = .995, p<.001; Hakka: r = .978, p<.001). When analyzing the data, VOT values for mispronounced words were omitted. Moreover, data for Hakka /pi/ in HE Tone were not analyzed due to incorrect word choices. A four-way mixed factorial ANOVA (Montgomery, 2009; the same test was used in Francis, Ciocca, & Yu, 2003) (place of articulation by vowel context by lexical tone by gender) was used to examine whether the variables significantly influenced each stop’s VOT. In addition, differences between the examined targets were analyzed using T-test or post-hoc tests  348  Jui-Feng Peng et al.  (Scheffe) (Gravetter & Wallnau, 2008); results were considered significant when the p value was less than 0.05. Four-way mixed factorial ANOVA can be illustrated in the following formula (Montgomery, 2009). yijklm = μ+τi+βj+γk +σl + (τβ)ij + (τγ)ik + (τσ)il + (βγ)jk + (βσ)il + (γσ)kl + (τβγ)ijk+ (τβσ)ijl+ (βγσ)jkl + (τγσ)ikl + (τβγσ)ijkl + εijklm i= 1, 2, 3 place j= 1, 2, 3 vowel k= 1, 2, 3, 4 tone l= 1, 2 gender m= 1, 2, …30 subjects 4. Results and Discussion When examining the VOT values for Mandarin stops, it became apparent that they tend to be longer than the mean VOT values reported in the studies by Liao (2005), Chao et al. (2006), and Chen et al. (2007) and shorter than those reported by Rochet and Fei (1991). Examining the methodologies in these previous studies indicated that the speech materials in Rochet and Fei’s study were monosyllabic, but disyllabic in the remaining four studies. Gósy (2001) claimed that speakers speak in a careful and disciplined way while uttering syllables and words in isolation. Therefore, participants are expected to produce monosyllables in a more careful manner as their speech tempo decreases. According to Kessinger and Blumstein (1998), VOTs get longer while the speaking rate slows down. This may explain why the mean VOTs for Mandarin stops in Rochet and Fei’s study were longer than in other reports. Another possible explanation might be regional differences in the target language. Although the target language is the same, participants in Rochet and Fei’s study grew up in Mainland China, while participants in the other studies were raised in Taiwan. Consequently, regional differences might be another origin of the variations. As a result, comparing the results of Liao (2005), Chao et al. (2006), and Chen et al. (2007) to those of the present study indicates that the mean VOT values for the stops in the present study tend to be longer than their counterparts in the other studies. Such differences stem primarily from the existence of non-words in the current study. During the recording process, speakers tended to produce non-words more carefully and at a lower speed because they were not familiar with the non-words. An examination of the data both with and without non-words separately demonstrated that the existence of non-words resulted in stops having longer mean VOTs.  Tonal Effects on Voice Onset Time  349  VOT VOT  4.1 Mandarin Chinese The statistical analyses were conducted using a four-way mixed factorial ANOVA. For Mandarin unaspirated stops, the results showed a primary effect of place of articulation, F(2, 972) = 522.9680; vowel context, F(2, 972) = 117.3569; lexical tone, F(3, 972) = 6.5506; and gender F(1, 972) = 56.9180 (all p < .001). These results indicate that the stop place of articulation (bilabial, alveolar, and velar), vowel context (/i/, /u/, /a/), lexical tone (HL, MR, FR, HF), and gender do create significant differences in VOT values of word-initial unaspirated stops. Furthermore, significant two-way interactions were also observed (Figures 3-4) between place of articulation and vowel context and between vowel context and gender. A complete ANOVA table showing interaction between variables is listed in Appendix 1.  40.0 35.0 30.0  Vowel /i/ /u/ /a/  25.0  20.0  15.0  10.0  /p/  /t/  /k/  Stop  Figure 3. Significant interaction between place of articulation and vowel context in unaspirated stops in Mandarin  26.0 24.0 22.0  Gender male female  20.0  18.0  16.0  14.0  12.0  /i/  /u/  /a/  Vowel  Figure 4. Significant interaction between vowel context and gender in unaspirated stops in Mandarin  As for aspirated stops, the results demonstrated a main effect of place of articulation, F(2, 972) = 95.2742; vowel context, F(2, 972) = 43.5079; lexical tone, F(3, 972) = 12.0121; and gender F(1, 972) = 20.3186, thereby indicating that VOT values of word-initial aspirated stops would vary in accordance with place of articulation, vowel context, lexical tone and gender (all p < .001). Significant two-way interactions occurred between place of articulation and vowel context, between place of articulation and lexical tone, and between vowel context and gender (Figures 5-7). For both unaspirated and aspirated stops in Mandarin, no significant three-way and four-way interactions were evident between variables. A complete ANOVA table showing interaction between variables is listed in Appendix 2.  350  Jui-Feng Peng et al.  Mean VOT  130.0 120.0 110.0  Vowel /i/ /u/ /a/  100.0  90.0  80.0  70.0  /ph/  /th/ Stop  /kh/  Figure 5. Significant interaction between place of articulation and vowel context in aspirated stops in Mandarin  Mean VOT  130.0 120.0 110.0 100.0  Tone High Level Mid Rising Falling Rising High Falling  90.0  80.0  /ph/  /th/ Stop  /kh/  Figure 6. Significant interaction between place of articulation and lexical tones in aspirated stops in Mandarin  110.0 105.0 100.0  Gender male female  95.0  90.0  85.0  80.0  /i/  /u/  /a/  Vowel  Figure 7. Significant interaction between vowel context and gender in aspirated stops in Mandarin  Table 1 lists mean VOT values and standard deviation of Mandarin stops in each lexical tone. ANOVA tests revealed that the lexical tone significantly influences VOTs of stops [unaspirated stops, F(3, 972) = 6.5506, p < .001; aspirated stops, F(3, 972) = 12.0121, p < .001]. A post-hoc test reveals that aspirated stops in HF have significantly shorter mean  VOT  Tonal Effects on Voice Onset Time  351  VOTs than stops in MR and FR (all p < .05). In addition, for both unaspirated and aspirated stops, stops in MR have the longest mean VOTs while stops in HF have the shortest mean VOTs. VOTs, from longest to shortest, occurred in MR, FR, HL, and HF - the same sequence as in Pearce (2009) and Liu et al. (2008). Yet, it is worth noting that, in both studies, some of the speech materials were non-words. It was subsequently determined that the sequence results from the existence of non-words because, in order to produce non-words correctly, participants tended to pronounce them at a slower speed, making the VOTs longer. Therefore, the current study further examined the data without non-words, in which the main variation occurred in participants’ productions of stops in MR tone. When analyzing the data with non-words, Mandarin unaspirated and aspirated stops in MR tone had the longest mean VOTs. Nevertheless, when excluding non-words, the results revealed that stops in MR tone did not have the longest mean VOTs, and unaspirated stops in MR tone even had significantly shorter mean VOTs than in HL and FR tones. The divergence revealed that participants’ productions of real words or non-words in MR tone were quite different. In addition, ANOVA tests revealed that lexical tone does not significantly influence stops’ VOTs by analyzing the data without non-words. Further studies are needed to have separate sets of wordlists for real words and non-words to verify the current findings. Table 1. Mean VOT values of Mandarin stops with different lexical tones. All measurements are in milliseconds (ms).  With non-words  Without non-words  unaspirated stops Mean SD  aspirated stops mean SD  unaspirated stops mean SD  aspirated stops mean SD  HL 20.20 (11.90)  92.72 (25.53) 17.71 (9.95)  88.69 (20.4)  MR 21.10 (12.68) 101.02 (30.21) 13.99 (6.03)  89.47 (23.31)  FR 20.89 (13.35)  97.03 (27.75) 17.00 (10.98)  92.30 (23.49)  HF 18.42 (9.94)  89.4 (25.72) 16.32 (9.07)  85.62 (24.18)  Table 2. Mean VOT values of six Mandarin stops with different lexical tones. All measurements are in milliseconds (ms).  With non-words (mean)  p t  k  ph  th  kh  HL 14.4 14.7 31.5 90.4 82.6 105.1 MR 16.0 15.1 32.2 89.9 91.1 122.1 FR 15.0 14.9 32.7 92.3 90.3 108.5 HF 13.3 14.2 27.7 85.0 82.1 101.0  *No real word data  Without non-words (mean)  pt  k  ph  th  kh  12.0 14.7 27.9 90.4 82.6 95.2  12.3 15.1 * 89.9 88.9 97.4  11.3 14.9 34.7 90.2 90.3 97.8  11.8 14.2 31.8 85.0 82.1 *  352  Jui-Feng Peng et al.  In the data with non-words in Table 2, individual stops mostly follow the pattern of MR with the longest VOT and HF with the shortest VOT, especially for /p, t, th, kh/. As for the data without non-words, the general pattern is also found in /p, t/.  4.2 Lexical Tone and VOT in Hakka For Hakka unaspirated stops, the results showed a main effect of place of articulation, F(2, 843) = 404.3395; vowel context, F(2, 843) = 69.8958; lexical tone, F(5, 843) = 6.3054; and gender F(1, 843) = 34.2724 (all p < .001). Similar to the findings in Mandarin stops, these results indicate that stop place of articulation (bilabial, alveolar, and velar), vowel context (/i/, /u/, /a/), lexical tone (HL, MR, FR, HF), and gender do make significant differences in VOT values of word-initial unaspirated stops. Significant two-way interactions occurred between the place of articulation and lexical tone (Figure 8), and significant three-way interactions occurred among place of articulation, vowel context, and lexical tone. A complete ANOVA table showing interaction between variables is listed in Appendix 3.  Mean VOT  35.0 30.0 25.0 20.0 15.0 10.0  Tone Low Rising Mid Falling High Level Low Entering Low Level High Entering  /p/  /t/  /k/  Stop  Figure 8. Significant interaction between place of articulation and lexical tone in unaspirated stops in Hakka  As for aspirated stops, the results also demonstrated a main effect of place of articulation, F(2, 798) = 55.6543; vowel context, F(2, 798) = 44.7708; lexical tone, F(5, 798) = 46.4587; and gender F(1, 798) = 42.0266, thereby indicating that VOT values of word-initial aspirated stops would vary in accordance with place of articulation, vowel context, lexical tone, and gender (all p < .001). Significant two-way interactions existed between place of articulation and vowel context, between place of articulation and lexical tone, and between vowel context and lexical tone (Figures 9-11). In addition, significant three-way interactions occurred among  Tonal Effects on Voice Onset Time  353  Mean VOT Mean VOT  place of articulation, vowel context, and lexical tone. A complete ANOVA table showing interaction between variables is listed in Appendix 4.  110.0 100.0 90.0  Vowel /i/ /u/ /a/  80.0  70.0  60.0  /ph/  /th/ Stop  /kh/  Figure 9. Significant interaction between place of articulation and vowel context in aspirated stops in Hakka  120.0 110.0 100.0 90.0 80.0 70.0 60.0  Tone Low Rising Mid Falling High Level Low Entering Low Level High Entering  50.0  /ph/  /th/ Stop  /kh/  Figure 10. Significant interaction between place of articulation and lexical tone in aspirated stops in Hakka.  100.0 90.0 80.0 70.0 60.0 50.0  /i/  /u/  /a/  Vowel  Tone Low Rising Mid Falling High Level Low Entering Low Level High Entering  Figure 11. Significant interaction between vowel context and lexical tones in aspirated stops in Hakka.  The mean VOT values and standard deviations for Hakka stops in each lexical tone are shown in Table 3. ANOVA tests indicated that the lexical tone significantly influences stops’  Mean VOT  354  Jui-Feng Peng et al.  VOTs [unaspirated stops, F(5, 843) = 6.3054, p < .001; aspirated stops, F(5, 798) = 46.4587, p < .001]. Unaspirated and aspirated stops in LR and LL have longer mean VOTs than stops in other tones, whereas the shortest mean VOTs for both unaspirated and aspirated stops are in HE. The post-hoc test revealed that aspirated stops in HE and LE have significantly shorter mean VOTs than those in other tones (all p < .001). In Hakka, HE and LE tones are entering tones, which are short, rapid, and end in a stop like /p, t, k/. Gu (2005) further claimed that the durations for entering tones are shorter than the durations for other tones. Therefore, the VOTs for stops in the entering tones are shorter than stops in other tones.  Table 3. Mean VOT values of Hakka stops with different lexical tones. All measurements are in milliseconds (ms)  Unaspirated stops mean (SD)  Aspirated stops mean (SD)  LR  20  (11.56)  86.83 (25.8)  MF  16.94 (8)  84.67 (26.56)  HL  18.88 (11.02)  81.32 (23.73)  LE  17.19 (9.44)  62.93 (18.36)  LL  19.4 (11.43)  90.08 (27.08)  HE  16.11 (7.98)  61.53 (20.36)  Table 4. Mean VOT values of six Hakka stops with different lexical tones. All measurements are in milliseconds (ms)  Without non-words (mean)  p  t  k  ph  th  kh  LR  13.4  16.2  30.3  88.7  76.1  97.0  MF  12.9  14.4  23.6  77.0  80.4  96.2  HL  12.4  15.0  28.4  80.8  79.1  83.9  LE  11.9  13.8  25.6  57.0  60.4  70.4  LL  13.5  15.7  29.7  81.0  82.5  110.9  HE  11.1  15.3  24.2  54.2  62.0  74.4  Table 4 shows that almost all of the stops follow the general pattern where LE and HE are among the shortest.  5. Conclusion  The study results revealed that lexical tones significantly influence VOT values for stops in Mandarin (with both real words and non-words), and significant tonal effect was also found in  Tonal Effects on Voice Onset Time  355  Hakka data of real words. Nevertheless, there is no significant tonal effect on VOT in Mandarin data with only real words. The study results are important as they suggest that future studies should take the influence of lexical tones into account when studying VOT values and when designing wordlists for stops in tonal languages. Although the tonal effects on VOT values are not consistently found in different sets of data, probably due to a methodology problem, the possibility of tonal effect on VOT values could not be excluded. Several factors might contribute to this inconsistency. First, we used different methods in eliciting non-words production in these two languages. In Mandarin, we used Zhuyin Fuhao to guide non-words productions, which might force participants to take a few seconds to figure out the new combinations of Chinese phonetic symbols. In contrast, we asked participants to read a real word first as a clue in producing a target non-word in Hakka. Second, many of the participants in Hakka were at the age range of 50-80, and we found the participants were not flexible enough to comprehend our instructions in producing non-words. Therefore, we decided not to take the data of non-words (mostly by guessing with uncertainly) into analysis in order to have a reliable result. (The discarded cells (non-words) are less than 25% of the total, which fulfills the requirements of ANOVA test for reliable test results (Gravetter & Wallnau, 2008). Future studies should keep the method of elicitation consistent across languages, recruit participants of similar age range, and include both real words and non-words in separate sets of word lists in order to verify the current study findings. Moreover, although the results of this study indicate that lexical tones influence VOT of stops to some extent, we cannot exclude the possibility of correlation between VOT and the duration of lexical tones. Further study can explore this possibility by adopting mean durations of each tone in Mandarin as the normalized parameters upon calculating mean VOTs of stops. References Auzou, P., Ozsancak, C., Morris, R. J., Jan, M., Eustache, F., & Hannequin, D. (2000). Voice onset time in aphasia, apraxia of speech and dysarthria: a review. Clinical Linguistics & Phonetics, 14(2), 131-150. Chao, K.-Y., Khattab, G., & Chen, L.- M. (2006). Comparison of VOT patterns in Mandarin Chinese and in English. Proceedings of the 4th Annual Hawaii International Conference on Arts and Humanities, 840-859. Chao, Y. R. (1967). Mandarin Primer. Cambridge: Harvard University Press. Chen, L.- M., Chao, K.- Y., & Peng, J.- F. (2007). VOT productions of word-initial stops in Mandarin and English: a cross-language study. Proceedings of the 19th Conference on Computational Linguistics and Speech Processing, 303-317. Cheng, C.- C. (1973). A Synchronic Phonology of Mandarin Chinese. The Hague: Mouton.  356  Jui-Feng Peng et al.  Cho, T., & Ladefoged, P. (1999). Variation and universals in VOT: evidence from 18 languages. Journal of Phonetics, 27, 207-229. Chung, R.- F. (2004). An introduction to Taiwan Hakka phonology. Taipei: Wu-Nan Book. Fant, G. (1973). Speech sounds and features. Cambridge: MIT Press. Francis, A. L., Ciocca, V., & Yu, J. M. C. (2003). Accuracy and variability of acoustic measures of voicing onset. Journal of the Acoustical Society of America, 113(2), 1025-1032. Gósy, M. (2001). The VOT of the Hungarian voiceless plosives in words and in spontaneous Speech. International Journal of Speech Technology, 4, 75-85. Gravetter, F. J., & Wallnau, L. B. (2008). Statistics for the behavioral sciences (8th ed.). Belmont, California: Wadsworth. Gu, G.- S. (2005). Phonology of Taiwan Hakka. In Gu, G.-S. (Ed.), Introduction to Taiwan Hakka, 117-161. Taipei: Wu-Nan. Jäncke, L. (1994). Variability and duration of voice onset time and phonation in stuttering and nonstuttering adults. Journal of Fluency Disorders, 19(1), 21-37. Keating, P. A., Linker, W., & Huffman, M. (1983). Patterns in allophone distribution for voiced and voiceless stops. Journal of Phonetics, 11, 277-290. Kessinger, R. H., & Blumstein, S. E. (1997). Effects of speaking rate on voice-onset time in Thai, French, and English. Journal of Phonetics, 25, 143-168. Kessinger, R. H., & Blumstein, S. E. (1998). Effects of speaking rate on voice-onset time and vowel production: some implications for perception studies. Journal of Phonetics, 26, 117-128. Liao, S. J. (2005). Interlanguage production of English stop consonants: a VOT analysis. Unpublished master’s thesis, National Kaohsiung Normal University, Kaohsiung, Taiwan. Lisker, L., & Abramson, A. S. (1964). A cross-language study of voicing in initial stops: acoustical measurements. Word, 20, 384-422. Lisker, L., & Abramson, A. S. (1967). Some effects of context on voice onset time in English stops. Language Speech, 10, 1-28. Liu, H., Ng, M. L., Wan, M., Wang, S., & Zhang,Y. (2008). The effect of tonal changes on voice onset time in Mandarin esophageal speech. Journal of Voice, 22(2), 210-218. Magloire, J., & Green, K. P. (1999). A cross-language comparison of speaking rate effects on the production of voice onset time in English and Spanish. Phonetica, 56, 158-185. Montgomery, D. C. (2009). Design and analysis of experiments, student solutions manual (7th ed.). Hoboken, N.J. :Wiley. Morris, R. J., McCrea, C. R., & Herring, K. D. (2008). Voice onset time differences between adult males and females: isolated syllables. Journal of Phonetics, 36, 308-317. Pearce, M. (2009). Kera tone and voicing interaction. Lingua, 119(6), 846-864.  Tonal Effects on Voice Onset Time  357  Riney, T. J., & Takagi, N. (1999). Global foreign accent and voice onset time among Japanese EFL speakers. Language Learning, 49(2), 275-302. Riney, T. J., Takagi, N., Ota, K., & Uchida, Y. (2007). The intermediate degree of VOT in Japanese initial voiceless stops. Journal of Phonetics, 35, 439-443. Rochet, B. L., & Fei, Y. (1991). Effect of consonant and vowel context on Mandarin Chinese VOT: production and perception. Canadian Acoustics, 19(4), 105-106. Rosner, B. S., López- Bascuas, L. E., García-Albea, J. E., & Fahey, R. P. (2000). Voice-onset times for Castilian Spanish initial stops. Journal of Phonetics, 28, 217-224. Tsen, C.Y. (1994). An investigation on voice onset time and tone production of Chinese aphasia. Bulletin of the Institute of History and Philology Academia Sinica, 65(1), 37-79. Zheng, X.- R., & Li, Y.- H. (2005). A contrastive study of VOT of English and Korean stops. Journal of Yanbian University, 38(4), 99-102.  358  Jui-Feng Peng et al.  Appendix 1. Four-way factorial ANOVA for Mandarin unaspirated stops  Source of Variation  SS  df  Place of articulation  62001  2  Vowel  13913  2  Gender  3374  
Approximating a spectral envelope via regularized discrete cepstrum coefficients has been proposed by previous researchers. In this paper, we study two problems encountered in practice when adopting this approach to estimate the spectral envelope. The first is which spectral peaks should be selected, and the second is which frequency axis scaling function should be adopted. After some efforts of trying and experiments, we propose two feasible solution methods for these two problems. Then, we combine these solution methods with the methods for regularizing and computing discrete cepstrum coefficients to form a spectral-envelope estimation scheme. This scheme has been verified, by measuring spectral-envelope approximation error, as being much better than the original scheme. Furthermore, we have applied this scheme to building a system for voice timbre transformation. The performance of this system demonstrates the effectiveness of the proposed spectral-envelope estimation scheme. Keywords: Spectral Envelope, Discrete Cepstrum, Harmonic-plus-noise Model, Voice Timbre Transformation. 1. Introduction Here, a spectral envelope means a magnitude-spectrum envelope. Various methods have been proposed to estimate the spectral envelope of a speech frame. For example, in LPC (linear prediction coding) based methods (O’Shaughnessy, 2000; Schwarz & Rodet, 1999), the frequency response of an all-pole model is used to approximate the spectral envelope of a speech frame. Nevertheless, the frequency response curve of an LPC all-pole model will usually go below the true envelope around speech formants, and go above the regions where ∗ Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, 43 Keelung Rd., Sec. 4, Taipei, Taiwan. E-mail: {guhy, M9615069}@mail.ntust.edu.tw  364  Hung-Yan Gu and Sung-Feng Tsai  spectrum magnitudes fall suddenly. This is illustrated in Figure 1 using a frame sliced from an utterance of /i/. Therefore, the mismatches between the LPC envelope curve and the true curve cannot be ignored. dB LPC envelope  deep mismatch cepstrum-smoothed envelope Hz Figure 1. LPC and cepstrum smoothed spectral curves for a frame from /i/. Besides LPC, several estimation methods are based on cepstrum analysis. The simplest one is to keep some leading cepstrum coefficients but truncate the remanded ones, i.e. replace them with zeros. Then, DFT (discrete Fourier transform) is used to transform the cepstrum coefficients back to the spectrum domain to obtain a smoothed spectrum curve. Nevertheless, such a smoothed spectrum curve is not really an envelope curve because it goes between the peaks and valleys of a DFT spectrum. One example is the lower smoothed curve in Figure 1. Therefore, a real cepstrum-based method to estimate a spectral envelope was proposed later by Imai and Abe (Imai & Abe, 1979; Robel & Rodet, 2005). They call this method true envelope estimation. In our opinion, this method is good but lacking in efficiency because a lot of computations are required. Similarly, the method proposed by Kawahara, Masuda-katsuse, and Cheveign (1999), STRAIGHT, is very accurate in its estimated spectral envelope. Nevertheless, it also requires a considerable number of computations and cannot be used to implement real-time systems currently. On the other hand, Galas and Rodet (1990) proposed the concept of discrete cepstrum and designed a feasible estimation method with this concept. Later, Cappé and Moulines (1996) improved this estimation method by adding a regularization technique to prevent unstable vibrating of the envelope curve from occurring. We think that estimating a spectral envelope with discrete cepstrum is a good approach if the feasibility and accuracy issues must be considered simultaneously. Therefore, we began to study the problems that will be encountered in practice.  A Discrete-cepstrum Based Spectrum-envelope Estimation Scheme and Its  365  Example Application of Voice Transformation  As an overview, the spectral envelope estimation scheme proposed here is shown in Figure 2. When a speech frame is given, its fundamental frequency is first detected in the first block. If a frame is decided to be voiced, its estimated fundamental frequency will be used later in the block, “spectral peaks selection”. Here, a method combining an autocorrelation function and AMDF (absolute magnitude difference function) is adopted to detect a frame’s fundamental frequency (Kim et al., 1998; Gu, Chang & Wu, 2004). Next, the frame is Hanning windowed and appended with zeros to form a sequence of 1,024 signal samples. This sequence is then transformed to frequency domain with FFT (fast Fourier transform) to obtain its magnitude spectrum. Given the magnitude spectrum, the block “spectral peaks selection” will determine which spectral peaks should be selected according to a method proposed here. After spectral peaks are selected, the frequency value of each selected peak is mapped to its target value with a frequency-axis scaling function proposed here. As the final step, the block “discrete cepstrum computation” will adopt an envelope-approximation criterion (Cappé & Moulines, 1996) to compute discrete cepstrum coefficients according to the selected and mapped spectral peaks.  speech frame  fundamental frequency detection  Hanning windowing  FFT  spectral envelope  discrete cepstrum computation  frequency axis scaling  spectral peak selection  Figure 2. Main flow of the spectral-envelope estimation scheme. In Figure 2, discrete-cepstrum computation is the main block, which already has been solved by other researchers (Cappé & Moulines, 1996). Nevertheless, the blocks, spectral-peak selection and frequency-axis scaling, still play important roles. When inappropriate peaks are selected or the frequency-axis is not scaled appropriately, the approximated spectral envelope will noticeably deviate from the true envelope. Therefore, we began to study these two blocks’ problems, and the results are presented in Sections 3 and 4, respectively. As to discrete cepstrum, its computation and regularization will be reviewed in Section 2. In Section 5, the proposed scheme is practically evaluated by applying the scheme to build a voice timbre transformation system.  366  Hung-Yan Gu and Sung-Feng Tsai  2. Spectral-envelope Estimation with Discrete Cepstrum  2.1 Discrete Cepstrum  The concept of discrete cepstrum was proposed by Galas and Rodet (1990). They adopted the  least square criterion to a given set of spectral peaks to derive cepstrum coefficients. Such a  derivation method is different from the conventional one. The conventional method transforms  the logarithmic magnitude-spectrum with inverse DFT (IDFT) to get its cepstrum coefficients.  In the conventional method, let the obtained cepstrum coefficients be c0, c1, …, cN-1 where N is the length of the signal sample sequence. According to these cepstrum coefficients, the  original logarithmic magnitude-spectrum can be restored with DFT, i.e.  N −1 − j 2π kn  log X (k) = ∑ cne N , 0 ≤ k ≤ N −1  (1)  n=0  where |X(k)|, k=0, 1, …, N-1 represent the magnitude spectrum. Since log|X(k)| is even  symmetric, i.e. log|X(k)| = log|X(N-k)|, the derived cepstrum coefficients are also even  symmetric, ck = cN-k. Therefore, Equation (1) can be rewritten as:  log  X (k)  =  c0  +  N 2  −1  2 ∑ cn  n=1  cos( 2π N  kn)  +  cN /2 cos(π k),  0 ≤ k ≤ N −1 .  (2)  If most of the terms on the right side of Equation (2) are cancelled except the leading  terms (e.g. p+1 terms), the magnitude spectrum computed, log S(f), would be a smoothed  version of the original, log|X(f)|. Here, the index variable, k, in Equations (1) and (2) is  replaced with f in order to change the frequency scale from bins to the normalized frequency  range from 0 to 1. Accordingly, log S(f) is computed as:  p log S( f ) = c0 + 2 ∑ cn ⋅ cos(2π f n) , n=1  f  =  0 N  ,  
Opinion holder identification aims to extract entities that express opinions in sentences. In this paper, opinion holder identification is divided into two subtasks: author’s opinion recognition and opinion holder labeling. Support vector machine (SVM) is adopted to recognize author’s opinions, and conditional random field algorithm (CRF) is utilized to label opinion holders. New features are proposed for both methods. Our method achieves an f-score of 0.734 in the NTCIR7 MOAT task on the Traditional Chinese side, which is the best performance among results of machine learning methods proposed by participants, and also it is close to the best performance of this task. In addition, inconsistent annotations of opinion holders are analyzed, along with the best way to utilize the training instances with inconsistent annotations being proposed. Keywords: Opinion Holder Identification, Opinion Mining, Conditional Random Field, Support Vector Machine. 1. Introduction Opinions describe subjective thinking of people. With the blooming of Web 2.0, a large number of free and online articles have become easily accessible. Although people are interested in the shifting of opinions, they cannot read such a large quantity of articles in a short time. Opinion mining can analyze opinions from many information sources automatically and helps extract opinions, along with determining their polarities, strength, holders, and targets. Opinion polarities tell us whether the current opinions are positive, neutral, or negative. The opinion strength then tells us the degree of their attitude, i.e., strong, medium, or weak. Opinion holders are the people who express opinions, while opinion targets are the objects of those opinions. Let us take “Mr. Wang loves to play baseball” as an example. In this opinion sentence, its polarity is positive, its strength is strong, the opinion holder is Mr. Wang, and the opinion target is playing baseball. It is an opinion from Mr. Wang that indicates  ∗ Department of Computer Science and Information Engineering, National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan, TEL: +886-2-33664888*311, FAX: +886-2-23628167 E-mail: {lwku, cylee}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw  384  Lun-Wei Ku et al.  he has a positive attitude towards playing baseball. Opinion holder identification is useful in knowing who has the same attitude, what kind of issues a specific person cares about, and whether there are different opinions from some specific persons. This technique can also be applied to social network analysis to discover who the opinion leader is. It is also important in an opinion question answering system as it can provide the owner of opinions. There are three major challenges in opinion holder identification: co-reference resolution, parsing nested opinions, and inconsistent annotation utilization. Like the conventional question answering problem, pronoun-antecedent and zero anaphor problems have to be solved before identifying opinion holders. Nested opinions are common in long sentences. People like to quote opinions of other people to show that they are impartial, but this behavior also implies that they agree with their quotes. In this case, we need to identify both the quoting and the quoted holders for further analysis. It is sometimes difficult to determine the holder of an opinion. For example, even though the holder is obvious in the opinion sentence, we may find that he represents some organization and is presenting the organization’s opinion. The following sentence is an example: “According to the media, [the] U.S. and China are discussing the agreement of terminating the usage of nuclear weapons; Becon said they have discussed this issue before.” In this sentence, “they” refers to the U.S. and China. Becon quoted words from the U.S. and China, so this is a nested structure. In addition, although this expression is said by the media and Becon, the holder should be the U.S. and China. These challenges all complicate the annotation process, and a double check and a selection process are necessary when generating the gold standard. 2. Related Work Pang and Lee (2008) have mentioned some important research projects in the domain of opinion mining. Kim and Hovy (2004) proposed four elements in opinion mining, including the opinion polarity, the opinion strength, the opinion holder, and the opinion target. Among them, the research for opinion holder identification is new. Previous researchers mainly have proposed two kinds of methods: heuristic rule based and machine learning based methods. 2.1 Heuristic Rule based Methods For heuristic rule based methods, Seki et al. (2009) utilized noun phrases and linguistic features, and adopted SVM to classify opinion holders into authors and non-authors in English and Japanese materials. Xu and Wang (2008) first solved the co-reference resolution, and extracted opinion holders by rules involving punctuation marks, conjunctions, prefixes,  Identification of Opinion Holders  385  suffixes, and opinion operators. They achieved an f-score of 0.825 in the NTCIR7 MOAT task on the Traditional Chinese side, which is the state of the art. 2.2 Machine Learning based Methods For the machine learning based methods, many researchers have adopted maximum entropy algorithms, SVM, or the conditional random field model. Kim and Hovy (2006) utilized the maximum entropy model to extract opinion holders and targets from news articles. They first found opinion words and labeled semantic roles, then identified the semantic roles that are opinion holders and targets. Kim (2007, 2008) classified opinion holders into authors, simple holders and co-referenced holders, then extracted lexical and syntactic features for SVM to select the best opinion holder. So far, this is the best method for English materials, and it achieved an f-score of 0.346. Wu (2008) used words and parts of speech as features in L2-norm linear SVM to solve this research problem as a similar method for named entity identification. Breck and Choi (2007, 2005) utilized lexical features, syntactic features, dictionary-based features, and dependency features by CRF to identify opinion holders. Meng and Wang (2008) used words, parts of speech, and opinion operators, while Liu and Zhao (2008) extracted parts of speech, semantic features, contextual features, dependency features, and position features by CRF. 2.3 Proposed Methods We propose a unique approach that divides opinion holder identification into two tasks: author’s opinion recognition and opinion holder labeling. We then find better strategies to perform these two tasks. For author’s opinion recognition, we adopt SVM by features such as words and their parts of speech, named entities, punctuation marks, the context, and opinion related information in the current sentence. Among them, some context features (the roles of verbs) and opinion related features (information of positive words, neutral words, negative words, and opinion operators) have not been utilized in opinion holder identification before. Detailed features will be described in Sections 3.2 and 3.3. 3. Opinion Holder Identification Five procedures of opinion holder identification are proposed in this paper, including text pre-processing, author’s opinion recognition, opinion holder labeling, post-processing, and result generation. Chinese word segmentation, parts of speech tagging, and named entity recognition are performed in the text pre-processing stage. Then, author’s opinions are recognized and opinion holder labeling determines the text segment referring to the holder. We have two strategies for applying the proposed methods of author’s opinion recognition and opinion holder labeling. These two strategies are described in Section 3.5. After that, this  386  Lun-Wei Ku et al.  labeled text segment is processed by the post-processing procedure to generate the final opinion holder. The flowchart describing these five procedures is shown in Figure 1.  Figure 1. Flowchart of opinion holder identification 3.1 Text Pre-processing Stage In the text pre-processing stage, we utilize the Chinese word segmentation system developed by Lo (2008). We, however, modify its segmentation module and add additional name dictionaries to it. The length limit of the modified Chinese name module is set looser and Japanese family names are added so that the segmentation system can recognize Japanese names, which are usually longer than Chinese names. Occupations, titles, and company names are also added to the dictionary of the segmentation system to provide useful holder relevant information. 3.2 Author’s Opinion Recognition Stage Author’s opinion recognition finds out whether the opinion holder of the current sentence is the author. In this paper, this task is viewed as a binary classification task and LIBSVM (Chang & Lin, 2001) is adopted for classification. The main features extracted for this task are words, parts of speech, named entities, punctuation marks, sentence components, and opinion operators. Table 1 shows all of the features utilized in this task. The lexicon features include  Identification of Opinion Holders  387  first person pronouns, which are often utilized by the authors to refer to themselves. The part-of-speech features include general pronouns and personal pronouns, because pronouns usually refer to persons and organizations and they can express opinions. The named-entity features are considered because they can either be the opinion holders or the opinion targets.  Table 1. Features for author’s opinion recognition  Feature Type  Feature Name FHasI  Feature Description Does the word “I” (我) appear?  Lexicon  FHasWe fNumI  Does the word “we” (我們) appear? The number of the word “I” (我)  fNumWe  The number of the word “we” (我們)  fHasPronoun  Are there any pronouns?  Part of speech  fHasManPronoun fNumPronoun  Are there any personal pronouns? The number of pronouns  fNumManPronoun The number of personal pronouns  fHasPer  Is there a person name (named entity)?  fHasLoc  Is there a location name (named entity)?  fHasOrg  Is there an organization name (named entity)?  fHasNa  Are there any common nouns?  fHasNb  Are there any proper nouns?  Named entity  fHasNc fNumLoc  Are there any common location nouns? The number of location names (named entity)  fNumOrg  The number of organization names (named entity)  fNumPer  The number of personal names (named entity)  fNumNa  The number of common names  fNumNb  The number of proper names  fNumNc fHasExclamation  The number of common location names Is there an exclamation mark (“！” or “!”) ?  Punctuation mark  fHasQuestion fHasColon fHasLeftQuotation  Is there a question mark (“？”or “?”) ? Is there a colon (“：”or “:”) ? Are there any quotation marks (“「” or “【” ) ?  fHasRightQuotation Are there any quotation marks (“」” or “】”) ?  fNumChar  The number of Chinese characters  Sentential  fNumWord  The number of Chinese words  fNumSubsen  The number of clauses  Opinion  fOperator1 to 203 Is there an opinion operator?  388  Lun-Wei Ku et al.  The punctuation features include punctuation marks that possibly co-occur with opinions. For example, exclamation points and question marks often appear in sentences expressed by people because they can bear sentiment, whereas colons and quotations are usually used to quote expressed words. The sentential features tell the length of the sentences by their composite characters, words, and clauses. We consider these features because we think that authors may need a sentence of a suitable length to express opinions. As to the opinion features, a total of 203 opinion operators, such as 說 (say), 指出 (point out), and 認為 (think), are collected manually from the earlier NTCIR corpus (Seki et al., 2008) for this task.  3.3 Opinion Holder Labeling Stage  Opinion holder labeling finds the text segment that represents the opinion holder. In the beginning, this task is also viewed as a binary classification problem for all words of a sentence, where the decision tree determines whether the current word is part of the opinion holder or not. CHAID decision tree algorithm provided by RapidMiner (Mierswa, Wurst, Klinkenberg, Scholz, & Euler, 2006) is adopted. It is a pruned decision tree using the chi-square test.  As the other alternative, we view the opinion holder labeling problem as a sequential  labeling problem. Therefore, the CRF algorithm (Lafferty, McCallum, & Pereira, 2001) is  selected to label whether each composite word is a portion of the opinion holder and CRF++  (Kudo, 2003) is adopted for implementation. Features for experiments are listed in Table 2.  Table 2. Features for opinion holder labeling  Feature Type Feature Name  Feature Description  Lexicon  fWord  The current Word  fPOS Part of speech fIsPronoun  Part of speech of the current word Is the current word a pronoun?  fIsNoun  Is the current word a noun?  fIsPer  Is the current word a person name?  Named entity fIsLoc  Is the current word a location name?  fIsOrg  Is the current word an organization name?  Punctuation mark  fAfterParen fBeforeColon  Does the current word appear one word after a parenthesis “」” or “】”? Does the current word appear one word before a colon “：” or “:”?  fNearSenStart  Is the current word close to the sentence head?  Sentential  fSenLen fWordOrder  The number of words in the current sentence The absolute position of the current word in the sentence  fWordPerc  The absolute position (in percentage) of the current word in the sentence  Identification of Opinion Holders  389  Context Opinion  fNearVerb fNearVerbPOS fDistNearVerb fHasOpKW fHasPosKW fHasNegKW fHasNeuKW fNearOpKW fNearPosKW fNearNegKW fNearNeuKW fNearOpKWPOS fNearPosKWPOS fNearNegKWPOS fNearNeuKWPOS fDistOpKW fDistPosKW fDistNegKW fDistNeuKW  The nearest verb in the current sentence The type (POS) of the nearest verb1, e.g., VA (transitive verb), VB (intransitive verb), etc. The distance between the current word and its nearest verb Is there an opinion operator in this sentence? Are there any positive opinion words in this sentence, e.g., 成功 (success), 同意 (agree)? Are there any negative opinion words in this sentence, e.g., 錯誤 (wrong), 失敗 (fail)? Are there any neutral opinion words in this sentence, e.g., 不予置評 (no comment), 兩難 (a difficult choice)? Nearest opinion word in this sentence Nearest positive opinion word in this sentence Nearest negative opinion word in this sentence Nearest neutral opinion word in this sentence POS of the nearest opinion operator POS of the nearest positive opinion word POS of the nearest negative opinion word POS of the nearest neutral opinion word The distance to the nearest opinion operator in this sentence POS of the nearest NTUSD positive opinion word in this sentence POS of the nearest NTUSD negative opinion word in this sentence POS of the nearest NTUSD neutral opinion word in this sentence  Features for opinion holder labeling include words, parts of speech, named entities, punctuation marks, sentential information, contextual information, and opinion related information. Some of the features are the same as those we have selected for the author’s opinion recognition. The lexicon feature is the current word to be determined. The part-of-speech features of the current word include its part of speech, and whether it is a noun or a pronoun. The binary properties of being a noun or a pronoun are emphasized here because they are the most commonly seen parts of speech in opinion holders. Punctuation marks also are considered as features here. Sentential features tell the position of the current word in the current sentence. They are included in the feature set because, according to our observations,  
In this paper, we introduce a POS tagging method for Taiwan Southern Min. We use the more than 62,000 entries of the Taiwanese-Mandarin dictionary and 10 million words of Mandarin training data to tag Taiwanese. The literary written Taiwanese corpora have both Romanized script and Han-Romanization mixed script, and include prose, novels, and dramas. We follow the tagset drawn up by CKIP. We developed a word alignment checker to assist with the word alignment for the two scripts. It searches the Taiwanese-Mandarin dictionary to find corresponding Mandarin candidate words, selects the most suitable Mandarin word using an HMM probabilistic model from the Mandarin training data, and tags the word using an MEMM classifier. We achieve an accuracy rate of 91.6% on Taiwanese POS tagging work, and we analyze the errors. We also discover some preliminary Taiwanese training data. Keywords: Taiwan Southern Min, POS tagging, written Taiwanese, Hidden Markov Model, Maximal Entropy Markov Model.  * Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan E-mail: {d93001, cykao}@csie.ntu.edu.tw + Institute of Information Science, Academia Sinica, Taipei, Taiwan E-mail: {glaxy, kchen}@iis.sinica.edu.tw # Independent scholar E-mail: kiatgak01@gmail.com  238  Un-Gian Iunn et al.  1. Introduction 1.1 Background There are about 46 million Southern Min speakers in the world. If we list languages by the size of their speaking population, Southern Min is ranked 21. The Southern Min speakers are mainly distributed in eight countries(Gordon, 2005). It is an important language that has received very little attention. The percentage of Southern Min speakers in Taiwan was over 70% (Huang, 1995). Taiwan has the highest percentage of Southern Min speakers in the world. We will call this language as “Taiwanese” for simplification in this paper. Many different types of written Taiwanese systems exist. Among these systems, the Han character script and one of the Romanized scripts (Pe̍h-ōe-jī, 白話字, abbrev. POJ, vernacular writing) are the most popular. Also, the mixture of the above two scripts, called the Han-Romanization mixed script (abbrev. as HR mixed script), has been adopted by many people (Iunn, 2009). 1.2 Motivation In order to establish the bases of written Taiwanese processing, we have constructed some tools over the past few years, including an online Taiwanese syllable dictionary (Iunn, 2003a); an online Taiwanese-Mandarin dictionary (abbrev. OTMD) (Iunn, 2000, 2003b); a 5,800,000 syllable HR mixed script and 3,400,000 syllable POJ script Taiwanese corpus; the online Taiwanese concordancer system based on this corpus (Iunn, 2003c; Iunn & Lau, 2007); preliminary Taiwanese word frequency reports for the Taiwanese POJ and HR mixed scripts, based on the above Taiwanese corpus (Iunn, 2005); the digital archive database for written Taiwanese (abbrev. DADWT) literature data with POJ and HR mixed script paragraph alignment (Iunn, 2007); etc. We intend to annotate the Taiwanese corpus with POS markers for more advanced applications, including Taiwanese tone sandhi TTS system improvement (Iunn et al. 2007), Taiwanese Treebank construction, etc. 1.3 Problem The primary difficulty encountered in the POS tagging of Taiwanese corpora is the question, “What is the Taiwanese POS tagset?” To date, no standard tagset for Taiwanese has been proposed. Under the circumstances, we have temporarily employed the Chinese POS tagset established by the CKIP Group of Academia Sinica (CKIP, 1993). Unfortunately, we still encountered some problems because we did not have a Taiwanese dictionary that contained  Modeling Taiwanese POS Tagging  239  Using Statistical Methods and Mandarin Training Data  the Mandarin POS tagset. The existing Taiwanese dictionaries merely contain basic vocabulary words, that is, nouns, verbs, adjectives, etc. Moreover, there was another problem to surmount – manpower shortage. We did not have enough manpower to fully execute the POS tagging of the Taiwanese corpora. Therefore, we proposed employing statistical procedures with the existing Mandarin resources and the OTMD to automatically complete the Taiwanese POS tagging. We used the Mandarin language model under the assumption that the word sequence in Taiwanese is similar to Mandarin. 1.4 Review Shi (2006) translated the Mandarin sentences in the book, “Modern Chinese 800 words ‘現代 漢語八百詞’ ” (by Shu-xiang Lü) into Taiwanese and Hakka to establish the T3 corpus and developed some editing tools to help in the construction of the T3 Treebank. Chou (2006) used the Brill tagger based on the HMM model to tag words in the T3 Treebank. They used a tagset size of 26, and attained tagging accuracy rates of 92.80% and 85.59% for the training and test data, respectively. T3 Treebank has not been released publicly. Thus, we decided to use different tagsets and different tagged corpora in our experiments. 2. POS Tagging Method Figure 1 shows our system architecture diagram.  Para. for para. alignment Taiwanese texts  Word alignment assistant Step 1  POS tagging  Step 4  OTMD  Search program  Word for word alignment Taiwanese texts Select the best Mandarin word  Step 2  Add Mandarin candidate words  Step 3  MEMM  Mandarin training data  HMM  Figure 1. Taiwanese POS Tagging System Architecture Diagram  240  Un-Gian Iunn et al.  At first, the text contains both POJ and HR mixed scripts with paragraph by paragraph alignment. Step 1 converts the texts to word alignment form. Step 2 adds the Mandarin candidate words (translations). Step 3 selects the best Mandarin translation using the HMM model. Finally, we decide the POS tagging of each word using the MEMM model. The following subsection will describe this process in detail. For example, the original texts are “Tâi-ôan tē-it kôan ê Gio̍k-san ê hū-kūn khah kē ê só.-chāi ... ” and “台灣 第一 懸 ê 玉山 ê 附近 較 低 ê 所在 ... ” Taiwan first high of Mt.Jade of nearby more low of place Step 1 converts the texts to word alignment form: “台灣/Tâi-ôan 第一/tē-it 懸/kôan ê/ê 玉山/Gio̍k-san ê/ê 附近/hū-kūn 較/khah 低/kē ê/ê 所在/só.-chāi … ” Then, Step 2 adds the Mandarin translations: “台灣/Tâi-ôan{台灣} 第一/tē-it{第一;絕頂} 懸/kôan{高} ê/ê{的} 玉山 /Gio̍k-san{玉山} ê/ê{的} 附近/hū-kūn{附近} 較/khah{較} 低/kē{低} ê/ê{的} 所在/só.-chāi(去 處; 地方;角頭;所在;處所;場所;間量} …” Step 3 selects the best Mandarin translation using the HMM model (we omit the original Taiwanese texts): “台灣 第一 高 的 玉山 的 附近 較 低 的 地方 …” Finally, Step 4 decides the POS tagging of each word using the MEMM model: “台灣/Tâi-ôan(Nc) 第一/tē-it(Neu) 懸/kôan(VH) ê/ê(DE) 玉山/ Gio̍k -san(Nc) ê/ê(DE) 附近/hū-kūn(Nc) 較/khah(Dfa) 低/kē(VH) ê/ê(DE) 所在/só.-chāi(Na)… ” We will illustrate our work with Figure 1 in the following subsections.  Modeling Taiwanese POS Tagging  241  Using Statistical Methods and Mandarin Training Data  2.1 Origin of the Corpus The corpus we chose is part of the DADWT project achievements of the National Museum of Taiwan Literature. It contains both POJ and HR mixed scripts with paragraph by paragraph alignment, including novels, prose, dramas, and poems (Iunn, 2007).  2.2 Word by Word Alignment First, we developed a word alignment program to aid manual processing. We arranged the word alignment of the two scripts, where the paragraphs were already aligned. This program not only collates the number of syllables in the two scripts, but it also compares and contrasts the two scripts with the entries of the OTMD. If the program does not find the two scripts within the same entry, it highlights the corresponding words to remind the user that the word may be an unknown word, an inconsistent usage of the Han character, or a typographical error. The OTMD was announced and has been online since 2000. The main data provider is Robert L. Cheng, but many anonymous contributors also offer entries and correct the typographical errors. There are a total of more than 62,000 entries. The URL is http://iug.csie.dahan.edu. tw/q. This dictionary offers POJ, HR mixed script, and Mandarin fields, with the POJ field also offering the different accents. The pronunciation function was added in 2006, and English translation was added to more than 10,000 entries in 2007 based on Embree (1984), which contains English, Mandarin, and POJ fields.  2.3 Finding the Corresponding Mandarin Candidate Words Next, we continued to search for the corresponding Mandarin candidate words from the POJ and HR mixed script word pairs via the OTMD. The mapping was one-to-many. In short, a Taiwanese word pair would have more than one Mandarin word counterpart. For example, “愛 /ài” in Taiwanese has the meanings of “愛”‘love (person),’ “喜歡”‘like (thing),’ “要” ‘want to,’ “需要”‘need to,’ etc. in Mandarin. Nevertheless, we were not able to find counterparts for certain words, since they were not contained in the OTMD. We also found some that had different HR mixed script usage. For instance, the Taiwanese word that appears as “較贏/khah-iâⁿ” ‘more than’ in the corpus appears as “khah 贏/khah-iâⁿ” in the dictionary. With regard to problems of this nature, we applied the following solution. If the POJ and HR mixed script word pair could not be found, we temporarily removed the HR mixed script and searched for the Mandarin word counterpart again using the POJ script. If the characters of HR mixed script were all Han characters, we regarded the Han characters as one of a Mandarin candidate word (assuming that the word is common to both Taiwanese and Mandarin).  242  Un-Gian Iunn et al.  This method might increase the number of the Mandarin candidate words, especially for single syllable words. For instance, the word pair “轉/chōan”‘turn’ appears in the text. We could not find an entry that contains both “轉” and “chōan” in the OTMD. The corresponding Mandarin translations of “chōan” in the dictionary are “扭”’twist’ and “上” ‘up’. We added “轉”‘turn’ as the supplementary Mandarin translation, but the meanings of these three words differ.  Table 1. Partial Entries of the OTMD  HR Mixed Script  POJ Script  Mandarin Translation  chōan  chōan  扭  撰  chōan  上  Note: There exists not “轉/chōan” entry in the OTMD. The Mandarin translation of “轉/chōan” will be “扭,” “上” and “轉”  If the strategy was still unable to find any results, the HR mixed script was directly recognized as the Mandarin candidate word. For instance, no dictionary entry was found for the word pair appearing as “有形/iú-hêng”‘tangible’ in the text, neither could one be found in the search using the POJ script “iú-hêng.” So, the HR mixed script “有形” was directly recognized as the Mandarin candidate word (Lau, 2007).  2.4 Selecting the Best Mandarin Translation We employed the Hidden Markov Model and Viterbi algorithm, and we made use of the bigram word training data of the ten-million word balanced Sinica corpus of the CKIP Group of Academia Sinica to select the most appropriate corresponding Mandarin word from the Mandarin candidate words. Figure 2 is an example. The selected words are boxed and bold.  Taiwanese Word  對/  古早/  以來/  琴/  Tùi  kó. –chá  í-lâi  khîm  有/  濟濟/  款/  ū  chē-chē khóan  ‘from’  ‘ago’  ‘since’ ‘instrument’ ‘has’ ‘many’ ‘appearance’  Corresponding Mandarin Word(s)  從 w11 對 w12 對子 w13 對於 w14  以前 w21 古代 w22 古時候 w23 從前 w24  以來 w31  琴 w41  有 w51  濟濟 w61 很多 w62  樣子 w71 樣式 w72 整理 w73  w1 = w11  w2 = w21  w3 = w31 w4 = w41 w5 = w51 w6 = w62 w7 = w71  Figure 2. An Example of Selecting the Best Mandarin Translation  Assume that a particular sentence contains m words. The first word, w1 , is selected from the candidate words of w11, w12 ,..., w1n1 ; the second word, w2 , is selected from the candidate words of w21, w22 ,..., w2n2 ; and the mth word, wm , is selected from the candidate  Modeling Taiwanese POS Tagging  243  Using Statistical Methods and Mandarin Training Data  words of wm1, wm2 ,..., wmnm . Sˆ = w1w2 "wm , which is the most probable word sequence, is selected from the candidate words, such that P( Sˆ = w1w2 "wm ) is maximized. The HMM assumes that the word wi is only influenced by the previous word wi−1 , thus:  ∏ P(Sˆ = w1w2 "wm ) ≅ P(w1) × m P(wi | wi−1)  (1)  i=2  Therefore, it searches for the word sequence Sˆ = w1w2 "wm , which maximizes  m  ∑ log P(w1) + log P(wi | wi−1)  (2)  i=2  We use the Laplace smoothing method to solve the problem of P(wi | wi−1) = 0 , where no bigram of wi−1wi could be found in the training data in other words. It should be noted that the word string Sˆ may not be a legal Mandarin sentence.  In practice, we use the Viterbi algorithm to eliminate repeated computation and reduce the time complexity from exponential time to polynomial time. If a sentence S has m words, and every word has n candidate words, the time complexity will be O(nm ) . The Viterbi algorithm reduces the time complexity to O(n2 × m) (Manning & Schütze, 1999).  2.5 Selecting the Most Appropriate POS According to the Corresponding Mandarin Word  We applied the Maximal Entropy Markov Model (MEMM) to the POS tag selection.  Manning and Schütze (1999) stated that “Maximum entropy modeling is a framework for integrating information from many heterogeneous information sources for classification. The data for a classification problem is described as a number of features. Each feature corresponds to a constraint on the model. …Choosing the maximum entropy model is motivated by the desire to preserve as much uncertainty as possible.”  MEMM includes a set of possible word and tag contexts, or “histories” (H), and the POS  tagging set (T):  k  ∏ p(h,t) = πμ  α f j (h,t ) j  (3)  j=1  where h ∈ H ,t ∈T , π is a normalization constant, {μ,α1,...,αk } are the positive model parameters, and {f1,..., fk } stands for the features f j (h,t) ∈ {0,1} . Parameter α j corresponds to the feature f j . The parameters {μ,α1,...,α k } are then chosen to maximize the likelihood of the training data using p:  n  n  k  ∏ ∏ ∏ L( p) =  p(hi ,ti ) =  πμ  α f j (hi ,ti ) j  (4)  i =1  i =1  j =1  244  Un-Gian Iunn et al.  As for the POS tag ti of the target word wi , we selected ten features including: (a) Words – five types of feature patterns: wi , wi−1, wi−2 wi−1, wi+1, wi+1wi+2 . (b) POS – two types of feature patterns: ti−1,ti−2ti−1 . (c) Morpheme – three types of feature patterns: m1, m2 , mn . The feature patterns m1, m2 , mn are designated to manipulate the unknown words. If wi is an unknown word, we segment wi with a maximal matching strategy; thus, wi = m1m2 "mn and, under certain circumstances, m2 = m3 = " = mn . If wi is a known word, the three morpheme features are set to null. Moreover, if wi is at the beginning or end of a sentence, certain features are likewise given a null value. For instance, when i=1, the feature values of wi−1, wi−2wi−1,ti−1,ti−2ti−1 , etc. are also null (Berger et al., 1996; McCallum et al., 2000; Rabiner, 1989; Ratnaparkhi, 1996; Samuelsson, 2003; Tai, 2007; Tsai & Chen, 2004). In MEMM, the dependencies of observations are flexibly modeled whereas HMM assumes that observations are independent. We think MEMM is more suitable for the POS tagging task. We used the “Maximum Entropy Modeling Toolkit for Python and C++” package provided by Zhang Le to implement our system (Le, 2003). The ten-million word POS tagged balanced Sinica corpus of the CKIP Group was used as the training data. Several million features were expanded from the ten features mentioned above, and the training time was about two days on Windows Server 2003 x64 SP2 with an Intel Xeon 3.2GHz processor (Quad-core), 8G DRAM.  3. Results  We used the aforementioned method to perform the Taiwanese POS tagging task; nevertheless,  as no standard answers were available to gauge the accuracy rate, we extracted partial results  and checked them manually. The primary consideration of the manual checking procedure was  the Chinese Word Segmentation and Tagging System of the CKIP group of Academia Sinica  (CKIP, 2004). We selected fourteen literary works belonging to three different eras – the  Ching Dynasty, the Japanese-ruled Period, and the Post-war Era. These literary works were in  the form of prose (seven), novels (five), and dramas (two). We selected the first paragraph  from each composition, or, if the length (number of syllables) of the first paragraph was less  than 60, we selected the second paragraph.  accuracy rate = (1− number of tagging errors ) 100%  (5)  number of total words  The test data list is shown in the Appendix. Table 2 shows the test data selected for manual checking. The number of syllables, words, and incorrectly selected Mandarin words,  Modeling Taiwanese POS Tagging  245  Using Statistical Methods and Mandarin Training Data  as well as the POS tagging inaccuracy of each paragraph are noted.  A total of 1,038 words (1,496 syllables) were selected, and manual checking showed that 90 words had been incorrectly selected and 87 words were found to have inaccurate POS tagging, thus placing the average POS tagging accuracy rate at 91.6%. It should be noted that sometimes, even when the corresponding Mandarin word selected was inappropriate, the POS tagging result was still accurate. On the other hand, an appropriate or correct corresponding Mandarin word did not always have accurate POS tagging.  Furthermore, sometimes one Taiwanese word would correspond to two Mandarin words. For instance, while the Taiwanese word “壁頂/piah-téng” ‘on the wall’ is treated as only one word, the Mandarin translation “牆壁 上” should be treated as two words. There are also occasions wherein two Taiwanese words would correspond to only one Mandarin word counterpart. For instance, the Mandarin counterpart of the Taiwanese words “Tiong-kok/中 國” ‘Chinese’ and “jī/字” ‘character’ was “中國字.” The former is processed as an unknown word, whereas the latter, which was separated into two independent words, was processed as two words. In these types of cases, if the POS tagging was accurate, we still regarded the results as accurate. If they were to be regarded as incorrect, the average accuracy rate would drop by around 2%.  Table 2. Tagging Accuracy Rate of The Test Data  id  No. of Syllables  No. of Words  Errors  Tagging errors  Accuracy rate(%)  
Researchers have developed many computational tools aimed at extracting collocations for both second language learners and lexicographers. Unfortunately, the tremendously large number of collocates returned by these tools usually overwhelms language learners. In this paper, we introduce a thesaurus-based semantic classification model that automatically learns semantic relations for classifying adjective-noun (A-N) and verb-noun (V-N) collocations into different thesaurus categories. Our model is based on iterative random walking over a weighted graph derived from an integrated knowledge source of word senses in WordNet and semantic categories of a thesaurus for collocation classification. We conduct an experiment on a set of collocations whose collocates involve varying levels of abstractness in the collocation usage box of Macmillan English Dictionary. Experimental evaluation with a collection of 150 multiple-choice questions commonly used as a similarity benchmark in the TOEFL synonym test shows that a thesaurus structure is successfully imposed to help enhance collocation production for L2 learners. As a result, our methodology may improve the effectiveness of state-of-the-art collocation reference tools concerning the aspects of language understanding and learning, as well as lexicography. Keywords: Collocations, Semantic Classification, Semantic Relations, Random Walk Algorithm, Meaning Access Index and WordNet.  ∗ CLCLP, TIGP, Academia Sinica, Taipei, Taiwan + Institute of Information Systems and Applications, NTHU, Hsinchu, Taiwan E-mail: {u901571, msgkate, smilet, jason.jschang}@gmail.com  258  Chung-Chi Huang et al.  1. Introduction Researchers have developed applications of computational collocation reference tools, such as several commercial collocation dictionary CD-ROMs, Word Sketch (Kilgarriff & Tugwell, 2001), TANGO (Jian et al., 2004), to answer queries (e.g., a search keyword “beach” for its adjective collocates) of collocation usage. These reference tools typically return collocates (e.g., adjective collocates for the pivot word “beach” are “rocky,” “golden,” “beautiful,” “raised,” “sandy,” “lovely,” “unspoiled,” “magnificent,” “deserted,” “fine,” “pebbly,” “splendid,” “crowded,” “superb,” etc.) extracted from a corpus of English texts (e.g., British National Corpus). Unfortunately, existing tools for language learning sometimes present too much information in a batch on a single screen. With corpus sizes rapidly growing to Web scale (e.g., Web 1 Trillion 5-gram Corpus), it is common to find hundreds of collocates for a query word. The bulk of information may frustrate and slow L2 learners’ progress of learning collocations. An effective language learning tool also needs to take into consideration second language learners’ absorbing capacity at one sitting. To satisfy the need for presenting a digestible amount of information at one time, a promising approach is to automatically partition collocations of a query word into various categories to support meaningful access to the search results and to give a thesaurus index to collocation reference tools. Consider the query “beach” in a search for its adjective collocates. Instead of generating a long list of adjectives like the above-mentioned applications, a better presentation could be composed of clusters of adjectives inserted into distinct semantic categories such as: {fine, lovely, superb, beautiful, splendid} assigned with a semantic label “Goodness,” {sandy, rocky, pebbly} assigned with a semantic label “Materials,” etc. Intuitively, by imposing a semantic structure on the collocations, we can bias the existing collocation reference tools towards giving a thesaurus-based semantic classification as one of the well-developed and convincingly useful collocation thesauri. We present a thesaurus-based classification system that automatically groups collocates of a given pivot word (here, the adjective collocates of a noun, the verb collocates of a noun, and the noun collocates of a verb) into semantically related classes expected to render highly useful applications in computational lexicography and second language teaching for L2 learners. A sample presentation for a collocation thesaurus is shown in Figure 1.  A Thesaurus-Based Semantic Classification of English Collocations  259  Figure 1. Sample presentation for the adjective collocate search query “beach”. Our thesaurus-based semantic classification model has determined the best semantic labels for 859 collocation pairs, focusing on: (1) A-N pairs and clustering over the adjectives (e.g., “fine beach”); (2) V-N pairs and clustering over the verbs (e.g., “develop relationship”); and (3) V-N pairs and clustering over the nouns (e.g., “fight disease”) from the specific underlying collocation reference tools (in this study, from JustTheWord). Our model automatically learns these useful semantic labels using the Random Walk Algorithm, an iterative graphical approach, and partitions collocates for each collocation types (e.g., the semantic category “Goodness” is a good thesaurus label for “fine” in the context of “beach” along with other adjective collocates such as “lovely,” “beautiful,” “splendid,” and “superb”). We describe the learning process of our thesaurus-based semantic classification model in more detail in Section 3. At runtime, we assign the most probable semantic categories to collocations (e.g., “sandy,” “fine,” “beautiful,” etc.) of a pivot word (e.g., “beach”) for semantic classification. In this paper, we exploit the Random Walk Algorithm to disambiguate word senses, assign semantic labels, and partition collocates into meaningful groups. The rest of the paper is organized as follows. We review the related work in the next section. Then, we present our method for automatic learning to classify collocations into semantically related categories, which is expected to improve the presentation of underlying collocation reference tools and support collocation acquisition by computer-assisted language learning applications for L2 learners (Section 3). As part of our evaluation, two metrics are designed with very little precedent of this kind. One, we assess the performance of resulting  260  Chung-Chi Huang et al.  collocation clusters by a robust evaluation metric; two, we evaluate the conformity of semantic labels by a three-point rubric test over a set of collocation pairs chosen randomly from the classifying results (Section 5). 2. Related Work Many natural language processing (NLP) applications in computational lexicography and second language teaching (SLT) build on one part of lexical acquisition emphasizing teaching collocation for L2 learners. In our work, we address an aspect of word similarity in the context of a given word (i.e., collocate similarity), in terms of use, acquisition, and ultimate success in language learning. This section offers the theoretical basis on which recommendations for improvements to the existing collocation reference tools are made, and it is made up of three major sections. In the first section, an argument is made in favor of collocation ability being an important part of language acquisition. Next, we show the need to change the current presentation of collocation reference tools. The final section examines other literature on computational measures for word similarity versus collocate similarity. 2.1 Collocations for L2 Learners The past decade has seen an increasing interest in the studies on collocations. This has been evident not only from a collection of papers introducing different definitions of the term “collocation” (Firth, 1957; Benson, 1985; Nattinger & DeCarrico, 1992; Nation, 2001), but also from the inclusive review of research on collocation teaching and the relation between collocation acquisition and language learning (Lewis, 1997; Hall, 1994). New NLP applications for extracting collocations, therefore, are a great boon to both L2 learners and lexicographers alike. SLT has long favored grammar and memorization of lexical items over learning larger linguistic units (Lewis, 2000). Nevertheless, several studies have shown the importance of acquisition of collocations; moreover, they have found specifically that the most important is learning the right verbs in verb-noun collocations (Nesselhauf, 2003; Liu, 2002). Chen (2004) showed that verb-noun (V-N) and adjective-noun (A-N) collocations were found to be the most frequent error patterns. Liu (2002) found that, in a study of English learners’ essays from Taiwan, 87% of miscollocations were attributed to the misuse of V-N collocations. Of those, 96% were due to the selection of the wrong verb. A simple example will suffice to illustrate: in English, one writes a check and also writes a letter while the equivalent Mandarin Chinese word for the verb “write” is “kai” (開) for a check and “xie” (寫) for a letter, but absolutely not “kai” (開) for a letter.  A Thesaurus-Based Semantic Classification of English Collocations  261  This type of language-specific idiosyncrasy is not encoded in either pedagogical grammars or lexical knowledge but is of utmost importance to fluent production of a language. 2.2 Meaning Access Indexing in Dictionaries Some attention has been paid to the investigation of the dictionary needs and reference skills of language learners (Scholfield, 1982; Béjoint, 1994), and one important cited feature is a structure to support users’ neurological processes in meaning access. Tono (1984) was among the first attempts to claim that the dictionary layout should be more user-friendly to help L2 learners access desired information more effectively. According to Tono (1992) in his subsequent empirical close examination of the matter, menus that summarize or subdivide definitions into groups at the beginning of entries in dictionaries would help users with limited reference skills to access the information in the dictionary entries more easily. The Longman Dictionary of Contemporary English, 3rd edition [ISBN 0-582-43397-5] (henceforth called LDOCE3), has just such a system called “Signposts”. When words have various distinct meanings, the LDOCE3 begins each sense anew with a word or short phrase which helps users more effectively discover the meaning they need. The Cambridge International Dictionary of English [ISBN 0-521-77575-2] does this as well, creating an index called “Guide Word" which provides similar functionality. Finally, the Macmillan English Dictionary for Advanced Learners [ISBN 0-333-95786-5], which has “Menus” for heavy-duty words with many senses, utilizes this approach as well. Therefore, in this paper, we introduce a classification model for imposing a thesaurus structure on collocations returned by existing collocation reference tools, aiming at facilitating concept-grasping of collocations for L2 learners. 2.3 Similarity of Semantic Relations The construction of practical, general word sense classification has been acknowledged to be one of the most difficult tasks in NLP (Nirenburg & Raskin, 1987), even with a wide range of lexical-semantic resources such as WordNet (Fellbaum, 1998) and Word Sketch (Kilgarriff & Tugwell, 2001). Lin (1997) presented an algorithm for word similarity measured by its distributional similarity. Unlike most corpus-based word sense disambiguation (WSD) algorithms, where different classifiers are trained for separate words, Lin used the same local context database as the knowledge source for measuring all word similarities. Approaches presented to recognize synonyms have been studied extensively (Landauer & Dumais, 1997; Deerwester et al., 1990; Turney, 2002; Rehder et al., 1998; Morris & Hirst, 1991; Lesk, 1986). Measures of recognizing collocate similarity, however, are not as well developed as measures of word similarity.  262  Chung-Chi Huang et al.  The most closely related work focuses on automatically classifying semantic relations in noun pairs (e.g., mason:stone) and evaluation with a collection of multiple-choice word analogy question from the SAT exam (Turney, 2006). Another related approach, presented in Nastase and Szpakowicz (2003), describes how to automatically classify a noun-modifier pair, such as “laser printer,” according to the semantic relation between the head noun (printer) and the modifier (laser). The evaluation is manually conducted by human labeling. For a review of work to a more fine-grained word classification, Pantel and Chklovski (2004) presented a semi-automatic method for extracting fine-grained semantic relations between verbs. VerbOcean (http://semantics.isi.edu/ocean/) is a broad-coverage semantic network of verbs, detecting similarity (e.g., transform::integrate), strength (e.g., wound::kill), antonymy (e.g., open::close), enablement (e.g., fight::win), and temporal happens-before (e.g., marry::divorce) relations between pairs of strongly associated verbs using lexico-syntactic pattern over the Web. Hatzivassiloglou and McKeown (1993) presented a method towards the automatic identification of adjectival scales. Based on statistical techniques with linguistic information derived from the corpus, the adjectives, according to their meaning based on a given text corpus, can be placed in one group describing different values of the same property. Their clustering algorithm suggests some degree of adjective scalability; nevertheless, it is interesting to note that the algorithm discourages recognizing the relationship among adjectives, e.g., missing the semantic associations (for example a semantic label of “time associated”) between new-old. More recently, Wanner et al. (2006) sought to semi-automatically classify the collocations from corpora via the lexical functions in dictionary as the semantic typology of collocation elements. While there is still a lack of fine-grained semantically-oriented organization for collocation, WordNet synset (i.e., synonymous words in a set) information can be explored to build a classification scheme for refinement of the model and develop a classifier to measure the distribution of class for the new tokens of words set foot in. Our method, which we will describe in the next section, uses a similar lexicon-based approach for a different setting of collocation classification. 3. Methodology 3.1 Problem Statement We focus on the preparation step of partitioning collocations into categories for collocation reference tools: providing words with semantic labels, thus, presenting collocates under thesaurus categories for ease of comprehension. The categorized collocations are then returned in groups as the output of the collocation reference tool. It is crucial that the collocation categories be fairly consistent with human judgment and that the categories of collocates cannot be so coarse-grained that they overwhelm learners or defeat the purpose of users’ fast access. Therefore, our goal is to provide semantic-based access to a well-founded collocation  A Thesaurus-Based Semantic Classification of English Collocations  263  thesaurus. The problem is now formally defined.  Problem Statement: We are given (1) a set of collocates Col = {C1, C2, …, Cn} (e.g., “sandy,” “beautiful,” “superb,” “rocky,” etc.) with corresponding parts-of-speech P={p| p ∈ Pos and Pos={noun,adjective,verb}} for a pivot word X (e.g., “beach”); (2) a combination of thesaurus categories (e.g., Roget’s Thesaurus), TC = {(W, P, L)} where a word W with a part-of-speech P is under the general-purpose semantic category L (e.g., feelings, materials, art, food, time, etc.); and (3) a lexical database (e.g., WordNet) as our word sense inventory SI for semantic relation population. SI is equipped with a measure of semantic relatedness: REL(S, S’) encodes semantic relations holding between word sense S and S’.  Our goal is to partition Col into subsets of similar collocates by means of integrated semantic knowledge crafted from the mapping of TC and SI, whose elements are likely to express related meanings in the same context of X. For this, we leverage a graph-based algorithm to assign the most probable semantic label L to each collocation, thus giving collocations a thesaurus index. For the rest of this section, we describe our solution to this problem. In the first stage of the process, we introduce an iterative graphical algorithm for providing each word with a word sense (Section 3.2.1) to establish integrated semantic knowledge. A mapping of words, senses, and semantic labels is thus constructed for later use of automatic collocation partitioning. In the second stage (Section 3.2.2), to reduce out-of-vocabulary (OOV) words in TC, we extend word coverage of limited TC by exploiting a lexical database (e.g., WordNet) as a word sense inventory, encoding words grouped into cognitive synonym sets and interlinked by semantic relations. In the third stage, we present a similar graph-based algorithm for collocation labeling using the extended TC and Random Walk on a graph in order to provide a semantic access to collocation reference tools of interest (Section 3.3). The approach presented here is generalizable to allow construction from any underlying semantic resource. Figure 2 shows a comprehensive framework for our unified approach.  A Thesaurus  Word Sense Inventory (e.g., WordNet)  Uncategorized Collocates  Random Walk on Word Sense Assignment  Random Walk on Semantic Label Assignment  A Collocation Thesaurus  Integrated Semantic Knowledge (ISK) Extension  Enriched ISK  Figure 2. A comprehensive framework for our classification model.  264  Chung-Chi Huang et al.  3.2 Learning to Build a Semantic Knowledge by Iterative Graphical Algorithms In this paper, we attempt to provide each word with a semantic label and attempt to partition collocations into thesaurus categories. In order to partition a large-scale collocation input and reduce the out-of-vocabulary (OOV) encounters for the model, we first incorporate word sense information in SI, into the thesaurus, i.e., TC, and extend the former integrated semantic knowledge (ISK) using semantic relations provided in SI. Figure 3 outlines the aforementioned process.  (1) Build an Integrated Semantic Knowledge (ISK) by Random Walk on Graph (Section 3.2.1) (2) Extend Word Coverage for Limited ISK by Lexical-Semantic Relations (Section 3.2.2) Figure 3. Outline of the learning process of our model.  3.2.1 Word Sense Assignment In the first stage (Step (1) in Figure 3), we use a graph-based sense linking algorithm which automatically assigns appropriate word senses to words under a thesaurus category. Figure 4 shows the algorithm.  Algorithm 1. Graph-based Word Sense Assignment Input: A word list, WL, under the same semantic label in the thesaurus TC; A word sense inventory SI. Output: A list of linked word sense pairs, {(W, S* )} Notation: Graph G = {V, E} is defined over admissible word senses (i.e., V) and their semantic relations (i.e., E). In other words, each word sense S constitutes a vertex v ∈ V while a semantic relation between senses S and S’ (or vertices) constitutes an edge in E. Word sense inventory SI is organized by semantic relations SR and REL(S,S’) identifies the semantic relations between sense of S and S’ in SI. PROCEDURE AssignWordSense(WL,SI)  Build weighted graph G of word senses and semantic relations  INITIALIZE V and E as two empty sets  FOR each word W in WL  FOR each of the n(W) admissible word senses, S, of W in SI  (1)  ADD node S to V  FOR each node pair (S,S’), where S and S’ belong to different words, in V × V  (2)  IF ( REL(S,S’) ≠ NULL and S ≠ S’ THEN ADD edge E(S,S’) to E and E(S’,S) to E  FOR each word W AND each of its word senses S in V  (3)  INITIALIZE Ps = 1/n(W) as the initial probability  A Thesaurus-Based Semantic Classification of English Collocations  265  (3a)  ASSIGN weight (1-d) to matrix element MS,S  (3b)  COMPUTE e(S) as the number of edges leaving S  FOR each other word W’≠ W in WL AND each sense S’ of W’  (3c)  IF there is an edge between S and S’ THEN ASSIGN Weight d/e(S) to MS,S’  OTHERWISE ASSIGN 0 to MS,S’  Score vertices in G  REPEAT  FOR each word W AND each of its word senses S  (4)  INTIALIZE QS to PS × MS,S  FOR each other word W’≠W in WL AND each sense S’ of W’  (4a)  INCREMENT QS by PS’ × MS’,S  FOR each word W, SUM QS over n(W) senses as Nw  FOR each word W AND each of its word senses S  (4b)  REPLACE PS by QS/Nw  UNTIL probability PS‘s converge  Assign word sense  (5) INITIALIZE List as NULL  FOR each word W in WL  (6)  APPEND (W,S*) to List where PS* is the maximum among senses of W  (7) OUTPUT List  Figure 4. Algorithm for Graph-based Word Sense Assignment.  The algorithm for the best sense assignment S* for W consists of three main parts: (1) construction of a weighted word sense graph; (2) sense scoring using the iterative Random Walk algorithm; and (3) word sense assignment.  In Step 1 of the algorithm, by referring to SI, we populate candidate n(W) senses for each word W in the word list, WL, under the same semantic category as vertices in graph G. In G, directed edges E(S,S’) and E(S’,S) are built between vertex S and vertex S’ if and only if there exists a semantic relation between the word sense S and S’ in SI. Figure 5 shows an example of such a graph.  S5 S4 S3 S2 S1  beautiful  fine  splendid  Figure 5. Sample graph built on the admissible word senses (vertical axis) for three words (horizontal axis) under the thesaurus category of “Goodness”. Note that self-loop edges are omitted for simplicity.  266  Chung-Chi Huang et al.  We initialize the probability concerning the sense S of a word W, Ps, to 1/n(W), uniform distribution among the senses of W (Step (3)). For example, in Figure 5, the probability of the fourth sense of the word “beautiful” is initialized to 0.2. Then, we construct a matrix, whose element Mx,y stands for the proportion of the probability Px , that will be propagated to node y. Since Mx,y may not be equal to M,y,x, the edges in G are directed. In matrix M, we assign 1-d to Mx,x where x ∈ V(Step (3a)) while the rest of the proportion (i.e., d) is uniformly distributed among the outgoing edges of the node x (Step (3c)). Take the fourth sense (Node 4 for short) of the word “beautiful” and the third sense (Node 8 for short) of the word “fine” in Figure 5 for example. M4,8 is d/2 since there are two outgoing edges for Node 4. On the other hand, M8,4 is d/3 in that there are three edges leaving Node 8. d is the damping factor and was first introduced by PageRank (Brin & Page, 1998), a link analysis algorithm. The damping factor is usually set around 0.85, indicating that eighty-five percent of the probability of a node will be distributed to its outbound nodes. In the second part of the algorithm, probabilities will be iteratively re-distributed among the senses of words until convergence of probabilities. For each sense S of a word W, first, (Step (4)) Qs is assigned to Ps × Ms,s (i.e., some proportion, Ms,s, of the probability of Ps is propagated to the node s), then (Step (4a)) Qs is incremented by Ps’ × Ms’,s, the ingoing probability propagation from node s’, whenever there is an edge between s’ and s. In Step (4b), we re-calculate the probability of the sense S, Ps, by dividing Qs by ∑ Qs′ , s′∈sense(W ) where S and S’ are different word senses of the same word W and sense(W) is the set of admissible senses of W in SI for the next iteration. ∑ Qs′ , or Nw in the algorithm, s′∈sense(W ) is the normalization factor. The propagation of probabilities at each iteration in this graph-based algorithm, or Random Walk Algorithm, ensures that if a node is semantically1 linked to another node with high probability, it will obtain quite a few probabilities from that node, indicating that this node may be important2 in that probabilities converse and tend to aggregate in senses (i.e., nodes) of words that are semantically related (i.e., connected). Finally, for each word, we identify the most probable sense and attach the sense to it (Step (6)). For instance, for the graph in Figure 6, the vertex on the vertical axis represented as the sense #3 of “fine” will be selected as the best sense for “fine” under the thesaurus category “Goodness” with other entry words, such as, “lovely,” “superb,” “beautiful,” and “splendid”. The output of this stage is a set of linked word sense pairs (W, S*) that can be utilized to extend the coverage of thesauri via semantic relations in SI. 
Code-mixing is a common phenomenon in bilingual societies. It refers to the intra-sentential switching of two different languages in a spoken utterance. This paper presents the first study on automatic recognition of Cantonese-English code-mixing speech, which is common in Hong Kong. This study starts with the design and compilation of code-mixing speech and text corpora. The problems of acoustic modeling, language modeling, and language boundary detection are investigated. Subsequently, a large-vocabulary code-mixing speech recognition system is developed based on a two-pass decoding algorithm. For acoustic modeling, it is shown that cross-lingual acoustic models are more appropriate than language-dependent models. The language models being used are character tri-grams, in which the embedded English words are grouped into a small number of classes. Language boundary detection is done either by exploiting the phonological and lexical differences between the two languages or is done based on the result of cross-lingual speech recognition. The language boundary information is used to re-score the hypothesized syllables or words in the decoding process. The proposed code-mixing speech recognition system attains the accuracies of 56.4% and 53.0% for the Cantonese syllables and English words in code-mixing utterances. Keywords: Automatic Speech Recognition, Code-mixing, Acoustic Modeling, Language Modeling 1. Introduction Code-switching and code-mixing are common phenomena in bilingual societies. According to John Gumperz (Gumperz, 1982), the definition of code-switching is “the juxtaposition within the same speech exchange of passages of speech belonging to two different grammatical  ∗ Department of Electronic Engineering, The Chinese University of Hong Kong E-mail: {ycchan, hwcao, pcching, tanlee}@ee.cuhk.edu.hk  282  Joyce Y. C. Chan et al.  systems or sub-systems”. Different combinations of languages are found in code-switching, for examples, Spanish-English in United States, German-Italian and French-Italian in Switzerland, and Hebrew-English in Israel (Auer, 1998). In Taiwan, code-switching between Chinese dialects, namely Mandarin and Taiwanese, has become common in recent years (Chen, 2004). Hong Kong is an international city where many people, especially the younger generation, are Cantonese and English bilinguals. English words are frequently embedded into spoken Cantonese. The switching of language tends to be intra-sentential, and it rarely involves linguistic units above the clause level. Hence, the term code-mixing is usually preferred (Li, 2000). In this case, Cantonese is the primary language, also known as the matrix language, and English is the secondary language, usually referred to as the embedded language (Halmari, 1997). Automatic speech recognition (ASR) is one of the key technologies in spoken language processing. An ASR system converts an input speech waveform into a sequence of words. Recently, ASR for multilingual applications has attracted great interest (Schultz & Kirchhoff, 2006). In state-of-the-art ASR systems, the input speech is assumed to contain only one language and the language identity is given. These systems are not able to handle code-mixing speech, which differs significantly from monolingual speech spoken by native speakers. This calls for special consideration in the design of acoustic models, lexical and language models, and in the decoding algorithm. There have been two different approaches to code-switching or code-mixing speech recognition (Lyu et al., 2006; Chan et al., 2006). The first approach involves a language boundary detection (LBD) algorithm that divides the input utterance into language-homogeneous segments. The language identity of each segment is determined, and the respective monolingual speech recognizer is applied. LBD for mixed-language utterances was studied by Wu et al. (2006) and Chan et al. (2004). Language-specific phonological and acoustic properties were used as the primary cues to identify the languages. The second approach aims to develop a cross-lingual speech recognition system, which can handle multiple languages in a single utterance. The acoustic models, language models, and pronunciation dictionary are designed to be multi-lingual and cover all languages concerned. In Lyu et al. (2006), automatic recognition of Mandarin-Taiwanese code-switching speech was investigated. It was found that Mandarin and Taiwanese, both of which are Chinese dialects, share a large percentage of lexicon items. Their grammar was also assumed to be similar. A one-pass recognition algorithm was developed using a character-based search net. It was shown that the one-pass approach outperforms LBD-based multi-pass approaches. In You et al. (2004), a mixed-lingual keyword spotting system was developed for auto-attendant applications. The keywords to be detected could be in either English or Chinese. This paper presents a study on automatic speech recognition of Cantonese-English  Automatic Recognition of Cantonese-English Code-Mixing Speech  283  code-mixing speech. Part of the work was reported in Chan et al. (2006). Our study covers all components of an ASR system, including acoustic models, language models, pronunciation dictionary, and search algorithm. Different approaches to LBD are also investigated. By understanding the linguistic properties of monolingual Cantonese and English, as well as code-mixing speech, the major difficulties in code-mixing speech recognition are revealed and possible solutions are suggested. We propose a two-pass recognition system, in which the acoustic and linguistic knowledge sources are integrated with language boundary information. Simulation experiments are carried out to evaluate the performance of the whole system as well as individual system components. 2. Difficulties in Code-mixing Speech Recognition  2.1 Linguistic Properties of Cantonese and English  Cantonese is a Chinese dialect. It is spoken by tens of millions of people in the provinces of Guangdong, Guangxi, Hong Kong, and Macau. A Chinese word in its written form is composed of a sequence of characters. In Cantonese, each Chinese character is pronounced as a monosyllable carrying a specific lexical tone (Ching et al., 2006). English is one of the most popular languages in the world. An English word is written as a sequence of letters. In spoken form, each word may consist of several syllables, some of which are designated to be stressed. Table 1 shows a pair of example words in Cantonese and English.  Table 1. Examples of Cantonese and English words in written and spoken format.  Written (orthographic transcription) Spoken (phonetic transcription)  產生  /ts a n/ /s  /  produce  /p r ´ »d j u˘ s/  Syllables can be divided into smaller units, namely consonants (C) and vowels (V). Cantonese syllables take the structures of V, CV, CVC, or VC (Ching et al., 1994). If tonal difference is not considered, the number of distinct Cantonese syllables is around 600 (Ching et al., 2006). The syllable structure in English is more complicated than that in Cantonese. Although many English syllables share the same canonical forms as given above, there also exist combinations like CCV, VCC, CCCV, and CCCVCC (Wester, 2003), which are not found in Cantonese. There are 22 consonants and 22 vowels (including diphthongs) in Cantonese, and 24 consonants and 14 vowels in American English (Ching et al., 1994; Ladefoged, 1999). Table 2 lists the IPA (International Phonetic Alphabet) symbols of these phonemes. Some of the phonemes in the two languages are labeled with the same IPA symbols by phoneticians, meaning that they are phonetically very close. Some of the other phonemes are also  284  Joyce Y. C. Chan et al.  considered to be very similar although they are labeled differently in the two languages, e.g., /au/ in Cantonese and /aU/ in English. Table 2. Phonemes of Cantonese and English. The phonemes that are labeled with the same IPA symbols in both Cantonese and English are listed first and boldfaced.  Cantonese phonemes  IPA symbol Example  p  [p a] (爸)  m  [m a] (媽)  f  [f a] (花)  t  [t a] (打)  tS  [tS y] (朱)  n  [n a] (拿)  s  [s a] (沙)  S  [S y] (書)  l  [l a] (啦)  j  [j åu] (憂)  k  [k a] (加)  N  [pH a N] (烹)  w  [w a] 蛙  h  [h a] (蝦)  I  [s I k] (色)  i  [s i] (絲)  E  [s E] (借)  U  [s U N] (鬆)  u  [f u] (夫)  pH  [pH a] (扒)  tH  [tH a] (他)  ts  [ts i] (之)  tsH  [tsH i] (痴)  tSH  [tSH y] (處)  kH  [kH a] (卡)  kW  [kW a] (瓜)  kWH  [kWH a] (誇)  y  [S y] (書)  ø  [h ø] (靴)  a  [s a] (沙)  å  [s å p] (濕)  P  [s P t] (恤)  ç  [s ç] (梳)  ei  [h ei] (稀)  Eu  [t Eu] (投)  ai  [w ai] (威)  Py  [s Py] (衰)  åi  [s åi] (西)  ui  [f ui] (灰)  iu  [s iu] (燒)  åu 
Taking Mandarin Possessive Construction (MPC) as an example, the present study investigates the relation between lexicon and constructional schemas in a quantitative corpus linguistic approach. We argue that the wide use of raw frequency distribution in traditional corpus linguistic studies may undermine the validity of the results and reduce the possibility for interdisciplinary communication. Furthermore, several methodological issues in traditional corpus linguistics are discussed. To mitigate the impact of these issues, we utilize phylogenic hierarchical clustering to identify semantic classes of the possessor NPs, thereby reducing the subjectivity in categorization that most traditional corpus linguistic studies suffer from. It is hoped that our rigorous endeavor in methodology may have far-reaching implications for theory in usage-based approaches to language and cognition. Keywords: Discourse-functional Grammar, Construction Grammar, Quantitative Corpus Linguistics, Possession, Clustering. 1. Introduction It has been observed that grammatical structures or patterns often serve as routinized formats, fulfilling specific communicative purposes in our daily interaction (Biq, 2001; Chui, 2000; Huang, 2003; Ono & Thompson, 1996; Tao & Thompson, 1994; Thompson & Couper-Kuhlen, 2005; Thompson & Hopper, 2001; Wray, 2002). Speakers’ knowledge of their native languages is argued to consist of “a structured inventory of conventional linguistic units, a unit 
This paper concerns a framework for building interactive speech-based language learning games. The core of the framework, the “dialogue manager,” controls the game procedure via a control script. The control script allows the developers to have easy access to the natural language process capabilities provided by six core building blocks. Using the framework, three games for Mandarin learning were implemented: a reading game, a translation game, and a question-answering game. We verified the effectiveness and usefulness of the framework by evaluating the three games. In the in-lab and public evaluation phases, we collected a total of 4025 utterances from 31 subjects. The evaluation showed that the game systems responded to the users’ utterances appropriately about 89% of the time, and assessment of the users’ performances correlated well with their human-judged proficiency. Keywords: Computer Aided Language Learning, Machine Translation, Automatic Question Generation, Automatic Answer Judging 1. Introduction Computer aids for second language learning have long been a promising yet difficult research topic. Despite much argument about the best way to teach a second language based on pedagogy, the most natural and effective source of second language education is the classroom and human tutors. Statistics, however, have shown a severe shortage of language teachers, compared to the number of language learners. For example, the current estimated number of Chinese language teachers worldwide is around 40,000, while the number of people trying to  * Spoken Language Systems Group, MIT Computer Science and Artificial Intelligence Laboratory, USA E-mail: {yushixu, seneff}@csail.mit.edu [Received July 3, 2009; Revised October 7, 2009; Accepted October 22, 2009]  134  Yushi Xu, and Stephanie Seneff  learn Chinese is about 1,000 times that1. The dramatic difference in the numbers not only results in many students not having a chance to find a suitable teacher, but also results in an under-emphasis on spoken communication, which many pedagogists agree to be an important skill, and which cannot be practiced by the student alone. Given this situation, it is natural to think of replacing a costly human tutor with a computer. Several criteria, however, must be satisfied for such a machine tutor to be interesting to the students. The computer needs to understand the student’s speech, and act intelligently enough to avoid being perceived as just an e-textbook. It should be able to offer a variety of activities, and to constantly provide rewards in order to motivate students to invest further effort to improve their skill level. In an attempt to meet these requirements, we have developed a versatile framework for building speech-based language learning games. The core of the framework is a dialogue manager, which is supported by a set of building blocks, each providing some high-level natural language processing operations. By combining these operations in different ways using a control script, we have implemented three distinct games in two domains. The three games, a reading game, a translation game, and a question-answering game, provide different types of challenges to beginner learners of Mandarin Chinese. The two domains, general travel and flights, expose the students to different sentence patterns and vocabulary. The language processing operations provided by the building blocks are general-purpose, and the control script can be viewed as a high-level programming language. The whole framework thus makes it relatively straightforward to develop other speech-based language learning games, or to export the existing games to other domains of interest with minimal effort. This paper will be organized as follows. We will first summarize some related work in Section 2. In Section 3, we will give a brief introduction of our three games. Then, in Section 4, the dialogue manager and its core building blocks will be described. Section 5 will describe the implementation of the three games in more detail, followed by their evaluations in Section 6. We will conclude and point to some future work in Section 7. 2. Related Work There has been a significant amount of previous research in the computer aided language learning (CALL) field. Most of the research has a single focus, for example, vocabulary training (Brown, Frishkoff, & Eskenazi, 2005), or reading comprehension tests (Kunichika, Katayama, Hirashima, & Takeuchi, 2003). Only a few systems have been designed to provide alternative types of activities. Many of these integrated systems have been packaged as a CD-ROM as a delivery mechanism. The software is then installed on a local machine for 
Many ESL students need to improve writing skills to pass various language tests; thus, writing teachers need to read many compositions and provide feedback. To help ESL teachers reduce their teaching load and to give students faster feedback, various English grammar checkers have been developed. Few of these PC-based grammar checkers, however, are widely available to ESL learners. As the Internet has become an important tool for language education, web-based grammar checkers have begun to emerge. In this paper, we first introduce two new web-based grammar checkers (Microsoft ESL Assistant and NTNU statistical grammar checker) and then compare their performance. Ten common EFL errors selected from a large Chinese EFL learner corpus were used to test these two grammar checkers. The test results showed that the NTNU statistical checker was far more sensitive to various learner errors, and it could detect eight types of selected errors. Microsoft ESL Assistant could only deal with five types of errors. Moreover, these two checkers both could not deal with fragments and run-on sentences errors. It seems clear that both checkers have room for improvement before they can provide satisfactory service to ESL learners. The Microsoft ESL Assistant should expand its coverage to detect more learner errors. NTNU checker should reduce false alarms and indicate the locations of errors more accurately. Learner errors are indeed complicated for developers of grammar checkers, but the strong need for a functional grammar checker deserves CALL researchers’ special attention. Keywords: ESL Writing, Errors, Grammar Checker, Ngrams, Rules.  ∗ English Department, National Taiwan Normal University E-mail: hjchen@ntnu.edu.tw [Received August 9, 2009; Revised November 15, 2009; November 14, 2009]  162  Hao-Jan Howard Chen  1. Introduction It is challenging for second language learners to become proficient writers of the target language. Learners need considerable writing practices before they can write accurately and fluently. In addition to providing more writing opportunities to students, many teachers and researchers believe that learners need to receive proper corrective feedback on their writings (Ferris, 1999, 2003, 2006). If learners only keep on writing and do not receive any corrective feedback, they will not be able to make progress quickly. Even though the role of corrective feedback in second language learning remains controversial, many teachers and learners firmly believe that feedback plays an important role in second language writing (cf. Ferris, 1999, 2003, 2006; Truscott, 1996, 1999, 2007; Goldstein, 2006; Guenette, 2007). In many ESL/EFL settings, writing teachers need to work with 40 or 50 students in their classes. Reading and correcting students’ essays is a great burden for many writing teachers. To reduce teachers’ loads in correcting common errors and to help students enhance their writing accuracy, various grammar checking tools have been developed in different countries. Many CALL researchers consider the development of grammar checkers to be part of the Intelligent CALL research (cf. Holland et al., 1995). The development of a useful grammar checker to identify and correct learners’ errors has been considered a very important research direction in CALL. However, most of the English grammar checkers developed by academic institutes could only deal with a limited set of grammar errors and were not made available to ESL students (e.g., Liou, 1991, 1992, 1993; More, 2006; Naber, 2003; Park, Palmer, & Washburn, 1997). Some commercial PC-based grammar checkers (e.g. Whitesmoke) are available, but their error detecting capacities are still limited according to some recent evaluation studies (Chiu, 2008). With the rapid development of artificial intelligence and natural language processing technologies, several automated essay scoring programs (e.g., Vantage My Access and ETS Criterion) have appeared recently in the ESL market (Attali, 2004; Burstein, Chodorow & Leacock, 2004; Elliot, 2001; Elliot & Mikulas, 2004; Han, Chodorow & Leacock, 2006; Higgins, Burstein & Attali, 2006). Grammar checkers are also included in these two leading writing tools. By subscribing to these commercial programs, students can choose from a wide range of practice essays topics to write multiple drafts and receive immediate corrective feedback in the form of both holistic scores and diagnostic comments on grammar, organization, style, and usage. These commercial software packages, however, are expensive, and ESL students who subscribe to these services can only use these programs for 3-6 months during the subscription period. Because of these limitations and high prices, many ESL/EFL students still do not have access to any of these programs. The better way of helping a large number of students is to  Evaluating Two Web-based Grammar Checkers –  
This paper presents an initial attempt to examine whether Rhetorical Structure Theory (RST) (Mann & Thompson, 1988) can be fruitfully applied to the detection of the coherence errors made by Taiwanese low-intermediate learners of English. This investigation is considered warranted for three reasons. First, other methods for bottom-up coherence analysis have proved ineffective (e.g., Watson Todd et al., 2007). Second, this research provides a preliminary categorization of the coherence errors made by first language (L1) Chinese learners of English. Third, second language discourse errors in general have received little attention in applied linguistic research. The data are 45 written samples from the LTTC English Learner Corpus, a Taiwanese learner corpus of English currently under construction. The rationale of this study is that diagrams which violate some of the rules of RST diagram formation will point to coherence errors. No reliability test has been conducted since this work is at an initial stage. Therefore, this study is exploratory and results are preliminary. Results are discussed in terms of the practicality of using this method to detect coherence errors, their possible consequences about claims for a typical inductive content order in the writing of L1 Chinese learners of English, and their potential implications for Automated Writing Evaluation (AWE) software, since discourse organization is one of the essay characteristics assessed by this software. In particular, the extent to which the kinds of errors detected through the RST analysis match those located by Criterion (Burstein, Chodorow, & Leachock, 2004), a well-known AWE software by Educational Testing Service (ETS), is discussed. ∗ Graduate Institute of Linguistics, National Taiwan University E-mail: sophiaskoufaki@ntu.edu.tw [Received August 10, 2009; Revised December 6, 2009; Accepted December 22, 2009]  182  Sophia Skoufaki  Keywords: Automated Writing Evaluation, Discourse Organization, Coherence Errors, Rhetorical Structure Theory. 1. Introduction Research findings indicate that English language learners produce various kinds of discourse errors in their writing, such as inductive patterns (e.g., Kaplan, 1966) and inappropriate coordination (e.g., Soter, 1988). However, the discourse errors of second language (L2) learners of English have not been examined in detail partly because at least some of them are more difficult to detect than other kinds of errors (e.g., syntactic, spelling). This paper describes an initial attempt to examine whether Rhetorical Structure Theory (RST) (Mann & Thompson, 1988) can be fruitfully applied to the detection of the coherence errors made by Taiwanese low-intermediate learners of English. In particular, this paper reports on a pilot study where 45 written samples from the LTTC English Learner Corpus, a Taiwanese learner corpus of English currently under construction, were analysed according to RST. It is hoped that this pilot study will provide some preliminary indication of the viability of this approach to coherence error detection. The results of this analysis will also serve as a preliminary list of coherence errors which may prove typical or not in further large-scale studies of this kind. A categorization of second language (L2) English coherence errors in general and of the coherence errors of particular learner populations has not been provided yet by applied linguists. Therefore, this pilot study is warranted because of its possible utility for research on English L2 discourse and the instruction of writing in English as an L2. Another aim of this study is to examine whether the most frequent of the errors detected through the RST analysis can be located by Criterion, a well-known AWE software by the Educational Testing Service (ETS). Automated Writing Evaluation (AWE) software such as Criterion (e.g., Burstein, Chodorow, & Leachock, 2004) and My Access! (e.g., Vantage Learning, 2007) locate and give diagnostic feedback only for a limited number of discourse errors. This issue has been pointed out by the computational linguists involved in the creation of AWE software (e.g., Higgins, Burstein, Marcu, & Gentile, 2004), but no study has been conducted with specific English learner populations to examine what discourse errors should be added to the inventory of discourse errors currently located via AWE software. Being a pilot study, the study reported here does not purport to fill this research gap but only to provide an initial step towards this goal. In the following two sections, this paper will offer further information on the motivation of this study. Then, it will offer some background information on RST. Third, it will provide an overview of the LTTC English Learner Corpus and will describe the data and method of the study. Fourth, it will describe findings from a qualitative and quantitative perspective. Fifth,  An Exploratory Application of Rhetorical Structure Theory to Detect Coherence Errors 183 in L2 English Writing: Possible Implications for Automated Writing Evaluation Software these results will be discussed in relation to a) whether RST analysis seems a viable method for coherence error detection, b) which factors seem to affect the coherence errors located in the data, c) whether results indicate inductive order patterns and d) how much they overlap with the coherence errors that can be located via Criterion. The paper will end with a summary of conclusions and directions for future research. 2. RST and Discourse Coherence Error Detection It is difficult to reliably identify coherence errors because readers of the same text may form different interpretations of the coherence relations among elements of the text (Mann & Thompson, 1988). Therefore, a bottom-up method of coherence error detection should be used so that coherence errors will be identified as reliably and objectively as possible. RST was chosen first because the output of other methods of locating coherence breaks, such as topical structure analysis and genre analysis in Watson Todd et al. (2007), has been shown to have little relationship with English teachers’ judgments. Second, strong correlations have been found between RST analyses which show that a text is coherent and subjective judgments that a text is coherent (Taboada & Mann, 2006a). Finally, RST has not been applied to the location of coherence errors (Higgins, Burstein, Marcu, & Gentile, 2004: 185), so an evaluation of its application for this purpose is interesting from a methodological perspective. 3. Discourse Coherence and L1 Chinese Learners of English Given the paucity of discourse error tagging in learner corpora (Díaz-Negrillo & Fernández-Domínguez, 2006) and the sparse research on discourse errors by learners of English, this pilot study aims to provide a preliminary categorization of discourse errors in the writing of low-intermediate Taiwanese learners of English. This list of errors will be supplemented and refined through further research. L1 Chinese learners of English make similar discourse errors to learners with other native tongues, but there have also been claims for typical L1 Chinese errors. However, these claims have not been examined sufficiently through quantitative methods. Therefore, the pilot study reported in this paper also partly functions as a preliminary quantitative test for one of these claims. This claim is that the paragraphs and essays of L1 Chinese learners of L2 English have an inductive rather than deductive order. It has been claimed that these learners present the main point of their writing only at the end of a paragraph or essay, whereas in L1 English writing the main point is presented first (e.g., Kaplan, 1966; Matalene, 1985). The claim for the use of an inductive order only by L1 Chinese learners of English (and not by native speakers of English) has been challenged. For example, Scollon and Scollon (1995) used ethnomethodology to show that inductive and deductive patterns both exist in the speech of  184  Sophia Skoufaki  both native speakers of English and native speakers of Chinese. The only difference between the two languages is that these patterns are used for different pragmatic purposes. However, their analysis relates only to spoken discourse, so one cannot draw any conclusions about the existence of inductive patterns in written native English. This research gap is filled by Chen (2008). In a quantitative study, he found, among other things, that the minority of the native speakers of English preferred essays written with an inductive rather than deductive pattern and nearly half of them preferred paragraphs written in an inductive rather than a deductive order. This finding indicates that inductive patters can be used in written English but they are more acceptable in paragraphs rather than in essays. Finally Mohan and Lo (1985) review Chinese writing textbooks and analyse Classical Chinese texts to show that the deductive pattern is the most usual and prescribed essay writing pattern in Chinese1. From a theoretical perspective, if the RST analysis of the texts in the pilot study can point to instances of inductive order, the controversial issue of whether the English discourse of L1 Chinese learners is characterized by inductive order will be able to be examined in more detail in later research. Moreover, if the present study indicates that inductive-order errors occur frequently in the data, this may be seen as a preliminary indication that AWE software should try to detect and categorize as errors cases of inductive content order. 4. Discourse Errors and Criterion The pilot study reported here is also motivated by one of the criticisms made about AWE software, that is, that the effectiveness of AWE software should not be tested only through “a posteriori statistical validation” but also through an “a priori investigation of what should be elicited by the test before its actual administration” (Weir, 2005: 17). In other words, high levels of agreement in the grades assigned to essays between human judges and software should not be the only criterion for software evaluation; the kinds of errors which are located by software should also match those located by human judges. Such concerns are warranted for practical reasons as well, since it has been shown that learners can fool AWE software, that is, they can get high scores although the content of their essays is inadequate (Herrington & Moran, 2001; Powers, Burstein, Chodorow, Fowles, & Kukich, 2002; Ware, 2005). Therefore, if AWE software is designed so as to locate the errors that a human judge would locate, wrong essay 
One of the most common lexical misuse problems in the second language context concerns near synonyms. Dictionaries and thesauri often overlook the nuances of near synonyms and make reference to near synonyms in providing definitions. The semantic differences and implications of near synonyms are not easily recognized and often fail to be acquired by L2 learners. This study addressed the distinctions of synonymous semantics in the context of second language learning and use. The purpose is to examine the effects of lexical collocation behaviors on identifying salient semantic features and revealing subtle difference between near synonyms. We conducted both analytical evaluation and empirical evaluation to verify that proper use of collocation information leads to learners’ successful comprehension of lexical semantics. Both results suggest that the process of organizing and identifying salient semantic features is favorable for and is accessible to a good portion of L2 learners, and thereby, improving near-synonym distinction. Keywords: Lexical Semantics, Near-synonym Distinction, Lexical Collocation Behavior. 1. Introduction One of the most common lexical misuse problems in the second language context concerns near synonyms. Near synonyms are lexical pairs or sets that have very similar cognitive or denotational meanings. Dictionaries and thesauri often overlook the evaluative distinctions among near synonyms and ‘end up showing certain circularity’ in providing semantic meaning (Tognini-Bonelli, 2001). L2 learners are left with individual judgment and preference in lexical choices of almost synonymous words. Near synonyms, however, may vary in ∗ Department of English, National Taiwan Normal University, Taipei, Taiwan + Department of Applied Foreign Languages, Kang Ning Junior College, Taipei, Taiwan E-mail: chingying.lee1212@gmail.com, cylee@knjc.edu.tw # Department of Computer Science, National Chengchi University, Taipei, Taiwan E-mail : jsliu@cs.nccu.edu.tw [Received July 1, 2009; Revised January 22, 2010; Accepted January 28, 2010]  206  Ching-Ying Lee and Jyi-Shane Liu  collocational or implicative behavior (Partington, 2004). Among a group of nearly synonymous words, some may indicate favorable conditions while others refer to unfavorable situations, and some may show approval while others imply disapproval. These subtle distinctions between near synonyms are not easily identified and may never be acquired by L2 learners. Lexical use is an area where L2 learners frequently demonstrate a number of errors. Many L2 learners rely on dictionaries and thesauri to provide denotational meaning of a lexical item without being aware of the subtle implications embedded in contexts. Implicit knowledge of lexical items is not easily taught. Semantic infelicities due to inappropriate lexical use leads to miscommunication and unfavorable social consequences. Therefore, misuse of lexical items, particularly among near synonyms, calls for more attention and treatment in L2 lexical learning. The purpose of this research is to explore the potential of applying computerized linguistic resources and observing collocation behaviors in semantic learning for near synonym distinction. We propose a categorized collocation profile with graded association strength to filter and organize salient semantic features. It serves as a guided process to help develop concrete conceptual links so semantic meaning and unique features of lexical items become more easily accessible to L2 learners. Both analytical evaluation and empirical evaluation are performed to examine the effects of collocation information on near synonym distinction. Observations and implications in regards to L2 semantics learning are described. 2. Literature Review Knowledge of the appropriate contextual use of the particular languages’ resources is a crucial component of linguistic competence (Barron, 2003). L2 learners often face difficulties in understanding subtle and elusive nuances of appropriateness (Dewaele, 2008). The task of making proper lexical decisions between near synonyms is particularly challenging for L2 learners and requires adequate semantic competence. It is inadequate to only know a word meaning or definition. A core lexical competence is characterized by appropriateness of word choices, particularly between near synonyms. The idea of using collocation information to observe the word sense has been developed in post-Firthian corpus linguistics. The relevant studies investigate how a lexical item functions to convey semantic meanings, or how it carries out its discursive or evaluative properties (Sinclair, 2003; Channell, 2000; Stubbs, 2001; Partington, 2004). L2 learners should be aware that lexical meanings cannot be determined only by semantics. Therefore, it is helpful to examine the effects of collocation information on lexical meaning and functions. According to Stubbs, ‘there are always semantic relations between node and collocates  Effects of Collocation Information on Learning Lexical Semantics for  207  Near Synonym Distinction  and among collocates themselves’ (2001). The collocational information is interpreted through the proximity of a consistent series of collocates (Louw, 2000). Its main function is to convey the speaker or writer’s attitude or evaluation. According to priming theory, Partington (2004) indicates that a person has a set of mental rules in the priming process, combined with the mental lexicon, of how items should collocate. In addition, the process by which lexical items are primed in one’s mind is highly contextually dependent. The corpus linguistic techniques for lexical collocation provide a distinctive way to study semantic profiles. The problem of near synonym distinction and appropriate lexical choice is especially daunting for second language learners (Mackay, 1980). The majority of vocabulary errors made by advanced language learners reflect learners’ confusion among similar lexical items in the second language. The language of explanations in dictionaries is somewhat arcane such that it becomes limited in accessibility and usefulness in practical L2 contexts. Martin (1984) discussed instructional approaches to synonym teaching and suggested the importance of providing common collocates to students. With the availability of computerized corpora, recent research has exploited concordances and collocation data for advising L2 learners in lexical choice (Yeh, et. al., 2007; Chang, et. al., 2008). Through enquiry into the interplay between lexical semantics of near synonyms and their collocation information, this study provides analytic and empirical observations and contributes to reducing L2 learners’ confusion of sophisticated lexical connotations and applications. 3. Methodology Corpus-based approaches to applied linguistics assert that lexical semantics can be revealed by study of a large corpus. The analysis of the corpus uses computational techniques to identify words that typically co-occur with a lexical item under investigation. Our study attempts to understand the potential of adopting corpus linguistics for the purpose of improving learners’ performance in lexical semantics. In particular, we focus on investigating the effects of lexical collocation information on near-synonym distinction in either the self-learning or the classroom context. Recent developments in concordancing tools include web-based systems that provide online access to query and retrieval. Both Sketch Engine (Kilgarriff, et. al., 2004) and VIEW (Davies, 2008a) are powerful tools for corpus-based language research. Research issues concerning lexical behavior, collocational pattern, syntax, and semantics can all be facilitated by the language data access capability and the statistical summarization functions of these state-of-the-art concordancing tools. For the purpose of exploring the potential of lexical collocation information for semantic grounding and synonym distinction, we adopted VIEW as the concordancing tool in our study and used it to retrieve collocation information based on its access to two large corpora, BNC (Burnard, 1995) and COCA (Davies, 2008b).  208  
Using a corpus-based approach, this paper analyzes figurative language through observing the Chinese five elements ( 五 行 ) of 金 ‘metal,’ 木 ‘wood,’ 水 ‘water,’ 火 ‘fire’ and 土 ‘earth.’ This work found that there are at least two types of figurative language in Mandarin Chinese – one of which occurs at the morphosyntactic level and the other occurs during the mappings between two domains (between the body part terms and these five elements). When the figurative uses of the co-occurring five elements with body part terms were tested in a psycholinguistic experiment composed of two groups of subjects (non-native and native speakers of Mandarin), a majority of the non-native speakers were unable to comprehend these figurative uses. This study attempts to prove that a linguistically-driven understanding of the five elements will be of great help to teaching or learning figurative language in a Mandarin L2 context. Keywords: Corpus, Five Elements, Figurative Language, Body Part, Learners of Chinese, Psycholinguistic Experiment. 1. Introduction The relationship between body part terms and emotion metaphors was discovered by early psychologists, such as William James (1884) and Carl Lange (1884), who suggested that the origin of emotions is inside one’s body. Linguists of present days, such as Kovecses (2003) and Wierzbicka (1999), have also examined emotions in English and compared them to those in different languages. In Yu’s (1995: 85) inspection of Mandarin metaphorical expressions related to anger and happiness in Chinese, he noted that the “underlying cognitive model based on the fundamental theories of Chinese medicine has led to a cultural emphasis in China of sensitivity to the physiological effects of emotions on the internal organs.” Therefore, it holds that Chinese people are aware of the relatedness between the five elements and emotions ∗ Department of English, National Chengchi University, Taipei, Taiwan E-mail: sfchung@nccu.edu.tw [Received August 4, 2009; Revised January 29, 2010; Accepted February 1, 2010]  222  Siaw-Fong Chung  in Chinese. Our current study is different from Yu’s by addressing the following questions. (1) (a) What are the distributional patterns of the Chinese five elements in corpora data? (b) To what extent will a corpus-based method help to extract figurative language containing the Chinese five elements? (c) How will a linguistic analysis contribute to the understanding of figurative language by learners of Mandarin as a second language?  In addition to extracting figurative language, our work aims to explain how a corpus-based method can be used to assist teaching and learning. We intend to see the extent to which corpora and collocational understanding help in extracting these figurative patterns and how these patterns can be applied to teaching and learning of Mandarin to foreigners.  2. The Chinese Five Elements (五行)  Traditional Chinese medicine believes that the five elements also control one’s internal body – they “are said to vanquish one another and to produce one another” (Veith, 2002: 19). These elements are also reckoned by philosophers to be phenomena that rule nature. Table 1 provides these resonances (of mapping), according to traditional Chinese beliefs.  From Table 1, it can be seen that the five elements are related to emotions (last column of Table 1) and to body parts (shaded).  Table 1. Five element resonances  方位  自然界 氣候 發展過程 五色 五味 時令  五行  臟  東 風  生 青酸 春 木 肝  南 暑  長 赤苦 夏 火 心  中 濕 西 燥 北 寒  化 黃 甘 長夏 土 脾 收 白辛 秋 金 肺 藏 黑鹹 冬 水 腎  腑 膽 小腸 胃 大腸 膀胱  人體 五官 目 舌 口 鼻 耳  形體 筋 血脈 肌肉 皮 骨  情志 怒 喜 思 悲 恐  Hicks, Hicks, and Mole (2004: 28) said that, in Chinese, “emotions create movement and disturbance in a person’s qi.” Yu (1995: 81) has also commented that “[w]herever qi is locally impeded, it will affect the circulation of blood and local pain may occur as a result of increased internal pressure in that area” and “[t]his may point to the reason why qi is one of the basic words for the emotion of anger.” From here, one can see how the Chinese relate emotions to the five elements (Table 1) and to body parts. Yet, despite the traditional beliefs about the five elements and body part terms, we found that the denotation of body parts and emotions may sometimes not be in accordance with our linguistic knowledge, except for some that we can immediately relate based on physiological knowledge. While the connectivity of some pairings (such as that between 火 ‘fire’ (heat) and 心 ‘heart;’ as well as 水 ‘water’  A Corpus-based Study on Figurative Language through the  223  Chinese Five Elements and Body Part Terms  and 膀胱 ‘bladder’) can be easily explained, many others, such as the combinations of 腎 ‘kidney’ and 水 ‘water,’ as well as 肺 ‘lung’ and 金 ‘metal,’ are not entirely linguistically-driven. Chinese speakers, however, do not seem to find this a problem – that is, they can use 肚子 ‘stomach’ and 水 ‘water’ on the one hand and believe that 腎 ‘kidney’ and 水 ‘water’ are closely related on the other. This discrepancy between world versus linguistic knowledge may be confusing to a learner of Mandarin. Therefore, we hope to provide some insights to explain these apparent ‘discrepancies’ from a linguistic perspective, further supported by empirical data from corpora and a psycholinguistic experiment1. It is also through a metaphor framework (Lakoff & Johnson, 1980; Lakoff, 1999) that we hope to explain the mapped meanings of these five elements when they appear as physical entities (of metal, wood, water, fire, and earth) and as abstract elements. This study claims that collocational data from corpora can be utilized to raise awareness amongst foreign learners of Mandarin so that patterns in the target language can be recognized. These patterns may cause difficulty for learners both at word formation and at sentential levels. For example, some non-existent associations in English (e.g., 肝 ‘liver’ with 火 ‘fire’ to mean ‘irascibility’) can be better explained with corpora data2. By providing quantitative data, our research can shed light on the differing conceptualizations a foreign learner of Mandarin may need to overcome. The following expresses the methodology used in this work. 3. Methodology and Results for Corpora Analyses All single- (e.g., 火 ‘fire’) and multiple- (e.g., 肝 火 liver-fire’) character expressions containing the five elements were extracted from the Academia Sinica Balanced Corpus of Modern Chinese (hereafter Sinica Corpus), shown in Table 2. From Table 2, a total of 25,079 instances were found containing these five elements either as single-character expressions (Column 4) or in multiple-character morphemes (Column 6). Among these, 水 ‘water’ constitutes the biggest proportion, with about 40% of the total number of instances. This may be due to the fact that water has a wide applications of functions – to drink, to wash, to flow, to move, to flood, etc., not mentioning its possibilities of combination with different morphemes ranging from aquatic-related attributes (e.g., 水 田 ‘paddy field’ and 水 產  
1B3 In this paper, we take Determinative-Measure Compounds as an example to demonstrate how the E-HowNet semantic composition mechanism works in deriving the sense representation for a newly coined determinative-measure (DM) compound. First, we define the sense of a closed set of each individual determiner and measure word in E-HowNet representation exhaustively. Afterwards, we make semantic composition rules to produce candidate sense representations for a newly coined DM. Then, we review development set to design sense disambiguation rules. We use these heuristic disambiguation rules to determine the appropriate context-dependent sense of a DM and its E-HowNet representation. The experiment shows that the current system reaches 89% accuracy in DM sense derivation and disambiguation. Keywords: Semantic Composition, Determinative-Measure Compounds, Sense Representations, Extended How Net, How Net 1. Introduction Building a knowledge base is time consuming work. The CKIP Chinese Lexical Knowledge Base has about 80 thousand lexical entries, and their senses are defined in terms of the E-HowNet format. E-HowNet is a lexical knowledge representation system. It extends the framework of HowNet (Dong et al., 2006) to allow semantic composition. Based on the framework of E-HowNet, we intend to establish an automatic semantic composition mechanism to derive sense of compounds and phrases from lexical senses (Chen et al., 2005b),  ∗ CKIP, Institute of Information Science, Academia Sinica E-mail: {glaxy; kitajava; kchen} @iis.sinica.edu.tw + Department of Language and Literature Studies, National Hsinchu University of Education E-mail: slhuang@mail.nhcue.edu.tw [Received November 3, 2008; Revised May 27, 2009; Accepted May 28, 2009]  20  Chia-Hung Tai et al.  (Huang et al., 2008). Determinative-Measure compounds (abbreviated as DM) are the most common compounds in Chinese. As a determiner and a measure normally coin a compound with unlimited versatility, the CKIP group does not define the E-HowNet representations for all DM compounds. Nevertheless, construction patterns for DMs are regular (Li et al., 2006). Therefore, an automatic identification schema in regular expression (Li et al., 2006) and a semantic composition method under the framework of E-HowNet for DM compounds were developed. In this paper, we take DMs as an example to demonstrate how the E-HowNet semantic composition mechanism works in deriving the sense representations for all DM compounds. The remainder of this paper is organized as follows. Section 2 presents the background knowledge of DM compounds and sense representation in E-HowNet. We’ll describe our method in Section 3 and discuss the experiment result in Section 4 before we present conclusions in Section 5. 2. Background There are numerous studies on determiners as well as measures, especially on the types of measures1 Tai (1994) asserts that classifiers and measures words are often treated together F under one single framework of analysis. Chao (1968) treats classifiers as one kind of measure word. In his definition, a measure is a bound morpheme which forms a DM compound with the determiners enumerated below. i. Demonstrative determiners, e.g. 這 “this”, 那 “ that”… ii. Specifying determiners, e.g. 每 “every”, 各 “each”… iii. Numeral determiners, e.g. 二 “two”, 百分之三 “three percent”, 四百五十 “four hundred and fifty”… iv. Quantitative determiners, e.g. 一 “one”, 滿 “full”, 許多 “many”… Measures are divided into nine classes by Chao (1968). Classifiers are defined as ‘individual measures’, which is one of the nine kinds of measures. i. classifiers, e.g. 本 “a (book)”, ii. classifier associated with V-O constructions, e.g. 手 “hand”, iii. group measures, e.g. 對 “pair”, iv. partitive measures, e.g. 些 “some”, 
Although some traditional readability formulas have shown high predictive validity in the r = 0.8 range and above (Chall & Dale, 1995), they are generally not based on genuine linguistic processing factors, but on statistical correlations (Crossley et al., 2008). Improvement of readability assessment should focus on finding variables that truly represent the comprehensibility of text as well as the indices that accurately measure the correlations. In this study, we explore the hierarchical relations between lexical items based on the conceptual categories advanced from Prototype Theory (Rosch et al., 1976). According to this theory and its development, basic level words like guitar represent the objects humans interact with most readily. They are acquired by children earlier than their superordinate words like stringed instrument and their subordinate words like acoustic guitar. Accordingly, the readability of a text is presumably associated with the ratio of basic level words it contains. WordNet (Fellbaum, 1998), a network of meaningfully related words, provides the best online open source database for studying such lexical relations. Our study shows that a basic level noun can be identified by its ratio of forming compounds (e.g. chair armchair) and the length difference in relation to its hyponyms. We compared graded readings for American children and high school English readings for Taiwanese students by several readability formulas and in terms of basic level noun ratios (i.e. the number of basic level noun types divided by the number of noun types in a text ). It is suggested that basic level noun ratios provide a robust and meaningful index of lexical complexity, which is directly associated with text readability.  ∗ Department of English, National Taiwan Normal University E-mail: {yenyenet, vennysu, yudalai, lchyang1112, shukai}@gmail.com [Received January 1, 2009; Revised April 11, 2009; Accepted April 15, 2009]  46  Shu-Yen Lin et al.  Keywords: Readability, Prototype Theory, WordNet, Basic Level Words, Compounds.  1. Introduction  Traditional methods of measuring text readability typically rely on surface-level linguistic  information such as the counting of sentences, words, syllables, or letters. Caution has long  been taken in correlating these formulas with the reading process (Davison & Kantor, 1982;  Rubin, 1985). In light of the many psycholinguistic findings on the reading process (Just &  Carpenter, 1987; Perfetti, 1985; Rayner & Pollatsek, 1994), we start our research by assuming,  in line with Rosch et al.’s Prototype Theory (Rosch & Mervis, 1975, Rosch et al., 1976) and  its later development (Rosch, 1977, 1978; Coleman & Kay, 1981; Lakoff, 1986; Tversky,  1990; Ungerer & Schmid, 1996), that words form conceptual hierarchies (e.g. furniture  chair  armchair) with lexical items at different levels posing varied processing  difficulties. Putting the logic into templates, the measurement of the lexical difficulty of a text  may be done by calculating the hierarchical levels at which its words fall. The best tool for our  study is WordNet, a large, open source electronic lexical database of English, in which the  different senses of words are interlinked in hierarchical structures by means of  conceptual-semantic relations.  Our research was comprised of two stages. In the preliminary experiments, we utilized WordNet to identify the characteristics of basic level nouns. It was found that a basic level noun can be identified by its ratio of forming compounds (e.g. chair armchair) and the length difference in relation to its full hyponyms. In the subsequent experiment, we compared selected readings in terms of their basic level noun ratios and their values calculated by several readability formulas. It is shown that basic level noun ratios are highly correlated with the text levels. Our study also indicates that there is a basic level in a lexical hierarchy which is easier to comprehend than its upper or lower levels. This finding challenges the intuitive idea underlying McNamara et al. (2002) that a word having more hypernym levels is more concrete, thus, easier to comprehend, and fewer hypernym levels indicate more abstract language that is harder to understand.  The remainder of this paper is organized as follows: Section 2 reviews the common indices that form the base of many traditional readability formulas and the criticism they have received. In Section 3, we review Prototype Theory and discuss how it can aid us in finding the lexical difficulty of a text. Section 4 is about methodology – how to identify basic level words and how to assess the validity of our method against other readability formulas. Section 5 reports the results of the assessment and discusses the strength and weaknesses of our approach. In this section, we also suggest what can be done in subsequent research.  Assessing Text Readability Using Hierarchical Lexical  47  Relations Retrieved from WordNet  2. Literature Review In this section we first summarize the indices of traditional readability formulas, give an account of the criticism these formulas meet, and introduce the purpose of our study. Among the multitude of factors underlying the reading process, we will focus on the lexical index.  2.1 Indices of Readability  2.1.1 Vocabulary Difficulty The earliest work on readability measurement goes back to Thorndike (1921) where word frequency in a corpus is considered an important index in computing vocabulary complexity. This is based on the logic that the higher the frequency of a word, the more common and easier it is. Followers of this logic compiled word lists that include often-used and seldom-used words where the presence or absence of particular words on the lists assesses vocabulary difficulty, thus text difficulty. Vocabulary difficulty is also measured by word length in many formulas, e.g., the Flesch formula (Flesch, 1943, 1948, 1950) and FOG formula (McCallum & Peterson, 1982), or in terms of number of syllables (Fry, 1968). This is based on another intuitive assumption that the longer a word is, the more difficult it is to comprehend (Bailin & Grafstein, 2001).  2.1.2 Syntactic Difficulty Syntactic complexity is another index in many readability formulas (Chall & Dale, 1995). For Dale & Chall (1948), Flesch (1948), and McCallum & Peterson (1982), syntactic complexity boils down to the average length of sentences in a text, although they vary in how they determine and utilize sentence length. The formula designed by Heilman, Collins-Thompson, Callan, & Eskenazi (2007) is a more recent example of this type. They propose that grammar-based predictions can be combined with vocabulary-based predictions to produce more accurate predictions of readability for both first and second language texts. They also suggest that language technologies must account for morphological features in languages which have a rich morphology, an issue relevant to grammatical features. Also taking account of syntactic complexity, Miltsakaki & Troutt (2007) bases their algorithm on three readability formulas: Lix, Rix, and Coleman-Liau. The number of sentences, words, long words (seven or more characters), and letters in the text are taken into account. Another example is Das & Roychoudhury’s work (2006), which built a readability index for Bangla using average sentence length (total words/ total sentences) and number of syllables per 100 words (total syllables/ total words*100).  48  Shu-Yen Lin et al.  2.1.3 Semantic Difficulty Semantic factors such as counting abstract words (Flesch, 1943; Cohen, 1975) and propositional density and inferences (Kintsch, 1974) have also been put into regression analyses of readability assessment. In addition to these projects, Wiener et al. (1990) proposes a scale based on ten categories of semantic relations, e.g., temporal ordering and causality, for assessing the utterance complexity. The reliability of the semantic scale was confirmed when it was applied to compare the utterances of fourth-, sixth-, and eighth-grade children, where significant differences in semantic density were found on their scale. Since 1920, more than fifty readability formulas have been proposed in the hopes of providing tools to measure readability more accurately and efficaciously (Crossley et al., 2007). Nonetheless, it is not surprising to see criticism over these formulas, given that reading is an extremely complex process. 2.2 Criticism of the Traditional Readability Formulas Although classic readability formulas provide a quick and easy method of predicting readability, they are often criticized for being superficial, unstable, or unable to offer information about deeper levels of text processing (McNamara et al., 1996). 2.2.1 Criticism of Lexical Difficulty Measurement Bailin & Grafstein (2001) question the validity of measuring vocabulary difficulty by the number of syllables per word or by the presence of words in a word list. They question the legitimacy of assessing vocabulary difficulty in terms of word length by showing that many mono- or bi-syllabic words are actually more esoteric, i.e. more unfamiliar, than longer polysyllabic terms. They also argue that the proposed link between readability and a vocabulary list of word frequency is narrowly based on the prerequisite that words in a language remain stable. The prerequisite, however, seems implausible as different socio-cultural groups have different core vocabularies and rapid cultural change makes many words out of fashion. 2.2.2 Criticism of Syntactic Difficulty Measurement Bailin & Grafstein (2001) also point out the flaw of a simple equation between syntactic complexity and sentence length by giving the sample sentences as follows: (1) I couldn’t answer your e-mail. There was a power outage. (2) I couldn’t answer your e-mail because there was a power outage. In terms of both absolute length and number of words, (2) is longer than (1), thus computed as more difficult by traditional readability formulas. Nevertheless, the subordinator  Assessing Text Readability Using Hierarchical Lexical  49  Relations Retrieved from WordNet  “because” in (2), which explicitly links the author’s inability to e-mail to the power outage, actually aids comprehension. As such, the authors suggest that language-oriented criteria be proposed, including deviations from prescriptive grammar, style (relative clauses, garden-path phrases, left-branching structures, etc.), and required background knowledge. 2.2.3 Criticism of Statistical Legitimacy The correlation between the indices and the measured variables was also challenged from the viewpoint of statistical legitimacy. Hua & Wang (2007) point out the methodological issue in the creation of the traditional readability formulas. The typical initial step is to select, as the criterion passages, standard graded texts whose readability has already been agreed upon. The next step is to sort out the factors that may affect the readability of the text. The factors that are highly correlated with the text difficulty are chosen as independent variables in regression analysis for forming a readability formula. The researchers, however, did not ascertain whether the factors incorporated into their regression model actually have a cause-effect relationship with the dependent variable, i.e., readability. Word length, used to equate semantic complexity, and sentence length, used for syntactic complexity, are intuitively correlated with readability, but non-scientifically correlated. Therefore, the authors suggest that researchers first analyze the independent variables qualitatively to confirm their cause-effect relationship with readability. Challenge also goes to the selection of criterion passages. Schriver (2000) suggests that readability formulas are inherently unreliable because they depend on criterion passages too short to reflect cohesiveness, too varied to support between-formula comparisons, and too text-oriented to account for the effects of lists, enumerated sequences, and tables on text comprehension. 2.3 Purpose of Research The criticisms of the traditional readability formulas by the various authors have a lot in common. They all urge adoption of language-oriented criteria based on independent evidence and a closer re-examination of the genuine relationship between the variables and the texts. It is our belief that this can only be done if we take account of the deeper levels of text processing. Reading is a multidimensional process; our pilot study aims to examine how a reader interacts with a text at the lexical level. We propose that the hierarchical status of a lexical item in our mental lexicon is a possible factor that affects lexical comprehensibility. We further suggest that there is a basic level in the lexical hierarchy which is the easiest to comprehend and serves as a meaningful indicator of text readability. To that end, we resort to Prototype Theory, which was proposed and developed by Rosch et al. (1976), among others.  50  Shu-Yen Lin et al.  3. Prototype Theory and Lexical Difficulties 3.1 Prototype Theory Prototype Theory was brought to cognitive linguistics by Rosch et al. (1976). The notion of prototype can be understood in two ways. First, prototype is used either to refer to object members that first come to one’s mind in an association experiment, or to those that can be recognized faster than other category members in a verification task. For example, when asked to give an example of “bird”, “robin” is more frequently cited than “ostrich”. Various researchers (Rosch, 1978; Lakoff, 1986; Brown, 1990; Tversky, 1990) use different names to label the prototypical member – “best example of a category”, “salient examples”, “clearest cases of category membership”, or “central and typical members.” The other way to define prototype is from a genuinely cognitive viewpoint. Prototype can be viewed as a mental representation, specifically as some kind of cognitive reference point (Rosch & Mervis, 1975; Coleman & Kay, 1981; Lakoff, 1986). Taking the two viewpoints together, we can view prototype as the central member or the cognitive reference point which other members of the category are anchored to. Through the anchoring process, cognitive categories are formed. The members within a particular cognitive category are anchored to the prototype with different parameters – whether the members are perceived as gestalt, how many category-wide attributes are shared by the members, and how homogeneous or heterogeneous the members are. The representation of a bird, for instance, does not consist of a set of features that all birds have. A robin or a penguin as a category member of the bird is anchored to the most typical or ideal category member of the bird (which may not exist in real life). Since a robin shares more of the features characteristic of a prototypical bird than a penguin shares, it is usually viewed by subjects as a better example of a bird. The same mental anchoring process can be applied to broader human categorization of those readily identifiable organisms and objects that surround us (Ungerer & Schmid, 1996: 60). As a result, entities within the cognitive category of DOG can be categorized as a “dog”, a “terrier”, a “Scottish terrier”, a “mammal”, or an “animal”. These cognitive categories are connected with each other in a hierarchical pattern. In this example, if we look at their relationship from the “bottom” of the hierarchy, Scottish terriers are subordinate to terriers, and terriers are subordinate to dogs. If we look at them from the “top” of the hierarchy, animals are viewed as superordinate to mammals, and mammals as superordinate to dogs. Turning to early interpretations of the basic level from the psychological viewpoints by Brown (1958) and Kay (1971), the basic level is where human beings perceive the most obvious difference between the organisms and objects of the world. Imagine an everyday conversation where a person says “Who moved that piano?” The naming of an object with “piano” will not strike us as noteworthy until the alternative “Who moved that stringed  Assessing Text Readability Using Hierarchical Lexical  51  Relations Retrieved from WordNet  instrument?” is brought to our attention. Both terms are truth-conditionally adequate, but only the former is commonly used. The superordinate word “stringed instrument” is not used because its meaning encompasses many basic level words, i.e. many kinds of musical instruments. In our example, using the word “stringed instrument” is too vague to represent the object: “Stringed instrument” does not, as “piano” does, denote the most obvious difference of the object from the other objects in the world. Likewise, using a subordinate level word, e.g. a “grand piano”, on a similar occasion is unusual except when the differentiation between different types of pianos is required. In ranking typicality of objects, the basic level is where the largest bundles of naturally correlated attributes are available for categorization (Rosch et al., 1976; Ungerer & Schmid, 1996: 67). In addition, the basic level is where gestalt perception occurs to the greatest extent, and this is particularly easy for prototypical examples. An “apple” has reddish or greenish skin, white pulp, and a round shape, while it is hard to pinpoint the features of “fruit”. For a layman, hardly any significant features can be added to “crab apple”. The underlying cognitive anchoring process of the psychological reality of the basic level and the prototype are very similar. In the same way as other peripheral members of a category are anchored to the prototypical member, other non-basic levels, namely superordinate and subordinate levels, are anchored to the basic level. Ungerer and Schmid (1996: 72) point out the two are actually a kind of symbiosis underpinned by two interdependent principles: First, prototype categories are most fully developed on the basic level. Second, basic levels only function as they do because they are structured as prototypical categories. The first principle can be explained by our earlier discussion on the basic level. Recall that this level offers the largest amount of correlated attributes, and the attributes are accumulated in their most completed form in the prototype and expressed by the category name (e.g., “Robin”, as the typical example of the category BIRD, accumulates most correlated attributes of medium size, feathers, flying and singing ability, etc. in a complete form.) As for the second principle, maximization of the efficiency of basic level categories by prototypes can be used to explain it (Rosch, 1977, 1978). That is, prototypes maximize the discontinuities or the distinctiveness of the basic level categories as they induce not only the greatest number of attributes shared inside the category, but the greatest number of attributes not shared by members of other categories. A typical example of a bird like “robin” can, while a non-typical example of the bird like “penguin” cannot, be easily distinguished from the category of fish because the latter shares more attributes with fish. Developmentally, basic level categories are acquired earlier by children than their superordinate and subordinate words. Conceptually, the basic level category represents the  52  Shu-Yen Lin et al.  concepts humans interact with most readily. Applying the hierarchical structure of conceptual categorization to lexical comprehensibility, we suggest that a concept at the basic level, hence, the word that denotes the concept, which we call a basic level word, is easier for the reader than its superordinate and subordinate words. If this is correct, then one text should be easier than another if it contains more basic level words. As the three-leveled hierarchy refers specifically to nouns in Prototype Theory, we confine our current study to the nominal category only. The best tool to study the relevant hierarchical relations in a broad framework with computational techniques is WordNet. 3.2 WordNet – A Hierarchically-Structured Lexical Database of English WordNet is a large online electronic lexical database of English. The words are interlinked by means of conceptual-semantic and lexical relations. Its underlying design principle has much in common with the hierarchical structure proposed in Prototype Theory. In the vertical dimension, the hypernym/hyponym relations among the nouns can be interpreted as hierarchical relations between conceptual categories. For instance, the direct hypernym of “apple” in WordNet is “edible fruit”. One of the direct hyponyms of “apple” is “crab apple”. Note, however, that hypernyms and hyponyms are relativized notions in WordNet. Theoretically speaking, any word may have hypernyms and hyponyms. “Crab apple,” being a hyponym of “apple,” is also a hypernym in relation to “Siberian crab apple”. An ontological tree may well exceed three levels. There are no labels or ready-to-be-used statistical information in WordNet that tell us which nouns fall into the basic level category. In the following sections we try to retrieve the basic level nouns as defined in Prototype Theory and apply the results in assessing text readability. 4. Methodology Three experiments were conducted. In the first experiment, we utilized the nouns used in Rosch et al’s experiments in order to discover their quantitative properties. The second experiment followed up the first one and tried to pinpoint the criteria of determining the quality of the nouns being basic. In the third experiment, we computed and compared the basic level noun ratios and readability scores of graded readings. Our results indicate that basic level noun ratios provide a robust and meaningful index of text readability. 4.1 Experiment 1 4.1.1 Design of Experiment 1 In our initial experiment, we examined the eighteen basic level words identified by Rosch et al. (1976: 388), checking their word length, lexical complexity, and their direct hypernyms as  Assessing Text Readability Using Hierarchical Lexical  53  Relations Retrieved from WordNet  well as direct hyponyms in WordNet. We speculate that a basic level word has these features: (1) It is relatively short (containing fewer letters than its hypernyms/hyponyms on average); (2) It is morphologically simple1; (3) It has more direct hyponym synsets than direct hypernym synsets2. Notice that some entries in WordNet are made up of more than one word. We assume that an item composed of two or more words is NOT a basic level word. A lexical entry composed of two or more words is a compound. The first word of a compound noun may or may not be a noun, and there may or may not be spaces or hyphens between the component words of a compound. For words having more than one sense, we focused only on the sense occurring in Rosch et al’s experiment. As an example, the noun “table” has six senses (i.e. synsets) in WordNet, but only the information in the sense of “a piece of furniture” is computed. Table 1 summarizes the results of Experiment 1.  Table 1. Eighteen basic level words in comparison with their direct hypernyms and direct hyponyms on word length, number of synsets, and morphological complexity*  Target word  Index of the inquired synset  Basic level  Direct hypernym  Direct hyponym  Word length  Morph. Complexity  Average word length  Number of synsets  Morph. Complexity  Average word length  Number of synsets  Morph. Complexity  guitar  0  6  A  18 1  B  8.8 6 A, B  piano  0  5  A  19 3  B  9.6 3 A, B  drum  0  4  A  20 1  B  7.4 8 A, B  apple  0  5  A  8.3 2 A, B 10.6 3 A, B  peach  2  5  A  8.7 2  B  N/A N/A N/A  grape  0  5  A  11 1  B  11.8 3 A, B  hammer  
A Chinese news summarization method is proposed in order to help humans deal with the message services of news briefs broadcast over cell phones. The problem to be solved here is unique because a strict length limit (69 or 45 characters) is imposed on the summaries for the message service. This requires some sort of automatic sentence fusion, rather than sentence selection alone. In the proposed method, important sentences were first identified based on the news content. They were matched against the news headline to determine a suitable position for concatenation with the headline to become candidates. These candidates were then ranked by their length and fitness for manual selection. In our evaluation, among 40 short news updates in the inside testing set, over 75% (80%) of the best candidates yield acceptable summaries without manual editing for the length limit of 69 (45) characters. These numbers, however, reduce to 70.7% (53.3%) for the outside testing set of 75 news stories of ordinary length. It seems that the shorter the length limit, the more difficult the problem of getting the summary from long stories. Nevertheless, the proposed method has the potential not only to reduce the cost of manual operation, but also to integrate and synchronize with other media in such services in the future. Keywords: Cell Phone Service, News Brief Message, Automated Summarization, Chinese News 1. Introduction The popularity of cell phones in the Taiwan area has reached the highest rate in the world during the last few years. Over 23 million cell phone numbers were used as of June 2002, which is slightly more than the population of Taiwan (Wang, 2002). To better utilize this ubiquitous communication device, a number of content providers have provided Chinese news ∗ National Taiwan Normal University, No.162, Sec. 1, Heping East Road., Taipei City, Taiwan (R.O.C.), 106, Tel: 886-2-77345535 E-mail: samtseng@ntnu.edu.tw [Received February 13, 2009; Revised July 16, 2009; Accepted July 17, 2009]  86  Yuen-Hsien Tseng  brief services over the cell phone, such as United Daily News (United Daily News, n.d.), Central News Agency (Central News Agency, n.d.), and PC Home in Taiwan. The Asahi Shimbun in Japan (the second largest news agency in the world) has provided such news message services at an inexpensive rate since 1999, in the hope that the increase in the number of the readers of their content could lead to an increase in the subscriptions to their newspaper (China Times, n.d.). As multimedia technologies continue to improve, future news service over the cell phone may not only include text, but also speech, images, or video, integrated and synchronized. To reach this vision, however, the operation cost should be low enough to sustain such services. Therefore, automated methods of cost containment would be of great help. The news brief shown on a cell phone is different from one on a desktop computer. Due to the limited screen size, a length limit is defined for each news message. This is usually 45 Chinese characters in PHS systems or 69 characters in other systems, including punctuation marks (United Daily News, n.d.). Summaries of this kind are longer than a news headline but shorter than a long Chinese sentence. For the benefit of the subscribers, the summaries should contain as much content as possible to reduce the frequency of retrieving the whole news story. Also the readability and coherence of the summaries are important factors that should be taken into account. From the research perspective, the task defined above is a challenge for automatic document summarization. Previous studies have shown that the shorter the summary required, the lower the performance of machine-generated summary (Lin & Hovy, 2003), hence, the more difficult the problem is. The task of news brief summarization for cell phones falls into this difficult category. On the other hand, human summarization of news stories for cell phones is not really a difficult problem. As mentioned above, the main issue is whether one can achieve this task in a low-cost and efficient way. Strictly maintaining the length limit requires a human summarizer to pay attention to the number of characters already there while making the summarization. If a machine could suggest a number of summary candidates, each with its length shown, for human selection, not only would the human summarizer be relieved of such tedious work and improve his/her efficiency, but also the task would become less difficult for machine summarization. This article proposes a Chinese news summarization technique to assist human summarizers in the above way, with the aim of meeting the considerations described above. Basically, our approach is a sentence fusion technique that merges the news headline with the body sentence that supplements the information carried by the headline. After a brief review of previous work in the next section, the detailed approach and its motivations are described. The performance is then evaluated and the results are shown. This is followed by a discussion of the strengths and weaknesses of the proposed method. Finally, we conclude this paper with  Summarization Assistant for  87  News Brief Services on Cellular Phones  some other possible applications and future work for further exploring Chinese news summarization techniques. 2. Related Work Automatic news summarization techniques have been widely explored in recent years, such as the summarization tasks in DUC (DUC, n.d.) or in NTCIR (Fukusima, Okumura, & Nanba, 2002). Several practical systems (e.g. (Hovy & Lin, 1999; Evans, Klavans, & McKeown, 2004; Radev, Otterbacher, Winkel, & Blair-Goldensohn, 2005)) have been developed in the past decade. The summarization techniques used in most studies can be divided into two approaches: abstraction and extraction (Mani, 2001; Radev, Hovy & McKeown, 2002). In abstraction, advanced natural language processing (NLP) techniques are applied to analyze sentential information and then to generate concise sentences with proper semantics. Sophisticated NLP techniques, such as anaphora resolution, may be used and certain human maintained knowledge bases or corpora may be needed. In extraction, statistical techniques are applied to rank and select the text snippets for a summary. Due to its relatively low cost and high robustness across application domains and document genres, most summarization tasks adopt the extraction approach (Carbonell & Goldstein, 1998; Lin & Hovy, 2002, Tseng, et al, 2007). Nevertheless, abstraction-based methods move the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and also synthesize information across sources (Barzilay & McKeown, 2005). Thus, the need for an abstraction-based approach is sometimes inevitable. Despite the vast literature already published, most of the studies are for English. Although some have focused on Chinese news (e.g. (Chen, Kuo, Huang, Lin & Wung, 2003)), none have been done for the problem discussed here. The problem to be solved in this paper is unique due to the facts that there is a strict length limit imposed and that the range of the length limit makes most simple sentence selection approaches invalid. Thus, an abstraction-based method or a similar one that requires sentence fusion or alteration is required. For example, in (Takefumi, Hidetaka, & Hiroshi, 2003) the authors reported a deletion-based approach to summarize a Web news article for PC to another short article for cell phones for Japanese. There, the length limit of the short article ranges from 50 to 100 Japanese characters. The approach first computes the values of TFxIDF for each clause in advance. A few significant sentences from the original article are then extracted based on the TFxIDF values. After that, verbose descriptions corresponding to the leaves of the dependency trees, having the lowest TFxIDF, are removed from the sentences until the length of the result of summarization is within the limit.  88  Yuen-Hsien Tseng  An important issue in automatic summarization is the evaluation of machine-derived summaries. This is not an easy task. Two main approaches are commonly applied: intrinsic and extrinsic evaluation (Mani, 2001). In intrinsic evaluation, manually prepared answers or evaluation criteria are compared with those that are machine generated. In extrinsic evaluation, automated summaries are evaluated based on their performance or influence on other tasks, such as document categorization. We adopt the intrinsic approach here since it is obviously suitable for our task. 3. The Proposed Summarizer To develop an automated Chinese news summarizer subject to the limitations of a cell phone, an understanding of the style of the news stories and how humans summarize them would be helpful. Table 1 lists three news examples and their English translations. As can be seen, these examples are short, with their bodies having only 1, 2, and 3 sentences, respectively. This is not uncommon for the stories to be transmitted to users’ cell phones, although longer stories may be selected as well. Given such short stories, a human summarizer has very few clues as to rewrite the story thoroughly to fit the length limit. The best he or she can do may be to cut and paste the snippets from the news text with minimal editing to avoid garbling the original meaning. The snippets to be cut and pasted can be enumerated then suggested by a computer for manual selection. Nevertheless, the possibilities of such enumeration would be huge if all substrings of the news text are blindly considered. As can be seen from the examples in Table 1, a Chinese sentence is often composed of several comma-separated clauses, which convey the meaning of the sentence in successive sequence. Chinese clauses are independent from each other in some circumstances and, thus, constitute a useful unit to be combined with others to make a new sentence. Although most of the combined sentences would be invalid, several of them would still be meaningful and sometimes more complete in content, especially for those from the beginning and ending clauses. Table 1. Three news examples for summarization1. The number in parenthesis is the number of characters in the preceding sentence. 太空探測器在遙遠的恒星周圍發現水的痕跡 (19) 美國航空航太總署的科學家星期三稱，新近在一顆遙遠的恒星周圍發現了水存在的 1 痕跡，這可以成為第一個支援除我們自己存在地外生命的證據。(64) #2001/07/13# Space Probe Sees Signs of Water Around Distant Star Newly detected signs of water around a distant star are the first evidence that planetary 
Feature statistics normalization techniques have been shown to be very successful in improving the noise robustness of a speech recognition system. In this paper, we propose an associative scheme in order to obtain a more accurate estimate of the statistical information in these techniques. By properly integrating codebook and utterance knowledge, the resulting associative cepstral mean subtraction (A-CMS), associative cepstral mean and variance normalization (A-CMVN), and associative histogram equalization (A-HEQ) behave significantly better than the conventional utterance-based and codebook-based versions in additive noise environments. For the Aurora-2 clean-condition training task, the new proposed associative histogram equalization (A-HEQ) provides an average recognition accuracy of 90.69%, which is better than utterance-based HEQ (87.67%) and codebook-based HEQ (86.00%). Keywords: Speech Recognition, Noise-Robust Feature, Codebook 1. Introduction The performance of a speech recognition system is often severely degraded when there is a mismatch between the acoustic conditions of the training and the application environments. This mismatch may come from various sources, such as additive noise, channel distortion, different speaker characteristics, and different speaking modes. A variety of robustness techniques with demonstrated improvement in system performance have been proposed to reduce this mismatch. For the purpose of handling additive noise, these robustness techniques can be roughly divided into three classes: adaptation of the speech models in the recognizer to make them better match the noise conditions, enhancement of the speech features before they are fed to the recognizer, and utilization of a noise robust representation of speech signals. In  ∗ Dept of Electrical Engineering, National Chi Nan University, Nantou County, Taiwan, Republic of China E-mail: aero3016@ms45.hinet.net; jwhung@ncnu.edu.tw [Received January 13, 2009; Revised May 13, 2009; Accepted May 19, 2009]  106  Wen-Hsiang Tu and Jeih-weih Hung  the first class of approaches, compensation is performed on the pre-trained recognition model parameters so that the modified recognition models can more effectively classify the mismatched testing speech features collected in the application environment. Typical examples of this class include the well-known noise masking (Holmes & Sedgwick, 1986; Klatt, 1979; Nadas, Nahamoo, & Picheny, 1988), speech and noise decomposition (SND) (Varga & Moore, 1990), hypothesized Wiener filtering (Berstein & Shallom, 1991; Beattie & Young, 1992), vector Taylor series (VTS) (Acero, Deng, Kristjansson, & Zhang, 2000), maximum likelihood linear regression (MLLR) (Leggester & Woodland, 1995), model-based stochastic matching (Sankar & Lee, 1996; Lee, 1998), statistical re-estimation (STAR) (Moreno, Raj, & Stem, 1996), and parallel model combination (PMC) (Gales & Young, 1993; 1995a; 1995b). In the second class of approaches, the obtained testing speech features are modified in order to fit the acoustic conditions of pre-trained recognition models more compatibly. Examples of this class include the well-known spectral subtraction (SS) (Boll, 1979), codeword-dependent cepstral normalization (CDCN) (Acero, 1990), feature-based stochastic matching (Sankar & Lee, 1996; Lee, 1998), vector Taylor series (Segura, Benitez, de la Torre, Dupont, & Rubio, 2002; Moreno, Raj, & Stem, 1998), multivariate Gaussian-based cepstral normalization (RATZ) (Moreno, Raj, & Stem, 1996), and stereo-based piecewise linear compensation for environments (SPLICE) (Deng, Acero, Jiang, Droppo, & Huang, 2001; Droppo, Deng, & Acero, 2001). In the third class of approaches, a special robust speech feature representation is developed to reduce the sensitivity to various acoustic conditions; one way to develop this new feature representation is to normalize the statistics of the original speech features in both training and testing conditions in order to reduce the mismatch caused by noise. These feature statistics normalization techniques include cepstral mean subtraction (CMS) (Atal, 1974), cepstral mean and variance normalization (CMVN) (Tibrewala & Hermansky, 1997), cepstral gain normalization (CGN) (Yoshizawa, Hayasaka, Wada, & Miyanaga, 2004), histogram equalization (HEQ) (Hilger & Ney, 2006), higher-order cepstral moment normalization (HOCMN) (Hsu & Lee, 2004), cepstral shape normalization (CSN) (Du & Wang, 2008) etc. A common advantage of these methods is simplicity of implementation, since all of them focus on the front-end speech feature processing without the need of changing the back-end model training and recognition schemes. Regardless of the simplicity, these methods usually improve the recognition performance significantly under a noise-corrupted application environment. A key process for most of the above normalization methods is to estimate the statistical information of speech features. For example, the first-order moment (mean), the first and second-order moments (mean and variance), and the probability distribution of features are required for CMS, CMVN, and HEQ, respectively. In most cases, the required statistical information is directly evaluated from the entire frame set of an utterance. Although simple in  Study of Associative Cepstral Statistics Normalization Techniques for  107  Robust Speech Recognition in Additive Noise Environments  implementation, the resulting utterance-based methods likely have some inherent drawbacks. First, they cannot be realized in an on-line manner since the computation and normalization of the statistics cannot be performed until the last frame of an utterance is received. Second, the number of frames in an utterance influences the accuracy of the obtained statistics. Third, since the length, or the number of different acoustic units, may vary from utterance to utterance, the normalized features of the same acoustic unit in an utterance may differ from those in another utterance. In our previous works (Hung, 2006; 2008), we proposed that the statistics of features be evaluated based on two codebooks, named "pseudo stereo codebooks". Construction of the codebook of clean speech cepstra can occur off-line and prior to recognition. The codebook of noise-corrupted speech cepstra for each testing utterance is constructed by properly integrating the clean-speech codebook and the noise estimates, which often can be extracted from the first several frames of the utterance. The resulting codebook-based methods are expected to obtain more accurate estimate of feature statistics, and they can be implemented in an almost on-line manner. In (Hung, 2008), we have shown that codebook-based CMS and CMVN outperform conventional utterance-based ones in recognition accuracy for additive noise environments. The original procedures in constructing the codebooks in (Hung, 2008), however, are somewhat simple, which possibly results in a less accurate estimate of the statistics for speech features. First of all, the clean speech codebook is built with all the feature vectors in the clean speech utterances for training. Since these utterances may contain quite long non-speech (silence) segments, it is likely that numerous codewords in the codebook just correspond to these non-speech parts. Second, the feature statistics are estimated by uniformly averaging the codewords, which ignores the relative significance of each codeword. Finally, the noise information only depends on the leading frames of an utterance, which may make the noise-corrupted speech codebook less accurate. This problem will be worse if the noise is non-stationary. Although updating the noise estimate within an utterance based on a voice activity detection (VAD) process can alleviate this problem, it will substantially increase the implementation complexity. Based on the above observations, in this paper, we propose to improve the accuracy of the feature statistics estimation in two aspects. First, the procedures of creating the pseudo stereo codebooks are modified so that they are more representative of the speech features. The resulting advanced pseudo stereo codebooks are shown to be more effective in the codebook-based methods than the original ones. Second, the information from both the codebook and the frames of the processed utterance are integrated, so that more accurate statistics of the features can be obtained in order to further enhance the feature statistics normalization techniques. This idea is realized on three well-known approaches, CMS, CMVN, and HEQ. We will show that the resulting "associative" methods are superior to the original  108  Wen-Hsiang Tu and Jeih-weih Hung  utterance-based and codebook-based ones in the Aurora-2 clean-condition training task. The remainder of the paper is organized as follows: Section 2 presents the construction of advanced pseudo stereo codebooks. Section 3 introduces our proposed associative cepstral normalization techniques. The experimental environment setup is described in Section 4, and the recognition results are given and discussed in Section 5. Finally, Section 6 contains brief conclusions.  2. The Construction of Advanced Pseudo Stereo Codebooks  In this section, we introduce the approach to constructing the advanced pseudo stereo codebooks for clean training and noise-corrupted testing environments, respectively. The corresponding procedures are also shown in Figure 1. The basic idea of the process for constructing these codebooks is as follows: during the feature extraction processes, we find an intermediate feature domain in which the clean speech and noise are linearly additive (assuming that the speech signal and noise are uncorrelated in the time domain). The clean speech codewords for the intermediate feature domain first are constructed then are linearly added to the noise estimates to compose the noise-corrupted speech codewords for that domain. Finally, they are transformed to the final feature domain following the remaining feature extraction processes. For the mel-frequency cepstral coefficients (MFCC), the intermediate feature mentioned above is the mel-spectrum, while for the other two types of speech features, linear prediction cepstral coefficients (LPCC) (Atal, 1974; Makhoul, 1975) and perceptual linear prediction cepstral coefficients (PLPCC) (Hermansky, 1990), both the auto-correlation coefficients and the magnitude spectrum can be selected as the intermediate feature. Therefore, the codebook construction process and the relating methods can be applied to MFCC, LPCC, and PLPCC.  For simplicity, the mel-frequency cepstral coefficients (MFCC) are used as the speech 
The Chinese aspect marker le has long been considered very difficult for CSL learners. Therefore, we created an computer-based interactive multimedia CSL program of the perfective le based on the linguistic studies of the perfective le [3,25,26,28,29] and explored its effectiveness. Results of this study didn’t show that the multimedia program as a self-learning tool outperform the printed materials significantly. Nevertheless, the result indicated that both the interactive multimedia program and the printed materials within their own groups do have significant effects on the members of the individual groups. This significance is the evidence supporting that the CSL program of le based on linguistic generalizations is effective. * This research was based on the project funded by the National Science Council (NSC) of the Republic of China under Contract No. NSC 97-2631-S-415-002. This paper is a revised and developed version of the first author’s master’s thesis. The second author is the primary investigator of this project and the third author is the co-investigator. We thank Dr. Jenny Yi-Chun Kuo and Dr. Jung-hsing Chang for their insightful comments at the oral defense. We also thank the anonymous reviewers of the ROCLING conference for their precious comments. All remaining errors are ours. 293  ኴ૞ ழᎎᑑಖψԱωԫऴ੡խ֮੡รԲ؆፿ᖂ฾ृऱܺᣄរհԫΔࢬ‫ءא‬ઔߒ௅ᖕ፿ߢᖂઔ ߒ๻ૠψԱωհ‫ڍ‬໾᧯յ೯ᓰ࿓Δࠀ൶ಘ‫ڍڼ‬໾᧯ᓰ࿓ኙխ֮੡รԲ؆፿ᖂ฾ृऱᖂ฾ ‫ګ‬யΖ࿨࣠᧩‫ق‬Δ‫܂‬੡۞ᖂՠࠠऱ‫ڍ‬໾᧯ᓰ࿓ઌለ࣍౐‫ء‬Δࠀ޲‫᧩ڶ‬ထऱᖂ฾‫ګ‬யΔྥ ۖΔ‫ڍ‬໾᧯ᓰ࿓ࡉ౐‫ࢬܑٺࠡڇء‬᥆հิܑխᖂ฾।෼ऱ᧩ထ஁ฆΔᎅࣔຍଡψԱωᓰ ࿓ኙխ֮੡รԲ؆፿ऱᖂ฾ृ‫ܗࢬڶ‬ટΖʳ Keywords: Chinese Aspect Marker le, Chinese as a Second Language Learners, Interactive multimedia program ᣂ᝶ဲΚழᎎᑑಖψԱωΔխ֮੡รԲ؆፿ᖂ฾ृΔ‫ڍ‬໾᧯յ೯ᓰ࿓ 
In this paper, we propose a framework for combining outputs from multiple on-line machine translation systems. This framework consists of several modules, including selection, substitution, insertion, and deletion. We evaluate the combination framework on IWSLT07 in travel domain, for the translation direction from Chinese to English. Three diﬀerent on-line machine translation systems, Google, Yahoo, and TransWhiz, are used in the investigation. The experimental results show that our proposed combination framework improves BLEU score from 19.15 to 20.55. It achieves an absolute improvement of 1.4 in the BLEU score. Keyword: Machine translation, System combination 
Speech recognition is one of the im portant p arts of search field in speech processing. Nevertheless, the speec h environment and speec h distance w ill mainly affect the reco gnition result. In this paper, a high adaptation far-fie ld noise speech recognition system is proposed. This system is combined with the m ethods of independent component analysis and subspace speech enh ancement, and then furth er filteri ng the noise of speech to improve the speech quality for recognition. The experimental results show that the proposed system is suitable for several presented noisy environments, and it can effectively improve the recognition rate. For the SNR evaluation, this proposed sy stem can make enhanced speech SNR with 20dB higher than original corrupted speech which ranges from 0dB to 10dB. 333  關鍵詞：語音辨識，盲訊號分離法，獨立成分分析，子空間語音增強，麥克風陣列 Keywords: Speech Recognition, Blind Source S eparation, Independent Component Analysis, Subspace Speech Enhancement, Microphone Array. 一、緒論 語言為人類彼此溝通時，最原始同樣也是最有效的方式，在科技蓬勃發展的現今， 如何使電腦辨識人類語言也成為語音處理上重要議題其中之一，因此對於語音辨識系 統，如何達到有效且精確的辨識結果，也是目前語音處理領域中熱門的研究議題。 對於語音辨識結果，影響語音辨識結果的相關因素很多，這些相關因素都會造成語 者語意和語音辨識結果的不匹配(mismatch)，其中影響辨識結果最重要的因素為環境中 所存在的背景雜訊，由於語音所存在的背景環境中，並非完全沒有遭受其他干擾雜訊影 響，例如在餐廳環境、地鐵環境、車內行駛環境等，都有背景雜訊的干擾源存在，這些 背景雜訊伴隨著語音進入辨識系統中，會嚴重影響到整體辨識結果，另外語者與辨識系 統距離也是另一種影響辨識結果的因素，語音能量會伴隨著距離而逐漸衰減，因此衰減 後的語音能量也會造成辨識率的降低。 圖一、背景環境雜訊干擾語音示意圖 為了改善上述所提到之環境雜訊以及語者距離所造成的辨識結果不匹配，我們針對 此雜訊語音做進一步分析，首先雜訊語音中包含了大量的雜訊資訊，因此如何取得雜訊 部份並加以去除為第一步重要的處理步驟，濾除相關的背景雜訊後，再來則是語者和辨 識系統之間的距離問題，當距離相距越大時，語音辨識系統所接收到的語音能量則越 小，因此對於濾除雜訊後的語音訊號，必須再進一步使用語音增強技術將加強語音訊號 能量，以提升之後的辨識結果，最後在進行辨識之前，再將增強後之語音訊號做端點偵 測處理，找出一段語音訊號中語音的實際位置再取得此語音資訊來進行辨識。 根據上述分析結果，在雜訊分離部份，我們採用盲訊號分離(Blind Signal Separation, BSS)的方法，使用獨立成分分析(Independent Component Analysis, ICA)方式來進行訊號 分離，取出相近似語音成分較多的部份，再透過子空間語音增強方式(Subspace Speech Enhancement)，將取出的語音訊號進一步去除殘餘噪聲並加強語音訊號，使其可用來進 行語音辨識之用，最後再利用語音活動偵測法(Voice Activity Detection, VAD)來偵測語音 所在位置，藉此來提升辨識效率。最後在末端的語音辨識器方面，我們使用英國劍橋大 334  學所提供的 HTK(Hidden Markov Model T oolkit)語音套件來進行識別，並判斷所產生的 結果是否正確。 本論文總共分成五個章節，第一章節為緒論，第二章節為本論文針對此辨識系統所 採用之各種研究方法並詳細加以介紹，第三章節則是介紹此語音辨識系統之系統架構， 第四章節則是實驗環境評估和設定以及實驗結果，最後第五章節則是對此辨識系統做一 精要結論及未來相關工作。 二、研究方法 本章節針對此遠距離雜訊語音辨識系統所採用的各種方法來加以詳述說明介紹。 （一）獨立成分分析法(Independent Component Analysis, ICA) 對於帶有噪聲的語音成分，由於原始語音成分和背景雜訊成分均為未知，因此要分離此 兩種未知訊號，我們可使用盲訊號分離方式，將此兩種未知訊號，分別從混合訊號中分 離出來，一般盲訊號分離問題可由下面示意圖表示：  圖二、盲訊號分離問題示意圖  如圖二所示，兩未知聲源訊號 s1 及 s2，透過混合矩陣 A 後，在麥克風接收端則會接收 到兩種混合訊號 x1 和 x2，此關係可由下列線性方程式表示。  x1 x2   a11s1  a12 s2  a21s1  a22 s2  ,  A    a11 a21  a12   a22     (1)  因此若假設聲源訊號 s1 為語者的語音成分、s2 為噪音成分，我們可從所接收到之混合訊 號 x1 及 x2 分離出原始的語音訊號以及噪音訊號，即可有效的去除雜訊，根據上述公式 (1)，要求得原始訊號 x1 和 x2，必須找出一個解混合矩陣 A ，使得接收到的混合訊號經 由 A 轉換後，可得到原來的聲源訊號，而此求解 A 之方法即為獨立成分分析法。  在使用獨立成分分析法求得解混合矩陣前，必須先行假設訊號源彼此獨立，然而在真實 情況下，訊號源並非都會彼此互相獨立，因此在進行獨立成分分析流程前，必須先經過 前置處理後才能找尋解混合矩陣，在此我們前置處理方式為集中變數(Centering)以及資 料白色化(Whitening)處理，在此先針對集中變數及資料白色化來作為說明。  335  
Search engines return thousands of pages per query. Many of them are relevant to the “query words”but not interesting to the “users”due to different domain-specific meanings of the query terms. Re-classification of the returned documents based on domain specific meanings of the query terms would therefore be most effective. A cross domain entropy (CDE) measure is proposed to extract characteristic domain specific words (DSW’s) for each node of existing hierarchical web document trees. Domain specific class models are built based on the respective DSW’s. Such class models are then used for directly classifying new documents into the hierarchy, instead of using hierarchical clustering techniques. High accuracy can be achieved with very few domain specific words. With only the top 5~10% DSW’s and a maximum entropy based classifier, 99% accuracy is observed when classifying documents of a news web site into 63 domains. The precision and recall of the extracted domain specific words are also higher than those extracted with conventional TF-IDF term weighting method. Keywords: Domain Specific Words, Hierarchical Classification, Maximum Entropy Classifier, Cross-Domain Entropy. 
In this paper, integration of speaker identification and speech recognition for intelligent doorway application has been proposed. Two target speakers will be identified through an one-word speech utterance. Moreover, this utterance will be recognized to be a pre-defined speech command. The speaker identification in the proposed framework is based on support vector machine (SVM). The “one-versus-one” approach is applied in this paper to classify test point input utterance according to the number of votes. As for the speech recognition, we use confusion matrix to develop an efficient phonetic set for a command-based multi-lingual system, the confusion matrix calculates acoustic similarities between every two phonemes. The proposed framework has been realized in the intelligent doorway application and will be applied to many other daily life computer speech applications. Keywords: SVM, confusion matrix, HTK, speaker identification, speech recognition. 1. Introduction In the real world, there are three commonly applications in speech recognition system, such as “who is speaker?”, “what is content?”, and “where is speaker?”. The contribution of this paper is to propose a practical consolidated framework to integrate both the speaker identification and speech recognition, with the aim at satisfaction of human computer interface in recognizing “who is speaker?” and “what is content?” at same time. Support Vector Machine has been explored and proved in speaker recognition for many years [1][2]. SVM has many desirable attributes that can classify and robust to sparse data without over-training and to make linear and non-linear decision via kernel functions [3]. However, due to complicated algorithm and time-consuming process in training SVM, thus it still not gained widespread utilization in many applications. Ubiquitous Robot Companion (URC) proposed a text-independent speaker identification using microphone-array on a robot and intends to enrich the interaction between human and robot [4]. Far-field speaker recognition proposed two approaches to improve the robustness of speaker recognition. The first is to use the conventional method based on acoustic feature. The second approach is to make use of higher-level 373  linguistic feature. However, the adverse environmental condition and adverse training-testing conditions still need to be considered and conquered under proposed benchmark environment [5]. Ubiquitous and robust text-Independent speaker recognition [6] proposed a new microphone-array configuration of framework for benchmark. This framework is used a mixer to received speech signal from six microphones, then the six channel speech signal are mixed and output only one signal for feature extraction. The mixed-language speech recognition has been researched for many years [7][8][9]. In this proposed consolidated of speech recognition system, the speaker independent voice command recognition is adopted, and with a string size of tens or more words. In addition, an acoustic and phoneme modeling based on confusion matrix for ubiquitous mixed-language speech of recognition system is integrated in proposed framework [10]. This system allows users to use given command to control electrical device via speech. The system is also flexibly applied in different command-based control applications by changing the dictionary description and grammar in each new work. The reminder of this paper is organized as follows. In Section 2, the basic theories of SVM algorithm for data classification as well as confusion Matrix of acoustic model for bilingual speech recognition are described. In Section 3, the proposed framework of consolidated speech recognition system is presented. The experimental results of proposed architecture are shown in Section 4. Finally, we draw our conclusion in Section 5.  2. Literature Review  2.1 SVM based Speaker Identification  The main concept of SVM is to use a partition hyperplane to maximize the distance  between support vectors of two classes features, and then to create a classifier between  two clusters of sample. The gain of the SVM-based pattern recognition method is  robust to sparse training data samples [11] [12]. This optimal hyperplane is obtained  by minimizing the following constrained optimization problem as shown in Eq. (1).   min w,b,  
In this paper, a novel entropy-based voice activity detection (VAD) algorithm is presented in variable-level noise environment. Since the frequency energy of different types of noise focuses on different frequency subband, the effect of corrupted noise on each frequency subband is different. It is found that the seriously obscured frequency subbands have little word signal information left, and are harmful for detecting voice activity segment (VAS). First, we use bark-scale wavelet decomposition (BSWD) to split the input speech into 24 critical subbands. In order to discard the seriously corrupted frequency subband, a method of adaptive frequency subband extraction (AFSE) is then applied to only use the frequency subband. Next, we propose a measure of entropy defined on the spectrum domain of selected frequency subband to form a robust voice feature parameter. In addition, unvoiced is usually eliminated. An unvoiced detection is also integrated into the system to improve the intelligibility of voice. Experimental results show that the performance of this algorithm is superior to the G.729B and other entropy-based VAD especially for variable-level background noise. Keywords: Voice Activity Detection, Bark-Scale Wavelet Decomposition, Adaptive Frequency Subband Extraction. 1. Introduction Voice activity detection (VAD) refers to the ability of distinguishing speech from noise and is 385  an integral part of a variety of speech communication systems, such as speech coding, speech recognition, hands-free telephony, audio conferencing and echo cancellation [1]. In the GSM-based wireless system, for instance, a VAD module [2] is used for discontinuous transmission to save battery power. Similarly, a VAD device is used in any variable bit rate codec [3] to control the average bit rate and the overall coding quality of speech. In wireless systems based on code division multiple access, this scheme is important for enhancing the system capacity by minimizing interference. Common VAD algorithms use short-term energy, zero-crossing rate and LPC coefficients [4] as feature parameters for detecting voice activity segment (VAS). Cepstral features [5], formant shape [6], and least-square periodicity measure [7] are some of the more recent metrics used in VAD designs. In the recently proposed G.729B VAD [8], a set of metrics including line spectral frequencies (LSF), low band energy, zero-crossing rate and full-band energy is used along with heuristically determined regions and boundaries to make a VAD decision for each 10 ms frame. In this paper we present a robust VAD algorithm for the detection of speech segment, which is based on the entropy of the spectrum domain of selected critical subband. First, the bark-scale wavelet decomposition (BSWD) is utilized to decompose the input speech signal into 24 critical subband signals. In contrast to the conventional wavelet packet decomposition, the BSWPD is designed to match the auditory critical bands as close as possible and has been applied into various speech processing systems [9, 10]. The entropy, on the other hand, a measure of amount of expected information, is broadly used in the field of coding theory. Shen et al. [11] first used it on speech detection and revealed that voiced spectral entropy is quite different from non-voiced one. Based on this character, the entropy-based approach is more reliable than pure energy-based methods in some cases, particularly when noise-level varies with time. Since the frequency energy of different types of noise focus on different frequency subbands, 386  Figure 1. The Block Diagram of Proposed VAD Algorithm the effect of corrupted noise on each frequency subband is different [12]. The seriously obscured frequency subbands have little word signal information left, and are harmful for detecting VAS. Based on the finds, we adopt the theory of adaptive frequency subband extraction (AFSE) to only uses the frequency subband which are slightest corrupted and discard the seriously obscured ones. The frequency subband energies are sorted and only the first several frequency subband with the highest energy are selected. Experiment results show that when more frequency subbands are corrupted by noise, the number of the selected frequency subbands decreases with the decrease of the SNR. A measure of entropy defined on the spectrum domain of selected frequency subband by the AFSE approach is proposed to refine the classical entropy-based VAD [12]. Finally, an unvoiced detection is integrated into entropy-based VAD system to improve the intelligibility of voice. 2. Implementation of the Proposed VAD Algorithm In the block diagram shown in Fig. 1, the proposed VAD algorithm consists of five main parts: 387  bark-scale wavelet decomposition, adaptive frequency subband extraction, calculation of spectral entropy, adaptive noise estimation, and unvoiced decision. In this section, the five main parts are described in turn. 2.1 Bark-scale wavelet decomposition (BSWD) Critical subband is widely used in perceptual auditory modeling [13]. In this section, we propose the wavelet tree structure of BSWD to mimic the time-frequency analysis of the critical subbands according to the hearing characteristics of human cochlea. A BSWD is used to decompose the speech signal into 24 critical wavelet subband signals, and it is implemented with an efficient five-level tree structure. The corresponding BSWD decomposition tree can be constructed as shown in Fig. 2. Observing the Fig.2, the input speech signal is obtained by using the high-pass filter and low-pass filter [14], implemented with the Daubechies family wavelet, where the symbol ↓ 2 denotes an operator of downsampling by 2. Figure 2. The Tree of Bark-Scale Wavelet Decomposition (BSWD) 2.2 Adaptive frequency subband extraction (AFSE) In fact, the frequency energies of difference types of noise are concentrated on different frequency subbands. This observation demonstrates that not all the frequency subbands have 388  harmful word signal information. In our algorithm, we must use only the useful frequency subbands or discard the harmful subbands for detecting VAS. Since our goal is to select some useful frequency subbands having the maximum word signal information, we need a parameter to stand for the amount of word signal information of each frequency subband. According to Wu et al. [12], the estimated pure speech signal is a good indicator. The frequency subbands energy of pure speech signal is accomplished by removing the frequency energy of background noise from the frequency energy of input noisy speech.  For the mth frame, the spectral energy of the ξ th subband is evaluated by the sum of  squares:  ∑ωξ ,h E(ξ , m) = X (ω, m) 2 ,  (1)  ωξ ,l  where X (ω,m) means the ωth wavelet coeffience. ωξ ,l and ωξ ,h denote the lower boundaries and the upper boundaries of the ξ th subband, respectively.  The ξ th frequency subbands energy of pure speech signal of the mth frame Eɶ (ξ , m) is estimated:  Eɶ (ξ , m) = E(ξ ,m) − Nɶ (ξ , m),  (2)  where Nɶ (ξ, m) is the noise power of the ξ th frequency subband.  During the initialization period, the noisy signal is assumed to be noise-only and the noise spectrum is estimated by averaging the initial 10 frames. To recursively estimate the noise power spectrum, the subband noise power, Nɶ (ξ, m) , can be adaptively estimated by smoothing filtering and be discussed later. It is found that the more the frequency subband covered by noise would result in the smaller the Eɶ (ξ , m) . Since the frequency subband with higher Eɶ (ξ , m) contains more pure speech  389  Figure 3. The Results of Correct Detection Accuracy with Number of Different Frequency Subband at –5dB, 10 dB and 30 dB under Three Types of Noise. information, we should sort the frequency subband according to their Eɶ (ξ , m) value.  That is,  Eɶ (I1, m) ≥ Eɶ (I2 , m) ≥ ⋯ ≥ Eɶ (IN , m),  (3)  where Ii is the index of the frequency subband with the ith max energy.  It means that the index of the frequency subband with higher energy is the more useful index  of one. Moreover, we should only select the useful frequency subbands for VAD results  output. That is, the first N frequency subbands I1, I2,…, IN are selected and denoted as the useful number of frequency subband, Nub , for the succeeding calculation of spectral entropy. According to the relation between the number of useful frequency subbands Nub and SNR  (shown as Fig. 3), we can see that the number of useful frequency subband increases with the  increase of SNR under three types noises including white noise, factory noise and vehicle  noise. Nub = 9 and Nub = 24 denote the boundary of Nub among the range from -5dB to  30dB, respectively.  390  Based on the above finds, a linear function can be used to simulate the relationship between Nub and SNR , and shown as Fig. 4.  Nub  (m)  =    [(24   −  9)  ×  9 (SNR(m) − (−5)) 30 − (−5)  +  9]  , SNR(m) < −5dB ,-5dB ≤ SNR(m) ≤ 30dB  (4)    24  ,SNR(m) > 30dB.  where [⋅] is the round off operator, and SNR(m) denotes a frame-based posterior SNR for the mth frame.  In addition, SNR(m) is depended on the all summation of subbnad-based posterior SNR snr(ξ , m) on the ξ th useful subband and defined as:  ∑ SNR(m) = 10log10 snr(ξ , m),  (5)  ξ ∈Nub  where  snr(ξ , m) =  X (ξ , m) 2 Nɶ (ξ , m) .  Figure 4. A Linear Function of the Relationship Between Nub and SNR 391  2.3 Calculation of spectral entropy  To calculate the spectral entropy, the probability density function (pdf) and the entropy calculation are both necessary steps.  The pdf for the spectrum can be estimated by normalized the frequency componemts:  P(ξ , m) = E(ξ , m) ∑N E(ω, m)  (6)  ω =1  where P(ξ , m) is the corresponding probability density, and N denotes the total number of critical subbnad divided by BSWD ( N = 24 in this paper).  Some frequency subbands, however, are corrupted seriously by additive noise, and those harmful subbands may result in low performance of entropy-based VAD if those are extracted. Moreover, we use only the useful frequency subbands to calculate a measure of entropy defined on the spectrum domain of selected frequency subbands. The probability associated with subband energy modified from (6) is described as follows:  ∑Nub P(ξ , m) = E(ξ ,m) E(ω, m),  (7)  ω =1  where Nub is the number of useful frequency subbands.  Having finishing applying the above constraints, the spectral entropy H (m) of frame m can be defined below.  ∑Nub H (m) = − P(ξ , m) ⋅ log[P(ξ ,m)].  (8)  ξ =1  The foregoing calculation of the spectral entropy parameter implies that the spectral entropy depends only on the variation of the spectral energy but not on the amount of spectral energy. Consequently, the spectral entropy parameter is robust against changing level of noise.  392  2.4 Adaptive noise estimation  To recursively estimate the noise power spectrum, the spectral power of subband noise can be estimated by averaging past spectral power values using a time and frequency dependent smoothing parameter as following:  Nɶ (ξ , m) = α (ξ , m) ⋅ Nɶ (ξ , m −1) + (1 − α (ξ , m)) ⋅ E(ξ , m)  (9)  where α (ξ , m) means the smoothing parameter and be defined as  1,  α (ξ , m)  =   1 +  
Word boundary detection in variable noise-level environments by support vector machine (SVM) using Low-band Wavelet Energy (LWE) and Zero Crossing Rate (ZCR) features is proposed in this paper. The Wavelet Energy is derived based on Wavelet transformation; it can reduce the affection of noise in a speech signal. With the inclusion of ZCR, we can robustly and effectively detect word boundary from noise with only two features. For detector design, a Gaussian-kernel SVM is used. The proposed detection method is applied to detection word boundaries for an isolated word recognition system in variable noisy environments. Experiments with different types of noises and various signal-to-noise ratios are performed. The results show that using the LWE and ZCR parameters-based SVM, good performance is achieved. Comparison with another robust detection method has also verified the performance of the proposed method. Keywords: Speech detection, word boundary detection, support vector machine, wavelet transform, noisy speech recognition. 1. INTRODUCTION For speech recognition, the detection of speech affects recognition performance. A robust word boundary detection method in the presence of variable-label noises is necessary and is studied in this paper. Depending on the characteristics of speech, a variety of parameters have been proposed for boundary detection. They include the time energy (the magnitude in time domain), zero crossing rate (ZCR) [1] and pitch information [2]. These parameters usually fail to detect word boundary when signal-to-noise ratio (SNR) is low. Another parameter concerning frequency domain has also been recently proposed. According to the frequency energy, the time-frequency (TF) parameter [3] which sums the energy in time domain and the frequency energy was presented. The TF-based algorithm may work well for fixed-level background noise. However, its detection performance degrades for background noise of various levels. For this problem, some modified TF parameters are proposed [4]. In [5], the idea of using Wavelet transform features as speech detection features was proposed. In this paper, we present a new Low-band Wavelet Energy (LWE) parameter which separates the speech from noise in the domain of Wavelet transform. Computation of the WE parameter is easier than the modified TF parameters, and it is shown in the experiment section that a better detection performance is achieved. 21  After the features for detection have been extracted, the next step is to determine thresholds and decision rules. Many decision methods based on computational intelligence techniques have been proposed, such as fuzzy neural networks (FNNs) [4] and neural networks (NNs) [6]. Generalization performance may be poor when FNNs and NNs are over-trained. To cope with the low generalization ability problem, a new learning method, the Support Vector Machine (SVM), has been proposed [7, 8]. SVM is a new and useful learning method whose formulation is based on the principle of structural risk minimization. Instead of minimizing an objective function based on training, SVM attempts to minimize a bound on the generalization error. SVM has gained wide acceptance due to its high generalization abilities for a wide range of applications. For this reason, this paper used a SVM as a detector. The rest of the paper is organized as follows. Section II introduces the derivation and analysis of the WE and ZCR parameters. Section III describes the SVM detector. Experiments on word boundary detection for noisy speech recognition are studied in Section IV. Finally, Section V draws conclusions.  2. ROBUST DETECTION PARAMETERS  Wavelet Transform (WT) is a technique for analyzing the time-frequency domain that is  most suited for a non-stationary signal [9]. For short-time analysis and discrete speech signal,  discrete-time WT (DTWT) is used. Let the amplitude of the k th point in the i th frame of a  noisy speech signal be denoted by s(i, k) and the frame length in sample number be  represented by N . The DTWT of the i -th speech frame is as follows,  ∑ DTWT(m, n) =  
In this paper, we investigate the noise-robustness of features based on the cepstral time coefﬁcients (CTC). By cepstral time coefﬁcients, we mean the coefﬁcients obtained from applying the discrete cosine transform to the commonly used mel-frequency cepstral coefﬁcients (MFCC). Furthermore, we apply temporal ﬁlters used for computing delta and acceleration dynamic features to the CTC, resulting in delta and acceleration features in the frequency domain. We experiment with ﬁve different variations of such CTC-based features. The evaluation is done on the Aurora 3 noisy digit recognition tasks with four different languages. The results show all but one such feature set performance gain, the other feature sets actually lead to performance gains. The best feature set achieves an improvement of 25% over the baseline feature set of MFCC. Keywords: MFCC, CTC, delta, robust feature 1. Introduction A front-end of a speech recognition system may consist of several stages for noise-robustness to achieve good performance. In the early stage of spectral domain, well-known methods such as spectral subtraction [1] and Wiener ﬁlter [2] may be applied. In the middle stage of cepstral domain, the mel-frequency cepstral coefﬁcients (MFCC) are commonly used as the static feature set. In the postprocessing stage, there may be normalization, temporal information integration, and transformation modules. It has been observed that simple normalization approaches, such as the cepstral mean subtraction (CMS) [3], cepstral variance normalization (CVN) [4], and histogram normalization (HEQ) [5] can lead to signiﬁcant performance improvement in recognition accuracy in noisy environment. Apparently such methods are capable of alleviating the mismatch between the clean and noisy data. In this paper we investigate novel features based on simple transformation methods. Speciﬁcally, we insert a window of static cepstral vectors in a matrix and then apply the discrete cosine transform (DCT) along the temporal axis. The coefﬁcents after the DCT is called the cepstral time coefﬁcients, 31  Figure 1: The block diagram of the proposed feature transformation methods.  and the resultant matrix is called the cepstral time matrix (CTM) [6, 7]. After CTM for each frame is extracted, we further apply normalization and routines for delta and acceleration feature extraction to the cepstral time coefﬁcients. The transformed features are combined with the static MFCC features to form the ﬁnal feature vector. This paper is organized as follows. Section 2 deﬁnes the cepstral time matrix and introduces the investigated feature transformations. The experimental setup and recognition results are described in Section 3. In Section 4, we draw conclusions.  2. Feature Transformations  Our feature extraction and transformation process is illustrated in Figure 1. We begin with a review of the cepstral time matrix, which is followed by the mathematical deﬁnition of the proposed additive transformation methods.  2.1. Cepstral Time Coefﬁcients  We ﬁrst insert a ﬁxed number of adjacent feature vectors in a matrix  
In this paper, we propose a novel scheme in performing feature statistics normalization techniques for robust speech recognition. In the proposed approach, the processed temporal-domain feature sequence is first converted into the modulation spectral domain. The magnitude part of the modulation spectrum is decomposed into non-uniform sub-band 39  segments, and then each sub-band segment is individually processed by the well-known normalization methods, like mean normalization (MN), mean and variance normalization (MVN) and histogram equalization (HEQ). Finally, we reconstruct the feature stream with all the modified sub-band magnitude spectral segments and the original phase spectrum using the inverse DFT. With this process, the components that correspond to more important modulation spectral bands in the feature sequence can be processed separately. For the Aurora-2 clean-condition training task, the new proposed sub-band spectral MN, MVN and HEQ provide relative error rate reductions of 18.66% and 23.58% over the conventional temporal MVN and HEQ, respectively. 一、簡介 雖然語音科技進步迅速，但自動語音辨識(automatic speech recognition, ASR)[1]仍是 一門值得繼續研究開發的課題。目前多數的語音辨識系統若在不受干擾的安靜環境下， 一般而言皆能得到相當滿意的辨識效果，然而若將其應用於實際的生活環境中，辨識效 能便會有所衰減，主要是實際生活環境中有許多的變異性(variation)影響辨識效能，其 中 影 響 語 音 辨 識 的 變 異 性 有 訓 練 環 境 與 測 試 環 境 之 間 的 環 境 不 匹 配 (environmental mismatch)、語者變異性(speaker variation)及發音的變異性(pronunciation variation)等因 素，這些因素都會明顯影響語音辨識系統的效能。因此在近幾十年來，持續不斷有許多 學者研究努力改善上述幾類的語音變異性，進而使語音辨識系統能更有效地運用於真實 的生活環境中。 針對環境不匹配所發展的許多強健性方法，大致上包含了特徵補償與模型補償兩大 類型，而特徵補償方法中其中有一類別的方向是針對語音辨識所用的特徵參數之統計量 作正規化處理，這些處理通常是作在特徵之時間序列域(temporal domain)上，例如倒頻 譜平均值正規化法(cepstral mean normalization, CMN)[2]、倒頻譜平均值與變異數正規化 法 (cepstral mean and variance normalization, CMVN)[3] 與 統 計 圖 等 化 法 (histogram equalization, HEQ)[4]等。 以上各種方法主要是執行在語音特徵的時間序列域上，但在其效能的分析上，我們 通常會去探討雜訊及通道效應對於原始特徵之調變頻譜的失真，及這些方法對於此失真 的改善程度，因此近年來，開始有學者提出直接於特徵之調變頻譜域上使用特徵統計正 規化法，如調變頻譜統計圖等化法(spectrum histogram equalization, SHE)[5]，此方法是 針對調變頻譜的強度頻譜之機率分佈(probability distribution)作正規化處理，驗證了直接 針對調變頻譜的強度成份作機率分佈的正規化的確帶來了明顯的特徵強健性效果。但是 以上各種技術，皆是直接或間接將語音特徵序列的全調變頻帶資訊作整體的處理，並未 對各不同的頻帶有不同的考慮。然而，根據許多的研究[6][7]證實，對語音辨識而言， 不同頻率的調變頻譜成份具有不相等的重要性；在文獻[8]中更明確地提到，調變頻譜 的偏低頻率成份資訊對於語音辨識有較大的助益，其中又以 1~16 Hz 之調變頻帶範圍的 成份最為重要。藉由以上之各觀點，在本論文中，我們提出了基於強度頻譜之分頻段調 變頻譜統計正規化法，一方面希望如 SHE 法，直接對於語音特徵序列之調變頻譜作正 規化處理，另一方面，則是希望在新方法中能異於過去之全調變頻帶之資訊一併處理的 方式，將調變頻帶作一系列的頻段切割，在每個子頻段中加以正規化其調變頻譜，進而 更有效地凸顯正規化的效能；在後面章節之一系列的實驗中，我們將呈現所提出之新方 法確實可以更有效地提昇語音特徵在雜訊環境的強健性，達到我們以上所提的目的。 本論文其他章節概要如下：在第二章中，我們介紹本論文所提出之分頻段的統計正 規化法其背景、原理及其相關的步驟說明。第三章將呈現並討論一系列分頻段調變頻譜 統計正規化法的實驗結果，並與其他時間序列域上的強健性技術結合，對此類結合方式 40  的辨識實驗加以探討與分析，以驗證此類結合方式是否具有良好的加成性。而在第四章 裡，則為一簡要的結論與未來展望。  二、基於強度頻譜之分頻段調變頻譜統計正規化法  在這一章中，我們將對所新提出的分頻段調變頻譜統計正規化法之背景與步驟作詳 細的說明，並且將以一段受雜訊干擾的語句為例，驗證這些新方法在降低雜訊干擾的效 能，及與其他相類似方法的初步比較。 (一) 分頻段調變頻譜統計正規化法 在本論文所提的新方法中，我們嘗試將調變頻譜中的強度頻譜(magnitude spectrum) 切割成許多子頻段，再分別對各自子頻段的統計值作正規化處理；我們所用的正規化演 算法，包括了除了文獻[5]之 SHE 技術所用的統計圖等化法(HEQ)外，也額外使用了較 簡易執行的平均值正規化法(MN)與平均值與變異數正規化法(MVN)，以期它們相較於 傳統全頻帶式的正規化法而言，能帶來更明顯的效能，或是能有效減低執行的複雜度。 我們所提的分頻段調變頻譜正規化法的詳細步驟分列於下： 1. 假設一段語音之梅爾倒頻譜特徵參數序列以下式(2-1)表示：  {x (m) [n ];1 ≤ n ≤ N }, 1 ≤ m ≤ M ,  式(2-1)  其中 M 為一語音特徵向量中特徵個數， N 表示為此單一語句的音框總數。每個特徵序 列 {x (m) [n ]} 經正規化處理後，以 {x(m) [n ]} 表示，我們希望新的特徵序列 {x(m) [n ]} 相對於  原始特徵序列而言，更具有強健性，使辨識效果有明顯地提升。在之後的敘述，為了精  簡符號的標示，我們省略了上標 " (m)" 符號。 2. 將特徵序列{x [n ];1 ≤ n ≤ N } 經 N 點離散傅立葉轉換(discrete Fourier transform, DFT) 後得到其調變頻譜{X [k ]} ，如下式。  ∑ X  [k  ]  =  N −1  x  −j 2πnk [n ]e K  ,  n=0  0 ≤ k ≤ ⎡⎢⎢N2 ⎤⎥⎥  式(2-2)  假設 {x [n ]} 的音框取樣頻率(frame rate)為 Fs Hz，則在其調變頻譜域上{X [k ]} 的頻率範  圍為  ⎡⎢⎢⎣0,  Fs 2  ⎤⎥⎥⎦  ；而由於  X  [k  ]為一複數，我們以極座標(polar  form)表示  X  [k  ]如下式：  X [k ] = A[k ]e jθk  式(2-3)  其中 A[k ] 是 X [k ]的強度成份， θ [k ] 是 X [k ]的相位成份，接下來我們只針對強度成份  {A[k ]}作調整，而保留相位成份{θ [k ]} 不變。  3.  將上一步驟調變頻譜的強度成分 ⎪⎨⎪⎪⎩⎧A[k ]; 0 ≤ k  ≤  ⎡⎢⎢  N 2  ⎤⎥⎥⎪⎬⎪⎪⎭⎫  以不等切(non-uniform)且倍頻  (octave)的方式，切割成 L 個頻段，每個頻段的範圍如下式(2-4)所示：  ⎪⎩⎪⎪⎧⎪⎪⎪⎪⎪⎨⎪⎪⎢⎣⎢⎢⎡⎢⎣⎡022,L−2−21L1−⎜⎝⎜⎛1F2⎜⎛⎜⎝s  Fs 2 ⎠⎟⎟⎟⎞,  ⎟⎟⎞⎠⎟⎥⎤⎥⎦ , 2 −1 2L−1  ⎝⎜⎜⎛  Fs 2  ⎠⎟⎟⎟⎞⎥⎦⎥⎤  ,  if = 1. if = 2, 3,..., L.  式(2-4)  由上式可以得知，調變頻譜低頻帶的部分被切割成較多個頻段，且每個頻段的長度較  41  短，相對地，高頻的部分被切割成較少的頻段，且每個頻段的長度較長。在將{A[k ]} 作  上述的頻段切割後，我們以{A } ⎡⎣k ' ⎤⎦ 表示其中的第 L 個頻段。此對於頻段不等切的原因，  在於我們之前所提，低調變頻帶對於語音辨識較為重要，理應分較多的頻段來個別處  理，而高調變頻帶相對而言較不重要，所以可將較大的頻段範圍一併處理。 4. 我們將上一步驟所得之不同頻段的強度頻譜{A ⎡⎣k ' ⎤⎦}作統計正規化處理。我們使用的  正規化法分別為：平均值正規化法(MN)、平均值與變異數正規化法(MVN)與統計圖等  化法(HEQ)，處理後的特徵即以{A } ⎡⎣k' ⎤⎦ 表示。詳細地說，平均值正規化法(MN)在此的  計算方式以下式(2-5)表示：  A ⎣⎡k ' ⎦⎤ = A ⎡⎣k ' ⎤⎦ − μ ,s + μ ,a ,  式(2-5)  其中，μ ,s 為單一(single)語句之分頻段強度頻譜的平均值，μ ,a 為全部(all)訓練語句之分  頻段強度頻譜的平均值。  平均值與變異數正規化法(MVN)在此的計算方式以式(2-6)表示：  A  ⎡⎣k ' ⎤⎦ = ⎜⎛⎜⎜⎜⎝A  ⎡⎣k ' ⎤⎦ σ  − ,s  μ  ,s  ⎟⎟⎟⎟⎟⎞⎠  ⋅  σ  ,a  +μ ,a  式(2-6)  其中， μ ,s 為單一語句之分頻段強度頻譜的平均值， σ ,s 為單一語句之分頻段強度頻譜 的標準差， μ ,a 為全部訓練語句之分頻段強度頻譜的平均值， σ ,a 為全部訓練語句之分  頻段強度頻譜的標準差。  統計圖等化法(HEQ)在此的計算方式以式(2-7)表示：  ( ( )) A  ⎣⎡k ' ⎦⎤  =  F −1 ,a  F ,s  A  ⎣⎡k ' ⎦⎤  式(2-7)  其中 F ,s (i) 為單一語句之分頻段強度頻譜的機率分佈，F ,a (i)為全部訓練語句之分頻段  強度頻譜的機率分佈。 5. 在處理完每一頻段之後，我們將各頻段的強度頻譜{A } ⎡⎣k' ⎤⎦ 照其頻率大小順序重新串  接起來，得到新的全頻段強度頻譜⎧⎪⎨⎪⎪⎩A[k ]; 0 ≤ k ≤ ⎡⎢⎢N2 ⎤⎥⎥⎫⎪⎬⎪⎪⎭ ，此即為統計正規化法處理後的 調變頻譜之強度成份，接著將{A[k ]}補回式(2-3)中的原本相位成分{θ [k ]}，再經逆轉換  離散傅立葉轉換(inverse discrete Fourier transform, IDFT)所得新的特徵 x [n ]，如下式(2-8)  表示：  ∑ ( ) x [n ] =  
Data acquisition is a major concern in text classification. The excessive human efforts required by conventional methods to build up quality training collection might not always be available to research workers. In this paper, we look into possibilities to automatically collect training data by sampling the Web with a set of given class names. The basic idea is to populate appropriate keywords and submit them as queries to search engines for acquiring training data. Two methods are presented in this study: One method is based on sampling the common concepts among the classes, and the other based on sampling the discriminative concepts for each class. A series of experiments were carried out independently on two different datasets, and the result shows that the proposed methods significantly improve classifier performance even without using manually labeled training data. Our strategy for 53  retrieving Web samples, we find that, is substantially helpful in conventional document classification in terms of accuracy and efficiency. Keywords: Unsupervised classification, text classification, Web mining 1. Introduction Document classification has been extensively studied in the fields of data mining and machine learning. Conventionally, document classification is a supervised learning task [1, 2] in which adequately labeled documents should be given so that various classification models, i.e., classifiers, can be learned accordingly. However, such requirement for supervised text classification has its limitations in practice. First, the cost to manually label sufficient amount of training documents can be high. Secondly, the quality of labor works is suspicious, especially when one is unfamiliar with the topics of given classes. Thirdly, in certain applications, such as email spam filtering, prototypes for documents considered as spam might change over time, and the need to access the dynamic training corpora specifically-tailored for this kind of application emerges. Automatic methods for data acquisition, therefore, can be very important in real-world classification work and require further exploration. Previous works on automatic acquisition of training sets can be divided in two types. One of which focused on augmenting a small number of labeled training documents with a large pool of unlabeled documents. The key idea from these works is to train an initial classifier to label the unlabeled documents and uses the newly-labeled data to retrain the classifier iteratively. Although classifying unlabeled data is efficient, human effort is still involved in the beginning of the training process. The other type of work focused on collecting training data from the Web. As more data is being put on the Web every day, there is a great potential to exploit the Web and devise algorithms that automatically fetch effective training data for diverse topics. A major challenge for Web-based methods is the way to locate quality training data by sending effective queries, e.g., class names, to search engines. This type of works can be found in [3, 4, 5, 6], which present an approach that assumes the search results initially returned from a class name are relevant to the class. Then the search results are treated as auto-labeled and additional associated terms with the class names are extracted from the labeled data. By sending the class names together with the associated terms, appropriate training documents can be retrieved automatically. Although generating queries is more convenient than manually collecting training data, the quality of the initial search results may not always be good especially when the given classes have multiple concepts. For example, the concepts of class “Apple” include company and fruit. Such a problem can be observed widely in various applications. The goal of this paper is, given a set of concept classes, to automatically acquire training corpus based merely on the names of the given classes. Similar to our previous attempts, we 54  employ a technique to produce keywords by expanding the concepts encompassed in the class names, query the search engines, and use the returned snippets as training instances in the subsequent classification tasks. Two issues may arise with this technique. First, the given class names are usually very short and ambiguous, making search results less relevant to the classes. Secondly, the expanded keywords generated from different classes may be very close to each other so that the corresponding search-result snippets have little discrimination power to distinguish one class from the others. We present two concept expansion methods to deal with these problems, respectively. The first method, expansion by common concepts, aims at alleviating the problem of ambiguous class names. The method utilizes the relations among the classes to discover their common concepts. For example, “company” could be one of the common concepts of classes “Apple” and “Microsoft”. Combined with the common concepts, relevant training documents to the given classes can be retrieved. The second method, expansion by discriminative concepts, aims at finding discriminative concepts among the given classes. For example, “iPod” could be one of the unique concepts of class “Apple”. Combined with the discriminative concepts, effective training documents that distinguish one class from another can be retrieved. Our methods are tested under two different experimental setups, the CS papers and Web pages classification tasks. The proposed methods are effective in retrieving quality training data by querying search engines. Moreover, the result shows that the obtained Web training data and manually labeled training data are complementary. Our methods can significantly improve classification accuracy when only a few manually labeled training data is available. Contribution of our work can be addressed as follows. We propose an automatic way to sample the Web and collect the training data with good quality. Apart from the previous work, our methods are fully automatic, reliable, and robust, and achieve an 81% accuracy in text classification tasks. With a little help from a small number of labeled data added into the scene, the classification accuracy can be as high up to 90%. Several experiment results are also revealed to help investigation and realization of automatic Web sampling methods, in which the difficulties encountered are presented in detail. The sections are organized as follows. In Sections 2 and 3, we present our basic idea and the two methodologies, respectively. The experiments are introduced in Section 4. In Section 5, we discuss the related work of this paper. Finally, in Section 6, we give out discussions and conclusions. 2. The Basic Idea Suppose that we are given a set of classes C=(c1, c2, …, cn), where ci is the name of the i-th class. We plan to generate keywords based on classes C, form a few queries and send them off to search engines so as to collect training instances. Our methods presented in this paper are independent of classification models; that is, any model can be incorporated with our methods. 55  To carefully examine the possibility of querying search engines for acquiring training data, we did an evaluation with different search engines, search-result types (snippet or document), and the number of search results. 5 CS-related classes were taken into account, including “Architecture”, “IR”, “Network”, “Programming”, and “Theory”. Each class name ci was sent to 3 search engines, including Google1, Yahoo!2, and Live Search3. Top 100 snippets were extracted as training data. We also gathered the research papers from the corresponding conferences to the 5 classes as the testing documents. Table 1 shows the performance of different search engines. Querying by the class names can achieve classification accuracy at a range from 0.35 to 0.56. More specifically, the three search engines perform well in “Programming” and “Theory” but poorly in the others on average. This arises from the fact that irrelevant documents may be located for those classes with ambiguous names. The way to query by the class names is not reliable due to the ambiguity of the class names (the first challenge). For example, the word “architecture” is widely used in CS, art and construction. From the results, we select Google as our backend search engine in this paper.  We further explore if the classification performance can be improved by downloading Web pages for training. The result is shown in Table 2. It reveals that Web pages might introduce more noises than snippets do, while the snippets summarize Web pages and capture the concepts of classes C by their context. Moreover, to download Web pages is time-consuming. Our methods, therefore, only retrieve snippets as the training source.  Table 1. Accuracy of different search engines for classification of CS papers.  Engine  Architecture IR  Network Programming Theory Avg.  Google 0.075  0.382 0.899 0.723  0.762  0.568  Yahoo! 0.112  0.022 0.094 0.863  0.665  0.351  Live Search 0.269  0.006 0.083 0.784  0.815  0.391  Intuitively, collecting more snippets or documents might enhance the performance. Table 3 shows the results of changing training data sizes from 100 to 900. It could be found that classification accuracy does not increase obviously when the numbers of snippets and documents reach 200 and 300, respectively. This is because much relevant information can be retrieved in top ranked search results returned by the search engine. Noises are unavoidably included from longer lists. Hence, simply fetching a large amount of snippets or documents from a single search result cannot achieve satisfactory performance. Even if we expand the queries, i.e., the class names, using pseudo-relevance feedback (PRF) [7, 8, 9], the improvement is still minor since the generated expanded keywords cannot effectively discriminate different classes (our second challenge). The performance comparison between our methods and PRF will be given in Section 4.2.  
It is difficult for users to formulate appropriate queries for search. In this paper, we propose an approach to query term selection by measuring the effectiveness of a query term in IR systems based on its linguistic and statistical properties in document collections. Two query formulation algorithms are presented for improving IR performance. Experiments on NTCIR-4 and NTCIR-5 ad-hoc IR tasks demonstrate that the algorithms can significantly improve the retrieval performance by 9.2% averagely, compared to the performance of the original queries given in the benchmarks. Experiments also show that our method can be applied to query expansion and works satisfactorily in selection of good expansion terms. Keywords: Query Formulation, Query Term Selection, Query Expansion. 1. Introduction Users are often supposed to give effective queries so that the return of an information retrieval (IR) system is anticipated to cater to their information needs. One major challenge they face is what terms should be generated when formulating the queries. The general assumption of previous work [14] is that nouns or noun phrases are more informative than other parts of speech (POS), and longer queries could provide more information about the underlying information need. However, are the query terms that the users believe to be well-performing really effective in IR? Consider the following description of the information need of a user, which is an example description query in NTCIR-4: Find articles containing the reasons for NBA Star Michael Jordan's retirement and what effect it had on the Chicago Bulls. Removing stop words is a common way to form a query such as “contain, reason, NBA Star, Michael Jordan, retirement, effect, had, Chicago Bulls”, which scores a mean average precision (MAP) of 0.1914. It appears obviously that terms contain and had carry relatively less information about the topic. Thus, we take merely nouns into account and generate another query, “reason, NBA Star, 69  Michael Jordan, retirement, effect, Chicago Bulls”, which achieves a better MAP of 0.2095. When carefully analyzing these terms, one could find that the meaning of Michael Jordan is more precise than that of NBA Star, and hence we improve MAP by 14% by removing NBA Star. Yet interestingly, the performance of removing Michael Jordan is not as worse as we think it would be. This might be resulted from that Michael Jordan is a famous NBA Star in Chicago Bulls. However, what if other terms such as reason and effect are excluded? There is no explicit clue to help users determine what terms are effective in an IR system, especially when they lack experience of searching documents in a specific domain. Without comprehensively understanding the document collection to be retrieved, it is difficult for users to generate appropriate queries. As the effectiveness of a term in IR depends on not only how much information it carries in a query (subjectivity from users) but also what documents there are in a collection (objectivity from corpora), it is, therefore, important to measure the effectiveness of query terms in an automatic way. Such measurement is useful in selection of effective and ineffective query terms, which can benefit many IR applications such as query formulation and query expansion. Conventional methods of retrieval models, query reformulation and expansion [13] attempt to learn a weight for each query term, which in some sense corresponds to the importance of the query term. Unfortunately, such methods could not explain what properties make a query term effective for search. Our work resembles some previous works with the aim of selecting effective terms. [1,3] focus on discovering key concepts from noun phrases in verbose queries with different weightings. Our work focuses on how to formulate appropriate queries by selecting effective terms or dropping ineffective ones. No weight assignments are needed and thus conventional retrieval models could be easily incorporated. [4] uses a supervised learning method for selecting good expansion terms from a number of candidate terms generated by pseudo-relevance feedback technique. However, we differ in that, (1) [4] selects specific features so as to emphasize more on the relation between original query and expansion terms without consideration of linguistic features, and (2) our approach does not introduce extra terms for query formulation. Similarly, [10] attempts to predict which words in query should be deleted based on query logs. Moreover, a number of works [2,5,6,7,9,15,16,18,19,20] pay attention to predict the quality or difficulty of queries, and [11,12] try to find optimal sub-queries by using maximum spanning tree with mutual information as the weight of each edge. However, their focus is to evaluate performance of a whole query whereas we consider units at the level of terms. Given a set of possible query terms that a user may use to search documents relevant to a topic, the goal of this paper is to formulate appropriate queries by selecting effective terms from the set. Since exhaustively examining all candidate subsets is not feasible in a large scale, we reduce the problem to a simplified one that iteratively selects effective query terms from the set. We are interested in realizing (1) what characteristic of a query term makes it effective or ineffective in search, and (2) whether or not the effective query terms (if we are able to predict) can improve IR performance. We propose an approach to automatically measure the effectiveness of query terms in IR, wherein a regression model learned from training data is applied to conduct the prediction of term effectiveness of testing data. Based on the measurement, two algorithms are presented, which formulate queries by selecting effective terms and dropping ineffective terms from the given set, respectively. The merit of our approach is that we consider various aspects that may influence retrieval performance, including linguistic properties of a query term and statistical relationships between terms in a document collection such as co-occurrence and context dependency. Their impacts on IR have been carefully examined. Moreover, we have conducted extensive experiments on NTCIR-4 and NTCIR-5 ad-hoc IR tasks to evaluate the performance of the 70  proposed approach. Based on term effectiveness prediction and two query formulation algorithms, our method significantly improve MAP by 9.2% on average, compared to the performance of the original queries given in the benchmarks. In the rest of this paper, we describe the proposed approach to term selection and query formulation in Section 2. The experimental results of retrieval performance are presented in Sections 3. Finally, in Section 4, we give our discussion and conclusions. 2. Term Selection Approach for Query Formulation 2.1 Observation When a user desires to retrieve information from document repositories to know more about a topic, many possible terms may come into the mind to form various queries. We call such set of the possible terms query term space T={t1, …, tn}. A query typically consists of a subset of T. Each query term tiT is expected to convey some information about the user information need. It is, therefore, reasonable to assume that each query term will have different degree of effectiveness in retrieving relevant documents. To explore the impact of one query term on retrieval performance, we start the discussion with a degeneration process, which is defined as a mapping function taking the set of terms T as input and producing set {T−{t1}, T−{t2},…,T−{tn}} as output. Mathematically, the mapping function is defined as: DeGen(T) = {T − {x}|xT}. By applying the degeneration process to the given n terms in T, we can construct a set of n queries ∆q = {∆q1 , ∆q2 ,…, ∆qi ,…, ∆qn }, where ∆qi = {t1, … , ti−1, ti+1, … , tn} stands for a query by removing ti from original terms T. Suppose query term space T well summaries the description of the user information need. Intuitively, we believe that the removal of a term (especially an important one) from T may result in a loss of information harming retrieval effectiveness. To realize how much such information loss may influence IR performance, we conduct an experiment on NTCIR-4 description queries. For each query, we construct its query term space T by dropping stop words. T is treated as a hypothetical user information need. The remaining terms in the description queries are individually, one at a time, selected to be removed to obtain ∆q. Three formulas are used to measure the impact of the removing terms and defined as: g min (T)  min (pf( qi ) - pf(T))/pf(T) ΔqiΔq g max (T)  max (pf( qi ) - pf(T))/pf(T) ΔqiΔq  g avg(T)  1 (pf( qi ) - pf(T))/pf(T) |T| i where pf(x) is a performance measurement for query x, g(T) computes the ratio of performance variation, which measures the maximum, minimum and average performance gain due to the removal of one of the terms from T, and |T| is the number of query terms in T. 71  We use Okapi as the retrieval model and mean average precision (MAP) as our performance  measurement for pf(x) in this experiment.  The experimental results are shown in Figure 1. When we remove one term from each of  the 50 topics {T}, in average, 46 topics have negative influence, i.e., gavg(T)<0. This means  that deleting one term from T mostly leads to a negative impact on MAP, compared to  original T. On the other hand, gmax(T)>0 shows that at least the removal of one term  positively improves MAP. By removing such terms we can obtain better performance. The  phenomenon appears in 35 out of 50 topics, which is statistically suggestive that there exists  noisy terms in most of user-constructed queries. In short, removing different terms from each  topic T causes MAP variation in different levels. Some query terms are highly  information-bearing, while others might hurt MAP. It is worth mentioned that we conduct the  same experiment with the Indri and TFIDF retrieval models using the Lemur toolkit [21]. The  results are quite consistent over different models. This characteristic makes it possible for the  effectiveness of a query term on IR to be learned and applied to query formulation.  100% 80% 60% 40% 20% 0% -20% -40% -60% -80% -100% -120%  93.06%  Gmax(q)  Gmin(q)  Gavg(q)  q Precision  83.04%  57.42%  66.89%  66.44%  67.74%  62.96%56.34%  21.40%  32.71% 30.09% 22.56% 18.14% 19.46%  28.40%35.7371%.9475%.8161%.0146%.1149%.2329%.112%0.451%2.5134%.6450%.974.0%33%7.4175%.1296%.459.8%21%6.3304%.3219%.6182%.38%30.0161%.632121%..8492%%18.03815%0.33.803.6%0%61%0.42%  54.11% 25.12%33.67% 17.23% 16.31%  
The identification of opinion holders aims to extract entities that express opinions in opinion sentences. In this paper, the task of opinion holder identification is divided into two subtasks: the identification of author’s opinions and the labeling of opinion holders. Support vector machine is adopted to identify author’s opinions, and conditional random field model (CRF) is utilized to label opinion holders. The proposed method achieves an F-score 0.734 in NTCIR7 MOAT task at traditional Chinese side. The proposed method achieves the best performance among participants who adopted machine learning methods, and also this performance was close to the best performance in this task. In addition, the ambiguous markings of opinion holders are analyzed, and the best way to utilize the training instances with ambiguous markings is proposed. 關鍵詞：意見持有者辨識，意見探勘，條件隨機域，支援向量機 Keywords: opinion holders identification, opinion mining, CRF, SVM. 一、緒論 意見代表人們對某個議題的主觀想法，人們常透過文章表述意見。隨著 Web2.0 的崛 起，網路上出現大量、免費與即時的資料，使用者對文章中的意見很感興趣，但卻無法 大量閱讀數以千萬計的資料。意見探勘 (opinion mining) 的技術可以幫助使用者自動分 析文章中的意見， Kim 和 Hovy[1] 在 2004 年提出意見中包括意見傾向 (opinion polarity)、意見強度 (opinion strength)、意見持有者 (opinion holder) 及評論目標 (opinion target) 四個要素。意見傾向描述此意見是正面、中立或負面，意見強度描述此意見的 語氣強弱，表述此意見的人或組織稱為意見持有者，而討論的主題則稱為評論目標。以 例句 1 為例，此句的意見傾向為正面、意見強度為強烈、意見持有者為王建民、評論 101  目標為打棒球。意見持有者通常會以一或多個詞的形式出現在意見句中，我們將這些詞 稱為意見持有者的代表詞，但有時意見持有者不會以詞的形式出現在意見句中，例如例 句 2 是作者根據例句 1「王建民」的意見推論的意見，例句 2 的意見持有者為文章作 者。 例句 1：王建民非常喜歡打棒球 例句 2：王建民應該也喜歡打網球 在意見探勘中，意見持有者辨識的技術對於了解有哪些人或組織在表述意見、某個人或 組織在哪些議題中發表過意見及兩個人或組織發表過的意見是否相似等相關資訊特別 重要。意見持有者辨識可應用於社群網路分析中，找出社群網路中是否存在著一些意見 領袖，他們的意見常被引用，也會影響其他使用者的意見。意見持有者辨識也可以應用 在意見問答系統中，找出某些意見是由哪些意見持有者提出的，並進而藉由意見持有者 的權威性與可靠度來輔助判斷答案的權威性與可靠度。 意見持有者辨識主要有三大挑戰：同指涉解析、巢狀結構及處理歧異的標記。意見持有 者 有 時 會 以 代 詞 (Anaphor) 的 形 式 出 現 在 文 句 中 ， 並 指 涉 到 前 面 的 先 行 詞 (Antecedent) ，例如例句 3 中的「雙方」即是指涉到「美國」與「中共」。 例句 3： 據媒體報導，美國在與中共討論簽署停止以核武相互瞄準協議的問題，貝肯說， 雙方過去就曾討論此事，前任國防部長裴利在中共國防部長遲浩田於一九九六年 十二月訪美時就曾提起，後來雙方在其他會議中也曾討論。 意見句有時會有巢狀結構 (nested structure) ，以例句 3 為例，文章作者引述「媒體報 導」的內容，「媒體報導」的內容又引述「貝肯」的發言，意見持有者常會是子句的主 詞，判斷意見持有者是哪一層結構中的主詞也是意見持有者辨識的一個重要議題。 標記意見持有者辨識所用的語料時，有時會出現標記歧異，不同的標記者可能會認為文 句的意見持有者為不同的實體。以例句 3 為例，一位標記者認為意見持有者為「國防 部發言人貝肯/貝肯」，另一位標記者卻認為意見持有者為『美「中」/雙方』，從文句 內容來看，意見持有者為「國防部發言人貝肯/貝肯」，但深究背後的意義，貝肯是轉 述『美「中」/雙方』的意見，兩種說法都沒錯，端看標記者的認知，也因此意見持有 者可能被多個標記者標記出不同的答案。，如何利用標記歧異的語料也是意見持有者辨 識的一大挑戰。 二、相關研究 Pang 和 Lee[2] 整理出意見探勘領域中重要的研究，意見持有者辨識的研究剛開始起 步，研究團隊使用的方法主要可分為以經驗法則 (heuristic rule) 為基礎與以機器學習為 基礎兩種。 (一)、以經驗法則為基礎的方法 以經驗法則為基礎的方法中，Yohei 等人[3] 先使用名詞片語與語法特徵值，透過支援 向量機，將意見持有者分為文章作者與非文章作者，接著再透過語法規則，選出最有可 能的具名實體，做為答案的意見持有者，他們主要專注於處理英文與日文的語料。Xu 102  和 Wong[4] 提出的方法是先解決同指涉問題，再使用經驗法則擷取出意見持有者，使 用的規則與標點符號、連接詞、字首 (prefix) 、字尾 (suffix) 與表述關鍵字相關，Xu 和 Wong 的方法是目前中文意見持有者辨識中效能最佳的，在 NTCIR7 多語意見分析 評比項目的繁體中文語料上， F 值可達到 0.825。 
This study examines the influence of lexical tone upon voice onset time (VOT) in Mandarin and Hakka. Examination of VOT values for Mandarin and Hakka word-initial stops /p, t, k, ph, th, kh/ followed by three vowels /i, u, a/ in different lexical tones revealed that lexical tone has a significant influence on the VOTs. The result is important because it suggests that future studies should take its influence into account when studying VOT values for stops in tonal languages. In Mandarin, stops’ VOTs, ordering from the longest to the shortest, are in Tone 2, Tone 3, Tone 1, and Tone 4: this sequence is the same as Liu, Ng, Wan, Wang, and Zhang’s (2008) [1] results. However, later it was found that the sequence results from the existence of non-words. Because in order to produce non-words correctly, participants tended to pronounce them at a lower speed, especially those in Tone 2. Therefore, we further examined the data without non-words, in which no clear sequence had been found. For Hakka, Post hoc tests (Scheffe) show that aspirated stops in Tones 4 and 8 have significantly shorter VOT values than they have in other tones. Keywords: Voice onset time, Mandarin tones, Hakka stops, Mandarin stops 1. Introduction The aim of this paper is to explore whether lexical tones influence the VOT values for word-initial stops. This issue is important because VOT is considered as a reliable phonetic feature to differentiate consonant stops ([2], [3], [4], [5], [6], [7]) and recently it has been used to study the language production of patients with language deficits or disorders ([8], [9]). Among the languages being investigated, some are tone languages, i.e. Mandarin, Cantonese, and Taiwanese. In a tonal language, the duration of each lexical tone is slightly different. Consequently, it is possible that lexical tone will affect stop’s voice onset time. However, few studies have taken this factor into consideration while studying tone languages. It is hoped that with data from Mandarin and Hakka, we can establish the groundwork for future studies related to VOTs in tonal languages. If lexical tone does have an influence on the VOT, it should be taken into account when creating stimulus words in future studies for tonal languages, thereby rendering studies more valid and reliable. 1.1 Voice onset time Lisker and Abramson (1964) [2] have defined voice onset time (VOT) as the 115  temporal interval from the release of an initial stop to the onset of glottal pulsing for a following vowel. It has been considered as a reliable phonetic cue to categorizing the stop consonants, i.e. voiced vs. voiceless or unaspirated vs. aspirated, in various languages ([2], [3], [4], [5], [6], [7], [10]). Additionally, by comparing VOT values for stops produced by native and non-native speakers for specific languages, researchers have provided some suggestions for language learning and teaching ([6], [11], [12]). Moreover, recently researchers have studied aphasia, apraxia and stuttering patients’ production deficits by observing their VOT values for stops ([8], [9]). 1.2 Factors affecting voice onset time When investigating stops, researchers found that the VOT values for stops varied in relation to the place of articulation. Cho and Ladefoged (1999) [4], sorted out researchers’ findings, have claimed that the further back the closure, the longer the VOT ([2], [4], [6], [13]). That is velar stops have the longest VOT values, alveolar stops the intermediate values, and bilabial stops have the shortest values. However, there are some exceptions. Alveolar stops in Tamil, Cantonese, Eastern Armenian, Hungarian, Japanese, and Mandarin, have shorter VOTs than bilabial stops ([2], [3], [5], [7], [12], [14]). Liu et al. (2008) [1] speculated that the VOT durations may be affected by tone, because different tones have different fundamental frequencies and pitch levels, which are determined mainly by the tension of the vibrating structure. In order to achieve different levels of tension, different amounts of time might be needed. Consequently, the VOT values may vary when they are in different lexical tone. Only a few studies have tried to examine whether lexical tone influences VOT values. For example, Liu et al. (2008) [1] studied the effect of tonal changes on VOTs between normal laryngeal and superior esophageal speakers of Mandarin Chinese, and reported that for normal laryngeal speakers there are significant differences of VOT values caused by lexical tones. In addition, stops in Tone 4 have significantly shorter mean VOT values than stops in Tones 2 and 3. The study by Liu et al. [1] is a pioneering piece of work in this field, but more evidence is still needed. Therefore, by carrying out a systematic study with respect to the influence of lexical tone for stop’s VOT using two tonal languages, i.e. Mandarin and Hakka, we try to verify previous findings in order to provide references for future linguistic studies on tonal languages. 1.3 The features of Mandarin and Hakka Mandarin Chinese and Hakka are tonal languages, in which a word’s meaning can be changed by the tone in which it is pronounced. Chao (1967) [15] suggested a numerical notation for lexical tones: dividing a speaker’s pitch range into four equal intervals by five points: 1 low, 2 half-low, 3 middle, 4 half-high, and 5 high. The numerical notation indicates how the pitches of a lexical tone change. For example, the numerical notation for Tone 2 in Mandarin is 35, which represents that the pitch will go from middle to high. Table 1 reveals the numerical notation for each lexical tone in Mandarin and Hakka. In Mandarin, there are four contrasting lexical tones, Tone 1 (high–level), Tone 2 (mid-rising), Tone 3 (falling-rising), and Tone 4 (high-falling). Sixian Hakka has six contrasted lexical tones, Tone 1 (24), Tone 2 (31), Tone3 (55), Tone 4 (32), Tone 5 (11), and Tone 8 (55). The pitch values for Tone 3 and Tone 7 are the same, therefore Tone 7 has been omitted. Although there are regional differences for Hakka, Sixian Hakka was chosen as it is the most widely used Hakka dialect in Taiwan. 116  Table 1. The numerical notations for lexical tones in Mandarin [15] and Hakka [16].  Lexical Tone  
A two-stage latent prosody model-language model (LPM-LM)-based approach is proposed to identify two Mandarin accent types spoken by native speakers in Mainland China and Taiwan. The frontend LPM tokenizes and jointly models the affections of speaker, tone and prosody state of an utterance. The backend LM takes the decoded prosody state sequences and builds n-grams to model the prosodic differences of the two accent types. Experimental results on a mixed TRSC and MAT database showed that fusion of the proposed LPM-LM with a SDC/GMM+PPR-LM+UPR-LM baseline system could further reduced the average accent identification error rate from 20.7% to 16.2%. Therefore, the proposed LPM-LM method is a promising approach. Keywords: Accent recognition, latent prosody model, Mandarin, Taiwan 1. Introduction Over the past decades, many approaches have been proposed to deal with language identification (LID) tasks. They tried to capture the specific characteristics of different languages. These characteristics roughly fall into three categories: the phonetic repertoire, the phonotactics, and the prosody. The mainstream system (as shown in NIST language recognition evaluation (LRE) 2007) [1] is usually based on the fusion of multiple acoustic and phonotactic systems. Although LID is extensively studied, less works have been done on accent identification (AID), especially for native speakers, such as American and Indian English, Mainland China and Taiwan Mandarin, Hindi and Urdu Hindustani and Caribbean and non-Caribbean Spanish. Comparing with LID task, AID of native speakers is more challenging because, (1) some linguistic knowledge, such as syllable structure, may be of little use since native speakers seldom make such mistakes; (2) difference among those speakers is relatively smaller than 125  that among foreign (non-native) speakers. In other words, the capacities of the popular acoustic and phonotactic approaches may be limited in this case. Many approaches have been proposed to model the prosodic differences between languages, dialects or accents [2], recently. Most of them are based on direct modeling of surface prosodic features, i.e., the raw prosodic features. For example, frame-level pitch flux features and GMMs were proposed in [3]; segmental-level pitch features were extracted using Legendre polynomials and modeled by ergodic Markov model in [4]; and supra-segment-level prosodic features were captured by n-gram in [5]. Figure 1. The block diagram of the proposed LPM-LM-based Mandarin accent identification system. Figure 2. The block diagram of the proposed LPM framework (speaker factor is omitted to simply this figure). However, surface prosodic features are often affected by many other non-prosodic latent factors, such as channel, speaker, phonetic context, and so on. Therefore, it is necessary to apply some feature normalization methods [6] to alleviate the unwanted affections. To absorb those unwanted affections, in this study a two-stage latent prosody model-language model (LPM-LM)-based approach as shown in Fig. 1 and 2 is proposed. The aim is to discriminate two Mandarin accent types spoken by native speakers in Mainland China and Taiwan. 126  In this approach, the frontend LPM [7] tokenizes (with the help of automatic speech recognizers (ASRs)) an input utterance into smaller prosodic units (sub-syllable in our case) and artificially introduces latent prosody states to represent the prosodic status of each token in an utterance. It then jointly models the affections of speaker, tone and prosody state on surface prosodic features in order to decode more precise prosody state sequences of the utterance. The backend LM then takes the decoded prosody state sequences and builds an n-gram to model the supra-segmental prosodic charactistics of each accent type. In more detail, LPM as shown in Fig. 2 (1) introduces a two-level hierarchical structure of speech prosody [8] with prosodic states and state transition probabilities and (2) describes the joint affections of latent factors in a state by a variable-parameter probability density function whose parameters varies as a function of those latent factor-dependent parameters. The purpose is to explain the variant due to speaker, phonetic context and, especially, tone factors. It is worth noting that (1) the proposed LPM-LM framework is similar to the popular parallel phone recognizer (PPR)-LM approach. However, the phone recognizers are replaced by automatic prosodic state tokenizers/labelers and, especially, (2) the LPM module could be trained in an unsupervised way to avoid any human annotation efforts. This paper is organized as follows. Section 2 reviews the LPM framework. Section 3 discusses the application of LPM-LM on Mandarin AID. Section 4 reports the experimental results on a Mainland China and Taiwan Mandarin corpus. Some conclusions are given in the last section.  2. Latent Prosody Model of Speech Prosody  
yrwang@mail.nctu.edu.tw 摘要 在本論文中提出一種以取樣點為單位(sample-based)的高時間解析度之音素端點自 動標示與切割的方法，有別於傳統分析語音信號以音框為單位(frame-based)或是音段為 單位(segment-based)的研究。本文中，我們提出了一些以取樣點為單位的聲學參數；由 實驗結果顯示，這些聲學參數在不同發音特徵之音素轉換間有明顯的變化率，有利於音 素切割位置之標記。我們利用這些發音特徵變化的聲學參數特性，建立一個高時間解析 度的自動音素端點標示與切割系統。由TCC-300國語語料庫進行自動端點標示之實驗結 果顯示，本論文所提出的方法比傳統以音框為單位之切割方法，亦即HMM之切割方法， 更能有效切出精準的短停頓、摩擦音、塞擦音等之音素端點位置。 Abstract This paper presents a sample-based phone boundary detection algorithm which can improve the accuracy of phone boundary labeling in speech signal. In the conventional phone labeling method adopted the frame-based approach, some acoustic features, like MFCCs, are used. And, the statistical approaches are employed to find the phone boundary based on these frame-based features. The HMM-based forced alignment method is most frequently used method. The main drawback of the frame-based approach lies in incapability of modeling rapid changes in speech signal; moreover, the time resolution of this approach is too coarse for some applications. To overcome this problem, a sample-wise phone boundary detection framework is proposed in this study. First, some sample-wise acoustic features are proposed which can properly model the variation of speech signal. The simple-based spectral KL distance is first employed for boundary candidates pre-selection in order to reduce the complexity of sample-based methods. Then, a supervised neural network is trained for phone boundary detection. Finally, the effectiveness of the proposed framework has been validated on automatic labeling of TCC-300 speech corpus. 137  關鍵詞：音素端點切割，帶通信號波封，sample-based 頻譜 KL 距離，監督式類神經網 路 Keywords: phone boundary segmentation, sub-band signal envelope, sample-based spectral KL distance, supervised neural network 一、緒論 正確音素切割位置在語音辨認的研究中可以提升辨識模型的可靠度與統計上一致 性進而提升辨識率，也扮演著語音合成方面合成聲音品質提升的重要因素之一。在全球 有人工切割位置的語料庫不多，最著名的是 TIMIT 語料庫，但是一個大型的連續語音 資料庫，使用人工標記切割位置的方式，不僅非常耗時且人工切割的標記位置也伴隨著 一個缺點，就是以人工做標記的動作時，會因為主觀上認定切割位置不同而使得標記的 位置缺乏一致性，因此一個能夠自動標記且具有精確切割位置的語料庫是非常重要的。 在語音信號處理中，自動音素之切割是一個非常重要的問題，儘管在過去有非常多 自動音素切割的研究[1]，一個具有高精準度的自動音素切割演算法，仍是一個可待持 續研究的課題。在過去一些自動音素切割與偵測的研究中，主要可分為 Model-based 及 Metric-based 或是上述兩種方法結合。 在 Model-based 方法中，最常被使用的就是以概似法則訓練的隱藏式馬可夫模型 (maximum likelihood-trained Hidden Markov Model, ML-trained HMM)做自動語音切割， 其效能可在正負 20 ms 之內佔有 90%的比率(inclusion rate)，而傳統 HMM 是以整段語句 所得到最大相似度函數(maximum likelihood, ML)為訓練準則，故其自動切割之位置並非 為最佳之音節或音素邊界位置。近年來有學者提出一些方法，其中以最小邊界錯誤 (minimum boundary error, MBE)為訓練準則之 HMM[2]，就使用自動給定之已知端點間 誤差最小化作為 HMM 模型之訓練準則，在 TIMIT 語料庫中，MBE-HMM 自動切割之 邊界與人工切割邊界誤差範圍 10 ms 之內的比率高達 79.75%，與傳統 ML-trained HMM 模型其百分比 71.23%相比，提昇許多；然而其自動切割位置只有 7.89%的邊界在人工 切割位置誤差 20 ms 之外。此外，也可使用其它圖形識別的方法如支撐向量機(support vector machine, SVM)[3]、類神經網路(neural network, NN)[4]，來對 HMM 之自動切割位 置再作進一步地修正，以獲得更好的結果。 而在 Metric-based 方法中，我們知道語音信號在一個音素中穩定的信號，其聲學參 數變化的速率就是決定一個音素邊界的重要線索，回顧一些文獻如 Rabiner[5]使用頻譜 轉換量測(spectral transition measure)的音素邊界偵測方法，應用在 TIMIT 語料庫其效能 可達到在誤差 20ms 的容忍範圍內，只有 23.1%的音素端點位置沒偵測出來 (missed detection rate, MD) 、 22.0% 誤 報 率 (false alarm rate, FA) 。 Kotropoulos[6] 結 合 Kullback-Leibler(KL)距離及貝式資訊法則(Bayesian Information Criterion, BIC)所提出的 DISTBIC 演算法來偵測語音信號之音素邊界，其效能在 NTIMIT 語料庫亦可達到 25.7% MD 與 23.3% FA 的結果。 在先前的音素切割方法中，無論 model-based 或 metric-based 的方法中，常用的語 138  音信號參數多與信號頻譜相關；且一般假設語音信號在短時間內為穩定的特性，故使用 frame-based 的 聲 學 參 數 ， 例 如 梅 爾 倒 頻 譜 係 數 (mel-frequency cepstral coefficients, MFCC) 。 然 而 ， 在 做 頻 譜 分 析 時 會 造 成 時 間 與 頻 譜 (time-spectrum) 上 之 不 確 定 性 (uncertain)，所以頻譜參數越精確就會犧牲時間精確度；但在 frame-based 架構中必須要 讓頻譜解析度越精細，以提昇辨認音素能力，而發音器官變化很快的音素如爆破音，其 音長可能小於一個音框，使得 frame-based 方法之切割位置與實際正確音素邊界位置之 間產生誤差，因此對於自動語音切割之研究提昇時間解析度，必可降低大量因音框之時 間解析度所造成的誤差。而語言學家就曾經提出一些用來區別發音特徵的參數，一般稱 之為 Articulation Parameter (AP)。其方法可用低解析度的頻帶，來區分像發音方式或發 音位置以及偵測一些 landmark 如 voice on-set，而不是用來辨認像音素的精細分類。由 以上敘述，在自動語音切割的應用，我們可以思考為了使得自動端點標示的時間精確度 能夠提昇，降低頻譜精確度的可行性。故在本文中，我們提出 sample-based 音素端點 偵測方法的架構，並與 frame-based HMM 切割位置做比較。 在本文中其它章節概要如下：在第二節中，我們首先說明 sample-based 音素端點 偵測方法的整體架構；第三節對於本論文中所提出之一些 sample-based 聲學參數的特性 做進一步地說明；第四節則是介紹利用上述 sample-based 聲學參數並使用多層感知器 (multi-layer perception, MLP)類神經網路架構的 sample-based 音素端點偵測方法；第五 節為實驗結果探討，並於第六節提出簡單的結論。 二、系統架構 一般傳統切割的方法，主要分成兩個部份，首先利用統計模式為基礎的方法，如 HMM-based forced alignment 當作初始切割位置，再藉由一些方法如 SVM 等，以進一步 修正初始切割位置(refinement)。本研究是對 TCC-300 語料庫做切割，先使用 HMM-based forced alignment 得到初始切割位置；接著，利用 sample-based 聲學參數進一步調整該初 始位置；並以 KL distance 挑選其候選端點，訓練一個 MLP 音素端點偵測器以得到最佳 之切割位置。由於 TCC-300 語料庫是由不同的語者所組成，所以在取得 HMM-based forced alignment 初始切割位置時，我們使用了語者調適的技術調將 HMM 模型調適成更 適合該語者之模型。接下來我們進一步介紹語者調適的流程以及 MLP 音素端點偵測器。 （一）、使用 SAT 及 SA 技術之 HMM phone-like unit alignment 流程 我們將使用下列流程做 TCC-300 語音資料庫 HMM 模型類音素層級(phone-like level) 之起始切割位置，就是將一個音節區分為聲母、介音、韻母及韻尾鼻音等部分，其方塊 圖如下： 在 HMM phone model training 後，我們再使用做 speaker adaptation training(SAT)； SAT 就是使用 constraint MLLR(CMLLR)對不同語者做語音參數的轉換；使用經語者轉 換(CMLLR)後之語音參數再重新訓練新的 HMM 模型將可獲得較佳之 speaker-dependent HMM 模型。做完 SAT 後，我們再做 HMM 做 model adaptation，使用 MLLR 技術來調 139  適 HMM 模型，它和 SAT 會有加成性的效果。如此就可以獲得較佳的 HMM 模型來做 強制對齊(force alignment)，作為 TCC-300 語料庫之音素的起始切割位置。 圖一、使用 SAT 及 SA 技術之 HMM alignment 流程 （二）、MLP 音素端點偵測器訓練流程 MLP 類神經網路被廣泛地運用在各個領域當中作為資料分類的架構，同時因為其 自我調適的能力、非線性的運算、具有學習能力等特性，故本研究使用此架構訓練一個 監督式(supervised)MLP 音素端點偵測器。訓練音素端點偵測器流程，其方塊圖如圖二。 圖二、使用 MLP 架構之類音素端點偵測流程 140  在圖二中，因經由 HMM alignment 獲得之初始切割位置仍不夠準確，故我們利用 sample-based 的聲學參數所提供之資訊來得到較好的切割位置以作為訓練 MLP 音素端 點偵測器時之答案。且為減少計算量，由預選擇(pre-selection)即簡單設定一個臨界值的 方法來挑選較為可能之候選端點(candidate)位置。接著將候選端點依目標函數(target function)分類後，訓練 MLP 音素端點偵測器直至收斂，最後使用 Viterbi search 演算法 在候選端點中得到該語句最佳之切割位置。 三、sample-based 聲學參數的特性 首先，本研究結合語言學家所提出的 AP，利用數個頻段來區分不同發音特徵之方 法，應用於切割語音信號可提高時間解析度由音框進一步精準至取樣點，並在此提出一 些 sample-based 的 AP 以用於描述不同語音屬性變化時的 AP 特性，來調整音素切割位 置之標記。在此節中將介紹本論文所提出之 sample-based 聲學參數及其在音素端點偵測 上之特性。  （一）、Sample-based 聲學參數  我們提出一些 sample-based 的 AP 如帶通信號波封(sub-band signal envelope)、參數  上升率(rate of rise, ROR)、頻譜熵(spectral entropy) 、sample-based spectral KL distance 及  spectral flatness，並觀察它們在不同語音屬性，如爆破音、鼻音、靜音等特性。以下，  我們進一步介紹本研究所使用的語音特徵參數：  1、帶通信號波封[7]  在語言學家所提出的 AP 中，有許多帶通濾波器，它們各自能用來區別不同的發音  方式或發音位置，常見的頻段(filter bank)[7]有以下：  0.0 – 0.4 KHz 0.8 – 1.5 KHz 1.2 – 2.0 KHz  2.0 – 3.5 KHz 3.5 – 5.0 KHz 5.0 – 8.0 KHz 例如在摩擦音、塞擦音中，在頻譜中之高頻段成份能量極強，低頻段成分能量較弱，  鼻音韻尾則是在低頻段的成份能量極強。這些頻段中能量在有明顯變化的時候，可視為  是語音信號開始改變的地方。但語言學家所使用的 AP 為帶通信號波封，而非現今語音  辨認器中常用的能量。故我們將這六個頻段之語音信號取出它的波封來當作本研究中所  使用的聲學參數。  我們在製作一個波封檢測器(envelope detector)時，為了保持波封變化快的時候能正  確地找到信號的波封，我們使用希爾伯特變換(Hilbert transform)後再經低通濾波器，求  取輸入信號的波封，一個信號 x[n] 的希爾伯特變換 H (x[n]) 的希爾伯特變換，如下式：  H (x[n]) = x[n] ⊗ h[n]  and  h[n]  =  0, 1/  nπ  ,  n is even n is odd  (1)  2、上升率[7]  語言學家所稱之上升率，就是在 frame-based 的語音特徵參數中所用的 delta-term：  ∑ ∑ [ ]  w RORx[n] =  i=−w i ⋅ x  n+i     /     w i=−w  i2     (2)  141  其中 x[n+i]為輸入參數資料，w 為求上升率所使用的音框寬度。本研究使用波封的  上升率、頻譜熵之上升率、各頻段信號波封的上升率等當作語音信號的聲學參數，來描  述各 sample-based 聲學參數的變化率。  3、頻譜熵 [9-10]  頻譜熵可用來描述信號在頻譜上的集中程度，若信號越集中在某一個頻段則頻譜熵  越小。在此，本研究使用先前所述之 6 個頻段，則頻譜熵 Hs 可以定義如下式表示：  ∑ H S = − Ei[n] log ( Ei[n])  (3)  i  ∑ Ei[n] =  ei 6  ej  (4)  j =1  其中 Ei[n]為第 i 個頻段之第 n 點正規化之後的波封。由頻譜熵對應到語音信號上， 可以發現短停頓類似於雜訊，在各個頻段都會出現，所以頻譜熵值較高；而韻母在頻譜  上的能量則較集中於低頻至中頻的部分，其頻譜熵值相對較低。  4、Sample-based spectral KL distance  將頻譜視為一個機率分佈，因此可用 KL distance 來描述頻譜上的相似程度。在語  音信號中計算兩點不同時間(m 與 n)的 spectral KL distance，dx(m,n)，可以由下式表示：  ∑ dx (m, n) =  6 i =1    (  Ei  [  n  ]  −  Ei  [m  ])  log     Ei[n]     Ei[m]   (5)  以上所敘述的參數頻譜熵、頻譜熵的上升率、sample-based KL distance 來觀察一段 語音信號其語音特徵的變化，這些語音特徵證實可以分辨不同語音屬性的邊界。 5、Spectral flatness[11] 使用正規化後之帶通信號波封計算的 flatness，F，表示如下式：   ∏   6  Ei  [  n  ]  1   /  6  ∑ F  =   i=1  
The BLEU scores and translation fluency for the current state-of-the-art SMT systems based on IBM models are still too low for publication purposes. The major issue is that stochastically generated sentences hypotheses, produced through a stack decoding process, may not strictly follow the natural target language grammar, since the decoding process is directed by a highly simplified translation model and n-gram language model, and a large number of noisy phrase pairs may introduce significant search errors. This paper proposes a statistical post-editing (SPE) model, based on a special monolingual SMT paradigm, to “translate”disfluent sentences into fluent sentences. However, instead of conducting a stack decoding process, the sentence hypotheses are searched from fluent target sentences in a large target language corpus or on the Web to ensure fluency. Phrase-based local editing, if necessary, is then applied to correct weakest phrase alignments between the disfluent and searched hypotheses using fluent target language phrases; such phrases are segmented from a large target language corpus with a global optimization criterion to maximize the likelihood of the training sentences, instead of using noisy phrases combined from bilingually wordaligned pairs. With such search-based decoding, the absolute BLEU scores are much higher than automatic post editing systems that conduct a classical SMT decoding process. We are also able to fully correct a significant number of disfluent sentences into completely fluent versions. The BLEU scores are significantly improved. The evaluation shows that on average 46% of translation errors can be fully recovered, and the BLEU score can be improved by about 26%. Keywords: Translation Fluency, Fluency-Based Decoding, Search-Based Decoding, Statistical Machine Translation, Automatic Post-Editing  
In this paper, we introduce an automatic method for classifying a given question using broad semantic categories in an existing lexical database (i.e., WordNet) as the class tagset. For this, we also constructed a large scale entity supersense database that contains over 1.5 million entities to the 25 WordNet lexicographer’s files (supersenses) from titles of Wikipedia entry. To show the usefulness of our work, we implement a simple redundancy-based system that takes the advantage of the large scale semantic database to perform question classification and named entity classification for open domain question answering. Experimental results show that the proposed method outperform the baseline of not using question classification. ᗫᒟ൚j ІਗਪᕚΫഈdਪᕚʱᗳdᗘคႧจ༟ࣘࢫdᗘၣdၪਿϵ߅ Keywords: question answering, question classification, semantic category, WordNet, Wikipedia. 1. Introduction Question classification is considered crucial to the question answering task due to its ability 209  to eliminating answer candidates irrelevant to the question. For example, answers to personquestions (e.g., Who wrote Hamlet?) should always be a person (e.g., William Shakespeare). Common classification strategies includes semantic categorization and surface patterns identification. In order to fully benefit from question classification techniques, answer candidates should be classified the same way as questions. Surface patterns identification methods classifies questions to sets of word-based patterns. Answers are then extracted from retrieved documents using these patterns. Without the help of external knowledge, surface pattern methods suffer from limited ability to exclude answers that are in irrelevant semantic classes, especially when using smaller or heterogeneous corpora. An other common approach uses external knowledge to classify questions to semantic types. In some previous QA systems that deploy question classification, named entity recognition (NER) techniques are used for selecting answers from classified candidates. State-of-the-art NER systems produce near human performances. Good results are often achieved by handcrafted complex grammar models or large amount of hand annotated training data. However, most high performance NER systems deal with a specific domain, focus on homogeneous corpora, and support a small set of NE types. For example, in the Message Understanding Conference 7 (MUC-7) NER task, the domain is “Airplane crashes, and Rocket/Missile Launches” using news reports as the corpus. There are only three NE classes containing seven sub classes: ORG, PERSON, LOCATION, DATE, TIME, MONEY, PERCENT. Notice that in the seven subclasses, only three of them are NEs of physical objects, others are number based entities. This is apparently insufficient for candidates filtering for general question answering. Owing to the need of wider range NE types, some of the later proposed NE classes construct of up to 200 sub classes, but NER systems targeting these types of fine-grained NE classes may not be precise enough to achieve high performance. The amount of supported classification types greatly influences the performance of QA systems. A coarse-grained classification achieving higher precision, may still be weak in excluding improper answers from further consideration. A fine-grained classification may seem a good approach, but the cost of high-precision classification may be too high to produce actual gain in QA systems. Moreover, in open domain QA, answers are not necessarily NEs nor can they be captured by using simple surface patterns. Using a small set of NE types to classify questions has its limits. We randomly analyzed 100 question/answer pairs from the Quiz-zone Web site (http://www.quiz-zone.co.uk/), only 70% of them are NEs. This shows being able to classify common nouns is still very important in developing QA systems. In order to support more general question anwering, where the answer can be NEs and common nouns, we took the approach of using finer-grained semantic categories in an existing lexical database (i.e., WordNet). WordNet is a large scale, hand-crafted lexical ontology database widely used in solving natural language processing related tasks. It provides taxonomy of word senses and relations of 155,327 basic vocabularies that can be used as an semantic taxonomy for entity classification. However, in the later sections of this paper, we will show that WordNet leave room for improvement in question classification and 210  answer validation, and more entities, especially NEs, are needed to achieve reasonable coverage for answer candidates filtering. With this in mind, we turn to Wikipedia, an online encyclopedia compiled by millions of volunteers all around the world, consisting articles of all kinds. It has become one of the largest reference tool ever. It is only natural that many researchers have used Wikipedia to help perform the QA task. Using WordNet semantic categories and rich information from Wikipedia, we propose an minimally supervised question classification method targeting at the 25 WordNet lexicographer’s files for question classification. Experimental results show promising precision and recall rates. The method involve extending WordNet coverage and producing the training data automatically from question/answer pairs, and training a maximum entropy model to perform for classification. The rest of the paper is organized as follows. In the next section, we review related work in question classification and question answering. In Section 3 we explain in detail the proposed method. Then, in Section 4 we report experimental results and conclude in Section 5. 2. Related Work Text Retrieval Conference (TREC) has been one of the major active research conferences in the field of question answering. The early tasks in the question answering track in TREC focuses on finding documents that contain the answer to the input question. No further extraction of exact answers from the retrieved documents is required. In an effort to foster more advanced research, the TREC 2005 QA Task focuses on systems capable of returning exact answers rather than just the documents containing answers. Three types of questions are given, including FACTOID, LIST, and OTHER. For every set of questions a target text is also given as the context of the set of questions. LIST questions require multiple answers for the topic, while FACTOID questions required only one correct answer. Therefore, many consider LIST questions are easier. More recent TREC QA Tasks focuses on complex, interactive question answering systems (ciQA). In ciQA Tasks, fixed-format template questions are given (e.g. What evidence is there for transport of [drugs] from [Mexico] to [the U.S.]?). Complex questions are answerable with several sentences or clauses. (e.g. United States arrested 167 people including 26 Mexican bankers) The design of an interactive query interface is also a part of this task. In this paper, we focus on the issue of classifying questions in order to effectively identify potential answers to FACTOID and LIST questions. More specifically, we focus on the first part of question answering task, namely identifying the semantic classes of the question (and answer) that can be used to formulate an effective query for document retrieval and to extract answers in the retrieved documents. The body of QA research most closely related to our work focuses on the framework of representing types of questions and automatic determination of question types from the given question. Ravichandran and Hovy [2002] proposed a question classification method that does not rely on external semantic knowledge, but rather classifies a question to different sets of 211  surface patterns, e.g. ENTITY was born in ANSWER, which requires ENTITY as an anchor phrase from the given question and impose no constraint on the semantic type of ANSWER. In contrast, we use a sizable set of question and answer pairs to learn how to classify a given question into a small number of types from the broad semantic types in the existing lexical knowledge base of WordNet. In a study more closely related to our work, Ciaramita and Johnson [2003] used WordNet for tagging out-of-vocabulary term with supersense for question answering and other tasks. They discovered it is necessary to augment WordNet by employing complex inferences involving world knowledge. We propose a similar method WikiSense1that uses Wikipedia titles to automatically create a database and extend WordNet by adding new Wikipedia titles tagged with supersenses. Our method, which we will describe in the next section, uses a different machine learning strategy and contextual setting, under the same representational framework Once the classes of the given questions have been determined, typical QA systems attempt to formulate and expand the query for each type of question or on a question by question basis. Kwok et al. [2001] proposed a method that matches the given question heuristically against a semi-automatic constructed set of question types in order to transform the question to effectively queries, and then extract potential answers from retrieved documents. Agichtein, Lawrence, and Gravano [2004] used question phrases (e.g., “what is a” in the question “What is a hard disk?”) to represent the question types and learn query expansion rules for each question type. Prager et al. [2002] describe an automatic method for identifying semantic type of expected answers. In general, query expansion is effective in bringing more relevant document to the top-ranked list. However, the contribution to the overall question answering task might be marginal only. In contrast to the previous work, we do not use question types to expand queries, but rather use question types to filter and re-rank potential answers, which may contribute more directly to the performance of question answering. Indeed, effective explicit question classification is crucial for pinpointing and ranking answers in the final stage of answer extraction. Ravichandran and Hovy [2002] proposed a method for learning untyped, anchored surface patterns in order to extract and rank answers for a given question type. However, as they pointed out, without external semantic information, surface classification suffers from extracting answer of improper class. Example shows a where-is question (e.g. Where is Rocky Mountains?) may be classified to the pattern “ENTITY in ANSWER” (”Rocky Mountains in ANSWER”), but with the retrieved text “...took photos of Rocky Mountains in the background when visiting...”, the system may mistakenly identifies “background” as the answer. Intuitively, by imposing a semantic type of LOCATION on answers, we can filter out such noise (background belongs to the type of COGNITION according to WordNet). In contrast, we do not rely on anchor phrases to extract answers but rather use question types and redundancy to filter potential answers. Another effective approach to extract and rank answers is based on redundancy. Brill, Lin, Banko, Dumais and Ng [2001] proposed a method that uses redundancy in two ways. First, relevant relation patterns (linguistic formulations) are identified in the retrieved documents, redundancies are counted. Second, answer redundancy is used to extract relevant 
The environ mental m ismatch caused by additiv e noise and/or channel distortion often degrades th e perform ance of a s peech reco gnition sys tem seriously . V arious ro bustness techniques have been proposed to reduce this mismatch, and one category of them aim s t o normalize the statistics of speech fea tures in bo th training and testing conditions. In general, these statistics norm alization methods deal with the sp eech feature sequ ences in a f ull-band manner, which som ewhat ignores the fact th at dif ferent m odulation frequency com ponents 251  have unequal importance for speech recognition. With the above observations, in this paper we propose that the speech feature streams be proce ssed in a sub-band ma nner. The processed temporal-domain feature sequence is first decomposed into non-uniform sub-bands us ing discrete wavelet transform (DWT), and then each sub-band stream is individuall y processed by the well-known normalization methods, like m ean and variance norm alization (MVN) and histogram equalization (HEQ) . Finally, we reconstruct the feature stream w ith all th e modif ied sub-band streams using inverse D WT. W ith this process, the com ponents that correspond to m ore important modulation spectral bands in the feature sequ ence can be processed separately . For the Aurora-2 clean-condition training task, the new proposed su b-band MVN and HEQ provide relative error rate reductions of 20.32% a nd 16.39% over the conventional MVN a nd HEQ, respectively. These results re veal that the proposed m ethods significantly enhance the robustness of speech features in noise-corrupted environments. 關鍵詞：離散小波轉換、語音辨識、強健性語音特徵參數 keywords: speech recognition, discrete wavelet transform, robust speech features 一、緒論 近年來，語音處理之領域的學者持續地開發研究，使語音處理相關理論與技術不 斷精進成熟，逐漸趨於實際應用的目的，就語音辨識(speech recogn ition)而言，其系統 常因所在環境之雜訊干擾或是傳輸通道的效應，而使辨識效能受到明顯影響。針對這樣 的問題，近年來的研究學者提出了一系列的環境強健性(environmental robustness)技術， 藉此降低雜訊或通道干擾或凸顯語音的獨特成份，而達到明顯的改進效果，本論文的研 究方向，即為開發出新的降低雜訊與通道干擾之相關的語音強健性演算法。然而，跟過 去相關之強健性技術較為不同的是，我們採用了小波轉換(wavelet transform)，對於語音 特徵之時間序列(temporal trajectory)加以處理，來改善語音特徵的強健性。 小波相關理論在訊號處理的範疇中雖已發展數十年，然而相對於其他許多理論而言，應 用於在語音強健性處理之領域中仍偏少數，而其應用的方向大致上主要包含了：語音強 化(speech enhancement)、語音端點偵測(voice activity detection, VAD)、強健性語音特徵 (robust speech feature)與聽覺濾波器設計(auditory filter design)等。我們將它們簡述如下： （一）語音強化(speech enhancement) 語音強化主要目的，通常是在一段訊號中，將雜訊抑制，並將語音訊號成份強調出 來，常用的方式是假設雜訊在頻譜(spectrum)上具有較為穩態(stationary)的特性，在頻域 上將雜訊成份減低，例如設計一濾波器來過濾雜訊等。而以目前基於小波的信號強化方 法，其中之一為 Donoho[1]學者所提出使用小波收縮(wavelet shrinkage)的方式，其方法 是由小波轉換所得之係數，經由門檻值的設定將雜訊適度地抑制。在其相關論文之實驗 結果顯示了，透過小波轉換處理的語音強化效能比起之前所提出的傳統語音強化方法 [2]要來的好。 （二）語音端點偵測(voice activity detection, VAD) 由於一段錄音(recording)裡可能包含有非語音的區段，如果一併辨識整段錄音，將 會影響辨識處理的速度，並可能造成辨識精確度明顯下降。語音端點偵測(voice activity detection, endpoint detection)相關技術即是於決定出一段訊號中真正語音存在的位置。在 傳統的作法上，以時域(time domain)而言，透過計算一段語音信號的能量(energy)或過零 252  率(zero-crossing rate)來決定含有語音成分的位置；在頻域(frequency domain)上，則通常 是計算語音頻譜的熵(entropy)來獲得語音成分的資訊[3]。而小波在此方向上所提出的技 術相對較多，譬如在文獻[4]中提到了使用小波轉換的係數能量比例判定語音及非語音 (non-speech)成分，或是在另一[5]文獻裡提出計算小波係數之變異數，將其視為一組隨 機變數(random variable) 經由機率理論之結果判定，所得分類方法相較於之前方式能更 精確判別出語音跟非語音之成份。 （三）強健性語音特徵擷取(robust speech feature extraction) 此類的語音處理技術方法目的是擷取不容易受到雜訊干擾的語音特徵參數，傳統 的強健性語音特徵擷取技術大多數是在探討語音特徵的頻譜性質進而發展而得，換句話 說，其所使用的轉換法為有名的傅立葉轉換(Fourier transform)。然而小波處理也相繼應 用於強健性語音特徵擷取技術上，例如，在[6]提出將原始梅爾倒頻譜特徵(mel-frequency cepstral co efficients, M FCC)中的離散餘弦轉換(discrete cosine trans form, DCT) 程序改變 為離散小波轉換(discrete wavelet transform, DWT)，其論文呈現的實驗結果顯示所得到的 特徵比原始 MFCC 更具有雜訊環境之強健性。 （四）聽覺濾波器設計(auditory filter design) 一般而言，語音辨識中特徵參數求取程序裡所應用的語音聽覺濾波器組為梅爾尺 度(mel-scaled)的濾波器組，這些濾波器其分佈特性為：1 kHz 頻率以下為線性分佈，1 kHz 以上頻率為非線性分佈，彼此相互部分重疊，其可近似模擬人耳聽覺效應。相對而言， 小波處理之研究學者[7]也提出了利用小波包(wavelet packet) 的特性來仿效人耳聽覺效 應，其適當透過一連串小波包轉換所切割的部份頻帶，選擇出能趨近於人耳聽覺的濾波 器組效應，而由於小波處理所得之彼此頻帶間都假設為不相關，即為互不影響，因此所 切割出來的各頻率範圍的語音信號都涵蓋了獨立的辨識資訊，其中的實驗結果驗證了以 上的處理可以優於傳統的梅爾濾波器組處理，達到將語音辨識精確度提升的目的。 在本論文中，所發展出的新技術，並不同於上述所提的幾個傳統小波處理所應用 的方向，而是著重於將小波處理其特殊的分頻技術適當地運用於語音特徵時間序列 (temporal trajectory) 上，結合各種統計正規化的技術，來處理小波轉後各子頻帶的特徵 時間序列，在之後的章節中我們將會逐步介紹此新技術，分析其主要觀念、作法與可能 優於傳統技術的原因，並以一系列的實驗證實此新技術相對於傳統相近的技術而言，更 能有效提昇語音辨識在雜訊干擾環境下的精確性。 本論文其餘的章節概要如下：在第二章裡，介紹目前常用之強健性特徵統計正規化法並 探討傳統統計正規化法之可能缺失。在第三章，我們將簡要介紹離散小波轉換之分頻技 術的實現，第四章為本論文的重點，我們將在此章中介紹我們所提出的新方法，即兩種 調變頻譜域的分頻統計特徵補償法：分頻帶平均值與變異數正規化法與分頻帶統計圖等 化法，並對其初步效果加以介紹。在第五章，我們將執行一系列的語音辨識實驗，來驗 證所提之新方法足以有效提昇語音特徵在雜訊環境下的強健性，最後，第六章則為簡要 結論，及未來可進一步研究的方向。 二、各種強健性技術介紹 在這裡我們首先目前常用之強健性特徵統計正規化法，之後探討傳統統計正規化法 之可能缺失，並說明為何使用小波轉換(discrete wavelet transform, DWT)改善這些問題。 由於語音辨識系統容易受到雜訊環境影響使得其辨識效能降低，因此語音處理相關 研究的學者針對此雜訊干擾的問題，提出諸多的強健性技術，這些技術中有一大類是藉 由正規化語音特徵的統計特性，來降低雜訊對語音特徵造成的失真。以下將介紹近年來 在強健性語音辨識中常用的幾種語音特徵正規化技術。其中包含了：倒頻譜平均消去法 253  (cepstral mean subtraction, CMS)[8] 、倒頻譜平均值與變異數正規化法(cepstral mean and variance normalization, MVN)[9]、倒頻譜平均與變異數正規化法結合自動回歸動態平均 濾波器法(cepstral m ean and variance norm alization p lus auto-regress ive m oving average filtering, MVA)[10]與統計圖正規化法(histogram equalization, HEQ)[11]等。 上述各種的正規化技術中，皆是把單一維特徵序列之所有特徵視為同一個隨機變數 的取樣(sample)，進而直接估測此隨機變數之統計參數，譬如期望值(mean)、變異數 (variance)與機率分佈(probability distribution)等。雖然程序上易於實現，卻相對忽略了一 段語句之中，其特徵隨時間變化的特性，例如調變頻譜的資訊。從另一觀點來看，這些 作法等同於將全部調變頻率之成份一併做處理。然而根據過去許多的研究發現，不同的 調變頻譜成份對於語音辨識擁有不同的重要性，更精確地說，在 N.Kanedera 學者[12] 詳細指出大部分的語音辨識資訊分布在 1 Hz 和 16 Hz 的調變頻率之間，且主要集中在 4 Hz 附近。因此，許多知名且成功的時間序列濾波器(temporal filters)[13,14] ，都是特別 強調出這些重要的調變頻率成分，進而顯示能有效改善雜訊環境下語音辨識的效能。 而前面介紹的各種特徵統計正規化演算法，可能缺失在於無法有效突顯不同調變頻 率成份對於語音辨識的重要性，因此我們希望能把一特徵時間序列中的不同頻率成份分 離出來，進而個別處理，初步的構想是能對於調變頻率較重要之低頻的部份較精細的處 理，相對比較不重要之高頻的部份則使用較粗略的方式處理。基於此目的，我們發現小 波轉換是個十分有用的工具，優點為其能對一頻率區域作不等分的切割，即將訊號其較 低頻率部分使用較窄的濾波器過濾出來，而高頻部分則用較寬的濾波器得之，之後對於 每個子頻帶的特徵序列作統計正規化法。這樣的程序，相較於傳統的全頻帶式的特徵統 計正規化法，理應可以進一步提昇處理後之特徵的強健性。之後一系列的章節，我們將 逐步介紹小波轉換之分頻理論以及所提出的分頻特徵統計正規化法，最後以實驗結果證 實此分頻式正規化法優於傳統之全頻式正規化方法。  三、小波轉換之分頻技術理論的概述 在這一章中，我們將專門討論小波轉換運用於離散時間訊號(discrete-time signal)的 分頻(frequency division) 技術，此應算是小波轉換最常被用以處理訊號的方向。首先我 們考慮一組典型雙通道的正交鏡像濾波器(quadrature-mirror filter bank, QMF)[15]，如圖 三中所示：  HH (z)  2  2  GH (z)  x[n]  y[n]  H L (z )  2  2  GL (z )  
. Cepstral statistics normalization techniques have been shown to be very successful at improving the noise robustness of speech features. In this paper, we propose a hybrid-based scheme to achieve a more accurate estimate of the statistical information of features in these techniques. By properly integrating codebook and utterance/segment knowledge, the 265  resulting hybrid-based normalization methods significantly outperform conventional utterance-based, segment-based and codebook-based ones in recognition accuracy. For the Aurora-2 clean-condition training task, the proposed hybrid codebook/segment-based histogram equalization (CS-HEQ) achieves an average recognition accuracy of 90.66%, which is better than utterance-based HEQ (87.62%), segment-based HEQ (85.92%) and codebook-based HEQ (85.29%). Furthermore, the high-performance CS-HEQ can be implemented with a short delay and can thus be applied in real-time online systems. A similar performance promotion can be also found in the methods of hybrid-based cepstral mean subtraction (CMS), cepstral mean and variance normalization (CMVN), cepstral gain normalization (CGN) and higher-order cepstral moment normalization (HOCMN). 關鍵詞：語音辨識、碼簿、特徵統計值估測法、強健性語音特徵參數 Keywords: speech recognition, codebook, feature statistics estimate, robust speech features 一、緒論 一語音辨識系統，當其應用於真實環境時，常因環境中諸多無法預期的變異性 (variation)，而使其辨識效能受到明顯影響，為了降低諸多的變異性所發展的各種技術， 一般而言統稱為強健性技術(robustness techniques)，而本論文中，我們則是主要著重於 發展降低環境之雜訊干擾或通道效應的強健性技術。 在諸多降低錄音環境之雜訊干擾的強健性演算法中，有一大類的方法是將訓練與測 試環境下的語音特徵其時間序列統計特性加以正規化(normalization)，以降低訓練與測 試環境之間的不匹配，達到提昇辨識率的目的。在這些演算法中，首要步驟通常是估測 語音特徵的統計值相關資訊，例如在 CMVN 法中所需估測的統計值為平均值(mean)與 變異數(variance)，而在 HEQ 法中必需估測出特徵時間序列的機率分佈(probability distribution)。這些統計估測值的精確度，直接影響到其對應之正規化演算法的效能。 在過去關於上述特徵統計正規化法的文獻中，根據不同的樣本來源，大致上有三種 統計值估測法，分別為整句式、片段式與碼簿式的估測法，顧名思義，第一種直接使用 了整句的語音特徵來估測統計值，第二種則使用了部分(片段)的語音特徵，而第三種則 間接透過語音特徵建立的碼簿[7]來作統計值之估測。我們發現這三種方法各有其優缺 點，因此在本論文中，我們所提出的新統計估測技術，適當地併合碼簿與整句或片段的 特徵資訊，希望得到更精準的語音特徵統計值，進而使各種特徵統計正規化法，在受雜 訊干擾的環境中能夠更有效地提昇語音特徵的強健性，以改善辨認精確度。 本論文其餘的章節概要如下：在第二章，我們將簡要介紹過去三種特徵統計值估測 法之步驟及其可能的優缺點。第三章則介紹我們新提出的兩種併合式(hybrid-based)的統 計值估測法，及其如何運用於各種特徵統計正規化法中。在第四章中，我們介紹語音辨 識實驗之語音資料庫、及新提出的兩種統計估測法在各種特徵統計正規化法的語音辨識 結果及其相關討論。最後，第五章為一簡要結論及未來研究之展望。 二、整句式、分段式與碼簿式特徵統計值估測法 我們在本論文中所討論的五種著名強健性語音特徵正規化技術，分別為倒頻譜平均 消去法(CMS)[1]、倒頻譜平均值與變異數正規化法(CMVN)[2,3]、高階倒頻譜動差正規 266  化法(HOCMN)[4]、倒頻譜增益正規化法(CGN)[5]以及倒頻譜統計圖等化法(HEQ)[6] 等，這些技術所需使用的特徵統計相關資訊，例如：平均值、變異數、高階動差或是機 率分佈等，可由不同的方法估測，而有不同的效果。在本章中，我們將介紹過去學者所 提 之 主 要 三 種 特 徵 統 計 值 估 測 法 ， 包 括 了 整 句 式 (utterance-based)[8] 、 分 段 式 (segment-based)[8]與碼簿式(codebook- based)[9]三類方法，及它們可能的優點與缺點。  (一) 整句式特徵統計值估測法  假設某單一語句之某一維特徵序列表示為  {x [n ];1 ≤ n ≤ N }  (式 2-1)  其中 N 為特徵序列之特徵總個數(即音框總數)。在整句式特徵統計值估測法裡，我們利  用(式 2-1)所列之單句所有特徵，共同估測第m 項特徵 x [m ] 的統計值。換言之，我們假 設 x [m ] 對應至一隨機變數 X [m ]，進而假設整句特徵序列{x [n ];1 ≤ n ≤ N } 為此隨機變  數之樣本(sample)，根據這些樣本，我們可估測出 X [m ]此隨機變數的各種統計值，例如：  1. X [m ]的期望值(平均值)為  2. X [m ]的變異數(variance)為  ∑ μ m = [ ] X [m ],(u )  
Traditional Information Extraction (IE) systems identify many unconnected facts. The objective of this paper is to define a new cross-document information extraction task and demonstrate a system which can extract, rank and track events in two dimensions: temporal and spatial. The system can automatically label the person entities involved in significant events as 'centroid arguments', and then present the events involving the same centroid on a time line and on a geographical map. 
Basilica is an event-driven software architecture for creating conversational agents as a collection of reusable components. Software engineers and computer scientists can use this general architecture to create increasingly sophisticated conversational agents. We have developed agents based on Basilica that have been used in various application scenarios and foresee that agents build on Basilica can cater to a wider variety of interactive situations as we continue to add functionality to our architecture. 
The Speech Transcription Analysis Tool (STAT) is an open source tool for aligning and comparing two phonetically transcribed texts of human speech. The output analysis is a parameterized set of phonological differences. These differences are based upon a selectable set of binary phonetic features such as [voice], [continuant], [high], etc. STAT was initially designed to provide sets of phonological speech patterns in the comparisons of various English accents found in the Speech Accent Archive http://accent.gmu.edu, but its scope and utility expand to matters of language assessment, phonetic training, forensic linguistics, and speech recognition. 
WordNet::SenseRelate::AllWords is a freely available open source Perl package that assigns a sense to every content word (known to WordNet) in a text. It ﬁnds the sense of each word that is most related to the senses of surrounding words, based on measures found in WordNet::Similarity. This method is shown to be competitive with results from recent evaluations including SENSEVAL-2 and SENSEVAL-3. 
Intended Audience The tutorial is targeted at any NLP researcher interested in data‐intensive processing and scalability issues in general. No background in parallel or distributed computing is necessary, but a prior knowledge of HLT is assumed. Course Objectives • Acquire understanding of the MapReduce programming model and how it relates to alternative approaches to concurrent programming. • Acquire understanding of how data‐intensive HLT problems (e.g., text retrieval, iterative optimization problems, etc.) can be solved using MapReduce. • Acquire understanding of the tradeoffs involved in designing MapReduce algorithms and awareness of associated engineering issues. Tutorial Topics The following lists topics that will be covered: • MapReduce algorithm design • Distributed counting applications (e.g., relative frequency estimation) • Applications to text retrieval • Applications to graph algorithms • Applications to iterative optimization algorithms (e.g., EM) • Practical Hadoop issues • Limitations of MapReduce Instructor Bios Jimmy Lin is an assistant professor in the iSchool at the University of Maryland, College Park. He joined the faculty in 2004 after completing his Ph.D. in Electrical Engineering and Computer Science at MIT. Dr. Lin’s research interests lie at the intersection of natural language processing and information retrieval. 
* N-gram collection Use of the MapReduce model; compressing intermediate data; minimizing communication overhead with good sharding functions. * Smoothing Challenges of Katz Backoff and Kneser-Ney Smoothing in a distributed system; Smoothing techniques that are easy to compute in a distributed system: Stupid Backoff, Linear Interpolation; minimizing communication by sharding and aggregation. 2) Model Size Reduction * Pruning Reducing the size of the model by removing n-grams that don't have much impact. Entropy pruning is simple to compute for Stupid Backoff, requires some effort for Katz and Kneser-Ney in a distributed system. Effects of extreme pruning. * Quantization Reducing the memory size of the model by storing approximations of the values. We discuss several quantizers; typically 4 to 8 bits are sufficient to store a floating point value. * Randomized Data Structures Reducing the memory size of the model by changing the set of n-grams that is stored. This typically lets us store models in 3 bytes per n-gram, independent of the n-gram 3 Proceedings of NAACL HLT 2009: Tutorials, pages 3–4, Boulder, Colorado, June 2009. c 2009 Association for Computational Linguistics  order without significant impact on quality. At the same time it provides very fast access to the n-grams. 3) Using Distributed Models * Serving Requesting a single n-gram in a distributed setup is expensive because it requires communication between machines. We show how to use a distributed language model in the first-pass of a decoder by batching up n-gram request. Target Audience Target audience are researchers in all areas that focus on or use large n-gram language models. 
In the second part, we will describe the data formats of each of the layers and talk about various design decisions that went into the creation of the architecture of the database and the individual tables comprising it, along with issues that came up during the representation process and compromises that were made without sacriﬁcing some primary objectives one of which being the independent existence of each layer that is necessary to allow multi-site collaboration. We will explain how the database schema attempts to interconnect all the layers. Then we will go into the details of the Python API that allows easy access to each of the layers and show that by making the objects closely resemble database tables, the API allows for their ﬂexible integration. This will be followed by a hands-on working session. 
During the second part of the tutorial, we will explore VerbNet extensions (how new classes were derived and created through manual and semi-automatic processes), and we will present on-going work on automatic acquisition of Levin-style classes in corpora. The latter is useful for domain-adaptation and tuning of VerbNet for real-world applications which require this. 
Second hour: • Problems of transcription (transliteration) • Generative models of transcription 
This paper examines the applicability of classiﬁer combination approaches such as bagging and boosting for coreference resolution. To the best of our knowledge, this is the ﬁrst effort that utilizes such techniques for coreference resolution. In this paper, we provide experimental evidence which indicates that the accuracy of the coreference engine can potentially be increased by use of bagging and boosting methods, without any additional features or training data. We implement and evaluate combination techniques at the mention, entity and document level, and also address issues like entity alignment, that are speciﬁc to coreference resolution. 
Cross Document Coreference (CDC) is the problem of resolving the underlying identity of entities across multiple documents and is a major step for document understanding. We develop a framework to efﬁciently determine the identity of a person based on extracted information, which includes unary properties such as gender and title, as well as binary relationships with other named entities such as co-occurrence and geo-locations. At the heart of our approach is a suite of similarity functions (specialists) for matching relationships and a relational density-based clustering algorithm that delineates name clusters based on pairwise similarity. We demonstrate the effectiveness of our methods on the WePS benchmark datasets and point out future research directions. 
Some of the Follow-Up Questions (FU Q) that an Interactive Question Answering (IQA) system receives are not topic shifts, but rather continuations of the previous topic. In this paper, we propose an empirical framework to explore such questions, with two related goals in mind: (1) modeling the different relations that hold between the FU Q’s answer and either the FU Q or the preceding dialogue, and (2) showing how this model can be used to identify the correct answer among several answer candidates. For both cases, we use Logistic Regression Models that we learn from real IQA data collected through a live system. We show that by adding dialogue context features and features based on sequences of domain-speciﬁc actions that represent the questions and answers, we obtain important additional predictors for the model, and improve the accuracy with which our system ﬁnds correct answers. 
In this paper, we present ﬁve models for sentence realisation from a bag-of-words containing minimal syntactic information. It has a large variety of applications ranging from Machine Translation to Dialogue systems. Our models employ simple and efﬁcient techniques based on n-gram Language modeling. We evaluated the models by comparing the synthesized sentences with reference sentences using the standard BLEU metric(Papineni et al., 2001). We obtained higher results (BLEU score of 0.8156) when compared to the state-of-art results. In future, we plan to incorporate our sentence realiser in Machine Translation and observe its effect on the translation accuracies. 
An annotation project typically has an abundant supply of unlabeled data that can be drawn from some corpus, but because the labeling process is expensive, it is helpful to pre-screen the pool of the candidate instances based on some criterion of future usefulness. In many cases, that criterion is to improve the presence of the rare classes in the data to be annotated. We propose a novel method for solving this problem and show that it compares favorably to a random sampling baseline and a clustering algorithm. 
We propose a method for modeling pronunciation variation in the context of spell checking for non-native writers of English. Spell checkers, typically developed for native speakers, fail to address many of the types of spelling errors peculiar to non-native speakers, especially those errors inﬂuenced by differences in phonology. Our model of pronunciation variation is used to extend a pronouncing dictionary for use in the spelling correction algorithm developed by Toutanova and Moore (2002), which includes models for both orthography and pronunciation. The pronunciation variation modeling is shown to improve performance for misspellings produced by Japanese writers of English. 
We describe the use of a weakly supervised bootstrapping algorithm in discovering contrasting semantic categories from a source lexicon with little training data. Our method primarily exploits the patterns in sentential contexts where different categories of words may appear. Experimental results are presented showing that such automatically categorized terms tend to agree with human judgements. 
Proﬁle hidden Markov models (Proﬁle HMMs) are speciﬁc types of hidden Markov models used in biological sequence analysis. We propose the use of Proﬁle HMMs for word-related tasks. We test their applicability to the tasks of multiple cognate alignment and cognate set matching, and ﬁnd that they work well in general for both tasks. On the latter task, the Proﬁle HMM method outperforms average and minimum edit distance. Given the success for these two tasks, we further discuss the potential applications of Proﬁle HMMs to any task where consideration of a set of words is necessary. 
This paper describes research on automatically building rapport. This is done by adapting responses in a spoken dialog system to users’ emotions as inferred from nonverbal voice properties. Emotions and their acoustic correlates will be extracted from a persuasive dialog corpus and will be used to implement an emotionally intelligent dialog system; one that can recognize emotion, choose an optimal strategy for gaining rapport, and render a response that contains appropriate emotion, both lexically and auditory. In order to determine the value of emotion modeling for gaining rapport in a spoken dialog system, the ﬁnal implementation will be evaluated using different conﬁgurations through a user study. 
We demonstrate that a supervised annotation learning approach using structured features derived from tokens and prior annotations performs better than a bag of words approach. We present a general graph representation for automatically deriving these features from labeled data. Automatic feature selection based on class association scores requires a large amount of labeled data and direct voting can be difﬁcult and error-prone for structured features, even for language specialists. We show that highlighted rationales from the user can be used for indirect feature voting and same performance can be achieved with less labeled data.We present our results on two annotation learning tasks for opinion mining from product and movie reviews. 
In machine transliteration we transcribe a name across languages while maintaining its phonetic information. In this paper, we present a novel sequence transduction algorithm for the problem of machine transliteration. Our model is discriminatively trained by the MIRA algorithm, which improves the traditional Perceptron training in three ways: (1) It allows us to consider k-best transliterations instead of the best one. (2) It is trained based on the ranking of these transliterations according to user-speciﬁed loss function (Levenshtein edit distance). (3) It enables the user to tune a built-in parameter to cope with noisy non-separable data during training. On an Arabic-English name transliteration task, our model achieves a relative error reduction of 2.2% over a perceptron-based model with similar features, and an error reduction of 7.2% over a statistical machine translation model with more complex features. 
Relation extraction is a challenging task in natural language processing. Syntactic features are recently shown to be quite effective for relation extraction. In this paper, we generalize the state of the art syntactic convolution tree kernel introduced by Collins and Duffy. The proposed generalized kernel is more flexible and customizable, and can be conveniently utilized for systematic generation of more effective application specific syntactic sub-kernels. Using the generalized kernel, we will also propose a number of novel syntactic sub-kernels for relation extraction. These kernels show a remarkable performance improvement over the original Collins and Duffy kernel in the extraction of ACE-2005 relation types.  "airport" and also between "police" and "airport" that can be shown in the following format. Phys.Located(Mark, airport) Phys.Located(police, airport) Relation extraction is a key step towards question answering systems by which vital structured data is acquired from underlying free text resources. Detection of protein interactions in biomedical corpora (Li et al., 2008) is another valuable application of relation extraction. Relation extraction can be approached by a standard classification learning method. We particularly use SVM (Boser et al., 1992; Cortes and Vapnik, 1995) and kernel functions as our classification method. A kernel is a function that calculates the inner product of two transformed vectors of a high dimensional feature space using the original feature vectors as shown in eq. 1.  K ( X i , X j ) = φ( X i ).φ( X j )  (1)  
This paper presents an overview of our participation in the TAC 2008 Opinion Pilot Summarization task, as well as the proposed and evaluated post-competition improvements. We first describe our opinion summarization system and the results obtained. Further on, we identify the system’s weak points and suggest several improvements, focused both on information content, as well as linguistic and readability aspects. We obtain encouraging results, especially as far as Fmeasure is concerned, outperforming the competition results by approximately 80%. 
We present a shallow approach to the sentence ordering problem. The employed features are based on discourse entities, shallow syntactic analysis, and temporal precedence relations retrieved from VerbOcean. We show that these relatively simple features perform well in a machine learning algorithm on datasets containing sequences of events, and that the resulting models achieve optimal performance with small amounts of training data. The model does not yet perform well on datasets describing the consequences of events, such as the destructions after an earthquake. 
When engaged in dialogues, people perform communicative actions to pursue speciﬁc communicative goals. Speech acts recognition attracted computational linguistics since long time and could impact considerably a huge variety of application domains. We study the task of automatic labeling dialogues with the proper dialogue acts, relying on empirical methods and simply exploiting lexical semantics of the utterances. In particular, we present some experiments in supervised and unsupervised framework on both an English and an Italian corpus of dialogue transcriptions. The evaluation displays encouraging results in both languages, especially in the unsupervised version of the methodology. 
Letter-to-phoneme conversion plays an important role in several applications. It can be a difﬁcult task because the mapping from letters to phonemes can be many-to-many. We present a language independent letter-to-phoneme conversion approach which is based on the popular phrase based Statistical Machine Translation techniques. The results of our experiments clearly demonstrate that such techniques can be used effectively for letter-tophoneme conversion. Our results show an overall improvement of 5.8% over the baseline and are comparable to the state of the art. We also propose a measure to estimate the difﬁculty level of L2P task for a language. 
Cohesive constraints allow the phrase-based decoder to employ arbitrary, non-syntactic phrases, and encourage it to translate those phrases in an order that respects the source dependency tree structure. We present extensions of the cohesive constraints, such as exhaustive interruption count and rich interruption check. We show that the cohesion-enhanced decoder signiﬁcantly outperforms the standard phrasebased decoder on English→Spanish. Improvements between 0.5 and 1.2 BLEU point are obtained on English→Iraqi system. 
This paper revisits optimal decoding for statistical machine translation using IBM Model 4. We show that exact/optimal inference using Integer Linear Programming is more practical than previously suggested when used in conjunction with the Cutting-Plane Algorithm. In our experiments we see that exact inference can provide a gain of up to one BLEU point for sentences of length up to 30 tokens. 
Hypergraphs are used in several syntaxinspired methods of machine translation to compactly encode exponentially many translation hypotheses. The hypotheses closest to given reference translations therefore cannot be found via brute force, particularly for popular measures of closeness such as BLEU. We develop a dynamic program for extracting the so called oracle-best hypothesis from a hypergraph by viewing it as the problem of ﬁnding the most likely hypothesis under an n-gram language model trained from only the reference translations. We further identify and remove massive redundancies in the dynamic program state due to the sparsity of n-grams present in the reference translations, resulting in a very efﬁcient program. We present runtime statistics for this program, and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components. 
We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase-based statistical machine translation. The approach avoids major complexity limitations via a two-pass architecture. The first pass is performed using a conventional phrase-based SMT model. The second pass is performed by a re-ordering strategy guided by shallow semantic parsers that produce both semantic frame and role labels. Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase-based SMT baseline – to our knowledge, the first successful application of semantic role labeling to SMT. 
We show how the integration of an extended lexicon model into the decoder can improve translation performance. The model is based on lexical triggers that capture long-distance dependencies on the sentence level. The results are compared to variants of the model that are applied in reranking of n-best lists. We present how a combined application of these models in search and rescoring gives promising results. Experiments are reported on the GALE Chinese-English task with improvements of up to +0.9% BLEU and -1.5% TER absolute on a competitive baseline. 
This paper explores corpus-based bilingual retrieval where the translation corpora used vary by source and size. We ﬁnd that the quality of translation alignments and the domain of the bitext are important. In some settings these factors are more critical than corpus size. We also show that judicious choice of tokenization can reduce the amount of bitext required to obtain good bilingual retrieval performance. 
 2 Related work  We present a large-scale, data-driven approach to computing distributional similarity scores for queries. We contrast this to recent webbased techniques which either require the offline computation of complete phrase vectors, or an expensive on-line interaction with a search engine interface. Independent of the computational advantages of our approach, we show empirically that our technique is more effective at ranking query alternatives that the computationally more expensive technique of using the results from a web search engine. 
Requiring only category names as user input is a highly attractive, yet hardly explored, setting for text categorization. Earlier bootstrapping results relied on similarity in LSA space, which captures rather coarse contextual similarity. We suggest improving this scheme by identifying concrete references to the category name’s meaning, obtaining a special variant of lexical expansion. 
In this paper we present a novel approach to categorizing comments in online reviews as either a qualiﬁed claim or a bald claim. We argue that this distinction is important based on a study of customer behavior in making purchasing decisions using online reviews. We present results of a supervised algorithm for learning this distinction. The two types of claims are expressed differently in language and we show that syntactic features capture this difference, yielding improvement over a bag-of-words baseline. 
Detailed image annotation necessary for reliable image retrieval involves not only annotating the image as a single artifact, but also annotating speciﬁc objects or regions within the image. Such detailed annotation is a costly endeavor and the available annotated image data are quite limited. This paper explores the feasibility of using image captions from scientiﬁc journals for the purpose of automatically annotating image regions. Salient image clues, such as an object location within the image or an object color, together with the associated explicit object mention, are extracted and classiﬁed using rule-based and SVM learners. 
In this paper, we present The gEoSpatial Language Annotator (TESLA)—a tool which supports human annotation of geospatial language corpora. TESLA interfaces with a GIS database for annotating grounded geospatial entities and uses Google Earth for visualization of both entity search results and evolving object and speaker position from GPS tracks. We also discuss a current annotation effort using TESLA to annotate location descriptions in a geospatial language corpus. 
Automatically detecting dialogue structure within corpora of human-human dialogue is the subject of increasing attention. In the domain of tutorial dialogue, automatic discovery of dialogue structure is of particular interest because these structures inherently represent tutorial strategies or modes, the study of which is key to the design of intelligent tutoring systems that communicate with learners through natural language. We propose a methodology in which a corpus of humanhuman tutorial dialogue is first manually annotated with dialogue acts. Dependent adjacency pairs of these acts are then identified through χ2 analysis, and hidden Markov modeling is applied to the observed sequences to induce a descriptive model of the dialogue structure. 
We investigate natural language understanding of partial speech recognition results to equip a dialogue system with incremental language processing capabilities for more realistic human-computer conversations. We show that relatively high accuracy can be achieved in understanding of spontaneous utterances before utterances are completed. 
Semi-supervised speaker clustering refers to the use of our prior knowledge of speakers in general to assist the unsupervised speaker clustering process. In the form of an independent training set, the prior knowledge helps us learn a speaker-discriminative feature transformation, a universal speaker prior model, and a discriminative speaker subspace, or equivalently a speaker-discriminative distance metric. The directional scattering patterns of Gaussian mixture model mean supervectors motivate us to perform discriminant analysis on the unit hypersphere rather than in the Euclidean space, which leads to a novel dimensionality reduction technique called spherical discriminant analysis (SDA). Our experiment results show that in the SDA subspace, speaker clustering yields superior performance than that in other reduceddimensional subspaces (e.g., PCA and LDA). 
A stochastic approach based on Dynamic Bayesian Networks (DBNs) is introduced for spoken language understanding. DBN-based models allow to infer and then to compose semantic frame-based tree structures from speech transcriptions. Experimental results on the French MEDIA dialog corpus show the appropriateness of the technique which both lead to good tree identiﬁcation results and can provide the dialog system with n-best lists of scored hypotheses. 
For a robot working in an open environment, a task-oriented language capability will not be sufﬁcient. In order to adapt to the environment, such a robot will have to learn language dynamically. We developed a System for Noun Concepts Acquisition from utterances about Images, SINCA in short. It is a language acquisition system without knowledge of grammar and vocabulary, which learns noun concepts from user utterances. We recorded a video of a child’s daily life to collect dialogue data that was spoken to and around him. The child is a member of a family consisting of the parents and his sister. We evaluated the performance of SINCA using the collected data. In this paper, we describe the algorithms of SINCA and an evaluation experiment. We work on Japanese language acquisition, however our method can easily be adapted to other languages. 
We examine the capacity of Web and corpus frequency methods to predict preferred count classiﬁers for nouns in Malay. The observed F-score for the Web model of 0.671 considerably outperformed corpus-based frequency and machine learning models. We expect that this is a fruitful extension for Web–as–corpus approaches to lexicons in languages other than English, but further research is required in other South-East and East Asian languages. 
We describe a simple strategy to achieve translation performance improvements by combining output from identical statistical machine translation systems trained on alternative morphological decompositions of the source language. Combination is done by means of Minimum Bayes Risk decoding over a shared Nbest list. When translating into English from two highly inﬂected languages such as Arabic and Finnish we obtain signiﬁcant improvements over simply selecting the best morphological decomposition. 
This work focuses on generating children’s HMM-based acoustic models for speech recognition from adult acoustic models. Collecting children’s speech data is more costly compared to adult’s speech. The patentpending method developed in this work requires only adult data to estimate synthetic children’s acoustic models in any language and works as follows: For a new language where only adult data is available, an adult male and an adult female model is trained. A linear transformation from each male HMM mean vector to its closest female mean vector is estimated. This transform is then scaled to a certain power and applied to the female model to obtain a synthetic children’s model. In a pronunciation verification task the method yields 19% and 3.7% relative improvement on native English and Spanish children’s data, respectively, compared to the best adult model. For Spanish data, the new model outperforms the available real children’s data based model by 13% relative. 
The automatic identiﬁcation of prosodic events such as pitch accent in English has long been a topic of interest to speech researchers, with applications to a variety of spoken language processing tasks. However, much remains to be understood about the best methods for obtaining high accuracy detection. We describe experiments examining the optimal domain for accent analysis. Speciﬁcally, we compare pitch accent identiﬁcation at the syllable, vowel or word level as domains for analysis of acoustic indicators of accent. Our results indicate that a word-based approach is superior to syllable- or vowel-based detection, achieving an accuracy of 84.2%. 
Most Spoken Dialog Systems are based on speech grammars and frame/slot semantics. The semantic descriptions of input utterances are usually deﬁned ad-hoc with no ability to generalize beyond the target application domain or to learn from annotated corpora. The approach we propose in this paper exploits machine learning of frame semantics, borrowing its theoretical model from computational linguistics. While traditional automatic Semantic Role Labeling approaches on written texts may not perform as well on spoken dialogs, we show successful experiments on such porting. Hence, we design and evaluate automatic FrameNet-based parsers both for English written texts and for Italian dialog utterances. The results show that disﬂuencies of dialog data do not severely hurt performance. Also, a small set of FrameNet-like manual annotations is enough for realizing accurate Semantic Role Labeling on the target domains of typical Dialog Systems. 
Various knowledge sources are used for spoken dialog systems such as task model, domain model, and agenda. An agenda graph is one of the knowledge sources for a dialog management to reflect a discourse structure. This paper proposes a clustering and linking method to automatically construct an agenda graph from human-human dialogs. Preliminary evaluation shows our approach would be helpful to reduce human efforts in designing prior knowledge. 
The paper presents a novel sentence pair extraction algorithm for comparable data, where a large set of candidate sentence pairs is scored directly at the sentence-level. The sentencelevel extraction relies on a very efﬁcient implementation of a simple symmetric scoring function: a computation speed-up by a factor of 30 is reported. On Spanish-English data, the extraction algorithm ﬁnds the highest scoring sentence pairs from close to 1 trillion candidate pairs without search errors. Signiﬁcant improvements in BLEU are reported by including the extracted sentence pairs into the training of a phrase-based SMT (Statistical Machine Translation) system. 
When linear classiﬁers cannot successfully classify data, we often add combination features, which are products of several original features. The searching for effective combination features, namely feature engineering, requires domain-speciﬁc knowledge and hard work. We present herein an efﬁcient algorithm for learning an L1 regularized logistic regression model with combination features. We propose to use the grafting algorithm with efﬁcient computation of gradients. This enables us to ﬁnd optimal weights efﬁciently without enumerating all combination features. By using L1 regularization, the result we obtain is very compact and achieves very efﬁcient inference. In experiments with NLP tasks, we show that the proposed method can extract effective combination features, and achieve high performance with very few features. 
Voice Search applications provide a very convenient and direct access to a broad variety of services and information. However, due to the vast amount of information available and the open nature of the spoken queries, these applications still suffer from recognition errors. This paper explores the utilization of personalization features for the post-processing of recognition results in the form of n-best lists. Personalization is carried out from three different angles: short-term, long-term and Web-based, and a large variety of features are proposed for use in a log-linear classiﬁcation framework. Experimental results on data obtained from a commercially deployed Voice Search system show that the combination of the proposed features leads to a substantial sentence error rate reduction. In addition, it is shown that personalization features which are very different in nature can successfully complement each other. 
We present an experiment aimed at understanding how to optimally use acoustic and prosodic information to predict a speaker’s level of certainty. With a corpus of utterances where we can isolate a single word or phrase that is responsible for the speaker’s level of certainty we use different sets of sub-utterance prosodic features to train models for predicting an utterance’s perceived level of certainty. Our results suggest that using prosodic features of the word or phrase responsible for the level of certainty and of its surrounding context improves the prediction accuracy without increasing the total number of features when compared to using only features taken from the utterance as a whole. 
We present a novel two-stage technique for detecting speech disﬂuencies based on Integer Linear Programming (ILP). In the ﬁrst stage we use state-of-the-art models for speech disﬂuency detection, in particular, hidden-event language models, maximum entropy models and conditional random ﬁelds. During testing each model proposes possible disﬂuency labels which are then assessed in the presence of local and global constraints using ILP. Our experimental results show that by using ILP we can improve the performance of our models with negligible cost in processing time. The less training data is available the larger the improvement due to ILP. 
Contrastive summarization is the problem of jointly generating summaries for two entities in order to highlight their differences. In this paper we present an investigation into contrastive summarization through an implementation and evaluation of a contrastive opinion summarizer in the consumer reviews domain. 
This paper presents a method for automatic topic identiﬁcation using a graph-centrality algorithm applied to an encyclopedic graph derived from Wikipedia. When tested on a data set with manually assigned topics, the system is found to signiﬁcantly improve over a simpler baseline that does not make use of the external encyclopedic knowledge. 
This paper proposes an approach for bilingual dictionary extraction from comparable corpora. The proposed approach is based on the observation that a word and its translation share similar dependency relations. Experimental results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional contextbased approach that uses bag-of-words around translation candidates. 
We adapt a semantic role parser to the domain of goal-directed speech by creating an artiﬁcial treebank from an existing text treebank. We use a three-component model that includes distributional models from both target and source domains. We show that we improve the parser’s performance on utterances collected from human-machine dialogues by training on the artiﬁcially created data without loss of performance on the text treebank. 
This paper describes work in progress towards using non-phonemic respellings as an additional source of information besides spelling in the process of extending pronunciation lexicons for speech recognition and text-tospeech systems. Preliminary experimental data indicates that the approach is likely to be successful. The major benefit of the approach is that it makes extending pronunciation lexicons accessible to average users. 
The optimal combination of language model (LM) and language understanding model (LUM) varies depending on available training data and utterances to be handled. Usually, a lot of effort and time are needed to ﬁnd the optimal combination. Instead, we have designed and developed a new framework that uses multiple LMs and LUMs to improve speech understanding accuracy under various situations. As one implementation of the framework, we have developed a method for selecting the most appropriate speech understanding result from several candidates. We use two LMs and three LUMs, and thus obtain six combinations of them. We empirically show that our method improves speech understanding accuracy. The performance of the oracle selection suggests further potential improvements in our system. 
Actively sampled data can have very different characteristics than passively sampled data. Therefore, it’s promising to investigate using different inference procedures during AL than are used during passive learning (PL). This general idea is explored in detail for the focused case of AL with cost-weighted SVMs for imbalanced data, a situation that arises for many HLT tasks. The key idea behind the proposed InitPA method for addressing imbalance is to base cost models during AL on an estimate of overall corpus imbalance computed via a small unbiased sample rather than the imbalance in the labeled training data, which is the leading method used during PL. 
Syntax-based MT systems have proven effective—the models are compelling and show good room for improvement. However, decoding involves a slow search. We present a new lazy-search method that obtains signiﬁcant speedups over a strong baseline, with no loss in Bleu. 
We present a policy-based error analysis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression. We demonstrate that these limitations arise from the strong assumption of locality of the decision making process in the search for an acceptable derivation in this paradigm. 
 In building practical two-way speech-to-speech translation systems the end user will always wish to use the system in an environment different from the original training data. As with all speech systems, it is important to allow the system to adapt to the actual usage situations. This paper investigates how a speech-to-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two. The platform is the CMU Iraqi-English portable two-way speechto-speech system as developed under the DARPA TransTac program. We show how machine translation, speech recognition and overall system performance can be improved on day 2 after adapting from day 1 in both a supervised and unsupervised way. 
The accuracy of a Cross Document Coreference system depends on the amount of context available, which is a parameter that varies greatly from corpora to corpora. This paper presents a statistical model for computing name perplexity classes. For each perplexity class, the prior probability of coreference is estimated. The amount of context required for coreference is controlled by the prior coreference probability. We show that the prior probability coreference is an important factor for maintaining a good balance between precision and recall for cross document coreference systems. 
Answer Validation is a topic of significant interest within the Question Answering community. In this paper, we propose the use of language modeling methodologies for Answer Validation, using corpus-based methods that do not require the use of external sources. Specifically, we propose a model for Answer Credibility which quantifies the reliability of a source document that contains a candidate answer and the Question’s Context Model.  development of an Answer Credibility score which quantifies reliability of a source document that contains a candidate answer with respect to the Question’s Context Model. Unlike many textual entailment methods, our methodology has the advantage of being applicable to question types for which hypothesis generation is not easily accomplished. The remainder of this paper describes our work in progress, including our model for Answer Credibility, our experiments and results to date, and future work.  
This paper describes how named entity (NE) classes can be used to improve broad coverage surface realization with the OpenCCG realizer. Our experiments indicate that collapsing certain multi-word NEs and interpolating a language model where NEs are replaced by their class labels yields the largest quality increase, with 4-grams adding a small additional boost. Substantial further beneﬁt is obtained by including class information in the hypertagging (supertagging for realization) component of the system, yielding a state-of-theart BLEU score of 0.8173 on Section 23 of the CCGbank. A targeted manual evaluation conﬁrms that the BLEU score increase corresponds to a signiﬁcant rise in ﬂuency. 
We propose a new method to rank a special category of time-sensitive queries that are year qualiﬁed. The method adjusts the retrieval scores of a base ranking function according to time-stamps of web documents so that the freshest documents are ranked higher. Our method, which is based on feedback control theory, uses ranking errors to adjust the search engine behavior. For this purpose, we use a simple but effective method to extract year qualiﬁed queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents. Our method was tested on a commercial search engine. The experiments show that our approach can signiﬁcantly improve relevance ranking for year qualiﬁed queries even if all the existing methods for comparison failed. 
This paper presents a new soft pattern matching method which aims to improve the recall with minimized precision loss in information extraction tasks. Our approach is based on a local tree alignment algorithm, and an effective strategy for controlling ﬂexibility of the pattern matching will be presented. The experimental results show that the method can signiﬁcantly improve the information extraction performance. 
This work addresses the problem of genre classiﬁcation of text and speech transcripts, with the goal of handling genres not seen in training. Two frameworks employing different statistics on word/POS histograms with a PCA transform are examined: a single model for each genre and a factored representation of genre. The impact of the two frameworks on the classiﬁcation of training-matched and new genres is discussed. Results show that the factored models allow for a ﬁner-grained representation of genre and can more accurately characterize genres not seen in training. 
The complexity of sentences characteristic to biomedical articles poses a challenge to natural language parsers, which are typically trained on large-scale corpora of non-technical text. We propose a text simplification process, bioSimplify, that seeks to reduce the complexity of sentences in biomedical abstracts in order to improve the performance of syntactic parsers on the processed sentences. Syntactic parsing is typically one of the first steps in a text mining pipeline. Thus, any improvement in performance would have a ripple effect over all processing steps. We evaluated our method using a corpus of biomedical sentences annotated with syntactic links. Our empirical results show an improvement of 2.90% for the Charniak-McClosky parser and of 4.23% for the Link Grammar parser when processing simplified sentences rather than the original sentences in the corpus. 
In recent years, Structural Correspondence Learning (SCL) is becoming one of the most promising techniques for sentiment-transfer learning. However, SCL model treats each feature as well as each instance by an equivalent-weight strategy. To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances. More specifically, W-SCL assigns a smaller weight to high-frequency domain-specific (HFDS) features and assigns a larger weight to instances with the same label as the involved pivot feature. The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features. 
MICA is a dependency parser which returns deep dependency representations, is fast, has state-of-the-art performance, and is freely available. 
In this paper, we examine user adaptation to the system’s lexical and syntactic choices in the context of the deployed Let’s Go! dialog system. We show that in deployed dialog systems with real users, as in laboratory experiments, users adapt to the system’s lexical and syntactic choices. We also show that the system’s lexical and syntactic choices, and consequent user adaptation, can have an impact on recognition of task-related concepts. This means that system prompt formulation, even in ﬂexible input dialog systems, can be used to guide users into producing utterances conducive to task success. 
We analyze the recognition errors made by a morph-based continuous speech recognition system, which practically allows an unlimited vocabulary. Examining the role of the acoustic and language models in erroneous regions shows how speaker adaptive training (SAT) and discriminative training with minimum phone frame error (MPFE) criterion decrease errors in different error classes. Analyzing the errors with respect to word frequencies and manually classiﬁed error types reveals the most potential areas for improving the system.  speech (CTS) domains. Named entities were a common cause for errors in the BN domain, and hesitation, repeats and partially spoken words in the CTS domain. This paper analyzes the errors made by a Finnish morph-based continuous recognition system (Hirsima¨ki et al., 2009). In addition to partitioning the errors using ERA, we compare the number of letter errors in different regions and analyze what kind of errors are corrected when speaker adaptive training and discriminative training are taken in use. The most potential error sources are also studied by partitioning the errors according to manual error classes and word frequencies.  
This paper presents empirical evidence for the orthogonality of the DIT++ multidimensional dialogue act annotation scheme, showing that the ten dimensions of communication which underlie this scheme are addressed independently in natural dialogue. 
In this paper, we propose the use of metadata contained in documents to improve coreference resolution. Speciﬁcally, we quantify the impact of speaker and turn information on the performance of our coreference system, and show that the metadata can be effectively encoded as features of a statistical resolution system, which leads to a statistically signiﬁcant improvement in performance. 
Conventional confusion network based system combination for machine translation (MT) heavily relies on features that are based on the measure of agreement of words in different translation hypotheses. This paper presents two new features that consider agreement of n-grams in different hypotheses to improve the performance of system combination. The first one is based on a sentence specific online n-gram language model, and the second one is based on n-gram voting. Experiments on a large scale Chinese-to-English MT task show that both features yield significant improvements on the translation performance, and a combination of them produces even better translation results. 
In this paper, we present a Chinese event extraction system. We point out a language specific issue in Chinese trigger labeling, and then commit to discussing the contributions of lexical, syntactic and semantic features applied in trigger labeling and argument labeling. As a result, we achieved competitive performance, specifically, F-measure of 59.9 in trigger labeling and F-measure of 43.8 in argument labeling.  same task as we did in this paper. However, to our knowledge, the language specific issue and feature contributions for Chinese event extraction have not been reported by earlier researchers. The remainder of the paper is organized as follows. Section 2 points out a language specific issue in Chinese trigger labeling and discusses two strategies of trigger labeling: word-based and character-based. Section 3 presents argument labeling. Section 4 discusses the experimental results. Section 5 concludes the paper. 2 Trigger Labeling  
In this paper, we describe and evaluate a bigram part-of-speech (POS) tagger that uses latent annotations and then investigate using additional genre-matched unlabeled data for self-training the tagger. The use of latent annotations substantially improves the performance of a baseline HMM bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. The performance of the latent tagger is further enhanced by self-training with a large set of unlabeled data, even in situations where standard bigram or trigram taggers do not beneﬁt from selftraining when trained on greater amounts of labeled training data. Our best model obtains a state-of-the-art Chinese tagging accuracy of 94.78% when evaluated on a representative test set of the Penn Chinese Treebank 6.0. 
Automatic post-editing (APE) systems aim at correcting the output of machine translation systems to produce better quality translations, i.e. produce translations can be manually postedited with an increase in productivity. In this work, we present an APE system that uses statistical models to enhance a commercial rulebased machine translation (RBMT) system. In addition, a procedure for effortless human evaluation has been established. We have tested the APE system with two corpora of different complexity. For the Parliament corpus, we show that the APE system signiﬁcantly complements and improves the RBMT system. Results for the Protocols corpus, although less conclusive, are promising as well. Finally, several possible sources of errors have been identiﬁed which will help develop future system enhancements.  Many of these works propose a combination of rule-based machine translation (RBMT) and statistical machine translation (SMT) systems, in order to take advantage of the particular capabilities of each system (Chen and Chen, 1997). A possible combination is to automatically postedit the output of a RBMT system employing a SMT system. In this work, we will apply this technique into two different corpora: Parliament and Protocols. In addition, we will propose a new human evaluation measure that will deal with the impact of the automatic post-editing. This paper is structured as follows: after a brief introduction of the RBMT, SMT, and APE systems in Section 2, Section 3 details the carried out experimentation, discussing its results. Finally, some conclusions and future work are presented in Section 4. 2 Systems description  
Recent research on multilingual statistical machine translation focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs. Due to the richness of available language resources, English is in general the pivot language of choice. In this paper, we investigate the appropriateness of languages other than English as pivot languages. Experimental results using state-ofthe-art statistical machine translation techniques to translate between twelve languages revealed that the translation quality of 61 out of 110 language pairs improved when a non-English pivot language was chosen. 
We compare two approaches to dependency tree linearization, a task which arises in many NLP applications. The ﬁrst one is the widely used ’overgenerate and rank’ approach which relies exclusively on a trigram language model (LM); the second one combines language modeling with a maximum entropy classiﬁer trained on a range of linguistic features. The results provide strong support for the combined method and show that trigram LMs are appropriate for phrase linearization while on the clause level a richer representation is necessary to achieve comparable performance.  an alternative. Thus, it would be of interest to draw such a comparison, especially on English data, where LMs are usually expected to work well. As an improvement to the LM-based approach, we propose a combined method which distinguishes between the phrase and the clause levels: • it relies on a trigram LM to order words within phrases; • it ﬁnds the order of clause constituents (i.e., constituents dependent on a ﬁnite verb) with a maximum entropy classiﬁer trained on a range of linguistic features.  
In this paper we compare three approaches to adverbial positioning using lexical, syntactic, semantic and sentence-level features. We ﬁnd that: (a), one- and two-stage classiﬁcationbased approaches can achieve almost 86% accuracy in determining the absolute position of adverbials; (b) a classiﬁer trained with only syntactic features gives performance close to that of a classiﬁer trained with all features; and (c) a surface realizer incorporating a two-stage classiﬁer for adverbial positioning as the second stage gives improvements of at least 10% in simple string accuracy over a baseline realizer for sentences containing adverbials. 
Word sense distributions are usually skewed. Predicting the extent of the skew can help a word sense disambiguation (WSD) system determine whether to consider evidence from the local context or apply the simple yet effective heuristic of using the ﬁrst (most frequent) sense. In this paper, we propose a method to estimate the entropy of a sense distribution to boost the precision of a ﬁrst sense heuristic by restricting its application to words with lower entropy. We show on two standard datasets that automatic prediction of entropy can increase the performance of an automatic ﬁrst sense heuristic. 
 2 Previous Work  Sentence Boundary Detection is widely used but often with outdated tools. We discuss what makes it difﬁcult, which features are relevant, and present a fully statistical system, now publicly available, that gives the best known error rate on a standard news corpus: Of some 27,000 examples, our system makes 67 errors, 23 involving the word “U.S.” 
We experiment with several chunking models. Deeper architectures achieve better generalization. Quadratic ﬁlters, a simpliﬁcation of a theoretical model of V1 complex cells, reliably increase accuracy. In fact, logistic regression with quadratic ﬁlters outperforms a standard single hidden layer neural network. Adding quadratic ﬁlters to logistic regression is almost as eﬀective as feature engineering. Despite predicting each output label independently, our model is competitive with ones that use previous decisions. 
Active learning has proven to be a successful strategy in quick development of corpora to be used in training of statistical natural language parsers. A vast majority of studies in this ﬁeld has focused on estimating informativeness of samples; however, representativeness of samples is another important criterion to be considered in active learning. We present a novel metric for estimating representativeness of sentences, based on a modiﬁcation of Zipf’s Principle of Least Effort. Experiments on WSJ corpus with a wide-coverage parser show that our method performs always at least as good as and generally signiﬁcantly better than alternative representativeness-based methods. 
 1.1 Related Work  Combining the 1-best output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser (Henderson and Brill, 1999; Sagae and Lavie, 2006). We propose three ways to improve upon existing methods for parser combination. First, we propose a method of parse hybridization that recombines context-free productions instead of constituents, thereby preserving the structure of the output of the individual parsers to a greater extent. Second, we propose an efﬁcient lineartime algorithm for computing expected f-score using Minimum Bayes Risk parse selection. Third, we extend these parser combination methods from multiple 1-best outputs to multiple n-best outputs. We present results on WSJ section 23 and also on the English side of a Chinese-English parallel corpus. 
This paper describes research on parsing Tagalog text for predicate–argument structure (PAS). We ﬁrst outline the linguistic phenomenon and corpus annotation process, then detail a series of PAS parsing experiments. 
The spoken term detection (STD) task aims to return relevant segments from a spoken archive that contain the query terms. This paper focuses on the decision stage of an STD system. We propose a term speciﬁc thresholding (TST) method that uses per query posterior score distributions. The STD system described in this paper indexes word-level lattices produced by an LVCSR system using Weighted Finite State Transducers (WFSTs). The target application is a sign dictionary where precision is more important than recall. Experiments compare the performance of different thresholding techniques. The proposed approach increases the maximum precision attainable by the system. 
This paper presents a new method for automatically generating abbreviations for Chinese organization names. Abbreviations are commonly used in spoken Chinese, especially for organization names. The generation of Chinese abbreviation is much more complex than English abbreviations, most of which are acronyms and truncations. The abbreviation generation process is formulated as a character tagging problem and the conditional random ﬁeld (CRF) is used as the tagging model. A carefully selected group of features is used in the CRF model. After generating a list of abbreviation candidates using the CRF, a length model is incorporated to re-rank the candidates. Finally the full-name and abbreviation co-occurrence information from a web search engine is utilized to further improve the performance. We achieved top-10 coverage of 88.3% by the proposed method. 
Information retrieval and spoken-term detection from audio such as broadcast news, telephone conversations, conference calls, and meetings are of great interest to the academic, government, and business communities. Motivated by the requirement for high-quality indexes, this study explores the effect of using both word and sub-word information to ﬁnd in-vocabulary and OOV query terms. It also explores the trade-off between search accuracy and the speed of audio transcription. We present a novel, vocabulary independent, hybrid LVCSR approach to audio indexing and search and show that using phonetic confusions derived from posterior probabilities estimated by a neural network in the retrieval of OOV queries can help in reducing misses. These methods are evaluated on data sets from the 2006 NIST STD task. 
In this paper, we discuss the beneﬁts of tightly coupling speech recognition and search components in the context of a speech-driven search application. We demonstrate that by incorporating constraints from the information repository that is being searched not only improves the speech recognition accuracy but also results in higher search accuracy. 
We supplement WordNet entries with information on the subjectivity of its word senses. Supervised classiﬁers that operate on word sense deﬁnitions in the same way that text classiﬁers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary deﬁnitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet deﬁnitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classiﬁcation, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. 
This paper introduces an integrative approach to automatic word sense subjectivity annotation. We use features that exploit the hierarchical structure and domain information in lexical resources such as WordNet, as well as other types of features that measure the similarity of glosses and the overlap among sets of semantically related words. Integrated in a machine learning framework, the entire set of features is found to give better results than any individual type of feature. 
This paper presents and compares WordNetbased and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses. 
We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 
In this paper we explore a learning-based approach to the problem of predicting language impairment in children. We analyzed spontaneous narratives of children and extracted features measuring different aspects of language including morphology, speech ﬂuency, language productivity and vocabulary. Then, we evaluated a learning-based approach and compared its predictive accuracy against a method based on language models. Empirical results on monolingual English-speaking children and bilingual Spanish-English speaking children show the learning-based approach is a promising direction for automatic language assessment. 
Conventional approaches to Chinese word segmentation treat the problem as a characterbased tagging task. Recently, semi-Markov models have been applied to the problem, incorporating features based on complete words. In this paper, we propose an alternative, a latent variable model, which uses hybrid information based on both word sequences and character sequences. We argue that the use of latent variables can help capture long range dependencies and improve the recall on segmenting long words, e.g., named-entities. Experimental results show that this is indeed the case. With this improvement, evaluations on the data of the second SIGHAN CWS bakeoff show that our system is competitive with the best ones in the literature. 
We present an unsupervised approach to reconstructing ancient word forms. The present work addresses three limitations of previous work. First, previous work focused on faithfulness features, which model changes between successive languages. We add markedness features, which model well-formedness within each language. Second, we introduce universal features, which support generalizations across languages. Finally, we increase the number of languages to which these methods can be applied by an order of magnitude by using improved inference methods. Experiments on the reconstruction of ProtoOceanic, Proto-Malayo-Javanic, and Classical Latin show substantial reductions in error rate, giving the best results to date. 
We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show signiﬁcant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 
We investigate the problem of unsupervised part-of-speech tagging when raw parallel data is available in a large number of languages. Patterns of ambiguity vary greatly across languages and therefore even unannotated multilingual data can serve as a learning signal. We propose a non-parametric Bayesian model that connects related tagging decisions across languages through the use of multilingual latent variables. Our experiments show that performance improves steadily as the number of languages increases. 
Recent applications of Tree-Adjoining Grammar (TAG) to the domain of semantics as well as new attention to syntactic phenomena have given rise to increased interested in more expressive and complex multicomponent TAG formalisms (MCTAG). Although many constructions can be modeled using tree-local MCTAG (TL-MCTAG), certain applications require even more ﬂexibility. In this paper we suggest a shift in focus from constraining locality and complexity through treeand set-locality to constraining locality and complexity through restrictions on the derivational distance between trees in the same tree set in a valid derivation. We examine three formalisms, restricted NS-MCTAG, restricted Vector-TAG and delayed TL-MCTAG, that use notions of derivational distance to constrain locality and demonstrate how they permit additional expressivity beyond TLMCTAG without increasing complexity to the level of set local MCTAG. 
 The big dog barks  Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts. Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns. In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing. Our model produces state-of-theart results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points. 
We introduce alignment models for Machine Translation that take into account the context of a source word when determining its translation. Since the use of these contexts alone causes data sparsity problems, we develop a decision tree algorithm for clustering the contexts based on optimisation of the EM auxiliary function. We show that our contextdependent models lead to an improvement in alignment quality, and an increase in translation quality when the alignments are used in Arabic-English and Chinese-English translation.  
Current phrase-based statistical machine translation systems process each test sentence in isolation and do not enforce global consistency constraints, even though the test data is often internally consistent with respect to topic or style. We propose a new consistency model for machine translation in the form of a graph-based semi-supervised learning algorithm that exploits similarities between training and test data and also similarities between different test sentences. The algorithm learns a regression function jointly over training and test data and uses the resulting scores to rerank translation hypotheses. Evaluation on two travel expression translation tasks demonstrates improvements of up to 2.6 BLEU points absolute and 2.8% in PER. 
In current phrase-based SMT systems, more training data is generally better than less. However, a larger data set eventually introduces a larger model that enlarges the search space for the translation problem, and consequently requires more time and more resources to translate. We argue redundant information in a SMT system may not only delay the computations but also affect the quality of the outputs. This paper proposes an approach to reduce the model size by ﬁltering out the less probable entries based on compatible data in an intermediate language, a novel use of triangulation, without sacriﬁcing the translation quality. Comprehensive experiments were conducted on standard data sets. We achieved signiﬁcant quality improvements (up to 2.3 BLEU points) while translating with reduced models. In addition, we demonstrate a straightforward combination method for more progressive ﬁltering. The reduction of the model size can be up to 94% with the translation quality being preserved. 
An important part of textual inference is making deductions involving monotonicity, that is, determining whether a given assertion entails restrictions or relaxations of that assertion. For instance, the statement ‘We know the epidemic spread quickly’ does not entail ‘We know the epidemic spread quickly via ﬂeas’, but ‘We doubt the epidemic spread quickly’ entails ‘We doubt the epidemic spread quickly via ﬂeas’. Here, we present the ﬁrst algorithm for the challenging lexical-semantics problem of learning linguistic constructions that, like ‘doubt’, are downward entailing (DE). Our algorithm is unsupervised, resource-lean, and effective, accurately recovering many DE operators that are missing from the handconstructed lists that textual-inference systems currently use. 
Nominals frequently surface without overtly expressed arguments. In order to measure the potential beneﬁt of nominal SRL for downstream processes, such nominals must be accounted for. In this paper, we show that a state-of-the-art nominal SRL system with an overall argument F1 of 0.76 suffers a performance loss of more than 9% when nominals with implicit arguments are included in the evaluation. We then develop a system that takes implicit argumentation into account, improving overall performance by nearly 5%. Our results indicate that the degree of implicit argumentation varies widely across nominals, making automated detection of implicit argumentation an important step for nominal SRL. 
We describe a generative model for clustering named entities which also models named entity internal structure, clustering related words by role. The model is entirely unsupervised; it uses features from the named entity itself and its syntactic context, and coreference information from an unsupervised pronoun resolver. The model scores 86% on the MUC-7 named-entity dataset. To our knowledge, this is the best reported score for a fully unsupervised model, and the best score for a generative model. 
We propose a principled probabilisitc framework which uses trees over the vocabulary to capture similarities among terms in an information retrieval setting. This allows the retrieval of documents based not just on occurrences of speciﬁc query terms, but also on similarities between terms (an effect similar to query expansion). Additionally our principled generative model exhibits an effect similar to inverse document frequency. We give encouraging experimental evidence of the superiority of the hierarchical Dirichlet tree compared to standard baselines. 
This paper introduces a new approach to ranking speech utterances by a system’s conﬁdence that they contain a spoken word. Multiple alternate pronunciations, or degradations, of a query word’s phoneme sequence are hypothesized and incorporated into the ranking function. We consider two methods for hypothesizing these degradations, the best of which is constructed using factored phrasebased statistical machine translation. We show that this approach is able to signiﬁcantly improve upon a state-of-the-art baseline technique in an evaluation on held-out speech. We evaluate our systems using three different methods for indexing the speech utterances (using phoneme, phoneme multigram, and word recognition), and ﬁnd that degradation modeling shows particular promise for locating out-of-vocabulary words when the underlying indexing system is constructed with standard word-based speech recognition. 
We propose a uniﬁed approach to web search query alterations in Japanese that is not limited to particular character types or orthographic similarity between a query and its alteration candidate. Our model is based on previous work on English query correction, but makes some crucial improvements: (1) we augment the query-candidate list to include orthographically dissimilar but semantically similar pairs; and (2) we use kernel-based lexical semantic similarity to avoid the problem of data sparseness in computing querycandidate similarity. We also propose an efﬁcient method for generating query-candidate pairs for model training and testing. We show that the proposed method achieves about 80% accuracy on the query alteration task, improving over previously proposed methods that use semantic similarity. 
Computational processing of text exchanged in interactive venues in which participants engage in simultaneous conversations can beneﬁt from techniques for automatically grouping overlapping sequences of messages into separate conversations, a problem known as “disentanglement.” While previous methods exploit both lexical and non-lexical information that exists in conversations for this task, the inter-dependency between the meaning of a message and its temporal and social contexts is largely ignored. Our approach exploits contextual properties (both explicit and hidden) to probabilistically expand each message to provide a more accurate message representation. Extensive experimental evaluations show our approach outperforms the best previously known technique. 
Morphological segmentation breaks words into morphemes (the basic semantic units). It is a key component for natural language processing systems. Unsupervised morphological segmentation is attractive, because in every language there are virtually unlimited supplies of text, but very few labeled resources. However, most existing model-based systems for unsupervised morphological segmentation use directed generative models, making it difﬁcult to leverage arbitrary overlapping features that are potentially helpful to learning. In this paper, we present the ﬁrst log-linear model for unsupervised morphological segmentation. Our model uses overlapping features such as morphemes and their contexts, and incorporates exponential priors inspired by the minimum description length (MDL) principle. We present efﬁcient algorithms for learning and inference by combining contrastive estimation with sampling. Our system, based on monolingual features only, outperforms a state-of-the-art system by a large margin, even when the latter uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor. 
We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically signiﬁcant improvements of +1.5 B and +1.1 B, respectively. We analyze the impact of the new features and the performance of the learning algorithm. 
The tree-transducer grammars that arise in current syntactic machine translation systems are large, ﬂat, and highly lexicalized. We address the problem of parsing efﬁciently with such grammars in three ways. First, we present a pair of grammar transformations that admit an efﬁcient cubic-time CKY-style parsing algorithm despite leaving most of the grammar in n-ary form. Second, we show how the number of intermediate symbols generated by this transformation can be substantially reduced through binarization choices. Finally, we describe a two-pass coarse-to-ﬁne parsing approach that prunes the search space using predictions from a subset of the original grammar. In all, parsing time reduces by 81%. We also describe a coarse-to-ﬁne pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 
We propose a novel probabilistic synchoronous context-free grammar formalism for statistical machine translation, in which syntactic nonterminal labels are represented as “soft” preferences rather than as “hard” matching constraints. This formalism allows us to efﬁciently score unlabeled synchronous derivations without forgoing traditional syntactic constraints. Using this score as a feature in a log-linear model, we are able to approximate the selection of the most likely unlabeled derivation. This helps reduce fragmentation of probability across differently labeled derivations of the same translation. It also allows the importance of syntactic preferences to be learned alongside other features (e.g., the language model) and for particular labeling procedures. We show improvements in translation quality on small and medium sized Chinese-to-English translation tasks. 
We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems. Similar to other preprocessing reordering approaches, our method can efﬁciently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding. For a set of ﬁve subject-object-verb (SOV) order languages, we show signiﬁcant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems. 
In this paper, we propose a method for learning reordering model for BTG-based statistical machine translation (SMT). The model focuses on linguistic features from bilingual phrases. Our method involves extracting reordering examples as well as features such as part-of-speech and word class from aligned parallel sentences. The features are classified with special considerations of phrase lengths. We then use these features to train the maximum entropy (ME) reordering model. With the model, we performed Chinese-to-English translation tasks. Experimental results show that our bilingual linguistic model outperforms the state-of-the-art phrase-based and BTG-based SMT systems by improvements of 2.41 and 1.31 BLEU points respectively. 
A wish is “a desire or hope for something to happen.” In December 2007, people from around the world offered up their wishes to be printed on confetti and dropped from the sky during the famous New Year’s Eve “ball drop” in New York City’s Times Square. We present an in-depth analysis of this collection of wishes. We then leverage this unique resource to conduct the ﬁrst study on building general “wish detectors” for natural language text. Wish detection complements traditional sentiment analysis and is valuable for collecting business intelligence and insights into the world’s wants and desires. We demonstrate the wish detectors’ effectiveness on domains as diverse as consumer product reviews and online political discussions. 
We address a text regression problem: given a piece of text, predict a real-world continuous quantity associated with the text’s meaning. In this work, the text is an SEC-mandated ﬁnancial report published annually by a publiclytraded company, and the quantity to be predicted is volatility of stock returns, an empirical measure of ﬁnancial risk. We apply wellknown regression techniques to a large corpus of freely available ﬁnancial reports, constructing regression models of volatility for the period following a report. Our models rival past volatility (a strong baseline) in predicting the target variable, and a single model that uses both can signiﬁcantly outperform past volatility. Interestingly, our approach is more accurate for reports after the passage of the Sarbanes-Oxley Act of 2002, giving some evidence for the success of that legislation in making ﬁnancial reports more informative. 
Domain adaptation is an important problem in named entity recognition (NER). NER classiﬁers usually lose accuracy in the domain transfer due to the different data distribution between the source and the target domains. The major reason for performance degrading is that each entity type often has lots of domainspeciﬁc term representations in the different domains. The existing approaches usually need an amount of labeled target domain data for tuning the original model. However, it is a labor-intensive and time-consuming task to build annotated training data set for every target domain. We present a domain adaptation method with latent semantic association (LaSA). This method effectively overcomes the data distribution difference without leveraging any labeled target domain data. LaSA model is constructed to capture latent semantic association among words from the unlabeled corpus. It groups words into a set of concepts according to the related context snippets. In the domain transfer, the original term spaces of both domains are projected to a concept space using LaSA model at ﬁrst, then the original NER model is tuned based on the semantic association features. Experimental results on English and Chinese corpus show that LaSA-based domain adaptation signiﬁcantly enhances the performance of NER. 
State of the art set expansion algorithms produce varying quality expansions for different entity types. Even for the highest quality expansions, errors still occur and manual refinements are necessary for most practical uses. In this paper, we propose algorithms to aide this refinement process, greatly reducing the amount of manual labor required. The methods rely on the fact that most expansion errors are systematic, often stemming from the fact that some seed elements are ambiguous. Using our methods, empirical evidence shows that average R-precision over random entity sets improves by 26% to 51% when given from 5 to 10 manually tagged errors. Both proposed refinement models have linear time complexity in set size allowing for practical online use in set expansion systems. 
This paper introduces a novel unsupervised constraint-driven learning algorithm for identifying named-entity (NE) transliterations in bilingual corpora. The proposed method does not require any annotated data or aligned corpora. Instead, it is bootstrapped using a simple resource – a romanization table. We show that this resource, when used in conjunction with constraints, can efﬁciently identify transliteration pairs. We evaluate the proposed method on transliterating English NEs to three different languages - Chinese, Russian and Hebrew. Our experiments show that constraint driven learning can signiﬁcantly outperform existing unsupervised models and achieve competitive results to existing supervised models. 
Syllables play an important role in speech synthesis and recognition. We present several different approaches to the syllabiﬁcation of phonemes. We investigate approaches based on linguistic theories of syllabiﬁcation, as well as a discriminative learning technique that combines Support Vector Machine and Hidden Markov Model technologies. Our experiments on English, Dutch and German demonstrate that our transparent implementation of the sonority sequencing principle is more accurate than previous implementations, and that our language-independent SVM-based approach advances the current state-of-the-art, achieving word accuracy of over 98% in English and 99% in German and Dutch. 
One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities. Adaptor grammars are a framework for deﬁning a variety of hierarchical nonparametric Bayesian models. This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task. With appropriate adaptor grammars and inference procedures we achieve an 87% word token f-score on the standard Brent version of the BernsteinRatner corpus, which is an error reduction of over 35% over the best previously reported results for this corpus. 
For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser). This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system. We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser. Our model produces a consistent output, where the named entity spans do not conﬂict with the phrasal spans of the parse tree. The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of up to 1.36% absolute F1 for parsing, and up to 9.0% F1 for named entity recognition. 
The extent to which the organization of natural language grammars reﬂects a drive to minimize dependency length remains little explored. We present the ﬁrst algorithm polynomial-time in sentence length for obtaining the minimal-length linearization of a dependency tree subject to constraints of mild context sensitivity. For the minimally contextsensitive case of gap-degree 1 dependency trees, we prove several properties of minimallength linearizations which allow us to improve the efﬁciency of our algorithm to the point that it can be used on most naturallyoccurring sentences. We use the algorithm to compare optimal, observed, and random sentence dependency length for both surface and deep dependencies in English and German. We ﬁnd in both languages that analyses of surface and deep dependencies yield highly similar results, and that mild contextsensitivity affords very little reduction in minimal dependency length over fully projective linearizations; but that observed linearizations in German are much closer to random and farther from minimal-length linearizations than in English. 
Statistical parsing models have recently been proposed that employ a bounded stack in timeseries (left-to-right) recognition, using a rightcorner transform deﬁned over training trees to minimize stack use (Schuler et al., 2008). Corpus results have shown that a vast majority of naturally-occurring sentences can be parsed in this way using a very small stack bound of three to four elements. This suggests that the standard cubic-time CKY chart-parsing algorithm, which implicitly assumes an unbounded stack, may be wasting probability mass on trees whose complexity is beyond human recognition or generation capacity. This paper ﬁrst describes a version of the rightcorner transform that is deﬁned over entire probabilistic grammars (cast as inﬁnite sets of generable trees), in order to ensure a fair comparison between bounded-stack and unbounded PCFG parsing using a common underlying model; then it presents experimental results that show a bounded-stack right-corner parser using a transformed version of a grammar signiﬁcantly outperforms an unboundedstack CKY parser using the original grammar. 
This paper presents a novel unsupervised method for hierarchical topic segmentation. Lexical cohesion – the workhorse of unsupervised linear segmentation – is treated as a multi-scale phenomenon, and formalized in a Bayesian setting. Each word token is modeled as a draw from a pyramid of latent topic models, where the structure of the pyramid is constrained to induce a hierarchical segmentation. Inference takes the form of a coordinate-ascent algorithm, iterating between two steps: a novel dynamic program for obtaining the globally-optimal hierarchical segmentation, and collapsed variational Bayesian inference over the hidden variables. The resulting system is fast and accurate, and compares well against heuristic alternatives. 
We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our ﬁnal model, HIERSUM, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content speciﬁcity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, HIERSUM yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)’s state-of-the-art discriminative system. We also explore HIERSUM’s capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation. 
We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reﬂects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be elegantly represented using a distribution over permutations called the generalized Mallows model. Our structureaware approach substantially outperforms alternative approaches for cross-document comparison and single-document segmentation.1 
In incremental spoken dialogue systems, partial hypotheses about what was said are required even while the utterance is still ongoing. We deﬁne measures for evaluating the quality of incremental ASR components with respect to the relative correctness of the partial hypotheses compared to hypotheses that can optimize over the complete input, the timing of hypothesis formation relative to the portion of the input they are about, and hypothesis stability, deﬁned as the number of times they are revised. We show that simple incremental post-processing can improve stability dramatically, at the cost of timeliness (from 90 % of edits of hypotheses being spurious down to 10 % at a lag of 320 ms). The measures are not independent, and we show how system designers can ﬁnd a desired operating point for their ASR. To our knowledge, we are the ﬁrst to suggest and examine a variety of measures for assessing incremental ASR and improve performance on this basis. 
Voice search is increasingly popular, especially for local business directory assistance. However, speech recognition accuracy on business listing names is still low, leading to user frustration. In this paper, we present a new algorithm for geo-centric language model generation for local business voice search for mobile users. Our algorithm has several advantages: it provides a language model for any user in any location; the geographic area covered by the language model is adapted to the local business density, giving high recognition accuracy; and the language models can be pre-compiled, giving fast recognition time. In an experiment using spoken business listing name queries from a business directory assistance service, we achieve a 16.8% absolute improvement in recognition accuracy and a 3-fold speedup in recognition time with geocentric language models when compared with a nationwide language model. 
In this paper, we show that linguistically motivated pronunciation rules can improve phone and word recognition results for Modern Standard Arabic (MSA). Using these rules and the MADA morphological analysis and disambiguation tool, multiple pronunciations per word are automatically generated to build two pronunciation dictionaries; one for training and another for decoding. We demonstrate that the use of these rules can signiﬁcantly improve both MSA phone recognition and MSA word recognition accuracies over a baseline system using pronunciation rules typically employed in previous work on MSA Automatic Speech Recognition (ASR). We obtain a signiﬁcant improvement in absolute accuracy in phone recognition of 3.77%–7.29% and a signiﬁcant improvement of 4.1% in absolute accuracy in ASR. 
Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries. However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufﬁciently diverse source segmentation lattices. In this work, we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding. Using a model optimized for German translation, we present results showing signiﬁcant improvements in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines. 
Statistical machine translation (SMT) models need large bilingual corpora for training, which are unavailable for some language pairs. This paper provides the ﬁrst serious experimental study of active learning for SMT. We use active learning to improve the quality of a phrase-based SMT system, and show signiﬁcant improvements in translation compared to a random sentence selection baseline, when test and training data are taken from the same or different domains. Experimental results are shown in a simulated setting using three language pairs, and in a realistic situation for Bangla-English, a language pair with limited translation resources. 
This paper presents a semi-supervised learning framework for mining Chinese-English lexicons from large amount of Chinese Web pages. The issue is motivated by the observation that many Chinese neologisms are accompanied by their English translations in the form of parenthesis. We classify parenthetical translations into bilingual abbreviations, transliterations, and translations. A frequency-based term recognition approach is applied for extracting bilingual abbreviations. A self-training algorithm is proposed for mining transliteration and translation lexicons. In which, we employ available lexicons in terms of morpheme levels, i.e., phoneme correspondences in transliteration and grapheme (e.g., sufﬁx, stem, and preﬁx) correspondences in translation. The experimental results veriﬁed the effectiveness of our approaches. 
This paper describes a lattice-based decoder for hierarchical phrase-based translation. The decoder is implemented with standard WFST operations as an alternative to the well-known cube pruning procedure. We ﬁnd that the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in fewer search errors, direct generation of translation lattices in the target language, better parameter optimization, and improved translation performance when rescoring with long-span language models and MBR decoding. We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFSTbased hierarchical decoder with hierarchical translation under cube pruning. 
This paper describes research on automatic assessment of the pronunciation quality of spontaneous non-native adult speech. Since the speaking content is not known prior to the assessment, a two-stage method is developed to ﬁrst recognize the speaking content based on non-native speech acoustic properties and then forced-align the recognition results with a reference acoustic model reﬂecting native and near-native speech properties. Features related to Hidden Markov Model likelihoods and vowel durations are extracted. Words with low recognition conﬁdence can be excluded in the extraction of likelihood-related features to minimize erroneous alignments due to speech recognition errors. Our experiments on the TOEFL R Practice Online test, an English language assessment, suggest that the recognition/forced-alignment method can provide useful pronunciation features. Our new pronunciation features are more meaningful than an utterance-based normalized acoustic model score used in previous research from a construct point of view. 
We investigate the task of performance prediction for language models belonging to the exponential family. First, we attempt to empirically discover a formula for predicting test set cross-entropy for n-gram language models. We build models over varying domains, data set sizes, and n-gram orders, and perform linear regression to see whether we can model test set performance as a simple function of training set performance and various model statistics. Remarkably, we ﬁnd a simple relationship that predicts test set performance with a correlation of 0.9997. We analyze why this relationship holds and show that it holds for other exponential language models as well, including class-based models and minimum discrimination information models. Finally, we discuss how this relationship can be applied to improve language model performance. 
This paper presents a new perspective to the language modeling problem by moving the word representations and modeling into the continuous space. In a previous work we introduced Gaussian-Mixture Language Model (GMLM) and presented some initial experiments. Here, we propose Tied-Mixture Language Model (TMLM), which does not have the model parameter estimation problems that GMLM has. TMLM provides a great deal of parameter tying across words, hence achieves robust parameter estimation. As such, TMLM can estimate the probability of any word that has as few as two occurrences in the training data. The speech recognition experiments with the TMLM show improvement over the word trigram model. 
In (Chen, 2009), we show that for a variety of language models belonging to the exponential family, the test set cross-entropy of a model can be accurately predicted from its training set cross-entropy and its parameter values. In this work, we show how this relationship can be used to motivate two heuristics for “shrinking” the size of a language model to improve its performance. We use the ﬁrst heuristic to develop a novel class-based language model that outperforms a baseline word trigram model by 28% in perplexity and 1.9% absolute in speech recognition word-error rate on Wall Street Journal data. We use the second heuristic to motivate a regularized version of minimum discrimination information models and show that this method outperforms other techniques for domain adaptation.  
In this paper we model discussions in online political blogs. To do this, we extend Latent Dirichlet Allocation (Blei et al., 2003), in various ways to capture different characteristics of the data. Our models jointly describe the generation of the primary documents (posts) as well as the authorship and, optionally, the contents of the blog community’s verbal reactions to each post (comments). We evaluate our model on a novel comment prediction task where the models are used to predict which blog users will leave comments on a given post. We also provide a qualitative discussion about what the models discover. 
With the in-depth study of sentiment analysis research, finer-grained opinion mining, which aims to detect opinions on different review features as opposed to the whole review level, has been receiving more and more attention in the sentiment analysis research community recently. Most of existing approaches rely mainly on the template extraction to identify the explicit relatedness between product feature and opinion terms, which is insufficient to detect the implicit review features and mine the hidden sentiment association in reviews, which satisfies (1) the review features are not appear explicit in the review sentences; (2) it can be deduced by the opinion words in its context. From an information theoretic point of view, this paper proposed an iterative reinforcement framework based on the improved information bottleneck algorithm to address such problem. More specifically, the approach clusters product features and opinion words simultaneously and iteratively by fusing both their semantic information and co-occurrence information. The experimental results demonstrate that our approach outperforms the template extraction based approaches. 
We address the problem of large-scale automatic detection of online reviews without using any human labels. We propose an efﬁcient method that combines two basic ideas: Building a classiﬁer from a large number of noisy examples and using the structure of the website to enhance the performance of this classiﬁer. Experiments suggest that our method is competitive against supervised learning methods that mandate expensive human effort. 
Work on sentiment analysis often focuses on the words and phrases that people use in overtly opinionated text. In this paper, we introduce a new approach to the problem that focuses not on lexical indicators, but on the syntactic “packaging” of ideas, which is well suited to investigating the identiﬁcation of implicit sentiment, or perspective. We establish a strong predictive connection between linguistically well motivated features and implicit sentiment, and then show how computational approximations of these features can be used to improve on existing state-of-the-art sentiment classiﬁcation results. 
In this paper, we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems. We present an efﬁcient low-memory method for constructing high-order approximate n-gram frequency counts. The method is based on a deterministic streaming algorithm which efﬁciently computes approximate frequency counts over a stream of data while employing a small memory footprint. We show that this method easily scales to billion-word monolingual corpora using a conventional (8 GB RAM) desktop machine. Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods. 
This paper reports the effect of corpus size on case frame acquisition for discourse analysis in Japanese. For this study, we collected a Japanese corpus consisting of up to 100 billion words, and constructed case frames from corpora of six different sizes. Then, we applied these case frames to syntactic and case structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words. 
The idea that some words carry more semantic content than others, has led to the notion of term specificity, or informativeness. Computational estimation of this quantity is important for various applications such as information retrieval. We propose a new method of computing term specificity, based on modeling the rate of learning of word meaning in Latent Semantic Analysis (LSA). We analyze the performance of this method both qualitatively and quantitatively and demonstrate that it shows excellent performance compared to existing methods on a broad range of tests. We also demonstrate how it can be used to improve existing applications in information retrieval and summarization. 
Tree substitution grammars (TSGs) are a compelling alternative to context-free grammars for modelling syntax. However, many popular techniques for estimating weighted TSGs (under the moniker of Data Oriented Parsing) suffer from the problems of inconsistency and over-ﬁtting. We present a theoretically principled model which solves these problems using a Bayesian non-parametric formulation. Our model learns compact and simple grammars, uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 
Both coarse-to-ﬁne and A∗ parsing use simple grammars to guide search in complex ones. We compare the two approaches in a common, agenda-based framework, demonstrating the tradeoffs and relative strengths of each method. Overall, coarse-to-ﬁne is much faster for moderate levels of search errors, but below a certain threshold A∗ is superior. In addition, we present the ﬁrst experiments on hierarchical A∗ parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarseto-ﬁne case because of accumulated slack in A∗ heuristics. 
This paper presents a ﬁrst-order logic learning approach to determine rhetorical relations between discourse segments. Beyond linguistic cues and lexical information, our approach exploits compositional semantics and segment discourse structure data. We report a statistically signiﬁcant improvement in classifying relations over attribute-value learning paradigms such as Decision Trees, RIPPER and Naive Bayes. For discourse parsing, our modiﬁed shift-reduce parsing model that uses our relation classiﬁer signiﬁcantly outperforms a right-branching majority-class baseline. 
Recent work has shown that explicitly identifying and ﬁltering non-anaphoric mentions prior to coreference resolution can improve the performance of a coreference system. We present a novel approach to this task of anaphoricity determination based on graph cuts, and demonstrate its superiority to competing approaches by comparing their effectiveness in improving a learning-based coreference system on the ACE data sets. 
The number of research publications in various disciplines is growing exponentially. Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material. In this paper we present the first steps in producing an automatically generated, readily consumable, technical survey. Specifically we explore the combination of citation information and summarization techniques. Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role. 
We describe a statistical model over linguistic areas and phylogeny. Our model recovers known areas and identiﬁes a plausible hierarchy of areal features. The use of areas improves genetic reconstruction of languages both qualitatively and quantitatively according to a variety of metrics. We model linguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman’s coalescent. 
Multi-task learning is the problem of maximizing the performance of a system across a number of related tasks. When applied to multiple domains for the same task, it is similar to domain adaptation, but symmetric, rather than limited to improving performance on a target domain. We present a more principled, better performing model for this problem, based on the use of a hierarchical Bayesian prior. Each domain has its own domain-speciﬁc parameter for each feature but, rather than a constant prior over these parameters, the model instead links them via a hierarchical Bayesian global prior. This prior encourages the features to have similar weights across domains, unless there is good evidence to the contrary. We show that the method of (Daume´ III, 2007), which was presented as a simple “preprocessing step,” is actually equivalent, except our representation explicitly separates hyperparameters which were tied in his work. We demonstrate that allowing different values for these hyperparameters signiﬁcantly improves performance over both a strong baseline and (Daume´ III, 2007) within both a conditional random ﬁeld sequence model for named entity recognition and a discriminatively trained dependency parser. 
The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide signiﬁcant speedups and (2) can even ﬁnd better solutions than those found by batch EM. We support these ﬁndings on four unsupervised tasks: part-of-speech tagging, document classiﬁcation, word segmentation, and word alignment. 
This paper explores several unsupervised approaches to automatic keyword extraction using meeting transcripts. In the TFIDF (term frequency, inverse document frequency) weighting framework, we incorporated partof-speech (POS) information, word clustering, and sentence salience score. We also evaluated a graph-based approach that measures the importance of a word based on its connection with other sentences or words. The system performance is evaluated in different ways, including comparison to human annotated keywords using F-measure and a weighted score relative to the oracle system performance, as well as a novel alternative human evaluation. Our results have shown that the simple unsupervised TFIDF approach performs reasonably well, and the additional information from POS and sentence score helps keyword extraction. However, the graph method is less effective for this domain. Experiments were also performed using speech recognition output and we observed degradation and different patterns compared to human transcripts. 
This paper introduces the Finite-State TurnTaking Machine (FSTTM), a new model to control the turn-taking behavior of conversational agents. Based on a non-deterministic ﬁnite-state machine, the FSTTM uses a cost matrix and decision theoretic principles to select a turn-taking action at any time. We show how the model can be applied to the problem of end-of-turn detection. Evaluation results on a deployed spoken dialog system show that the FSTTM provides signiﬁcantly higher responsiveness than previous approaches. 
Automatically extracting social meaning and intention from spoken dialogue is an important task for dialogue systems and social computing. We describe a system for detecting elements of interactional style: whether a speaker is awkward, friendly, or ﬂirtatious. We create and use a new spoken corpus of 991 4-minute speed-dates. Participants rated their interlocutors for these elements of style. Using rich dialogue, lexical, and prosodic features, we are able to detect ﬂirtatious, awkward, and friendly styles in noisy natural conversational data with up to 75% accuracy, compared to a 50% baseline. We describe simple ways to extract relatively rich dialogue features, and analyze which features performed similarly for men and women and which were gender-speciﬁc. 
In this paper, we extend methods from Roark and Hollingshead (2008) for reducing the worst-case complexity of a context-free parsing pipeline via hard constraints derived from ﬁnite-state tagging pre-processing. Methods from our previous paper achieved quadratic worst-case complexity. We prove here that alternate methods for choosing constraints can achieve either linear or O(N log2N ) complexity. These worst-case bounds on processing are demonstrated to be achieved without reducing the parsing accuracy, in fact in some cases improving the accuracy. The new methods achieve observed performance comparable to the previously published quadratic complexity method. Finally, we demonstrate improved performance by combining complexity bounding methods with additional high precision constraints. 
This paper introduces three new syntactic models for representing speech with repairs. These models are developed to test the intuition that the erroneous parts of speech repairs (reparanda) are not generated or recognized as such while occurring, but only after they have been corrected. Thus, they are designed to minimize the differences in grammar rule applications between ﬂuent and disﬂuent speech containing similar structure. The three models considered in this paper are also designed to isolate the mechanism of impact, by systematically exploring different variables. 
Human sentence processing involves integrating probabilistic knowledge from a variety of sources in order to incrementally determine the hierarchical structure for the serial input stream. While a large number of sentence processing effects have been explained in terms of comprehenders’ rational use of probabilistic information, effects of local coherences have not. We present here a new model of local coherences, viewing them as resulting from a belief-update process, and show that the relevant probabilities in our model are calculable from a probabilistic Earley parser. Finally, we demonstrate empirically that an implemented version of the model makes the correct predictions for the materials from the original experiment demonstrating local coherence effects. 
After completing her degree in Physics she spent a year as a volunteer at a National Children’s Home. Among other things she helped disabled children organize their lives with the aid of a computer. This clearly inﬂuenced her choice of further study and career. Moving to the University of Edinburgh in 1985, she gained a distinguished Masters in Knowledge Based Systems in 1986, and a Ph.D. in Artiﬁcial Intelligence in 1989. Subsequently, she went on to carry out postdoctoral research at Cambridge and Glasgow, then teaching Computing at Glasgow before joining us at Heriot-Watt in 1995. Alison was promoted to Senior Lecturer in 2002. I am sure that, were it not for her long battle with cancer, she would have enjoyed considerable further academic advancement and recognition. 
After completing her degree in Physics she spent a year as a volunteer at a National Children’s Home. Among other things she helped disabled children organize their lives with the aid of a computer. This clearly inﬂuenced her choice of further study and career. Moving to the University of Edinburgh in 1985, she gained a distinguished Masters in Knowledge Based Systems in 1986, and a Ph.D. in Artiﬁcial Intelligence in 1989. Subsequently, she went on to carry out postdoctoral research at Cambridge and Glasgow, then teaching Computing at Glasgow before joining us at Heriot-Watt in 1995. Alison was promoted to Senior Lecturer in 2002. I am sure that, were it not for her long battle with cancer, she would have enjoyed considerable further academic advancement and recognition. 
After completing her degree in Physics she spent a year as a volunteer at a National Children’s Home. Among other things she helped disabled children organize their lives with the aid of a computer. This clearly inﬂuenced her choice of further study and career. Moving to the University of Edinburgh in 1985, she gained a distinguished Masters in Knowledge Based Systems in 1986, and a Ph.D. in Artiﬁcial Intelligence in 1989. Subsequently, she went on to carry out postdoctoral research at Cambridge and Glasgow, then teaching Computing at Glasgow before joining us at Heriot-Watt in 1995. Alison was promoted to Senior Lecturer in 2002. I am sure that, were it not for her long battle with cancer, she would have enjoyed considerable further academic advancement and recognition. 
I was told that I should give an acceptance speech and was furnished with example texts by previous recipients. They wrote about the development and impact of their ideas. So I will tell you about my beginnings and motivations and then focus on the contributions of my IBM team. In this way the text will have some historical value and may clear up certain widely held misconceptions. 1. Beginnings Information Theory seemed to be one of the most prestigious disciplines during my years as a student at MIT (1954–1962). The faculty included the founders of the ﬁeld— Shannon, Fano, Elias, and others. Some of my contemporaries were Viterbi, Jacobs, Kleinrock (founders of Qualcom), Gallagher, Kailath, and Massey. Not daring to approach Shannon himself, I asked Professor Fano to be my thesis adviser. I was making slow progress when in 1961, after three years of trying, I succeeded in extricating my future wife Milena from communist Czechoslovakia (how this was accomplished is another story) and married her. One problem we needed to solve was how she should occupy herself during the long hours I was spending in the underground stacks of the MIT library. At the time the famous linguist Roman Jacobson was simultaneously a University Professor at Harvard and an Institute Professor at MIT. Russian by origin, he spent 18 inter-war years (1920–1938) in Czechoslovakia, where he became one of the founders of the Prague Linguistic Circle. He had a Czech wife, the anthropologist Svatava Pirkova. He continued to maintain his connections with Czechs, and even young Czechs. I was invited to dinner at his house several times, once also with the newly arrived Milena. Jacobson was well known to have an eye for beautiful young women and he was reputed to enjoy exercising his inﬂuence. When my wife asked him for advice as to what to do, he suggested that she take up a fellowship at MIT which he would arrange for her to get. As promised, she got the fellowship and enrolled in the Ph.D. program of the Linguistics department. It should be appreciated that Jacobson did not interview her in a non-social setting and was aware that her previous schooling ∗ Center for Language and Speech Processing, Johns Hopkins University, 320 Barton Hall, 3400 N. Charles Street, Baltimore, MD 21218, USA. E-mail: jelinek@jhu.edu. This article is the text of the talk given on receipt of the ACL’s Lifetime Achievement Award in 2009. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 4  in Prague consisted only of one year of Slavic studies followed by two years at the Film Academy. So Milena started attending lectures, several of them taught by Noam Chomsky. I sat in with her and got the crazy notion that I should switch from Information Theory to Linguistics. I went so far as to explore this notion with Professor Chomsky. Of course, word got around to my adviser Fano, whom it really upset. He declared that I could contemplate switching only after I had received my doctorate in Information Theory. I had no choice other than to obey. Soon thereafter, my thesis almost ﬁnished, I started interviewing at universities for a faculty position. After my job talk at Cornell I was approached by the eminent linguist Charles Hockett, who said that he hoped that I would accept the Cornell offer and help develop his ideas on how to apply Information Theory to Linguistics. That decided me. Surprisingly, when I took up my post in the fall of 1962, there was no sign of Hockett. After several months I summoned my courage and went to ask him when he wanted to start working with me. He answered that he was no longer interested, that he now concentrated on composing operas. Discouraged a second time, I devoted the next ten years to Information Theory. This was the golden period of government support for science and technology. It seemed easy to get grants. Perhaps that was the reason I neglected to make any arrangements for work during the coming summer of 1972. I phoned Joe Raviv (whom I knew as a colleague from my sabbatical at IBM in 1968–69) to ask if I could spend three months in his group in Yorktown Heights. His answer was “Certainly, the sooner you arrive the better. We are starting to work on speech recognition.” By the time I arrived to take up that summer job, Raviv was promoted to manager of the IBM Scientiﬁc Center in Haifa, and IBM was negotiating with Professor Jonathan Allen of MIT to take over the speech group. These negotiations were not successful, and several weeks later the job was offered to me. I requested Cornell to grant me a leave of absence; they did, and I joined IBM (the following year I asked for and got another year, but when I tried to carry out the same maneuver in 1974, I was turned down). Why did IBM start research in speech recognition? Believe it or not, IBM was worried that, with the advance of computing power, there might soon come a time when all the need for further improvements would disappear, and IBM business would dry up. Somebody came up with the suggestion that speech recognition would require lots of computing cycles. A task force was put together in 1971 to study the matter. The group included John Cocke (inventor of RISC machines), Herman Goldstine (right hand of von Neumann in research leading to ENIAC) and others. It recommended that a Continuous Speech Recognition group be established in the Research Division. So the CSR group was started in early 1972 under the management of Joe Raviv. At the time IBM had a small speech group in one of its development laboratories in Raleigh, NC. (Actually, IBM “had” speech recognition even earlier. At the 1964 World’s Fair in New York, Ernest Nassimbene demonstrated an isolated digits recognizer “in a shoe box.”) Its three main members, Das, Dixon, and Tappert, were transferred from Raleigh to the Research Division in Yorktown. High management concluded that to get going the speech group would need the help of linguists. It transferred temporarily Fred Damerau, Stan Petrick, and Jane Robinson from linguistics to CSR. The stafﬁng of the group was then completed by volunteers from the Computer Sciences Department: Lalit Bahl, Raimo Bakis, George Nagy, and others (later Jim and Janet Baker joined as well). But at the time only Bakis, Das, Dixon, and Tappert knew anything about speech. Towards the end of the summer I took over the direction of the group and received a gift from heaven: the freshly graduated physicist Robert Mercer, who in the spring accepted an IBM job in a group that was abolished before he arrived in September. 484  Jelinek  The Dawn of Statistical ASR and MT  Table 1 Sentences from the Resource Management language. Show locations and C-ratings for all deployed subs that were in their home ports April 5. List the cruisers in Persian Sea that have casualty reports earlier than Jarrett’s oldest one. How many ships were in Galveston May 3rd? Is Puffer’s remaining fuel sufﬁcient to arrive in port at the present speed? How many long tuns is the average displacement of ships in Bering Strait? 2. The Competition In 1971, parallel to the work of the IBM task force, ARPA established a project in Speech Understanding. I don’t know what led to that decision, but the main forces behind it were Allen Newell and J. C. R. Licklinder. Funds were provided to Carnegie Mellon, Systems Development Corporation, Bolt Beranek & Newman, and probably SRI, Sperry-Univac, University of Pennsylvania, UC Berkeley, and UCLA. Not all of these institutions were to ﬁeld complete systems. For instance, Ohio, UCLA, and Berkeley provided consulting by linguists (Peter Ladefoged, Vicky Fromkin, John Ohala, Michael O’Malley, and June Shoup). Here are the names of some other researchers who attended the meetings organized by the new project: Raj Reddy, Dennis Klatt, L. D. Erman, V. Lesser, Bruce Lowerry, Bonnie Nash-Webber, George White, Fil Alleva, Wayne Ward, Don Walker, Victor Zue, Stephanie Sennef, Bill Woods, John Makhoul, Wayne Lea, Beatrice Oshika, and Janet and Jim Baker. IBM was invited to attend the meetings, but we did not compete. ARPA was a six-year project which was supposed to recognize (and interpret?) sentences from a “Resource Management” grammar; for an example of the sentences generated, see Table 1. At the end of the six-year period the project was declared a success because it “met its goals.” Clearly the best of the constructed recognizers was the Dragon System (Baker 1975) implemented by the Bakers, graduate students at CMU. It used Hidden Markov models (HMMs) whereas the rest of the ARPA participants based their work on templates, Dynamic Time Warping (DTW), and hand-written rules. The best of these latter systems was Harpy, developed by another CMU graduate student, Bruce Lowerry. 3. IBM’s Initial Formulation For our ﬁrst task we decided to recognize sentences generated by the so-called New Raleigh grammar, a ﬁnite–state device whose schematic is shown in Figure 1. The grammar is actually a Hidden Markov Model. State transitions generate words and are taken with uniform probability. Generation starts in the initial state, a transition is taken, and a word from the list associated with that transition is selected with uniform probability; then one of the transitions out of the new state is taken (again with uniform probability), a word corresponding to that transition is again selected at random, a new state is reached, and so on. The process continues until one arrives at the ﬁnal state. The grammar generates such bizarre sentences as are shown in Table 2.1 
Northwestern University This article discusses the transition from annotated data to a gold standard, that is, a subset that is sufﬁciently noise-free with high conﬁdence. Unless appropriately reinterpreted, agreement coefﬁcients do not indicate the quality of the data set as a benchmarking resource: High overall agreement is neither sufﬁcient nor necessary to distill some amount of highly reliable data from the annotated material. A mathematical framework is developed that allows estimation of the noise level of the agreed subset of annotated data, which helps promote cautious benchmarking. 1. Introduction By and large, the reason a computational linguist engages in an annotation project is to build a reliable data set for the eventual testing, and possibly training, of an algorithm performing the task. Hence, the crucial question regarding the annotated data set is whether it is good for benchmarking. For classiﬁcation tasks, the current practice is to infer this information from the value of an inter-annotator agreement coefﬁcient such as the κ statistic (Cohen 1960; Siegel and Castellan 1988; Carletta 1996). If agreement is high, the whole of the data set is good for training and testing; the remaining disagreements are typically adjudicated by an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju, Badulescu, and Moldovan 2006) or through discussion (Litman, Hirschberg, and Swerts 2006), or, in case of more than two annotators, the majority label is chosen (Vieira and Poesio 2000).1 There are some studies where cases of disagreement were removed from test data (Markert and Nissim 2002; Dagan, Glickman, and Magnini 2006). If agreement is low, the whole data set is discarded as unreliable. The threshold of acceptability seems to have stabilized around κ = 0.67 (Carletta 1996; Di Eugenio and Glass 2004). There is little understanding, however, of exactly how and how well the value of κ reﬂects the quality of the data for benchmarking purposes. We develop a model of annotation generation that allows estimation of the level of noise in a specially constructed gold standard. A gold standard with a noise ﬁgure supports cautious benchmarking, ∗ Kellogg School of Management, Northwestern University, Evanston, IL, beata@northwestern.edu. ∗∗ Kellogg School of Management, Northwestern University, Evanston, IL, e-beigman@northwestern.edu. 
We present a Chinese word segmentation model learned from punctuation marks which are perfect word delimiters. The learning is aided by a manually segmented corpus. Our method is considerably more effective than previous methods in unknown word recognition. This is a step toward addressing one of the toughest problems in Chinese word segmentation. 1. Introduction Paragraphs are composed of sentences. Hence when a paragraph begins, a sentence must begin, and as a paragraph closes, some sentence must ﬁnish. This observation is the basis of the sentence boundary detection method proposed by Riley (1989). Similarly, sentences consist of words. As a sentence begins or ends there must be word boundaries. Inspired by this notion, we invent a method to learn a Chinese word segmentation model with punctuation marks in a large raw corpus. The learning is guided by a segmented corpus (Section 3.2). Section 4 demonstrates that our method improves notably the recognition of out-of-vocabulary (OOV) words with respect to approaches which use only annotated data (Xue 2003; Low, Ng, and Guo 2005). This work has practical implications in that the OOV problem has long been a big challenge for the research community. 2. Segmentation as Tagging We call the ﬁrst character of a Chinese word its left boundary L, and the last character its right boundary R. If we regard L and R as random events, then we can derive four events (or tags) from them: b = L ·R, m = L · R, s = L · R, e = L · R ∗ Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China. E-mail: eemath@gmail.com. ∗∗ Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China. E-mail: sms@mail.thu.edu.cn. Submission received: 16 July 2008; revised submission received: 26 March 2009; accepted for publication: 4 May 2009. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 4  Here R means not R, and thus tag b represents the left but not the right boundary of a word. The other tags can be interpreted similarly. This coding scheme was used by Borthwick (1999) and Xue (2003), where b, m, s, and e stand for begin, middle, only member, and end of a word, respectively. We reformulate them in terms of L and R to facilitate the presentation of our method. For a sentence S = c1c2 · · · cn and a sequence T = t1t2 · · · tn of b, m, s, e tags, we deﬁne  n  P (T|S) = Pr(ti|contexti)  (1)  i=1  where contexti is ci with up to four surrounding characters. The legal tag sequence (e.g., tag b followed by s is illegal) with highest P gives the segmentation result of S. Then from Equation (1) it is obvious that knowing the probability distribution of b, m, s, and e given context is adequate for carrying out Chinese word segmentation. The purpose of this article is to show that punctuation can play a major role in estimating this distribution. We use the maximum entropy approach to model the conditional probability Pr(y|x), which has the following parametric form according to Berger, Della Pietra, and Della Pietra (1996):  Pr(y|x)  =  
Carlo Strapparava† Fondazione Bruno Kessler – IRST We present a semi-supervised technique for word sense disambiguation that exploits external knowledge acquired in an unsupervised manner. In particular, we use a combination of basic kernel functions to independently estimate syntagmatic and domain similarity, building a set of word-expert classiﬁers that share a common domain model acquired from a large corpus of unlabeled data. The results show that the proposed approach achieves state-of-the-art performance on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although it uses a considerably smaller number of training examples than other methods. 1. Introduction A signiﬁcant challenge in many natural language processing tasks is to reduce the need for labeled training data while maintaining an acceptable performance. This is especially true for word sense disambiguation (WSD) because when moving from the somewhat artiﬁcial lexical-sample task to the more realistic all-words task it is practically impossible to collect a large number of training examples for each word sense. Thus, many supervised approaches, explicitly designed for the lexical-sample task, cannot be applied to the all-words task, even though they exhibit excellent performance. This has led to the somewhat paradoxical situation in which completely different methods have been developed for the two tasks, although they represent two sides of the same coin. To address this problem, in recent work we presented a semi-supervised approach based on kernel methods for WSD (Strapparava, Gliozzo, and Giuliano 2004; Gliozzo, Giuliano, and Strapparava 2005; Giuliano, Gliozzo, and Strapparava 2006). In particular, we explored the following research directions: (1) independently modeling domain and syntagmatic aspects of sense distinction to improve feature representativeness; and (2) exploiting external knowledge acquired from unlabeled data, with the purpose of drastically reducing the amount of labeled training data. The ﬁrst direction is based on the linguistic assumption that syntagmatic and domain (associative) relations are crucial for representing sense distinctions, but they are originated by different phenomena. Regarding the second direction, one can hope to obtain a more accurate prediction ∗ FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: giuliano@fbk.eu. ∗∗ FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: gliozzo@fbk.eu. † FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: strappa@fbk.eu. Submission received: 23 December 2006; revised submission received: 28 February 2008; accepted for publication: 17 April 2008. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 4  by taking into account unlabeled data relevant to the learning problem (Chapelle, Scho¨ lkopf, and Zien 2006). As a matter of fact, to test this hypothesis, most of the lexical sample tasks of Senseval-3 (Mihalcea and Edmonds 2004) were provided with a large amount of unlabeled training data, as well as the usual labeled training data. However, at that time, we were the only team to use the unlabeled data (Strapparava, Gliozzo, and Giuliano 2004). In this article, we review our technique that combines domain and syntagmatic information in order to deﬁne a complete kernel for WSD. The rest of the article is organized as follows. In Section 2, we provide a general introduction to the kernel methods, in which we give the basis for understanding our approach. Exploiting kernel methods, we can deﬁne and combine individual kernels representing information from different sources in a principled way. After this introductory section, in Section 3 we present the kernels that we developed for WSD. This includes a detailed description of the individual kernels and the way we deﬁne the composite ones. We present our experiments in Section 4. The results obtained on a range of lexical-sample tasks and on the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our approach achieves state-of-the-art performance. Finally, in Section 5, we offer conclusions and some directions for future research.  2. Kernel Methods  Kernel methods are a popular machine learning approach within the natural language processing community. They are theoretically well founded in statistical learning theory and have shown good empirical results in many applications (Vapnik 1999; Cristianini and Shawe-Taylor 2000; Scho¨ lkopf and Smola 2002; Shawe-Taylor and Cristianini 2004). The strategy adopted by kernel methods consists of splitting the learning problem into two parts. They ﬁrst embed the input data in a suitable feature space, and then use a linear algorithm to discover nonlinear patterns in the input space. Typically, the mapping is performed implicitly by a so-called kernel function. The kernel function is a similarity measure between the input data that depends exclusively on the speciﬁc data type and domain. A typical similarity function is the inner product between feature vectors. Characterizing the similarity of the inputs plays a crucial role in determining the success or failure of the learning algorithm, and it is one of the central questions in the ﬁeld of machine learning. Formally, the kernel is a function K : X × X → R that takes as input two data objects (e.g., vectors, texts, or parse trees) and outputs a real number characterizing their similarity, with the property that the function is symmetric and positive semi-deﬁnite. That is, for all xi, xj ∈ X satisﬁes  K(xi, xj) = φ(xi), φ(xj)  (1)  where φ is an (implicit) mapping from X to an (inner product) feature space F. Kernels are used inside learning algorithms such as support vector machines (SVM) or kernel perceptrons as the interface between the algorithm and the data. The kernel function is then the only domain speciﬁc element of the system, while the learning algorithm is a general purpose component. The idea behind the SVM (one of the best known kernel-based learning algorithms) is to map the set of training data into a high-dimensional feature space F via a mapping function φ : X → F, and construct a separating hyperplane with maximum margin (i.e.,  514  Giuliano, Gliozzo, and Strapparava  Kernel Methods for Minimally Supervised WSD  the minimum distance between the hyperplane and data points) in that space. The use  of an appropriate non-linear transformation φ of the input yields a nonlinear decision  boundary in the input space. Kernel functions make possible the use of feature spaces  with an exponential or even inﬁnite number of dimensions. Instead of performing the  explicit feature mapping φ, one can use a kernel function, which permits the (efﬁcient)  computation of inner products in high-dimensional feature spaces without explicitly  carrying out the mapping φ. This is called the kernel trick in the machine learning  literature (Boser, Guyon, and Vapnik 1992).  Finally, we point out the theoretical tools required to create new kernels, and com-  bine individual kernels to form composite ones. Of course, not every similarity function  is a valid kernel because, by deﬁnition, kernels should be equivalent to some inner  product in a feature space. The function K : X × X → R is a valid kernel provided that its kernel matrices1 are positive semi-deﬁnite2 for all training sets S = {x1, ..., xl}, the  so-called ﬁnitely positive semi-deﬁnite property. Note that deﬁning similarity measures  by means of kernels may be more intuitive than performing the explicit mapping in the  feature space. Furthermore, this formulation does not require the set X to be a vector  space: for example, we shall deﬁne kernels that take strings as input.  This result is not only useful because it opens new perspectives to deﬁne kernel  functions that only implicitly correspond to a feature mapping φ. Another consequence  is that it can be used to prove a set of rules for combining basic kernels to obtain compos-  ite ones. This will allow us to integrate heterogeneous sources of information in a simple  and effective way. We shall use the following properties of kernels to deﬁne our compos-  ite kernels. Let k1 and k2 be kernels over X × X; then the following functions are kernels:  r r  k(xi, k(xi,  xj xj  ) )  = =  k1(xi, xj) + k2(xi, xj) c · k1(xi, xj), c ∈ R+  r  k(xi, xj)  =  √ k1(xi,xj ) k1(xi,xi )×k1(xj,xj )  (normalization)  In summary, we can deﬁne a kernel function by following different strategies: (1) providing an explicit feature mapping φ : X → Rn; (2) deﬁning a similarity function that is symmetric and positive semi-deﬁnite; and (3) composing different valid kernels, using the closure properties of kernels. This forms the basis for the approach described in the following section.  3. Kernel Methods for WSD  Our approach to WSD consists of representing linguistic phenomena independently and then deﬁning a combination method to integrate them. As described in the previous section, the kernel function is the only task-speciﬁc component of the learning algorithm. Thus, to develop a WSD system, we only need to deﬁne appropriate kernel functions to represent the domain and syntagmatic aspects of sense distinction and, second, exploit the properties of kernel functions to deﬁne a composite kernel to combine and extend the individual kernels. The resulting WSD system consists of two families of kernels: the domain and the syntagmatic kernels. The former family, described in Section 3.1, models the domain  
Daniel Gildea† University of Rochester Kevin Knight‡ USC/Information Science Institute Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages. We develop a theory of binarization for synchronous context-free grammars and present a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set signiﬁcantly improves the speed and accuracy of a state-of-the-art syntaxbased machine translation system. We also discuss the more general, and computationally more difﬁcult, problem of ﬁnding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem. 1. Introduction Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case ∗ Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: lhuang@isi.edu, liang.huang.sh@gmail.com. ∗∗ 1600 Amphitheatre Parkway, Mountain View, CA 94303. E-mail: haozhang@google.com. † Computer Science Dept., University of Rochester, Rochester NY 14627. E-mail: gildea@cs.rochester.edu. ‡ Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Submission received: 14 March 2007; revised submission received: 8 January 2009; accepted for publication: 25 March 2009. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 4  for all synchronous rules; we investigate algorithms for non-binarizable rules as well. In particular: r We develop a technique called synchronous binarization and devise a linear-time binarization algorithm such that the resulting rule set allows efﬁcient algorithms for both synchronous parsing and decoding with integrated n-gram language models. r We examine the effect of this binarization method on end-to-end translation quality on a large-scale Chinese-to-English syntax-based system, compared to a more typical baseline method, and a state-of-the-art phrase-based system. r We examine the ratio of binarizability in large, empirically derived rule sets, and show that the vast majority is binarizable. However, we also provide, for the ﬁrst time, real examples of non-binarizable cases veriﬁed by native speakers. r In the ﬁnal, theoretical, sections of this article, we investigate the general problem of ﬁnding the most efﬁcient synchronous parsing or decoding strategy for arbitrary synchronous context-free grammar (SCFG) rules, including non-binarizable cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difﬁculty of binarization for efﬁcient synchronous parsing. One way around this difﬁculty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. We explain how synchronous rule binarization interacts with n-gram language models and affects decoding for machine translation in Section 2. We deﬁne binarization formally in Section 3, and present an efﬁcient algorithm for the problem in Section 4. Experiments described in Section 51 show that binarization improves machine translation speed and quality. Some rules cannot be binarized, and we present a decoding strategy for these rules in Section 6. Section 7 gives a solution to the general theoretical problem of ﬁnding optimal decoding and synchronous parsing strategies for arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our Chinese–English data. These ﬁnal two sections are of primarily theoretical interest, as nonbinarizable rules have not been shown to beneﬁt real-world machine translation systems. However, the algorithms presented may become relevant as machine translation systems improve.  
This article presents an investigation of corpus-based methods for the automation of help-desk e-mail responses. Speciﬁcally, we investigate this problem along two operational dimensions: (1) information-gathering technique, and (2) granularity of the information. We consider two information-gathering techniques (retrieval and prediction) applied to information represented at two levels of granularity (document-level and sentence-level). Document-level methods correspond to the reuse of an existing response e-mail to address new requests. Sentence-level methods correspond to applying extractive multi-document summarization techniques to collate units of information from more than one e-mail. Evaluation of the performance of the different methods shows that in combination they are able to successfully automate the generation of responses for a substantial portion of e-mail requests in our corpus. We also investigate a meta-selection process that learns to choose one method to address a new inquiry e-mail, thus providing a uniﬁed response automation solution. 1. Introduction E-mail inquiries sent to help desks often “revolve around a small set of common questions and issues.”1 This means that help-desk operators spend most of their time dealing with problems that have been previously addressed. Further, a signiﬁcant proportion of help-desk responses contain a low level of technical content, addressing, for example, inquiries sent to the wrong group, or requests containing insufﬁcient detail about the customer’s problem. Organizations and clients would beneﬁt if an automated process was employed to deal with the easier problems, and the efforts of human operators were focused on difﬁcult, atypical problems. However, even the automation of responses to the “easy” problems is a difﬁcult task. Although such inquiries revolve around a relatively small set of issues, speciﬁc ∗ Faculty of Information Technology, Monash University, Wellington Road, Clayton, Victoria 3800, Australia. Currently employed at Paciﬁc Brands Services Group, Building 10, 658 Church St, Richmond, Victoria 3121, Australia. E-mail: yuvalmarom@gmail.com. ∗∗ Faculty of Information Technology, Monash University, Wellington Road, Clayton, Victoria 3800, Australia. E-mail: Ingrid.Zukerman@infotech.monash.edu.au. 
Attending recent computational linguistics conferences, it is hard to ignore the phenomenal amount of research devoted to statistical machine translation (SMT). Driven by the wide availability of open-source translation systems, corpora, and evaluation tools, a research area that was once the preserve of large research groups has become accessible to those of more modest resources. Although the current state-of-the-art SMT systems have matured into robust commercial systems, capable of providing reasonable quality translations for a variety of domains, they remain limited by naive modeling assumptions and a heavy reliance on heuristics. These limitations have led researchers to ask the question of whether the adoption of techniques from the machine learning literature could allow more complex translations to be modeled effectively. As such, this book, focused on the application of machine learning to SMT, is particularly timely in capturing the current interest of the machine translation community. Learning Machine Translation is presented in two parts. The ﬁrst, titled “Enabling Technologies,” focuses on research peripheral to machine translation. Topics covered include the acquisition of parallel corpora, cross-language named-entity processing, and language modeling. The second part covers core machine translation system building, presenting a number of approaches applying discriminative machine learning techniques within a SMT decoder. Much of the content of the book arose from the Machine Learning for Multilingual Access Workshop held at the Neural Information Processing conference in 2006. As SMT is not a frequent topic at that conference, the bridging of research from the mainstream machine learning community with research on MT is particularly promising. A ﬁne example of this cross-over is Chapter 9, “Kernel-Based Machine Translation,” in which a novel approach to estimating translation models is presented. However, this promise is not entirely fulﬁlled, as some contributions either fail to make use of machine learning or are somewhat obscure, unlikely to impact on the mainstream SMT community. 1. Chapter 1: A Statistical Machine Translation Primer In the ﬁrst chapter, “A Statistical Machine Translation Primer,” the editors seek to both introduce the concept of the book as well as give a brief tutorial on current SMT techniques. In these aims they succeed, describing the elements of current approaches to SMT succinctly. Although those foreign to the ﬁeld would not come away from reading this chapter able to implement a translation model, pointers to research publications  
The last two decades were marked by a complete paradigm shift in computational linguistics. Frustrated by the inability of applications based on explicit linguistic knowledge to scale up to real-world needs, and, perhaps more deeply, frustrated with the dominating theories in formal linguistics, we looked instead to corpora that reﬂect language use as our sources of (implicit) knowledge. With the shift in methodology came a subtle change in the goals of our entire enterprise. Two decades ago, a computational linguist could be interested in developing NLP applications; or in formalizing (and reasoning about) linguistic processes. These days, it is the former only. A superﬁcial look at the papers presented in our main conferences reveals that the vast majority of them are engineering papers, discussing engineering solutions to practical problems. Virtually none addresses fundamental issues in linguistics. There’s nothing wrong with engineering work, of course. Every school of technology has departments of engineering in areas as diverse as Chemical Engineering, Mechanical Engineering, Aeronautical Engineering, or Biomedical Engineering; there’s no reason why there shouldn’t also be a discipline of Natural Language Engineering. But in the more established disciplines, engineering departments conduct research that is informed by some well-deﬁned branch of science. Chemical engineers study chemistry; electrical engineers study physics; aeronautical engineers study dynamics; and biomedical engineers study biology, physiology, medical sciences, and so on. The success of engineering is also in part due to the choice of the “right” mathematics. The theoretical development of several scientiﬁc areas, notably physics, went alongside mathematical developments. Physics could not have accounted for natural phenomena without such mathematical infrastructure. For example, the development of (partial) differential equations went hand in hand with some of the greatest achievement in physics, and this branch of mathematics later turned out to be applicable also to chemistry, electrical engineering, and economics, among many other scientiﬁc ﬁelds. ∗ Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il. © 2009 Association for Computational Linguistics  
Lane Schwartz∗ University of Minnesota This article describes a framework for incorporating referential semantic information from a world model or ontology directly into a probabilistic language model of the sort commonly used in speech recognition, where it can be probabilistically weighted together with phonological and syntactic factors as an integral part of the decoding process. Introducing world model referents into the decoding search greatly increases the search space, but by using a single integrated phonological, syntactic, and referential semantic language model, the decoder is able to incrementally prune this search based on probabilities associated with these combined contexts. The result is a single uniﬁed referential semantic probability model which brings several kinds of context to bear in speech decoding, and performs accurate recognition in real time on large domains in the absence of example in-domain training sentences. 1. Introduction The capacity to rapidly connect language to referential meaning is an essential aspect of communication between humans. Eye-tracking studies show that humans listening to spoken directives are able to actively attend to the entities that the words in these directives might refer to, even while the words are still being pronounced (Tanenhaus et al. 1995; Brown-Schmidt, Campana, and Tanenhaus 2002). This timely access to referential information about input utterances may allow listeners to adjust their preferences among likely interpretations of noisy or ambiguous utterances to favor those that make sense in the current environment or discourse context, before any lower-level disambiguation decisions have been made. This same capability in a spoken language interface system could allow reliable human–machine interaction in the idiosyncratic language of day-to-day life, populated with proper names of co-workers, objects, and events not found in broad training corpora. When domain-speciﬁc training corpora are ∗ Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455. E-mail: schuler@cs.umn.edu; swu@cs.umn.edu; lane@cs.umn.edu. Submission received: 25 April 2007; revised submission received: 4 March 2008; accepted for publication: 2 June 2008. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 3  not available, a referential semantic interface could still exploit its model of the world: the data to which it is an interface, and patterns characterizing these data. This article describes a framework for incorporating referential semantic information from a world model or ontology directly into a statistical language model of the sort commonly used in speech recognition, where it can be probabilistically weighted together with phonological and syntactic factors as an integral part of the decoding process. Introducing world model referents into the decoding search greatly increases the search space, but by using a single integrated phonological, syntactic, and referential semantic language model, the decoder is able to incrementally prune this search based on probabilities associated with these combined contexts. Semantic interpretation is deﬁned dynamically in this framework, in terms of transitions over time from less constrained referents to more constrained referents. Because it is deﬁned dynamically, interpretation in this framework can incorporate dependencies on referential context—for example, constraining interpretations to a presumed set of entities, or a presumed setting—which may be ﬁxed prior to recognition, or dynamically hypothesized earlier in the recognition process. This contrasts with other recent systems which interpret constituents only given ﬁxed inter-utterance contexts or explicit syntactic arguments (Schuler 2001; DeVault and Stone 2003; Gorniak and Roy 2004; Aist et al. 2007). Moreover, because it is deﬁned dynamically, in terms of transitions, this context-dependent interpretation framework can be directly integrated into a Viterbi decoding search, like ordinary state transitions in a Hidden Markov Model. The result is a single uniﬁed referential semantic probability model which brings several kinds of referential semantic context to bear in speech decoding, and performs accurate recognition in real time on large domains in the absence of example domain-speciﬁc training sentences. The remainder of this article is organized as follows: Section 2 will describe related approaches to interleaving semantic interpretation with speech recognition. Section 3 will provide deﬁnitions for world models used in semantic interpretation, and language models used in speech decoding, which will form the basis of a referential semantic language model, deﬁned in Section 4. Then Section 5 will describe an evaluation of this model in a sample spoken language interface application.  2. Related Work Early approaches to incremental interpretation (Mellish 1985; Haddock 1989) apply semantic constraints associated with each word in a sentence to progressively winnow the set of individuals that could serve as referents in that sentence. These incrementally constrained referents are then used to guide the syntactic analysis of the sentence, dispreferring analyses with empty interpretations in the current environment or discourse context. Similar approaches were applied to broad-coverage text processing, querying a large commonsense knowledge base as a world model (Martin and Riesbeck 1986). But this winnowing is done deterministically, invoking default assumptions and potentially exponential backtracking when default assumptions fail. The idea of basing analysis decisions on constrained sets of referent individuals was later extended to pursue multiple interpretations at once by exploiting polynomial structure-sharing in a dynamic programming parser (Schuler 2001; DeVault and Stone 2003; Gorniak and Roy 2004; Aist et al. 2007). The resulting shared interpretation is similar to underspeciﬁed semantic representations (Bos 1996), except that the representation mainly preserves syntactic ambiguity rather than semantic (e.g., quanti- 314  Schuler, Wu, and Schwartz  A Framework for Fast Incremental Interpretation  ﬁer scoping) ambiguity, and the size complexity of the parser chart representation is polynomially bounded. This approach was further extended to support hypothetical referents (DeVault and Stone 2003), domains with continuous relations (Gorniak and Roy 2004), and updates to the shared parser chart by components handling other levels of linguistic analysis in parallel, during real-time recognition (Aist et al. 2007). The advantage of this use of the parser chart is that it allows a straightforward mapping between syntax and semantics using familiar compositional semantic representations. But the standard dynamic programming algorithm for parsing derives its complexity bounds from the fact that each recognized constituent can be analyzed independently of every other constituent. These independence assumptions must be relaxed if dynamic context dependencies are to be applied across sibling constituents (e.g., in the package data directory, open . . . , where the ﬁles to be opened should be restricted to the contents of the package data directory). More importantly, from an engineering perspective, the dynamic programming algorithm for parsing runs in cubic time, not linear, which means this interpretation framework cannot be directly applied to continuous audio streams. Interface systems therefore typically perform utterance or sentence segmentation as a stand-alone pre-process, without integrating syntactic or referential semantic dependencies into this decision. Finally, some speech recognition systems employ inter-utterance context-dependent language models that are pre-compiled into word n-grams for particular discourse or environment states, and swapped out between utterances (Young et al. 1989; Lemon and Gruenstein 2004; Seneff et al. 2004). But in some cases accurate interpretation will require spoken language interfaces to exploit context continuously during utterance recognition, not just between utterances. For example, the probability distribution over the next word in the utterance go to the package data directory and get the . . . (or in the package data directory get the . . . ) will depend crucially on the linguistic and environment context leading up to this point: the meaning of package data directory in the ﬁrst part of this directive, as well as the objects that will be available once this part of the directive has been carried out. Moreover, in rich environments pre-compilation to word n-grams can be expensive, since all referents in the world model must be considered to build accurate n-grams. This will not be practical if environments change frequently.  3. Background In contrast to the approaches described in Section 2, this article proposes an incremental interpretation framework which is entirely contained within a single-pass probabilistic decoding search. Essentially, this approach directly integrates model theoretic semantics, summarized in Section 3.1, with conventional probabilistic time-series models used in speech recognition, summarized in Section 3.2. 3.1 Referential Semantics Semantic interpretation requires a framework within which a speaker’s intended meanings can be formalized. Sections 3.1.1 and 3.1.2 describe a model theoretic approach to semantic interpretation that will later be extended in Section 4.1. The referential states deﬁned here will then be incorporated into a representation of nested syntactic constituents in a hierarchic time-series model in Section 4.2. Some of the notation introduced here is summarized later in Table 1 (Section 4). 315  Computational Linguistics  Volume 35, Number 3  Figure 1 A subsumption lattice (laid on its side) over the power set of a domain containing three individuals: ι1, ι2, and ι3. Subsumption relations are represented as gray arrows from supersets (or super-concepts) to subsets (or sub-concepts). 3.1.1 Model Theory. The language model described in this article deﬁnes semantic referents in terms of a world model M. In model theory (Tarski 1933; Church 1940), a world model is deﬁned as a tuple M = E, · containing a domain of individuals E = {ι1, ι2, . . . } and an interpretation function · to interpret expressions in terms of those individuals. This interpretation function accepts expressions φ of various types: logical statements, of simple type T (for example, the demo ﬁle is writable) which may be true or false; references to individuals, of simple type E (for example, the demo ﬁle) which may refer to any individual in the world model; or functors of complex type α, β , which take an argument of type α and produce output of type β. Functor expressions φ of type α, β can be applied to other expressions ψ of type α as arguments to yield expressions φ(ψ) of type β (for example, writable may take the demo ﬁle as an argument and return true). By nesting functors, complex expressions can be deﬁned, denoting sets or properties of individuals: E, T (for example, writable), relations over individual pairs: E, E, T (for example, contains), or ﬁrst-order functors over sets: E, T , E, T (for example, a comparative adjective like larger). 3.1.2 Ontological Promiscuity. First-order or higher models (in which functors can take sets as arguments) can be mapped to equivalent zero-order models (with functors deﬁned only on entities). This is generally motivated by a desire to allow sets of individuals to be described in much the same way as individuals themselves (Hobbs 1985). Entities in a zero-order model M can be deﬁned from individuals in a higherorder model M∗ by mapping or reifying each set S = {ι1, ι2, . . . } in P (EM∗ ) (or each set of sets in P (P (EM∗ )), etc.) as an entity eS in a new domain EM.1 Relations l interpreted as zero-order functors in M can be deﬁned directly from relations l∗ interpreted as higher-order functors (over sets) in M∗ by mapping each instance of S1, S2 in l∗ M∗ : P (EM∗ )×P (EM∗ ) to a corresponding instance of eS1 , eS2 in l M : EM ×EM. Set subsumption in M∗ can then be deﬁned on entities made from reiﬁed sets in M, similar to ‘ISA’ relations over concepts in knowledge representation systems (Brachman and Schmolze 1985). These subset or subsumption relations can be represented in a subsumption lattice, as shown in Figure 1, with supersets to the left connecting to subsets to the right. This representation will be used in Section 4 to deﬁne weighted transitions over ﬁrst-order referents in a statistical time-series model of interpretation. 
Multimodal grammars provide an effective mechanism for quickly creating integration and understanding capabilities for interactive systems supporting simultaneous use of multiple input modalities. However, like other approaches based on hand-crafted grammars, multimodal grammars can be brittle with respect to unexpected, erroneous, or disﬂuent input. In this article, we show how the ﬁnite-state approach to multimodal language processing can be extended to support multimodal applications combining speech with complex freehand pen input, and evaluate the approach in the context of a multimodal conversational system (MATCH). We explore a range of different techniques for improving the robustness of multimodal integration and understanding. These include techniques for building effective language models for speech recognition when little or no multimodal training data is available, and techniques for robust multimodal understanding that draw on classiﬁcation, machine translation, and sequence edit methods. We also explore the use of edit-based methods to overcome mismatches between the gesture stream and the speech stream. 1. Introduction The ongoing convergence of the Web with telephony, driven by technologies such as voice over IP, broadband Internet access, high-speed mobile data networks, and handheld computers and smartphones, enables widespread deployment of multimodal interfaces which combine graphical user interfaces with natural modalities such as speech and pen. The critical advantage of multimodal interfaces is that they allow user input and system output to be expressed in the mode or modes to which they are best suited, given the task at hand, user preferences, and the physical and social environment of the interaction (Oviatt 1997; Cassell 2001; Andre´ 2002; Wahlster 2002). There is also an increasing body of empirical evidence (Hauptmann 1989; Nishimoto et al. 1995; Cohen et al. 1998a; Oviatt 1999) showing user preference and task performance advantages of multimodal interfaces. In order to support effective multimodal interfaces, natural language processing techniques, which have typically operated over linear sequences of speech or text, ∗ 180 Park Avenue, Florham Park, NJ 07932. E-mail: srini@research.att.com. ∗∗ 180 Park Avenue, Florham Park, NJ 07932. E-mail: johnston@research.att.com. Submission received: 26 May 2006; revised submission received: 6 May 2008; accepted for publication: 11 July 2008. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 3  need to be extended in order to support integration and understanding of multimodal language distributed over multiple different input modes (Johnston et al. 1997; Johnston 1998b). Multimodal grammars provide an expressive mechanism for quickly creating language processing capabilities for multimodal interfaces supporting input modes such as speech and gesture (Johnston and Bangalore 2000). They support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. Johnston and Bangalore (2005) show that such grammars can be compiled into ﬁnite-state transducers, enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities. In this article, we show how multimodal grammars and their ﬁnite-state implementation can be extended to support more complex multimodal applications. These applications combine speech with complex pen input including both freehand gestures and handwritten input. More general mechanisms are introduced for representation of gestures and abstraction over speciﬁc content in the gesture stream along with a new technique for aggregation of gestures. We evaluate the approach in the context of the MATCH multimodal conversational system (Johnston et al. 2002b), an interactive city guide. In Section 2, we present the MATCH application, the architecture of the system, and our experimental method for collection and annotation of multimodal data. In Section 3, we evaluate the baseline approach on the collected data. The performance of this baseline approach is limited by the use of hand-crafted models for speech recognition and multimodal understanding. Like other approaches based on hand-crafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous, and disﬂuent input. This is particularly problematic for multimodal interfaces if they are to be used in noisy mobile environments. To overcome this limitation we explore a broad range of different techniques for improving the robustness of both speech recognition and multimodal understanding components. For automatic speech recognition (ASR), a corpus-driven stochastic language model (SLM) with smoothing can be built in order to overcome the brittleness of a grammarbased language model. However, for multimodal applications there is often very little training data available and collection and annotation of realistic data can be very expensive. In Section 5, we examine and evaluate various different techniques for rapid prototyping of the language model for the speech recognizer, including transformation of out-of-domain data, grammar sampling, adaptation from wide-coverage grammars, and speech recognition models built on conversational corpora (Switchboard). Although some of the techniques presented have been reported in the literature, we are not aware of work comparing the effectiveness of these techniques on the same domain and using the same data sets. Furthermore, the techniques are general enough that they can be applied to bootstrap robust gesture recognition models as well. The presentation here focuses on speech recognition models, partly due to the greater impact of speech recognition performance compared to gesture recognition performance on the multimodal application described here. However, in Section 7 we explore the use of robustness techniques on gesture input. Although the use of an SLM enables recognition of out-of-grammar utterances, resulting in improved speech recognition accuracy, this may not help overall system performance unless the multimodal understanding component itself is made robust to unexpected inputs. In Section 6, we describe and evaluate several different techniques for making multimodal understanding more robust. Given the success of discriminative classiﬁcation models in related applications such as natural language call 346  Bangalore and Johnston  Robust Understanding in Multimodal Interfaces  routing (Haffner, Tur, and Wright 2003; Gupta et al. 2004) and semantic role labeling (Punyakanok, Roth, and Yih 2005), we ﬁrst pursue a purely data-driven approach where the predicate of a multimodal command and its arguments are determined by classiﬁers trained on an annotated corpus of multimodal data. However, given the limited amount of data available, this approach does not provide an improvement over the grammar-based approach. We next pursue an approach combining grammar and data where robust understanding is viewed as a statistical machine translation problem where out-of-grammar or misrecognized language must be translated to the closest language the system can understand. This approach provides modest improvement over the grammar-based approach. Finally we explore an edit-distance approach which combines grammar-based understanding with knowledge derived from the underlying application database. Essentially, if a string cannot be parsed, we attempt to identify the in-grammar string that it is most similar to, just as in the translation approach. This is achieved by using a ﬁnite-state edit transducer to compose the output of the ASR with the grammar-based multimodal alignment and understanding models. We have presented these techniques as methods for improving the robustness of the multimodal understanding by processing the speech recognition output. Given the higher chance of error in speech recognition compared to gesture recognition, we focus on processing the speech recognition output to achieve robust multimodal understanding. However, these techniques are also equally applicable to gesture recognition output. In Section 7, we explore the use of edit techniques on gesture input. Section 8 concludes and discusses the implications of these results. 2. The MATCH Application Urban environments present a complex and constantly changing body of information regarding restaurants, cinema and theater schedules, transportation topology, and timetables. This information is most valuable if it can be delivered effectively while mobile, since users’ needs change rapidly and the information itself is dynamic (e.g., train times change and shows get cancelled). MATCH (Multimodal Access To City Help) is a working city guide and navigation system that enables mobile users to access restaurant and subway information for urban centers such as New York City and Washington, DC (Johnston et al. 2002a, 2002b). MATCH runs stand-alone on a tablet PC (Figure 1) or in client-server mode across a wireless network. There is also a kiosk version of the system (MATCHkiosk) (Johnston and Bangalore 2004) which incorporates a life-like talking head. In this article, we focus on the mobile version of MATCH, in which the user interacts with a graphical interface displaying restaurant listings and a dynamic map showing locations and street information. The inputs can be speech, drawings on the display with a stylus, or synchronous multimodal combinations of the two modes. The user can ask for reviews, cuisine, phone number, address, or other information about restaurants and for subway directions to restaurants and locations. The system responds with graphical callouts on the display, synchronized with synthetic speech output. For example, a user can request to see restaurants using the spoken command show cheap italian restaurants in chelsea. The system will then zoom to the appropriate map location and show the locations of restaurants on the map. Alternatively, the user could give the same command multimodally by circling an area on the map and saying show cheap italian restaurants in this neighborhood. If the immediate environment is too noisy or public, the same command can be given completely using a pen stylus as in Figure 2, by circling an area and writing cheap and italian. 347  Computational Linguistics  Volume 35, Number 3  Figure 1 MATCH on tablet. Similarly, if the user says phone numbers for these two restaurants and circles two restaurants as in Figure 3(a) [A], the system will draw a callout with the restaurant name and number and say, for example, Time Cafe can be reached at 212-533-7000, for each restaurant in turn (Figure 3(a) [B]). If the immediate environment is too noisy or public, the same command can be given completely in pen by circling the restaurants and writing phone (Figure 3(b)). The system also provides subway directions. For example, if the user says How do I get to this place? and circles one of the restaurants displayed on the map the system will ask Where do you want to go from?. The user can then respond with speech (for example, 25th Street and 3rd Avenue), with pen by writing (for example, 25th St & 3rd Ave), or multimodally (for example, from here, with a circle gesture indicating the location). The system then calculates the optimal subway route and generates a multimodal presentation coordinating graphical presentation of each stage of the route with spoken instructions indicating the series of actions the user needs to take (Figure 4). Map-based systems have been a common application area for exploring multimodal interaction techniques. One of the reasons for this is the effectiveness and naturalness of combining graphical input to indicate spatial locations with spoken input to specify commands. See Oviatt (1997) for a detailed experimental investigation illustrating the Figure 2 Unimodal pen command. 348  Bangalore and Johnston  Robust Understanding in Multimodal Interfaces  Figure 3 (a) Two area gestures. (b) Phone command in pen. Figure 4 Multimodal subway route. advantages of multimodal input for map-based tasks. Previous map-based multimodal prototypes can be broken down into two main task domains: map annotation tasks and information search tasks. Systems such as QuickSet (Cohen et al. 1998b) focus on the use of speech and pen input in order to annotate the location of features on a map. Other systems use speech and pen input to enable users to search and browse for information through direct interaction with a map display. In the ADAPT system (Gustafson et al. 2000), users browse for apartments using combinations of speaking and pointing. In the Multimodal Maps system (Cheyer and Julia 1998), users perform travel planning tasks such as searching for hotels and points of interest. MATCH is an information search application providing local search capabilities combined with transportation directions. As such it is most similar to the Multimodal Maps application, though it provides more powerful and robust language processing and multimodal integration capabilities, while the language processing in the Multimodal Maps application is limited to simple Verb Object Argument constructions (Cheyer and Julia 1998). In the next section we explain the underlying architecture and the series of components which enable the MATCH user interface. 2.1 MATCH Multimodal Architecture The underlying architecture that supports MATCH consists of a series of re-usable components which communicate over IP through a facilitator (MCUBE) (Figure 5). Figure 6 shows the ﬂow of information among components in the system. In earlier 349  Computational Linguistics  Volume 35, Number 3  Figure 5 Multimodal architecture. versions of the system, communication was over socket connections. In later versions of the system communication between components uses HTTP. Users interact with the system through a Multimodal User Interface client (MUI) which runs in a Web browser. Their speech is processed by the WATSON speech recognition server (Gofﬁn et al. 2005) resulting in a weighted lattice of word strings. When the user draws on the map their ink is captured and any objects potentially selected, such as currently displayed restaurants, are identiﬁed. The electronic ink is broken into a lattice of strokes and sent to both gesture and handwriting recognition components which  Figure 6 Multimodal architecture ﬂowchart. 350  Bangalore and Johnston  Robust Understanding in Multimodal Interfaces  enrich this stroke lattice with possible classiﬁcations of strokes and stroke combinations. The gesture recognizer uses a variant of the template matching approach described by Rubine (1991). This recognizes symbolic gestures such as lines, areas, points, arrows, and so on. The stroke lattice is then converted into an ink lattice which represents all of the possible interpretations of the user’s ink as either symbolic gestures or handwritten words. The word lattice and ink lattice are integrated and assigned a combined meaning representation by the multimodal integration and understanding component (Johnston and Bangalore 2000; Johnston et al. 2002b). Because we implement this component using ﬁnite-state transducers, we refer to this component as the Multimodal Finite State Transducer (MMFST). The approach used in the MMFST component for integrating and interpreting multimodal inputs (Johnston et al. 2002a, 2002b) is an extension of the ﬁnite-state approach previously proposed (Bangalore and Johnston 2000; Johnston and Bangalore 2000, 2005). (See Section 3 for details.) This provides as output a lattice encoding all of the potential meaning representations assigned to the user’s input. The meaning is represented in XML, facilitating parsing and logging by other system components. MMFST can receive inputs and generate outputs using multiple communication protocols, including the W3C EMMA standard for representation of multimodal inputs (Johnston et al. 2007). The meaning lattice is ﬂattened to an n-best list and passed to a multimodal dialog manager (MDM) (Johnston et al. 2002b), which reranks the possible meanings in accordance with the current dialogue state. If additional information or conﬁrmation is required, the MDM enters into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating a coordinated sequence of graphical actions and TTS prompts. This score is passed back to the MUI. The MUI then coordinates presentation of graphical content with synthetic speech output using the AT&T Natural Voices TTS engine (Beutnagel et al. 1999). The subway route constraint solver (SUBWAY) is a backend server built for the prototype which identiﬁes the best route between any two points in the city. In the given example where the user says phone for these two restaurants while circling two restaurants (Figure 3(a) [A]), assume the speech recognizer returns the lattice in Figure 7 (Speech). The gesture recognition component also returns a lattice (Figure 7, Gesture) indicating that the user’s ink is either a selection of two restaurants or a geographical area. The multimodal integration and understanding component (MMFST) combines these two input lattices into a lattice representing their combined meaning (Figure 7, Meaning). This is passed to the multimodal dialog manager (MDM) and from there to the MUI where it results in the display in Figure 3(a) [B] and coordinated TTS output. The multimodal integration and understanding component utilizes a declarative multimodal grammar which captures both the structure and the interpretation of multimodal and unimodal commands. This formalism and its ﬁnite-state implementation for the MATCH system are explained in detail in Section 3. This multimodal grammar is in part derived automatically by reference to an underlying ontology of the different kinds of objects in the application. Speciﬁc categories in the ontology, such as located entity, are associated with templates and macros that are used to automatically generate the necessary grammar rules for the multimodal grammar and to populate classes in a class-based language model (Section 5). For example, in order to add support for a new kind of entity, for example, bars, a category bar is added to the ontology as a subtype of located entity along with speciﬁcation of the head nouns used for this new category, the attributes that apply to it, the symbol to use for it in the gesture representation, and a reference to the appropriate table to ﬁnd bars 351  Computational Linguistics  Volume 35, Number 3  Figure 7 Multimodal example. in the underlying application database. The appropriate multimodal grammar rules are then derived automatically as part of the grammar compilation process. Because the new entity type bar is assigned the ontology category located entity, the grammar will automatically support deictic reference to bars with expressions such as this place in addition to the more speciﬁc this bar. In the next section, we explain the data collection procedure we employed in order to evaluate the system and provide a test set for experimenting with different techniques for multimodal integration and understanding. 2.2 Multimodal Data Collection A corpus of multimodal data was collected in a laboratory setting from a genderbalanced set of 16 ﬁrst-time novice users. The subjects were AT&T personnel with no prior knowledge of the system and no experience building spoken or multimodal systems. A total of 833 user interactions (218 multimodal/491 speech-only/124 penonly) resulting from six sample task scenarios involving ﬁnding restaurants of various types and getting their names, phones, addresses, or reviews, and getting subway directions between locations were collected and annotated. Figure 8 shows the experimental set-up. Subjects interacted with the system in a soundproof room separated from the experimenter by one-way glass. Two video feeds were recorded, one from a scan converter connected to the system, the other from a camera located in the subject room, which captured a side-on view of the subject and the display. The system ran on a Fujitsu tablet computer networked to a desktop PC logging server located next to the experimenter. The subject’s audio inputs were captured using both a close-talking headset microphone and a desktop microphone (which captured both user input and system audio). As the user interacted with the system a multimodal log in XML format was captured on the logging server (Ehlen, Johnston, and Vasireddy 2002). The log contains a detailed record of the subject’s speech and pen inputs and the system’s internal processing steps and responses, with links to the relevant audio ﬁles and speech recognition lattices. 352  Bangalore and Johnston  Robust Understanding in Multimodal Interfaces  Figure 8 Experimenter and subject set-up. The experimenter started out each subject with a brief tutorial on the system, showing them the pen and how to click on the display in order to turn on the microphone. The tutorial was intentionally vague and broad in scope so the subjects might overestimate the system’s capabilities and approach problems in new ways. The experimenter then left the subject to complete, unassisted, a series of six sample task scenarios of varying complexity. These involved ﬁnding restaurants of various types and getting their names, phones, addresses, or reviews, and getting subway directions between locations. The task scenarios were presented in a GUI on the tablet next to the map display. In our pilot testing, we presented users with whole paragraphs describing scenarios. We found that users would often just rephrase the wording given in the paragraph, thereby limiting the utility of the data collection. Instead, in this data collection we presented what the user had to ﬁnd as a table (Table 1). This approach elicited a broader range of inputs from users. After completing the scenarios the user then completed an online questionnaire on the tablet regarding their experience with the system. This consisted of a series of Likert scale questions to measure user satisfaction (Walker, Passonneau, and Boland 2001). After the questionnaire the experimenter came into the experiment room and conducted an informal qualitative post-experiment feedback interview. The next phase of the data collection process was to transcribe and annotate the users’ input. Transcription is more complex for multimodal systems than for speechonly systems because the annotator needs not just to hear what the user said but also to see what they did. The browser-based construction of the multimodal user interface enabled us to rapidly build a custom version of the system which serves as an online multimodal annotation tool (Figure 9). This tool extends the approach described in Ehlen, Johnston, and Vasireddy (2002) with a graphical interface for construction of Table 1 Example scenario. Use MATCH to ﬁnd the name, address, and phone number of a restaurant matching the following criteria: Food Type Location Vegetarian Union Square 353  Computational Linguistics  Volume 35, Number 3  Figure 9 Multimodal log annotation tool. gesture annotations and a tool for automatically deriving the meaning annotation for out-of-grammar examples. This tool allows the annotator to dynamically replay the users’ inputs and system responses on the interactive map system itself, turn by turn, and add annotations to a multimodal log ﬁle, encoded in XML. The annotation utilizes the map component of the system (Figure 9(1)). It provides coordinated playback of the subject’s audio with their electronic ink, enabling the user to rapidly annotate multimodal data without having to replay video of the interaction. The user interface of the multimodal log viewer provides ﬁelds for the annotator to transcribe the speech input, the gesture input, and the meaning. A series of buttons and widgets are provided to enable the annotator to rapidly and accurately transcribe the user’s gesture and the appropriate meaning representation without having to remember the speciﬁcs of the gesture and meaning representations (Figure 9(2)). After transcribing the speech and gesture, the annotator hits a button to conﬁrm these, and they are recorded in the log and copied down to a second ﬁeld used for annotating the meaning of the input (Figure 9(3)). It would be both time consuming and error-prone to have the annotator code in the meaning representation for each input by hand. Instead the multimodal understanding system is integrated into the multimodal annotation tool directly. The interface allows the annotator to adjust the speech and gesture inputs and send them through the multimodal understander until they get the meaning they are looking for (Figure 9(4)). When the multimodal understander returns multiple possibilities an n-best list is presented and the annotator hits the button next to the appropriate interpretation in order to select it as the annotated meaning. We found this to be a very effective method of annotating meaning, although it does require the annotator to have some knowledge of what inputs are acceptable to the system. In addition to annotating the speech, gesture, and meaning, annotators also checked off a series of ﬂags indicating various properties of the exchange, such as whether the input was partial, whether there was a user error, and so on. The result of this effort was a 354  Bangalore and Johnston  Robust Understanding in Multimodal Interfaces  corpus of 833 user interactions all fully annotated with speech, gesture, and meaning transcriptions. 3. Multimodal Grammars and Finite-State Multimodal Language Processing One of the most critical technical challenges in the development of effective multimodal systems is that of enabling multimodal language understanding; that is, determining the user’s intent by integrating and understanding inputs distributed over multiple modes. In early work on this problem (Neal and Shapiro 1991; Cohen 1991, 1992; Brison and Vigouroux 1993; Koons, Sparrell, and Thorisson 1993; Wauchope 1994), multimodal understanding was primarily speech-driven,1 treating gesture as a secondary dependent mode. In these systems, incorporation of information from the gesture input into the multimodal meaning is triggered by the appearance of expressions in the speech input whose reference needs to be resolved, such as deﬁnite and deictic noun phrases (e.g., this one, the red cube). Multimodal integration was essentially a procedural add-on to a speech or text understanding system. Johnston et al. (1997) developed a more declarative approach where multimodal integration is modeled as uniﬁcation of typed feature structures (Carpenter 1992) assigned to speech and gesture inputs. Johnston (1998a, 1998b) utilized techniques from natural language processing (uniﬁcation-based grammars and chart parsers) to extend the uniﬁcation-based approach and enable handling of inputs with more than one gesture, visual parsing, and more ﬂexible and declarative encoding of temporal and spatial constraints. In contrast to the uniﬁcation-based approaches, which separate speech parsing and multimodal integration into separate processing stages, Johnston and Bangalore (2000, 2005) proposed a one-stage approach to multimodal understanding in which a single grammar speciﬁed the integration and understanding of multimodal language. This avoids the complexity of interfacing between separate speech understanding and multimodal parsing components. This approach is highly efﬁcient and enables tight coupling with speech recognition, because the grammar can be directly compiled into a cascade of ﬁnite-state transducers which can compose directly with lattices from speech recognition and gesture recognition components. In this section, we explain how the ﬁnite-state approach to multimodal language understanding can be extended beyond multimodal input with simple pointing gestures made on a touchscreen (as in Johnston and Bangalore [2000, 2005]) to applications such as MATCH with complex gesture input combining freeform drawings with handwriting recognition. This involves three signiﬁcant extensions to the approach: the development of a gesture representation language for complex pen input combining freehand drawing with selections and handwriting (Section 3.1); a new more scalable approach to abstraction over the speciﬁc content of gestures within the ﬁnite-state mechanism (Section 3.3); and a new gesture aggregation algorithm which enables robust handling of the integration of deictic phrases with a broad range of different selection gestures (Section 3.4). In Section 3.2, we illustrate the use of multimodal grammars for this application with a fragment of the multimodal grammar for MATCH and illustrate how this grammar is compiled into a cascade of ﬁnite-state transducers. Section 3.5 addresses the issue of temporal constraints on multimodal integration. In Section 3.6, we describe the multimodal dialog management mechanism used in the system and how  
Paul Hoffmann∗∗ University of Pittsburgh Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation). However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word’s prior polarity. Positive words are used in phrases expressing negative sentiments, or vice versa. Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment. The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task. Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity. The evaluation includes assessing the performance of features across multiple machine learning algorithms. For all learning algorithms except one, the combination of all features together gives the best performance. Another facet of the evaluation considers how the presence of neutral instances affects the performance of features for distinguishing between positive and negative polarity. These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system’s ability to identify when an instance is neutral. 1. Introduction Sentiment analysis is a type of subjectivity analysis (Wiebe 1994) that focuses on identifying positive and negative opinions, emotions, and evaluations expressed in natural language. It has been a central component in applications ranging from recognizing ∗ School of Informatics, Edinburgh EH8 9LW, U.K. E-mail: twilson@inf.ed.ac.uk. ∗∗ Department of Computer Science, Pittsburgh, PA 15260, USA. E-mail: {wiebe,hoffmanp}@cs.pitt.edu. Submission received: 14 November 2006; revised submission received: 8 March 2008; accepted for publication: 16 April 2008. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 3  inﬂammatory messages (Spertus 1997), to tracking sentiments over time in online discussions (Tong 2001), to classifying positive and negative reviews (Pang, Lee, and Vaithyanathan 2002; Turney 2002). Although a great deal of work in sentiment analysis has targeted documents, applications such as opinion question answering (Yu and Hatzivassiloglou 2003; Maybury 2004; Stoyanov, Cardie, and Wiebe 2005) and review mining to extract opinions about companies and products (Morinaga et al. 2002; Nasukawa and Yi 2003) require sentence-level or even phrase-level analysis. For example, if a question answering system is to successfully answer questions about people’s opinions, it must be able not only to pinpoint expressions of positive and negative sentiments, such as we ﬁnd in sentence (1), but also to determine when an opinion is not being expressed by a word or phrase that typically does evoke one, such as condemned in sentence (2). (1) African observers generally approved (positive) of his victory while Western governments denounced (negative) it. (2) Gavin Elementary School was condemned in April 2004. A common approach to sentiment analysis is to use a lexicon with information about which words and phrases are positive and which are negative. This lexicon may be manually compiled, as is the case with the General Inquirer (Stone et al. 1966), a resource often used in sentiment analysis. Alternatively, the information in the lexicon may be acquired automatically. Acquiring the polarity of words and phrases is itself an active line of research in the sentiment analysis community, pioneered by the work of Hatzivassiloglou and McKeown (1997) on predicting the polarity or semantic orientation of adjectives. Various techniques have been proposed for learning the polarity of words. They include corpus-based techniques, such as using constraints on the co-occurrence in conjunctions of words with similar or opposite polarity (Hatzivassiloglou and McKeown 1997) and statistical measures of word association (Turney and Littman 2003), as well as techniques that exploit information about lexical relationships (Kamps and Marx 2002; Kim and Hovy 2004) and glosses (Esuli and Sebastiani 2005; Andreevskaia and Bergler 2006) in resources such as WordNet. Acquiring the polarity of words and phrases is undeniably important, and there are still open research challenges, such as addressing the sentiments of different senses of words (Esuli and Sebastiani 2006b; Wiebe and Mihalcea 2006), and so on. However, what the polarity of a given word or phrase is when it is used in a particular context is another problem entirely. Consider, for example, the underlined positive and negative words in the following sentence. (3) Philip Clapp, president of the National Environment Trust, sums up well the general thrust of the reaction of environmental movements: “There is no reason at all to believe that the polluters are suddenly going to become reasonable.” The ﬁrst underlined word is Trust. Although many senses of the word trust express a positive sentiment, in this case, the word is not being used to express a sentiment at all. It is simply part of an expression referring to an organization that has taken on the charge of caring for the environment. The adjective well is considered positive, and indeed it is positive in this context. However, the same is not true for the words reason 400  Wilson, Wiebe, and Hoffmann  Recognizing Contextual Polarity  and reasonable. Out of context, we would consider both of these words to be positive.1 In context, the word reason is being negated, changing its polarity from positive to negative. The phrase no reason at all to believe changes the polarity of the proposition that follows; because reasonable falls within this proposition, its polarity becomes negative. The word polluters has a negative connotation, but here in the context of the discussion of the article and its position in the sentence, polluters is being used less to express a sentiment and more to objectively refer to companies that pollute. To clarify how the polarity of polluters is affected by its subject role, consider the purely negative sentiment that emerges when it is used as an object: They are polluters. We call the polarity that would be listed for a word in a lexicon the word’s prior polarity, and we call the polarity of the expression in which a word appears, considering the context of the sentence and document, the word’s contextual polarity. Although words often do have the same prior and contextual polarity, many times a word’s prior and contextual polarities differ. Words with a positive prior polarity may have a negative contextual polarity, or vice versa. Quite often words that are positive or negative out of context are neutral in context, meaning that they are not even being used to express a sentiment. Similarly, words that are neutral out of context, neither positive or negative, may combine to create a positive or negative expression in context. The focus of this work is on the recognition of contextual polarity—in particular, disambiguating the contextual polarity of words with positive or negative prior polarity. We begin by presenting an annotation scheme for marking sentiment expressions and their contextual polarity in the Multi-perspective Question Answering (MPQA) opinion corpus. We show that, given a set of subjective expressions (identiﬁed from the existing annotations in the MPQA corpus), contextual polarity can be annotated reliably. Using the contextual polarity annotations, we conduct experiments in automatically distinguishing between prior and contextual polarity. Beginning with a large lexicon of clues tagged with prior polarity, we identify the contextual polarity of the instances of those clues in the corpus. The process that we use has two steps, ﬁrst classifying each clue as being in a neutral or polar phrase, and then disambiguating the contextual polarity of the clues marked as polar. For each step in the process, we experiment with a variety of features and evaluate the performance of the features using several different machine learning algorithms. Our experiments reveal a number of interesting ﬁndings. First, being able to accurately identify neutral contextual polarity—when a positive or negative clue is not being used to express a sentiment—is an important aspect of the problem. The importance of neutral examples has previously been noted for classifying the sentiment of documents (Koppel and Schler 2006), but ours is the ﬁrst work to explore how neutral instances affect classifying the contextual polarity of words and phrases. In particular, we found that the performance of features for distinguishing between positive and negative polarity greatly degrades when neutral instances are included in the experiments. We also found that achieving the best performance for recognizing contextual polarity requires a wide variety of features. This is particularly true for distinguishing  
This article presents a novel bootstrapping approach for improving the quality of feature vector weighting in distributional word similarity. The method was motivated by attempts to utilize distributional similarity for identifying the concrete semantic relationship of lexical entailment. Our analysis revealed that a major reason for the rather loose semantic similarity obtained by distributional similarity methods is insufﬁcient quality of the word feature vectors, caused by deﬁcient feature weighting. This observation led to the deﬁnition of a bootstrapping scheme which yields improved feature weights, and hence higher quality feature vectors. The underlying idea of our approach is that features which are common to similar words are also most characteristic for their meanings, and thus should be promoted. This idea is realized via a bootstrapping step applied to an initial standard approximation of the similarity space. The superior performance of the bootstrapping method was assessed in two different experiments, one based on direct human gold-standard annotation and the other based on an automatically created disambiguation dataset. These results are further supported by applying a novel quantitative measurement of the quality of feature weighting functions. Improved feature weighting also allows massive feature reduction, which indicates that the most characteristic features for a word are indeed concentrated at the top ranks of its vector. Finally, experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied. 1. Introduction 1.1 Motivation Distributional word similarity has long been an active research area (Hindle 1990; Ruge 1992; Grefenstette 1994; Lee 1997; Lin 1998; Dagan, Lee, and Pereira 1999; Weeds and ∗ Department of Information Science, Bar-Ilan University, Ramat-Gan, Israel. E-mail: zhitomim@mail.biu.ac.il. ∗∗ Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel. E-mail: dagan@cs.biu.ac.il. Submission received: 6 December 2006; revised submission received: 9 July 2008; accepted for publication: 21 November 2008. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 3  Weir 2005). This paradigm is inspired by Harris’s distributional hypothesis (Harris 1968), which states that semantically similar words tend to appear in similar contexts. In a computational realization, each word is characterized by a weighted feature vector, where features typically correspond to other words that co-occur with the characterized word in the context. Distributional similarity measures quantify the degree of similarity between a pair of such feature vectors. It is then assumed that two words that occur within similar contexts, as measured by similarity of their context vectors, are indeed semantically similar. The distributional word similarity measures were often applied for two types of inferences. The ﬁrst type is making similarity-based generalizations for smoothing word co-occurrence probabilities, in applications such as language modeling and disambiguation. For example, assume that we need to estimate the likelihood of the verb– object co-occurrence pair visit–country, although it did not appear in our sample corpus. Co-occurrences of the verb visit with words that are distributionally similar to country, such as state, city, and region, however, do appear in the corpus. Consequently, we may infer that visit–country is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus, and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts. A second type of semantic inference, which primarily motivated our own research, is meaning-preserving lexical substitution. Many NLP applications, such as question answering, information retrieval, information extraction, and (multi-document) summarization, need to recognize that one word can be substituted by another one in a given context while preserving, or entailing the original meaning. Naturally, recognizing such substitutable lexical entailments is a prominent component within the textual entailment recognition paradigm, which models semantic inference as an applicationindependent task (Dagan, Glickman, and Magnini 2006). Accordingly, several textual entailment systems did utilize the output of distributional similarity measures to model entailing lexical substitutions (Jijkoun and de Rijke 2005; Adams 2006; Ferrandez et al. 2006; Nicholson, Stokes, and Baldwin 2006; Vanderwende, Menezes, and Snow 2006). In some of these papers the distributional information typically complements manual lexical resources in textual entailment systems, most notably WordNet (Fellbaum 1998). Lexical substitution typically requires that the meaning of one word entails the meaning of the other. For instance, in question answering, the word company in a question can be substituted in an answer text by ﬁrm, automaker, or subsidiary, whose meanings entail the meaning of company. However, as it turns out, traditional distributional similarity measures do not capture well such lexical substitution relationships, but rather capture a somewhat broader (and looser) notion of semantic similarity. For example, quite distant co-hyponyms such as party and company also come out as distributionally similar to country, due to a partial overlap of their semantic properties. Clearly, the meanings of these words do not entail each other. Motivated by these observations, our long-term goal is to investigate whether the distributional similarity scheme may be improved to yield tighter semantic similarities, and eventually better approximation of lexical entailments. This article presents one component of this research plan, which focuses on improving the underlying semantic 436  Zhitomirsky-Geffet and Dagan  Bootstrapping Distributional Feature Vector Quality  quality of distributional word feature vectors. The article describes the methodology, deﬁnitions, and analysis of our investigation and the resulting bootstrapping scheme for feature weighting which yielded improved empirical performance.  1.2 Main Contributions and Outline As a starting point for our investigation, an operational deﬁnition was needed for evaluating the correctness of candidate pairs of similar words. Following the lexical substitution motivation, in Section 3 we formulate the substitutable lexical entailment relation (or lexical entailment, for brevity), reﬁning earlier deﬁnitions in Geffet and Dagan (2004, 2005). Generally speaking, this relation holds for a pair of words if a possible meaning of one word entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. Lexical entailment overlaps partly with traditional lexical semantic relationships, while capturing more generally the lexical substitution needs of applications. Empirically, high inter-annotator agreement was obtained when judging the output of distributional similarity measures for lexical entailment. Next, we analyzed the typical behavior of existing word similarity measures relative to the lexical entailment criterion. Choosing the commonly used measure of Lin (1998) as a representative case, the analysis shows that quite noisy feature vectors are a major cause for generating rather “loose” semantic similarities. On the other hand, one may expect that features which seem to be most characteristic for a word’s meaning should receive the highest feature weights. This does not seem to be the case, however, for common feature weighting functions, such as Point-wise Mutual Information (Church and Patrick 1990; Hindle 1990). Following these observations, we developed a bootstrapping formula that improves the original feature weights (Section 4), leading to better feature vectors and better similarity predictions. The general idea is to promote the weights of features that are common for semantically similar words, since these features are likely to be most characteristic for the word’s meaning. This idea is implemented by a bootstrapping scheme, where the initial (and cruder) similarity measure provides an initial approximation for semantic word similarity. The bootstrapping method yields a high concentration of semantically characteristic features among the top-ranked features of the vector, which also allows aggressive feature reduction. The bootstrapping scheme was evaluated in two experimental settings, which correspond to the two types of applications for distributional similarity. First, it achieved signiﬁcant improvements in predicting lexical entailment as assessed by human judgments, when applied over several base similarity measures (Section 5). Additional analysis relative to the lexical entailment dataset revealed cleaner and more characteristic feature vectors for the bootstrapping method. To obtain a quantitative analysis of this behavior, we deﬁned a measure called average common-feature rank ratio. This measure captures the idea that a prominent feature for a word is expected to be prominent also for semantically similar words, while being less prominent for unrelated words. To the best of our knowledge this is the ﬁrst proposed measure for direct analysis of the quality of feature weighting functions, without the need to employ them within some vector similarity measure. As a second evaluation, we applied the bootstrapping scheme for similarity-based prediction of co-occurrence likelihood within a typical pseudo-word sense disambiguation experiment, obtaining substantial error reductions (Section 7). Section 8 concludes 437  Computational Linguistics  Volume 35, Number 3  this article, suggesting the relevance of our analysis and bootstrapping scheme for the general use of distributional feature vectors.1  2. Background: Distributional Similarity Models This section reviews the components of the distributional similarity approach and speciﬁes the measures and functions that were utilized by our work. The Distributional Hypothesis assumes that semantically similar words appear in similar contexts, suggesting that semantic similarity can be detected by comparing contexts of words. This is the underlying principle of the vector-based distributional similarity model, which comprises two phases. First, context features for each word are constructed and assigned weights; then, the weighted feature vectors of pairs of words are compared by a vector similarity measure. The following two subsections review typical methods for each phase. 2.1 Features and Weighting Functions In the typical computational setting, word contexts are represented by feature vectors. A feature represents another word (or term) w with which w co-occurs, and possibly speciﬁes also the syntactic relationship between the two words, as in Grefenstette (1994), Lin (1998), and Weeds and Weir (2005). Thus, a word (or term) w is represented by a feature vector, where each entry in the vector corresponds to a feature f . Pado and Lapata (2007) demonstrate that using syntactic dependency-based features helps to distinguish among classes of lexical relations, which seems to be more difﬁcult when using “bag of words” features that are based on co-occurrence in a text window. A syntactic-based feature f for a word w is deﬁned as a triple: fw, syn rel, f role where fw is a context word (or term) that co-occurs with w under the syntactic dependency relation syn rel. The feature role ( f role) corresponds to the role of the feature word fw in the syntactic dependency, being either the head (denoted h) or the modiﬁer (denoted m) of the relation. For example, given the word company, the feature earnings, gen, h corresponds to the genitive relationship company’s earnings, and investor, pcomp of, m corresponds to the prepositional complement relationship the company of the investor.2 Throughout this article we use syntactic dependency relationships generated by the Minipar dependency parser (Lin 1993). Table 1 lists common Minipar dependency relations involving nouns. Minipar also identiﬁes multi-word expressions, which is  
Speech and Language Processing is a general textbook on natural language processing, with an excellent coverage of the area and an unusually broad scope of topics. It includes statistical and symbolic approaches to NLP, as well as the main methods of speech processing. I would rank it as the most appropriate introductory and reference textbook for purposes such as an introductory fourth-year undergraduate or graduate course, a general introduction for an interested reader, or an NLP reference for a researcher or other professional working in an area related to NLP. The book’s contents are organized in an order corresponding to different levels of natural language processing. After the introductory chapter 1, there are ﬁve parts: r Part I, Words: ﬁve chapters covering regular expressions, automata, words, transducers, n-grams, part-of-speech tagging, hidden Markov models, and maximum entropy models. r Part II, Speech: ﬁve chapters covering phonetics, speech synthesis, recognition, and phonology. r Part III, Syntax: ﬁve chapters covering a formal grammar of English, syntactic and statistical parsing, feature structures and uniﬁcation, and complexity of language classes. r Part IV, Semantics and Pragmatics: ﬁve chapters covering representation of meaning, computational semantics, lexical and computational lexical semantics, and computational discourse. r Part V, Applications: four chapters covering information extraction, question answering, summarization, dialogue and conversational agents, and machine translation. The ﬁrst edition of the book appeared in 2000 with the same title, and a very similar size and structure. The structure has been changed by breaking the old part “Words” into two parts “Words” and “Speech,” merging two old parts “Semantics” and “Pragmatics” into one “Semantics and Pragmatics,” and introducing one new part “Applications.” I considered the old edition also to be the textbook of choice for a course in NLP, but even though the changes may not appear to be signiﬁcant, the new edition is a marked improvement, both in overall content structure as well as in presenting topics at a ﬁner-grained level. Topics on speech synthesis and recognition are signiﬁcantly expanded; maximum entropy models are introduced and very well  Computational Linguistics  Volume 35, Number 3  explained; and statistical parsing is covered better with an explanation of the principal ideas in probabilistic lexicalized context-free grammars. Both editions include very detailed examples, with actual numerical values and computation, explaining various methods such as n-grams and smoothing. As another example, maximum entropy modeling is a popular topic but in many books explained only superﬁcially, while here it is presented in a well-motivated and very intuitive way. The learning method is not covered, and more details about it would be very useful. The new edition conveniently includes the following useful reference tables on endpapers: regular expression syntax, Penn Treebank POS tags, some WordNet 3.0 relations, and major ARPAbet symbols. The book was written with a broad coverage in mind (language and speech processing; symbolic and stochastic approaches; and algorithmic, probabilistic, and signalprocessing methodology) and a wide audience: computer scientists, linguists, and engineers. This has a positive side, because there is an educational need, especially in computer science, to present NLP in a broad, integrated way; this has seemed to be always very challenging and books with wide coverage were rare or non-existent. For example, Allen’s (1995) Natural Language Understanding presented mostly a symbolic approach to NLP, whereas Manning and Schu¨ tze’s (1999) Foundations of Statistical Natural Language Processing presented an exclusively statistical approach. However, there is also a negative side to the wide coverage—it is probably impossible to present material in an order that would satisfy audiences from different backgrounds, in particular, linguists vs. computer scientists and engineers. In my particular case, I started teaching a graduate course in Natural Language Processing in 2002 at Dalhousie University, which later became a combined graduate/ undergraduate course. My goal was to present an integrated view of NLP with an emphasis on two main paradigms: knowledge-based or symbolic, and probabilistic. Not being aware of Jurafsky and Martin’s book at the time, I was using Manning and Schu¨ tze’s book for the probabilistic part, and Sag and Wasow’s (1999) book Syntactic Theory: A Formal Introduction for the symbolic part. I was very happy to learn about Jurafsky and Martin’s book, since it ﬁtted my course objectives very well. Although I keep using the book, including this new edition in Fall 2008, and ﬁnd it a very good match with the course, there is quite a difference between the textbook and the course in order of topics and the overall philosophy, so the book is used as a main supportive reading reference and the course notes are used to navigate students through the material. I will discuss some of the particular differences and similarities between Jurafsky and Martin’s book and my course syllabus, as I believe my course is representative of the NLP courses taught by many readers of this journal. The book introduces regular expressions and automata in Chapter 2, and later introduces context-free grammars in Chapter 12, followed by some general discussion about formal languages and complexity in Chapter 16. This is a somewhat disrupted sequence of topics from formal language theory, which should be covered earlier in a typical undergraduate computer science program. Of course, it is not only a very good idea but necessary to cover these topics in case a reader is not familiar with them; however, they should be presented as one introductory unit. Additionally, a presentation with an emphasis on theoretical background, rather than practical issues of using regular expressions, would be more valuable. For example, the elegance of the deﬁnition of regular sets, using elementary sets and closure of three operations, is much more appealing and conceptually important than shorthand tricks of using practical regular expressions, which are given more space and visibility. As another example, it is hard to understand the choice of discussing equivalence of deterministic 464  Book Review and non-deterministic ﬁnite automata in a small, note-like subsection (2.2.7), yet giving three-quarters of a page to an exponential algorithm for NFSA recognition (in Figure 2.19), with a page-long discussion. It may be damaging to students even to mention such a poor algorithm choice as the use of backtracking or a classical search algorithm for NFSA acceptance. Context-free grammars are described in subsection 12.2.1; besides the need to have them earlier in the course, actually as a part of introductory background review, more space should be given to this important formalism. In addition to the concepts of derivation and “syntactic parsing,” the following concepts should be introduced as well: parse trees, left-most and right-most derivation, sentential forms, the language induced by a grammar, context-free languages, grammar ambiguity, ambiguous sentences, bracketed representation of the parse trees, and a grammar induced by a treebank. Some of these concepts are introduced in other parts of the book. More advanced concepts would be desirable as well, such as pumping lemmas, provable non-context-freeness of some languages, and push-down automata. As noted earlier, the order of the book contents follows the levels of NLP, starting with words and speech, then syntax, and ending with semantics and pragmatics, followed by applications. From my perspective, having applications at the end worked well; however, while levels of NLP are an elegant and important view of the NLP domain, it seems more important that students master the main methodological approaches to solving problems rather than the NLP levels of those problems. Hence, my course is organized around topics such as n-gram models, probabilistic models, naive Bayes, Bayesian networks, HMMs, uniﬁcation-based grammars, and similar, rather than following NLP levels and corresponding problems, such as POS tagging, word-sense disambiguation, language modeling, and parsing. For example, HMMs are introduced in Chapter 5, as a part of part-of-speech tagging; language modeling is discussed in Chapter 4; and naive Bayes models are discussed in Chapter 20. The discussion of uniﬁcation in the book could be extended. It starts with feature structures in Chapter 15, including discussion of uniﬁcation, implementation, modeling some natural language phenomena, and types and inheritance. The uniﬁcation algorithm (Figure 15.8, page 511) is poorly chosen. A better choice would be a standard, elegant, and efﬁcient algorithm, such as Huet’s (e.g., Knight 1989). The recursive algorithm used in the book is not as efﬁcient, elegant, nor easy to understand as Huet’s, and it contains serious implementational traps. For example, it is not emphasized that the proper way to maintain the pointers is to use the UNION-FIND data structure (e.g., Cormen et al. 2002). If the pointers f1 and f2 are identical, there is no need to set f1.pointer to f2. Finally, if f1 and f2 are complex structures, it is not a good idea to make a recursive call before their uniﬁcation is ﬁnished, since these structures may be accessed and uniﬁed with other structures during the recursive call. The proper way to do it is to use a stack or queue (usually called sigma) in Huet’s style, add pointers to structures to be uniﬁed on the stack, and unify them after the uniﬁcation of current feature structure nodes is ﬁnished. Actually, this is similar to the use of “agenda” earlier in the book, so it would ﬁt well with previous algorithms. Regarding the order of the uniﬁcation topics, I prefer an approach with a historical order, starting from classical uniﬁcation and resolution, followed by deﬁnite-clause grammars, and then following with feature structures. The Prolog programming language is a very important part in the story of uniﬁcation, and should not be skipped, as it is here. More could be written about type hierarchies and their implementation, especially because they are conceptually very relevant to the recent popular use of ontologies and the Semantic Web. 465  Computational Linguistics  Volume 35, Number 3  As a ﬁnal remark on the order, I found it useful in a computer science course to present all needed linguistic background at the beginning, such as English word classes (in Chapter 5), morphology (in Chapter 3), typical rules in English syntax (in Chapter 12), and elements of semantics (in Chapter 19), and even a bit of pragmatics. As can be seen, these pieces are placed throughout the book. The introduction of English syntax in Chapter 12 is excellent and better than what can be typically found in NLP books, but nonetheless, the ordering of the topics could be better: Agreement and other natural language phenomena are intermixed with context-free rules, whereas in my course those two were separated. The point should be that context-free grammars are a very elegant formalism, but phenomena such as agreement, movement, and subcategorization are the issues that need to be addressed in natural languages and are not handled by a context-free grammar (cf. Sag and Wasow 1999). I also used the textbook in a graduate reading course on speech processing, with emphasis on speech synthesis. The book was a useful reference, but the coverage was sufﬁcient for only a small part of the course. The following are some minor remarks: The title of Chapter 13, “Syntactic Parsing,” is unusual because normally parsing is considered to be a synonym for syntactic processing. The chapter describes the classical parsing algorithms for formal languages, such as CKY and Earley’s, and the next chapter describes statistical parsing. Maybe a title such as “Classical Parsing,” “Symbolic Parsing,” or simply “Parsing” would be better. The Good–Turing discounting on page 101 and the formula (4.24) are not well explained. The formula (14.36) on page 479 for harmonic mean is not correct; the small fractions in the denominator need to be added. In conclusion, there are places that could be improved, and in particular, I did not ﬁnd that the order of material was the best possible. Nonetheless, the book is recommended as ﬁrst on the list for a textbook in a course in natural language processing.  References Allen, James. 1995. Natural Language Understanding. The Benjamin/Cummings Publishing Company, Inc., Redwood City, CA. Cormen, Thomas H., Leiserson, Charles E., Rivest, Ronald L., and Stein, Clifford. 2002. Introduction to Algorithms, 2nd edition. The MIT Press, Cambridge, MA. Knight, Kevin. 1989. Uniﬁcation: A  multidisciplinary survey. ACM Computing Surveys, 21(1): 93–124. Manning, Christopher D. and Schu¨ tze, Hinrich. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA. Sag, Ivan A. and Wasow, Thomas. 1999. Syntactic Theory: A Formal Introduction. CSLI Publications, Stanford, CA.  
The event was full of computation and linguistics, yet devoid of computational linguistics. The language documentation community uses technology to process language, but is largely ignorant of the ﬁeld of natural language processing. I pondered what we have to offer this community: “Send us your 10 million words of Nahuatl-English bitext and we’ll do you a machine translation system!” “Show us your Bambara WordNet and we’ll use it to train a word sense disambiguation tool!” “Write up the word-formation rules of Inuktitut in this arcane format and we’ll give you a morphological analyzer!” Is there not some more immediate contribution we could offer? Over the past 15 years, the ﬁeld of computational linguistics has been revolutionized by the ready availability of large corpora. Landmark dates are the founding of the Linguistic Data Consortium (1992) and the ﬁrst Workshop on Very Large Corpora (1993). While the CL community has been pre-occupied with the new-found technical capabilities for collecting and processing large amounts of data, the ﬁeld of linguistics has been undergoing a revolution of its own. It is also dominated with the use of new-found technical capabilities for collecting and processing large amounts of data. However, in this case, the data comes from languages that are facing extinction. Back in 1992, Michael Krauss, of the Alaska Native Language Center, issued the world’s linguists with a wake-up call, calculating that “at the rate things are going— the coming century will see either the death or the doom of 90 per cent of mankind’s languages” (Krauss 1992, page 7). He exhorted linguists to document these languages “lest linguistics go down in history as the only science that presided obliviously over the disappearance of 90 per cent of the very ﬁeld to which it is dedicated” (page 10). This message was delivered at the 15th International Congress of Linguists in Quebec, and also in Language, the journal of the Linguistic Society of America.2 ∗ Department of Computer Science and Software Engineering, University of Melbourne, Victoria 3010, Australia. E-mail: sb@csse.unimelb.edu.au. 
Aline Villavicencio Federal University of Rio Grande do Sul, Brazil, and University of Bath, UK 1. Introduction Prepositions1—as well as prepositional phrases (PPs) and markers of various sorts— have a mixed history in computational linguistics (CL), as well as related ﬁelds such as artiﬁcial intelligence, information retrieval (IR), and computational psycholinguistics: On the one hand they have been championed as being vital to precise language understanding (e.g., in information extraction), and on the other they have been ignored on the grounds of being syntactically promiscuous and semantically vacuous, and relegated to the ignominious rank of “stop word” (e.g., in text classiﬁcation and IR). Although NLP in general has beneﬁtted from advances in those areas where prepositions have received attention, there are still many issues to be addressed. For example, in machine translation, generating a preposition (or “case marker” in languages such as Japanese) incorrectly in the target language can lead to critical semantic divergences over the source language string. Equivalently in information retrieval and information extraction, it would seem desirable to be able to predict that book on NLP and book about NLP mean largely the same thing, but paranoid about drugs and paranoid on drugs suggest very different things. Prepositions are often among the most frequent words in a language. For example, based on the British National Corpus (BNC; Burnard 2000), four out of the top-ten most-frequent words in English are prepositions (of, to, in, and for). In terms of both parsing and generation, therefore, accurate models of preposition usage are essential to avoid repeatedly making errors. Despite their frequency, however, they are notoriously difﬁcult to master, even for humans (Chodorow, Tetreault, and Han 2007). For example, Lindstromberg (2001) estimates that less than 10% of upper-level English as a Second 
This article describes how semantic role resources can be exploited for preposition disambiguation. The main resources include the semantic role annotations provided by the Penn Treebank and FrameNet tagged corpora. The resources also include the assertions contained in the Factotum knowledge base, as well as information from Cyc and Conceptual Graphs. A common inventory is derived from these in support of deﬁnition analysis, which is the motivation for this work. The disambiguation concentrates on relations indicated by prepositional phrases, and is framed as word-sense disambiguation for the preposition in question. A new type of feature for word-sense disambiguation is introduced, using WordNet hypernyms as collocations rather than just words. Various experiments over the Penn Treebank and FrameNet data are presented, including prepositions classiﬁed separately versus together, and illustrating the effects of ﬁltering. Similar experimentation is done over the Factotum data, including a method for inferring likely preposition usage from corpora, as knowledge bases do not generally indicate how relationships are expressed in English (in contrast to the explicit annotations on this in the Penn Treebank and FrameNet). Other experiments are included with the FrameNet data mapped into the common relation inventory developed for deﬁnition analysis, illustrating how preposition disambiguation might be applied in lexical acquisition. 1. Introduction English prepositions convey important relations in text. When used as verbal adjuncts, they are the principal means of conveying semantic roles for the supporting entities described by the predicate. Preposition disambiguation is a challenging problem. First, prepositions are highly polysemous. A typical collegiate dictionary has dozens of senses for each of the common prepositions. Second, the senses of prepositions tend to be closely related to one another. For instance, there are three duplicate role assignments among the twenty senses for of in The Preposition Project (Litkowski and Hargraves 2006), a resource containing semantic annotations for common prepositions. ∗ Institute for Language and Information Technologies, Baltimore, MD 21250. E-mail: tomohara@umbc.edu. ∗∗ Department of Computer Science, Pittsburgh, PA 15260. E-mail: wiebe@cs.pitt.edu. Submission received: 7 August 2006; accepted for publication: 21 February 2007. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 2  Consider the disambiguation of the usages of on in the following sentences: (1) The cut should be blocked on procedural grounds. (2) The industry already operates on very thin margins. The choice between the purpose and manner meanings for on in these sentences is difﬁcult. The purpose meaning seems preferred for sentence 1, as grounds is a type of justiﬁcation. For sentence 2, the choice is even less clear, though the manner meaning seems preferred. This article presents a new method for disambiguating prepositions using information learned from annotated corpora as well as knowledge stored in declarative lexical resources. The approach allows for better coverage and ﬁner distinctions than in previous work in preposition disambiguation. For instance, a traditional approach would involve manually developing rules for on that specify the semantic type of objects associated with the different senses (e.g., time for temporal). Instead, we infer this based on lexical associations learned from annotated corpora. The motivation for preposition disambiguation is to support a system for lexical acquisition (O’Hara 2005). The focus of the system is to acquire distinguishing information for the concepts serving to deﬁne words. Large-scale semantic lexicons mainly emphasize the taxonomic relations among the underlying concepts (e.g., is-a and partof ), and often lack sufﬁcient differentiation among similar concepts (e.g., via attributes or functional relations such as is-used-for). For example, in WordNet (Miller et al. 1990), the standard lexical resource for natural language processing, the only relations for beagle and Afghan are that they are both a type of hound. Although the size difference can be inferred from the deﬁnitions, it is not represented in the WordNet semantic network. In WordNet, words are grouped into synonym sets called synsets, which represent the underlying concepts and serve as nodes in a semantic network. Synsets are ordered into a hierarchy using the hypernym relation (i.e., is-a). There are several other semantic relations, such as part-whole, is-similar-to, and domain-of . Nonetheless, in version 2.1 of WordNet, about 30% of the synsets for noun entries are not explicitly distinguished from sibling synsets via semantic relations. To address such coverage problems in lexicons, we have developed an empirical approach to lexical acquisition, building upon earlier knowledge-based approaches in dictionary deﬁnition analysis (Wilks, Slator, and Guthrie 1996). This involves a two-step process: Deﬁnitions are ﬁrst analyzed with a broad-coverage parser, and then the resulting syntactic relationships are disambiguated using statistical classiﬁcation. A crucial part of this process is the disambiguation of prepositions, exploiting online resources with semantic role usage information. The main resources are the Penn Treebank (PTB; Marcus et al. 1994) and FrameNet (Fillmore, Wooters, and Baker 2001), two popular corpora providing rich annotations on English text, such as the semantic roles associated with prepositional phrases in context. In addition to the semantic role annotations from PTB and FrameNet, traditional knowledge bases (KBs) are utilized to provide training data for the relation classiﬁcation. In particular, the Factotum KB (Cassidy 2000) is used to provide additional training data for prepositions that are used to convey particular relationships. Information on preposition usage is not explicitly encoded in Factotum, so a new corpus analysis technique is employed to infer the associations. Details on the lexical acquisition process, including application and evaluation, can be found in O’Hara (2005). This article focuses on the aspects of this method relevant to the processing of prepositions. In particular, here we speciﬁcally address preposition 152  O’Hara and Wiebe  Exploiting Resources for Preposition Disambiguation  disambiguation using semantic role annotations from PTB, FrameNet, and Factotum. In each case, classiﬁcation experiments are presented using the respective resources as training data with evaluation via 10-fold cross validation. This article is organized as follows. Section 2 presents background information on the relation inventories used during classiﬁcation, including one developed speciﬁcally for deﬁnition analysis. Section 3 discusses the relation classiﬁers in depth with results given for four different inventories. Section 4 discusses related work in relation disambiguation, and Section 5 presents our conclusions. 2. Semantic Relation Inventories The representation of natural language utterances often incorporates the notion of semantic roles, which are analogous to the slots in a frame-based representation. In particular, there is an emphasis on the analysis of thematic roles, which serve to tie the grammatical constituents of a sentence to the underlying semantic representation. Thematic roles are also called case roles, because in some languages the grammatical constituents are indicated by case inﬂections (e.g., ablative in Latin). As used here, the term “semantic role” refers to an arbitrary semantic relation, and the term “thematic role” refers to a relation intended to capture the semantics of sentences (e.g., event participation). Which semantic roles are used varies widely in Natural Language Processing (NLP). Some systems use just a small number of very general roles, such as beneﬁciary. At the other extreme, some systems use quite speciﬁc roles tailored to a particular domain, such as catalyst in the chemical sense. 2.1 Background on Semantic Roles Bruce (1975) presents an account of early case systems in NLP. For the most part, those systems had limited case role inventories, along the lines of the cases deﬁned by Fillmore (1968). Palmer (1990) discusses some of the more contentious issues regarding case systems, including adequacy for representation, such as reliance solely upon case information to determine semantics versus the use of additional inference mechanisms. Barker (1998) provides a comprehensive summary of case inventories in NLP, along with criteria for the qualitative evaluation of case systems (generality, completeness, and uniqueness). Linguistic work on thematic roles tends to use a limited number of roles. Frawley (1992) presents a detailed discussion of twelve thematic roles and discusses how they are realized in different languages. During the shift in emphasis away from systems that work in small, self-contained domains to those that can handle open-ended domains, there has been a trend towards the use of larger sets of semantic primitives (Wilks, Slator, and Guthrie 1996). The WordNet lexicon (Miller et al. 1990) serves as one example of this. A synset is deﬁned in terms of its relations with any of the other 100,000+ synsets, rather than in terms of a set of features like [±ANIMATE]. There has also been a shift in focus from deep understanding (e.g., story comprehension) facilitated by specially constructed KBs to shallow surface-level analysis (e.g., text extraction) facilitated by corpus analysis. Both trends seem to be behind the increase in case inventories in two relatively recent resources, namely FrameNet (Fillmore, Wooters, and Baker 2001) and OpenCyc (OpenCyc 2002), both of which deﬁne well over a hundred case roles. However, provided that the case roles are well structured in an inheritance hierarchy, both paraphrasability and coverage can be addressed by the same inventory. 153  Computational Linguistics  Volume 35, Number 2  2.2 Inventories Developed for Corpus Annotation With the emphasis on corpus analysis in computational linguistics, there has been a shift away from relying on explicitly-coded knowledge towards the use of knowledge inferred from naturally occurring text, in particular text that has been annotated by humans to indicate phenomena of interest. For example, rather than manually developing rules for preferring one sense of a word over another based on context, the most successful approaches have automatically learned the rules based on word-sense annotations, as evidenced by the Senseval competitions (Kilgarriff 1998; Edmonds and Cotton 2001). The Penn Treebank version II (Marcus et al. 1994) provided the ﬁrst large-scale set of case annotations for general-purpose text. These are very general roles, following Fillmore (1968). The Berkeley FrameNet (Fillmore, Wooters, and Baker 2001) project currently provides the most comprehensive set of semantic roles annotations. These are at a much ﬁner granularity than those in PTB, making them quite useful for applications learning semantics from corpora. Relation disambiguation experiments for both of these role inventories are presented subsequently. 2.2.1 Penn Treebank. The original PTB (Marcus, Santorini, and Marcinkiewicz 1993) provided syntactic annotations in the form of parse trees for text from the Wall Street Journal. This resource is very popular in computational linguistics, particularly for inducing part-of-speech taggers and parsers. PTB version II (Marcus et al. 1994) added 20 functional tags, including a few thematic roles such as temporal, direction, and purpose. These can be attached to any verb complement but normally occur with clauses, adverbs, and prepositions. For example, Figure 1 shows a parse tree using the extended annotation format. In addition to the usual syntactic constituents such as NP and VP, function tags are included. For example, the second NP gives the subject. This also shows that the ﬁrst prepositional phrase (PP) indicates the time frame, whereas the last PP indicates the  Sentence: In 1982, Sports & Recreation’s managers and certain passive investors purchased the company from Brunswick Corp. of Skokie, Ill.  Parse: (S (PP-TMP In (NP 1982)), (NP-SBJ (NP (NP (NP Sports) & (NP Recreation) ’s) managers) and (NP certain passive investors)) (VP purchased (NP the company) (PP-CLR from (NP (NP Brunswick Corp.) (PP-LOC of (NP (NP Skokie) , (NP Ill))) ))) .)  temporal extent grammatical subject closely related locative  Figure 1 Penn Treebank II parse tree annotation sample. The functional tags are shown in boldface.  154  O’Hara and Wiebe  Exploiting Resources for Preposition Disambiguation  Table 1 Frequency of Penn Treebank II semantic role annotations. Relative frequencies estimated over the counts for unique assignments given in the PTB documentation (bkt tags.lst), and descriptions based on Bies et al. (1995). Omits low-frequency benefactive role. The syntactic role annotations generally have higher frequencies; for example, the subject role occurs 49% of the time (out of about 240,000 total annotations).  Role  Freq.  Description  temporal .113 indicates when, how often, or how long  locative .075 place/setting of the event  direction .026 starting or ending location (trajectory)  manner .021 indicates manner, including instrument  purpose .017 purpose or reason  extent  .010 spatial extent  location. The second PP is tagged as closely-related, which is one of the miscellaneous PTB function tags that are more syntactic in nature: “[CLR] occupy some middle ground between arguments and adjunct” (Bies et al. 1995). Frequency information for the semantic role annotations is shown in Table 1. 2.2.2 FrameNet. FrameNet (Fillmore, Wooters, and Baker 2001) is striving to develop an English lexicon with rich case structure information for the various contexts that words can occur in. Each of these contexts is called a frame, and the semantic relations that occur in each frame are called frame elements (FE). For example, in the communication frame, there are frame elements for communicator, message, medium, and so forth. FrameNet annotations occur at the phrase level instead of the grammatical constituent level as in PTB. Figure 2 shows an example. Table 2 displays the top 25 semantic roles by frequency of annotation. This shows that the semantic roles in FrameNet can be quite speciﬁc, as with the roles cognizer, evaluee, and addressee. In all, there are over 780 roles annotated with over 288,000 tagged instances. Sentence: Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling standalone workstations to communicate over public or private ISDN networks. Annotation: Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling C FE=“Communicator” PT=“NP” standalone workstations /C to C TARGET=“y” communicate /C C FE=“Medium” PT=“PP” over public or private ISDN networks /C . Figure 2 FrameNet annotation sample. The constituent (C) tags identify the phrases that have been annotated. The frame element (FE) attributes indicate the semantic roles, and the phrase type (PT) attributes indicate the traditional grammatical category for the phrase. For simplicity, this example is formatted in the earlier FrameNet format, but the information is taken from the latest annotations (lu5.xml). 155  Computational Linguistics  Volume 35, Number 2  Table 2 Common FrameNet semantic roles. The top 25 of 773 roles are shown, representing nearly half of the total annotations (about 290,000). Descriptions based on FrameNet 1.3 frame documentation.  Role  Freq.  Description  agent  .037 person performing the intentional act  theme  .031 object being acted on, affected, etc.  experiencer  .029 being who has a physical experience, etc.  goal  .028 endpoint of the path  speaker  .028 individual that communicates the message  stimulus  .026 entity that evokes response  manner  .025 manner of performing an action, etc.  degree  .024 degree to which event occurs  self-mover  .023 volitional agent that moves  message  .021 the content that is communicated  path  .020 the trajectory of motion, etc.  cognizer  .018 person who perceives the event  source  .017 the beginning of the path  time  .016 the time at which the situation occurs  evaluee  .016 thing about which a judgment has been made  descriptor  .015 attributes, traits, etc. of the entity  body-part  .014 location on the body of the experiencer  content  .014 situation or state-of-affairs that attention is focused on  topic  .014 subject matter of the communicated message, etc.  item  .012 entity whose scalar property is speciﬁed  target  .011 entity which is hit by a projectile  garment  .011 clothing worn  addressee  .011 entity that receives a message from the communicator  protagonist  .011 person to whom a mental property is attributed  communicator .010 the person who communicates a message  2.3 Other A recent semantic role resource that is starting to attract interest is the Proposition Bank (PropBank), developed at the University of Pennsylvania (Palmer, Gildea, and Kingsbury 2005). It extends the Penn Treebank with information on verb subcategorization. The focus is on annotating all verb occurrences and all their argument realizations that occur in the Wall Street Journal, rather than select corpus examples as in FrameNet. Therefore, the role inventory is heavily verb-centric, for example, with the generic labels arg0 through arg4 denoting the main verbal arguments to avoid misinterpretations. Verbal adjuncts are assigned roles based on PTB version II (e.g., argM-LOC and argMTMP). PropBank has been used as the training data in recent semantic role labeling competitions as part of the Conferences on Computational Natural Language Learning (Carreras and Ma`rquez 2004, 2005). Thus, it is likely to become as inﬂuential as FrameNet in computational semantics. The Preposition Project similarly adds information to an existing semantic role resource, namely FrameNet. It is being developed by CL Research (Litkowski and Hargraves 2006) and endeavors to provide comprehensive syntactic and semantic information on various usages of prepositions, which often are not represented well in semantic lexicons (e.g., they are not included at all in WordNet). The Preposition Project uses the sense distinctions from the Oxford Dictionary of English and integrates syntactic information about prepositions from comprehensive grammar references. 156  O’Hara and Wiebe  Exploiting Resources for Preposition Disambiguation  2.4 Inventories for Knowledge Representation This section describes three case inventories: one developed for the Cyc KB (Lenat 1995), one used to deﬁne Conceptual Graphs (Sowa 1984), and one for the Factotum KB (Cassidy 2000). The ﬁrst two are based on a traditional knowledge representation paradigm. With respect to natural language processing, these approaches are more representative of the earlier approaches in which deep understanding is the chief goal. Factotum is also based on a knowledge representation paradigm, but in a sense also reﬂects the empirical aspect of the corpus annotation approach, because the annotations were developed to address the relations implicit in Roget’s Thesaurus. In this article, relation disambiguation experiments are only presented for Factotum, given that the others do not readily provide sufﬁcient training data. However, the other inventories are discussed because each provides relation types incorporated into the inventory used below for the deﬁnition analysis (see Section 3.5). 2.4.1 Cyc. The Cyc system (Lenat 1995) is the most ambitious knowledge representation project undertaken to date, in development since 1984. The full Cyc KB is proprietary, which has hindered its adoption in natural language processing. However, to encourage broader usage, portions of the KB have been made freely available to the public. For instance, there is an open-source version of the system called OpenCyc (www.opencyc.org), which covers the upper part of the KB and also includes the Cyc inference engine, KB browser, and other tools. In addition, researchers can obtain access to ResearchCyc, which contains most of the KB except for proprietary information (e.g., internal bookkeeping assertions). Cyc uses a wide range of role types: very general roles (e.g., beneﬁciary); commonly occurring situational roles (e.g., victim); and highly specialized roles (e.g., catalyst). Of the 8,756 concepts in OpenCyc, 130 are for event-based roles (i.e., instances of actorslot) with 51 other semantic roles (i.e., other instances of role). Table 3 shows the most commonly used event-based roles in the KB. 2.4.2 Conceptual Graphs. The Conceptual Graphs (CG) mechanism was introduced by Sowa (1984) for knowledge representation as part of his Conceptual Structures theory. The original text listed two dozen or so thematic relations, such as destination and initiator. In all, 37 conceptual relations were deﬁned. This inventory formed the basis for most work in Conceptual Graphs. Recently, Sowa (1999) updated the inventory to allow for better hierarchical structuring and to incorporate the important thematic roles identiﬁed by Somers (1987). Table 4 shows a sample of these roles, along with usage estimates based on corpus analysis (O’Hara 2005). 2.4.3 Factotum. The Factotum semantic network (Cassidy 2000) developed by Micra, Inc., makes explicit many of the relations in Roget’s Thesaurus.1 Outside of proprietary resources such as Cyc, Factotum is the most comprehensive KB with respect to functional relations, which are taken here to be non-hierarchical relations, excluding attributes. OpenCyc does include deﬁnitions of many non-hierarchical relations. However, there are not many instantiations (i.e., relationship assertions), because it concentrates on the higher level of the ontology.  
Prepositions are an important and frequently used category in both English and Romance languages. In a corpus study of one million English words, Fang (2000) shows that one in ten words is a preposition. Moreover, about 10% of the 175 most frequent words in a corpus of 20 million Spanish words were found to be prepositions (Almela et al. 2005). Studies on language acquisition (Romaine 1995; Celce-Murcia and LarsenFreeman 1999) have shown that the acquisition and understanding of prepositions in languages such as English and Romance is a difﬁcult task for native speakers, and even more difﬁcult for second language learners. For example, together with articles, prepositions represent the primary source of grammatical errors for learners of English as a foreign language (Gocsik 2004). ∗ Linguistics and Computer Science Departments, University of Illinois at Urbana-Champaign, Urbana, IL 61801. E-mail: girju@illinois.edu. Submission received: 1 August 2006; revised submission received: 20 January 2008; accepted for publication: 17 March 2008. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 2  Although the complexity of preposition usage has been argued for and documented by various scholars in linguistics, psycholinguistics, and computational linguistics, very few studies have been done on the function of prepositions in natural language processing (NLP) applications. The reason is that prepositions are probably the most polysemous category and thus, their linguistic realizations are difﬁcult to predict and their cross-linguistic regularities difﬁcult to identify (Saint-Dizier 2005a). In this article we investigate the role of prepositions in the task of automatic semantic interpretation of English nominal phrases and compounds. The problem is simple to deﬁne: Given a compositional noun phrase (the meaning of the phrase derives from the meaning of the constituents) constructed out of a pair of nouns, N1 N2, one representing the head and the other the modiﬁer, determine the semantic relationship between the two nouns. For example, the noun–noun compound family estate encodes a POSSESSION relation, while the nominal phrase the faces of the children refers to PART-WHOLE. The problem, although simple to state, is difﬁcult for automatic semantic interpretation. The reason is that the meaning of these constructions is most of the time implicit (it cannot be easily recovered from morphological analysis). Interpreting nominal phrases and compounds correctly requires various types of information, from world knowledge to lexico-syntactic and discourse information. This article focuses on nominal phrases of the type N P N and noun compounds (N N) and investigates the problem based on cross-linguistic evidence from a set of six languages: English, Spanish, Italian, French, Portuguese, and Romanian. The choice of these constructions is empirically motivated. In a study of 6,200 (Europarl1) and 2,100 (CLUVI2) English token nominal phrase and compound instances randomly chosen from two English–Romance parallel text collections of different genres, we show that over 80% of their Romance noun phrase translations are encoded by N P N and N N constructions. For instance, beer glass, an English compound of the form N1 N2, translates into N2 P N1 instances in Romance: tarro de cerveza (‘glass of beer’) in Spanish, bicchiere da birra (‘glass for beer’) in Italian, verre a` bie`re (‘glass at/to beer’) in French, copo de cerveja (‘glass of beer’) in Portuguese, and pahar de bere (‘glass of beer’) in Romanian. In this article, in addition to the sense translation (in italics), when relevant we also provide the word-by-word gloss (in ‘parentheses’). Moreover, we use N1, N2 to denote the two lexical nouns that encode a semantic relation (where N1 is the syntactic modiﬁer and N2 is the syntactic head), and Arg1, Arg2 to denote the semantic arguments of the relation encoded by the two nouns. For example, beer glass encodes a PURPOSE relation where Arg1 (beer) is the purpose of Arg2 (‘glass’; thus ‘glass (used) for beer’). We argue here that the syntactic directionality given by the head-modiﬁer relation (N1 N2 in noun compounds and N2 P N1 in nominal phrases) is not always the same as the semantic directionality given by the semantic argument frame of the semantic relation. Otherwise said, N1 does not always map to Arg1 and N2 to Arg2 for any given relation. Languages choose different nominal phrases and compounds to encode relationships between nouns. For example, English nominal phrases and compounds of the  
The article describes a pilot implementation of a grammar containing different types of locative PPs. In particular, we investigate the distinction between static and directional locatives, and between different types of directional locatives. Locatives may act as modiﬁers as well as referring expressions depending on the syntactic context. We handle this with a single lexical entry. The implementation is of Norwegian locatives, but English locatives are both discussed and compared to Norwegian locatives. The semantic analysis is based on a proposal by Markus Kracht (2002), and we show how this analysis can be incorporated into Minimal Recursion Semantics (MRS) (Copestake et al. 2005). We discuss how the resulting system may be applied in a transferbased machine translation system, and how we can map from a shallow MRS representation to a deeper semantic representation. 1. Introduction Locative prepositional phrases (PPs) pose several puzzles, both to syntax and semantics. First, locatives may be either static (Example (1)), directional (Example (2)), or ambiguous (Example (3)): (1) Kim slept in Paris. (2) Kim is driving into Paris. (3) The mouse ran under the table. As we see, the PP under the table may locate the whole event (Example (4)), it may express the goal of motion (Example (5)), or it may express (parts of) the path of motion (Example (6)): (4) The mouse ran around under the table. (5) The mouse ran under the table and stayed there. ∗ ILN, PO Box 1102 Blindern, 0317 Oslo, Norway. E-mail: fredrik.jorgensen@iln.uio.no. ∗∗ IFI, PO Box 1080 Blindern, 0316 Oslo, Norway. E-mail: jtl@iﬁ.uio.no. Submission received: 29 June 2006; revised submission received: 16 November 2007; accepted for publication: 26 January 2008. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 2  (6) The mouse ran under the table into a hole in the wall.hole in the wall.  Second, we may use a combination of prepositions in order to express, for example, the source of motion, as seen in Example (7):  (7) A mouse appeared from under the table.  Here, under the table seems to refer to a location, and it is from that expresses the directional component of the locative. Third, we may see differences in the local interpretation of a PP and the same PP in a wider context. One popular example is the verb put, where both a static and a directional (goal) PP complement is accepted, and where both types of PP complements are interpreted as the goal of the motion.  (8) Kim put the book on/onto the table.  Fourth, a preposition may not always be followed by an argument; consider down in Example (9). When it occurs without an NP complement it may be followed by another PP as in Example (10). How can the semantics of this construction be described compositionally?  (9) A child ran down.  (10) A child ran down under the bridge.  Another example of this phenomenon is how intransitive locatives disambiguate between different readings in Norwegian. The locatives inne and inn are static and directional (goal), in relation to a location which is inside in some (contextually determined) sense. When one of these locatives is succeeded by a locative PP, the second PP is given the same interpretation with respect to motion, as shown in Examples (11) and (12).  (11) Musa  løp innestatic istatic hullet.  Mouse.DEF ran insidestatic instatic hole.DEF  ‘The mouse ran insidestatic the hole.’  (12) Musa  løp inngoal igoal hullet.  Mouse.DEF ran insidegoal ingoal hole.DEF  ‘The mouse ran into the hole.’  Furthermore, new readings are available when a PP is preceded by inn, as seen in Examples (13) and (14).  (13) Helikopteret ﬂøy overstatic or path byen. Helicopter.DEF ﬂew overstatic or path city.DEF. ‘The helicopter ﬂew overstatic or path the city.’ (14) Helikopteret ﬂøy inn overgoal byen. Helicopter.DEF ﬂew in overgoal city.DEF. ‘The helicopter ﬂew [to above]/[in over] the city.’  230  Jørgensen and Lønning  Minimal Recursion Semantic Analysis of Locatives  Finally, we see a variation in how different languages express locatives. Some languages have rich case systems, whereas others use adpositions to express locatives. The focus of this article is locative prepositions in Norwegian and English. We will not consider locative case systems here, but we will see differences in which locatives are lexicalized as prepositions, as shown in Example (15), where the Norwegian locative derfra is a lexicalization of the corresponding English complex PP from there. (15) Musa kom derfra. Mouse.DEF came there-from. ‘The mouse came from (over) there.’ Our goal in this article consists of three parts. First, we take recent developments within formal semantic approaches to locatives (in particular Kracht 2002) and show how these can be implemented in Minimal Recursion Semantics (MRS; Copestake et al. 2005), a ﬂat and computationally tractable semantic meta-language for ﬁrst-order logic. Second, we determine the degree to which the insights from this particular approach to locatives can be implemented in a non-transformational, uniﬁcation-based computational grammar based on HPSG (Pollard and Sag 1994). And third, we investigate what consequences the current analysis has for syntax and the syntax–semantics interface. Our implementation is of a fragment of Norwegian. Norwegian is in many respects similar to English, and where they are similar we will use examples from English. There are also some interesting differences, and we will use them to illustrate how the new representations can be exploited in an experimental semantic transfer-based machine translation system, like the LOGON system (Oepen et al. 2004), which uses transfer representations based on MRS. The rest of the article is organized as follows. In Section 2, we consider various approaches to locatives, before we describe the formal semantic approach proposed by Kracht (2002) and introduce MRS in Section 3. In Section 4, we start to present our own solution to how these two approaches can be combined. We proceed to classify Norwegian locative adverbs and prepositions and show how various constructions can be handled in Section 5. In Section 6, we describe the main results from our implementation of a grammar producing the intended Kracht-style MRS representations. In Section 7, we argue that the current analysis is useful for a range of NLP applications, and sketch how it can be applied in a machine translation system. Finally we compare our approach to other computational approaches in Section 8, discuss evaluation of the analysis in Section 9, and conclude in Section 10. 2. Syntax and Semantics of Locatives In this section, we consider relevant linguistic literature on locatives which will serve as a background for our proposal. We will return to a comparison between our approach and other computational linguistic approaches in Section 8. 2.1 Relevant Issues There is a growing literature on language and space, but not all of it is relevant for our purposes. We are interested in how locatives should be handled in a computational grammar with a compositional semantic component, and in particular how static and 231  Computational Linguistics  Volume 35, Number 2  directional locatives are treated. There are basically three types of questions relevant to our article, for which we will review relevant linguistic literature: Q1. Formal Semantics: What do locatives denote? Are locatives referential (denoting a location) or modiﬁcational (denoting a property of events and entities)? Q2. Syntax: Are locative PPs complements or adjuncts? If we want our grammar to have compositional semantics, this question is closely related to the previous one. Analyzing locative PPs as adjuncts is hard to combine with the referential interpretation of locatives, as this either requires that the modiﬁed phrase also denotes a location, or that some additional machinery is introduced to map the location to a property. Q3. How is the distinction between static and directional locatives handled? Static and directional locatives differ both in syntactic distribution and truth conditions, and as we have observed, occurrences of locatives may be ambiguous between the two. A prominent question in the literature on locative prepositions is the geometric structure of particular prepositions: for example, what geometric relationship between the ball and the cup has to be realized for an expression like the ball is in the cup to be true (e.g., Herskovits 1986)? As interesting as these questions are, we think they belong to the lexicon and the lexical semantic domain and fall outside the scope of this article. There is also interesting work which relates the meaning of individual prepositions to various types of psychological studies; in particular, studies of under which conditions the various prepositions are used (see, e.g., many of the references in Bloom et al. [1996] and in van der Zee and Slack [2003]). But again this plays a complementary role to our work. One particular question in the lexical semantics of prepositions is the distinction between what Zwarts and Winter (2000), following Herskovits (1986), call projective and non-projective prepositions. A non-projective preposition, like outside, requires only spatial knowledge of the location of the two objects, while a projective preposition, like behind, requires some further information about directions from the reference object. Levinson (1996) makes a further distinction between three types of reference frames called (i) intrinsic, where behind the house means on the other side of the house than what would be classiﬁed as the front side of the house, (ii) extrinsic, where behind the house is on the opposite side of the house than the speaker, and (iii) absolute, which applies to expressions like north of the house. We also think that these distinctions belong to the lexical semantic domain.  2.2 Hjelmslev’s Theory of Cases One of the earliest contributions to the study of locatives is Hjelmslev’s (1935) theory of cases. Hjelmslev views case as a relation between two objects (Q1 above), which may be nominal or verbal (Q1/Q2). Cases may be either complements or modiﬁers (Q2), and are expressed through adpositions or inﬂections. These observations are based on data from a wide range of languages, including languages with locative case systems, such as Hungarian and the Caucasian language Tabasaran. Hjelmslev proposes a threedimensional case system (Q3), where the dimensions express directionality, coherence, and subjectivity. The directional dimension consists of approach, neutrality, and sep- 232  Jørgensen and Lønning  Minimal Recursion Semantic Analysis of Locatives  aration. The coherence dimension has two interpretations, inherence (enclosure) and adherence (contact). And ﬁnally, the subjectivity dimension describes the dependence relation between the point of view and the location, being either subjective, objective, or neutral. For example, whereas behind or left of is dependent on the point of view, under is not, according to Hjelmslev. Hjelmslev’s theory of cases relates to all three questions, and it has formed the basis of much of the later work done on locatives. Even though we will focus on languages where locatives are expressed by prepositions in adpositions, and ignore languages where locatives are expressed through inﬂectional case, it is a point for us to base our approach on formalisms that also extend to languages where locatives are expressed by case. When it comes to the dimensions, directionality is particularly important for the questions we raised in the introduction, while the latter two dimensions relate to lexical semantics, which we will not pursue here. 2.3 Bierwisch’s Grammar of Locatives Bierwisch (1988) gives a detailed account of both syntactic and semantic aspects of locative prepositions in German, and is particularly interested in the relationship between the denotation of locatives (Q1) and the consequences for the syntactic treatment of locatives (Q2). Bierwisch notes that locative PPs serve as predicates (Example (16)), arguments (Example (17)), and modiﬁcational adjuncts (Examples (18)–(19)) (examples taken from [1988, page 5]; our translations). (16) Er ist in der Schule. He is in school. (17) Der Brief liegt auf dem Tisch. The letter is lying on the table. (18) Ich kaufe das Buch in Berlin. I will buy the book in Berlin. (19) eine Bru¨ cke u¨ ber die Moldau a bridge over Moldau Bierwisch notes that even though predication, complementation, and modiﬁcation are clearly distinct syntactic relations, distinguishing between locatives as adverbial or optional complements is in many cases arbitrary (Q2). Bierwisch argues that directionality is a syntactic feature, ±DIR, on locatives. But as we shall see, a more complex inventory of syntactic features corresponding to different types of directionality is needed. Bierwisch (1988) discusses two different positions with respect to the denotations of locative PPs (Q1): The referential interpretation, where locative PPs denote regions, just like NPs denote things, and the modiﬁcational interpretation, where the locative PPs denote properties of being located at a certain place: The referential interpretation seems to be appropriate for PPs in argument position, as e.g., Hans liegt im Bett can plausibly be said to express a relation between Hans and a place denoted by im Bett. It is difﬁcult to see, however, how on this account PPs can serve as modiﬁers or predicatives — unless a place is construed as a property, but that would violate the gist of the referential interpretation. The modiﬁcational interpretation, on the other hand, concerns itself with PPs as adjuncts, but seems to be 233  Computational Linguistics  Volume 35, Number 2  in trouble with PPs in argument position. From this, one might be tempted to draw the conclusion that both interpretations are partially right and that they both are needed. (Bierwisch 1988, page 8) We will not discuss predicatives in this article, but refer to Kracht (2002) for treatment of adnominal locatives as a property of individuals. The duality with respect to the interpretation of locatives as referential or modiﬁcational is central to our article, and we will show, building on Kracht, how this can be handled in a computational grammar.  2.4 Jackendoff’s Conceptual Semantics  Ray Jackendoff’s (1983, 1990) conceptual semantics is a decompositional theory of meaning, heavily inﬂuenced by X-bar theory. Conceptual semantics organizes a repertoire of major conceptual categories, the semantic parts of speech, into function– argument structures. Lexical entries are encoded as Lexical Conceptual Structures (LCSs), as exempliﬁed by the entry for the preposition in in (Example (20)), where we ﬁnd the lexeme, the part-of-speech category, the selectional restriction, and the semantics on four separate lines. Particularly interesting in our context is the semantic analysis of static and directional locatives (Q1/Q3), and how they combine with verbs (Q2). The LCSs of static locatives are claimed to have one layer, a Place function (Example (20)), as opposed to the LCS of directional locatives, which have two layers, a Path function as the outer function, and a Place function as the inner function (Example (21)). According to Jackendoff, there are ﬁve different Path functions: TO, FROM, TOWARD, AWAY-FROM, and VIA. These map from a Place to a Path.   (20) in PNPj [Place IN ( [Thing    ]j) ]   (21) into PNPj [Path TO ( [Place IN ( [Thing    ]j) ] ) ]  Furthermore, Jackendoff (1990, page 45) treats directional locatives as arguments to motion verbs, such that, for example, run subcategorizes for an optional directional locative argument (Q2), where the optionality is represented with angle brackets, note the LCS for run given in Example (22). In the analysis of Example (23) the optional PP argument of run is coindexed with the PP into the room. Through co-indexing (j) this results in the conceptual structure in Example (24), where the path denoted by the PP enters into the function GO(subject entity, path) and thereby expresses the traversing of the path. By convention, the entity indexed i is taken to be the subject position.   (22) run V PPj [Event GO ( [Thing  ]i ,[Path    ]j) ]  234  Jørgensen and Lønning  Minimal Recursion Semantic Analysis of Locatives  (23) [S [NP John [VP ran [PP into [NP the room ]]]] (24) [Event GO ( [Thing JOHN ], [Path TO ( [Place IN ( [Thing ROOM ]) ]) ]) ] Jackendoff’s description clearly shows that matters are more complex than the usual static/directional distinction. First, he introduces a ﬁner classiﬁcation between the different directionals. Second, he proposes there be a common core, the place, to different prepositions (e.g., in and into) and to the different uses of a preposition (e.g., the placeuse and the TO- and VIA-uses of under). Both observations will be part of our analysis. But there are several problems with Jackendoff’s approach. First, one will have to specify for any verb which can be modiﬁed by a directional PP, that the modiﬁer is an optional part of the verb’s semantics (or LCS). Because one and the same verb (e.g., run) can be combined with several directional PPs at the same time, as in Example (6), one would need a way to combine the semantics of the different locative PPs. Whether this should be done by merging the directionals into a single path, or by accepting multiple PP complements, is unclear in Jackendoff’s theory. We believe incorporating the optional modiﬁer’s semantics into the verb’s lexical entry leads to a very elaborate and inﬂexible semantics, and would prefer to avoid this, if possible. The second, and more serious, weakness is that static and directional locatives are treated differently in the syntax. Static locatives are regarded as adjuncts, whereas directional locatives are regarded as complements of motion verbs. Unless there is provided syntactic evidence for this distinction, this violates the autonomy of syntax, as the semantics of the construction (whether it is the event or its participants being located) governs the syntactic analysis (whether the construction is analyzed as an adjunct or a complement). There are of course different syntactic restrictions on what kind of verbs static and directional PPs can modify, but this applies to many modifying PPs, and is not in itself sufﬁcient to warrant different syntactic analyses. We believe that a uniform treatment of locatives, giving the same syntactic analysis independent of directionality, would provide a more sound and consistent analysis. 2.5 Fong’s Directionals as Diphasic Events Fong (1997) analyzes directional locatives. She focuses on the locative cases of Finnish, but includes comparisons to other languages, including English. Her main focus of interest is on how directional locative case in Finnish and directional prepositions in English can be used when there is no movement involved. In Finnish one uses expressions which literally when translated into English would become forgot my wallet into my car, and in English we have formulations like a bridge from Buda to Pest. Whereas other approaches take the spatial use as basic, and consider the more abstract uses as derived, Fong takes the opposite stand. The locative case is not in itself about paths. It has a more abstract meaning, and can be used when there is an underlying ordered domain, together with a proposition changing truth value, either from false to true or from true to false along the ordering. This abstract structure can be mapped to a more concrete structure, including space in English, and space or time in Finnish. Thus in Example (25) there is an ordering in space such that Example (26) is false at the beginning of this ordering, and true towards the end of the ordering. (25) He went into the room. (26) He is in the room. 235  Computational Linguistics  Volume 35, Number 2  Kracht (2002) compares his approach to Fong’s. One problem is that Fong’s approach does not extend to the VIA-reading (in Jackendoff’s terminology) of sentences like Example (3). From our point of view, it is also a point that she does not describe a compositional analysis. Also our focus in this article is strictly on the spatial readings of the locatives. 2.6 Zwarts’s Vector Space Semantics In a series of papers, Zwarts (1997, 2003a, 2003b, 2005; Zwarts and Winter 2000) has developed a semantics for locatives based on vectors. A main motivation for this proposal is the observation that locatives may be modiﬁed by phrases expressing distance or direction; for example, a measure phrase (ten meters), an adverb (diagonally), or a dimensional adjective ( far), as seen in Example (27). (27) Superman is far/diagonally/ten meters above the building. Many approaches to locatives will consider above the building to denote a region and let this be a set of points. It is then not obvious how the modiﬁer ten meters can apply to this region. Zwarts and Winter (2000) propose instead to let the locative PP denote a set of vectors (Q1). Vectors are directed line segments between points in space. Loosely speaking, we may say that above the building denotes the set of vertical vectors starting in the building and ending above it. By then letting ten meters denote the vectors having a length of ten meters, the modiﬁcation can be considered set intersection, and the result are all vectors of length ten meters starting in the building and ending vertically above it. Syntactically, Zwarts and Winter consider locatives to be modifying adjuncts. They mainly consider how nouns are modiﬁed. Because nouns denote sets of individuals, and locatives denote sets of vectors, they assume a primitive semantic function loc which maps individuals to their regions as sets of points (or vectors). This is used for the landmark (e.g., the building in the locative PP above the building). When a locative modiﬁes a noun, the inverse function is used, such that the set of vectors is turned into the set of objects which are located within the set of (endpoints of) the vectors. Even though this approach seems to solve the semantic problem of this particular type of construction, we are not sure how general it can be made. The compositional analysis of Zwarts and Winter (2000) is applied to modiﬁcation of nouns and to predicative uses of locatives; they do not consider verbal modiﬁcation. We can see how the approach can be adopted to verbal modiﬁcation of static locatives in an event based framework by introducing a function loc e, mapping events to regions, but we do not see how it extends to directional locatives. In later articles, Zwarts (2003b, 2005) extend the approach to the directional PPs. A directional PP does not denote a set of vectors, but a set of paths. A path can be considered a sequence of vectors. For example, in John went to the store, the path John followed can be formalized as a sequence of vectors, {vi|i ∈ I} for an ordered set I, where each vi has the store as its starting point, where John successfully traverses their endpoints, and where the last vector also has the store as its endpoint. Intuitively this is a correct description, but Zwarts (2003b, 2005) does not show how this can be handled in a compositional grammar. It is a particular problem for a compositional treatment that static PPs and directional PPs are ascribed denotations of different types, unless they are also given different syntactic analyses (cf. discussion on Jackendoff in Section 2.4).  236  Jørgensen and Lønning  Minimal Recursion Semantic Analysis of Locatives  2.7 Kracht’s Two-Layered Locatives  In Kracht (2002), a novel approach to the semantics of locatives is described. Kracht presents a solution to Bierwisch’s (1988) problem of the duality of locatives, by claiming that locatives consist of two layers (Q1/Q2). The lower layer, called a localizer phrase (LP), deﬁnes a region or location, while the higher layer, called a modalizer phrase (MP), deﬁnes events of motion with respect to this region (Q1). Both from a semantic and syntactic point of view, Kracht claims a locative expression is structured as [MP M [LP L NP]]. Here, L is a localizer which together with the NP forms an LP whose semantic contribution is a location. M is a modalizer which together with the LP forms an MP, whose semantic contribution is an event modiﬁer. The analysis is motivated on the one hand by the dualistic nature of locative semantics, and on the other hand by syntactic and morphological data. Locatives are realized differently in different languages. Some languages use pre- or postpositions, whereas other languages use locative case. In particular, case languages like Finnish, Hungarian, and Tsez realize the two different layers as different sufﬁxes. Still, whether the locative is expressed by an adposition or case, the M+L most often form a unit. This is by far the most frequent case for English, meaning that the preposition under in Example (3) is analyzed as shown in Example (28).  (28) MP  M+L  NP  under  ...  In English, the two-layering is usually not explicit, but it is found in constructions like Example (7), corresponding to:  (29) MP  M  LP  L  NP  from under  ...  MPs denote sets of events, and their syntactic function is that of adverbial modiﬁcation. The data Kracht discusses give support for ﬁve different types of modalizers or modes, which are the heads of the MPs, denoting ﬁve different types of events.1 Thus, if Kracht is right, the modes are true semantical interlingua predicates. The modes are:  1. static (st), corresponding to Jackendoff’s Place Function 2. coinitial (ci), corresponding to Jackendoff’s Path Function FROM 3. coﬁnal (cf), corresponding to Jackendoff’s Path Function TO  
This article describes the application of computational models of spatial prepositions to visually situated dialog systems. In these dialogs, spatial prepositions are important because people often use them to refer to entities in the visual context of a dialog. We ﬁrst describe a generic architecture for a visually situated dialog system and highlight the interactions between the spatial cognition module, which provides the interface to the models of prepositional semantics, and the other components in the architecture. Following this, we present two new computational models of topological and projective spatial prepositions. The main novelty within these models is the fact that they account for the contextual effect which other distractor objects in a visual scene can have on the region described by a given preposition. We next present psycholinguistic tests evaluating our approach to distractor interference on prepositional semantics, and illustrate how these models are used for both interpretation and generation of prepositional expressions. 1. Introduction A growing number of computer applications share a visualized (virtual or real) space with the user, for example graphic design programs, computer games, navigation aids, robot systems, and so forth. If these systems are to be equipped with dialog interfaces, they must be able to participate in visually situated dialog. Visually situated dialog is spoken from a particular point of view within a physical or simulated context. From theoretical linguistic and cognitive perspectives, visually situated dialog systems are interesting as they provide ideal testbeds for investigating the interaction between language and vision. From a human–computer interaction (HCI) perspective, visually situated dialog systems promise many advantages to users interacting with these systems. In this article we describe computational models for the interpretation and generation of visually situated locative expressions involving topological and projective spatial prepositions. Contributions An inherent aspect of visually situated dialog is reference to objects in the physical environment in which the dialog occurs. People often use locative ∗ School of Computing, Dublin Institute of Technology, Kevin Street, Dublin 8, Ireland. E-mail: john.kelleher@comp.dit.ie. ∗∗ School of Computer Science and Informatics, University College Dublin, Belﬁeld, Dublin 4, Ireland. E-mail: ﬁntan.costello@ucd.ie. Submission received: 31 July 2006; revised submission received: 30 March 2007; accepted for publication: 4 July 2007. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 2  expressions, in particular spatial prepositions, to pick out objects in the visual environment. In this article we present computational models of the semantics of spatial prepositions and illustrate how these models can be used in a visually situated dialog system for reference resolution and generation. These models are designed to handle reference resolution and generation in complex visual environments containing multiple objects, and to account for the contextual inﬂuence which the presence of multiple objects has on the semantics of spatial prepositions. In this our models move beyond other accounts, which typically do not model the contextual inﬂuence of other objects on spatial semantics. Because most real-world visual scenes are complex and contain multiple objects, our models for the semantics of spatial prepositions are important for visually situated dialog systems intended to operate usefully in the real world. Overview We begin in Section 2 by describing some terminology we use when discussing locative expressions. In Section 3 we present an abstract architecture for a visually situated dialog system and, using this architecture, illustrate how the spatial reasoning component of the architecture interacts with the other components of the system. In Section 4 we review psycholinguistic data on the semantics of spatial prepositions. Section 5 reviews previous computational models of spatial prepositional semantics. Section 6 presents our computational models accounting for the semantics of spatial prepositions and the inﬂuence of visual context on those semantics, and Section 7 presents psycholinguistic evaluation of these models. Section 8 presents applications of the models in implemented systems. Section 8.1 presents an application of our models to the interpretation of locative expressions, based on Kelleher, Kruijff, and Costello (2006), and Section 8.2 presents algorithms which use these models to generate locative expressions to identify objects in visual scenes from Kelleher and Kruijff (2006).  2. Terminology Our computational models are designed to interpret and generate locative expressions involving spatial prepositions. The term locative expression describes “an expression involving a locative prepositional phrase together with whatever the phrase modiﬁes (noun, clause, etc.)” (Herskovits 1986, page 7). In this article we use the term target (T) to refer to the object that is being located by a locative expression and the term landmark1 (L) to refer to the object relative to which the target’s location is described; see Example (1). We will use the term distractor to describe any object in the visual context that is neither the landmark nor the target.  Example 1 [The man]T near [the table]L. The English lexicon of spatial prepositions numbers above 80 members (not considering compounds such as right next to) (Landau 1996). Within this set a distinction can be made between static and dynamic prepositions: static prepositions primarily2 denote 
Introduction to Information Retrieval by Manning, Raghavan, and Schu¨ tze is an up-todate, thorough, and systematic introduction to information retrieval (IR) from a computer science perspective. Written as a textbook, its main audience is graduate and senior undergraduate students taking IR courses. The book will also be valuable to researchers in other computer science ﬁelds, such as computational linguistics, as well as to professional practitioners wishing to delve into the IR ﬁeld. The book is structured into 21 chapters, which gradually unfold the subject of information retrieval, starting with the fundamentals (such as Boolean retrieval, document indexing, vector-space model, and evaluation in IR) and moving on to more advanced topics (such as probabilistic models, XML retrieval, text classiﬁcation, machine learning for IR, document clustering, and Web retrieval). Pedagogical features of the book include short exercises at the end of each section and brief overviews of related research literature at the end of each chapter. Some of the major strengths of the book are its accessibility, clarity, and good balance between theory and practice. There are many concrete examples throughout the book that facilitate understanding of complex topics. Although the book covers a broad selection of the major established and emerging topics in IR, it largely bypasses two important subjects, in my opinion: natural language processing techniques in IR and interactive information retrieval. Although the authors refer to some research done in these areas in various chapters, they do not give them the same thorough treatment given to other topics in the book. To compensate, in the preface the authors provide references to the detailed coverage of these and some other topics in other textbooks. It also might have been useful if the authors introduced some specialized IR tasks, such as opinion retrieval or enterprise search, which might beneﬁt from more advanced NLP techniques. Chapter 1 gives a succinct and focused introduction to the main concepts in IR, such as term, index, document, query, recall, precision, and so on. It outlines the main principles of Boolean retrieval, brieﬂy criticizes it, and compares it to ranked retrieval. The authors also present a good real-world example of a commercial Boolean retrieval system. Chapter 2 provides a detailed discussion of the initial stages of the document indexing process that include tokenization, stemming and lemmatization, stopwords removal, and approaches to dealing with phrases at the indexing stage, namely bigram indexing and the use of positional indexes. In this chapter the authors discuss some linguistic aspects of these processes. For example, when examining tokenization, they discuss various morphological and other aspects of languages that complicate this process (e.g., hyphenation in English, compound nouns in German, and word  Computational Linguistics  Volume 35, Number 2  sequences in East Asian languages). A brief outline of word segmentation approaches is provided. The authors also give a very good summary of the key relevant research works in these areas at the end of the chapter. In Chapters 3, 4, and 5 the authors discuss data structures for indexes, index construction, and compression. These three chapters will most likely be of least interest to the computational linguistics community. However, two topics that might be of interest are the use of wildcard queries by users and spelling correction of queries, discussed in Chapter 3. In their discussion of wildcard queries, the authors examine only the use of wildcards in queries to represent different morphological variants of a word (e.g., American vs. British), the user’s uncertainty in the correct spelling of a word, and stemmed words (e.g., judicia* to represent both judicial and judiciary). It would have been interesting if the authors also discussed the use of wildcards to represent entire words, since some commercial search engines started to provide this functionality, allowing users to search for a phrase with a user-speciﬁed number of words in the middle (e.g., the use of ﬁne * me to represent ﬁne by me, ﬁne with me, and ﬁne for me). Chapters 6 and 7 introduce the fundamentals of ranked document retrieval, term frequency and inverse document frequency, and the vector-space model. The authors brieﬂy touch upon phrase queries, and how they can be handled by the vector-space model. Phrase or proximity-based retrieval is an important problem in IR, but unfortunately, the authors do not discuss in detail different approaches to proximity-based term weighting. Chapter 8 is devoted to evaluation in IR, and presents major evaluation frameworks, such as the Text Retrieval Conference (TREC) and classical evaluation measures, such as mean average precision, precision at different cutoff points, as well as the more recently developed measure NDCG (normalized discounted cumulative gain) for evaluation with graded (non-binary) relevance judgments. The penultimate section in the chapter discusses various approaches to presenting retrieved documents in the ranked list shown to the user, such as snippets, and query-independent and querybiased document summaries. Chapter 9 reviews relevance feedback and query expansion. Query expansion (QE) following relevance feedback is one of the most effective techniques in IR. The authors provide an overview of the main types of QE: local, whereby the query is modiﬁed on the basis of retrieved documents, and global, which is query independent. Among the local methods, they introduce here the classic Rocchio algorithm. Probabilistic approaches to query expansion following relevance feedback are discussed in detail in Chapter 11 after the authors introduce probabilistic models of IR. Query expansion following relevance feedback can be either automatic (AQE), whereby the system selects terms and adds them to the query, or interactive (IQE), whereby the selected terms are shown to the user for further selection. The authors only discuss AQE in the context of relevance feedback. Among the global QE methods, they mention the use of manual and automatically generated thesauri, as well as approaches to QE on the Web, such as suggestion of related queries. Chapter 10 discusses XML retrieval, including such topics as a vector-space model for XML retrieval and INEX (Initiative for the Evaluation of XML retrieval), the main evaluation framework for XML retrieval. Chapters 11 and 12 focus on probabilistic information retrieval and language modeling, respectively. Chapter 11 introduces the theoretical underpinnings of probabilistic IR models, and describes the Robertson and Spa¨rck Jones probabilistic model and the BM25 term weighting function. Chapter 12 starts by describing the basic approach to language modeling in IR and then reviews some of its extensions. 308  Book Review Chapters 13, 14, and 15 discuss approaches to text classiﬁcation, starting with naive Bayes classiﬁcation, and then moving on to vector-space classiﬁcation and support vector machines. All topics are presented in sufﬁcient detail, supplemented with references to the key papers in these areas. 
Universitat Polite`cnica de Vale`ncia Elsa Cubel† Universitat Polite`cnica de Vale`ncia Antonio Lagarda† Universitat Polite`cnica de Vale`ncia Jesu´ s Toma´s‡ Universitat Polite`cnica de Vale`ncia Juan-Miguel Vilar§ Universitat Jaume I  Oliver Bender∗∗ RWTH Aachen Jorge Civera† Universitat Polite`cnica de Vale`ncia Shahram Khadivi∗∗ RWTH Aachen Hermann Ney∗∗ RWTH Aachen Enrique Vidal† Universitat Polite`cnica de Vale`ncia  Current machine translation (MT) systems are still not perfect. In practice, the output from these systems needs to be edited to correct errors. A way of increasing the productivity of the whole translation process (MT plus human work) is to incorporate the human correction activities within the translation process itself, thereby shifting the MT paradigm to that of computer-assisted translation. This model entails an iterative process in which the human translator activity is included in the loop: In each iteration, a preﬁx of the translation is validated (accepted or amended) by the human and the system computes its best (or n-best) translation sufﬁx hypothesis to complete this preﬁx. A successful framework for MT is the so-called statistical (or pattern recognition) framework. Interestingly, within this framework, the adaptation of MT systems to the interactive scenario affects mainly the search process, allowing a great reuse of successful techniques and models. In this article, alignment templates, phrase-based models, and stochastic ﬁnite-state transducers are used to develop computer-assisted translation systems. These systems were assessed in a European project (TransType2) in two real tasks: The translation of printer manuals; manuals and the translation of the Bulletin of the European Union. In each task, the following three pairs of languages were involved (in both translation directions): English–Spanish, English–German, and English–French.  ∗ Departament d’Enginyeria i Cie`ncies dels Computadors, Universitat Jaume I, 12071 Castello´ de la Plana, Spain. ∗∗ Lehrstuhl fu¨ r Informatik VI, RWTH Aachen University of Technology, D-52056 Aachen, Germany. † Institut Tecnolo` gic d’Informa`tica, Departament de Sistemes Informa`tics i Computacio´ , Universitat Polite`cnica de Vale`ncia, 46071 Vale`ncia, Spain. ‡ Institut Tecnolo` gic d’Informa`tica, Departament de Comunicacions, Universitat Polite`cnica de Vale`ncia, 46071 Vale`ncia, Spain. § Departament de Llenguatges i Sistemes Informa`tics, Universitat Jaume I, 12071 Castello´ de la Plana, Spain. Submission received: 1 June 2006; revised submission received: 20 September 2007; accepted for publication: 19 December 2007.  © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 1  1. Introduction to Computer-Assisted Translation Research in the ﬁeld of machine translation (MT) aims to develop computer systems which are able to translate text or speech without human intervention. However, present translation technology has not been able to deliver fully automated high-quality translations. Typical solutions to improving the quality of the translations supplied by an MT system require manual post-editing. This serial process prevents the MT system from taking advantage of the knowledge of the human translator, and the human translator cannot take advantage of the adaptive ability of the MT system. An alternative way to take advantage of the existing MT technologies is to use them in collaboration with human translators within a computer-assisted translation (CAT) or interactive framework (Isabelle and Church 1997). Historically, CAT and MT have been considered different but close technologies (Kay 1997) and more so for one of the most popular CAT technologies, namely, translation memories (Bowker 2002; Somers 2003). Interactivity in CAT has been explored for a long time. Systems have been designed to interact with human translators in order to solve different types of (lexical, syntactic, or semantic) ambiguities (Slocum 1985; Whitelock et al. 1986). Other interaction strategies have been considered for updating user dictionaries or for searching through dictionaries (Slocum 1985; Whitelock et al. 1986). Speciﬁc proposals can be found in Tomita (1985), Zajac (1988), Yamron et al. (1993), and Sen, Zhaoxiong, and Heyan (1997), among others. An important contribution to CAT technology, carried out within the TransType project, is worth mentioning (Foster, Isabelle, and Plamondon 1997; Langlais, Foster, and Lapalme 2000; Foster 2002; Langlais, Lapalme, and Loranger 2002). It entailed an interesting focus shift in which interaction is directly aimed at the production of the target text, rather than at the disambiguation of the source text, as in earlier interactive systems. The idea proposed in that work was to embed data-driven MT techniques within the interactive translation environment. The hope was to combine the best of both paradigms: CAT, in which the human translator ensures high-quality output, and MT, in which the machine ensures a signiﬁcant gain in productivity. Following these TransType ideas, the innovative embedding proposed here consists in using a complete MT system to produce full target sentence hypotheses, or portions thereof, which can be accepted or amended by a human translator. Each correct text segment is then used by the MT system as additional information to achieve further, hopefully improved, suggestions. More speciﬁcally, in each iteration, a preﬁx of the target sentence is somehow ﬁxed by the human translator and, in the next iteration, the system predicts a best (or n-best) translation sufﬁx(es)1 to complete this preﬁx. We will refer to this process as interactive-predictive machine translation (IPMT). This approach introduces two important requirements: First, the models have to provide adequate completions and, second, this has to happen efﬁciently. Taking these requirements into account, stochastic ﬁnite-state transducers (SFSTs), alignment templates (ATs), and phrase-based models (PBMs) are compared in this work. In previous works these models have proven adequate for conventional MT (Vidal 1997; Amengual et al. 2000; Ney et al. 2000; Toma´s and Casacuberta 2001; Och and Ney 2003; Casacuberta and Vidal 2004; Och and Ney 2004; Vidal and Casacuberta 2004). This article shows that  
University of Essex  Chris Mellish∗∗ University of Aberdeen Jon Oberlander‡ University of Edinburgh  In this article we discuss several metrics of coherence deﬁned using centering theory and investigate the usefulness of such metrics for information ordering in automatic text generation. We estimate empirically which is the most promising metric and how useful this metric is using a general methodology applied on several corpora. Our main result is that the simplest metric (which relies exclusively on NOCB transitions) sets a robust baseline that cannot be outperformed by other metrics which make use of additional centering-based features. This baseline can be used for the development of both text-to-text and concept-to-text generation systems. 1. Introduction Information ordering (Barzilay and Lee 2004), that is, deciding in which sequence to present a set of preselected information-bearing items, has received much attention in recent work in automatic text generation. This is because text generation systems need to organize the content in a way that makes the output text coherent, that is, easy to read and understand. The easiest way to exemplify coherence is by arbitrarily reordering the sentences of a comprehensible text. This process very often gives rise to documents that do not make sense although the information content is the same before and after the reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000). Entity coherence, which is based on the way the referents of noun phrases (NPs) relate subsequent clauses in the text, is an important aspect of textual organization. Since the early 1980s, when it was ﬁrst introduced, centering theory has been an inﬂuential framework for modelling entity coherence. Seminal papers on centering such as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and Weinstein (1995, page 215) suggest that centering may provide solutions for information ordering. Indeed, following the pioneering work of McKeown (1985), recent work on text generation exploits constraints on entity coherence to organize information (Mellish et al. 1998; Kibble and Power 2000, 2004; O’Donnell et al. 2001; Cheng 2002; Lapata ∗ Computer Laboratory, William Gates Building, Cambridge CB3 0FD, UK. Nikiforos.Karamanis@cl.cam.ac.uk. ∗∗ Department of Computing Science, King’s College, Aberdeen AB24 3UE, UK. † Department of Computer Science, Wivenhoe Park, Colchester CO4 3SQ, UK. ‡ School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. Submission received: 15 May 2006; revised submission received: 15 December 2007; accepted for publication: 7 January 2008. © 2008 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 1  2003; Barzilay and Lee 2004; Barzilay and Lapata 2005, among others). Although these approaches often make use of heuristics related to centering, the features of entity coherence they employ are usually deﬁned informally. Additionally, centering-related features are combined with other coherence-inducing factors in ways that are based mainly on intuition, leaving many equally plausible options unexplored. Thus, the answers to the following questions remain unclear: (i) How appropriate is centering for information ordering in text generation? (ii) Which aspects of centering are most useful for this purpose? These are the issues we investigate in this paper, which presents the ﬁrst systematic evaluation of centering for information ordering. To do this, we deﬁne centering-based metrics of coherence which are compatible with several extant information ordering approaches. An important insight of our work is that centering can give rise to many such metrics of coherence. Hence, a general methodology for identifying which of these metrics represent the most promising candidates for information ordering is required. We adopt a corpus-based approach to compare the metrics empirically and demonstrate the portability and generality of our evaluation methods by experimenting with several corpora. Our main result is that the simplest metric (which relies exclusively on NOCB transitions) sets a baseline that cannot be outperformed by other metrics that make use of additional centering-related features. Thus, we provide substantial insight into the role of centering as an information ordering constraint and offer researchers working on text generation a simple, yet robust, baseline to use against their own information ordering approaches during system development. The article is structured as follows: In Section 2 we discuss our information ordering approach in relation to other work on text generation. After a brief introduction to centering in Section 3, Section 4 demonstrates how we derived centering data structures from existing corpora. Section 5 discusses how centering can be used to deﬁne various metrics of coherence suitable for information ordering. Then, Section 6 outlines a corpus-based methodology for choosing among these metrics. Section 7 reports on the results of our experiments and Section 8 discusses their implications. We conclude the paper with directions for future work and a summary of our main contributions.1 2. Information Ordering Information ordering has been investigated by substantial recent work in text-totext generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from  
Given a set CON of k constraints in the framework of Optimality Theory (OT; Prince and Smolensky 1993), what is the capacity of CON as a classiﬁcation scheme for samples of language data? In OT, constraints are functions that map candidates to natural numbers, where each candidate is a member of the (possibly inﬁnite) set of possible derivations of an input form i supplied by the candidate generating function GEN(i). The number that a constraint Ci assigns to a candidate indicates how many times that candidate violates Ci. A grammar is a ranking of the constraints that imposes a total ordering on CON, RCON (or simply R when CON is clear from the context), and the language that is generated by grammar R is the set of candidates that are optimal according to R as in Deﬁnition 1. Deﬁnition 1 a. Candidate a is more harmonic than candidate b according to R, written a b, if they share the same input and a is assigned fewer violations by the highestranked constraint that assigns different numbers of violations to a and b. b. Candidate a is optimal according to ranking R iff no other candidate generated by GEN is more harmonic than a. Because each of the k! rankings of CON is a different grammar that generates a potentially unique language, one natural measure of the classiﬁcatory capacity of CON ∗ Department of Linguistics, University of Chicago, 1010 E. 59th St., Chicago, IL 60637. jriggle@uchicago.edu. Many thanks to Alan Prince, Jeff Heinz, Greg Kobele, Colin Wilson, and two anonymous Computational Linguistics reviewers for helpful comments and suggestions. Any errors are, of course, my own. Submission received: 22 December 2006; revised submission received: 9 October 2007; accepted for publication: 17 December 2007. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 1  is the upper bound of k! languages in what Prince and Smolensky (1993, page 27) dub the factorial typology of the constraint set. Another complexity metric that is useful in analyses of learnability (especially for non-ﬁnite concept classes) is the cardinality of the largest data set of which each subset corresponds to a different ranking hypothesis. The idea of measuring the complexity of a concept class (in the case at hand, a set of grammars) in this way comes from the work of Vapnik and Chervonenkis (1971) and is known as the Vapnik-Chervonenkis dimension (VCD). In OT, the VCD of a constraint set CON (i.e., the concept class consisting of languages generated by rankings of CON) is the size of the largest sample (set of candidates) that is shatterable as in Deﬁnition 2. Deﬁnition 2 A sample S is shatterable by a constraint set CON iff, for every partitioning of S into two disjoint sets T and F (including the null/whole partition), there is at least one ranking RCON that makes every s ∈ T optimal but no s ∈ F optimal. Vapnik and Chervonenkis’s deﬁnition of shatterability has interesting implications for samples consisting of OT candidates. For instance, each candidate in a shatterable sample S must be an input → output mapping for a unique input form because two candidates a and b with the same input would either tie with identical sets of violations or show harmonic inequality. In the case of a tie, no ranking could realize a partitioning that separates a and b and, in the case of harmonic inequality, no ranking could realize a partitioning in which a and b are simultaneously optimal. More generally, the VCD places an upper bound on the number of distinct grammar hypotheses that can be realized over any sample of linguistic data consisting of OT candidates, and thus provides a ready measure of the complexity of the hypothesis space in Optimality Theory. The VCD of a concept class is obviously not independent of its size. As Blumer et al. (1989) point out, for any ﬁnite concept class C, the VCD is bounded at log2 |C| because it takes at least 2d hypotheses to associate a unique hypothesis with every subset of a sample of size d. Thus, because the number of grammars (hypotheses) over k constraints is ﬁnite—one grammar for each of the k! rankings—the VCD of OT is bounded at log2 k!. Or, put more simply, because log2 x! ≤ x log2 x, this establishes k log2 k as an upper bound on the VCD of OT. In this article, I will show how the structure of the hypothesis space in Optimality Theory provides a tighter bound on the VCD of OT than the bound established by the ﬁnitude of the hypothesis space. I will improve upon the inherent bound of k log2 k by showing that the VCD of OT with k constraints is actually bounded at k − 1 and thus grows linearly with the size of |CON|. The complexity measured by the VC dimension has a number of ramiﬁcations for learning in Optimality Theory. For instance, the VCD of a concept class places an absolute lower bound on the number of mistakes that any error-driven learning algorithm can be guaranteed of achieving (Littlestone 1988). This fact tells us that it may yet be possible to improve upon the quadratic mistake bound of (k2 − k)/2 for Recursive Constraint Demotion (Tesar and Smolensky 1993, 2000; Tesar 1995, 1997, 1998), the reigning mistake bound for any OT learning algorithm. The VCD of a concept class also provides a very general bound on the number of data samples that are required for learning in probabilistic models of learning that will be discussed in Section 5. 2. Elementary Ranking Conditions The main result for the VC dimension of OT will be given in Section 4. First, some supporting results will be established showing that there is an upper bound of k − 1 on 48  Riggle  Ranking Hypotheses in OT  shatterable sets of statements about constraint rankings that are expressed with Prince’s (2002) Elementary Ranking Conditions. If our sample space X consists of candidates, then any x ∈ X can be described in terms of the set of constraint rankings under which x is optimal. Prince (2002) provides a scheme for encoding this kind of ranking information called an Elementary Ranking Condition (ERC). In this section, I will review some formal properties of ERCs that are relevant for establishing the VC dimension of OT. Prince demonstrates many formal properties of ERCs beyond those covered here and shows that ERCS are equivalent to the implication-negation fragment of the three-valued relevance logic RM3 (cf. Anderson and Belnap 1975). This section will review properties of ERCs that are most relevant for the results at hand. For formal proofs and a complete exposition of the logic of ERCs, see Prince (2002). For a constraint set CON containing k constraints, ERCs are k-length vectors that use the symbols L, e, and W to encode logical statements about rankings. Each constraint is assigned an arbitrary numeric index, and in each ERC α, the ith coordinate αi refers to the constraint with ith index Ci. The meaning of an ERC is that at least one constraint whose corresponding coordinate contains a W outranks all of the constraints whose coordinates contain L’s. Thus, W, e, L, L means that C1 outranks both C3 and C4, while L, L, W, W means that either C3 or C4 outranks both C1 and C2. ERCs can be constructed by comparing candidates as in Deﬁnition 3. Note that Ci(a) denotes the number of times candidate a violates the constraint with index i. Deﬁnition 3 Given a constraint set CON with k constraints indexed {1 ... k} and two candidates that share the same input, the function ercCON(a, b) returns an ERC α = α1, ..., αk that describes the rankings under which a b.1   αi = W if Ci(a) < Ci(b) erc(a, b) = α1, ..., αk where  αi = L if Ci(a) > Ci(b) αi = e if Ci(a) = Ci(b)  The symbol W in αi of erc(a, b) = α is a mnemonic for the fact that Ci favors a (the winner), whereas an L in coordinate i is a mnemonic for the fact that Ci favors b (the loser). An e in αi indicates that the candidates are equivalent according to Ci.  Example 1  input C1 C2 C3  cand. a * ** * erc(b, a) = L, W, W = b a if C2 or C3 outranks C1  cand. b ** *  erc(a, b) = W, L, L = a b if C1 outranks C2 and C3  (*’s indicate number of violations)  Note the symmetry between erc(a, b) = W, L, L , which says that candidate a is more harmonic than b under any ranking where C1 outranks both C2 and C3, and erc(b, a),  
Suzanne Stevenson† University of Toronto Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it is not clear exactly how people learn and understand them. They are of special interest to linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the properties of idioms in the linguistics literature, there is not much agreement on which properties are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have mostly been overlooked by researchers in computational linguistics. In this article, we look into the usefulness of some of the identiﬁed linguistic properties of idioms for their automatic recognition. Speciﬁcally, we develop statistical measures that each model a speciﬁc property of idiomatic expressions by looking at their actual usage patterns in text. We use these statistical measures in a type-based classiﬁcation task where we automatically separate idiomatic expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of the measures in a token identiﬁcation task where we distinguish idiomatic and literal usages of potentially idiomatic expressions in context. 1. Introduction Idioms form a heterogeneous class, with prototypical examples such as by and large, kick the bucket, and let the cat out of the bag. It is hard to ﬁnd a single agreed-upon deﬁnition that covers all members of this class (Glucksberg 1993; Cacciari 1993; Nunberg, Sag, and Wasow 1994), but they are often deﬁned as sequences of words involving some degree of semantic idiosyncrasy or non-compositionality. That is, an idiom has a different ∗ Department of Computer Science, University of Toronto, 6 King’s College Rd., Toronto, ON M5S 3G4, Canada. E-mail: afsaneh@cs.toronto.edu. ∗∗ Department of Computer Science, University of Toronto, 6 King’s College Rd., Toronto, ON M5S 3G4, Canada. E-mail: pcook@cs.toronto.edu. † Department of Computer Science, University of Toronto, 6 King’s College Rd., Toronto, ON M5S 3G4, Canada. E-mail: suzanne@cs.toronto.edu. Submission received: 12 September 2007; revised submission received: 29 February 2008; accepted for publication: 6 May 2008. © 2009 Association for Computational Linguistics  Computational Linguistics  Volume 35, Number 1  meaning from the simple composition of the meaning of its component words. Idioms are widely and creatively used by speakers of a language to express ideas cleverly, economically, or implicitly, and thus appear in all languages and in all text genres (Sag et al. 2002). Many expressions acquire an idiomatic meaning over time (Cacciari 1993); consequently, new idioms come into existence on a daily basis (Cowie, Mackin, and McCaig 1983; Seaton and Macaulay 2002). Automatic tools are therefore necessary for assisting lexicographers in keeping lexical resources up to date, as well as for creating and extending computational lexicons for use in natural language processing (NLP) systems. Though completely frozen idioms, such as by and large, can be represented as words with spaces (Sag et al. 2002), most idioms are syntactically well-formed phrases that allow some variability in expression, such as shoot the breeze and hold ﬁre (Gibbs and Nayak 1989; d’Arcais 1993; Fellbaum 2007). Such idioms allow a varying degree of morphosyntactic ﬂexibility—for example, held ﬁre and hold one’s ﬁre allow for an idiomatic reading, whereas typically only a literal interpretation is available for ﬁre was held and held ﬁres. Clearly, a words-with-spaces approach does not work for phrasal idioms. Hence, in addition to requiring NLP tools for recognizing idiomatic expressions (types) to include in a lexicon, methods for determining the allowable and preferred usages (a.k.a. canonical forms) of such expressions are also needed. Moreover, in many situations, an NLP system will need to distinguish a usage (token) of a potentially idiomatic expression as either idiomatic or literal in order to handle a given sequence of words appropriately. For example, a machine translation system must translate held ﬁre differently in The army held their ﬁre and The worshippers held the ﬁre up to the idol. Previous studies focusing on the automatic identiﬁcation of idiom types have often recognized the importance of drawing on their linguistic properties, such as their semantic idiosyncrasy or their restricted ﬂexibility, pointed out earlier. Some researchers have relied on a manual encoding of idiom-speciﬁc knowledge in a lexicon (Copestake et al. 2002; Odijk 2004; Villavicencio et al. 2004), whereas others have presented approaches for the automatic acquisition of more general (hence less distinctive) knowledge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work that looks into the acquisition of the distinctive properties of idioms has been limited, both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid, and Spranger 2004). Our goal is to develop unsupervised means for the automatic acquisition of lexical, syntactic, and semantic knowledge about a broadly documented class of idiomatic expressions. Speciﬁcally, we focus on a cross-linguistically prominent class of phrasal idioms which are commonly and productively formed from the combination of a frequent verb and a noun in its direct object position (Cowie, Mackin, and McCaig 1983; Nunberg, Sag, and Wasow 1994; Fellbaum 2002), for example, shoot the breeze, make a face, and push one’s luck. We refer to these as verb+noun idiomatic combinations or VNICs.1 We present a comprehensive analysis of the distinctive linguistic properties of phrasal idioms, including VNICs (Section 2), and propose statistical measures that capture each property (Section 3). We provide a multi-faceted evaluation of the measures (Section 4), showing their effectiveness in the recognition of idiomatic expressions (types)—that is, separating them from similar-on-the-surface literal phrases—as well as their superiority to existing state-of-the-art techniques. Drawing on these statistical measures, we also propose an unsupervised method for the automatic acquisition of an idiom’s canonical  
The study of discourse can be undertaken from different perspectives (e.g., linguistic, cognitive, or computational) with differing purposes in mind (e.g., to study language use or to analyze social practices). The aim of Discourse on the Move is to show that it is possible and proﬁtable to join quantitative and qualitative analyses to study discourse structures. Whereas corpus-based quantitative discourse analysis focuses on the distributional discourse patterns of a corpus as a whole with no indication of how patterns are distributed in individual texts, manual qualitative analysis is always carried out on a small number of texts and does not support large generalizations of the ﬁndings. The book proposes two methodological approaches—top-down and bottomup—that combine the quantitative and qualitative views into a corpus-based description of discourse organization. Such a description provides detailed analyses of individual texts and the generalization of these analyses across all the texts of a genre-speciﬁc corpus. Top-down is the more traditional (not necessarily corpus-based) approach in which researchers establish functional–qualitative methods to develop an analytical framework capable of describing the types of discourse units in a target corpus. In this approach, linguistic–quantitative analyses come as a later step to facilitate the interpretation of discourse types. In contrast, the bottom-up approach begins with a linguistic– quantitative analysis based on the automatic segmentation of texts into discourse units on the basis of vocabulary distributional patterns. In this approach, the functional– qualitative analysis that provides an interpretation of the discourse types is performed as a later step. Both top-down and bottom-up analyses can be broken down into seven procedural steps, but the order of the steps in the two approaches is not the same. The steps to be followed in top-down methods are these: 1. Determination of communicative/functional categories. 2. Segmentation. 3. Classiﬁcation. 4. Linguistic analysis of each unit. 5. Linguistic description of discourse categories. 6. Text structure analysis. 7. Description of discourse organizational tendencies.  Computational Linguistics  Volume 35, Number 1  In the top-down studies presented in this book, the communicative/functional categories used to segment texts into meaningful units of discourse are identiﬁed through move analysis and appeals analysis. Moves segment texts according to the communicative functions of texts (Swales 1981, 1990), whereas the primary role of appeals— derived from the Aristotelian theory of persuasion and employed by Perelman (1982) to develop his theory of “new rhetoric”—is to make the reader ’act’. For this reason, appeals analysis is often applied to persuasive texts. The steps to be followed in bottom-up methods are these: 1. Automatic segmentation. 2. Linguistic analysis of each unit. 3. Classiﬁcation. 4. Linguistic description of discourse categories. 5. Determination of communicative/functional categories. 6. Text structure analysis. 7. Description of discourse organizational tendencies. In the bottom-up studies presented in this book, the computational methods used to automatically identify vocabulary-based discourse units (VBDUs) are based on Hearst’s (1994, 1997) TextTiling procedure. TextTiling is a quantitative procedure that compares the words used in contiguous segments of text. If the vocabulary in two segments is very similar, the two segments are analyzed as belonging to the same discourse unit; otherwise they are analyzed as two distinct units. Therefore, one major difference between the two approaches is the unit of analysis. In the top-down case, the units of analysis (i.e., moves and appeals) are directly interpretable by discourse analysts, whereas bottom-up VBDUs are more complex to describe. However, the bottom-up method can be easily applied to large corpora and is replicable, whereas the top-down approach is more subjective. In both approaches, the linguistic analysis of discourse units relies on multidimensional analysis, the wellknown statistical method developed by Biber to study linguistic variation. Instructively, the book ends with a comparison of two independent analyses—one top-down and one bottom-up, described in Chapters 4 and 7, respectively—carried out on two different corpora, one containing biochemistry research articles, the other including articles about the more general discipline of biology. Although the expectation that both methods would reveal and underpin a similar inherent structure in the articles of both corpora is met to some extent, there are still several aspects that are problematic and have no ready explanation. The authors acknowledge that additional research is needed to shed more light on these aspects. The book is easy to read and well structured. It consists of a preface, nine chapters— divided into two parts—and two appendices. Top-down methods (based on move analysis and appeals analysis) and their application to direct-mail letters, biochemistry research articles, and fund-raising letters are described in Chapters 2–5. Bottomup methods (based on VBDUs) and their application to biology research articles and spoken university lectures are presented in Chapters 6–8. The introductory Chapter 1 contains a synthetic overview of the several perspectives and purposes guiding discourse analysis, a useful distinction between register and genre, the motivation for the book, and its research questions. The ﬁnal Chapter 9 presents a critical comparison between the two approaches and outlines the many questions to be addressed in further studies and the directions to be explored in future research. The two appendices document the steps included in multidimensional analysis (Appendix I) and list the 106  Book Reviews  
This paper reports the on-going research of a thesis project investigating a computational model of early language acquisition. The model discovers word-like units from crossmodal input data and builds continuously evolving internal representations within a cognitive model of memory. Current cognitive theories suggest that young infants employ general statistical mechanisms that exploit the statistical regularities within their environment to acquire language skills. The discovery of lexical units is modelled on this behaviour as the system detects repeating patterns from the speech signal and associates them to discrete abstract semantic tags. In its current state, the algorithm is a novel approach for segmenting speech directly from the acoustic signal in an unsupervised manner, therefore liberating it from a pre-defined lexicon. By the end of the project, it is planned to have an architecture that is capable of acquiring language and communicative skills in an online manner, and carry out robust speech recognition. Preliminary results already show that this method is capable of segmenting and building accurate internal representations of important lexical units as ‘emergent’ properties from crossmodal data. 
CCG, one of the most prominent grammar frameworks, efﬁciently deals with deletion under coordination in natural languages. However, when we expand our attention to more analytic languages whose degree of pro-dropping is more free, CCG’s decomposition rule for dealing with gapping becomes incapable of parsing some patterns of intra-sentential ellipses in serial verb construction. Moreover, the decomposition rule might also lead us to overgeneration problem. In this paper the composition rule is replaced by the use of memory mechanism, called CCG-MM. Fillers can be memorized and gaps can be induced from an input sentence in functional application rules, while ﬁllers and gaps are associated in coordination and serialization. Multimodal slashes, which allow or ban memory operations, are utilized for ease of resource management. As a result, CCG-MM is more powerful than canonical CCG, but its generative power can be bounded by partially linear indexed grammar. 
Reading is known to be an essential task in language learning, but ﬁnding the appropriate text for every learner is far from easy. In this context, automatic procedures can support the teacher’s work. Some tools exist for English, but at present there are none for French as a foreign language (FFL). In this paper, we present an original approach to assessing the readability of FFL texts using NLP techniques and extracts from FFL textbooks as our corpus. Two logistic regression models based on lexical and grammatical features are explored and give quite good predictions on new texts. The results shows a slight superiority for multinomial logistic regression over the proportional odds model. 
This paper deals with the task of ﬁnding generally applicable substitutions for a given input term. We show that the output of a distributional similarity system baseline can be ﬁltered to obtain terms that are not simply similar but frequently substitutable. Our ﬁlter relies on the fact that when two terms are in a common entailment relation, it should be possible to substitute one for the other in their most frequent surface contexts. Using the Google 5-gram corpus to ﬁnd such characteristic contexts, we show that for the given task, our ﬁlter improves the precision of a distributional similarity system from 41% to 56% on a test set comprising common transitive verbs. 
Given the great amount of deﬁnite noun phrases that introduce an entity into the text for the ﬁrst time, this paper presents a set of linguistic features that can be used to detect this type of deﬁnites in Spanish. The efﬁciency of the different features is tested by building a rule-based and a learning-based chain-starting classiﬁer. Results suggest that the classiﬁer, which achieves high precision at the cost of recall, can be incorporated as either a ﬁlter or an additional feature within a coreference resolution system to boost its performance. 
We have adapted a classiﬁcation approach coming from optical character recognition research to the task of speech emotion recognition. The classiﬁcation approach enjoys the representational power of a syntactic method and efﬁciency of statistical classiﬁcation. The syntactic part implements a tree grammar inference algorithm. We have extended this part of the algorithm with various edit costs to penalise more important features with higher edit costs for being outside the interval, which tree automata learned at the inference stage. The statistical part implements an entropy based decision tree (C4.5). We did the testing on the Berlin database of emotional speech. Our classiﬁer outperforms the state of the art classiﬁer (Multilayer Perceptron) by 4.68% and a baseline (C4.5) by 26.58%, which proves validity of the approach. 
In this article, compound processing for translation into German in a factored statistical MT system is investigated. Compounds are handled by splitting them prior to training, and merging the parts after translation. I have explored eight merging strategies using different combinations of external knowledge sources, such as word lists, and internal sources that are carried through the translation process, such as symbols or parts-of-speech. I show that for merging to be successful, some internal knowledge source is needed. I also show that an extra sequence model for part-ofspeech is useful in order to improve the order of compound parts in the output. The best merging results are achieved by a matching scheme for part-of-speech tags. 
Generalized Vector Space Models (GVSM) extend the standard Vector Space Model (VSM) by embedding additional types of information, besides terms, in the representation of documents. An interesting type of information that can be used in such models is semantic information from word thesauri like WordNet. Previous attempts to construct GVSM reported contradicting results. The most challenging problem is to incorporate the semantic information in a theoretically sound and rigorous manner and to modify the standard interpretation of the VSM. In this paper we present a new GVSM model that exploits WordNet’s semantic information. The model is based on a new measure of semantic relatedness between terms. Experimental study conducted in three TREC collections reveals that semantic information can boost text retrieval performance with the use of the proposed GVSM. 
Often, there is a need to use the knowledge from multiple ontologies. This is particularly the case within the context of medical imaging, where a single ontology is not enough to provide the complementary knowledge about anatomy, radiology and diseases that is required by the related applications. Consequently, semantic integration of these different but related types of medical knowledge that is present in disparate domain ontologies becomes necessary. Medical ontology alignment addresses this need by identifying the semantically equivalent concepts across multiple medical ontologies. The resulting alignments can then be used to annotate the medical images and related patient text data. A corresponding semantic search engine that operates on these annotations (i.e. alignments) instead of simple keywords can, in this way, deliver the clinical users a coherent set of medical image and patient text data. 
In this paper we compare different approaches to extract deﬁnitions of four types using a combination of a rule-based grammar and machine learning. We collected a Dutch text corpus containing 549 deﬁnitions and applied a grammar on it. Machine learning was then applied to improve the results obtained with the grammar. Two machine learning experiments were carried out. In the ﬁrst experiment, a standard classiﬁer and a classiﬁer designed speciﬁcally to deal with imbalanced datasets are compared. The algorithm designed speciﬁcally to deal with imbalanced datasets for most types outperforms the standard classiﬁer. In the second experiment we show that classiﬁcation results improve when information on deﬁnition structure is included. 
Frolog is a text-adventure game whose goal is to serve as a laboratory for testing pragmatic theories of accommodation. To this end, rather than implementing ad-hoc mechanisms for each task that is necessary in such a conversational agent, Frolog integrates recently developed tools from computational linguistics, theorem proving and artiﬁcial intelligence planning. 
 2 Related works  In this paper, we present a novel approach for automatic summarization. Our system, called CBSEAS, integrates a new method to detect redundancy at its very core, and produce more expressive summaries than previous approaches. Moreover, we show that our system is versatile enough to integrate opinion mining techniques, so that it is capable of producing opinion oriented summaries. The very competitive results obtained during the last Text Evaluation Conference (TAC 2008) show that our approach is efﬁcient. 
We present a web service for natural language parsing, prediction, generation, and translation using grammars in Portable Grammar Format (PGF), the target format of the Grammatical Framework (GF) grammar compiler. The web service implementation is open source, works with any PGF grammar, and with any web server that supports FastCGI. The service exposes a simple interface which makes it possible to use it for interactive natural language web applications. We describe the functionality and interface of the web service, and demonstrate several applications built on top of it. 
This paper describes a self-learning software agent who collects and learns knowledge from the web and also exchanges her knowledge via dialogues with the users. The agent is built on top of information extraction, web mining, question answering and dialogue system technologies, and users can freely formulate their questions within the gossip domain and obtain the answers in multiple ways: textual response, graph-based visualization of the related concepts and speech output. 
 We demonstrate an open-source natural language generation engine that produces descriptions of entities and classes in English and Greek from OWL ontologies that have been annotated with linguistic and user modeling information expressed in RDF. We also demonstrate an accompanying plug-in for the Prote´ge´ ontology editor, which can be used to create the ontology’s annotations and generate previews of the resulting texts by invoking the generation engine. The engine has been embedded in robots acting as museum tour guides in the physical world and in Second Life; here we demonstrate the latter application. 
 This paper introduces eHumanities Desktop- an online system for corpus management and analysis in support of Computing in the Humanities. Design issues and the overall architecture are described as well as an initial set of applications which are offered by the system.  
We compare the phenomena of clausal coordinate ellipsis in Estonian, a Finno-Ugric language, and German, an Indo-European language. The rules underlying these phenomena appear to be remarkably similar. Thus, the software module ELLEIPO, which was originally developed to generate clausal coordinate ellipsis in German and Dutch, works for Estonian as well. In order to extend ELLEIPO’s coverage to Estonian, we only had to adapt the lexicon and some syntax rules unrelated to coordination. We describe the language-independent rules for coordinate ellipsis that ELLEIPO applies to non-elliptical syntactic structures in both target languages. 
Foma is a compiler, programming language, and C library for constructing ﬁnite-state automata and transducers for various uses. It has speciﬁc support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC ﬁnite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the ‘Mathematical Operators’ Unicode block. 
 The GIVE Challenge is a new Internetbased evaluation effort for natural language generation systems. In this paper, we motivate and describe the software infrastructure that we developed to support this challenge.  
The subject of this demonstration is natural language interaction, focusing on adaptivity and proﬁling of the dialogue management and the generated output (text and speech). These are demonstrated in a museum guide use-case, operating in a simulated environment. The main technical innovations presented are the proﬁling model, the dialogue and action management system, and the text generation and speech synthesis systems. 
This paper presents a tool for extracting multi-word expressions from corpora in Modern Greek, which is used together with a parallel concordancer to augment the lexicon of a rule-based machinetranslation system. The tool is part of a larger extraction system that relies, in turn, on a multilingual parser developed over the past decade in our laboratory. The paper reviews the various NLP modules and resources which enable the retrieval of Greek multi-word expressions and their translations: the Greek parser, its lexical database, the extraction and concordancing system. 
This paper describes Read-X, a system designed to identify text that is appropriate for the reader given his thematic choices and the reading ability associated with his educational background. To our knowledge, Read-X is the ﬁrst web-based system that performs real-time searches and returns results classiﬁed thematically and by reading level within seconds. To facilitate educators or students searching for reading material at speciﬁc reading levels, Read-X extracts the text from the html, pdf, doc, or xml format and makes available a text editor for viewing and editing the extracted text. 
The growing popularity of multimedia documents requires language technologies to approach automatic language analysis and generation from yet another perspective: that of its use in multimodal communication. In this paper, we present a support tool for COSMOROE, a theoretical framework for modelling multimedia dialectics. The tool is a text-based search interface that facilitates the exploration of a corpus of audiovisual ﬁles, annotated with the COSMOROE relations. 
GF is a grammar formalism that has a powerful type system and module system, permitting a high level of abstraction and division of labour in grammar writing. GF is suited both for expert linguists, who appreciate its capacity of generalizations and conciseness, and for beginners, who beneﬁt from its static type checker and, in particular, the GF Resource Grammar Library, which currently covers 12 languages. GF has a notion of multilingual grammars, enabling code sharing, linguistic generalizations, rapid development of translation systems, and painless porting of applications to new languages. 
 Multimodal conversational spoken dialogues using physical and virtual agents provide a potential interface to motivate and support users in the domain of health and ﬁtness. The paper presents a multimodal conversational Companion system focused on health and ﬁtness, which has both a stationary and a mobile component.  
This paper discusses computational compositional semantics from the perspective of grammar engineering, in the light of experience with the use of Minimal Recursion Semantics in DELPH-IN grammars. The relationship between argument indexation and semantic role labelling is explored and a semantic dependency notation (DMRS) is introduced. 
This paper present an overview of some emerging trends in the application of NLP in the domain of the so-called Digital Humanities and discusses the role and nature of metadata, the annotation layer that is so characteristic of documents that play a role in the scholarly practises of the humanities. It is explained how metadata are the key to the added value of techniques such as text and link mining, and an outline is given of what measures could be taken to increase the chances for a bright future for the old ties between NLP and the humanities. There is no data like metadata! 
We present a simple and effective method for extracting parallel sentences from comparable corpora. We employ a statistical machine translation (SMT) system built from small amounts of parallel texts to translate the source side of the nonparallel corpus. The target side texts are used, along with other corpora, in the language model of this SMT system. We then use information retrieval techniques and simple ﬁlters to create French/English parallel data from a comparable news corpora. We evaluate the quality of the extracted data by showing that it signiﬁcantly improves the performance of an SMT systems. 
We present a classiﬁer to predict contextual polarity of subjective phrases in a sentence. Our approach features lexical scoring derived from the Dictionary of Affect in Language (DAL) and extended through WordNet, allowing us to automatically score the vast majority of words in our input avoiding the need for manual labeling. We augment lexical scoring with n-gram analysis to capture the effect of context. We combine DAL scores with syntactic constituents and then extract ngrams of constituents from all sentences. We also use the polarity of all syntactic constituents within the sentence as features. Our results show signiﬁcant improvement over a majority class baseline as well as a more difﬁcult baseline consisting of lexical n-grams. 
In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation. Our algorithm uses the full graph of the LKB efﬁciently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efﬁcient and that it could be tuned to be faster. 
The lack of positive results on supervised domain adaptation for WSD have cast some doubts on the utility of handtagging general corpora and thus developing generic supervised WSD systems. In this paper we show for the ﬁrst time that our WSD system trained on a general source corpus (BNC) and the target corpus, obtains up to 22% error reduction when compared to a system trained on the target corpus alone. In addition, we show that as little as 40% of the target corpus (when supplemented with the source corpus) is sufﬁcient to obtain the same results as training on the full target data. The key for success is the use of unlabeled data with SVD, a combination of kernels and SVM. 
We propose a system which builds, in a semi-supervised manner, a resource that aims at helping a NER system to annotate corpus-speciﬁc named entities. This system is based on a distributional approach which uses syntactic dependencies for measuring similarities between named entities. The speciﬁcity of the presented method however, is to combine a clique-based approach and a clustering technique that amounts to a soft clustering method. Our experiments show that the resource constructed by using this cliquebased clustering system allows to improve different NER systems. 
Machine translation (MT) systems have improved signiﬁcantly; however, their outputs often contain too many errors to communicate the intended meaning to their users. This paper describes a collaborative approach for mediating between an MT system and users who do not understand the source language and thus cannot easily detect translation mistakes on their own. Through a visualization of multiple linguistic resources, this approach enables the users to correct difﬁcult translation errors and understand translated passages that were otherwise bafﬂing. 
Parallel Multiple Context-Free Grammar (PMCFG) is an extension of context-free grammar for which the recognition problem is still solvable in polynomial time. We describe a new parsing algorithm that has the advantage to be incremental and to support PMCFG directly rather than the weaker MCFG formalism. The algorithm is also top-down which allows it to be used for grammar based word prediction. 
Syntactic Reordering of the source language to better match the phrase structure of the target language has been shown to improve the performance of phrase-based Statistical Machine Translation. This paper applies syntactic reordering to English-to-Arabic translation. It introduces reordering rules, and motivates them linguistically. It also studies the effect of combining reordering with Arabic morphological segmentation, a preprocessing technique that has been shown to improve Arabic-English and EnglishArabic translation. We report on results in the news text domain, the UN text domain and in the spoken travel domain. 
In this paper, we present an integrated model of the two central tasks of dialog management: interpreting user actions and generating system actions. We model the interpretation task as a classication problem and the generation task as a prediction problem. These two tasks are interleaved in an incremental parsing-based dialog model. We compare three alternative parsing methods for this dialog model using a corpus of human-human spoken dialog from a catalog ordering domain that has been annotated for dialog acts and task/subtask information. We contrast the amount of context provided by each method and its impact on performance. 
Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task. The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset. 
In this paper we present a human-based evaluation of surface realisation alternatives. We examine the relative rankings of naturally occurring corpus sentences and automatically generated strings chosen by statistical models (language model, loglinear model), as well as the naturalness of the strings chosen by the log-linear model. We also investigate to what extent preceding context has an effect on choice. We show that native speakers do accept quite some variation in word order, but there are also clearly factors that make certain realisation alternatives more natural. 
This paper describes a method using morphological rules and heuristics, for the automatic extraction of large-coverage lexicons of stems and root word-forms from a raw text corpus. We cast the problem of high-coverage lexicon extraction as one of stemming followed by root word-form selection. We examine the use of POS tagging to improve precision and recall of stemming and thereby the coverage of the lexicon. We present accuracy, precision and recall scores for the system on a Hindi corpus. 
This paper presents a feasibility study for implementing lexical morphology principles in a machine translation system in order to solve unknown words. Multilingual symbolic treatment of word-formation is seducing but requires an in-depth analysis of every step that has to be performed. The construction of a prototype is firstly presented, highlighting the methodological issues of such approach. Secondly, an evaluation is performed on a large set of data, showing the benefits and the limits of such approach. 
Sentence ﬂuency is an important component of overall text readability but few studies in natural language processing have sought to understand the factors that deﬁne it. We report the results of an initial study into the predictive power of surface syntactic statistics for the task; we use ﬂuency assessments done for the purpose of evaluating machine translation. We ﬁnd that these features are weakly but signiﬁcantly correlated with ﬂuency. Machine and human translations can be distinguished with accuracy over 80%. The performance of pairwise comparison of ﬂuency is also very high—over 90% for a multi-layer perceptron classiﬁer. We also test the hypothesis that the learned models capture general ﬂuency properties applicable to human-written text. The results do not support this hypothesis: prediction accuracy on the new data is only 57%. This ﬁnding suggests that developing a dedicated, task-independent corpus of ﬂuency judgments will be beneﬁcial for further investigations of the problem. 
We present an algorithm for pronounanaphora (in English) that uses Expectation Maximization (EM) to learn virtually all of its parameters in an unsupervised fashion. While EM frequently fails to ﬁnd good models for the tasks to which it is set, in this case it works quite well. We have compared it to several systems available on the web (all we have found so far). Our program signiﬁcantly outperforms all of them. The algorithm is fast and robust, and has been made publically available for downloading. 
In this paper, we present an efﬁcient query selection algorithm for the retrieval of web text data to augment a statistical language model (LM). The number of retrieved relevant documents is optimized with respect to the number of queries submitted. The querying scheme is applied in the domain of SMS text messages. Continuous speech recognition experiments are conducted on three languages: English, Spanish, and French. The web data is utilized for augmenting in-domain LMs in general and for adapting the LMs to a user-speciﬁc vocabulary. Word error rate reductions of up to 6.6 % (in LM augmentation) and 26.0 % (in LM adaptation) are obtained in setups, where the size of the web mixture LM is limited to the size of the baseline in-domain LM. 
In this paper, we ﬁrst demonstrate the interest of the Loopy Belief Propagation algorithm to train and use a simple alignment model where the expected marginal values needed for an efﬁcient EM-training are not easily computable. We then improve this model with a distortion model based on structure conservation. 
We present a method which, given a few words deﬁning a concept in some language, retrieves, disambiguates and extends corresponding terms that deﬁne a similar concept in another speciﬁed language. This can be very useful for cross-lingual information retrieval and the preparation of multi-lingual lexical resources. We automatically obtain term translations from multilingual dictionaries and disambiguate them using web counts. We then retrieve web snippets with cooccurring translations, and discover additional concept terms from these snippets. Our term discovery is based on coappearance of similar words in symmetric patterns. We evaluate our method on a set of language pairs involving 45 languages, including combinations of very dissimilar ones such as Russian, Chinese, and Hebrew for various concepts. We assess the quality of the retrieved sets using both human judgments and automatically comparing the obtained categories to corresponding English WordNet synsets. 
We describe a methodology for learning a disambiguation model for deep pragmatic interpretations in the context of situated task-oriented dialogue. The system accumulates training examples for ambiguity resolution by tracking the fates of alternative interpretations across dialogue, including subsequent clariﬁcatory episodes initiated by the system itself. We illustrate with a case study building maximum entropy models over abductive interpretations in a referential communication task. The resulting model correctly resolves 81% of ambiguities left unresolved by an initial handcrafted baseline. A key innovation is that our method draws exclusively on a system’s own skills and experience and requires no human annotation. 
Building on work detecting errors in dependency annotation, we set out to correct local dependency errors. To do this, we outline the properties of annotation errors that make the task challenging and their existence problematic for learning. For the task, we deﬁne a feature-based model that explicitly accounts for non-relations between words, and then use ambiguities from one model to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally. 
Spoken Language Understanding aims at mapping a natural language spoken sentence into a semantic representation. In the last decade two main approaches have been pursued: generative and discriminative models. The former is more robust to overﬁtting whereas the latter is more robust to many irrelevant features. Additionally, the way in which these approaches encode prior knowledge is very different and their relative performance changes based on the task. In this paper we describe a machine learning framework where both models are used: a generative model produces a list of ranked hypotheses whereas a discriminative model based on structure kernels and Support Vector Machines, re-ranks such list. We tested our approach on the MEDIA corpus (human-machine dialogs) and on a new corpus (human-machine and humanhuman dialogs) produced in the European LUNA project. The results show a large improvement on the state-of-the-art in concept segmentation and labeling. 
In this paper, we explore ways of improving an inference rule collection and its application to the task of recognizing textual entailment. For this purpose, we start with an automatically acquired collection and we propose methods to reﬁne it and obtain more rules using a hand-crafted lexical resource. Following this, we derive a dependency-based structure representation from texts, which aims to provide a proper base for the inference rule application. The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible improvements. 
Large scale annotated corpora are prerequisite to developing high-performance semantic role labeling systems. Unfortunately, such corpora are expensive to produce, limited in size, and may not be representative. Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning. Our algorithm augments a small number of manually labeled instances with unlabeled examples whose roles are inferred automatically via annotation projection. We formulate the projection task as a generalization of the linear assignment problem. We seek to ﬁnd a role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 
We investigate linguistic features that correlate with the readability of texts for adults with intellectual disabilities (ID). Based on a corpus of texts (including some experimentally measured for comprehension by adults with ID), we analyze the significance of novel discourselevel features related to the cognitive factors underlying our users’ literacy challenges. We develop and evaluate a tool for automatically rating the readability of texts for these users. Our experiments show that our discourselevel, cognitively-motivated features improve automatic readability assessment. 
Mobile voice-enabled search is emerging as one of the most popular applications abetted by the exponential growth in the number of mobile devices. The automatic speech recognition (ASR) output of the voice query is parsed into several ﬁelds. Search is then performed on a text corpus or a database. In order to improve the robustness of the query parser to noise in the ASR output, in this paper, we investigate two different methods to query parsing. Both methods exploit multiple hypotheses from ASR, in the form of word confusion networks, in order to achieve tighter coupling between ASR and query parsing and improved accuracy of the query parser. We also investigate the results of this improvement on search accuracy. Word confusionnetwork based query parsing outperforms ASR 1-best based query-parsing by 2.7% absolute and the search performance improves by 1.8% absolute on one of our data sets. 
The paper presents a multi-document summarization system which builds companyspeciﬁc summaries from a collection of ﬁnancial news such that the extracted sentences contain novel and relevant information about the corresponding organization. The user’s familiarity with the company’s proﬁle is assumed. The goal of such summaries is to provide information useful for the short-term trading of the corresponding company, i.e., to facilitate the inference from news to stock price movement in the next day. We introduce a novel query (i.e., company name) expansion method and a simple unsupervized algorithm for sentence ranking. The system shows promising results in comparison with a competitive baseline. 
This paper presents a conditional random ﬁeld-based approach for identifying speaker-produced disﬂuencies (i.e. if and where they occur) in spontaneous speech transcripts. We emphasize false start regions, which are often missed in current disﬂuency identiﬁcation approaches as they lack lexical or structural similarity to the speech immediately following. We ﬁnd that combining lexical, syntactic, and language model-related features with the output of a state-of-the-art disﬂuency identiﬁcation system improves overall word-level identiﬁcation of these and other errors. Improvements are reinforced under a stricter evaluation metric requiring exact matches between cleaned sentences annotator-produced reconstructions, and altogether show promise for general reconstruction efforts. 
Although a lot of progress has been made recently in word segmentation and POS tagging for Chinese, the output of current state-of-the-art systems is too inaccurate to allow for syntactic analysis based on it. We present an experiment in improving the output of an off-the-shelf module that performs segmentation and tagging, the tokenizer-tagger from Beijing University (PKU). Our approach is based on transformation-based learning (TBL). Unlike in other TBL-based approaches to the problem, however, both obligatory and optional transformation rules are learned, so that the ﬁnal system can output multiple segmentation and POS tagging analyses for a given input. By allowing for a small amount of ambiguity in the output of the tokenizer-tagger, we achieve a very considerable improvement in accuracy. Compared to the PKU tokenizertagger, we improve segmentation F-score from 94.18% to 96.74%, tagged word F-score from 84.63% to 92.44%, segmented sentence accuracy from 47.15% to 65.06% and tagged sentence accuracy from 14.07% to 31.47%. 
We explore the problem of resolving the second person English pronoun you in multi-party dialogue, using a combination of linguistic and visual features. First, we distinguish generic and referential uses, then we classify the referential uses as either plural or singular, and ﬁnally, for the latter cases, we identify the addressee. In our ﬁrst set of experiments, the linguistic and visual features are derived from manual transcriptions and annotations, but in the second set, they are generated through entirely automatic means. Results show that a multimodal system is often preferable to a unimodal one.  Besides being important for computational implementations, resolving you is also an interesting and challenging research problem. As for third person pronouns such as it, some uses of you are not strictly referential. These include discourse marker uses such as you know in example (1), and generic uses like (2), where you does not refer to the addressee as it does in (3). (1) It’s not just, you know, noises like something hitting. (2) Often, you need to know speciﬁc button sequences to get certain functionalities done. (3) I think it’s good. You’ve done a good review.  
 NP  Many different types of features have been shown to improve accuracy in parse reranking. A class of features that thus far has not been considered is based on a projection of the syntactic structure of a translation of the text to be parsed. The intuition for using this type of bitext projection feature is that ambiguous structures in one language often correspond to unambiguous structures in another. We show that reranking based on bitext projection features increases parsing accuracy significantly. 
We present parsing algorithms for various mildly non-projective dependency formalisms. In particular, algorithms are presented for: all well-nested structures of gap degree at most 1, with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power; all well-nested structures with gap degree bounded by any constant k; and a new class of structures with gap degree up to k that includes some ill-nested structures. The third case includes all the gap degree k structures in a number of dependency treebanks. 
 This paper presents six novel approaches to biographic fact extraction that model structural, transitive and latent properties of biographical data. The ensemble of these proposed models substantially outperforms standard pattern-based biographic fact extraction methods and performance is further improved by modeling inter-attribute correlations and distributions over functions of attributes, achieving an average extraction accuracy of 80% over seven types of biographic attributes.  
This paper presents an application of ﬁnite state transducers weighted with feature structure descriptions, following Amtrup (2003), to the morphology of the Semitic language Tigrinya. It is shown that feature-structure weights provide an efﬁcient way of handling the templatic morphology that characterizes Semitic verb stems as well as the long-distance dependencies characterizing the complex Tigrinya verb morphotactics. A relatively complete computational implementation of Tigrinya verb morphology is described. 
We introduce cube summing, a technique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independence assumptions. It is inspired by cube pruning (Chiang, 2007; Huang and Chiang, 2007) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals. When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of Goodman (1999). When non-local features are included, cube summing does not reduce to any semiring, but is compatible with generic techniques for solving dynamic programming equations. 
We present a framework for interfacing a PCFG parser with lexical information from an external resource following a different tagging scheme than the treebank. This is achieved by deﬁning a stochastic mapping layer between the two resources. Lexical probabilities for rare events are estimated in a semi-supervised manner from a lexicon and large unannotated corpora. We show that this solution greatly enhances the performance of an unlexicalized Hebrew PCFG parser, resulting in state-of-the-art Hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens. 
In this paper, we describe experiments conducted on identifying a person using a novel unique correlated corpus of text and audio samples of the person’s communication in six genres. The text samples include essays, emails, blogs, and chat. Audio samples were collected from individual interviews and group discussions and then transcribed to text. For each genre, samples were collected for six topics. We show that we can identify the communicant with an accuracy of 71% for six fold cross validation using an average of 22,000 words per individual across the six genres. For person identification in a particular genre (train on five genres, test on one), an average accuracy of 82% is achieved. For identification from topics (train on five topics, test on one), an average accuracy of 94% is achieved. We also report results on identifying a person’s communication in a genre using text genres only as well as audio genres only. 
This paper presents the end-to-end evaluation of an automatic simultaneous translation system, built with state-of-the-art components. It shows whether, and for which situations, such a system might be advantageous when compared to a human interpreter. Using speeches in English translated into Spanish, we present the evaluation procedure and we discuss the results both for the recognition and translation components as well as for the overall system. Even if the translation process remains the Achilles’ heel of the system, the results show that the system can keep at least half of the information, becoming potentially useful for ﬁnal users. 
Named entity recognition for morphologically rich, case-insensitive languages, including the majority of semitic languages, Iranian languages, and Indian languages, is inherently more difﬁcult than its English counterpart. Worse still, progress on machine learning approaches to named entity recognition for many of these languages is currently hampered by the scarcity of annotated data and the lack of an accurate part-of-speech tagger. While it is possible to rely on manually-constructed gazetteers to combat data scarcity, this gazetteer-centric approach has the potential weakness of creating irreproducible results, since these name lists are not publicly available in general. Motivated in part by this concern, we present a learning-based named entity recognizer that does not rely on manually-constructed gazetteers, using Bengali as our representative resource-scarce, morphologicallyrich language. Our recognizer achieves a relative improvement of 7.5% in Fmeasure over a baseline recognizer. Improvements arise from (1) using induced afﬁxes, (2) extracting information from online lexical databases, and (3) jointly modeling part-of-speech tagging and named entity recognition. 
This paper examines unsupervised approaches to part-of-speech (POS) tagging for morphologically-rich, resource-scarce languages, with an emphasis on Goldwater and Grifﬁths’s (2007) fully-Bayesian approach originally developed for English POS tagging. We argue that existing unsupervised POS taggers unrealistically assume as input a perfect POS lexicon, and consequently, we propose a weakly supervised fully-Bayesian approach to POS tagging, which relaxes the unrealistic assumption by automatically acquiring the lexicon from a small amount of POS-tagged data. Since such relaxation comes at the expense of a drop in tagging accuracy, we propose two extensions to the Bayesian framework and demonstrate that they are effective in improving a fully-Bayesian POS tagger for Bengali, our representative morphologicallyrich, resource-scarce language. 
We extend the factored translation model (Koehn and Hoang, 2007) to allow translations of longer phrases composed of factors such as POS and morphological tags to act as templates for the selection and reordering of surface phrase translation. We also reintroduce the use of alignment information within the decoder, which forms an integral part of decoding in the Alignment Template System (Och, 2002), into phrase-based decoding. Results show an increase in translation performance of up to 1.0% BLEU for out-of-domain French–English translation. We also show how this method compares and relates to lexicalized reordering. 
We describe reﬁnements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modiﬁcations to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation. Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various ﬁltering strategies are then applied to assess the impact on translation speed and quality. Results are reported on the 2008 NIST Arabic-toEnglish evaluation task. 
As empirically demonstrated by the last SensEval exercises, assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed. One possible reason could be the use of inappropriate set of meanings. In fact, WordNet has been used as a de-facto standard repository of meanings. However, to our knowledge, the meanings represented by WordNet have been only used for WSD at a very ﬁne-grained sense level or at a very coarse-grained class level. We suspect that selecting the appropriate level of abstraction could be on between both levels. We use a very simple method for deriving a small set of appropriate meanings using basic structural properties of WordNet. We also empirically demonstrate that this automatically derived set of meanings groups senses into an adequate level of abstraction in order to perform class-based Word Sense Disambiguation, allowing accuracy ﬁgures over 80%. 
We describe a method for creating a nonEnglish subjectivity lexicon based on an English lexicon, an online translation service and a general purpose thesaurus: Wordnet. We use a PageRank-like algorithm to bootstrap from the translation of the English lexicon and rank the words in the thesaurus by polarity using the network of lexical relations in Wordnet. We apply our method to the Dutch language. The best results are achieved when using synonymy and antonymy relations only, and ranking positive and negative words simultaneously. Our method achieves an accuracy of 0.82 at the top 3,000 negative words, and 0.62 at the top 3,000 positive words. 
The present paper is concerned with statistical parsing of constituent structures in German. The paper presents four experiments that aim at improving parsing performance of coordinate structure: 1) reranking the n-best parses of a PCFG parser, 2) enriching the input to a PCFG parser by gold scopes for any conjunct, 3) reranking the parser output for all possible scopes for conjuncts that are permissible with regard to clause structure. Experiment 4 reranks a combination of parses from experiments 1 and 3. The experiments presented show that nbest parsing combined with reranking improves results by a large margin. Providing the parser with different scope possibilities and reranking the resulting parses results in an increase in F-score from 69.76 for the baseline to 74.69. While the F-score is similar to the one of the ﬁrst experiment (n-best parsing and reranking), the ﬁrst experiment results in higher recall (75.48% vs. 73.69%) and the third one in higher precision (75.43% vs. 73.26%). Combining the two methods results in the best result with an F-score of 76.69. 
This paper addresses the problem of extracting the most important facts from a news article. Our approach uses syntactic, semantic, and general statistical features to identify the most important sentences in a document. The importance of the individual features is estimated using generalized iterative scaling methods trained on an annotated newswire corpus. The performance of our approach is evaluated against 300 unseen news articles and shows that use of these features results in statistically signiﬁcant improvements over a provenly robust baseline, as measured using metrics such as precision, recall and ROUGE. 
In this paper we compare and contrast two approaches to Machine Translation (MT): the CMU-UKA Syntax Augmented Machine Translation system (SAMT) and UPC-TALP N-gram-based Statistical Machine Translation (SMT). SAMT is a hierarchical syntax-driven translation system underlain by a phrase-based model and a target part parse tree. In N-gram-based SMT, the translation process is based on bilingual units related to word-to-word alignment and statistical modeling of the bilingual context following a maximumentropy framework. We provide a stepby-step comparison of the systems and report results in terms of automatic evaluation metrics and required computational resources for a smaller Arabic-to-English translation task (1.5M tokens in the training corpus). Human error analysis clariﬁes advantages and disadvantages of the systems under consideration. Finally, we combine the output of both systems to yield signiﬁcant improvements in translation quality. 
We present a Hebrew to English transliteration method in the context of a machine translation system. Our method uses machine learning to determine which terms are to be transliterated rather than translated. The training corpus for this purpose includes only positive examples, acquired semi-automatically. Our classiﬁer reduces more than 38% of the errors made by a baseline method. The identiﬁed terms are then transliterated. We present an SMTbased transliteration model trained with a parallel corpus extracted from Wikipedia using a fairly simple method which requires minimal knowledge. The correct result is produced in more than 76% of the cases, and in 92% of the instances it is one of the top-5 results. We also demonstrate a small improvement in the performance of a Hebrew-to-English MT system that uses our transliteration module. 
We show how global constraints such as transitivity can be treated intensionally in a Zero-One Integer Linear Programming (ILP) framework which is geared to ﬁnd the optimal and coherent partition of coreference sets given a number of candidate pairs and their weights delivered by a pairwise classiﬁer (used as reliable clustering seed pairs). In order to ﬁnd out whether ILP optimization, which is NPcomplete, actually is the best we can do, we compared the ﬁrst consistent solution generated by our adaptation of an efﬁcient Zero-One algorithm with the optimal solution. The ﬁrst consistent solution, which often can be found very fast, is already as good as the optimal solution; optimization is thus not needed. 
One way to construct semantic representations in a robust manner is to enhance shallow language processors with semantic components. Here, we provide a model theory for a semantic formalism that is designed for this, namely Robust Minimal Recursion Semantics (RMRS). We show that RMRS supports a notion of entailment that allows it to form the basis for comparing the semantic output of different parses of varying depth. 
We propose a novel algorithm for extracting dependencies from the derivations of a large fragment of CCG. Unlike earlier proposals, our dependency structures are always tree-shaped. We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 
In this paper, we present a novel approach to integrate speech recognition and rulebased machine translation by lattice parsing. The presented approach is hybrid in two senses. First, it combines structural and statistical methods for language modeling task. Second, it employs a chart parser which utilizes manually created syntax rules in addition to scores obtained after statistical processing during speech recognition. The employed chart parser is a uniﬁcation-based active chart parser. It can parse word graphs by using a mixed strategy instead of being bottom-up or top-down only. The results are reported based on word error rate on the NIST HUB-1 word-lattices. The presented approach is implemented and compared with other syntactic language modeling techniques. 
An open problem in dependency parsing is the accurate and efﬁcient treatment of non-projective structures. We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms. In this paper, we provide two key tools for this approach. First, we show how to reduce nonprojective dependency parsing to parsing with Linear Context-Free Rewriting Systems (LCFRS), by presenting a technique for extracting LCFRS from dependency treebanks. For efﬁcient parsing, the extracted grammars need to be transformed in order to minimize the number of nonterminal symbols per production. Our second contribution is an algorithm that computes this transformation for a large, empirically relevant class of grammars. 
Handling terminology is an important matter in a translation workﬂow. However, current Machine Translation (MT) systems do not yet propose anything proactive upon tools which assist in managing terminological databases. In this work, we investigate several enhancements to analogical learning and test our implementation on translating medical terms. We show that the analogical engine works equally well when translating from and into a morphologically rich language, or when dealing with language pairs written in different scripts. Combining it with a phrasebased statistical engine leads to signiﬁcant improvements. 
We present a language-pair independent terminology extraction module that is based on a sub-sentential alignment system that links linguistically motivated phrases in parallel texts. Statistical ﬁlters are applied on the bilingual list of candidate terms that is extracted from the alignment output. We compare the performance of both the alignment and terminology extraction module for three different language pairs (French-English, French-Italian and French-Dutch) and highlight languagepair speciﬁc problems (e.g. different compounding strategy in French and Dutch). Comparisons with standard terminology extraction programs show an improvement of up to 20% for bilingual terminology extraction and competitive results (85% to 90% accuracy) for monolingual terminology extraction, and reveal that the linguistically based alignment module is particularly well suited for the extraction of complex multiword terms. 
We use a machine learner trained on a combination of acoustic and contextual features to predict the accuracy of incoming n-best automatic speech recognition (ASR) hypotheses to a spoken dialogue system (SDS). Our novel approach is to use a simple statistical User Simulation (US) for this task, which measures the likelihood that the user would say each hypothesis in the current context. Such US models are now common in machine learning approaches to SDS, are trained on real dialogue data, and are related to theories of “alignment” in psycholinguistics. We use a US to predict the user’s next dialogue move and thereby re-rank n-best hypotheses of a speech recognizer for a corpus of 2564 user utterances. The method achieved a signiﬁcant relative reduction of Word Error Rate (WER) of 5% (this is 44% of the possible WER improvement on this data), and 62% of the possible semantic improvement (Dialogue Move Accuracy), compared to the baseline policy of selecting the topmost ASR hypothesis. The majority of the improvement is attributable to the User Simulation feature, as shown by Information Gain analysis. 
We present the results of a large-scale, end-to-end human evaluation of various sentiment summarization models. The evaluation shows that users have a strong preference for summarizers that model sentiment over non-sentiment baselines, but have no broad overall preference between any of the sentiment-based models. However, an analysis of the human judgments suggests that there are identiﬁable situations where one summarizer is generally preferred over the others. We exploit this fact to build a new summarizer by training a ranking SVM model over the set of human preference judgments that were collected during the evaluation, which results in a 30% relative reduction in error over the previous best summarizer. 
The quality of the part-of-speech (PoS) annotation in a corpus is crucial for the development of PoS taggers. In this paper, we experiment with three complementary methods for automatically detecting errors in the PoS annotation for the Icelandic Frequency Dictionary corpus. The ﬁrst two methods are language independent and we argue that the third method can be adapted to other morphologically complex languages. Once possible errors have been detected, we examine each error candidate and hand-correct the corresponding PoS tag if necessary. Overall, based on the three methods, we handcorrect the PoS tagging of 1,334 tokens (0.23% of the tokens) in the corpus. Furthermore, we re-evaluate existing state-ofthe-art PoS taggers on Icelandic text using the corrected corpus. 
We present a uniﬁed view of many translation algorithms that synthesizes work on deductive parsing, semiring parsing, and efﬁcient approximate search algorithms. This gives rise to clean analyses and compact descriptions that can serve as the basis for modular implementations. We illustrate this with several examples, showing how to build search spaces for several disparate phrase-based search strategies, integrate non-local features, and devise novel models. Although the framework is drawn from parsing and applied to translation, it is applicable to many dynamic programming problems arising in natural language processing and other areas. 
We address the task of automatically predicting if summarization system performance will be good or bad based on features derived directly from either single- or multi-document inputs. Our labelled corpus for the task is composed of data from large scale evaluations completed over the span of several years. The variation of data between years allows for a comprehensive analysis of the robustness of features, but poses a challenge for building a combined corpus which can be used for training and testing. Still, we ﬁnd that the problem can be mitigated by appropriately normalizing for differences within each year. We examine different formulations of the classiﬁcation task which considerably inﬂuence performance. The best results are 84% prediction accuracy for single- and 74% for multi-document summarization. 
We introduce a word segmentation approach to languages where word boundaries are not orthographically marked, with application to Phrase-Based Statistical Machine Translation (PB-SMT). Instead of using manually segmented monolingual domain-speciﬁc corpora to train segmenters, we make use of bilingual corpora and statistical word alignment techniques. First of all, our approach is adapted for the speciﬁc translation task at hand by taking the corresponding source (target) language into account. Secondly, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions. 
Lexical-semantic resources are used extensively for applied semantic inference, yet a clear quantitative picture of their current utility and limitations is largely missing. We propose system- and application-independent evaluation and analysis methodologies for resources’ performance, and systematically apply them to seven prominent resources. Our ﬁndings identify the currently limited recall of available resources, and indicate the potential to improve performance by examining non-standard relation types and by distilling the output of distributional methods. Further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid, as well as its prior validity likelihood. 
In this paper, we explore unsupervised techniques for the task of automatic short answer grading. We compare a number of knowledge-based and corpus-based measures of text similarity, evaluate the effect of domain and size on the corpus-based measures, and also introduce a novel technique to improve the performance of the system by integrating automatic feedback from the student answers. Overall, our system signiﬁcantly and consistently outperforms other unsupervised methods for short answer grading that have been proposed in the past. 
Automatic detection of general relations between short texts is a complex task that cannot be carried out only relying on language models and bag-of-words. Therefore, learning methods to exploit syntax and semantics are required. In this paper, we present a new kernel for the representation of shallow semantic information along with a comprehensive study on kernel methods for the exploitation of syntactic/semantic structures for short text pair categorization. Our experiments with Support Vector Machines on question/answer classiﬁcation show that our kernels can be used to greatly improve system accuracy. 
Recent research has shown that language and the socio-cognitive phenomena associated with it can be aptly modeled and visualized through networks of linguistic entities. However, most of the existing works on linguistic networks focus only on the local properties of the networks. This study is an attempt to analyze the structure of languages via a purely structural technique, namely spectral analysis, which is ideally suited for discovering the global correlations in a network. Application of this technique to PhoNet, the co-occurrence network of consonants, not only reveals several natural linguistic principles governing the structure of the consonant inventories, but is also able to quantify their relative importance. We believe that this powerful technique can be successfully applied, in general, to study the structure of natural languages. 
We present a novel graph-based algorithm for the automated disambiguation of glosses in lexical knowledge resources. A dictionary graph is built starting from senses (vertices) and explicit or implicit relations in the dictionary (edges). The approach is based on the identiﬁcation of edge sequences which constitute cycles in the dictionary graph (possibly with one edge reversed) and relate a source to a target word sense. Experiments are performed on the disambiguation of ambiguous words in the glosses of WordNet and two machine-readable dictionaries. 
Many parsing techniques including parameter estimation assume the use of a packed parse forest for efficient and accurate parsing. However, they have several inherent problems deriving from the restriction of locality in the packed parse forest. Deterministic parsing is one of solutions that can achieve simple and fast parsing without the mechanisms of the packed parse forest by accurately choosing search paths. We propose (i) deterministic shift-reduce parsing for unification-based grammars, and (ii) best-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 
Named entity recognition (NER) for English typically involves one of three gold standards: MUC, CoNLL, or BBN, all created by costly manual annotation. Recent work has used Wikipedia to automatically create a massive corpus of named entity annotated text. We present the ﬁrst comprehensive crosscorpus evaluation of NER. We identify the causes of poor cross-corpus performance and demonstrate ways of making them more compatible. Using our process, we develop a Wikipedia corpus which outperforms gold standard corpora on crosscorpus evaluation by up to 11%. 
Many methods are available for computing semantic similarity between individual words, but certain NLP tasks require the comparison of word pairs. This paper presents a kernel-based framework for application to relational reasoning tasks of this kind. The model presented here combines information about two distinct types of word pair similarity: lexical similarity and relational similarity. We present an efﬁcient and ﬂexible technique for implementing relational similarity and show the effectiveness of combining lexical and relational models by demonstrating state-ofthe-art results on a compound noun interpretation task. 
This article presents empirical evaluations of aspects of annotation for the linguistic property of animacy in Swedish, ranging from manual human annotation, automatic classiﬁcation and, ﬁnally, an external evaluation in the task of syntactic parsing. We show that a treatment of animacy as a lexical semantic property of noun types enables generalization over distributional properties of these nouns which proves beneﬁcial in automatic classiﬁcation and furthermore gives signiﬁcant improvements in terms of parsing accuracy for Swedish, compared to a state-of-theart baseline parser with gold standard animacy information. 
A set of labeled classes of instances is extracted from text and linked into an existing conceptual hierarchy. Besides a significant increase in the coverage of the class labels assigned to individual instances, the resulting resource of labeled classes is more effective than similar data derived from the manually-created Wikipedia, in the task of attribute extraction over conceptual hierarchies. 
Current approaches to the prediction of associations rely on just one type of information, generally taking the form of either word space models or collocation measures. At the moment, it is an open question how these approaches compare to one another. In this paper, we will investigate the performance of these two types of models and that of a new approach based on compounding. The best single predictor is the log-likelihood ratio, followed closely by the document-based word space model. We will show, however, that an ensemble method that combines these two best approaches with the compounding algorithm achieves an increase in performance of almost 30% over the current state of the art. 
In this paper we introduce the notion of “frame relatedness”, i.e. relatedness among prototypical situations as represented in the FrameNet database. We ﬁrst demonstrate the cognitive plausibility of that notion through an annotation experiment, and then propose different types of computational measures to automatically assess relatedness. Results show that our measures provide good performance on the task of ranking pairs of frames. 
An important part of question answering is ensuring a candidate answer is plausible as a response. We present a ﬂexible approach based on discriminative preference ranking to determine which of a set of candidate answers are appropriate. Discriminative methods provide superior performance while at the same time allow the ﬂexibility of adding new and diverse features. Experimental results on a set of focused What ...? and Which ...? questions show that our learned preference ranking methods perform better than alternative solutions to the task of answer typing. A gain of almost 0.2 in MRR for both the ﬁrst appropriate and ﬁrst correct answers is observed along with an increase in precision over the entire range of recall. 
 We present an extensive study on the problem of detecting polarity of words. We consider the polarity of a word to be either positive or negative. For example, words such as good, beautiful, and wonderful are considered as positive words; whereas words such as bad, ugly, and sad are considered negative words. We treat polarity detection as a semi-supervised label propagation problem in a graph. In the graph, each node represents a word whose polarity is to be determined. Each weighted edge encodes a relation that exists between two words. Each node (word) can have two labels: positive or negative. We study this framework in two different resource availability scenarios using WordNet and OpenOfﬁce thesaurus when WordNet is not available. We report our results on three different languages: English, French, and Hindi. Our results indicate that label propagation improves signiﬁcantly over the baseline and other semisupervised learning methods like Mincuts and Randomized Mincuts for this task.  
We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex tradeoffs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing MATCH data. We then train a NLG policy using Reinforcement Learning (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy signiﬁcantly outperforms all the prior approaches. 
In this paper, four state-of-art probabilistic taggers i.e. TnT tagger, TreeTagger, RF tagger and SVM tool, are applied to the Urdu language. For the purpose of the experiment, a syntactic tagset is proposed. A training corpus of 100,000 tokens is used to train the models. Using the lexicon extracted from the training corpus, SVM tool shows the best accuracy of 94.15%. After providing a separate lexicon of 70,568 types, SVM tool again shows the best accuracy of 95.66%. 
We present several algorithms for assigning heads in phrase structure trees, based on different linguistic intuitions on the role of heads in natural language syntax. Starting point of our approach is the observation that a head-annotated treebank deﬁnes a unique lexicalized tree substitution grammar. This allows us to go back and forth between the two representations, and deﬁne objective functions for the unsupervised learning of head assignments in terms of features of the implicit lexicalized tree grammars. We evaluate algorithms based on the match with gold standard head-annotations, and the comparative parsing accuracy of the lexicalized grammars they give rise to. On the ﬁrst task, we approach the accuracy of handdesigned heuristics for English and interannotation-standard agreement for German. On the second task, the implied lexicalized grammars score 4% points higher on parsing accuracy than lexicalized grammars derived by commonly used heuristics. 
We present a general model and conceptual framework for specifying architectures for incremental processing in dialogue systems, in particular with respect to the topology of the network of modules that make up the system, the way information ﬂows through this network, how information increments are ‘packaged’, and how these increments are processed by the modules. This model enables the precise speciﬁcation of incremental systems and hence facilitates detailed comparisons between systems, as well as giving guidance on designing new systems. 
Multi-source statistical machine translation is the process of generating a single translation from multiple inputs. Previous work has focused primarily on selecting from potential outputs of separate translation systems, and solely on multi-parallel corpora and test sets. We demonstrate how multi-source translation can be adapted for multiple monolingual inputs. We also examine different approaches to dealing with multiple sources, including consensus decoding, and we present a novel method of input combination to generate lattices for multi-source translation within a single translation model. 
This paper presents the results of a series of experiments which examine the impact of two information status categories (given and new) and frequency of occurrence on pitch accent realisations. More speciﬁcally the experiments explore within-type similarity of pitch accent productions and the effect information status and frequency of occurrence have on these productions. The results indicate a signiﬁcant inﬂuence of both pitch accent type and information status category on the degree of withintype variability, in line with exemplartheoretic expectations. 
Automatic image annotation is an attractive approach for enabling convenient access to images found in a variety of documents. Since image captions and relevant discussions found in the text can be useful for summarizing the content of images, it is also possible that this text can be used to generate salient indexing terms. Unfortunately, this problem is generally domainspeciﬁc because indexing terms that are useful in one domain can be ineffective in others. Thus, we present a supervised machine learning approach to image annotation utilizing non-lexical features1 extracted from image-related text to select useful terms. We apply this approach to several subdomains of the biomedical sciences and show that we are able to reduce the number of ineffective indexing terms. 
This paper describes a fully incremental dialogue system that can engage in dialogues in a simple domain, number dictation. Because it uses incremental speech recognition and prosodic analysis, the system can give rapid feedback as the user is speaking, with a very short latency of around 200ms. Because it uses incremental speech synthesis and self-monitoring, the system can react to feedback from the user as the system is speaking. A comparative evaluation shows that naïve users preferred this system over a non-incremental version, and that it was perceived as more human-like. 1  This paper presents a dialogue system, called NUMBERS, in which all components operate incrementally. We had two aims: First, to explore technical questions such as how the components of a modularized dialogue system should be arranged and made to interoperate to support incremental processing, and which requirements incremental processing puts on dialogue system components (e.g., speech recognition, prosodic analysis, parsing, discourse modelling, action selection and speech synthesis). Second, to investigate whether incremental processing can help us to better model certain aspects of human behaviour in dialogue systems – especially turntaking and feedback – and whether this improves the user’s experience of using such a system.  
We propose an unsupervised method for distinguishing literal and non-literal usages of idiomatic expressions. Our method determines how well a literal interpretation is linked to the overall cohesive structure of the discourse. If strong links can be found, the expression is classiﬁed as literal, otherwise as idiomatic. We show that this method can help to tell apart literal and non-literal usages, even for idioms which occur in canonical form. 
This paper describes POS tagging experiments with semi-supervised training as an extension to the (supervised) averaged perceptron algorithm, ﬁrst introduced for this task by (Collins, 2002). Experiments with an iterative training on standard-sized supervised (manually annotated) dataset (106 tokens) combined with a relatively modest (in the order of 108 tokens) unsupervised (plain) data in a bagging-like fashion showed signiﬁcant improvement of the POS classiﬁcation task on typologically different languages, yielding better than state-of-the-art results for English and Czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %). 
Latent conditional models have become popular recently in both natural language processing and vision processing communities. However, establishing an effective and efﬁcient inference method on latent conditional models remains a question. In this paper, we describe the latent-dynamic inference (LDI), which is able to produce the optimal label sequence on latent conditional models by using efﬁcient search strategy and dynamic programming. Furthermore, we describe a straightforward solution on approximating the LDI, and show that the approximated LDI performs as well as the exact LDI, while the speed is much faster. Our experiments demonstrate that the proposed inference algorithm outperforms existing inference methods on a variety of natural language processing tasks. 
We discuss text summarization in terms of maximum coverage problem and its variant. We explore some decoding algorithms including the ones never used in this summarization formulation, such as a greedy algorithm with performance guarantee, a randomized algorithm, and a branch-andbound method. On the basis of the results of comparative experiments, we also augment the summarization model so that it takes into account the relevance to the document cluster. Through experiments, we showed that the augmented model is superior to the best-performing method of DUC’04 on ROUGE-1 without stopwords. 
This paper presents a chunking-based discriminative approach to full parsing. We convert the task of full parsing into a series of chunking tasks and apply a conditional random ﬁeld (CRF) model to each level of chunking. The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efﬁciently obtained by using a depthﬁrst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 
In this paper, we address the problem of mining transliterations of Named Entities (NEs) from large comparable corpora. We leverage the empirical fact that multilingual news articles with similar news content are rich in Named Entity Transliteration Equivalents (NETEs). Our mining algorithm, MINT, uses a cross-language document similarity model to align multilingual news articles and then mines NETEs from the aligned articles using a transliteration similarity model. We show that our approach is highly effective on 6 different comparable corpora between English and 4 languages from 3 different language families. Furthermore, it performs substantially better than a state-of-the-art competitor. 
Existing work in the extraction of commonsense knowledge from text has been primarily restricted to factoids that serve as statements about what may possibly obtain in the world. We present an approach to deriving stronger, more general claims by abstracting over large sets of factoids. Our goal is to coalesce the observed nominals for a given predicate argument into a few predominant types, obtained as WordNet synsets. The results can be construed as generically quantiﬁed sentences restricting the semantic type of an argument position of a predicate. 
A corpus-based technique is described to improve the efﬁciency of wide-coverage high-accuracy parsers. By keeping track of the derivation steps which lead to the best parse for a very large collection of sentences, the parser learns which parse steps can be ﬁltered without signiﬁcant loss in parsing accuracy, but with an important increase in parsing efﬁciency. An interesting characteristic of our approach is that it is self-learning, in the sense that it uses unannotated corpora. 
This paper presents a new, exemplar-based model of thematic ﬁt. In contrast to previous models, it does not approximate thematic ﬁt as argument plausibility or ‘ﬁt with verb selectional preferences’, but directly as semantic role plausibility for a verb-argument pair, through similaritybased generalization from previously seen verb-argument pairs. This makes the model very robust for data sparsity. We argue that the model is easily extensible to a model of semantic role ambiguity resolution during online sentence comprehension. The model is evaluated on human semantic role plausibility judgments. Its predictions correlate signiﬁcantly with the human judgments. It rivals two state-of-theart models of thematic ﬁt and exceeds their performance on previously unseen or lowfrequency items. 
Concept taxonomies offer a powerful means for organizing knowledge, but this organization must allow for many overlapping and fine-grained perspectives if a general-purpose taxonomy is to reflect concepts as they are actually employed and reasoned about in everyday usage. We present here a means of bootstrapping finely-discriminating taxonomies from a variety of different starting points, or seeds, that are acquired from three different sources: WordNet, ConceptNet and the web at large. 
Abstract-like text summarisation requires a means of producing novel summary sentences. In order to improve the grammaticality of the generated sentence, we model a global (sentence) level syntactic structure. We couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words. We also introduce a new search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree. We treat the allocation of modiﬁers to heads as a weighted bipartite graph matching (or assignment) problem, a well studied problem in graph theory. Using BLEU to measure performance on a string regeneration task, we found an improvement, illustrating the beneﬁt of the spanning tree approach armed with an argument satisfaction model. 
 2 The impact of window size  We introduce an alternative approach to extracting word pair associations from corpora, based purely on surface distances in the text. We contrast it with the prevailing windowbased co-occurrence model and show it to be more statistically robust and to disclose a broader selection of significant associative relationships - owing largely to the property of scale-independence. In the process we provide insights into the limiting characteristics of window-based methods which complement the sometimes conflicting application-oriented literature in this area. 
As the arm of NLP technologies extends beyond a small core of languages, techniques for working with instances of language data across hundreds to thousands of languages may require revisiting and recalibrating the tried and true methods that are used. Of the NLP techniques that has been treated as “solved” is language identiﬁcation (language ID) of written text. However, we argue that language ID is far from solved when one considers input spanning not dozens of languages, but rather hundreds to thousands, a number that one approaches when harvesting language data found on the Web. We formulate language ID as a coreference resolution problem and apply it to a Web harvesting task for a speciﬁc linguistic data type and achieve a much higher accuracy than long accepted language ID approaches. 
We investigate the possibility of exploiting character-based dependency for Chinese information processing. As Chinese text is made up of character sequences rather than word sequences, word in Chinese is not so natural a concept as in English, nor is word easy to be deﬁned without argument for such a language. Therefore we propose a character-level dependency scheme to represent primary linguistic relationships within a Chinese sentence. The usefulness of character dependencies are veriﬁed through two specialized dependency parsing tasks. The ﬁrst is to handle trivial character dependencies that are equally transformed from traditional word boundaries. The second furthermore considers the case that annotated internal character dependencies inside a word are involved. Both of these results from character-level dependency parsing are positive. This study provides an alternative way to formularize basic characterand word-level representation for Chinese. 
We present the ﬁrst unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task. 
Unknown lexical items present a major obstacle to the development of broadcoverage semantic role labeling systems. We address this problem with a semisupervised learning approach which acquires training instances for unseen verbs from an unlabeled corpus. Our method relies on the hypothesis that unknown lexical items will be structurally and semantically similar to known items for which annotations are available. Accordingly, we represent known and unknown sentences as graphs, formalize the search for the most similar verb as a graph alignment problem and solve the optimization using integer linear programming. Experimental results show that role labeling performance for unknown lexical items improves with training data produced automatically by our method. 
Semantic Role Labeling (SRL) has proved to be a valuable tool for performing automatic analysis of natural language texts. Currently however, most systems rely on a large training set, which is manually annotated, an effort that needs to be repeated whenever different languages or a different set of semantic roles is used in a certain application. A possible solution for this problem is semi-supervised learning, where a small set of training examples is automatically expanded using unlabeled texts. We present the Latent Words Language Model, which is a language model that learns word similarities from unlabeled texts. We use these similarities for different semi-supervised SRL methods as additional features or to automatically expand a small training set. We evaluate the methods on the PropBank dataset and ﬁnd that for small training sizes our best performing system achieves an error reduction of 33.27% F1-measure compared to a state-of-the-art supervised baseline. 
Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute ﬁrst-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs). We then introduce a novel second-order expectation semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy). This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk. We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a beneﬁt of up to 1.0 BLEU point. 
Minimum error rate training (MERT) involves choosing parameter values for a machine translation (MT) system that maximize performance on a tuning set as measured by an automatic evaluation metric, such as BLEU. The method is best when the system will eventually be evaluated using the same metric, but in reality, most MT evaluations have a human-based component. Although performing MERT with a human-based metric seems like a daunting task, we describe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU. 
Cube pruning is a fast inexact method for generating the items of a beam decoder. In this paper, we show that cube pruning is essentially equivalent to A* search on a speciﬁc search space with speciﬁc heuristics. We use this insight to develop faster and exact variants of cube pruning. 
Current methods of using lexical features in machine translation have difﬁculty in scaling up to realistic MT tasks due to a prohibitively large number of parameters involved. In this paper, we propose methods of using new linguistic and contextual features that do not suffer from this problem and apply them in a state-ofthe-art hierarchical MT system. The features used in this work are non-terminal labels, non-terminal length distribution, source string context and source dependency LM scores. The effectiveness of our techniques is demonstrated by significant improvements over a strong baseline. On Arabic-to-English translation, improvements in lower-cased BLEU are 2.0 on NIST MT06 and 1.7 on MT08 newswire data on decoding output. On Chinese-to-English translation, the improvements are 1.0 on MT06 and 0.8 on MT08 newswire data. 
Methods that learn from prior information about input features such as generalized expectation (GE) have been used to train accurate models with very little effort. In this paper, we propose an active learning approach in which the machine solicits “labels” on features rather than instances. In both simulated and real user experiments on two sequence labeling tasks we show that our active learning method outperforms passive learning with features as well as traditional active learning with instances. Preliminary experiments suggest that novel interfaces which intelligently solicit labels on multiple features facilitate more efﬁcient annotation. 
In this paper, we propose a novel class of graphs, the tripartite directed acyclic graphs (tDAGs), to model ﬁrst-order rule feature spaces for sentence pair classiﬁcation. We introduce a novel algorithm for computing the similarity in ﬁrst-order rewrite rule feature spaces. Our algorithm is extremely efﬁcient and, as it computes the similarity of instances that can be represented in explicit feature spaces, it is a valid kernel function. 
We study graphical modeling in the case of stringvalued random variables. Whereas a weighted ﬁnite-state transducer can model the probabilistic relationship between two strings, we are interested in building up joint models of three or more strings. This is needed for inﬂectional paradigms in morphology, cognate modeling or language reconstruction, and multiple-string alignment. We propose a Markov Random Field in which each factor (potential function) is a weighted ﬁnite-state machine, typically a transducer that evaluates the relationship between just two of the strings. The full joint distribution is then a product of these factors. Though decoding is actually undecidable in general, we can still do efﬁcient joint inference using approximate belief propagation; the necessary computations and messages are all ﬁnitestate. We demonstrate the methods by jointly predicting morphological forms. 
We present a framework to extract the most important features (tree fragments) from a Tree Kernel (TK) space according to their importance in the target kernelbased machine, e.g. Support Vector Machines (SVMs). In particular, our mining algorithm selects the most relevant features based on SVM estimated weights and uses this information to automatically infer an explicit representation of the input data. The explicit features (a) improve our knowledge on the target problem domain and (b) make large-scale learning practical, improving training and test time, while yielding accuracy in line with traditional TK classiﬁers. Experiments on semantic role labeling and question classiﬁcation illustrate the above claims. 
Because of the importance of proteinprotein interaction (PPI) extraction from text, many corpora have been proposed with slightly differing deﬁnitions of proteins and PPI. Since no single corpus is large enough to saturate a machine learning system, it is necessary to learn from multiple different corpora. In this paper, we propose a solution to this challenge. We designed a rich feature vector, and we applied a support vector machine modiﬁed for corpus weighting (SVM-CW) to complete the task of multiple corpora PPI extraction. The rich feature vector, made from multiple useful kernels, is used to express the important information for PPI extraction, and the system with our feature vector was shown to be both faster and more accurate than the original kernelbased system, even when using just a single corpus. SVM-CW learns from one corpus, while using other corpora for support. SVM-CW is simple, but it is more effective than other methods that have been successfully applied to other NLP tasks earlier. With the feature vector and SVMCW, our system achieved the best performance among all state-of-the-art PPI extraction systems reported so far. 
Traditionally, machine learning approaches for information extraction require human annotated data that can be costly and time-consuming to produce. However, in many cases, there already exists a database (DB) with schema related to the desired output, and records related to the expected input text. We present a conditional random ﬁeld (CRF) that aligns tokens of a given DB record and its realization in text. The CRF model is trained using only the available DB and unlabeled text with generalized expectation criteria. An annotation of the text induced from inferred alignments is used to train an information extractor. We evaluate our method on a citation extraction task in which alignments between DBLP database records and citation texts are used to train an extractor. Experimental results demonstrate an error reduction of 35% over a previous state-of-the-art method that uses heuristic alignments. 
Many named entities contain other named entities inside them. Despite this fact, the ﬁeld of named entity recognition has almost entirely ignored nested named entity recognition, but due to technological, rather than ideological reasons. In this paper, we present a new technique for recognizing nested named entities, by using a discriminative constituency parser. To train the model, we transform each sentence into a tree, with constituents for each named entity (and no other syntactic structure). We present results on both newspaper and biomedical corpora which contain nested named entities. In three out of four sets of experiments, our model outperforms a standard semi-CRF on the more traditional top-level entities. At the same time, we improve the overall F-score by up to 30% over the ﬂat model, which is unable to recover any nested entities. 
Information Extraction (IE) systems that extract role ﬁllers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role ﬁllers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-ﬁller recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context. 
This paper presents a parse-and-paraphrase paradigm to assess the degrees of sentiment for product reviews. Sentiment identification has been well studied; however, most previous work provides binary polarities only (positive and negative), and the polarity of sentiment is simply reversed when a negation is detected. The extraction of lexical features such as unigram/bigram also complicates the sentiment classification task, as linguistic structure such as implicit long-distance dependency is often disregarded. In this paper, we propose an approach to extracting adverb-adjective-noun phrases based on clause structure obtained by parsing sentences into a hierarchical representation. We also propose a robust general solution for modeling the contribution of adverbials and negation to the score for degree of sentiment. In an application involving extracting aspect-based pros and cons from restaurant reviews, we obtained a 45% relative improvement in recall through the use of parsing methods, while also improving precision. 
This work investigates design choices in modeling a discourse scheme for improving opinion polarity classiﬁcation. For this, two diverse global inference paradigms are used: a supervised collective classiﬁcation framework and an unsupervised optimization framework. Both approaches perform substantially better than baseline approaches, establishing the efﬁcacy of the methods and the underlying discourse scheme. We also present quantitative and qualitative analyses showing how the improvements are achieved. 
This paper studies sentiment analysis of conditional sentences. The aim is to determine whether opinions expressed on different topics in a conditional sentence are positive, negative or neutral. Conditional sentences are one of the commonly used language constructs in text. In a typical document, there are around 8% of such sentences. Due to the condition clause, sentiments expressed in a conditional sentence can be hard to determine. For example, in the sentence, if your Nokia phone is not good, buy this great Samsung phone, the author is positive about “Samsung phone” but does not express an opinion on “Nokia phone” (although the owner of the “Nokia phone” may be negative about it). However, if the sentence does not have “if”, the first clause is clearly negative. Although “if” commonly signifies a conditional sentence, there are many other words and constructs that can express conditions. This paper first presents a linguistic analysis of such sentences, and then builds some supervised learning models to determine if sentiments expressed on different topics in a conditional sentence are positive, negative or neutral. Experimental results on conditional sentences from 5 diverse domains are given to demonstrate the effectiveness of the proposed approach. 
 This paper investigates a new task, subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. We provide empirical evidence that SWSD is more feasible than full word sense disambiguation, and that it can be exploited to improve the performance of contextual subjectivity and sentiment analysis systems.  
We describe a novel approach for syntaxbased statistical MT, which builds on a variant of tree adjoining grammar (TAG). Inspired by work in discriminative dependency parsing, the key idea in our approach is to allow highly ﬂexible reordering operations during parsing, in combination with a discriminative model that can condition on rich features of the sourcelanguage string. Experiments on translation from German to English show improvements over phrase-based systems, both in terms of BLEU scores and in human evaluations. 
In this work, we propose two extensions of standard word lexicons in statistical machine translation: A discriminative word lexicon that uses sentence-level source information to predict the target words and a trigger-based lexicon model that extends IBM model 1 with a second trigger, allowing for a more ﬁne-grained lexical choice of target words. The models capture dependencies that go beyond the scope of conventional SMT models such as phraseand language models. We show that the models improve translation quality by 1% in BLEU over a competitive baseline on a large-scale task. 
We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle “non-local” features. Similar approximate inference techniques support efﬁcient parameter estimation with hidden variables. We use the decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on nonisomorphism. 
We present a method to align words in a bitext that combines elements of a traditional statistical approach with linguistic knowledge. We demonstrate this approach for Arabic-English, using an alignment lexicon produced by a statistical word aligner, as well as linguistic resources ranging from an English parser to heuristic alignment rules for function words. These linguistic heuristics have been generalized from a development corpus of 100 parallel sentences. Our aligner, UALIGN, outperforms both the commonly used GIZA++ aligner and the state-of-theart LEAF aligner on F-measure and produces superior scores in end-to-end statistical machine translation, +1.3 BLEU points over GIZA++, and +0.7 over LEAF. 
Combining information extraction systems yields signiﬁcantly higher quality resources than each system in isolation. In this paper, we generalize such a mixing of sources and features in a framework called Ensemble Semantics. We show very large gains in entity extraction by combining state-of-the-art distributional and patternbased systems with a large set of features from a webcrawl, query logs, and Wikipedia. Experimental results on a webscale extraction of actors, athletes and musicians show signiﬁcantly higher mean average precision scores (29% gain) compared with the current state of the art. 
A signiﬁcant portion of the world’s text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal speciﬁcity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by deﬁning a one-to-one correspondence between LDA’s latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA’s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-speciﬁc document snippets. As a multi-label text classiﬁer, our model is competitive with a discriminative baseline on a variety of datasets. 
Keyphrases are widely used as a brief summary of documents. Since manual assignment is time-consuming, various unsupervised ranking methods based on importance scores are proposed for keyphrase extraction. In practice, the keyphrases of a document should not only be statistically important in the document, but also have a good coverage of the document. Based on this observation, we propose an unsupervised method for keyphrase extraction. Firstly, the method ﬁnds exemplar terms by leveraging clustering techniques, which guarantees the document to be semantically covered by these exemplar terms. Then the keyphrases are extracted from the document using the exemplar terms. Our method outperforms sate-of-the-art graphbased ranking methods (TextRank) by 9.5% in F1-measure. 
One of the most desired information types when planning a trip to some place is the knowledge of transport, roads and geographical connectedness of prominent sites in this place. While some transport companies or repositories make some of this information accessible, it is not easy to ﬁnd, and the majority of information about uncommon places can only be found in web free text such as blogs and forums. In this paper we present an algorithmic framework which allows an automated acquisition of map-like information from the web, based on surface patterns like “from X to Y”. Given a set of locations as initial seeds, we retrieve from the web an extended set of locations and produce a map-like network which connects these locations using transport type edges. We evaluate our framework in several settings, producing meaningful and precise connection sets. 
In this paper, we address the issue of automatic extending lexical resources by exploiting existing knowledge repositories. In particular, we deal with the new task of linking FrameNet and Wikipedia using a word sense disambiguation system that, for a given pair frame – lexical unit (F, l), ﬁnds the Wikipage that best expresses the the meaning of l. The mapping can be exploited to straightforwardly acquire new example sentences and new lexical units, both for English and for all languages available in Wikipedia. In this way, it is possible to easily acquire good-quality data as a starting point for the creation of FrameNet in new languages. The evaluation reported both for the monolingual and the multilingual expansion of FrameNet shows that the approach is promising. 
Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We ﬁnd that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations. 
Machine involvement has the potential to speed up language documentation. We assess this potential with timed annotation experiments that consider annotator expertise, example selection methods, and suggestions from a machine classiﬁer. We ﬁnd that better example selection and label suggestions improve efﬁciency, but effectiveness depends strongly on annotator expertise. Our expert performed best with uncertainty selection, but gained little from suggestions. Our non-expert performed best with random selection and suggestions. The results underscore the importance both of measuring annotation cost reductions with respect to time and of the need for cost-sensitive learning methods that adapt to annotators. 
We present a fully automatic method for content selection evaluation in summarization that does not require the creation of human model summaries. Our work capitalizes on the assumption that the distribution of words in the input and an informative summary of that input should be similar to each other. Results on a large scale evaluation from the Text Analysis Conference show that input-summary comparisons are very effective for the evaluation of content selection. Our automatic methods rank participating systems similarly to manual model-based pyramid evaluation and to manual human judgments of responsiveness. The best feature, JensenShannon divergence, leads to a correlation as high as 0.88 with manual pyramid and 0.73 with responsiveness evaluations. 
We propose a novel unsupervised approach for distinguishing literal and non-literal use of idiomatic expressions. Our model combines an unsupervised and a supervised classiﬁer. The former bases its decision on the cohesive structure of the context and labels training data for the latter, which can then take a larger feature space into account. We show that a combination of both classiﬁers leads to signiﬁcant improvements over using the unsupervised classiﬁer alone. 
A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006). In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times. 
Automatically detecting human social intentions from spoken conversation is an important task for dialogue understanding. Since the social intentions of the speaker may differ from what is perceived by the hearer, systems that analyze human conversations need to be able to extract both the perceived and the intended social meaning. We investigate this difference between intention and perception by using a spoken corpus of speed-dates in which both the speaker and the listener rated the speaker on ﬂirtatiousness. Our ﬂirtationdetection system uses prosodic, dialogue, and lexical features to detect a speaker’s intent to ﬂirt with up to 71.5% accuracy, signiﬁcantly outperforming the baseline, but also outperforming the human interlocuters. Our system addresses lexical feature sparsity given the small amount of training data by using an autoencoder network to map sparse lexical feature vectors into 30 compressed features. Our analysis shows that humans are very poor perceivers of intended ﬂirtatiousness, instead often projecting their own intended behavior onto their interlocutors. 
We present an implicit discourse relation classiﬁer in the Penn Discourse Treebank (PDTB). Our classiﬁer considers the context of the two arguments, word pair information, as well as the arguments’ internal constituent and dependency parses. Our results on the PDTB yields a signiﬁcant 14.1% improvement over the baseline. In our error analysis, we discuss four challenges in recognizing implicit relations in the PDTB. 
Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difﬁcult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step. 
Binarization of Synchronous Context Free Grammars (SCFG) is essential for achieving polynomial time complexity of decoding for SCFG parsing based machine translation systems. In this paper, we first investigate the excess edge competition issue caused by a leftheavy binary SCFG derived with the method of Zhang et al. (2006). Then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary SCFGs. We present an algorithm that iteratively improves the resulting binary SCFG, and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous binarization method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 
In this work we present a novel technique to rescore fragments in the Data-Oriented Translation model based on their contribution to translation accuracy. We describe three new rescoring methods, and present the initial results of a pilot experiment on a small subset of the Europarl corpus. This work is a proof-of-concept, and is the ﬁrst step in directly optimizing translation decisions solely on the hypothesized accuracy of potential translations resulting from those decisions. 
Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called “low density” languages. But pivoting requires additional parallel texts. We address this problem by deriving paraphrases monolingually, using distributional semantic similarity measures, thus providing access to larger training resources, such as comparable and unrelated monolingual corpora. We present what is to our knowledge the ﬁrst successful integration of a collocational approach to untranslated words with an end-to-end, state of the art SMT system demonstrating signiﬁcant translation improvements in a low-resource setting. 
This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it compares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). It is found that a model free approach signiﬁcantly outperforms T3 on the particular data we created from the Internet. We also discuss what might have caused T3’s poor performance. 
This paper presents an effective method for generating natural language sentences from their underlying meaning representations. The method is built on top of a hybrid tree representation that jointly encodes both the meaning representation as well as the natural language in a tree structure. By using a tree conditional random ﬁeld on top of the hybrid tree representation, we are able to explicitly model phrase-level dependencies amongst neighboring natural language phrases and meaning representation components in a simple and natural way. We show that the additional dependencies captured by the tree conditional random ﬁeld allows it to perform better than directly inverting a previously developed hybrid tree semantic parser. Furthermore, we demonstrate that the model performs better than a previous state-of-the-art natural language generation model. Experiments are performed on two benchmark corpora with standard automatic evaluation metrics. 
This paper shows that discriminative reranking with an averaged perceptron model yields substantial improvements in realization quality with CCG. The paper conﬁrms the utility of including language model log probabilities as features in the model, which prior work on discriminative training with log linear models for HPSG realization had called into question. The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpus-engineered grammar. 
Experiments are reported that investigate the effect of various source document representations on the accuracy of the sentence extraction phase of a multidocument summarisation task. A novel representation is introduced based on generic relation extraction (GRE), which aims to build systems for relation identiﬁcation and characterisation that can be transferred across domains and tasks without modiﬁcation of model parameters. Results demonstrate performance that is signiﬁcantly higher than a non-trivial baseline that uses tf*idf -weighted words and at least as good as a comparable but less general approach from the literature. Analysis shows that the representations compared are complementary, suggesting that extraction performance could be further improved through system combination. 
In this paper we propose a novel statistical language model to capture long-range semantic dependencies. Speciﬁcally, we apply the concept of semantic composition to the problem of constructing predictive history representations for upcoming words. We also examine the inﬂuence of the underlying semantic space on the composition task by comparing spatial semantic representations against topic-based ones. The composition models yield reductions in perplexity when combined with a standard n-gram language model over the n-gram model alone. We also obtain perplexity reductions when integrating our models with a structured language model. 
Word sense disambiguation is typically phrased as the task of labeling a word in context with the best-ﬁtting sense from a sense inventory such as WordNet. While questions have often been raised over the choice of sense inventory, computational linguists have readily accepted the bestﬁtting sense methodology despite the fact that the case for discrete sense boundaries is widely disputed by lexical semantics researchers. This paper studies graded word sense assignment, based on a recent dataset of graded word sense annotation. 
The sense of a preposition is related to the semantics of its dominating prepositional phrase. Knowing the sense of a preposition could help to correctly classify the semantic role of the dominating prepositional phrase and vice versa. In this paper, we propose a joint probabilistic model for word sense disambiguation of prepositions and semantic role labeling of prepositional phrases. Our experiments on the PropBank corpus show that jointly learning the word sense and the semantic role leads to an improvement over state-of-theart individual classiﬁer models on the two tasks. 
We report in this paper a way of doing Word Sense Disambiguation (WSD) that has its origin in multilingual MT and that is cognizant of the fact that parallel corpora, wordnets and sense annotated corpora are scarce resources. With respect to these resources, languages show different levels of readiness; however a more resource fortunate language can help a less resource fortunate language. Our WSD method can be applied to a language even when no sense tagged corpora for that language is available. This is achieved by projecting wordnet and corpus parameters from another language to the language in question. The approach is centered around a novel synset based multilingual dictionary and the empirical observation that within a domain the distribution of senses remains more or less invariant across languages. The effectiveness of our approach is verified by doing parameter projection and then running two different WSD algorithms. The accuracy values of approximately 75% (F1-score) for three languages in two different domains establish the fact that within a domain it is possible to circumvent the problem of scarcity of resources by projecting parameters like sense distributions, corpus-co-occurrences, conceptual distance, etc. from one language to another. 
Much NLP research on Multi-Word Expressions (MWEs) focuses on the discovery of new expressions, as opposed to the identiﬁcation in texts of known expressions. However, MWE identiﬁcation is not trivial because many expressions allow variation in form and differ in the range of variations they allow. We show that simple rule-based baselines do not perform identiﬁcation satisfactorily, and present a supervised learning method for identiﬁcation that uses sentence surface features based on expressions’ canonical form. To evaluate the method, we have annotated 3350 sentences from the British National Corpus, containing potential uses of 24 verbal MWEs. The method achieves an F-score of 94.86%, compared with 80.70% for the leading rule-based baseline. Our method is easily applicable to any expression type. Experiments in previous research have been limited to the compositional/non-compositional distinction, while we also test on sentences in which the words comprising the MWE appear but not as an expression. 
Statistical bilingual word alignment has been well studied in the context of machine translation. This paper adapts the bilingual word alignment algorithm to monolingual scenario to extract collocations from monolingual corpus. The monolingual corpus is first replicated to generate a parallel corpus, where each sentence pair consists of two identical sentences in the same language. Then the monolingual word alignment algorithm is employed to align the potentially collocated words in the monolingual sentences. Finally the aligned word pairs are ranked according to refined alignment probabilities and those with higher scores are extracted as collocations. We conducted experiments using Chinese and English corpora individually. Compared with previous approaches, which use association measures to extract collocations from the co-occurring word pairs within a given window, our method achieves higher precision and recall. According to human evaluation in terms of precision, our method achieves absolute improvements of 27.9% on the Chinese corpus and 23.6% on the English corpus, respectively. Especially, we can extract collocations with longer spans, achieving a high precision of 69% on the long-span (>6) Chinese collocations. 
The recently introduced online conﬁdence-weighted (CW) learning algorithm for binary classiﬁcation performs well on many binary NLP tasks. However, for multi-class problems CW learning updates and inference cannot be computed analytically or solved as convex optimization problems as they are in the binary case. We derive learning algorithms for the multi-class CW setting and provide extensive evaluation using nine NLP datasets, including three derived from the recently released New York Times corpus. Our best algorithm outperforms state-of-the-art online and batch methods on eight of the nine tasks. We also show that the conﬁdence information maintained during learning yields useful probabilistic information at test time. 
This paper explores two classes of model adaptation methods for Web search ranking: Model Interpolation and error-driven learning approaches based on a boosting algorithm. The results show that model interpolation, though simple, achieves the best results on all the open test sets where the test data is very different from the training data. The tree-based boosting algorithm achieves the best performance on most of the closed test sets where the test data and the training data are similar, but its performance drops significantly on the open test sets due to the instability of trees. Several methods are explored to improve the robustness of the algorithm, with limited success.  
This paper addresses the issue of extracting contexts and answers of questions from post discussion of online forums. We propose a novel and uniﬁed model by customizing the structural Support Vector Machine method. Our customization has several attractive properties: (1) it gives a comprehensive graphical representation of thread discussion. (2) It designs special inference algorithms instead of generalpurpose ones. (3) It can be readily extended to different task preferences by varying loss functions. Experimental results on a real data set show that our methods are both promising and ﬂexible. 
User clicks on a URL in response to a query are extremely useful predictors of the URL’s relevance to that query. Exact match click features tend to suffer from severe data sparsity issues in web ranking. Such sparsity is particularly pronounced for new URLs or long queries where each distinct query-url pair will rarely occur. To remedy this, we present a set of straightforward yet informative query-url n-gram features that allows for generalization of limited user click data to large amounts of unseen query-url pairs. The method is motivated by techniques leveraged in the NLP community for dealing with unseen words. We find that there are interesting regularities across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value < 0.0001 level over a strong baseline with exact match clickthrough features. 
The ambiguity of person names in the Web has become a new area of interest for NLP researchers. This challenging problem has been formulated as the task of clustering Web search results (returned in response to a person name query) according to the individual they mention. In this paper we compare the coverage, reliability and independence of a number of features that are potential information sources for this clustering task, paying special attention to the role of named entities in the texts to be clustered. Although named entities are used in most approaches, our results show that, independently of the Machine Learning or Clustering algorithm used, named entity recognition and classiﬁcation per se only make a small contribution to solve the problem. 
In this paper, we investigate how an accurate question classiﬁer contributes to a question answering system. We ﬁrst present a Maximum Entropy (ME) based question classiﬁer which makes use of head word features and their WordNet hypernyms. We show that our question classiﬁer can achieve the state of the art performance in the standard UIUC question dataset. We then investigate quantitatively the contribution of this question classiﬁer to a feature driven question answering system. With our accurate question classiﬁer and some standard question answer features, our question answering system performs close to the state of the art using TREC corpus. 
This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semisupervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The ﬁrst extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to secondorder parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech. 
This paper presents a simple and effective approach to improve dependency parsing by using subtrees from auto-parsed data. First, we use a baseline parser to parse large-scale unannotated data. Then we extract subtrees from dependency parse trees in the auto-parsed data. Finally, we construct new subtree-based features for parsing algorithms. To demonstrate the effectiveness of our proposed approach, we present the experimental results on the English Penn Treebank and the Chinese Penn Treebank. These results show that our approach signiﬁcantly outperforms baseline systems. And, it achieves the best accuracy for the Chinese data and an accuracy which is competitive with the best known systems for the English data. 
While traditional work on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the author’s mood, gender, age, or sentiment. Without knowing the user’s intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires. To address this problem, we propose a novel way of incorporating user feedback into a clustering algorithm, which allows a user to easily specify the dimension along which she wants the data points to be clustered via inspecting only a small number of words. This distinguishes our method from existing ones, which typically require a large amount of effort on the part of humans in the form of document annotation or interactive construction of the feature space. We demonstrate the viability of our method on several challenging sentiment datasets. 
Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. There are a number of such lexical resources available, but it is often suboptimal to use them as is, because general purpose lexical resources do not reﬂect domain-speciﬁc lexical usage. In this paper, we propose a novel method based on integer linear programming that can adapt an existing lexicon into a new one to reﬂect the characteristics of the data more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of ﬁne-grained polarity classiﬁcation. 
Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on signiﬁcant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of afﬁxes. Further, the lexicon has properties that support the Polyanna Hypothesis. Using the General Inquirer as gold standard, we show that our lexicon has 14 percentage points more correct entries than the leading WordNet-based high-coverage lexicon (SentiWordNet). In an extrinsic evaluation, we obtain signiﬁcantly higher performance in determining phrase polarity using our thesaurus-based lexicon than with any other. Additionally, we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above. 
We develop a general method to match unstructured text reviews to a structured list of objects. For this, we propose a language model for generating reviews that incorporates a description of objects and a generic review language model. This mixture model gives us a principled method to ﬁnd, given a review, the object most likely to be the topic of the review. Extensive experiments and analysis on reviews from Yelp show that our language model-based method vastly outperforms traditional tfidf-based methods. 
Mitchell et al. (2008) demonstrated that corpus-extracted models of semantic knowledge can predict neural activation patterns recorded using fMRI. This could be a very powerful technique for evaluating conceptual models extracted from corpora; however, fMRI is expensive and imposes strong constraints on data collection. Following on experiments that demonstrated that EEG activation patterns encode enough information to discriminate broad conceptual categories, we show that corpus-based semantic representations can predict EEG activation patterns with signiﬁcant accuracy, and we evaluate the relative performance of different corpus-models on this task. 
Distance-based (windowless) word assocation measures have only very recently appeared in the NLP literature and their performance compared to existing windowed or frequency-based measures is largely unknown. We conduct a largescale empirical comparison of a variety of distance-based and frequency-based measures for the reproduction of syntagmatic human assocation norms. Overall, our results show an improvement in the predictive power of windowless over windowed measures. This provides support to some of the previously published theoretical advantages and makes windowless approaches a promising avenue to explore further. This study also serves as a ﬁrst comparison of windowed methods across numerous human association datasets. During this comparison we also introduce some novel variations of window-based measures which perform as well as or better in the human association norm task than established measures. 
In previous research in automatic verb classiﬁcation, syntactic features have proved the most useful features, although manual classiﬁcations rely heavily on semantic features. We show, in contrast with previous work, that considerable additional improvement can be obtained by using semantic features in automatic classiﬁcation: verb selectional preferences acquired from corpus data using a fully unsupervised method. We report these promising results using a new framework for verb clustering which incorporates a recent subcategorization acquisition system, rich syntactic-semantic feature sets, and a variation of spectral clustering which performs particularly well in high dimensional feature space. 
Most existing information retrieval (IR) systems do not take much advantage of natural language processing (NLP) techniques due to the complexity and limited observed effectiveness of applying NLP to IR. In this paper, we demonstrate that substantial gains can be obtained over a strong baseline using NLP techniques, if properly handled. We propose a framework for deriving semantic text matching features from named entities identiﬁed in Web queries; we then utilize these features in a supervised machine-learned ranking approach, applying a set of emerging machine learning techniques. Our approach is especially useful for queries that contain multiple types of concepts. Comparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries. 
Inspired by the success of English grapheme-to-phoneme research in speech synthesis, many researchers have proposed phoneme-based English-to-Chinese transliteration models. However, such approaches have severely suffered from the errors in Chinese phoneme-to-grapheme conversion. To address this issue, we propose a new English-to-Chinese transliteration model and make systematic comparisons with the conventional models. Our proposed model relies on the joint use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration. 
Many approaches to unsupervised morphology acquisition incorporate the frequency of character sequences with respect to each other to identify word stems and afﬁxes. This typically involves heuristic search procedures and calibrating multiple arbitrary thresholds. We present a simple approach that uses no thresholds other than those involved in standard application of χ2 signiﬁcance testing. A key part of our approach is using document boundaries to constrain generation of candidate stems and afﬁxes and clustering morphological variants of a given word stem. We evaluate our model on English and the Mayan language Uspanteko; it compares favorably to two benchmark systems which use considerably more complex strategies and rely more on experimentally chosen threshold values. 
We extend previous work on fully unsupervised part-of-speech tagging. Using a non-parametric version of the HMM, called the inﬁnite HMM (iHMM), we address the problem of choosing the number of hidden states in unsupervised Markov models for PoS tagging. We experiment with two non-parametric priors, the Dirichlet and Pitman-Yor processes, on the Wall Street Journal dataset using a parallelized implementation of an iHMM inference algorithm. We evaluate the results with a variety of clustering evaluation metrics and achieve equivalent or better performances than previously reported. Building on this promising result we evaluate the output of the unsupervised PoS tagger as a direct replacement for the output of a fully supervised PoS tagger for the task of shallow parsing and compare the two evaluations. 
We propose a new model for unsupervised POS tagging based on linguistic distinctions between open and closed-class items. Exploiting notions from current linguistic theory, the system uses far less information than previous systems, far simpler computational methods, and far sparser descriptions in learning contexts. By applying simple language acquisition techniques based on counting, the system is given the closed-class lexicon, acquires a large open-class lexicon and then acquires disambiguation rules for both. This system achieves a 20% error reduction for POS tagging over state-of-the-art unsupervised systems tested under the same conditions, and achieves comparable accuracy when trained with much less prior information. 
Current statistical machine translation (SMT) systems are trained on sentencealigned and word-aligned parallel text collected from various sources. Translation model parameters are estimated from the word alignments, and the quality of the translations on a given test set depends on the parameter estimates. There are at least two factors affecting the parameter estimation: domain match and training data quality. This paper describes a novel approach for automatically detecting and down-weighing certain parts of the training corpus by assigning a weight to each sentence in the training bitext so as to optimize a discriminative objective function on a designated tuning set. This way, the proposed method can limit the negative effects of low quality training data, and can adapt the translation model to the domain of interest. It is shown that such discriminative corpus weights can provide signiﬁcant improvements in Arabic-English translation on various conditions, using a state-of-the-art SMT system. 
Training a statistical machine translation starts with tokenizing a parallel corpus. Some languages such as Chinese do not incorporate spacing in their writing system, which creates a challenge for tokenization. Moreover, morphologically rich languages such as Korean present an even bigger challenge, since optimal token boundaries for machine translation in these languages are often unclear. Both rule-based solutions and statistical solutions are currently used. In this paper, we present unsupervised methods to solve tokenization problem. Our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation. 
Tree Adjoining Grammars have well-known advantages, but are typically considered too difﬁcult for practical systems. We demonstrate that, when done right, adjoining improves translation quality without becoming computationally intractable. Using adjoining to model optionality allows general translation patterns to be learned without the clutter of endless variations of optional material. The appropriate modiﬁers can later be spliced in as needed. In this paper, we describe a novel method for learning a type of Synchronous Tree Adjoining Grammar and associated probabilities from aligned tree/string training data. We introduce a method of converting these grammars to a weakly equivalent tree transducer for decoding. Finally, we show that adjoining results in an end-to-end improvement of +0.8 BLEU over a baseline statistical syntax-based MT model on a large-scale Arabic/English MT task. 
This paper describes a time-series model for parsing transcribed speech containing disﬂuencies. This model differs from previous parsers in its explicit modeling of a buffer of recent words, which allows it to recognize repairs more easily due to the frequent overlap in words between errors and their repairs. The parser implementing this model is evaluated on the standard Switchboard transcribed speech parsing task for overall parsing accuracy and edited word detection. 
The recent availability of large corpora for training N-gram language models has shown the utility of models of higher order than just trigrams. In this paper, we investigate methods to control the increase in model size resulting from applying standard methods at higher orders. We introduce signiﬁcance-based N-gram selection, which not only reduces model size, but also improves perplexity for several smoothing methods, including Katz backoff and absolute discounting. We also show that, when combined with a new smoothing method and a novel variant of weighted-difference pruning, our selection method performs better in the trade-off between model size and perplexity than the best pruning method we found for modiﬁed Kneser-Ney smoothing. 
Randomised techniques allow very big language models to be represented succinctly. However, being batch-based they are unsuitable for modelling an unbounded stream of language whilst maintaining a constant error rate. We present a novel randomised language model which uses an online perfect hash function to efﬁciently deal with unbounded text streams. Translation experiments over a text stream show that our online randomised model matches the performance of batch-based LMs without incurring the computational overhead associated with full retraining. This opens up the possibility of randomised language models which continuously adapt to the massive volumes of texts published on the Web each day. 
While speaking spontaneously, speakers often make errors such as self-correction or false starts which interfere with the successful application of natural language processing techniques like summarization and machine translation to this data. There is active work on reconstructing this errorful data into a clean and ﬂuent transcript by identifying and removing these simple errors. Previous research has approximated the potential beneﬁt of conducting word-level reconstruction of simple errors only on those sentences known to have errors. In this work, we explore new approaches for automatically identifying speaker construction errors on the utterance level, and quantify the impact that this initial step has on word- and sentence-level reconstruction accuracy. 
Strictly corpus-based measures of semantic distance conﬂate co-occurrence information pertaining to the many possible senses of target words. We propose a corpus–thesaurus hybrid method that uses soft constraints to generate word-senseaware distributional proﬁles (DPs) from coarser “concept DPs” (derived from a Roget-like thesaurus) and sense-unaware traditional word DPs (derived from raw text). Although it uses a knowledge source, the method is not vocabularylimited: if the target word is not in the thesaurus, the method falls back gracefully on the word’s co-occurrence information. This allows the method to access valuable information encoded in a lexical resource, such as a thesaurus, while still being able to effectively handle domainspeciﬁc terms and named entities. Experiments on word-pair ranking by semantic distance show the new hybrid method to be superior to others. 
In this paper, we ﬁrst compare several strategies to handle the newly proposed three-way Recognizing Textual Entailment (RTE) task. Then we deﬁne a new measurement for a pair of texts, called Textual Relatedness, which is a weaker concept than semantic similarity or paraphrase. We show that an alignment model based on the predicate-argument structures using this measurement can help an RTE system to recognize the Unknown cases at the ﬁrst stage, and contribute to the improvement of the overall performance in the RTE task. In addition, several heterogeneous lexical resources are tested, and different contributions from them are observed. 
Measuring the similarity between two texts is a fundamental problem in many NLP and IR applications. Among the existing approaches, the cosine measure of the term vectors representing the original texts has been widely used, where the score of each term is often determined by a TFIDF formula. Despite its simplicity, the quality of such cosine similarity measure is usually domain dependent and decided by the choice of the termweighting function. In this paper, we propose a novel framework that learns the term-weighting function. Given the labeled pairs of texts as training data, the learning procedure tunes the model parameters by minimizing the speciﬁed loss function of the similarity score. Compared to traditional TFIDF term-weighting schemes, our approach shows a signiﬁcant improvement on tasks such as judging the quality of query suggestions and ﬁltering irrelevant ads for online advertising. 
Semantic similarity is a central concept that extends across numerous ﬁelds such as artiﬁcial intelligence, natural language processing, cognitive science and psychology. Accurate measurement of semantic similarity between words is essential for various tasks such as, document clustering, information retrieval, and synonym extraction. We propose a novel model of semantic similarity using the semantic relations that exist among words. Given two words, ﬁrst, we represent the semantic relations that hold between those words using automatically extracted lexical pattern clusters. Next, the semantic similarity between the two words is computed using a Mahalanobis distance measure. We compare the proposed similarity measure against previously proposed semantic similarity measures on Miller-Charles benchmark dataset and WordSimilarity353 collection. The proposed method outperforms all existing web-based semantic similarity measures, achieving a Pearson correlation coefﬁcient of 0.867 on the Millet-Charles dataset. 
This paper introduces a new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies, from seven different grammatical constructions. We run a series of off-theshelf parsers on the corpus to evaluate how well state-of-the-art parsing technology is able to recover such dependencies. The overall results range from 25% accuracy to 59%. These low scores call into question the validity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers. 
We connect two scenarios in structured learning: adapting a parser trained on one corpus to another annotation style, and projecting syntactic annotations from one language to another. We propose quasisynchronous grammar (QG) features for these structured learning tasks. That is, we score a aligned pair of source and target trees based on local features of the trees and the alignment. Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to a synchronous grammar, which would insist on some form of structural parallelism. In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence. On the more difﬁcult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments. Our experiments show that unsupervised QG projection improves on parses trained using only highprecision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone. When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. ∗The ﬁrst author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst. We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.  
We investigate the effectiveness of selftraining PCFG grammars with latent annotations (PCFG-LA) for parsing languages with different amounts of labeled training data. Compared to Charniak’s lexicalized parser, the PCFG-LA parser was more effectively adapted to a language for which parsing has been less well developed (i.e., Chinese) and beneﬁted more from selftraining. We show for the ﬁrst time that self-training is able to signiﬁcantly improve the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data. Our approach achieves stateof-the-art parsing accuracies for a single parser on both English (91.5%) and Chinese (85.2%). 
Applying statistical parsers developed for English to languages with freer wordorder has turned out to be harder than expected. This paper investigates the adequacy of different statistical parsing models for dealing with a (relatively) free word-order language. We show that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on conﬁgurational information. We conclude that the form-function separation ingrained in RR models makes them better suited for parsing nonconﬁgurational phenomena. 
Sets of lexical items sharing a signiﬁcant aspect of their meaning (concepts) are fundamental in linguistics and NLP. Manual concept compilation is labor intensive, error prone and subjective. We present a web-based concept extension algorithm. Given a set of terms specifying a concept in some language, we translate them to a wide range of intermediate languages, disambiguate the translations using web counts, and discover additional concept terms using symmetric patterns. We then translate the discovered terms back into the original language, score them, and extend the original concept by adding backtranslations having high scores. We evaluate our method in 3 source languages and 45 intermediate languages, using both human judgments and WordNet. In all cases, our cross-lingual algorithm signiﬁcantly improves high quality concept extension. 
Bilingual dictionaries are vital resources in many areas of natural language processing. Numerous methods of machine translation require bilingual dictionaries with large coverage, but less-frequent language pairs rarely have any digitalized resources. Since the need for these resources is increasing, but the human resources are scarce for less represented languages, efficient automatized methods are needed. This paper introduces a fully automated, robust pivot language based bilingual dictionary generation method that uses the WordNet of the pivot language to build a new bilingual dictionary. We propose the usage of WordNet in order to increase accuracy; we also introduce a bidirectional selection method with a flexible threshold to maximize recall. Our evaluations showed 79% accuracy and 51% weighted recall, outperforming representative pivot language based methods. A dictionary generated with this method will still need manual post-editing, but the improved recall and precision decrease the work of human correctors. 
We present a novel approach for multilingual document clustering using only comparable corpora to achieve cross-lingual semantic interoperability. The method models document collections as weighted graph, and supervisory information is given as sets of must-linked constraints for documents in different languages. Recursive k-nearest neighbor similarity propagation is used to exploit the prior knowledge and merge two language spaces. Spectral method is applied to ﬁnd the best cuts of the graph. Experimental results show that using limited supervisory information, our method achieves promising clustering results. Furthermore, since the method does not need any language dependent information in the process, our algorithm can be applied to languages in various alphabetical systems. 
Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. 
We have designed, implemented and evaluated an end-to-end system spellchecking and autocorrection system that does not require any manually annotated training data. The World Wide Web is used as a large noisy corpus from which we infer knowledge about misspellings and word usage. This is used to build an error model and an n-gram language model. A small secondary set of news texts with artiﬁcially inserted misspellings are used to tune conﬁdence classiﬁers. Because no manual annotation is required, our system can easily be instantiated for new languages. When evaluated on human typed data with real misspellings in English and German, our web-based systems outperform baselines which use candidate corrections based on hand-curated dictionaries. Our system achieves 3.8% total error rate in English. We show similar improvements in preliminary results on artiﬁcial data for Russian and Arabic. 
Models of language learning play a central role in a wide range of applications: from psycholinguistic theories of how people acquire new word knowledge, to information systems that can automatically match content to users’ reading ability. We present a novel statistical approach that can infer the distribution of a word’s likely acquisition age automatically from authentic texts collected from the Web. We then show that combining these acquisition age distributions for all words in a document provides an effective semantic component for predicting reading difﬁculty of new texts. We also compare our automatically inferred acquisition ages with norms from existing oral studies, revealing interesting historical trends as well as differences between oral and written word acquisition processes. 
In the early days of email, widely-used conventions for indicating quoted reply content and email signatures made it easy to segment email messages into their functional parts. Today, the explosion of different email formats and styles, coupled with the ad hoc ways in which people vary the structure and layout of their messages, means that simple techniques for identifying quoted replies that used to yield 95% accuracy now ﬁnd less than 10% of such content. In this paper, we describe Zebra, an SVM-based system for segmenting the body text of email messages into nine zone types based on graphic, orthographic and lexical cues. Zebra performs this task with an accuracy of 87.01%; when the number of zones is abstracted to two or three zone classes, this increases to 93.60% and 91.53% respectively. 
This paper presents a new method of developing a large-scale hyponymy relation database by combining Wikipedia and other Web documents. We attach new words to the hyponymy database extracted from Wikipedia by using distributional similarity calculated from documents on the Web. For a given target word, our algorithm first finds k similar words from the Wikipedia database. Then, the hypernyms of these k similar words are assigned scores by considering the distributional similarities and hierarchical distances in the Wikipedia database. Finally, new hyponymy relations are output according to the scores. In this paper, we tested two distributional similarities. One is based on raw verbnoun dependencies (which we call “RVD”), and the other is based on a large-scale clustering of verb-noun dependencies (called “CVD”). Our method achieved an attachment accuracy of 91.0% for the top 10,000 relations, and an attachment accuracy of 74.5% for the top 100,000 relations when using CVD. This was a far better outcome compared to the other baseline approaches. Excluding the region that had very high scores, CVD was found to be more effective than RVD. We also confirmed that most relations extracted by our method cannot be extracted merely by applying the well-known lexicosyntactic patterns to Web documents. 
Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task. Parallelization and optimizations are necessary. We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web. The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes. We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. 
Many algorithms extract terms from text together with some kind of taxonomic classiﬁcation (is-a) link. However, the general approaches used today, and speciﬁcally the methods of evaluating results, exhibit serious shortcomings. Harvesting without focusing on a speciﬁc conceptual area may deliver large numbers of terms, but they are scattered over an immense concept space, making Recall judgments impossible. Regarding Precision, simply judging the correctness of terms and their individual classiﬁcation links may provide high scores, but this doesn’t help with the eventual assembly of terms into a single coherent taxonomy. Furthermore, since there is no correct and complete gold standard to measure against, most work invents some ad hoc evaluation measure. We present an algorithm that is more precise and complete than previous ones for identifying from web text just those concepts ‘below’ a given seed term. Comparing the results to WordNet, we ﬁnd that the algorithm misses terms, but also that it learns many new terms not in WordNet, and that it classiﬁes them in ways acceptable to humans but different from WordNet. 
Machine learning offers a range of tools for training systems from data, but these methods are only as good as the underlying representation. This paper proposes to acquire representations for machine learning by reading text written to accommodate human learning. We propose a novel form of semantic analysis called reading to learn, where the goal is to obtain a high-level semantic abstract of multiple documents in a representation that facilitates learning. We obtain this abstract through a generative model that requires no labeled data, instead leveraging repetition across multiple documents. The semantic abstract is converted into a transformed feature space for learning, resulting in improved generalization on a relational learning task. 
Traditional learning-based coreference resolvers operate by training a mentionpair classiﬁer for determining whether two mentions are coreferent or not. Two independent lines of recent research have attempted to improve these mention-pair classiﬁers, one by learning a mentionranking model to rank preceding mentions for a given anaphor, and the other by training an entity-mention classiﬁer to determine whether a preceding cluster is coreferent with a given mention. We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 
Knowledge of noun phrase anaphoricity might be profitably exploited in coreference resolution to bypass the resolution of non-anaphoric noun phrases. However, it is surprising to notice that recent attempts to incorporate automatically acquired anaphoricity information into coreference resolution have been somewhat disappointing. This paper employs a global learning method in determining the anaphoricity of noun phrases via a label propagation algorithm to improve learningbased coreference resolution. In particular, two kinds of kernels, i.e. the feature-based RBF kernel and the convolution tree kernel, are employed to compute the anaphoricity similarity between two noun phrases. Experiments on the ACE 2003 corpus demonstrate the effectiveness of our method in anaphoricity determination of noun phrases and its application in learning-based coreference resolution. 
In this paper, we employ the centering theory in pronoun resolution from the semantic perspective. First, diverse semantic role features with regard to different predicates in a sentence are explored. Moreover, given a pronominal anaphor, its relative ranking among all the pronouns in a sentence, according to relevant semantic role information and its surface position, is incorporated. In particular, the use of both the semantic role features and the relative pronominal ranking feature in pronoun resolution is guided by extending the centering theory from the grammatical level to the semantic level in tracking the local discourse focus. Finally, detailed pronominal subcategory features are incorporated to enhance the discriminative power of both the semantic role features and the relative pronominal ranking feature. Experimental results on the ACE 2003 corpus show that the centeringmotivated features contribute much to pronoun resolution. 
The Person Cross Document Coreference systems depend on the context for making decisions on the possible coreferences between person name mentions. The amount of context required is a parameter that varies from corpora to corpora, which makes it difficult for usual disambiguation methods. In this paper we show that the amount of context required can be dynamically controlled on the basis of the prior probabilities of coreference and we present a new statistical model for the computation of these probabilities. The experiment we carried on a news corpus proves that the prior probabilities of coreference are an important factor for maintaining a good balance between precision and recall for cross document coreference systems. 
We apply machine learning to the Linear Ordering Problem in order to learn sentence-speciﬁc reordering models for machine translation. We demonstrate that even when these models are used as a mere preprocessing step for German-English translation, they signiﬁcantly outperform Moses’ integrated lexicalized reordering model. Our models are trained on automatically aligned bitext. Their form is simple but novel. They assess, based on features of the input sentence, how strongly each pair of input word tokens wi, wj would like to reverse their relative order. Combining all these pairwise preferences to ﬁnd the best global reordering is NP-hard. However, we present a non-trivial O(n3) algorithm, based on chart parsing, that at least ﬁnds the best reordering within a certain exponentially large neighborhood. We show how to iterate this reordering process within a local search algorithm, which we use in training. 
Current statistical machine translation systems usually extract rules from bilingual corpora annotated with 1-best alignments. They are prone to learn noisy rules due to alignment mistakes. We propose a new structure called weighted alignment matrix to encode all possible alignments for a parallel text compactly. The key idea is to assign a probability to each word pair to indicate how well they are aligned. We design new algorithms for extracting phrase pairs from weighted alignment matrices and estimating their probabilities. Our experiments on multiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in signiﬁcant less extraction time. 
We present a new phrase-based conditional exponential family translation model for statistical machine translation. The model operates on a feature representation in which sentence level translations are represented by enumerating all the known phrase level translations that occur inside them. This makes the model a good match with the commonly used phrase extraction heuristics. The model’s predictions are properly normalized probabilities. In addition, the model automatically takes into account information provided by phrase overlaps, and does not suffer from reference translation reachability problems. We have implemented an open source translation system Sinuhe based on the proposed translation model. Our experiments on Europarl and GigaFrEn corpora demonstrate that ﬁnding the unique MAP parameters for the model on large scale data is feasible with simple stochastic gradient methods. Sinuhe is fast and memory efﬁcient, and the BLEU scores obtained by it are only slightly inferior to those of Moses. 
In a linguistically-motivated syntax-based translation system, the entire translation process is normally carried out in two steps, translation rule matching and target sentence decoding using the matched rules. Both steps are very timeconsuming due to the tremendous number of translation rules, the exhaustive search in translation rule matching and the complex nature of the translation task itself. In this paper, we propose a hyper-tree-based fast algorithm for translation rule matching. Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules. 
In this paper we investigate temporal patterns of web search queries. We carry out several evaluations to analyze the properties of temporal proﬁles of queries, revealing promising semantic and pragmatic relationships between words. We focus on two applications: query suggestion and query categorization. The former shows a potential for time-series similarity measures to identify speciﬁc semantic relatedness between words, which results in state-of-the-art performance in query suggestion while providing complementary information to more traditional distributional similarity measures. The query categorization evaluation suggests that the temporal proﬁle alone is not a strong indicator of broad topical categories. 
A large body of recent research has been investigating the acquisition and application of applied inference knowledge. Such knowledge may be typically captured as entailment rules, applied over syntactic representations. Efﬁcient inference with such knowledge then becomes a fundamental problem. Starting out from a formalism for entailment-rule application we present a novel packed data-structure and a corresponding algorithm for its scalable implementation. We proved the validity of the new algorithm and established its efﬁciency analytically and empirically. 
We present a discriminative substring decoder for transliteration. This decoder extends recent approaches for discriminative character transduction by allowing for a list of known target-language words, an important resource for transliteration. Our approach improves upon Sherif and Kondrak’s (2007b) state-of-theart decoder, creating a 28.5% relative improvement in transliteration accuracy on a Japanese katakana-to-English task. We also conduct a controlled comparison of two feature paradigms for discriminative training: indicators and hybrid generative features. Surprisingly, the generative hybrid outperforms its purely discriminative counterpart, despite losing access to rich source-context features. Finally, we show that machine transliterations have a positive impact on machine translation quality, improving human judgments by 0.5 on a 4-point scale. 
The design of practical language applications by means of statistical approaches requires annotated data, which is one of the most critical constraint. This is particularly true for Spoken Dialog Systems since considerably domain-speciﬁc conceptual annotation is needed to obtain accurate Language Understanding models. Since data annotation is usually costly, methods to reduce the amount of data are needed. In this paper, we show that better feature representations serve the above purpose and that structure kernels provide the needed improved representation. Given the relatively high computational cost of kernel methods, we apply them to just re-rank the list of hypotheses provided by a fast generative model. Experiments with Support Vector Machines and different kernels on two different dialog corpora show that our re-ranking models can achieve better results than state-of-the-art approaches when small data is available. 
There have been increasing needs for task speciﬁc rankings in web search such as rankings for speciﬁc query segments like long queries, time-sensitive queries, navigational queries, etc; or rankings for speciﬁc domains/contents like answers, blogs, news, etc. In the spirit of ”divide-andconquer”, task speciﬁc ranking may have potential advantages over generic ranking since different tasks have task-speciﬁc features, data distributions, as well as featuregrade correlations. A critical problem for the task-speciﬁc ranking is training data insufﬁciency, which may be solved by using the data extracted from click log. This paper empirically studies how to appropriately exploit click data to improve rank function learning in task-speciﬁc ranking. The main contributions are 1) the exploration on the utilities of two promising approaches for click pair extraction; 2) the analysis of the role played by the noise information which inevitably appears in click data extraction; 3) the appropriate strategy for combining training data and click data; 4) the comparison of click data which are consistent and inconsistent with baseline function. 
Recently system combination has been shown to be an effective way to improve translation quality over single machine translation systems. In this paper, we present a simple and effective method to systematically derive an ensemble of SMT systems from one baseline linear SMT model for use in system combination. Each system in the resulting ensemble is based on a feature set derived from the features of the baseline model (typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 
Current system combination methods usually use confusion networks to ﬁnd consensus translations among different systems. Requiring one-to-one mappings between the words in candidate translations, confusion networks have difﬁculty in handling more general situations in which several words are connected to another several words. Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations. Experiments show that our approach achieves signiﬁcant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 
We present a scalable joint language model designed to utilize ﬁne-grain syntactic tags. We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora. We advocate the use of relatively simple tags that do not require deep linguistic knowledge of the language but provide more structural information than POS tags and can be derived from automatically generated parse trees – a combination of properties that allows easy adoption of this model for new languages. We propose two ﬁne-grain tagsets and evaluate our model using these tags, as well as POS tags and SuperARV tags in a speech recognition task and discuss future directions. 
This paper investigates the effect of direction in phrase-based statistial machine translation decoding. We compare a typical phrase-based machine translation decoder using a left-to-right decoding strategy to a right-to-left decoder. We also investigate the effectiveness of a bidirectional decoding strategy that integrates both mono-directional approaches, with the aim of reducing the effects due to language speciﬁcity. Our experimental evaluation was extensive, based on 272 different language pairs, and gave the surprising result that for most of the language pairs, it was better decode from right-to-left than from left-to-right. As expected the relative performance of left-to-right and rightto-left strategies proved to be highly language dependent. The bidirectional approach outperformed the both the left-toright strategy and the right-to-left strategy, showing consistent improvements that appeared to be unrelated to the speciﬁc languages used for translation. Bidirectional decoding gave rise to an improvement in performance over a left-to-right decoding strategy in terms of the BLEU score in 99% of our experiments. 
We describe a process for automatically detecting decision-making sub-dialogues in multi-party, human-human meetings in real-time. Our basic approach to decision detection involves distinguishing between different utterance types based on the roles that they play in the formulation of a decision. In this paper, we describe how this approach can be implemented in real-time, and show that the resulting system’s performance compares well with other detectors, including an off-line version. 
We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking. While standard models require as many as 100K distinct features, we derive models with as little as 1K features that perform as well or better on different domains. These robust reduced models indicate that the way rare lexical features contribute to classiﬁcation in NLP is not fully understood. Contrastive error analysis (with and without lexical features) indicates that lexical features do contribute to resolving some semantic and complex syntactic ambiguities – but we ﬁnd this contribution does not generalize outside the training corpus. As a general strategy, we believe lexical features should not be directly derived from a training corpus but instead, carefully inferred and selected from other sources. 
Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems). 
In this paper, we provide descriptive and empirical approaches to effectively extracting underlying dependencies among parsing errors. In the descriptive approach, we deﬁne some combinations of error patterns and extract them from given errors. In the empirical approach, on the other hand, we re-parse a sentence with a target error corrected and observe errors corrected together. Experiments on an HPSG parser show that each of these approaches can clarify the dependencies among individual errors from each point of view. Moreover, the comparison between the results of the two approaches shows that combining these approaches can achieve a more detailed error analysis. 
Textual entailment recognition plays a fundamental role in tasks that require indepth natural language understanding. In order to use entailment recognition technologies for real-world applications, a large-scale entailment knowledge base is indispensable. This paper proposes a conditional probability based directional similarity measure to acquire verb entailment pairs on a large scale. We targeted 52,562 verb types that were derived from 108 Japanese Web documents, without regard for whether they were used in daily life or only in speciﬁc ﬁelds. In an evaluation of the top 20,000 verb entailment pairs acquired by previous methods and ours, we found that our similarity measure outperformed the previous ones. Our method also worked well for the top 100,000 results. 
Recent syntactic extensions of statistical translation models work with a synchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus. The decoders accompanying these extensions typically exceed quadratic time complexity. This paper extends the Direct Translation Model 2 (DTM2) with syntax while maintaining linear-time decoding. We employ a linear-time parsing algorithm based on an eager, incremental interpretation of Combinatory Categorial Grammar (CCG). As every input word is processed, the local parsing decisions resolve ambiguity eagerly, by selecting a single supertag–operator pair for extending the dependency parse incrementally. Alongside translation features extracted from the derived parse tree, we explore syntactic features extracted from the incremental derivation process. Our empirical experiments show that our model signiﬁcantly outperforms the state-of-the art DTM2 system. 
In this paper, we address the task of crosslingual semantic relatedness. We introduce a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages. Through experiments performed on several language pairs, we show that the method performs well, with a performance comparable to monolingual measures of relatedness. 
System combination has emerged as a powerful method for machine translation (MT). This paper pursues a joint optimization strategy for combining outputs from multiple MT systems, where word alignment, ordering, and lexical selection decisions are made jointly according to a set of feature functions combined in a single log-linear model. The decoding algorithm is described in detail and a set of new features that support this joint decoding approach is proposed. The approach is evaluated in comparison to state-of-the-art confusion-network-based system combination methods using equivalent features and shown to outperform them significantly. 
We introduce an extension to CCG that allows form and function to be represented simultaneously, reducing the proliferation of modiﬁer categories seen in standard CCG analyses. We can then remove the non-combinatory rules CCGbank uses to address this problem, producing a grammar that is fully lexicalised and far less ambiguous. There are intrinsic beneﬁts to full lexicalisation, such as semantic transparency and simpler domain adaptation. The clearest advantage is a 52-88% improvement in parse speeds, which comes with only a small reduction in accuracy. 
Jointly parsing two languages has been shown to improve accuracies on either or both sides. However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations. Here we propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser learns to exploit reorderings as additional observation, but not bothering to build the target-side tree as well. We show speciﬁcally how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conﬂicts. Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art baseline, with negligible (∼6%) efﬁciency overhead, thus much faster than biparsing. 
There have been considerable attempts to incorporate semantic knowledge into coreference resolution systems: different knowledge sources such as WordNet and Wikipedia have been used to boost the performance. In this paper, we propose new ways to extract WordNet feature. This feature, along with other features such as named entity feature, can be used to build an accurate semantic class (SC) classiﬁer. In addition, we analyze the SC classiﬁcation errors and propose to use relaxed SC agreement features. The proposed accurate SC classiﬁer and the relaxation of SC agreement features on ACE2 coreference evaluation can boost our baseline system by 10.4% and 9.7% using MUC score and anaphor accuracy respectively. 
We present a method for detecting and correcting multiple real-word spelling errors using the Google Web 1T 3-gram data set and a normalized and modiﬁed version of the Longest Common Subsequence (LCS) string matching algorithm. Our method is focused mainly on how to improve the detection recall (the fraction of errors correctly detected) and the correction recall (the fraction of errors correctly amended), while keeping the respective precisions (the fraction of detections or amendments that are correct) as high as possible. Evaluation results on a standard data set show that our method outperforms two other methods on the same task. 
In this paper, we present a semi-supervised method for automatic speech act recognition in email and forums. The major challenge of this task is due to lack of labeled data in these two genres. Our method leverages labeled data in the SwitchboardDAMSL and the Meeting Recorder Dialog Act database and applies simple domain adaptation techniques over a large amount of unlabeled email and forum data to address this problem. Our method uses automatically extracted features such as phrases and dependency trees, called subtree features, for semi-supervised learning. Empirical results demonstrate that our model is effective in email and forum speech act recognition. 
This paper employs morphological structures and relations between sentence segments for opinion analysis on words and sentences. Chinese words are classified into eight morphological types by two proposed classifiers, CRF classifier and SVM classifier. Experiments show that the injection of morphological information improves the performance of the word polarity detection. To utilize syntactic structures, we annotate structural trios to represent relations between sentence segments. Experiments show that considering structural trios is useful for sentence opinion analysis. The best f-score achieves 0.77 for opinion word extraction, 0.62 for opinion word polarity detection, 0.80 for opinion sentence extraction, and 0.54 for opinion sentence polarity detection. 
We present a system that ﬁnds short definitions of terms on Web pages. It employs a Maximum Entropy classiﬁer, but it is trained on automatically generated examples; hence, it is in effect unsupervised. We use ROUGE-W to generate training examples from encyclopedias and Web snippets, a method that outperforms an alternative centroid-based one. After training, our system can be used to ﬁnd deﬁnitions of terms that are not covered by encyclopedias. The system outperforms a comparable publicly available system, as well as a previously published form of our system. 
Virtual evidence (VE), ﬁrst introduced by (Pearl, 1988), provides a convenient way of incorporating prior knowledge into Bayesian networks. This work generalizes the use of VE to undirected graphical models and, in particular, to conditional random ﬁelds (CRFs). We show that VE can be naturally encoded into a CRF model as potential functions. More importantly, we propose a novel semisupervised machine learning objective for estimating a CRF model integrated with VE. The objective can be optimized using the Expectation-Maximization algorithm while maintaining the discriminative nature of CRFs. When evaluated on the CLASSIFIEDS data, our approach significantly outperforms the best known solutions reported on this task. 
This paper proposes a novel method to reﬁne the grammars in parsing by utilizing semantic knowledge from HowNet. Based on the hierarchical state-split approach, which can reﬁne grammars automatically in a data-driven manner, this study introduces semantic knowledge into the splitting process at two steps. Firstly, each part-of-speech node will be annotated with a semantic tag of its terminal word. These new tags generated in this step are semantic-related, which can provide a good start for splitting. Secondly, a knowledge-based criterion is used to supervise the hierarchical splitting of these semantic-related tags, which can alleviate overﬁtting. The experiments are carried out on both Chinese and English Penn Treebank show that the reﬁned grammars with semantic knowledge can improve parsing performance signiﬁcantly. Especially with respect to Chinese, our parser achieves an F1 score of 87.5%, which is the best published result we are aware of. 
We examine the problem of overcoming noisy word-level alignments when learning tree-to-string translation rules. Our approach introduces new rules, and reestimates rule probabilities using EM. The major obstacles to this approach are the very reasons that word-alignments are used for rule extraction: the huge space of possible rules, as well as controlling overﬁtting. By carefully controlling which portions of the original alignments are reanalyzed, and by using Bayesian inference during re-analysis, we show signiﬁcant improvement over the baseline rules extracted from word-level alignments. 
This paper connects two research areas: automatic tagging on the web and statistical keyphrase extraction. First, we analyze the quality of tags in a collaboratively created folksonomy using traditional evaluation techniques. Next, we demonstrate how documents can be tagged automatically with a state-of-the-art keyphrase extraction algorithm, and further improve performance in this new domain using a new algorithm, “Maui”, that utilizes semantic information extracted from Wikipedia. Maui outperforms existing approaches and extracts tags that are competitive with those assigned by the best performing human taggers. 
The work presented in this paper explores a supervised method for learning a probabilistic model of a lexicon of VerbNet classes. We intend for the probabilistic model to provide a probability distribution of verb-class associations, over known and unknown verbs, including polysemous words. In our approach, training instances are obtained from an existing lexicon and/or from an annotated corpus, while the features, which represent syntactic frames, semantic similarity, and selectional preferences, are extracted from unannotated corpora. Our model is evaluated in type-level verb classiﬁcation tasks: we measure the prediction accuracy of VerbNet classes for unknown verbs, and also measure the dissimilarity between the learned and observed probability distributions. We empirically compare several settings for model learning, while we vary the use of features, source corpora for feature extraction, and disambiguated corpora. In the task of verb classiﬁcation into all VerbNet classes, our best model achieved a 10.69% error reduction in the classiﬁcation accuracy, over the previously proposed model. 
The use of lexical semantic knowledge in information retrieval has been a ﬁeld of active study for a long time. Collaborative knowledge bases like Wikipedia and Wiktionary, which have been applied in computational methods only recently, offer new possibilities to enhance information retrieval. In order to ﬁnd the most beneﬁcial way to employ these resources, we analyze the lexical semantic relations that hold among query and document terms and compare how these relations are represented by a measure for semantic relatedness. We explore the potential of different indicators of document relevance that are based on semantic relatedness and compare the characteristics and performance of the knowledge bases Wikipedia, Wiktionary and WordNet. 
In this research we aim to detect subjective sentences in multimodal conversations. We introduce a novel technique wherein subjective patterns are learned from both labeled and unlabeled data, using n-gram word sequences with varying levels of lexical instantiation. Applying this technique to meeting speech and email conversations, we gain signiﬁcant improvement over state-of-the-art approaches. Furthermore, we show that coupling the pattern-based approach with features that capture characteristics of general conversation structure yields additional improvement. 
We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resourcepoor source language X1 into a resourcerich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian→English (using Malay) and Spanish→English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data. 
This paper presents an investigation of the relation between words and their gender in two gendered languages: German and Romanian. Gender is an issue that has long preoccupied linguists and bafﬂed language learners. We verify the hypothesis that gender is dictated by the general sound patterns of a language, and that it goes beyond sufﬁxes or word endings. Experimental results on German and Romanian nouns show strong support for this hypothesis, as gender prediction can be done with high accuracy based on the form of the words. 
This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas semantics concerns to entity types and lexical sequences. We investigate the effectiveness of such representations in the automated relation extraction from texts. We process the above data by means of Support Vector Machines along with the syntactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combination of the above kernels achieves high effectiveness and signiﬁcantly improves the current state-of-the-art. 
This paper presents an approach to automatic acquisition of the argumentpredicate relations from a semantically annotated corpus. We use SALSA, a German newspaper corpus manually annotated with role-semantic information based on frame semantics. Since the relatively small size of SALSA does not allow to estimate the semantic relatedness in the extracted argument-predicate pairs, we use a larger corpus for ranking. Two experiments have been performed in order to evaluate the proposed approach. In the ﬁrst experiment we compare automatically extracted argument-predicate relations with the gold standard formed from associations provided by human subjects. In the second experiment we calculate correlation between automatic relatedness measure and human ranking of the extracted relations. 
Distinguishing speculative statements from factual ones is important for most biomedical text mining applications. We introduce an approach which is based on solving two sub-problems to identify speculative sentence fragments. The ﬁrst sub-problem is identifying the speculation keywords in the sentences and the second one is resolving their linguistic scopes. We formulate the ﬁrst sub-problem as a supervised classiﬁcation task, where we classify the potential keywords as real speculation keywords or not by using a diverse set of linguistic features that represent the contexts of the keywords. After detecting the actual speculation keywords, we use the syntactic structures of the sentences to determine their scopes. 
This paper presents preliminary results on the detection of cultural differences from people’s experiences in various countries from two perspectives: tourists and locals. Our approach is to develop probabilistic models that would provide a good framework for such studies. Thus, we propose here a new model, ccLDA, which extends over the Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and crosscollection mixture (ccMix) (Zhai et al., 2004) models on blogs and forums. We also provide a qualitative and quantitative analysis of the model on the cross-cultural data. 
We propose a novel objective function for discriminatively tuning log-linear machine translation models. Our objective explicitly optimizes the BLEU score of expected n-gram counts, the same quantities that arise in forestbased consensus and minimum Bayes risk decoding methods. Our continuous objective can be optimized using simple gradient ascent. However, computing critical quantities in the gradient necessitates a novel dynamic program, which we also present here. Assuming BLEU as an evaluation measure, our objective function has two principle advantages over standard max BLEU tuning. First, it speciﬁcally optimizes model weights for downstream consensus decoding procedures. An unexpected second beneﬁt is that it reduces overﬁtting, which can improve test set BLEU scores when using standard Viterbi decoding. 
Three methods are proposed to classify queries by intent (CQI), e.g., navigational, informational, commercial, etc. Following mixed-initiative dialog systems, search engines should distinguish navigational queries where the user is taking the initiative from other queries where there are more opportunities for system initiatives (e.g., suggestions, ads). The query intent problem has a number of useful applications for search engines, affecting how many (if any) advertisements to display, which results to return, and how to arrange the results page. Click logs are used as a substitute for annotation. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. We start with a simple Na¨ıve Bayes baseline that works well when there is plenty of training data. When training data is less plentiful, we back off to nearby URLs in a click graph, using a method similar to Word-Sense Disambiguation. Thus, we can infer that designer trench is commercial because it is close to www.saksﬁfthavenue.com, which is known to be commercial. The baseline method was designed for precision and the backoff method was designed for recall. Both methods are fast and do not require crawling webpages. We recommend a third method, a hybrid of the two, that does no harm when there is plenty of training data, and generalizes better when there isn’t, as a strong baseline for the CQI task. 
This paper presents a new approach to selecting the initial seed set using stratified sampling strategy in bootstrapping-based semi-supervised learning for semantic relation classification. First, the training data is partitioned into several strata according to relation types/subtypes, then relation instances are randomly sampled from each stratum to form the initial seed set. We also investigate different augmentation strategies in iteratively adding reliable instances to the labeled set, and find that the bootstrapping procedure may stop at a reasonable point to significantly decrease the training time without degrading too much in performance. Experiments on the ACE RDC 2003 and 2004 corpora show the stratified sampling strategy contributes more than the bootstrapping procedure itself. This suggests that a proper sampling strategy is critical in semi-supervised learning. 
There is plenty of evidence that emotion analysis has many valuable applications. In this study a blog emotion corpus is constructed for Chinese emotional expression analysis. This corpus contains manual annotation of eight emotional categories (expect, joy, love, surprise, anxiety, sorrow, angry and hate), emotion intensity, emotion holder/target, emotional word/phrase, degree word, negative word, conjunction, rhetoric, punctuation and other linguistic expressions that indicate emotion. Annotation agreement analyses for emotion classes and emotional words and phrases are described. Then, using this corpus, we explore emotion expressions in Chinese and present the analyses on them. 
This paper proposes a probabilistic model for associative anaphora resolution in Japanese. Associative anaphora is a type of bridging anaphora, in which the anaphor and its antecedent are not coreferent. Our model regards associative anaphora as a kind of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 
It is well known that pragmatic knowledge is useful and necessary in many difﬁcult language processing tasks, but because this knowledge is difﬁcult to acquire and process automatically, it is rarely used. We present an open information extraction technique for automatically extracting a particular kind of pragmatic knowledge from text, and we show how to integrate the knowledge into a Markov Logic Network model for quantiﬁer scope disambiguation. Our model improves quantiﬁer scope judgments in experiments. 
Most existing systems for Chinese Semantic Role Labeling (SRL) make use of full syntactic parses. In this paper, we evaluate SRL methods that take partial parses as inputs. We ﬁrst extend the study on Chinese shallow parsing presented in (Chen et al., 2006) by raising a set of additional features. On the basis of our shallow parser, we implement SRL systems which cast SRL as the classiﬁcation of syntactic chunks with IOB2 representation for semantic roles (i.e. semantic chunks). Two labeling strategies are presented: 1) directly tagging semantic chunks in onestage, and 2) identifying argument boundaries as a chunking task and labeling their semantic types as a classiﬁcation task. For both methods, we present encouraging results, achieving signiﬁcant improvements over the best reported SRL performance in the literature. Additionally, we put forward a rule-based algorithm to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL. 
In this paper we address the problem of identifying a broad range of term variations in Japanese web search queries, where these variations pose a particularly thorny problem due to the multiple character types employed in its writing system. Our method extends the techniques proposed for English spelling correction of web queries to handle a wider range of term variants including spelling mistakes, valid alternative spellings using multiple character types, transliterations and abbreviations. The core of our method is a statistical model built on the MART algorithm (Friedman, 2001). We show that both string and semantic similarity features contribute to identifying term variation in web search queries; specifically, the semantic similarity features used in our system are learned by mining user session and click-through logs, and are useful not only as model features but also in generating term variation candidates efficiently. The proposed method achieves 70% precision on the term variation identification task with the recall slightly higher than 60%, reducing the error rate of a naïve baseline by 38%. 
Argumentative Zoning (AZ) is an analysis of the argumentative and rhetorical structure of a scientiﬁc paper. It has been shown to be reliably used by independent human coders, and has proven useful for various information access tasks. Annotation experiments have however so far been restricted to one discipline, computational linguistics (CL). Here, we present a more informative AZ scheme with 15 categories in place of the original 7, and show that it can be applied to the life sciences as well as to CL. We use a domain expert to encode basic knowledge about the subject (such as terminology and domain speciﬁc rules for individual categories) as part of the annotation guidelines. Our results show that non-expert human coders can then use these guidelines to reliably annotate this scheme in two domains, chemistry and computational linguistics. 
Set expansion refers to expanding a partial set of “seed” objects into a more complete set. One system that does set expansion is SEAL (Set Expander for Any Language), which expands entities automatically by utilizing resources from the Web in a language-independent fashion. In this paper, we illustrated in detail the construction of character-level wrappers for set expansion implemented in SEAL. We also evaluated several kinds of wrappers for set expansion and showed that character-based wrappers perform better than HTML-based wrappers. In addition, we demonstrated a technique that extends SEAL to learn binary relational concepts (e.g., “x is the mayor of the city y”) from only two seeds. We also show that the extended SEAL has good performance on our evaluation datasets, which includes English and Chinese, thus demonstrating language-independence. 
Named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambiguous identiﬁer in a standard database. One approach to this task is supervised classiﬁcation. However, the availability of training data is often limited, and the available data sets tend to be imbalanced and, in some cases, heterogeneous. We propose a new method that distinguishes a named entity by ﬁnding the informative keywords in its surrounding context, and then trains a model to predict whether each keyword indicates the semantic class of the entity. While maintaining a comparable performance to supervised classiﬁcation, this method avoids using expensive manually annotated data for each new domain, and thus achieves better portability. 
Bootstrapping is the process of improving the performance of a trained classiﬁer by iteratively adding data that is labeled by the classiﬁer itself to the training set, and retraining the classiﬁer. It is often used in situations where labeled training data is scarce but unlabeled data is abundant. In this paper, we consider the problem of domain adaptation: the situation where training data may not be scarce, but belongs to a different domain from the target application domain. As the distribution of unlabeled data is different from the training data, standard bootstrapping often has difﬁculty selecting informative data to add to the training set. We propose an effective domain adaptive bootstrapping algorithm that selects unlabeled target domain data that are informative about the target domain and easy to automatically label correctly. We call these instances bridges, as they are used to bridge the source domain to the target domain. We show that the method outperforms supervised, transductive and bootstrapping algorithms on the named entity recognition task. 
In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them. By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level. This concept is then implemented for extracting relations between product features and expressions of opinions. Experimental evaluations show that the mining task can beneﬁt from phrase dependency parsing. 
This paper proposes a method that speeds up a classiﬁer trained with many conjunctive features: combinations of (primitive) features. The key idea is to precompute as partial results the weights of primitive feature vectors that appear frequently in the target NLP task. A trie compactly stores the primitive feature vectors with their weights, and it enables the classiﬁer to ﬁnd for a given feature vector its longest preﬁx feature vector whose weight has already been computed. Experimental results for a Japanese dependency parsing task show that our method speeded up the SVM and LLM classiﬁers of the parsers, which achieved accuracy of 90.84/90.71%, by a factor of 10.7/11.6. 
In this paper, we propose a linear model-based general framework to combine k-best parse outputs from multiple parsers. The proposed framework leverages on the strengths of previous system combination and re-ranking techniques in parsing by integrating them into a linear model. As a result, it is able to fully utilize both the logarithm of the probability of each k-best parse tree from each individual parser and any additional useful features. For feature weight tuning, we compare the simulated-annealing algorithm and the perceptron algorithm. Our experiments are carried out on both the Chinese and English Penn Treebank syntactic parsing task by combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively. 
Automated mining of novel documents or sentences from chronologically ordered documents or sentences is an open challenge in text mining. In this paper, we describe the preprocessing techniques for detecting novel Chinese text and discuss the inﬂuence of different Part of Speech (POS) ﬁltering rules on the detection performance. Experimental results on APWSJ and TREC 2004 Novelty Track data show that the Chinese novelty mining performance is quite different when choosing two dissimilar POS ﬁltering rules. Thus, the selection of words to represent Chinese text is of vital importance to the success of the Chinese novelty mining. Moreover, we compare the Chinese novelty mining performance with that of English and investigate the impact of preprocessing steps on detecting novel Chinese text, which will be very helpful for developing a Chinese novelty mining system. 
The problem of re-ranking initial retrieval results exploring the intrinsic structure of documents is widely researched in information retrieval (IR) and has attracted a considerable amount of time and study. However, one of the drawbacks is that those algorithms treat queries and documents separately. Furthermore, most of the approaches are predominantly built upon graph-based methods, which may ignore some hidden information among the retrieval set. This paper proposes a novel document reranking method based on Latent Dirichlet Allocation (LDA) which exploits the implicit structure of the documents with respect to original queries. Rather than relying on graphbased techniques to identify the internal structure, the approach tries to find the latent structure of “topics” or “concepts” in the initial retrieval set. Then we compute the distance between queries and initial retrieval results based on latent semantic information deduced. Empirical results demonstrate that the method can comfortably achieve significant improvement over various baseline systems. 
Terminology Databases (TDBs) are one of the most commonly used tools by translators. It has been suggested that a massive collaboration process like Wikipedia could have beneficial effects on TDBs, by spreading their creation and maintenance costs across a large number of individuals, and by fostering collaboration between terminologists, translators, domain experts, and even members of the general public. We refer to this process as Collaborative Multilingual Terminology (CMT). This paper describes how we designed and implemented software for supporting CMT, by combining features from both Terminology Database Management Systems and collaborative wiki systems. Our system, Tiki-CMT, is built on top of the TikiWiki Content Management System. 
The structure of terminological data The basic unit for terminology work is the concept. Multiple terms—synonyms—can represent the same concept within one subject area. Such synonyms can be distinguished by their various criteria such as their degree of acceptance or conformity to particular standards, or their use by a particular company or department. They may also be used differently in different geographical regions. For example, the terms “context”, “terminological context” and “example of use” are synonyms in the subject field terminology work. The first is a short form, the second the full form, and the third, a non-technical synonym. Their use in discussing terminology might be allowed, preferred and discouraged, respectively. This conceptual orientation is distinct from the alphabetic organization of a dictionary or a glossary. An English dictionary entry normally contains definitions of one or more concepts that can be represented by a single headword.3 A thesaurus is also alphabetically organized, listing synonyms for each individual concept. A monolingual glossary is a simple, alphabetically organized list of one-to-one correspondences between terms and the concepts which they represent. A terminology database, in contrast, is organized by concept. However, all of the information required to generate dictionaries, thesauri and glossaries is contained within a terminology database, even though the primary form of indexing in the database is different. Page 1 of 14  Because terms and their use vary according to factors such as region, company or product line, usage labels are necessary to distinguish between them. Administrative information is needed, for example to document who created or last modified a term, and whether the term is admitted, preferred or deprecated, for example. And grammatical information may also be useful, for example to determine if the term is a noun or a verb. The number and nature of textual supports and usage labels is determined by the purpose for which the terminological data is being stored. So we need a concept-oriented structure with various types of usage labels to distinguish the various terms associated with a concept. The lexical arrangement of a dictionary is not suitable for the storage of terminological data, as it does not completely reflect the necessary conceptual organization. Translators and terminology So far, I’ve described terminology work as a monolingual discipline. As such, it is best applied on the authoring and documentation side. As far as translators are concerned, this kind of monolingual terminology work is of limited application. It is, however, very relevant to our work, especially when it has not been adequately carried out, as it is one of the main factors governing the quality of the original document. The terminological data in use at an LSP is multilingual, and thus also has to take account of differences between the conceptual structures of the source and target languages. Different types of equivalence, such as inclusion as opposed to complete equivalence, play a greater role, and the terminologist’s job, which is already hard enough, becomes much more difficult. For this reason, multilingual terminology records require more sophisticated usage labels to differentiate between domains, regions and customers, for example, and to incorporate additional information. Added to the complexity of multilingual terminology work is the fact that individual translators and small translation companies are frequently faced with an extremely wide range of subject material. It is quite normal to work on a jeweller’s customer magazine, a popular science magazine covering the work of an international company’s corporate research department and an automotive company’s sustainability report, all on the same day. The demands on any collections of terminology are equally varied. Translators have a love-hate relationship with terminology. Terminology is concerned with meaning—the translator's essential interest—yet the resources available for terminological research are strictly limited. So translators and LSPs need access to short cuts such as specialist dictionaries and customers’ in-house terminology. In practice, LSPs need a wide range of terminological data, the exact scope of which is very dependent on the type of work being carried out. This data can be supplied by the customer or generated internally. Whereas an individual translator may only need a simple glossary containing source and target terms for a specific customer, any work involving machine translation is going to require the production of domain-specific dictionaries containing a great deal of linguistic information. However, while some customers will have their own terminological resources and will be willing and able to make them available to the translators, other customers will either regard their terminology as top secret intellectual property or have no awareness of what terminology is. Terminology for LSPs in practice Terminology work is best carried out by terminologists working with the developers, documenters and product managers. However, although it is reasonable to expect that customers have developed Page 2 of 14  their own terminology for their products, this isn’t always the case. This is despite the fact that terminology is a good investment for companies producing and exporting technology. One case study showed a return on investment of 172 per cent over the first three years of a project to establish terminology for a client4, and a clear approach to quantifying costs and ROI on such projects5 has also been presented. Nonetheless, many companies still haven’t developed monolingual terminology for internal use, let alone made it available for translation. Even when terminology has been established, there may be administrative, technical or political reasons why external service providers do not have access to it. When such a situation arises, the LSP has no choice but to start thinking about how it is going to ensure at least a minimal degree of terminological consistency. From the LSP's viewpoint, this terminology work is both a significant cost factor and an investment, albeit in many respects a very speculative one. The individual terms in terminology records should be stemmed or lemmatised. The capitalization should be correct for the term as it would appear in context—i.e. in the middle of a sentence in the target language. The singular form should be used, unless the term is always encountered in the plural form. Terminology records for use by a LSP are subject to a number of special considerations. For example, it may also be necessary to store an inflected form if this form is going to produce more and/or more accurate hits for the automatic terminology recognition module of a CAT suite. Sources of terminological data Common sources of terminological data for translators are the customers themselves, either knowingly when they provide material or otherwise, when Google is let loose on their websites; monolingual and multilingual dictionaries on paper or online; encyclopaedias and general interest publications on paper or online; specialist publications on paper or online; self-citation from previous jobs; and whatever the translator’s magpie brain has picked up over the years. Customers’ existing terminology Customers’ existing terminology can be of extremely variable quality. It can be lexically or conceptually organized, and delivered in any imaginable form. Such externally supplied terminology will usually be processed in some form of text-based file such as Word or Excel before being imported into the appropriate terminology database for the LSP’s preferred CAT tools suite. OCR or file format conversion may be necessary before this processing can be carried out. The processing itself may be also be complex and difficult, involving a substantial amount of manipulation to convert complex and disordered data into usable structures, and to correct, unify and lemmatise the resulting terminology records. Figure 1: Excerpt from an Excel list of terminology supplied by a customer The example shown is an excerpt from an Excel table with more than 3,500 rows. The first three rows (52–54) show one German term corresponding to three English terms. Row 55 is a simple Page 3 of 14  one-to-one correspondence, as is row 56, although in this case it doesn’t look like it. Rows 57 and 58 contain two German and two English terms. In this excerpt, the heavy borders do indicate the concepts, although this was not true throughout the file. Fortunately, Excel provides reasonably sophisticated string functions which can be used to tackle such problems. These functions can, for example, be used to mark characters such as commas or parentheses which frequently indicate multiple terms or term forms in a single column. Collection of terminology In-house, LSP employees can research terminology in advance using existing material such as supplied TMs, literature or any supplied bilingual corpora, and by actively searching for material to build a bilingual corpus for the specific job. They can also collect “terminology on the fly” during the translation stage and during the revision and review steps External suppliers such as freelance translators also carry out terminology work when translating texts. The challenges for an LSP here are, on the one hand, to ensure that the translator conforms to the use of established terminology and, on the other, to ensure that the benefits of any terminology work carried out by the external supplier are also available to the LSP and are communicated to any other suppliers and personnel working on the same project. People don’t always appreciate the lengths to which translators will go to turn in a good job. One of our regular freelancers sends us his terminology research notes cut-and-pasted from the results of various searches. These can run to 35 pages in a 1.5-MB file—for a 2,200-word translation. Our strategy when collecting terminology can be summed up as “record terminology which you feel the need to research.” The theory is that if one translator needs to look it up, we can save the next translator the effort. Conversely, translators sometimes fail to record terms which might take other translators hours to track down. In such a case, the reviser or reviewer applies the same test of “Do I need to research this?”. All of this is, of course, subject to the pressure of deadlines and dependent on the likelihood of repeat work from the customer in question. Training and assistance with issues such as lemmatisation, what information to put in which columns, and how to effectively research the information in the first place are essential, both for in-house personnel and for external suppliers if they are to cooperate in this area. Over the years, we have developed simple forms for glossaries containing the minimum necessary information to establish a terminology record. We supply freelancers with a simple MS Word template file in A4 landscape page format containing a style called Wordlist, which has tabs set for source term, target term, note and term source. Wordfast’s ability to support multiple glossaries in the form of tab-delimited text files is also very useful when working with and as an external supplier. We have defined our use of the columns in a Wordfast glossary as source term, target term, note, term source, context sentence and context source. The translator can, for example, define glossary 1 as supplied, job-specific terminology, glossary 2 as new terminology and glossary 3 as existing general terminology. New terms can be added to one or more glossaries as the terms are researched, and glossary 2 returned with the job. In-house, we use a simple form accessed through our intranet-based job-planning database to record terminology for specific projects. This form deliberately has space for only a limited amount of information. All terminology collected in this way is instantly available to any in-house personnel assigned to the project and can be validated and exported in MultiTerm format. Page 4 of 14  Figure 2: Intranet term entry screen Affordable strategies for terminological research in an LSP Affordable strategies for terminological research in an LSP are extremely constrained. Internet discipline, the use of known research sources such as bilingual corpora, and a structured approach to using Google are essential. Sites based on user-generated content are useful when approached with caution. Where sourced terminology is not available, a combination of informed creativity and consistent practice has to fill the gap. Particularly useful free sources can be divided into three categories, institutional databases and corpora, user-supported sites and search engines. Institutional terminology databases and corpora include the EU’s IATE and the Canadian government’s Termium Plus, both of which offer public access to large databases of multilingual terminology, and EUR-Lex, the EU’s website of legal and other public documents, which can be regarded as a multilingual corpus. Searching the corpus for a source language term and then calling up a bilingual display of the document is relatively easy. User-supported multilingual dictionaries and forums such as LEO and dict.cc can be helpful, but must be approached with caution, as the quality of much user-supported material online can vary between excellent and appalling. Linguee is a new development combining search engine technology with multilingual corpora and user feedback. Founded by an ex-Google postdoc and a colleague, it presents itself as “Linguee— The Web as a dictionary.” It currently offers German and English only, but the company states that other languages are planned and funded. Linguee crawls the Web looking for multilingual documents and also uses donated corpora. The material is rated and verified both automatically and by humans. User-supported features are also provided for editing and rating results. Linguee has rapidly become one of our regular search tools. It is, however, currently noticeable that a large proportion of the bilingual corpora used originate from .de domains. This results in a significant proportion of the results found appearing to be the product of non-native speakers or even machine translation. The in-house human and user-supported editing and verification provide mechanisms to alleviate this problem, but the use of a larger, and thus more balanced GermanEnglish corpus will undoubtedly further improve the quality of the results. What Linguee appears to do is similar in many ways to a structured approach to using Google. Simply including the name of the target language along with the source term will return any indexed multilingual Websites with the target language, and will also bring up many usersupported sites with appropriate entries. Such hits include forums such as ProZ’s Kudos system, which is mostly used by professional translators, and is of corresponding quality. Unfortunately, “search engine optimization” means that some hits are for dictionary and forum sites which do not Page 5 of 14  contain any useful information about the term, but invite you to add such information. It is easy to query Google with logical expressions, or expressions and phrases, while filtering by file type wanted. Searches can also be limited by domain, which can prove very useful when trying to establish whether a target term simply looks strange or is a direct translation by a non-native, nonspecialist translator. This feature can also be used to get an impression of the regional prevalence of a term, although the relative size of the domains search has to be taken into account when doing this. Other options on the Google Advanced Search screen are searches by Language, which is not particularly useful, as a large proportion of the Web seems to be declared as the default language, regardless of what the site actually contains. With so many possibilities for searches, the issue of cost vs. benefit becomes significant. One tool which makes it possible to define a number of searches with different parameters on different websites, then to execute them simultaneously on a selected term by means of a hot key combination is IntelliWebSearch, a compiled AutoHotKey script by Mike Farrell. This utility copies and cleans up a selected search term and sends it to a number of preset websites. Figure 3: IntelliWebSearch search dialog It is provided as freeware, but requires significant customization. The author offers paid training via the Web. Validation of terminology Terminology records generated during the course of a translation job should be validated before storage. Terminology established at the pre-translation stage should be validated at this stage, before it is distributed. When working with a translation process according to EN 15038, it makes sense to validate any terminology captured during the translation and revision stages at the revision stage. Final adjustments may be necessary at the review stage. The appropriate strategy is very dependent on the nature of the material to be translated. Where a substantial body of validated terminology already exists, pre-translation terminology work is usually unnecessary. Validation of a terminology record for translation use involves making sure that the data is useful, that it is correctly recorded and that its content is correct. The first step in validating a terminology record for an LSP has to be the question: “Is it useful?” Translators, especially if they lack experience of terminology work, sometimes record terms that are not especially useful, such as relatively common words used in conventional ways. Such records can be discarded before wasting more time on them. Page 6 of 14  Correct recording requires that the term itself be recorded in a standardized form—lemmatized or stemmed. The correct form for recording a term will be specific to each language and part of speech, but will normally represent the simplest form. For an English noun, this is the singular form, written in lower case—unless it is a proper noun or a trade name written in capitals. For English verbs it is conventional to drop the infinitive particle. German nouns must be recorded singular, capitalized and in the nominative case. Correct recording also requires that all of the information necessary for a terminology record be collected. Our strategy here is to collect the minimum amount of data to make the record useful, while ensuring that we have sufficient data to deduce or research additional information should we require it. In addition to verifying the formal correctness and completeness of the terminology record, the correctness of the data must be verified. If the translator has understood the term and its context, and correctly recorded the source, then this element of the validation is basically a “sanity check”. If, however, the translator has failed to find a convincing source, or if the reviser is unhappy with the use of the target term, then the reviser should take corrective measures. The same applies to a reviewer, should the service specifications require a review stage. It is important to bear in mind that, for an LSP, the source and target terms are treated differently. The source term may need to be lemmatized or stemmed and, if obvious problems exist, a query raised with the customer, but it must basically be taken as correct and recorded. The target term, on the other hand, must be additionally validated as an equivalent term, and this equivalence documented. Constraints The obvious constraints on terminology work in an LSP are time, money and expertise, and are interrelated. The justifications for carrying out terminology work at an LSP are that it: generates cost savings by eliminating duplication of effort; improves the quality of the finished product; and can be sold to the customer as a value added service. All of these justifications are subject to the constraints previously mentioned. Further constraints on terminology work in an LSP include software and file format compatibility, and access to specific packages. Issues here can be as basic as the character encoding. NonUnicode software still exists and is still in widespread use. We still sometimes occasionally encounter problems with extended characters in RTF files becoming corrupted between Windows and Mac OSX applications, and with issues of unsupported or differently encoded glyphs in typefaces for DTP, the euro symbol and typographers’ quotes being good examples of the latter issues. The software in use also limits the nature of the terminology records that can be stored. Wordfast, for example, permits the storage of source and target terms in a simple glossary along with some additional information. MultiTerm, in contrast, provides an open-ended, user-definable, concept-oriented database. The information necessary for a terminology record will vary according to the purpose for which the terminology is being collected. For purposes of automatic terminology recognition and human translation, it is not always necessary to record gender and part of speech, although this information is essential if the data is being collected for machine translation. Page 7 of 14  There are also arguments for storing terminological data in a form appropriate to the nature of the processes utilizing it—normally the form most likely to generate the highest proportion of useful hits. In some cases, it may be sensible to store a plural or otherwise inflected form of a term instead of or as a synonym in addition to the lemma. These additional forms will depend on the language, the part of speech and the program used. At present, most CAT software uses fuzzy matching for automatic terminology recognition. Such systems have problems dealing with significantly different plural forms. One approach to this issue is exemplified by Wordfast glossaries, which can be prepared for “manual fuzzy terminology recognition” by using asterisks as wildcard characters. Such an approach conflicts with the terminological policy of storing the term as its simplest form and raises the question of whether to maintain formally correct terminology records, relying on an element of fuzzy and/or linguistic processing in the software, or to store the terminology in a form that is immediately suitable for delivering the highest number of useful hits in the software environment used. There is, however, a strong trend toward incorporating a greater element of linguistic processing in CAT systems, trading language-independence for increased capabilities such as subsegment matching, and a degree of machine translation capability. These developments suggest that following the terminologists’ approach of storing the term as its simplest form and with additional linguistic information will pay off in the medium term. At present, however, it can be appropriate to maintain glossaries and terminology databases for use with specific software. Formats, incompatibilities, solutions The result of the situation described above is a flood of data in a wide range of formats. Different solutions tend to be implemented, often on an ad hoc basis, and terminological data with varying degrees of reliability is collected in a number of different forms, not all of which permit easy interchange. The terminology applications of the CAT tools in use at an LSP may not all be available to everyone involved in the translation and terminology research processes, the applications may not be compatible, and they may not be capable of exchanging data. Taking TransForm GmbH as an example, our main in-house TM system at present is SDL Trados 2006. This is not a completely satisfactory combination, but porting more than ten year’s worth of Workbench and MultiTerm data to new systems is not a trivial task, and we have put it off as long as we could. MultiTerm is a very flexible, concept-oriented database, and we have historically based our terminology records on the default structure. The data is organized by customer, with differing attributes for different customers, so although the basic structures of data for customer A and customer B are broadly similar, the detailed structures are not. There has, however, been a tendency to simplify the amount of usage data collected over the years. We send out presegmented Word files with inserted terminology for translation in Trados or Wordfast. For many years now, we have recommended that our regular freelancers use Wordfast—in the form of the Classic, Word-based version. External translators supply terminology in MS Word glossaries. In-house translators use the intranet-based system to capture terminology. Customers deliver whatever they think best. Our reasons for sticking with such an old terminology system are a combination of its compatibility with Translator’s Workbench, its utility as a flexible concept-oriented terminology database system, and an element of lock-in due to MultiTerm’s proprietary database structure and the fact that migrating terminological data between systems has been considered notoriously difficult. Although Wordfast is an excellent TM tool, it only supports simple glossaries. While this Page 8 of 14  approach has much to commend it, especially for individual freelancers, it is inadequate for our terminological needs as an LSP. Practical consequences The consequences of this situation include restricted interoperability, a need to run concurrent, incompatible systems, and pressure to upgrade to extremely expensive server-based solutions. The market for TM systems is also becoming increasingly fragmented, with a wide range of systems in use. One result of these developments is an increasing need for a means of consolidating and maintaining terminology independently of proprietary formats. The emergence of TBX as an open standard more or less enthusiastically adopted by many of the main TM system vendors has opened the door to fulfilling this need. TBX and TBX-Basic TBX has been developed by OSCAR, the open standards body of LISA, the Localisation Industry Standards Association. TBX is an open, XML-based standard for the exchange of terminological data. It is soon to be published by ISO as an international standard. It grew out of the MachineReadable Terminology Interchange Format (MARTIF), which itself built on earlier initiatives to promote the interchange of terminological data6. The TBX format offers substantial advantages to the user: As an open standard, it is effectively future-proof As an open standard, there is a degree of pressure on tool vendors to support the format It is clearly defined It is relatively easy to work with because it is a form of XML, itself a well-defined, simple, wellunderstood text-based format It is available for use without licensing fees TBX offers a further advantage in the form of TBX-Basic. Because TBX is capable of much more than handling the relatively simple terminological markup required by small and medium-sized language industry applications, the LISA Terminology Special Interest Group came up with a lightweight version of TBX known as TBX-Basic. This is a terminological markup language (TML) aimed at users of the sort of terminology resources that are commonly developed to support translation and localization processes. TBX-Basic has a simple three-level structure. As in MultiTerm, all terms grouped together in a single concept are considered synonyms. The terms themselves are grouped by language. The highest or concept level can hold information such as subject description, a definition with or without a source and cross-reference to, e.g. an image file. The language level can also hold a definition with or without a source, and the term level holds all the information specific to an individual term. Administrative and transactional information can be present on any level, as can notes. It is fully compliant with TBX, but offers a restricted subset of those TBX features considered most useful for smaller applications Two types of compliance with TBX and TBX-Basic can be distinguished. These are compliance on the structural and syntactic level and compliance on the content level. Compliance on the structural or syntactic level is relatively easy to check by using a suitable validation program such as tbxcheck, which is available from http://sourceforge.net/projects/tbxutil/ Page 9 of 14  Compliance on the content level is another matter, as it can depend on the purpose for which the terminological data is maintained. For example, in TBX-Basic, each term in any data intended to be submitted to any form of machine processing must have a part of speech explicitly indicated. if the data is only intended for human consultation, the part of speech may be omitted, provided that either a definition or a context is present. Furthermore, both definitions and contexts are defined in greater detail. Our answer: a TBX-compliant terminology repository I decided that the best way forward was to use TBX or TBX-Basic to store all of our terminological data. By converting all of our data to a defined, open format with significant and increasing support in the industry, we would ensure future compatibility. The discipline enforced by conforming to a standard would also tend to improve the quality of the data over the medium and long terms. By making the system capable of handling TBX, we automatically made it possible to use TBX-Basic for our existing data without precluding changes in the terminological markup language—the dialect or subset of TBX—at a future date. The use of a widely supported open standard to store terminological data also opens up the possibility of generating different subsets of that data in different forms, for example as glossaries in the format for use with a specific program, or in the shape of an automatically formatted online HTML or PDF dictionary. This capability enables decoupling the repository data from the current state of data subsets specifically generated for use with a specific program. Although the data subset required for a specific use is stable, the individual data records can be expanded within the standard defined by TBX-Basic and TBX without affecting the mechanisms used to generate the application-specific file. Or, to rephrase that, we can use as much or as little of the stored XML data as we require for a specific purpose without compromising the stored data. The restricted subset of TBX features in TBX-Basic actually fits with the reduced amount of usage data that we have moved toward collecting, which encouraged me to plan on using TBX-Basic as the format for storing all of our terminology records. As our terminological data has been collected over at least 16 years, immediate full compliance with TBX-Basic on the content level will only be possible for data collected once the new system has been implemented and integrated into our operational processes. Legacy data will need additional manual input to be brought to compliance. Careful use of the implicit information mentioned previously will help to mitigate these issues. This approach enables future extension of the legacy data to include, for example, grammatical gender and part of speech. It also offers a solution to the dilemma of whether to store inflected forms or forms with wildcard characters as used by some Wordfast glossaries, for example, although the latter step in particular would require the use of extensions not covered by TBX-Basic and would raise compliance issues. One possible way forward here would be to store the additional information necessary to generate such non-standard datasets as metadata and generate the datasets by an export process as required. Data conversion The main issue in converting data between terminological database formats is mapping. TBXBasic uses a concept-oriented, three-level structure with concept, language and term levels. The information stored on each level is constrained, both in terms of what may be stored and what must be stored. Both explicit and implicit information must be handled by the mapping Page 10 of 14  Implicit terminological information Whether supplied by customers or external partners, or generated in-house, terminology records contain explicit information and often have implicit information such as their source or the domain associated with them. Due to the structure of the collected and supplied terminology data, some of this information may be implicit for one half of a source-target pair and explicit for the other. At TransForm GmbH, our legacy terminological data is organized into termbases for specific customer organizations, which themselves may contain usage labels for distinctions by organizational units such as subsidiary or department on the one hand, and by geographical region on the other. We have also commonly recorded usage labels for terminological data for the target language terms, but not for the source language terms. This asymmetric nature is a feature of terminological data captured during the translation process. Translators tend to find glossaries the most useful way to deal with terminology. The existence of a concept is implicit in the existence of a pair of source and target-language terms, but the concept is not explicitly stored in a Wordfast glossary, for example. The process of importing existing or newly recorded terminological data into a data repository must aim to capture all such implicit information. The consolidation of different databases into a repository also requires special attention to usage labels to preserve the information implicit in the original arrangement of separate databases and to ensure customer confidentiality. Handling implicit and explicit information in terminology records Implicit information can be derived from existing data, or can be manually entered during the import process. Establishing which implicit information falls into which of these categories is an important part of defining the mapping to be carried out when converting the data. Some source language term usage information can be inferred from explicit information available for the target term. For example, the source of a source language term can reasonably be taken to be the customer who has sent the job for translation. This can be obtained from the database from which the term record originates. Some of the usage information stored for the target language term may also be applicable to the source language term; for example, the project information stored may provide information on a specific organizational unit. In the case of our existing MultiTerm databases, this information must be derived or manually entered when the terminology records are transferred to the new system. WordFast glossaries and other similar terminology lists are usually related to a specific job, so the customer, customer account and job also represent important implicit information which has to be captured at the time of data import. Supplementary information on terminology captured in our intranet database system can be derived from the job planning database. Terminology recorded in the intranet database is linked to customer and account information, so a substantial amount of usage information can be derived when the data is ported. We are currently developing the use of the input and output templates to take account of such information in the import and export processes. Examples of mappings to TBX-Basic MultiTerm 5.5 MultiTerm 5.5 was the last file-based version of Trados’ original product. It is a flexible conceptoriented system which makes use of four different types of field—index fields, which are defined Page 11 of 14  as languages and contain the terms themselves; attribute fields, which can hold values selected from user-defined picklists; text fields; and system fields holding administrative information. The order, number and relationships of attribute and text fields are not constrained. This flexibility can make it difficult to predict what’s coming next when parsing a MultiTerm export file. The basic mapping for a simple MultiTerm 5.5 export file to TBX-Basic is shown in Figure 4. Figure 4: Simple MultiTerm to TBX-Basic mapping The system fields are mapped to transactional information and to the concept level. The date fields require a format conversion. The index fields contain both the language, which must be translated to the language subtag of the XML language tag and mapped to the language level, and the term itself, which must be mapped to the term level. The text fields are mapped to the term level. Wordfast Figure 5: Simple Wordfast to TBX-Basic mapping The simple glossary format means that much of the information required to create a TBX entry must be either derived or input when the data record is converted. Source and target languages, project information and some transactional information must be entered. Page 12 of 14  Intranet terminology Figure 6: Simple intranet to TBX-Basic mapping The mapping to TBX is relatively simple here, as most of the data is either present in its final form or can be derived from other tables in the planning database. Customers’ data Customers’ terminological data arrives in a wide variety of forms, many of them involving Excel tables. Excel data is converted to tab-delimited text and imported using a modified version of the procedure for dealing with Wordfast glossaries. The software Working together with Wolf Dietrich von Loeffelholz, who has been maintaining and developing software for TransForm GmbH on a freelance basis since 2001, we have used an open standardsbased approach to develop a TBX-compliant terminology repository with the ability to import, manage, store and export terminological data. The system was designed for compatibility with our existing software and systems. It is entirely built on open-source software and XML, and is TBXcompliant. It also supports LDAP authentification. The existing input/output formats are MultiTerm 5.5, Wordfast glossaries, SQL, XML and TBX. The data storage format is XML/TBX, with metadata in a database. XML templates are used to support the import and export of the widest possible range of data formats. This makes it relatively easy to add new import/export formats. The hosting system can be Linux, Windows, etc. and the major databases are supported. The XML-based multilingual front end utilizes Joomla CMS as the view layer and is designed for easy localization. Concept-based management of stored terminology The system is designed to enable the concept-based management of the stored terminology, including the combination and separation of terms and the maintenance of concept/term histories and ratings. Export capabilities One of the features of the approach chosen is that defining an import template effectively defines an export template too. And of course XML is extremely versatile and relatively easy to transform, so that exporting subsets of the data in any format that the system can import, and as TBX data, is straightforward, as is the automatic production of glossaries and dictionaries in a range of forms. Page 13 of 14  Current status The system is Web-based, and is accessed via a SSL connection. The site is self-certified. Initial tests of import and export functions have been carried out. Beta testing by TransForm GmbH and further development of the filters and user interface are now in progress. Once the results of these tests have reached an appropriate level, we intend to start further testing by third parties and with data using more complex character sets, and will proceed with the development of templates for the import and export of other formats. Future strategies for terminology management at TransForm Our plans call for the import of all of our existing terminological data and its conversion and storage as TBX. Step-by-step validation of existing terminology records where necessary, and the consolidation of non-confidential terminology into domain-specific terminology databases, should prove particularly helpful in eliminating duplication of terminological research. We will also then have all of our existing terminological data in a form in which it can be maintained and preserved without fear of incompatibility with future developments in TM software. 
The work of the Bible Societies is to translate and publish scripture in the mother tongue languages of the world. Many translations are prepared for constituencies where the general knowledge of the text may be quite low. Identifying the key pericopes of scripture in an extent of more than 1,000 pages can be challenging. Help for the reader is most commonly given in the form of a concordance which lists the important narratives and other key areas of the text indexed by the key words which most closely represent the themes of the text. The British & Foreign Bible Society in the UK has developed a system to allow an existing concordance to the Bible to be used as a model for a similar concordance in another language. Built upon the automatic glossing technology developed by BFBS, the system is wholly language independent and automates the vast proportion of the task. Key words are rst glossed from the model text to the new target. Sets of quotation references for each word are then compiled. These sets are then subset by reference to the quotation sets selected for the original key words in the model concordance. 
Modern labour-intensive communication-based industries, such as call centres, are increasingly outsourced to Asian countries where a dialect of English is widely spoken, and the pool of suitable staff is large. Despite the distances involved, this is highly cost effective, but is not without its drawbacks. In particular, when UK speakers hear a strong accent they often react negatively. This is partly due to the assumption that a strong accent normally indicates a lack of experience with, and a poor knowledge of, the language, and it may be felt that the call will somehow be delayed or its meaning misunderstood [1, 2]. Although the former assumption is usually unjustified, the latter may be true; unless the listener is attuned to the accent in question, it may be difficult to understand the speaker without frequent requests for repetition and clarification. This applies to the speech of both parties: both the UK-based caller and the Asian call centre operative. A difference in accents can significantly impair communication in both directions. An automated real-time system to reduce the misunderstanding between speakers with significantly different accents would be of great value to these industries. 
We present the online resources developed as part of eCoLoMedia, a European collaborative project in the domain of translator training, the technology used in implementing its website and in localising project materials, and discuss the impact that the rise of entertainment and cultural industries has had in this field of translation. We analyse the results of a needs analysis survey carried out in 2008 by the Institute of Translation and Interpreting (ITI) and illustrate how the results of that study influenced the design and creation of materials. The aim of this project is to encourage trainers or individuals to gain knowledge in emerging translation fields and provide a chance for hands-on practice, be it in class or at home, using online materials on topics ranging from subtitling and voice-over to games and Flash localisation. 1. Introduction Across the globe, the television and cinema sectors are becoming increasingly diversified. The market share of European films in Europe has grown from 20% in 1999 to 30% today1, and their market share also shows constant growth in China, where 45% of films distributed are foreign. Moreover, video gaming is the fastest growing European content industry, with total revenues of €6.3 billion in 20062, exceeding those of cinema box offices. Yet these audiovisual products are impressive not only in terms of economic power, but also for their linguistic and cultural diversity, which requires them to undergo several localisation processes before distribution, involving specialist translators skilled in using specialised translation tools. eCoLoMedia (Vocational Training in Multimedia eContent Localisation: Developing shareable and customisable resources for vocational training in multimedia eContent localisation), a Leonardo-funded project, brings together academics, professional associations, content developers and software developers to build online resources to respond to these identified needs in vocational multimedia translation training. The following sections explain the project’s journey, from the needs analysis carried out by ITI, and the design, production and localisation of materials, through to 
The back office set-up The software has a “Translation Box” interface that will be familiar to users of online translation services, and it’s possible to run an entire project from this GUI. In the document flow designed for Siemens Nederland the MT application is usually deployed via one or several servers. The current back office set-up involves a mail gateway running on a Linux platform, several servers on Solaris 10 and Linux platforms, and clients running on the Windows XP platform. Latest developments One of the basic aims in developing this software has been, not only to keep the language data used by the program separate from the translation engine, but to keep them in a totally “open” format so that they can be maintained even from a simple text editor and – more importantly – so that they can be utitlised by other language tools. Use of non-proprietary formats and platform independence have in fact been the two key planks in the design and further development of this MT application. This has also been a major consideration in the author’s recent development of a prototype Malay-English translator, which will hopefully be taken from prototype to something more useful by using crowdsourcing to review bilingual resources acquired from public sources. Easy access to the data will be essential here. Over the past five years, as a result of using the Dutch-English translation software to handle major projects, we have seen significant growth in the volume of bilingual data which has been stored in two parallel repositories: a Multiword Expression Respository consulted by the MT application and a Trados Translation Memory database containing over a million translation units relating to industrial subjects. At the beginning of this year it was decided to place all the language resources in a single container which could be exploited by both MT and TM programs. After trying out a number of structures to provide such a container, we decided to adopt a simplified form of XLIFF. Page | 1  
Translated Language Service Provider Human Translation in 80 languages, 9000 customers, 100K translations delivered. Core Technology A fully automated translation workflow. Customers can access 35,000 translators through a Web Service. 
In this new scenario, a fundamental question has arisen: Who actually owns the assets? In other words, who holds the intellectual property rights to these resources and what legal protection is available to them? With the increasingly widespread use of this technology it is important to seek a solution to the ownership issue, since otherwise conflicts will inevitably arise. When an agency or corporate client instructs a translator to deliver the TM file created in the translation process together with the translation itself, are they acting legitimately as the legally authorised owners of the file? Or are they perhaps exploiting their relative position of dominance to coerce translators into handing over items of value which they would rather keep locked up on their hard discs? To be able to buy, sell, licence or assign these resources we  need a clear understanding of who owns what and the best starting point for achieving such an understanding is an analysis of intellectual property legislation in the European Union and the United States, as well as on an international scale. It is advisable firstly to clarify the meaning of "intellectual property" and "copyright", as these terms tend to be confused by the lay user. According to the World Intellectual Property Organisation (WIPO), intellectual property can be split into two main areas: copyright plus related rights, and industrial property. These are generally understood to include rights relating to the following:  Literary, artistic and scientific works (copyright)  Performances of performing artists, phonograms, and broadcasts (related rights)  Inventions in all fields of human endeavour, scientific discoveries, industrial designs, 
We have produced a test collection for machine translation (MT). Our test collection includes approximately 2 000 000 sentence pairs in Japanese and English, which were extracted from patent documents and can be used to train and evaluate MT systems. Our test collection also includes search topics for crosslingual information retrieval, to evaluate the contribution of MT to retrieving patent documents across languages. We performed a task for MT at the NTCIR workshop and used our test collection to evaluate participating groups. This paper describes scientiﬁc knowledge obtained through our task. Keywords: Machine translation, Patent information, Cross-lingual information retrieval 
In this paper, we describe the construction of a parallel Chinese-English patent sentence corpus which is created from noisy parallel patents. First, we use a publicly available sentence aligner to find parallel sentence candidates in the noisy parallel data. Then we compare and evaluate three individual measures and different ensemble techniques to sort the parallel sentence candidates according to the confidence score and filter out those with low scores as the noisy data. The experiment shows that the combination of measures outperforms the individual measures, and that filtering out low-quality sentence pairs is readily justified as it can improve SMT performance. Finally, we arrive at the final corpus consisting of 160K sentence pairs in which about 90% are correct or partially correct alignments. 
The paper addresses the issue of resource reuse in patent translation. It presents an efficient patent keyword/phrase extraction tool and illustrates how the tool can be used in patent translation by both human experts and MT developers. The keyword extraction is based on a new hybrid methodology providing for intelligent output and computationally attractive properties. The tool is composed of two modules, - an NP extractor and a patent-tuned scoring module. It does not require a corpus for calculating keywords and relies only on statistical information in a single document which is an advantage for developers and users who would not depend on the corpus availability. The approach is portable across domains, languages and applications. 
Patent sentences have long, complicated structures, and correctly analyzing and translating such sentences is difficult. We have classified the modified relationships in Japanese patent sentences and constructed an error correcting system for analysis of such sentences. In the present paper, we investigate manually whether the Japanese case frames, which are automatically derived from a web corpus, correspond to disambiguated English words. The present study reveals that case frames of verbs originating from active nouns may be useful for disambiguating English verbs, whereas the case frames of traditional Japanese verbs are problematic. Keywords: patent sentence, case frame, translation disambiguation, Japanese-English correspondence  
This paper describes the state of the practice for Human Translation (HT), the established tools, the research, and the capability gaps. The paper is a summary of the tutorial at the Association for Machine Translation of the Americas 2009 conference. 
The paper explores a way to learn post-editing fixes of raw MT outputs automatically by combining two different types of statistical machine translation (SMT) systems in a linear fashion. Our proposed system (which we call a chained system) consists of two SMT systems: (i) a syntax-based SMT system and (ii) a phrase-based SMT system (Koehn, 2004). We first translate source sentences of the bitext training data into a target language, using the syntax-based SMT. This provides us the monolingual parallel data that consist of the raw MT outputs and their corresponding human translations. We then build a phrasebased SMT system, using the monolingual parallel corpus. Our system is thus a chain of a syntax-based SMT system and a phrase-based SMT system. The benefit of the chained system is to learn post-editing fixes automatically via a phrase-based SMT system (Simard, et al., 2007a/b). We investigated the impact from the chained system on the initial SMT system in terms of BLEU, using typologically different language pairs. The results of our experiments strongly indicate that the second part of the chained system can compensate the weaknesses of the initial SMT system in a robust way by providing human-like fixes. 
We propose a generic rule induction framework that is informed by syntax from both sides of a parsed parallel corpus, as sets of structural, boundary and labeling related constraints. Factoring syntax in this manner empowers our framework to work with independent annotations coming from multiple resources and not necessarily a single syntactic structure. We then explore the issue of lexical coverage of translation models learned in different scenarios using syntax from one side vs. both sides. We speciﬁcally look at how the non-isomorphic nature of parse trees for the two languages affects coverage. We propose a novel technique for restructuring targetside parse trees, that generates alternate isomorphic target trees that preserve the syntactic boundaries of constituents that were aligned in the original parse trees. We also show that combining rules extracted by restructuring syntactic trees on both sides produces signiﬁcantly better translation models. The improved precision and coverage of our syntax tables particularly ﬁll in for the lack of lexical coverage in Syntax based Machine Translation approaches. 
In this paper, we evaluate for the first time the use of Machine Translation technology to repair general errors in second language (L2) authoring. Contrary to previously evaluated approaches which rely exclusively on unilingual models of L2, this method takes into account both languages, and is thus able to model linguistic interference phenomena where the author produces an erroneous word for word translation of his L1 intent. We evaluate a simple roundtrip MT approach on a corpus of foreign-sounding errors produced in the context of French as a Second Language. We show that the roundtrip approach is better at repairing linguistic interference errors than non-interference ones, and that it is better at repairing errors which only involve function words. We also show that the first leg of the roundtrip (inferring the author's L1 intent) is more sensitive to error type and more error prone than the second leg (rendering a correct L1 intent back into L2). 
In this work we have deal with the reordering problem in Spanish-Basque statistical machine translation, comparing three different approaches and analyzing their strength and weakness. Tested approaches cover the more usual techniques: lexicalized reordering implemented on Moses, preprocessing based on hand deﬁned rules over the syntactic analysis of the source and statistical translation. According with the obtained results, the three reordering techniques improves the results of the baseline. We observe different behaviour at combining techniques. While the use of the Syntax-Based reordered corpus together with the lexicalized reordering get the best results, training the lexicalized reordering on the statistically reordered source does not improve the performance of the single methods. 
This paper reports on a preliminary study testing the use of eye tracking as a method for evaluating machine translation output. 50 French machine translated sentences, 25 rated as excellent and 25 rated as poor in an earlier human evaluation, were selected. 10 native speakers of French were instructed to read the MT sentences for comprehensibility. Their eye gaze data were recorded noninvasively using a Tobii 1750 eye tracker. They were also asked to record retrospective protocols while watching a replay of their eye gaze reading data. The average gaze time and fixation count were found to be significantly higher for the “bad” sentences, while average fixation duration was not significantly different. Evaluative comments uttered during the retrospective protocols were also found to agree to a satisfactory degree with previous human evaluation. Overall, we found that the eye tracking method correlates reasonably well with human evaluation of MT output. Key words: MT Evaluation, user-based evaluation, eye tracking, gaze time, fixation count, fixation duration, retrospective protocols 
In this work, we show how an existing rulebased, general-purpose machine translation system may be improved and adapted automatically to a given domain, whenever parallel corpora are available. We perform this adaptation by extracting dictionary entries from the parallel data. From this initial set, the application of these rules is tested against the baseline performance. Rules are then pruned depending on sentence-level improvements and deteriorations, as evaluated by an automatic stringbased metric. Experiments using the Europarl dataset show a 3% absolute improvement in BLEU over the original rule-based system. 
This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. Traditional hypothesis alignment algorithms such as TER, HMM and IHMM do not directly utilise the context information of the source side but rather address the alignment issues via the output data itself. In this paper, a source-side context-informed (SSCI) hypothesis alignment method is proposed to carry out the word alignment and word reordering issues. First of all, the source–target word alignment links are produced as the hidden variables by exporting source phrase spans during the translation decoding process. Secondly, a mapping strategy and normalisation model are employed to acquire the 1to-1 alignment links and build the confusion network (CN). The source-side context-based method outperforms the state-of-the-art TERbased alignment model in our experiments on the WMT09 English-to-French and NIST Chinese-to-English data sets respectively. Experimental results demonstrate that our proposed approach scores consistently among the best results across different data and language pair conditions. 
In Minimum Error Rate Training (MERT), the parameters of an SMT system are tuned on a certain evaluation metric to improve translation quality. In this paper, we present empirical results in which parameters tuned on one metric (e.g. BLEU) may not lead to optimal scores on the same metric. The score can be improved signiﬁcantly by tuning on an entirely different metric (e.g. METEOR, by 0.82 BLEU points or 3.38% relative improvement on WMT08 English–French dataset). We analyse the impact of choice of objective function in MERT and further propose three combination strategies of different metrics to reduce the bias of a single metric, and obtain parameters that receive better scores (0.99 BLEU points or 4.08% relative improvement) on evaluation metrics than those tuned on the standalone metric itself. 
Translation spotting consists in automatically identifying the translations of a user query inside a bitext. This task, when it relies solely on statistical word alignment algorithms, fails to achieve excellent results. In this paper, we show that identifying the translations of a query during a ﬁrst translation spotting stage provides relevant information that can be used in a second stage to improve the precision of the results. This method is similar to the relevance feedback used in the information retrieval domain to enhance retrieval. 
To address the shortage of Japanese-English parallel corpora, we developed a parallel corpus by collecting open source software manuals from the Web. The constructed corpus contains approximately 500 thousand sentence pairs that were aligned automatically by an existing method. We also conducted statistical machine translation (SMT) experiments with the corpus and conﬁrmed that the corpus is useful for SMT. 
In this paper, we present 2D-Linking, a new unsupervised method for word alignment that is based on association scores between words in a bitext. 2D-Linking can align m-to-n units. It is very efﬁcient because it requires only two passes over the data and less memory than other methods. We show that 2D-Linking is superior to competitive linking and as good as or better than symmetrized IBM Model 1 in terms of alignment quality and that it supports trading off precision against recall.  
In most statistical machine translation (SMT) systems, bilingual segments are extracted via word alignment. In this paper we compare alignments tuned directly according to alignment F-score and BLEU score in order to investigate the alignment characteristics that are helpful in translation. We report results for two different SMT systems (a phrase-based and an n-gram-based system) on Chinese to English IWSLT data, and Spanish to English European Parliament data. We give alignment hints to improve BLEU score, depending on the SMT system used and the type of corpus. 
We propose a reordering method to improve the ﬂuency of the output of the phrase-based SMT (PBSMT) system. We parse the translation results that follow the source language order into non-projective dependency trees, then reorder dependency trees to obtain ﬂuent target sentences. Our method ensures that the translation results are grammatically correct and achieves major improvements over PBSMT using dependency-based metrics. 
We examine the contribution of reliable elements in French– and English–Japanese alignment from comparable corpora, using transliterated elements and scientiﬁc compounds as anchor points among context-vectors of elements to align. We highlight those elements in context-vector normalisation to give them a higher priority in context-vector comparison. We carry out experiments on small comparable corpora to show that those elements can efﬁciently be used to improve the quality of the alignment. 
In this paper we describe a six-ways parallel public-domain corpus consisting of 2100 United Nations General Assembly Resolutions with translations in the six ofﬁcial languages of the United Nations, with an average of around 3 million tokens per language. The corpus is available in a preprocessed, formatting-normalized TMX format with paragraphs aligned across multiple languages. We describe the background to the corpus and its content, the process of its construction, and some of its interesting properties. 
Most of the existing, easily available parallel texts to train a statistical machine translation system are from international organizations that use a particular jargon. In this paper, we consider the automatic adaptation of such a translation model to the news domain. The initial system was trained on more than 200M words of UN bitexts. We then explore large amounts of in-domain monolingual texts to modify the probability distribution of the phrase-table and to learn new task-speciﬁc phrase-pairs. This procedure achieved an improvement of 3.5 points BLEU on the test set in an Arabic/French statistical machine translation system. This result compares favorably with other large state-of-the-art systems for this language pair. 
Statistical Machine Translation (SMT) systems rely heavily on the quality of the phrase pairs induced from large amounts of training data. Apart from the widely used method of heuristic learning of n-gram phrase translations from word alignments, there are numerous methods for extracting these phrase pairs. One such class of approaches uses translation information encoded in parallel treebanks to extract phrase pairs. Work to date has demonstrated the usefulness of translation models induced from both constituency structure trees and dependency structure trees. Both syntactic annotations rely on the existence of natural language parsers for both the source and target languages. We depart from the norm by directly obtaining dependency parses from constituency structures using head percolation tables. The paper investigates the use of aligned chunks induced from percolated dependencies in French–English SMT and contrasts it with the aforementioned extracted phrases. We observe that adding phrase pairs from any other method improves translation performance over the baseline n-gram-based system, percolated dependencies are a good substitute for parsed dependencies, and that supplementing with our novel head percolation-induced chunks shows a general trend toward improving all system types across two data sets up to a 5.26% relative increase in BLEU. 
The aim of this paper is to further compare two versions of our spoken language translator MedSLT which differ in terms of grammatical coverage; the first version is restricted to Yes-No answers and short elliptical sentences, while the second version allows all of the above but also full sentences. Previous work in this direction has focused on the relation between Word Error Rate (WER) and task performance (Starlander et al., 2008). In this paper this comparison is extended to study in more detail the task performance with respect to the number of successful interactions and to explain why some utterances could not be correctly recognized. Additionally this paper will explore how these results correlate with the quality of the translation produced by both versions of the systems. 
This paper summarises the results of a pilot project conducted to investigate the correlation between automatic evaluation metric scores and post-editing speed on a segment by segment basis. Firstly, the results from the comparison of various automatic metrics and post-editing speed will be reported. Secondly, further analysis is carried out by taking into consideration other relevant variables, such as text length and structures, and by means of multiple regression. It has been found that different automatic metrics achieve different levels and types of correlation with post-editing speed. We suggest that some of the source text characteristics and machine translation errors may be able to account for the gap between the automatic metric scores and post-editing speed, and may also help with understanding human post-editing process. 
The contribution discusses variants of architectures of hybrid MT systems. The three main types of architectures are: coupling of systems (serial or parallel), architecture adaptations (integrating novel components into SMT or RMT architectures, either by pre/post-editing, or by system core modifications), and genuine hybrid systems, combining components of different paradigms. The interest is to investigate which resources are required for which types of systems, and to which extent the proposals contribute to an overall increase in MT quality. 
We have developed a web site called Minna no Hon’yaku (“Translation for Everyone by Everyone”), which hosts online volunteer translators. Its core features are (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor QRedit. Translators who use QRedit daily reported an up to 30 per cent reduction of the overall translation time. As of 3 July 2009, there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan. 
Rule based machine translation methods require a set of sophisticated transfer rules for good accuracy. To manually build such a bilingual resource, one requires many man-years of work performed by linguistic specialists. This cost is too high, especially in case of less represented language pairs, such as Hungarian and Japanese. This paper proposes a simple and robust method to automatically build a large coverage transfer rule set for the Hungarian-Japanese language pair. Our method uses a small parsed bilingual corpus and a bilingual dictionary of the selected languages. We concentrate on accurately inducing the most frequent target language translation rules from all instances of a source language rule. We achieved good accuracy especially for low level rules, which are especially important in case of agglutinative languages. 
The way of mining comparable corpora and the strategy of dictionary extraction are two essential elements of bilingual dictionary extraction from comparable corpora. This paper first proposes a method, which uses the interlanguage link in Wikipedia, to build comparable corpora. The large scale of Wikipedia ensures the quantity of collected comparable corpora. Besides, because the inter-language link is created by article author, the quality of collected corpora can also be guaranteed. After that, this paper presents an approach, which combines context heterogeneity similarity and dependency heterogeneity similarity, to extract bilingual dictionary from the collected comparable corpora. Experimental results show that because of combining the advantages of context heterogeneity similarity and dependency heterogeneity similarity appropriately, the proposed approach outperforms both the two individual approaches. 
In this paper, we describe our ongoing research project of Virtual Babel, a contextaware machine translation system for Second Life, one of the most popular virtual worlds. We augment the Second Life viewer to intercept the incoming/outgoing chat messages and reroute the message to a statistical machine translation server. The returned translations are appended to the original text message to help users to understand the foreign language. Virtual Babel provides a platform to study cross-lingual conversations facilitated by machine translation in virtual worlds and we observe interesting phenomena that are not present in document translations. Virtual Babel is aware of the non-verbal context of the conversation. Language model and translation models are trained from collected conversations and are used to generate translations according to observed non-verbal context of the conversation. 
This paper is a case study of the deployment of new MT technology aiming at improving the overall Post-Editing (PE) experience. One of the main challenges in having MT output post-edited within a localization workflow is being able to meet ever increasing quality expectations, especially from a terminology perspective. In Symantec‟s localization process we constantly seek to overcome this obstacle by refining our MT output in a number of ways. Several layers of customization have therefore been added over the years to our commercial MT system (such as User Dictionaries, Translation Stylesheets and Automated Post-Editing). Despite obtaining substantial quality gains with these techniques, improvements are still sought to minimize PE effort. The deployment of a novel technology, based on SYSTRAN's new hybrid approach, is presented in this paper. This paper focuses on the technical and linguistic challenges associated with the integration of this new technology into an existing MT workflow. 
Localization in 80 languages, 9,000 customers Mission Removing administrative overhead: faster and cost‐effective. Make translators more productive. Focus Web content and Information Technology Value Proposition Cost reduction through workflow automation  My wish‐list for the perfect TM Platform  Millions of segments, still FAST Web 2.0, Wiki‐like crowd contributions One‐click upload/download TM Adaptive Matching, A priori probability Propagate changes real‐time Fully integrated Statistical MT Open standard  MyMemory  The Solution  Free, Collaborative Web Translation Memory  Barriers to contribution Privacy and IP Time vs Value Will people contribute? Yes! 30 words per second for 1 year 
1. Provide overview of current research in MT. 2. Provide overview of research papers at this conference. • Trends & background information: – More & more research activity – most current research in MT involves statistical MT = SMT (as opposed to rule-based MT = RBMT) • open-source packages & data have lowered barriers to entry • e.g., GIZA++ for word alignment, Moses decoding and LDC for data – SMT needs bilingual training data - much research on gathering such data – tuning SMT system requires automatic evaluation metrics – you’ll hear “BLEU” a lot – MT teams participate in regular international competitions (e.g., NIST, ACL)  Goals & Trends in Presentation • Trends & background information (cont.): – Funding of research: • US gov’t interested mainly in En as target (GALE = Ar En, Ch En; NIST = same + Urdu En); • EU mostly interested in European languages; • Large American corporations (e.g. Microsoft) interested mainly in En as source. – SMT systems are quickly improving (better algorithms, more training data) – Some European language pairs (En ↔ Fr, En ↔ Sp) may have reached quality required for wide usability – Ch↔ En increasingly important; more & more Chinese researchers getting involved – More & more use of syntax in SMT – Combination of MT systems is surprisingly effective – Google Translate’s SMT has become the gold standard; being used surreptitiously by professional translators, kids cheating on homework, etc. – Commercial offerings available for deploying SMT in-house  Some Gaps • Not enough user studies • Not enough work on incorporating MT into translators’ tools (e.g., translation memory) • Too much focus on clever new techniques applied to old problems, instead of 
This paper presents a few ideas of the Center for Translation Studies at University of Illinois on the present and future of human and machine transla­ tion. In particular, as we will elaborate at our panel, we believe that we should focus more on how we prepare humanities students to succeed as translat­ ors in a technology-driven environment. At the MT Summit panel, we will discuss the interface between human and machine translation and the re­ lated pedagogical and research issues. 
y Translation memory from previous projects y Domain dictionaries y … BigTM.net 
This paper describes some of the kinds of predictable errors in Machine Translation (MT). It then discusses means of alerting end-users of MT to the possible presence of such errors, including by providing training and/or by providing automated MT ratings, MT color coding and/or symbols, and footnotes and annotation. It also discusses the need for some kind of reliability measure and/or information to the MT consumer, and the likelihood of the MT user being open to using this kind of input. Some of the suggestions made for usercentric MT are also applicable to translatorcentric MT. 
We describe and report initial results on using virtual machines as a vehicle to deploy machine translation technology to the marketplace. Virtual machines can bridge the gap between the computing infrastructure typically used in research environments and commodity PCs typical of ofﬁce environments. A key component is the compact representation of the underlying databases and models in tightly packed tries, which allows us to run state-ofthe art translation technology on regular ofﬁce PCs. 
We propose a novel source-side dependency tree reordering model for statistical machine translation, in which subtree movements and constraints are represented as reordering events associated with the widely used lexicalized reordering models. This model allows us to not only efﬁciently capture the statistical distribution of the subtree-to-subtree transitions in training data, but also utilize it directly at the decoding time to guide the search process. Using subtree movements and constraints as features in a log-linear model, we are able to help the reordering models make better selections. It also allows the subtle importance of monolingual syntactic movements to be learned alongside other reordering features. We show improvements in translation quality in English→Spanish and English→Iraqi translation tasks. 
In this paper, we propose to enhance the phrase translation model with association measures as new feature functions. These features are estimated on counts of phrase pair co-occurrence and their marginal counts. Four feature functions, namely, Dice coefficient, log-likelihood-ratio, hyper-geometric distribution and link probability are exploited and compared. Experimental results demonstrate that the performance of the phrase translation model can be improved by enhancing it with these association based feature functions. Moreover, we study the correlation between the features to predict the usefulness of a new association feature given the existing features. 
The Defense Advanced Research Projects Agency (DARPA) Spoken Language Communication and Translation System for Tactical Use (TRANSTAC) program has experimented with applying automated metrics to speech translation dialogues. For translations into English, BLEU, TER, and METEOR scores correlate well with human judgments, but scores for translation into Arabic correlate with human judgments less strongly. This paper provides evidence to support the hypothesis that automated measures of Arabic are lower due to variation and inflection in Arabic by demonstrating that normalization operations improve correlation between BLEU scores and Likert-type judgments of semantic adequacy — as well as between BLEU scores and human judgments of the successful transfer of the meaning of individual content words from English to Arabic. 
We introduce the ﬁrst machine translation system for Yiddish-English and English-Yiddish. We discuss challenges presented by this language and their solutions, including an algorithm for cognate extraction. 
In this paper we study in detail the relation between word alignment and phrase extraction. First, we analyze different word alignments according to several characteristics and compare them to hand-aligned data. Secondly, we analyzed the phrase-pairs generated by these alignments. We observed that the number of unaligned words has a large impact on the characteristics of the phrase table. A manual evaluation of phrase pair quality showed that the increase in the number of unaligned words results in a lower quality. Finally, we present translation results from using the number of unaligned words as features from which we obtain up to 2BP of improvement. 
In this paper, we propose a hybrid spoken language translation method utilizing sentence segmentation. By portioning the sentence using the result of syntax analysis, we can utilize rule-based control of the integration of subtranslations translated by a suitable method for each segment. We also report a preliminary experiment on translation quality of our prototype Japaneseto-English translation system. We conﬁrmed that our method achieved a 13.4% advantage in NIST score for the individual RBMT method, and a 6.0% advantage for the individual EBMT method. 
We built 462 machine translation systems for all language pairs of the Acquis Communautaire corpus. We report and analyse the performance of these system, and compare them against pivot translation and a number of system combination methods (multi-pivot, multisource) that are possible due to the available systems. 
We investigate novel types of assistance for human translators, based on statistical machine translation methods. We developed Caitra, a tool that makes suggestions for sentence completion, shows word and phrase translation options, and allows postediting of machine translation output. A user study validates the types of assistance and provides insight into the human translation process. 
We investigate the possibility of automatically detecting whether a piece of text is an original or a translation. On a large parallel English-French corpus where reference information is available, we ﬁnd that this is possible with around 90% accuracy. We further study the implication this has on Machine Translation performance. After separating our corpus according to translation direction, we train direction-speciﬁc phrase-based MT systems and show that they yield improved translation performance. This suggests that taking directionality into account when training SMT systems may have a signiﬁcant effect on output quality. 
In this paper, we present a reordering model based on Maximum Entropy. This model is extended from a hierarchical reordering model with PBSMT (Galley and Manning, 2008), which integrates syntactic information directly in decoder as features of MaxEnt model. The advantages of this model are (1) maintaining the strength of phrase based approach with a hierarchical reordering model, (2) many kinds of linguistic information integrated in PBSMT as arbitrary features of MaxEntropy model. The experiment results with English-Vietnamese pair showed that our approach achieves improvements over the system which use a lexical hierarchical reordering model (Galley and Manning, 2008). 
A method for evaluating MT performance embedded in Cross-Language Instant Messaging (CLIM) systems is presented. A web interface that provided concurrent realtime translation for instant messaging from multiple MT services was developed and used by paid participants to collaborate on a photo identification task. The method showed a task performance benefit due to the availability of multiple translation alternatives. The method also provides a new evaluation metric for MT systems based on user‟s task motivated choices. This method was used to compare two English-Japanese online translation systems, one from Google, and one from Excite/Japan. 
We propose to estimate the probability that a target word appears in the translation of a given source sentence using a multilayer perceptron. At the expense of ignoring word order and repetition, our model does not assume word alignments and consider all source words jointly when evaluating the probability of a target word. We compared our model against IBM1 which does not consider word order either. Our model was comparable with IBM1 when predicting the target words that should appear in the translation of a source sentence. When our model was extended to include alignment information, it surpassed IBM1 on all the metrics we used. 
This paper describes methods for integrating source language and target language information for machine aided human translation (MAHT) of text documents. These methods are applied to a language translation task involving a human translator dictating a ﬁrst draft translation of a source language document. A method is presented which integrates target language automatic speech recognition (ASR) models with source language statistical machine translation (SMT) and named entity recognition (NER) information at the phonetic level. Information extracted from a source language document including translation model probabilities and translated named entities are combined with acoustic-phonetic information obtained from phone lattices produced by the ASR system. Phone-level integration allows the combined MAHT system to correctly decode words that are either not in the ASR vocabulary or would have been incorrectly decoded by the ASR system. It is shown that the combined MAHT system results in a decrease in word error rate on the dictated translations of 32% relative to a stand alone baseline ASR system. 
We explore the problem of integrating a phrase-based MT system within a computerassisted translation (CAT) environment. We argue that one way of achieving successful integration is to design an MT system that behaves more like the translation memory (TM) component of CAT systems. This implies producing MT output that is consistent with that of a TM when high-similarity material exists in the training data; it also implies providing the MT system with a component that is capable of ﬁltering out machine translations that are less likely to be useful. We propose solutions to both problems, and evaluate their impact on three different data sets. Our results indicate that the proposed approach leads to systems that produce better output than a TM, for a larger portion of the source text. 
Statistical MT is limited by reliance on large parallel corpora. We propose Lemmatic MT, a new paradigm that extends MT to a far broader set of languages, but requires substantial manual encoding effort. We present PANLINGUAL TRANSLATOR, a prototype Lemmatic MT system with high translation adequacy on 59% to 99% of sentences (average 84%) on a sample of 6 language pairs that Google Translate (GT) handles. GT ranged from 34% to 93%, average 65%. PANLINGUAL TRANSLATOR also had high translation adequacy on 27% to 82% of sentences (average 62%) from a sample of 5 language pairs not handled by GT. 
We investigate the problem of estimating the quality of the output of machine translation systems at the sentence level when reference translations are not available. The focus is on automatically identifying a threshold to map a continuous predicted score into “good” / “bad” categories for ﬁltering out bad-quality cases in a translation post-edition task. We use the theory of Inductive Conﬁdence Machines (ICM) to identify this threshold according to a conﬁdence level that is expected for a given task. Experiments show that this approach gives improved estimates when compared to those based on classiﬁcation or regression algorithms without ICM. 
We describe an approach for ﬁltering phrase tables in a Statistical Machine Translation system, which relies on a statistical independence measure called Noise, ﬁrst introduced in (Moore, 2004). While previous work by (Johnson et al., 2007) also addressed the question of phrase table ﬁltering, it relied on a simpler independence measure, the p-value, which is theoretically less satisfying than the Noise in this context. In this paper, we use Noise as the ﬁltering criterion, and show that when we partition the bi-phrase tables in several sub-classes according to their complexity, using Noise leads to improvements in BLEU score that are unreachable using pvalue, while allowing a similar amount of pruning of the phrase tables. 
We propose to mine parallel texts from mixedlanguage web pages. We deﬁne a mixedlanguage web page as a web page consisting of (at least) two languages. We mined Japanese-English parallel texts from mixedlanguage web pages. We presented the statistics for extracted parallel texts and conducted machine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 
In this paper we present an extension of a phrase-based decoder that dynamically chunks, reorders, and applies phrase translations in tandem. A maximum entropy classiﬁer is trained based on the word alignments to ﬁnd the best positions to chunk the source sentence. No language speciﬁc or syntactic information is used to build the chunking classiﬁer. Words inside the chunks are moved together to enable the decoder to make longdistance re-orderings to capture the word order differences between languages with different sentence structures. To keep the search space manageable, phrases inside the chunks are monotonically translated, thus by eliminating the unnecessary local re-orderings, it is possible to perform long-distance re-orderings beyond the common ﬁxed distortion limit. Experiments on German to English translation are reported. 
This paper presents an overall introduction to the CWMT2008 evaluation and focuses on its two new metrics: BLEU-SBP (Chiang et al., 2008) and linguistic check-point method (Zhou et al., 2008). BLEU-SBP is a revised BLEU with strict brevity penalty. Our experiments validated BLEU-SBP’s effectivity in resolving the nondecomposability problem of both NIST-BLEU and IBMBLEU at sentence level. Linguistic checkpoint method (LCM) is a linguistic diagnostic evaluation method based on automatically constructed linguistic check-points, and our evaluation indicates that this method can be used to evaluate the capability of an MT system in translating various linguistic phenomena, and revealed good correlations between BLEU score and LCM scores in most tasks. With the aid of these metrics, we disclosed some detailed performance differences between statistical MT systems and rulebased MT systems. In addition, through the study on some practical cases we suggest that the high BLEU score doesn’t necessarily mean high translation adequacy. 
Bitexts collected from web-based materials that are officially published on government websites can be used for a variety of purposes in language analysis and natural language processing. Mining officially published web pages can thus be an invaluable undertaking for translators in government departments who are producing the translations, and for machine translation researchers who are studying how those translations are produced. In this paper, we present the StatCan Daily Translation Extraction System (SDTES) and demonstrate how it is used to induce translations from officially published bilingual materials from government websites in Canada. New evaluation results show that SDTES is a very effective system for identifying and extracting sentences that are translation pairs from most of the federal government web pages which are currently under the CLF2 (Common Look and Feel for the Internet 2.0) framework. 
This paper describes the Language Technology Resource Center (LTRC), a U.S. Government website for providing information and tools for users of languages (e.g., translators, analysts, etc.) The LTRC provides information on a broad range of products and tools, and provides a means for product developers and researchers to provide the U.S. Government and the public with information about their work. The LTRC is part of the Foreign Language Resource Center (FLRC) program, which provides support to Government and professional organizations regarding language tools.  2 History In 2000, in response to the crisis in Albania, MITRE began posting and distributing information about available tools to support the relief effort. In 2002, the effort received a small amount of funding from the Office of the Secretary of Defense. In 2004, it became one of the first projects of the Language and Speech Exploitation Resources (LASER) Advanced Concept Technology Demonstration (ACTD). The program was transitioned to the U.S. Army and currently reports to the Foreign Language Program Office in the Army G2. The United States Army Intelligence and Security Command (USAINSCOM) is the Executive Agent.  
Commercial off-the-shelf machine translation engines and translation support tools, such as translation memory (TM), have been developed primarily for translating grammatically well-formed, edited text. The real-world, foreign language (FL) document collections that our translators work with consist instead of noisy and complex image files. We are currently conducting experiments that involve building and evaluating the effectiveness of different multi-component workflows for the automated processing and translation of these FL images into English text. To support the project’s ongoing needs for translations, we are developing a software framework designed with and for our translator that (i) streamlines users’ access to and capability to add-in and modify existing online tools and data resources (ex. MT, TM, dictionaries, morphological analyzers, LM), (ii) builds persistent data objects for later re-use, and (iii) provides users with a configuration screen page to select the tools and data resources for their sessions, to set their tools’ options and display options for their translation screen page. This paper introduces our extreme programming approach to the software engineering of this new, hands-on translator’s framework called TREAT (Toolkit and Resource Environment to Assist Translation), where the translator---as subject matter expert and experienced software user---participates fully in the software design, evaluation, and iterative modification processes.  
Building the Automated Translation System § In 2002 - rudimentary automated system: very poor results 
A National Translation Mission (NTM) has recently been launched by Government of India with the primary objective of providing greater access to knowledge across the country in all Indian languages through translation of book and documents. In this paper, I present a brief account of the current Indian linguistic scenario and outline a framework for integration of human and machine translation that will provide impetus to MT researchers, developers and the translation industry in the country. 
The EDEAL project seeks to identify, collect, evaluate, and enhance resources relevant to processing collected material in African languages. Its priority languages are Swahili, Hausa, Oromo, and Yoruba. Resources of interest include software for OCR, Machine Translation (MT), and Named Entity Extraction (NEE), as well as data resources for developing and evaluating tools for these languages, and approaches—whether automated or manual—for developing capabilities for languages that lack significant data resources and reference material. We have surveyed the available resources, and the project is now in its first execution phase, focused on providing end-to-end capabilities and solid data coverage for a single language; we have chosen Swahili since it has the best existing coverage to build on. The results of the work will be freely available to the U.S. Government community. 
 2 Keys for Maturity  This paper describes the work environment, IT tools and interactions that have come about after almost 30 years of MT use at the Pan American Health Organization (PAHO). We present the key ingredients that have contributed to the creation of a mature and stable environment for our translation service and provide some practical suggestions and solutions that we have worked out over the years. 
•Background •Methodology •Case Studies •Conclusion  Automated Translation Solutions  
This paper presents the automatic translation management system known as TransLI developed for machine translation of Court judgments from English to French and from French to English. NLP’s solution addresses our clientele’s volume and time management issues. TransLI’s statistical algorithm has been trained on a large corpus of legal documents as to significantly reduce production time and costs. We describe the importance of review and postediting mechanisms for judicial translations as part of the SMT-integrated workflow. This system is already being used by several government departments in Canada. 
Adobe recently began exploring the integration of Machine Translation (MT) technology into its localization workflow. The primary question that we sought to answer was whether post-editing Machine Translation output was faster than translating the text from scratch. The exploration occurred in two stages: a small pilot followed by a larger project localizing Adobe product documentation. The project used two MT engines (one statistical and one traditional) both of which were trained with Adobe data and lexicons. Initially, a small test set of 800-2000 words of documentation was machine translated and post-edited, and based on the positive results we proceeded to the second stage, localizing about 200,000 words of new text. The second stage completed successfully, but with some complications, including: • The post-editing rate and the MT quality varied significantly between files. Additionally, differences in the first and second pilot test data created differences in MT quality. • The technical integration of MT with the localization management system has been much more difficult than expected. • The translation vendor reported that postediting required more-highly-skilled trans-  lators to perform the task, which was contrary to our expectations. 
For this work we have carried out a number of analysis experiments comparing raw MT output produced by Microsoft’s Treelet MT engine (Quirk et al., 2005) with its human post-edited counterpart, for English–German and English–French. Through these experiments we identify a number of interesting post-editing patterns, both textual (string-based) and constituent-based. In this paper we discuss our analysis methodologies, present some of our results and provide information on how this type of analysis can be of beneﬁt to translation systems and posteditors, with a view to improving initial MT output and consequently post-editor productivity. In addition, we also discuss the MT and post-editing workﬂow at Microsoft and results from MT post-editing pilots for a number of different language pairs. 
Objective • Determine the effect of data pooling among multiple 
 language in lexicon then, construction of a Letter-  To-Sound (LTS) conversion system is important.  Persian writing system, like all other Arabic script-based Importance of LTS conversion systems increases  languages, is special because of omission of some vo- for Arabic script-based languages like Persian be-  wels in its standard orthography.  cause of omission of some vowels in their standard  Lack of these vowels causes some problems in Text -To- orthography.  Speech systems because full transcription of words is needed for synthesis. Then construction of a Letter-ToSound conversion system is necessary for Text-To-  Generally, there are two major methods for letterto-sound conversion. The first is based on using  Speech systems because it is not possible to list all words some hand written phonological rules. For example  of a language with their corresponding pronunciation in in Festival Speech Synthesis system (Black et al.,  a lexicon.  1999), a basic form of a phonological rule is as fol-  In this paper, we have presented a Persian Letter-To- lows:  Sound conversion system based on Classification and (LEFTCONTEXT [ITEM] RGHTCONTEXT =  Regression Tree.  NEWITEMS)  The training data is a lexicon of 32,000 words with their It means that if ITEM appears in the specified right  corresponding pronunciation which is extracted from and left context then the output string is to contain  Persian linguistic database corpora. The CART is built with Wagon that is a tool of Edinburg Speech Tools for constructing decision trees in Festival.  NEWITEMS. Any of LEFTCONTEXT, RIGHTCONTEXT or NEWITEMS may be empty.  The final accuracy of this system is 93.61 %, which An example is (# [ch] C = k). The special character  means that this system is able to predict Persian words’ # denotes a word boundary, and the symbol C de-  pronunciation comparatively by a high accuracy in com- notes the set of all consonants. This rule states that  parison with the same system for English which is 94.6% a ch at the start of a word followed by a consonant  accurate to predict English words’ pronunciation in Fes- is to be rendered as the k phoneme (Black et al.,  tival. Also accuracy of the implemented Persian Letter- 1999).  To-Sound system in festival is more than other previous Writing letter to sound rules by hand is hard and  systems which are implemented out of Festival.  time consuming, an alternate method is also availa-  
The disfluencies inherent in spontaneous speaking and out-of-vocabulary words omnipresent in any transcribed oral utterance by speech recognition, are a real challenge for speech understanding systems. Thus, we propose in this paper, a method for processing disfluencies and out-ofvocabulary words in the context of automatic Arabic speech understanding. Our method based on a robust and partial analysis of Arabic oral utterances (conceptual segments analysis) is effective for the treatment of such phenomena. This method has been tested through the understanding module of SARF system, an interactive vocal server for Tunisian railway information. 
Phrase re-ordering is a well-known obstacle to robust machine translation for language pairs with signiﬁcantly different word orderings. For Arabic-English, two languages that usually differ in the ordering of subject and verb, the subject and its modiﬁers must be accurately moved to produce a grammatical translation. This operation requires more than base phrase chunking and often deﬁes current phrase-based statistical decoders. We present a conditional random ﬁeld sequence classiﬁer that detects the full scope of Arabic noun phrase subjects in verb-initial clauses at the Fβ=1 61.3% level, a 5.0% absolute improvement over a statistical parser baseline. We suggest methods for integrating the classiﬁer output with a statistical decoder and present preliminary machine translation results.  
A cross-linguistically very rare type of clitic, the endoclitic, occurs in Pashto. Like infixes, endoclitics can be inserted inside of a word, but by splitting words apart into separate nonadjacent pieces which themselves might not have any meaning. Unlike infixes, however, endoclitics are not inflections; their meaning is unrelated to that of their host word. This paper discusses some of the problems endoclitics cause for processing Pashto, both written and spoken. 
In this paper, we present a powerful Arabic morphological analyzer and generator. The approach employs ﬁnite state machines enriched with uniﬁcation capability. The presented system is used as a component in both statistical and rule based machine translation systems. We give detailed illustrations on how we handle nominal and verbal morphology in Arabic. Issues regarding derivational morphology and morphological generation are also addressed. Stimulating problems particular to Arabic and our solutions to these problems are explored meanwhile. An evaluation of the system is presented at the end. 
PEnTrans is an automatic bidirectional English/ Persian text translator. It contains two main modules, PEnT1,2. PEnT1 translates English sentences into Persian and PEnT2 performs translation from Persian to English. WSD which is an important part in translation is done in both systems by employing a combination of extended dictionary and corpus based approaches in PEnT1 and employing a combination of rule based, knowledge based and corpus based approaches in PEnT2. In this paper, introducing PEnTrans and its components, we propose a new WSD method by presenting a hybrid measure to score different senses of a word according to its condition in a sentence and other words in the sentence. 
Arabic is a highly inflectional language, with a rich morphology, relatively free word order, and two types of sentences: nominal and verbal. Arabic natural language processing in general is still underdeveloped and Arabic natural language generation (NLG) is even less developed. In particular, Arabic natural language generation from Interlingua was only investigated using template-based approaches. Moreover, tools used for other languages are not easily adaptable to Arabic due to the Arabic language complexity at both the morphological and syntactic levels. In this paper, we report our attempt at developing a rule-based Arabic generator for task-oriented interlingua-based spoken dialogues. Examples of syntactic generation results from the Arabic generator will be given and will illustrate how the system works. Our proposed syntactic generator has been effectively evaluated using real test data and achieved satisfactory results.  
Morphological and syntactic annotation of multi-token units confront several problems due to the concatenating nature of Persian script and so its orthographic variation. In the present paper, by the analysis of the different collocation types of the tokens, the compositional, non-compositional and semicompositional constructions are described and then, in order to explain these constructions, the static and dynamic multi-token units will be introduced for the non-generative and generative structures of the verbs, infinitives, prepositions, conjunctions, adverbs, adjectives and nouns. Defining the multi-token unit templates for these categories is one of the important results of this research. The findings can be input to the Persian Treebank generator systems. Also, the machine translation systems using the rule-based methods to parse the texts can utilize the results in text segmentation and parsing. 
The paper presents a strategy for deriving English to Urdu translation using English to Hindi MT system. The English-Hindi lexical database is used to collect all possible Hindi words and phrases. These are further augmented by including their morphological variations and attaching all possible postpositions. This list is used to provide mapping from Hindi to Urdu. There may be change in gender and a word or a word group may be of multiple parts of speech. These are resolved using information available from EnglishHindi MT. As Urdu is structurally very close to Hindi using similar post-positions, the output obtained is as acceptable as the Hindi translation. 
A bst r act In this paper, we present how a tool called TerminoWeb can be used to help translators find background information on the Web about a domain, or more specifically about terms found in a text to be translated. TerminoWeb contains different modules working together to achieve such goal: (1) a Web search module specifically tuned for informative texts and glossaries where background knowledge is likely to be found, (2) a term extractor module to automatically discover important terms of a source text, (3) a query generator module to automatically launch multiple queries on the Web from a set of extracted terms. The result of these first three steps is a background knowledge corpus which can then be explored by (4) a corpus exploration module in search of definitional sentences and concordances. In this article, an in-depth example is used to provide a proof of concept of TerminoWeb’s background information search and exploration capability. 
SECTra_w is a Web-based system offering several services, such as supporting MT evaluation campaigns and online post-editing of MT results, to produce reference translations adapted to classical MT systems not built by machine learning from a parallel corpus. The service we are interested in here is the possibility for its users to import a document, or a set of documents (not only a list of preprocessed segments), and achieve highquality translation by applying on-line human post-edition to results of Machine Translation systems. A community of contributing posteditors may carry out the on-line human postedition. In this paper, we describe the use of SECTra_w to translate into French a set of 25 html documents (about 220,000 words) on water and ecology from the on-line Encyclopedia of Life Support Systems1 (EOLSS) using a contributive on-line human post-edition framework. 
This paper presents a technology and a representation for gathering and analysing User Activity Data (UAD) from human translation sessions. We discuss recent advances in the ﬁeld of translation process research and investigate how insights from this branch of research could be instrumentalised for the design translation tools. New technologies and novel ways of using existing technology could emerge with enhanced knowledge about translator’s behaviour and a tight integration of human and machine translation models. 
Bitextor is a free/open-source application for harvesting translation memories from multilingual websites. It downloads all the HTML ﬁles in a website, preprocesses them into a coherent format and, ﬁnally, applies a set of heuristics to select pairs of ﬁles which are candidates to contain the same text in two different languages (bitexts). From these parallel texts, translation memories are generated in TMX format using the library LibTagAligner, which uses the HTML tags and the length of text chunks to perform the alignment. 
Machine-translated segments are increasingly included as fuzzy matches within the translation-memory systems in the localisation workflow. This study presents preliminary results on the correlation between these two types of segments in terms of productivity and final quality. In order to test these variables, we set up an experiment with a group of eight professional translators using an on-line postediting tool and a statistical-base machine translation engine. The translators were asked to translate new, machine-translated and translation-memory segments from the 80-90 percent value using a post-editing tool without actually knowing the origin of each segment, and to complete a questionnaire. The findings suggest that translators have higher productivity and quality when using machinetranslated output than when processing fuzzy matches from translation memories. Furthermore, translators’ technical experience seems to have an impact on productivity but not on quality. Finally, we offer an overview of our current research. 
This paper reports on feedback received from translation professionals on the translation editing environments they use, within or in conjunction with their Translation Memory system. Four options of environments emerge and their respective advantages and disadvantages are discussed. It is shown how these environments impact on the translation process that their users follow and how the process could be improved. A number of needs concerning the functionality of translation editors, as well as the user interface, are presented with reference to particular use contexts. 
This paper examines the performance of multilingual parameterized grammar rules on speech recognition. We present a performance comparison of two different types of Japanese and English grammar-based speech recognizers. One system is derived from monolingual grammar rules and the other from multilingual parameterized grammar rules. The latter one uses hence the same grammar rules for creation of the language models for these two different languages. We carried out experiments on speech recognition of limited domain dialog application. These experiments show that the language models derived from multilingual parameterized grammar rules (1) perform equally well on both tested languages, on English and Japanese, and (2) that the performance is comparable with the recognizers derived from monolingual grammars that were explicitly developed for these languages. This suggests that the sharing grammar resources between different languages could be one solution for more efficient development of rule-based speech recognizers.
In this paper, we present a new approach to disambiguation Arabic using a joint rule-based model which is conceptualized using Dependency Grammar. This approach helps in highly accurate analysis of sentences. The analysis produces a semantic net like structure expressed by means of Universal Networking Language (UNL) - a recently proposed interlingua. Extremely varied and complex phenomena of Arabic language have been addressed.
Semantic access to multimedia content in audiovisual archives is to a large extent dependent on quantity and quality of the metadata, and particularly the content descriptions that are attached to the individual items. However, the manual annotation of collections puts heavy demands on resources. A large number of archives are introducing (semi) automatic annotation techniques for generating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the extraction of keywords from textual resources related to TV programs to be archived (context documents), in collaboration with the Dutch audiovisual archives, Sound and Vision. This paper investigates the suitability of Automatic Speech Recognition transcripts produced in the CATCH-CHoral project for generating such keywords, which we evaluate against manual annotations of the documents, and against keywords automatically generated from context documents describing the TV programs{'} content.
We tried to cope with the complex morphology of Turkish by applying different schemes of morphological word segmentation to the training and test data of a phrase-based statistical machine translation system. These techniques allow for a considerable reduction of the training dictionary, and lower the out-of-vocabulary rate of the test set. By minimizing differences between lexical granularities of Turkish and English we can produce more refined alignments and a better modeling of the translation task. Morphological segmentation is highly language dependent and requires a fair amount of linguistic knowledge in its development phase. Yet it is fast and light-weight {--} does not involve syntax {--} and appears to benefit our IWSLT09 system: our best segmentation scheme associated to a simple lexical approximation technique achieved a 50{\%} reduction of out-of-vocabulary rate and over 5 point BLEU improvement above the baseline.
In this paper, we propose a new method for training translation rules for a Synchronous Context-free Grammar. A bilingual chart parser is used to generate the parse forest, and EM algorithm to estimate expected counts for each rule of the ruleset. Additional rules are constructed as combinations of reliable rules occurring in the parse forest. The new method of proposing additional translation rules is independent of word alignments. We present the theoretical background for this method, and initial experimental results on German-English translations of Europarl data.
Despite many differences between phrase-based, hierarchical, and syntax-based translation models, their training and testing pipelines are strikingly similar. Drawing on this fact, we extend the Moses toolkit to implement hierarchical and syntactic models, making it the first open source toolkit with end-to-end support for all three of these popular models in a single package. This extension substantially lowers the barrier to entry for machine translation research across multiple models.
This paper focuses on the problem of language model adaptation in the context of Chinese-English cross-lingual dialogs, as set-up by the challenge task of the IWSLT 2009 Evaluation Campaign. Mixtures of n-gram language models are investigated, which are obtained by clustering bilingual training data according to different available human annotations, respectively, at the dialog level, turn level, and dialog act level. For the latter case, clustering of IWSLT data was in fact induced through a comparable Italian-English parallel corpus provided with dialog act annotations. For the sake of adaptation, mixture weight estimation is performed either at the level of single source sentence or test set. Estimated weights are then transferred to the target language mixture model. Experimental results show that, by training different specific language models weighted according to the actual input instead of using a single target language model, significant gains in terms of perplexity and BLEU can be achieved.
This demo shows the network-based speech-to-speech translation system. The system was designed to perform realtime, location-free, multi-party translation between speakers of different languages. The spoken language modules: automatic speech recognition (ASR), machine translation (MT), and text-to-speech synthesis (TTS), are connected through Web servers that can be accessed via client applications worldwide. In this demo, we will show the multiparty speech-to-speech translation of Japanese, Chinese, Indonesian, Vietnamese, and English, provided by the NICT server. These speech-to-speech modules have been developed by NICT as a part of A-STAR (Asian Speech Translation Advanced Research) consortium project1.
This paper gives an overview of the evaluation campaign results of the International1Workshop on Spoken Language Translation (IWSLT) 2009 . In this workshop, we focused on the translation of task-oriented human dialogs in travel situations. The speech data was recorded through human interpreters, where native speakers of different languages were asked to complete certain travel-related tasks like hotel reservations using their mother tongue. The translation of the freely-uttered conversation was carried out by human interpreters. The obtained speech data was annotated with dialog and speaker information. The translation directions were English into Chinese and vice versa for the Challenge Task, and Arabic, Chinese, and Turkish, which is a new edition, into English for the standard BTEC Task. In total, 18 research groups participated in this year{'}s event. Automatic and subjective evaluations were carried out in order to investigate the impact of task-oriented human dialogs on automatic speech recognition (ASR) and machine translation (MT) system performance, as well as the robustness of state-of-the-art MT systems for speech-to-speech translation in a dialog scenario.
In this paper, we describe the techniques that are explored in the AppTek system to enhance the translations in the Turkish to English track of IWSLT09. The submission was generated using a phrase-based statistical machine translation system. We also researched the usage of morpho-syntactic information and the application of word reordering in order to improve the translation results. The results are evaluated based on BLEU and METEOR scores. We show that the usage of morpho-syntactic information yields 3 BLEU points gain in the overall system.
This paper describes the Barcelona Media SMT system in the IWSLT 2009 evaluation campaign. The Barcelona Media system is an statistical phrase-based system enriched with source context information. Adding source context in an SMT system is interesting to enhance the translation in order to solve lexical and structural choice errors. The novel technique uses a similarity metric among each test sentence and each training sentence. First experimental results of this technique are reported in the Arabic and Chinese Basic Traveling Expression Corpus (BTEC) task. Although working in a single domain, there are ambiguities in SMT translation units and slight improvements in BLEU are shown in both tasks (Zh2En and Ar2En).
In this paper, we give a description of the Machine Translation (MT) system developed at DCU that was used for our fourth participation in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT 2009). Two techniques are deployed in our system in order to improve the translation quality in a low-resource scenario. The first technique is to use multiple segmentations in MT training and to utilise word lattices in decoding stage. The second technique is used to select the optimal training data that can be used to build MT systems. In this year{'}s participation, we use three different prototype SMT systems, and the output from each system are combined using standard system combination method. Our system is the top system for Chinese{--}English CHALLENGE task in terms of BLEU score.
This paper reports on the participation of FBK at the IWSLT 2009 Evaluation. This year we worked on the Arabic-English and Turkish-English BTEC tasks with a special effort on linguistic preprocessing techniques involving morphological segmentation. In addition, we investigated the adaptation problem in the development of systems for the Chinese-English and English-Chinese challenge tasks; in particular, we explored different ways for clustering training data into topic or dialog-specific subsets: by producing (and combining) smaller but more focused models, we intended to make better use of the available training data, with the ultimate purpose of improving translation quality.
This year{'}s GREYC translation system is an improved translation memory that was designed from scratch to experiment with an approach whose goal is just to improve over the output of a standard translation memory by making heavy use of sub-sentential alignments in a restricted case of translation by analogy. The tracks the system participated in are all BTEC tracks: Arabic to English, Chinese to English, and Turkish to English.
In this paper, we describe the system and approach used by the Institute for Infocomm Research (I2R) for the IWSLT 2009 spoken language translation evaluation campaign. Two kinds of machine translation systems are applied, namely, phrase-based machine translation system and syntax-based machine translation system. To test syntax-based machine translation system on spoken language translation, variational systems are explored. On top of both phrase-based and syntax-based single systems, we further use rescoring method to improve the individual system performance and use system combination method to combine the strengths of the different individual systems. Rescoring is applied on each single system output, and system combination is applied on all rescoring outputs. Finally, our system combination framework shows better performance in Chinese-English BTEC task.
This paper describes the ICT Statistical Machine Translation systems that used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2009. For this year{'}s evaluation, we participated in the Challenge Task (Chinese-English and English-Chinese) and BTEC Task (Chinese-English). And we mainly focus on one new method to improve single system{'}s translation quality. Specifically, we developed a sentence-similarity based development set selection technique. For each task, we finally submitted the single system who got the maximum BLEU scores on the selected development set. The four single translation systems are based on different techniques: a linguistically syntax-based system, two formally syntax-based systems and a phrase-based system. Typically, we didn{'}t use any rescoring or system combination techniques in this year{'}s evaluation.
This paper describes the LIG experiments in the context of IWSLT09 evaluation (Arabic to English Statistical Machine Translation task). Arabic is a morphologically rich language, and recent experimentations in our laboratory have shown that the performance of Arabic to English SMT systems varies greatly according to the Arabic morphological segmenters applied. Based on this observation, we propose to use simultaneously multiple segmentations for machine translation of Arabic. The core idea is to keep the ambiguity of the Arabic segmentation in the system input (using confusion networks or lattices). Then, we hope that the best segmentation will be chosen during MT decoding. The mathematics of this multiple segmentation approach are given. Practical implementations in the case of verbatim text translation as well as speech translation (outside of the scope of IWSLT09 this year) are proposed. Experiments conducted in the framework of IWSLT evaluation campaign show the potential of the multiple segmentation approach. The last part of this paper explains in detail the different systems submitted by LIG at IWSLT09 and the results obtained.
This paper describes the systems developed by the LIUM laboratory for the 2009 IWSLT evaluation. We participated in the Arabic and Chinese to English BTEC tasks. We developed three different systems: a statistical phrase-based system using the Moses toolkit, an Statistical Post-Editing system and a hierarchical phrase-based system based on Joshua. A continuous space language model was deployed to improve the modeling of the target language. These systems are combined by a confusion network based approach.
This paper describes the MIT-LL/AFRL statistical MT system and the improvements that were developed during the IWSLT 2009 evaluation campaign. As part of these efforts, we experimented with a number of extensions to the standard phrase-based model that improve performance on the Arabic and Turkish to English translation tasks. We discuss the architecture of the MIT-LL/AFRL MT system, improvements over our 2008 system, and experiments we ran during the IWSLT-2009 evaluation. Specifically, we focus on 1) Cross-domain translation using MAP adaptation and unsupervised training, 2) Turkish morphological processing and translation, 3) improved Arabic morphology for MT preprocessing, and 4) system combination methods for machine translation.
This paper describes the NICT SMT system used in the International Workshop on Spoken Language Translation (IWSLT) 2009 evaluation campaign. We participated in the Challenge Task. Our system was based on a fairly common phrase-based machine translation system. We used two methods for stabilizing MERT.
This paper reports on the participation of CASIA (Institute of Automation Chinese Academy of Sciences) at the evaluation campaign of the International Workshop on Spoken Language Translation 2009. We participated in the challenge tasks for Chinese-to-English and English-to-Chinese translation respectively and the BTEC task for Chinese-to-English translation only. For all of the tasks, system performance is improved with some special methods as follows: 1) combining different results of Chinese word segmentation, 2) combining different results of word alignments, 3) adding reliable bilingual words with high probabilities to the training data, 4) handling named entities including person names, location names, organization names, temporal and numerical expressions additionally, 5) combining and selecting translations from the outputs of multiple translation engines, 6) replacing Chinese character with Chinese Pinyin to train the translation model for Chinese-to-English ASR challenge task. This is a new approach that has never been introduced before.
We describe the system developed by the team of the National University of Singapore for the Chinese-English BTEC task of the IWSLT 2009 evaluation campaign. We adopted a state-of-the-art phrase-based statistical machine translation approach and focused on experiments with different Chinese word segmentation standards. In our official submission, we trained a separate system for each segmenter and we combined the outputs in a subsequent re-ranking step. Given the small size of the training data, we further re-trained the system on the development data after tuning. The evaluation results show that both strategies yield sizeable and consistent improvements in translation quality.
We present the UOT Machine Translation System that was used in the IWSLT-09 evaluation campaign. This year, we participated in the BTEC track for Chinese-to-English translation. Our system is based on a string-to-tree framework. To integrate deep syntactic information, we propose the use of parse trees and semantic dependencies on English sentences described respectively by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets.
We have developed a two-stage machine translation (MT) system. The first stage is a rule-based machine translation system. The second stage is a normal statistical machine translation system. For Chinese-English machine translation, first, we used a Chinese-English rule-based MT, and we obtained {''}ENGLISH{''} sentences from Chinese sentences. Second, we used a standard statistical machine translation. This means that we translated {''}ENGLISH{''} to English machine translation. We believe this method has two advantages. One is that there are fewer unknown words. The other is that it produces structured or grammatically correct sentences. From the results of experiments, we obtained a BLEU score of 0.3151 in the BTEC-CE task using our proposed method. In contrast, we obtained a BLEU score of 0.3311 in the BTEC-CE task using a standard method (moses). This means that our proposed method was not as effective for the BTEC-CE task. Therefore, we will try to improve the performance by optimizing parameters.
We describe our Arabic-to-English and Turkish-to-English machine translation systems that participated in the IWSLT 2009 evaluation campaign. Both systems are based on the Moses statistical machine translation toolkit, with added components to address the rich morphology of the source languages. Three different morphological approaches are investigated for Turkish. Our primary submission uses linguistic morphological analysis and statistical disambiguation to generate morpheme-based translation models, which is the approach with the better translation performance. One of the contrastive submissions utilizes unsupervised subword segmentation to generate non-linguistic subword-based translation models, while another contrastive system uses word-based models but makes use of lexical approximation to cope with out-of-vocabulary words, similar to the approach in our Arabic-to-English submission.
In this paper, we describe the machine translation system developed at the Polytechnic University of Valencia, which was used in our participation in the International Workshop on Spoken Language Translation (IWSLT) 2009. We have taken part only in the Chinese-English BTEC Task. In the evaluation campaign, we focused on the use of our hybrid translation system over the provided corpus and less effort was devoted to the use of preand post-processing techniques that could have improved the results. Our decoder is a hybrid machine translation system that combines phrase-based models together with syntax-based translation models. The syntactic formalism that underlies the whole decoding process is a Chomsky Normal Form Stochastic Inversion Transduction Grammar (SITG) with phrasal productions and a log-linear combination of probability models. The decoding algorithm is a CYK-like algorithm that combines the translated phrases inversely or directly in order to get a complete translation of the input sentence.
This paper describes the University of Washington{'}s system for the 2009 International Workshop on Spoken Language Translation (IWSLT) evaluation campaign. Two systems were developed, one each for the BTEC Chinese-to-English and Arabic-to-English tracks. We describe experiments with different preprocessing and alignment combination schemes. Our main focus this year was on exploring a novel semi-supervised approach to N-best list reranking; however, this method yielded inconclusive results.
India being multilingual, there is a demand for translation both among Indian languages as well as from English to Indian languages. Translation being not reliable, Anusaaraka aims to provide complete access to the source text in addition to translation. With an appropriate division of load between man and machine, Kannada-Hindi Anusaaraka, developed in early 90s, demonstrated that it is possible to reduce the language barrier considerably. However it is necessary for an Anusaaraka reader to undergo some training on the syntactic divergences and special notation used to handle the divergences in word-meaning mappings between the source and the target language. In the later version of Anusaaraka, in order to reduce the burden on a user, the state-of-the-art MT system formed an important component of it. Care was taken to develop the architecture in such a way that, it can cater to the needs of diverse requirements ranging from faithful access to the full ﬂedged translation. 
India being multilingual, there is a demand for translation both among Indian languages as well as from English to Indian languages. Translation being not reliable, Anusaaraka aims to provide complete access to the source text in addition to translation. With an appropriate division of load between man and machine, Kannada-Hindi Anusaaraka, developed in early 90s, demonstrated that it is possible to reduce the language barrier considerably. However it is necessary for an Anusaaraka reader to undergo some training on the syntactic divergences and special notation used to handle the divergences in word-meaning mappings between the source and the target language. In the later version of Anusaaraka, in order to reduce the burden on a user, the state-of-the-art MT system formed an important component of it. Care was taken to develop the architecture in such a way that, it can cater to the needs of diverse requirements ranging from faithful access to the full ﬂedged translation. 
This paper describes Apertium: a free/open-source machine translation platform (engine, toolbox and data), its history, its philosophy of design, its technology, the community of developers, the research and business based on it, and its prospects and challenges, now that it is five years old.
This paper describes some of the issues found when adapting and extending the Matxin free-software machine translation system to other language pairs. It sketches out some of the characteristics of Matxin and offers some possible solutions to these issues.
This paper describes OpenLogos, a rule-driven machine translation system, and the syntactic-semantic taxonomy SAL that underlies this system. We illustrate how SAL addresses typical problems relating to source language analysis and target language synthesis. The adaptation of OpenLogos resources to a specific application concerning paraphrasing in Portuguese is also described here. References are provided for access to OpenLogos and to SAL.
This article describes the development of a shallow-transfer machine translation system from Swedish to Danish in the Apertium platform. It gives details of the resources used, the methods for constructing the system and an evaluation of the translation quality. The quality is found to be comparable with that of current commercial systems, despite the particularly low coverage of the lexicons.
We describe the development of a two-way shallow-transfer machine translation system between Norwegian Nynorsk and Norwegian Bokma ̊l built on the Apertium platform, using the Free and Open Source resources Norsk Ordbank and the Oslo{--}Bergen Constraint Grammar tagger. We detail the integration of these and other resources in the system along with the construction of the lexical and structural transfer, and evaluate the translation quality in comparison with another system. Finally, some future work is suggested.
This article describes the development of an open-source morphological analyser for Bengali Language using 􏰁nitestate technology. First we discuss the challenges of creating a morphological analyser for a highly in􏰂ectional language like Bengali and then propose a solution to that using lttoolbox, an open-source 􏰁nite-state toolkit. We then evaluate the performance of our developed system and propose ways of improving it further.
Some machine translation services like Google Ajax Language API have become very popular as they make the collaboratively created contents of the web 2.0 available to speakers of many languages. One of the keys of its success is its clear and easy-to-use application programming interface (API) and a scalable and reliable service. This paper describes a highly scalable implementation of an Apertium-based translation web service, that aims to make contents available to speakers of lesser resourced languages. The API of this service is compatible with Google{'}s one, and the scalability of the system is achieved by a new architecture that allows adding or removing new servers at any time; for that, an application placement algorithm which decides which language pairs should be translated on which servers is designed. Our experiments show how the resulting architecture improves the translation rate in comparison to existing Apertium-based servers.
Service Oriented Architecture (SOA) is a paradigm for organising and using distributed services that may be under the control of different ownership domains and implemented using various technology stacks. In some contexts, an organisation using an IT infrastructure implementing the SOA paradigm can take a great benefit from the integration, in its business processes, of efficient machine translation (MT) services to overcome language barriers. This paper describes the architecture and the design patterns used to develop an MT service that is efficient, scalable and easy to integrate in new and existing business processes. The service is based on Apertium, a free/opensource rule-based machine translation platform.
This paper describes the implementation of a second-order hidden Markov model (HMM) based part-of-speech tagger for the Apertium free/opensource rule-based machine translation platform. We describe the part-ofspeech (PoS) tagging approach in Apertium and how it is parametrised through a tagger definition file that defines: (1) the set of tags to be used and (2) constrain rules that can be used to forbid certain PoS tag sequences, thus refining the HMM parameters and increasing its tagging accuracy. The paper also reviews the Baum-Welch algorithm used to estimate the HMM parameters and compares the tagging accuracy achieved with that achieved by the original, first-order HMM-based PoS tagger in Apertium.
This article describes the needs of UOC regarding translation and how these needs are satisfied by Prompsit further developing a free rule-based machine translation system: Apertium. We initially describe the general framework regarding linguistic needs inside UOC. Then, section 2 introduces Apertium and outlines the development scenario that Prompsit executed. After that, section 3 outlines the specific needs of UOC and why Apertium was chosen as the machine translation engine. Then, section 4 describes some of the features specially developed in this project. Section 5 explains how the linguistic data was improved to increase the quality of the output in Catalan and Spanish. And, finally, we draw conclusions and outline further work originating from the project.
Grup de Processament del Llenguatge Natural Departament de Llenguatges i Sistemes Informa`tics Universitat Polit`ecnica de Catalunya May 13, 2009  Jesu´s Gim´enez  Empirical Machine Translation and its Evaluation  Empirical Machine Translation How are Empirical MT Systems DevSetloaptiesdticTaol dMaya?chiEnevaTlurantisolantiMonethods Tackling the Negative Eﬀe Outline  
EAMT 2009 Barcelona  Outline Base problem of the phrase translation Word features Learning problem Alignment of words Example Performance comparison  Main Components of the translator system Phrase translator - the main topic of this presentation. A well known system: GIZA++ Additional postprocessing tools, e.g. in Moses Decoder, which can ﬁt better to the phrase dictionary generated by maximum margin learning procedure.  The base learning problem of phrase translation A phrase implies a binary classiﬁcation of the words of a sentence; the words within the phrase are the positive cases, the remaining part gives the negative ones. The translation can be interpreted as a propagation of the classes of a source sentence into the corresponding target sentence. It might be interpreted either as an inductive or a transductive learning problem.  The learning schema  class − − − − − − − − − − − + + + + − − − −  source words Je vous demands donc a` nouveau de faire le ne´cessaire pour que nous puissions disposer d une cha˜i ne ne´erlandaise  predicted class ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −) ?(+, −)  target words I would therefore once more ask you to ensure that we get a Dutch channel as well  Computational difﬁculties If the sentence length in words is 30 and the maximum length allowed of non-gapped phrases is 5 then 140 binary classiﬁcation problems have to be solved! Does any acceptable efﬁcient joint approximation schema exist at all?  A learning approach The Support Vector Machine(SVM) has proved to be a highly accurate learning tool, but it is able to deal only with binary outputs. The learning framework of the SVM can be extended to predict arbitrary vector represented outputs with no additional cost, we will call it Maximum Margin Regression(MMR) in the sequel. The details are discussed when the concrete learning problem is unfolded. MATLAB source code of the solver and a demo for multiclass classiﬁcation in MMR is freely available on the web.  The skeleton of the phrase translation Sentence-wise word relations, the building blocks: global relationships between word pairs, local relations, inference between global and local relations, Estimating phrases, Collect those sequences of source and target words which have the highest accumulated word-wise relations.  A projection rule of the sentences  Mapping words  Mapping phrases  P1 ⇔ R1, P2 ⇔ R2, P1 ∩ P2 ⇔ R1 ∩ R2, P1 ∪ P2 ⇔ R1 ∪ R2, P1 \ P2 ⇔ R1 \ R2, P2 \ P1 ⇔ R2 \ R1. Intersections mapped into corresponding intersections of the subsets of words those we might consider as phrases. Obviously it can be achieved only approximately!  Global versus local relations of words  Interference of global and local relations:  Strong global: Frequent co-occurrences, Strong local: adjacent(or almost adjacent) positions  Globally weak  Globally strong  Locally weak Locally strong  High conﬁdence Likely  No relation Likely  No relation High conﬁdence  There is a relation There is a relation  case of rare words!  Sentence-wised word distances  Distances: The distances measure the co-occurrences of words and their relative positions within the sentences. A co-occurrence with high distance is down scaled.  Within a language:  dS (w1, w2)  =  mini1∈I(w1),i2∈I(w2)  |  i1 nS  −  i2 nS  |  Between two languages:  dSs ,St  (w1,  w2)  =  mini1∈I(w1),i2∈I(w2)  |  i1 nSs  −  i2 nSt  |  Sentence-wised word similarities  Similarities: Linear: Gaussian:  sS (w1, w2) = 1 − dS (w1, w2) sSs,St (w1, w2) = 1 − dSs,St (w1, w2)  sS (w1, w2) = e  −  dS2  (w1 σ  ,w2 )  sSs,St (w1, w2) = e  −  dS2 s  ,St (w1 σ  ,w2  )  Logistic:  sS (w1, w2)  =  
 Marco Turchi - Univ. of Bristol  Learning to Translate  2  Motivation  A belief in SMT is that “more data → better translation”. But: how much parallel text do we need to obtain acceptable translation? Do we have a constant increase in performance when adding more data? If we have an exhaustive amount of parallel data, can the SMT model be a limitation? Can we find the current limitation of the SMT approach? Some helpful facts: data availability (Europarl, Hansard, UN corpus, Web, …); recent advances in software (Moses, …); computing power (HPC cluster, cloud computing, …).  Marco Turchi - Univ. of Bristol  Learning to Translate  3  Motivation  Extensive study of a Phrase based SMT system using Moses, Europarl and a HPC cluster. Try to answer the previous questions by extrapolating the performance of the system under different conditions: constantly increasing the training; changing the system parameters; adding noise to the system parameters; … Investigate the potentials and limitations of the current technology analysing a STM system as a Learning System. Explore new aspects of a SMT system under a machine learning point of view. Confirm some previous results in SMT field. Suggest some possible research directions.  Marco Turchi - Univ. of Bristol  Learning to Translate  4  Introduction Performance of a learning system is result of (at least) two effects: representation power of the hypothesis class: how well the system can approximate the target behaviour; statistical effects: how well the system can estimate the best element of the hypothesis class.  Marco Turchi - Univ. of Bristol  Learning to Translate  5  Introduction They interact, with richest classes being better approximators of the target behaviour, but requiring more training data to identify the best hypothesis.  In SMT, learning task is complicated by the fact that the probability of encountering new words or expressions never vanishes.  Marco Turchi - Univ. of Bristol  Learning to Translate  6  Introduction  These observations lead us to analyze: learning and unlearning curves; flexibility of the representation class; stability of the model; Experiments: 1. role of training set size on performance on new sentences; 2. role of training set size on performance on known sentences; 3. role of phrase length in translation table; 4. model perturbation: analysis and unlearning curves.  Marco Turchi - Univ. of Bristol  Learning to Translate  7  Experimental Setup  Software Moses. Giza++: IBM model 1, 2, 3, and 4 with number of iterations for model 1 equal to 5, model 2 equal to 0, model 3 and 4 equal to 3. SRILM: n-gram order equal to 3 and the Kneser-Ney smoothing algorithm. Mert: 100 the number of nbest target sentence for each develop sentence. Training, development and test set sentences are tokenized and lowercased. Maximum number of tokens for each sentence in the training pair is 50. TMs were limited to a phrase-length of 7 words. LMs were limited to 3.  Marco Turchi - Univ. of Bristol  Learning to Translate  8  Experimental Setup Data Europarl Release v3 Spanish-English corpus. Training set: 1,259,914 pairs. Test and Development sets 2,000 pairs each. Evaluation Scores BLEU, NIST, Meteor, TER, Unigram Recall, Unigram Precision, FMean, F1, Penalty and Fragmentation. BLEU is used as evaluation score after we observed its high correlation to the other scores on the corpus.  Marco Turchi - Univ. of Bristol  Learning to Translate  9  Experimental Setup Hardware University of Bristol cluster machine, http://www.acrc.bris.ac.uk/acrc/hpc.htm. 96 nodes each with two dual-core opteron processors. 8 Gb of RAM memory per node (2 Gb per core). SilverStorm Infiniband high-speed connectivity throughout for parallel code message passing. General Parallel File System (GPFS). Storage 11 terabytes. Torque v2.1.6p17 as the Resource Manager. Maui v3.2.6p16 as the scheduler.  Marco Turchi - Univ. of Bristol  Learning to Translate  10  Role of training set size on performance on unknown sentences Analyse how performance is affected by training set size, by creating learning curves. Create subsets of the complete corpus by subsampling sentences from a uniform distribution, with and without replacement; with replacement: analyse the performance on different training sets of the same size and the effects of optimization phase; without replacement: study the SMT learning curves in the Linear-Linear and Linear-Log scales.  Marco Turchi - Univ. of Bristol  Learning to Translate  11  Role of training set size on performance on unknown sentences Create subsets of the complete corpus by subsampling sentences from a uniform distribution, with replacement. Ten random subsets for each of the 20 chosen sizes (each size 5%, 10%, etc of the complete corpus). For each subset, a new instance of Moses has been created. Development and test sets contain 2,000 pairs each. The experiments have been run for the models with and without the optimization step.  Marco Turchi - Univ. of Bristol  Learning to Translate  12  Role of training set size on performance on unknown sentences 1. Small error bars. 2. Benefits optimization phase. 3. Curves affected by the Birthday paradox.  Marco Turchi - Univ. of Bristol  Learning to Translate  13 
SMART workshop, Barcelona 2009  Cyril Goutte  Motivation  SMART workshop, Barcelona 2009 / 1  We address two questions: 1. Is there a difference between original and (human-) translated text and can we detect it reliably? 2. If so, can we use that to improve Machine Translation quality?  Cyril Goutte  Motivation  SMART workshop, Barcelona 2009 / 2  We address two questions: 1. Is there a difference between original and (human-) translated text and can we detect it reliably? 2. If so, can we use that to improve Machine Translation quality? Our answers: 1. Yes: on the Canadian Hansard, we get 90+% accuracy. 2. Yes: on French-English, we obtain up to 0.6 BLEU point increase.  Cyril Goutte  Problem setting  SMART workshop, Barcelona 2009 / 3  Translations often have a “feel” of the original language: Translationese. If translationese is real, it may be possible to detect it! Earlier studies: Baroni&Bernardini (2006): detect original vs. translation is a monolingual Italian corpus, with accuracy up to 87%. van Halteren (2008) : detect source language in multi-parallel corpus and identify source language markers. Both show that various aspects of translationese are detectable. We experiment on a large bilingual corpus (Hansard) and investigate how detecting translation direction may impact Machine Translation quality. Cyril Goutte  Index  SMART workshop, Barcelona 2009 / 4  
 Vocabulary: {wi | i = 1, …, N }  Documents are represented with vectors (word space):   Example:  Document set: d1 = “Canonical Correlation Analysis” d2 = “Numerical Analysis” d3 = “Numerical Linear Algebra”  Document vector representation: x1 = (1, 1, 1, 0, 0, 0) x2 = (0, 0, 1, 1, 0, 0) x3 = (0, 0, 0, 1, 1, 1,)  Vocabulary: {“Canonical ”, “Correlation ”, “Analysis”, “Numerical ”, “Linear ”, “Algebra”}   similarity(di, dj) = <xi / ||xi||, xj / ||xj||> = cos(∢(xi, xj))  d1 = “Kanonična korelacijska analiza” d2 = “Numerična analiza” d3 = “Numerična linearna algebra”  x1 = (1, 1, 1, 0, 0, 0) x2 = (0, 0, 1, 1, 0, 0) x3 = (0, 0, 0, 1, 1, 1,)  x1  x2  x3  x1 1.0 0.4 0.0  x2 0.4 1.0 0.4  x3 0.0 0.4 1.0   Input: aligned training set {(xi,yi) | xi∈ℝn, yi∈ℝm, i = 1, …,ℓ}  CCA is attacking the following problem: Find directions wx∈ℝn and wy∈ℝm, along which pairs (xi,yi) are maximally correlated:  Formulation (before regularization):  Kovariančna matrika je definirana z:   Can be transformed to generalized eigenvalue problem: 
May 13, 2009  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 1/29  Structured Prediction Each (multi-label) output contains multiple (micro-)labels  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 2/29  Structured Prediction Each (multi-label) output contains multiple (micro-)labels Micro-labels interacts each other  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 2/29  linear program is solved iteratively via column generation. In this case, the process of generating most violated constraints is Structured Prediction equivalent to searching for highest-scored misclassified incorrect multilabels, which can be easily achieved by decoding the structure based on current estimations. In addition, we also explore the integration of column generation and an extragradient method for linear programming to gain further efficiency.  point for the next iteratio tively expect that it conve the solution. Hence, this cially when the constrain umn generation iteration. ess, based on a very sma  Dual Objective  EacNchaont(hmoannulydltldeio-alerasbbitterhaler)yposrtourputocpstuuertdescmoaenntdhtoaladirnghsears-mstchuaelletaippdrlvoeabnl(etammgseic,sirtotsh-ca)otlvait-bels Micerroin-glanbuemlbserinbtoeurnadcatlssoesahcohwsontihceergeneralization properties. ExaSmtprulec: tuserqeudenCclealsasbiefliicnagti(oHnMM)  Observations x: x1  x2  x3  x4  x5  Labels y: y11  y21  y31  y41  y51  Primal Objective Experimental Res  y12  y22  y32  y42  y52  y23 Correct Multilabels: Y  y43 Incorrect Multilabels: Y  Model  E  CRF 4.5  MIRA 4.9  Perceptron 5.3  LP/Simplex 4.9  LP/Extragradient 4.9  HMM 20.  LP Formulation  5000  We modify the SVM-style formulation of structured classification 4500  problems by replacing the L2-regularization in the objective func- 4000  tion with an L1-regularization, yielding an LP problem that sepa- 3500  ime [s]  rates correct multilabels from potential incorrect ones with ap- 3000  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. S25z00edm´ak,  2/29  linear program is solved iteratively via column generation. In this case, the process of generating most violated constraints is Structured Prediction equivalent to searching for highest-scored misclassified incorrect multilabels, which can be easily achieved by decoding the structure based on current estimations. In addition, we also explore the integration of column generation and an extragradient method for linear programming to gain further efficiency.  point for the next iteratio tively expect that it conve the solution. Hence, this cially when the constrain umn generation iteration. ess, based on a very sma  Dual Objective  EacNchaont(hmoannulydltldeio-alerasbbitterhaler)yposrtourputocpstuuertdescmoaenntdhtoaladirnghsears-mstchuaelletaippdrlvoeabnl(etammgseic,sirtotsh-ca)otlvait-bels Micerroin-glanbuemlbserinbtoeurnadcatlssoesahcohwsontihceergeneralization properties. ExaSmtprulec: tuserqeudenCclealsasbiefliicnagti(oHnMM)  Observations x: x1  x2  x3  x4  x5  Labels y: y11  y21  y31  y41  y51  Primal Objective Experimental Res  y12  y22  y32  y42  y52  Model  E  CRF 4.5  MIRA 4.9  y23  y43  Perceptron 5.3  LP/Simplex 4.9  Correct Multilabels: Y  Incorrect Multilabels: Y  LP/Extragradient 4.9  More examples: parsing tree, bipartite matching, hierarchical  HMM 20.  clasLsiPﬁcFaotiromn,ueltaction  5000  We modify the SVM-style formulation of structured classification 4500  problems by replacing the L2-regularization in the objective func- 4000  tion with an L1-regularization, yielding an LP problem that sepa- 3500  ime [s]  rates correct multilabels from potential incorrect ones with ap- 3000  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. S25z00edm´ak,  2/29  Structured Prediction (Cont.) Predict multi-label y = y1, y2, . . . , yl for an input object x.  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 3/29  Structured Prediction (Cont.) Predict multi-label y = y1, y2, . . . , yl for an input object x. Formally, given input and output space X and Y, learn a w-parameterized function f : X × Y → R, such that the prediction yˆ ∈ Y for an arbitrary x ∈ X is derived by: yˆ = arg max f (x, y; w) y∈Y  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 3/29  Structured Prediction (Cont.) Predict multi-label y = y1, y2, . . . , yl for an input object x. Formally, given input and output space X and Y, learn a w-parameterized function f : X × Y → R, such that the prediction yˆ ∈ Y for an arbitrary x ∈ X is derived by: yˆ = arg max f (x, y; w) y∈Y Assume f is from the linear family, and deﬁne the joint feature mapping Φ : X × Y → Rd . Then we have: yˆ = arg max w Φ(x, y) y∈Y  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 3/29  Structured Prediction (Cont.)  Predict multi-label y = y1, y2, . . . , yl for an input object x. Formally, given input and output space X and Y, learn a w-parameterized function f : X × Y → R, such that the prediction yˆ ∈ Y for an arbitrary x ∈ X is derived by: yˆ = arg max f (x, y; w) y∈Y Assume f is from the linear family, and deﬁne the joint feature mapping Φ : X × Y → Rd . Then we have: yˆ = arg max w Φ(x, y) y∈Y Seek the w-parameterized hyperplane separating the positive and negative training examples S = {(xi , yi )}mi=1 with large margin.  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 3/29  Existing Techniques Structured Perceptron [Collins, 2002]  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 4/29  Existing Techniques Structured Perceptron [Collins, 2002] Margin Infused Relaxed Algorithm (MIRA) [Crammer et al., 2006]  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 4/29  Existing Techniques Structured Perceptron [Collins, 2002] Margin Infused Relaxed Algorithm (MIRA) [Crammer et al., 2006] SVM-type Algorithms  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 4/29  Existing Techniques Structured Perceptron [Collins, 2002] Margin Infused Relaxed Algorithm (MIRA) [Crammer et al., 2006] SVM-type Algorithms Hidden Markov Support Vector Machines [Altun et al., 2003] and extensions [Tsochantaridis et al., 2005]  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 4/29  Existing Techniques Structured Perceptron [Collins, 2002] Margin Infused Relaxed Algorithm (MIRA) [Crammer et al., 2006] SVM-type Algorithms Hidden Markov Support Vector Machines [Altun et al., 2003] and extensions [Tsochantaridis et al., 2005] Max-Margin Markov Networks [Taskar et al., 2003]  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 4/29  Existing Techniques Structured Perceptron [Collins, 2002] Margin Infused Relaxed Algorithm (MIRA) [Crammer et al., 2006] SVM-type Algorithms Hidden Markov Support Vector Machines [Altun et al., 2003] and extensions [Tsochantaridis et al., 2005] Max-Margin Markov Networks [Taskar et al., 2003] Combinatorial Models [Taskar et al., 2004,2005,2006]  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 4/29  Large-Margin Separation SVM-style formulation: max γ w,γ s.t. w ∆Φ(xi , yi , y) ≥ γ, ∀y = yi , i = 1, . . . , m; w 2 = 1.  May 13, 2009  L1-Regularized Structured Prediction  Z. Wang, J. Shawe-Taylor, S. Szedm´ak, 5/29  Large-Margin Separation  SVM-style formulation:  max γ w,γ s.t. w ∆Φ(xi , yi , y) ≥ γ, ∀y = yi , i = 1, . . . , m; w 2 = 1.  Equivalent form:  min w  
 Approach  Features  Algorithms  Method  Experiments  Scenario 1: providing score to the end-user that is as close as possible to a human score  Scenario 2: filtering out bad translations for professional translators  The task of CE for MT   Goal: given the output of a Machine Translation (MT) system for a given input, provide an estimate of its quality.   Motivation: assessing the quality of translations is   Time consuming:  Los investigadores dicen gripe porcina tiene «pleno potencial pandémico", difundiendo rápidamente entre las personas y es probable que vaya mundial en los próximos seis a nueve meses, con uno de cada tres de la población mundial infectada.   Not possible - if user does not know the source language:  शोधकतार्ओं  सूअर लू पूण्र  क्षमता है , लोग को त काल  छह से नौ महीन म , एक तीन  ....  पै डिे मक और वैि क दिु नया की  की अगले आबादी  The task of CE for MT Uses: Is it worth providing this translation as suggestion to the professional translator? – Filter out “bad” translations to avoid professional translators wasting time reading / post-editing them. Should this translation be highlighted as “suspect” to the reader? – Make end-users aware of the translation quality.  General approach Different from MT evaluation (BLEU, NIST, etc): reference translations are NOT available Unit: word, phrase or sentence Embedded to SMT system (word or phrase probabilities) or dedicated layer (machine learning problem) Traditional approach:  Binary problem: distinguish between good and bad translations Training data: data automatically annotated with NIST/BLEU or manually annotated (e.g. 1-5 scores)  General approach Different from MT evaluation (BLEU, NIST, etc): reference translations are NOT available Unit: word, phrase or sentence Embedded to SMT system (word or phrase probabilities) or dedicated layer (machine learning problem) Traditional approach:  Binary problem: distinguish between good and bad translations  Continuous score Training data: data automatically annotated with NIST/BLEU or manually annotated (e.g. 1-5 scores), from several MT systems, text domains and language pairs  Method n Identify and extract information sources. n Refine the set of information sources to keep only the relevant ones  Increase performance n Learn a model to produce quality scores  Regression algorithm n Apply the model to predict quality scores for new translations  Features  Resource & language independent features  Black-box (77): from the input and translation sentences, monolingual or parallel corpora, e.g.:  Source and target sentence lengths and their ratios  Language model and other statistics in the corpus  Shallow syntax checking (target and target against source)  Average number of possible translations per source word (SMT)  Practical scenario:  Useful when it is not possible to have access to internal features of the MT systems (commercial systems, e.g.).  Provides a way to perform the task of CE across different MT systems, which may use different frameworks.  Features  Glass-box (54): depend on some aspect of the translation process, e.g.:  Language model (target) using n-best list – word/phrase-based  Proximity with other hypothesis in the n-best list  MT base model features  Distortion count, gap count, (compositional) bi-phrase probability  Search nodes in the graph (aborted, pruned)  Proportion of unknown words in the source  Richer scenario:  When it is possible to have access to internal features of the MT systems.  Learning methods  Feature selection: Partial Least Squares (PLS)  Regression: PLS, SVM  Partial Least Square Regression  Projects the original data onto a different space of latent variables (or “components”)  Used to find the fundamental relations between two matrices (input X and Y response variables): tries to find the multidimensional direction in the X space that explains the maximum variance direction in the Y space  Provides by-product an ordering of the original features according to their importance  Particularly indicated when the features in X are strongly correlated (multicollinearity)  the case in our datasets  Partial Least Square Regression  Ordinary multiple regression problem: Y  XBw  F  Where:  Bw is the regression matrix computed directly using an optimal number of components.  F is the residual matrix.  When X is standardized, an element of Bw with large absolute value indicates an important X-variable.  Feature Selection with PLS  Method:  Compute the Bw matrix on some training data for different numbers of components (all possible)  Sort the absolute value of the bw-coefficients. This produces a list of features from the most important to the less important (Lb)  Select the top n features (and number of components) on a validation set  Produce predictions using these n features on a test set  Evaluate predictions using appropriate metrics  Feature Selection with PLS   Method:  Compute the Bw matrix on some training data for different numbers of components (all possible)  Sort the absolute value of the bw-coefficients. This produces a list of features from the most important to the less important (Lb)  Done for each i-th training subsample: obtain several Lb(i)  66  7  …  35  10  44  56  …  9  10  …  …  …  …  …  66  56  35  9   The final list L is obtained picking the most “voted” features for each column (mode): L = {66, 56, …, 35, 10}  Feature Selection with PLS  Method:  Compute the Bw matrix on some training data for different numbers of components (all possible)  Sort the absolute value of the bw-coefficients. This produces a list of features from the most important to the less important (Lb)  Select the top n features (and number of components) on a validation set  Add one feature one by one  Analyze learning curves to verify the prediction error  Select the top n features and the number of components that minimize the prediction error  Feature Selection with PLS  Method:  Compute the Bw matrix on some training data for different numbers of components (all possible)  Sort the absolute value of the bw-coefficients. This produces a list of features from the most important to the less important (Lb)  Select the top n features (and number of components) on a validation set  Produce predictions using these n features on a test set  PLS for regression  Feature Selection with PLS   Method:  Compute the Bw matrix on some training data for different numbers of components (all possible)  Sort the absolute value of the bw-coefficients. This produces a list of features from the most important to the less important (Lb)  Select the top n features on a validation set  Produce predictions using these n features on a test set  Evaluate predictions using appropriate metrics  Root Mean Square Error (RMSPE) over all subsamples   RMSPE   
The origin and early history of research and development in machine translation might suggest that it is only interesting, or applicable, to the greatest of major languages. But founders' effects do not persist in eras of furious technical change, unless they concern essentially arbitrary aspects, such as technical standards. Machine translation, and language technologies more generally, may yet be very useful to minority languages, promoting and extending both their use and their status, in a world where there may be more than one dominant language. The title quote is borrowed from F. Spencer Chapman’s 1949 book on his experience of jungle warfare. 
Endangered languages may require more ﬂexible language technologies than stable ones because they may not be standardized and they may be in a cycle of losing, replacing, and borrowing vocabulary and grammar. This paper argues that the coverage and content of language technologies should be in the hands of the speech community, and that it needs to be adaptable and learn from users. This calls for new approaches, possibly based on active learning to allow the language technologies to be as ﬂexible and changeable as languages generally are. The paper also addresses ways in which the development of a machine translation system can be initiated when resources are scarce, including the experience of the AVENUE project with Mapudungun (Chile) and In˜upiaq (Alaska). 
Translating unknown words between related languages using a character-based statistical machine translation model can be beneﬁcial. In this paper, we describe a simple method to combine character-based models with standard word-based models to increase the coverage of a phrase-based SMT system. Using this approach, we can show a modest improvement when translating between Norwegian and Swedish. The potentials of applying character-based models to closely related languages is also illustrated by applying the character model on its own. The performance of such an approach is similar to the word-level baseline and closer to the reference in terms of string similarity. 
Computer Assisted Translation tools remain the preferred solution of human translators when publication quality is of concern. In this paper, we present our ongoing efforts conducted within TS3, a project which aims at improving the commercial bilingual concordancer TransSearch. The core technology of this Web-based service mainly relies on sentence-level alignment. In this study, we discuss and evaluate the embedding of statistical word-level alignment.  erage of 177 000 queries a month over a one-year period (2006–2007). Figure 1 provides a screenshot of a session with the current concordancer TransSearch. A user submitted the multi-word query in keeping with to which the system responded with a webpage showing the ﬁrst 25 pairs of sentences in the TM that contain an occurrence of the query. As can be observed, nothing in the target material retrieved is emphasized, which forces the user to read the examples retrieved until an appropriate translation was found.  
We investigate the problem of predicting the quality of sentences produced by machine translation systems when reference translations are not available. The problem is addressed as a regression task and a method that takes into account the contribution of different features is proposed. We experiment with this method for translations produced by various MT systems and different language pairs, annotated with quality scores both automatically and manually. Results show that our method allows obtaining good estimates and that identifying a reduced set of relevant features plays an important role. The experiments also highlight a number of outstanding features that were consistently selected as the most relevant and could be used in different ways to improve MT performance or to enhance MT evaluation. 
This paper reports an experiment on evaluating and improving MT quality of light-verb construction (LVCs) – combinations of a ‘semantically depleted’ verb and its complement. Our method uses construction-level human evaluation for systematic discovery of mistranslated contexts and creating automatic pre-editing rules, which make the constructions more tractable for Rule-Based Machine Translation (RBMT) systems. For rewritten phrases we achieve about 40% reduction in the number of incomprehensible translations into English from both French and Russian. The proposed method can be used for enhancing automatic pre-editing functionality of state-of-theart MT systems. It will allow MT users to create their own rewriting rules for frequently mistranslated constructions and contexts, going beyond existing systems’ capabilities offered by user dictionaries and do-not translate lists. 
Recently novel MT evaluation metrics have been presented which go beyond pure string matching, and which correlate better than other existing metrics with human judgements. Other research in this area has presented machine learning methods which learn directly from human judgements. In this paper, we present a novel combination of dependency- and machine learning-based approaches to automatic MT evaluation, and demonstrate greater correlations with human judgement than the existing state-of-the-art methods. In addition, we examine the extent to which our novel method can be generalised across different tasks and domains. 
In this paper, a human evaluation of a Catalan-Spanish Ngram-based statistical machine translation system is used to develop speciﬁc techniques based on the use of grammatical categories, lexical categorisation and text processing, for the enhancement of the ﬁnal translation. The system is successfully improved when testing with ad hoc and general corpora, as it is shown in the ﬁnal automatic evaluation. 
This paper presents three successful techniques to translate prepositions heading verbal complements by means of rich linguistic information, in the context of a rule-based Machine Translation system for an agglutinative language with scarce resources. This information comes in the form of lexicalized syntactic dependency triples, verb subcategorization and manually coded selection rules based on lexical, syntactic and semantic information. The ﬁrst two resources have been automatically extracted from monolingual corpora. The results obtained using a new evaluation methodology show that all proposed techniques improve precision over the baselines, including a translation dictionary compiled from an aligned corpus, and a state-of-the-art statistical Machine Translation system. The results also show that linguistic information in all three techniques are complementary, and that a combination of them obtains the best F-score results overall. 
This paper presents an extension for a bilingual n-gram statistical machine translation (SMT) system based on allowing translation units with gaps. Our gappy translation units can be seen as a ﬁrst step towards introducing hierarchical units similar to those employed in hierarchical MT systems. Our goal is double. On the one hand we aim at capturing the beneﬁts of the higher generalization power shown by hierarchical systems. On the other hand, we want to avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make diﬃcult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system. 
Segmentation is widely used in adapting Statistical Machine Translation to highly inﬂected languages as Basque. The way this segmentation is carried out impacts on the quality of the translation. In order to look for the most adequate segmentation for a Spanish-Basque system, we have tried different segmentation options and analyzed their effects on the translation quality. Although all segmentation options used in this work are based on the same morphological analysis, translation quality varies signiﬁcantly depending on the segmentation criteria used. Most of the segmentation options outperform the baseline according to all metrics, except the one which splits words according the morpheme boundaries. From here we can conclude the importance of the development of the segmentation criteria in SMT. 
The paper presents a study of a challenging task in machine translation and crosslanguage information retrieval – translation of toponyms. Due to their linguistic and extra-linguistic nature, toponyms deserve a special treatment. The overall translation process includes two stages of processing: dictionary-based and out-ofvocabulary toponym translation. The latter is divided into three steps: source string normalisation, translation, and target string normalisation. The translation process implies an application of translation strategies and linguistic toponym translation patterns. Possible translation strategies, including transliteration and translation per se along with combined strategies, and linguistic toponym translation patterns, including multi-word patterns as well, were investigated and implemented for English-Latvian machine translation. 10,000 The UK-related toponyms from Geonames were selected for a development set. The evaluation of output quality on basis of a test set has showed 67% accuracy in out-ofvocabulary translation: 58% on a set containing one-word toponymic units and 81% on a multi-word test set. 
We present an environment for the recognition and translation of Named Entities (NEs). The environment consists of a new formalism for the Named Entity Recognition and Translation (NERT), a parsing mechanism that reads the rules, recognizes Named Entities in given texts and suggests their translation, as well as a set of tools for the evaluation. We suggest a method for the evaluation of (sets of) NERT rules that uses raw (not annotated) bilingual corpora. 
We investigate the impact of the original source language (SL) on French–English PB-SMT. We train four conﬁgurations of a state-of-the-art PB-SMT system based on French–English parallel corpora which differ in terms of the original SL, and conduct experiments in both translation directions. We see that data containing original French and English translated from French is optimal when building a system translating from French into English. Conversely, using data comprising exclusively French and English translated from several other languages is suboptimal regardless of the translation direction. Accordingly, the clamour for more data needs to be tempered somewhat; unless the quality of such data is controlled, more training data can cause translation performance to decrease drastically, by up to 38% relative BLEU in our experiments. 
A machine translated sentence is seldom completely correct. Conﬁdence measures are designed to detect incorrect words, phrases or sentences, or to provide an estimation of the probability of correctness. In this article we describe several word- and sentence-level conﬁdence measures relying on different features: mutual information between words, n-gram and backward n-gram language models, and linguistic features. We also try different combination of these measures. Their accuracy is evaluated on a classiﬁcation task. We achieve 17% error-rate (0.84 f-measure) on wordlevel and 31% error-rate (0.71 f-measure) on sentence-level. 
In this paper we investigate possibilities of the development of a task-speciﬁc translation component for cross-language question answering. We focus on the optimization of phrase-based SMT for models trained on very limited data resources. We also look at the combination of such systems with another approach based on example-based MT with proportional analogies. In our experiments we could improve a strong baseline of a general purpose MT engine with more than 5 BLEU points. 
This paper describes the development of two prototype systems for machine translation between North Sámi and Lule Sámi. Experiments were conducted in rule-based machine translation (RBMT), using the Apertium platform, and statistical machine translation (SMT) using the Mosesdecoder. The experiments show that both approaches have their advantages and disadvantages, and that they can both make use of pre-existing linguistic resources. 
Collocations constitute a subclass of multi-word expressions that are particularly problematic for machine translation, due 1) to their omnipresence in texts, and 2) to their morpho-syntactic properties, allowing virtually unlimited variation and leading to long-distance dependencies. Since existing MT systems incorporate mostly local information, these are arguably ill-suited for handling those collocations whose items are not found in close proximity. In this article, we describe an integrated environment in which collocations (and possibly their translation equivalents) are ﬁrst identiﬁed from text corpora and stored in the lexical database of a translation system, then they are employed by this system, which is capable of dealing with syntactic transformations as it is based on a deep linguistic approach. We compare the performance of our system (in terms of collocation translation adequacy) with that of two major MT systems, one statistical, and the other rule-based. Our results conﬁrm that syntactic variation affects translation quality and show that a deep syntactic approach is more robust in this sense, especially for languages with freer word order (e.g., German) and richer morphology (e.g., Italian) than English. 
 This paper discusses the automated translation of Norwegian nominal compounds into English, combining (a) compound segmentation, (b) component translation, (c) bi-lingual translation templates, and (d) probabilistic ranking. In this approach, a Norwegian compound will typically give rise to a large number of possible translations, and the selection of the ‘right’ candidate is approaches as an interesting machine learning problem. Our work extends the seminal approach of Tanaka and Baldwin in several ways, including a clariﬁcation of some ﬁne points of their earlier work, adaptation to a more adequate machine learning framework, application to a Germanic language with a small speech community and very limited existing resources, and systematic experimentation along several dimensions of variation.  
State-of-the-art statistical machine translation systems make use of a large translation table obtained after scoring a set of bilingual phrase pairs automatically extracted from a parallel corpus. The number of bilingual phrase pairs extracted from a pair of aligned sentences grows exponentially as the length of the sentences increases; therefore, the number of entries in the phrase table used to carry out the translation may become unmanageable, especially when online, ‘on demand’ translation is required in real time. We describe the use of closed-class words to ﬁlter the set of bilingual phrase pairs extracted from the parallel corpus by taking into account the alignment information and the type of the words involved in the alignments. On four European language pairs, we show that our simple yet novel approach can ﬁlter the phrase table by up to a third yet still provide competitive results compared to the baseline. Furthermore, it provides a nice balance between the unﬁltered approach and pruning using stop words, where the deterioration in translation quality is unacceptably high. 
In this paper we describe an approach to target language modeling which is based on a large treebank. We assume a bag of bags as input for the target language generation component, leaving it up to this component to decide upon word and phrase order. An experiment with Dutch as target language shows that this approach to candidate translation reranking outperforms standard n-gram modeling, when measuring output quality with BLEU, NIST, and TER metrics. 
This paper investigates the idea of adapting language models for phrases that have poor translation quality. We apply a selective adaptation criterion which uses a classiﬁer to locate the most difﬁcult phrase of each source language sentence. A special adapted language model is constructed for the highlighted phrase. Our adaptation heuristic uses lexical features of the phrase to locate the relevant parts of the parallel corpus for language model training. As we vary the experimental setup by changing the size of the SMT training data, our adaptation method consistently shows strong improvements over the baseline systems. 
 Statistically estimated phrase-based models promised to further the state-of-the-art, however, several works reported a performance decrease with respect to heuristically estimated phrase-based models. In this work we present a latent variable phrase-based translation model inspired by the hidden semi-Markov models, that does not degrade the system. Experimental results report an improvement over the baseline. Additionally, it is observed that both Baum-Welch and Viterbi trainings obtain the very same result, suggesting that most of the probability mass is gathered into one single bilingual segmentation.  
 Recent advances have allowed algorithms  that learn from aligned natural language  texts to exploit aligned sentences in more  than two languages. We investigate ways  of combining  N 2  bilingual aligned cor-  pora together to create a multilingual  aligned corpus across N languages. As a  result of the combination of several cor-  pora, our algorithms output a multilingual  corpus, with each aligned tuple assigned  a quality score called ‘strength’ that may  be used when learning from the multilin-  gual corpus. We show that the addition  of bilingual corpora used with alignment  strengths can signiﬁcantly improve Statis-  tical Machine Translation quality on an  Arabic→English task.  
Constraint satisfaction inference is presented as a generic, theory-neutral inference engine for machine translation. The approach enables the integration of many different solutions to aspects of the output space, including classiﬁcation-based translation models that take source-side context into account, as well as stochastic components such as target language models. The approach is contrasted with a word-based SMT system using the same decoding algorithm, but optimising a different objective function. The incorporation of sourceside context models in our model ﬁlters out many irrelevant candidate translations, leading to superior translation scores. 
Translation is an indispensable process for socioeconomic and cultural development in a multilingual society. The use of translation tools such as translation memories and machine translation systems can be beneficial in supporting the human translator. Unfortunately, the availability of these tools is limited for resource-scarce languages. In this paper, we introduce the Autshumato Integrated Translation Environment that facilitates a comprehensive set of translation tools, including machine translation and translation memory. The Autshumato Integrated Translation Environment is specifically developed for translation between the official South African languages, but it is in essence languageindependent and can therefore be used to translate between any language pair. 
In this paper we address the problem of translating between languages with word order disparity. The idea of augmenting statistical machine translation (SMT) by using a syntax-based reordering step prior to translation, proposed in recent years, has been quite successful in improving translation quality. We present a new technique for extracting syntax-based reordering rules, which are derived through a syntactically augmented alignment of source and target texts. The parallel corpus with reordered source side is then passed to an N -gram-based machine translation system and the obtained results are contrasted with a monotone system performance. In experiments, we show signiﬁcant improvement for the Chinese-to-English translation task. 
The paper addresses the issue of MT knowledge acquisition and describes a new hybrid methodology for automatic extraction of multi-word nominal terminology. The approach is based on statistical techniques merged into a strongly lexicalized Constraint Grammar paradigm. It is targeted at intelligent output and computationally attractive properties. 
This article describes an initial statistical machine translation system between Breton, a Celtic language spoken in France, and French. It also describes a method for leveraging existing resources from an incomplete rule-based machine translation system to improve the coverage and translation quality of the statistical system by generating expanded bilingual vocabulary lists. Results are presented which show that the use of this method improves the results of the system with respect to both the baseline, and the baseline with a lemmato-lemma bilingual lexicon. 
We present a series of empirical studies aimed at illuminating more precisely the likely contribution of semantic roles in improving statistical machine translation accuracy. The experiments reported study several aspects key to success: (1) the frequencies of types of SMT errors where semantic parsing and role labeling could help, and (2) if and where semantic roles offer more accurate guidance to SMT than merely syntactic annotation, and (3) the potential quantitative impact of realistic semantic role guidance to SMT systems, in terms of BLEU and METEOR scores. 
In this paper, we deal with the problem of a large number of unaligned words in automatically learned word alignments for machine translation (MT). These unaligned words are the reason for ambiguous phrase pairs extracted by a statistical phrase-based MT system. In translation, this phrase ambiguity causes deletion and insertion errors. We present hard and optional deletion approaches to remove the unaligned words in the source language sentences. Improvements in translation quality are achieved both on large and small vocabulary tasks with the presented methods. 
Current approaches to statistical machine translation try to incorporate more structure into the translation process by including explicit syntactic information in form of a formal grammar (with a possible, but not necessary, correspondence to a linguistic motivated grammar). These more structured models incur into an increased generation cost, and efﬁcient algorithms must be developed. In this paper we concentrate on the cube growing algorithm, a lazy version of the cube grow algorithm. The efﬁciency of this algorithm depends on a heuristic for language model computation, which is only scarcely discussed in the original paper. In this paper we investigate the effect of this heuristic on translation performance and efﬁciency and propose a new heuristic which efﬁciently decreases memory requirements and computation time, while maintaining translation performance. 
We introduce a syntactically enhanced word alignment model that is more ﬂexible than state-of-the-art generative word alignment models and can be tuned according to different end tasks. First of all, this model takes the advantages of both unsupervised and supervised word alignment approaches by obtaining anchor alignments from unsupervised generative models and seeding the anchor alignments into a supervised discriminative model. Second, this model offers the ﬂexibility of tuning the alignment according to different optimisation criteria. Our experiments show that using our word alignment in a Phrase-Based Statistical Machine Translation system yields a 5.38% relative increase on IWSLT 2007 task in terms of BLEU score. 
